2006.iwslt-evaluation.21,1983.tc-1.13,0,0.576574,"specific processing are presented in Sections 6 and 7. We then present experiments and the official evaluation results. Section 10 describes additional analyses performed after the official evaluation and Section 11 concludes. 2. Data The UW system participated in the open data track. For training we used the BTEC Italian-English training data provided for this evaluation campaign, along with the devset1, devset2, and devset3, resulting in approximately 190K words of in-domain training data (including punctuation). In addition, we used the publicly available Europarl corpus of Italian/English [1] for training the translation model. This corpus is very different from BTEC in that it contains edited transcriptions of parliamentary proceedings; thus, the domain differs from that of a travel task, and the style is that of written text. The size of the Europarl corpus is approximately 17M words. We also used the Fisher corpus for training certain second-pass language models. The Fisher corpus is a collection of English conversational telephone speech covering a variety of speakers and topics. It consists of approximately 2.3M word tokens. All development/evaluation was done on devset4, sin"
2006.iwslt-evaluation.21,koen-2004-pharaoh,0,0.0495537,"immediata scarcerazione. However, these modification did not affect translation performance significantly. 145 4. First-Pass Translation System Two analogous lexical scores are computed, e.g.: We use a multi-pass statistical phrase-based translation system based on a log-linear probability model: e∗ = argmaxe p(e|f ) = argmaxe { K X λk φk (e, f )} (1) where e is and English and f a foreign sentence, φ(e, f ) is a feature function defined on both sentences, and λ is a feature weight. The first pass generates up to 2000 translation hypotheses per sentence using the public-domain Pharaoh decoder [2] and a combination of the following nine model scores: two phrase-based translation scores two lexical translation scores data source indicator feature word transition penalty phrase penalty distortion penalty language model score X 1 p(fj |ei ) |{i|a(i) = j}| (3) a(i)=j where j ranges over words in phrase f¯ and i ranges over words in phrase e¯. Here, we use two phrase tables concomitantly, one trained from each data source (BTEC and Europarl). We use the two phrase tables jointly, without renormalization of probabilities. An additional binary feature in the log-linear combination indicates w"
2006.iwslt-evaluation.21,P03-1021,0,0.0553976,"o of these are explained below (Section 4.1). The word transition and phrase penalty are constant weights added for each word/phrase used in the translation, thus controlling the length of the translation. The distortion penalty assigns a weight proportional to the distance by which phrases are reordered during decoding; here, the distortion penalty is constant since monotone decoding is used and no reordering is allowed. (Initial experiments show that monotone decoding outperforms non-monotone decoding.) Weights for these scores are optimized using the minimumerror rate training procedure in [3]. The optimization criterion is the BLEU score on the development set as defined above (Section 2). The second pass rescores the first-pass output with additional, more advanced model scores. A postprocessing step is then performed to restore true case and punctuation. 4.1. Translation Model The translation model is defined over a segmentation of source and target sentence into phrases: f = f¯1 , f¯2 , ..., f¯M and e = e¯1 , e¯2 , ..., e¯M . Phrase pairs of up to length 7 are extracted from the training corpus which was previously wordaligned using GIZA++. The extraction method is the techniqu"
2006.iwslt-evaluation.21,J03-1002,0,0.00589373,"ion criterion is the BLEU score on the development set as defined above (Section 2). The second pass rescores the first-pass output with additional, more advanced model scores. A postprocessing step is then performed to restore true case and punctuation. 4.1. Translation Model The translation model is defined over a segmentation of source and target sentence into phrases: f = f¯1 , f¯2 , ..., f¯M and e = e¯1 , e¯2 , ..., e¯M . Phrase pairs of up to length 7 are extracted from the training corpus which was previously wordaligned using GIZA++. The extraction method is the technique described in [4] and implemented in [2]: the corpus is first aligned in both translation directions, the intersection of the alignment points is taken, and additional alignment points are added heuristically. For each phrase pair, two phrasal ¯ e) and P (¯ ¯ are computed translation probabilities, P (f|¯ e|f), (one for each direction) from the relative frequency estimate on the training data, e.g.: count(¯ e, f¯) P (¯ e|f¯) = ¯ count(f) J Y j=1 k=1 • • • • • • • Scorelex (f¯|¯ e) = (2) 5. Rescoring The rescoring stage uses the first pass model scores along with five additional scores, as described below. Scor"
2006.iwslt-evaluation.21,N04-1021,0,0.0566079,"tional scores are: • • • • • a 4-gram language model score a POS n-gram score rank in N-best list Factored Language Model score ratio, and focused language model score The last three are novel features in our system. 4-gram language model score (lm) This is the score of a 4-gram language model trained on the English side of the BTEC training corpus using modified Kneser-Ney smoothing. POS n-gram model score (pos) The part-of-speech (POS) sequence of a given target sentence can be indicative of the sentence’s syntactical wellformedness and thus translation fluency. Although it was cautioned in [6] that applying POS taggers directly to MT hypotheses may generate unexpected results (e.g. inserting a verb tag when there is no verb in the sentence), in practice we have found it useful to apply a POS language model to our N-best lists. We obtain POS annotations by applying the Maximum Entropy tagger of [7]. This tagger has been trained on the Wall Street Journal corpus; we apply it directly to our training set and N-best lists. In order to increase the training data for the POS n-gram we also used the Fisher 146 250 histogram count (bin=50) 200 150 100 50 0 0 100 200 300 400 rank of oracle"
2006.iwslt-evaluation.21,N03-2002,1,0.881972,"nk:2) the store is open on sundays (rank:1) the store is it open on sundays (rank:3) For our experiments, we slightly modified the above rank feature by applying a log function to the raw values. This bounds the features to a smaller range, similar to that of other features in the log-linear combination. We found that this did slightly better in our experiments than raw integer ranks. As shown in experiments (Section 8), the rank feature, is consistently the most useful feature in rescoring despite its simplicity. Ratio of Factored Language Model scores (ratio) Factored Language Models (FLMs) [8] are a flexible language modeling framework that can incorporate diverse sources of information, such as morphology, POS tags, etc. Previous experiments on using FLMs to rescore machine translation N-best lists have seen mixed results: little gain was shown for translation into English [9] but larger gains were shown for translation into Spanish, a morphologically richer language, especially under mismatched conditions [10]. Here, we use FLMs with three sources of information: words, part-of-speech, and data-driven word clusters in a trigram context. Word clusters were obtained by Brown cluste"
2006.iwslt-evaluation.21,W05-0821,1,0.855585,"ar combination. We found that this did slightly better in our experiments than raw integer ranks. As shown in experiments (Section 8), the rank feature, is consistently the most useful feature in rescoring despite its simplicity. Ratio of Factored Language Model scores (ratio) Factored Language Models (FLMs) [8] are a flexible language modeling framework that can incorporate diverse sources of information, such as morphology, POS tags, etc. Previous experiments on using FLMs to rescore machine translation N-best lists have seen mixed results: little gain was shown for translation into English [9] but larger gains were shown for translation into Spanish, a morphologically richer language, especially under mismatched conditions [10]. Here, we use FLMs with three sources of information: words, part-of-speech, and data-driven word clusters in a trigram context. Word clusters were obtained by Brown clustering [11] using 500 word classes. In this work, we apply FLMs to rescore English, but improve upon previous attempts by using two FLMs together in a discriminative fashion. In order to train the backoff structure and smoothing options of an FLM we use a genetic algorithm [12]. This require"
2006.iwslt-evaluation.21,J92-4003,0,0.173358,"a flexible language modeling framework that can incorporate diverse sources of information, such as morphology, POS tags, etc. Previous experiments on using FLMs to rescore machine translation N-best lists have seen mixed results: little gain was shown for translation into English [9] but larger gains were shown for translation into Spanish, a morphologically richer language, especially under mismatched conditions [10]. Here, we use FLMs with three sources of information: words, part-of-speech, and data-driven word clusters in a trigram context. Word clusters were obtained by Brown clustering [11] using 500 word classes. In this work, we apply FLMs to rescore English, but improve upon previous attempts by using two FLMs together in a discriminative fashion. In order to train the backoff structure and smoothing options of an FLM we use a genetic algorithm [12]. This requires a held-out set for iteratively optimizing the model parameters. While normally the references for some development set would be used for this purpose, in the context of machine translation we use the oracle-best hypotheses from the first pass, to ensure that the model is optimized on hypotheses that are likely to re"
2006.iwslt-evaluation.21,C04-1022,1,0.893952,"n into English [9] but larger gains were shown for translation into Spanish, a morphologically richer language, especially under mismatched conditions [10]. Here, we use FLMs with three sources of information: words, part-of-speech, and data-driven word clusters in a trigram context. Word clusters were obtained by Brown clustering [11] using 500 word classes. In this work, we apply FLMs to rescore English, but improve upon previous attempts by using two FLMs together in a discriminative fashion. In order to train the backoff structure and smoothing options of an FLM we use a genetic algorithm [12]. This requires a held-out set for iteratively optimizing the model parameters. While normally the references for some development set would be used for this purpose, in the context of machine translation we use the oracle-best hypotheses from the first pass, to ensure that the model is optimized on hypotheses that are likely to result in a good BLEU score. Here, we form two held-out sets, one consisting of the set of oracle best hypotheses from the N-best lists, and the other consisting of the set of oracle worst hypotheses from the N-best lists. The FLM optimized on the oracle best hypothese"
2006.iwslt-evaluation.21,N04-1023,0,0.0463176,"est hypotheses from the N-best lists, and the other consisting of the set of oracle worst hypotheses from the N-best lists. The FLM optimized on the oracle best hypotheses should give high probability to sentences with high BLEU scores, while the FLM optimized on the oracle worst sentences will give high probability to sentences with low BLEU scores. The score used for rescoring then LM1 (e) is φratio (e) = F F LM2 (e) , where F LMi (e), i = 1, 2 is the probability of sentence e evaluated by the first and second FLMs, respectively. This method is analogous to the “splitting” technique used in [13], which divides the N-best list into good and bad sentences for training a perceptron-style learner. Our method differs in that instead of using a discriminative classifier, we use two generative models (FLMs) and take the log-probability ratio. This allows us to take advantage of the estimation techniques developed for language models. Focused LM (focus, focusF) The focused language model is a dynamically generated language model that focuses only on those words that occur in the N-best list. During the training phase of rescoring, we collect all the words in the N-best lists and use our trai"
2006.iwslt-evaluation.21,E06-1005,0,0.0472346,"Missing"
2009.iwslt-evaluation.19,P05-1071,0,0.0368422,"op in BLEU on the held-out set. However, we experimented with combining both tables. To this end the individual tables were combined into a single table containing the 11 standard features (phrasal, lexical, and reordering scores and phrase penalty) plus two additional binary features that indicate which alignment model produced each entry in the phrase table. The weights for these features were optimized along with all other features in the first-pass MERT tuning. 4.3. Language Models 5.1. Preprocessing We preprocessed the Arabic data by using the the Columbia University MADA and TOKAN tools [5]. We compared two tokenization schemes: the first splits off the conjunctions w+, f+, the particles l+, the b+ preposition and the definite article Al+. It also normalizes different variants of alif, final yaa and taa marbuta. The second scheme (equivalent to TOKAN’s D2 scheme) does not split off Al+ but instead separates the prefix s+. Differences between the two schemes were slight; the first scheme yielded a 0.2 increase in BLEU on the heldout set. 5.2. Word Alignment and Phrase Tables As in the Chinese-English system, we trained word alignments using both GIZA++ and MTTK. We found that MTT"
2009.iwslt-evaluation.19,N03-1017,0,0.00778709,"-best lists, which has previously shown improvements on 2007 IWSLT data. We additionally explored different preprocessing schemes for both language pairs, as well as methods for combining phrase tables based on different word alignments. In the following sections we first describe the data, general baseline system and post-processing steps, before describing language-pair specific methods and the semi-supervised reranking method. Our baseline system for this year’s task is a state-of-the-art, two-pass phrase-based statistical machine translation system, based on a log-linear translation model [7]. e∗ = argmaxe p(e|f ) = argmaxe { K X λk φk (e, f )} (1) k=1 where e is an English sentence, f is a foreign sentence, φk (e, f ) is a feature function defined on both sentences, and λk is a feature weight. We trained this model within the Moses development and decoding framework [8]. The feature functions used in this year’s system include: • two phrase-based translation scores, one for each translation direction 2. Corpora and Preprocessing As mandated by the evaluation guidelines, the only data that was used for system development was the official data provided by IWSLT. Training data for t"
2009.iwslt-evaluation.19,P07-2045,0,0.0428537,"e data, general baseline system and post-processing steps, before describing language-pair specific methods and the semi-supervised reranking method. Our baseline system for this year’s task is a state-of-the-art, two-pass phrase-based statistical machine translation system, based on a log-linear translation model [7]. e∗ = argmaxe p(e|f ) = argmaxe { K X λk φk (e, f )} (1) k=1 where e is an English sentence, f is a foreign sentence, φk (e, f ) is a feature function defined on both sentences, and λk is a feature weight. We trained this model within the Moses development and decoding framework [8]. The feature functions used in this year’s system include: • two phrase-based translation scores, one for each translation direction 2. Corpora and Preprocessing As mandated by the evaluation guidelines, the only data that was used for system development was the official data provided by IWSLT. Training data for the BTEC tasks consisted of approximately 20,000 sentence pairs in both the ChineseEnglish and Arabic-English tracks. We used the combined development datasets (about 500 sentences each) for initial system tuning, except for the IWSLT 2008 eval set, which we used as a held-out set for"
2009.iwslt-evaluation.19,J03-1002,0,0.0225109,"es the probablity of a sequence of orientations o = (o1 , o2 , . . . , oM ) P (o|f, e) = M Y P (oi |¯ ei , f¯ai , ai , ai−1 ) (4) i=1 where each oi takes one of the three values: monotone, swap, and discontinuous. This model adds six feature functions to the overall log-linear model: for each of the three orientations, the orders of the source phrase with respect to both the previous and the next source phrase are considered. The feature scores are again estimated by relative frequency. The training corpus was word-aligned by GIZA++; subsequently, phrases were extracted using the technique in [6] and as implemented in the Moses training scripts [8]. We also used an alternative word-alignment based on the MTTK [6] implementation of an HMM-based word-to-phrase alignment model with bigram probabilities. This yielded mixed results, as described in later sections. Word count and phrase count penalties are constant weights added for each word/phrase used in the translation; the distortion penalty is a weight that increases in proportion to the number of positions by which phrases are reordered during translation. The language models used are n-gram models as further described below. The wei"
2009.iwslt-evaluation.19,P03-1021,0,0.012006,"of an HMM-based word-to-phrase alignment model with bigram probabilities. This yielded mixed results, as described in later sections. Word count and phrase count penalties are constant weights added for each word/phrase used in the translation; the distortion penalty is a weight that increases in proportion to the number of positions by which phrases are reordered during translation. The language models used are n-gram models as further described below. The weights for these scores were optimized using an in-house implementation of the minimum-error rate training (MERT) procedure developed in [9]. Our optimization criterion was the BLEU score on the available development set. 3.2. Language Models For first-pass decoding we used trigram language models. We built all of our language models using the SRILM toolkit [4] with modified Kneser-Ney discounting and interpolating all n-gram estimates of order > 1. Due to the small size of the training corpus, we experimented with lowering the minimum count requirement to 1 for all n-grams. This yielded different results for the two different tasks, which are further described below. 3.3. Decoding Our system used the Moses decoder to generate 100"
2009.iwslt-evaluation.19,W08-0336,0,0.0155344,"ut by re-attaching the possessive particle and restoring true case. Truecasing is done by a noisy-channel model as implemented in the disambig tool in the SRILM package. It uses a 4-gram model trained over a mixed-case representation of the BTEC training corpus and a probabilistic mapping table for lowercaseuppercase word variants. The first letter at the beginning of each sentence was uppercased deterministically. 4. Chinese → English 4.1. Preprocessing Although the Chinese training data was pre-segmented we nonetheless explored other segmentation tools. First, we used the Stanford segmenter [2] to resegment the Chinese data, as it provides templates for annotating numbers and dates, potentially aiding in word alignment and phrase extraction. In another experiment, an in-house tool [11] was used to simply markup dates and numbers in the presegmented BTEC data. Third, we developed our own markup tool for numbers. In both Chinese and English, numbers are represented by a combination of a limited set of number words. A simple method for detecting numbers is to first obtain a set of number words and then search for subsentential chunks that are comprised of only number words. These chunk"
2010.iwslt-evaluation.19,2005.iwslt-1.19,0,\N,Missing
2010.iwslt-evaluation.19,W09-0424,0,\N,Missing
2010.iwslt-evaluation.19,P05-1056,0,\N,Missing
2010.iwslt-evaluation.19,2009.iwslt-evaluation.5,0,\N,Missing
2010.iwslt-evaluation.19,2005.eamt-1.19,0,\N,Missing
2010.iwslt-evaluation.19,zhang-etal-2004-interpreting,0,\N,Missing
2010.iwslt-evaluation.19,2010.iwslt-papers.5,1,\N,Missing
2010.iwslt-papers.5,D10-1044,0,\N,Missing
2010.iwslt-papers.5,W08-0334,0,\N,Missing
2010.iwslt-papers.5,W09-0424,0,\N,Missing
2010.iwslt-papers.5,D09-1040,0,\N,Missing
2010.iwslt-papers.5,J07-3002,0,\N,Missing
2010.iwslt-papers.5,D09-1074,0,\N,Missing
2010.iwslt-papers.5,D08-1090,0,\N,Missing
2010.iwslt-papers.5,W10-1759,0,\N,Missing
2010.iwslt-papers.5,J04-4002,0,\N,Missing
2010.iwslt-papers.5,P06-1002,0,\N,Missing
2010.iwslt-papers.5,D07-1036,0,\N,Missing
2010.iwslt-papers.5,2005.mtsummit-papers.11,0,\N,Missing
2010.iwslt-papers.5,W10-1713,0,\N,Missing
2010.iwslt-papers.5,2009.mtsummit-papers.5,0,\N,Missing
2011.mtsummit-papers.11,P06-1002,0,0.0218152,"s et al., 2009; Foster et al., 2010). A promising aspect about the latter two papers, in particular, is that they are able to incorporate supervised information (likelihood or expected TER on the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marcu, 2009). There are also adaptation methods that do not target a particular step in the training pipeline. For example, the information retrieval approach (Hildebrand et al., 2005) begins by identifying a subset of out-of-domain bitext most similar to in-domain; this data subset can be used for any (or all) steps of the training pipeline. An alternative approach is to train separate translation models for in-domain and out-of-domain data, then combine the final models log-linearly (Koehn and Schroeder, 2007) or dynamically (Finch and Sumita, 2008; L¨u et al., 2007). 5 Conclusi"
2011.mtsummit-papers.11,W09-0432,0,0.0169878,"ng (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A"
2011.mtsummit-papers.11,J07-2003,0,0.0432673,"archical rules) required for the extraction/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in"
2011.mtsummit-papers.11,W07-0722,0,0.0256665,", and (b) if so, which step. Following the training pipeline in Section 1, we briefly survey the approaches in prior work: 1) Word alignment model training: Assume a probabilistic model of alignment, where the parameters (e.g. lexical translation probabilities in the IBM Models) are estimated from a mix of in-domain 119 and out-of-domain data. One method is to interpolate separate sets of parameters estimated from indomain and out-domain data. Wu et. al.(2005) sets the interpolation weights to be proportional to the relative frequency of observances in in-domain and out-of-domain data, while (Civera and Juan, 2007) treats it as a hidden parameter in a mixture model. These methods are similar to ours in that the motivation is to improve alignments, but differs in that the focus is on training (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual cor"
2011.mtsummit-papers.11,P11-1043,0,0.0359624,"Missing"
2011.mtsummit-papers.11,P09-2058,0,0.0240098,"ed from indomain and out-domain data. Wu et. al.(2005) sets the interpolation weights to be proportional to the relative frequency of observances in in-domain and out-of-domain data, while (Civera and Juan, 2007) treats it as a hidden parameter in a mixture model. These methods are similar to ours in that the motivation is to improve alignments, but differs in that the focus is on training (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that al"
2011.mtsummit-papers.11,2010.iwslt-papers.5,1,0.759658,"data. Outof-domain data is heterogeneous, consisting of Hansards, broadcast conversations, weblogs, etc. The data statistics are shown in Table 1. We compare 3 phrasal SMT systems: • in-domain model: Step1: Train word alignment model on in-domain bitext (M in ). Step 2-to-4: Alignment inference on in-domain text, followed by phrase extraction and scoring. • general-domain model: Step1: Train word alignment model on concatenated in-domain and out-of-domain bitext (M gen ). Step2-to-4: same as in-domain model. This simple approach is a strong adaptation baseline competitive in many tasks, c.f. (Duh et al., 2010). • bayes: Step1-to-2: Algorithm 1. Step3-to-4: same as in-domain model. Note that out-of-domain information is used only up to step 2 and excluded in further steps of the 4 www.itl.nist.gov/iad/mig/tests/mt/doc/ 118 pipeline. This clarifies the analysis: if we were to include out-of-domain bitext for phrase extraction, our SMT system might acquire new phrases, which reduces out-of-vocabulary rate and confounds the analysis of alignment inference results. Further, from preliminary experiments, we found that adding out-of-domain bitext to all four steps actually degrade results sometimes, due t"
2011.mtsummit-papers.11,W08-0334,0,0.0154527,"weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marcu, 2009). There are also adaptation methods that do not target a particular step in the training pipeline. For example, the information retrieval approach (Hildebrand et al., 2005) begins by identifying a subset of out-of-domain bitext most similar to in-domain; this data subset can be used for any (or all) steps of the training pipeline. An alternative approach is to train separate translation models for in-domain and out-of-domain data, then combine the final models log-linearly (Koehn and Schroeder, 2007) or dynamically (Finch and Sumita, 2008; L¨u et al., 2007). 5 Conclusions and Future Work We proposed a flexible and efficient method for domain adaptation in machine translation. The idea is to decompose the word alignment process into model training and alignment inference, and view the latter as a sequential Bayesian update problem. The advantages of our approach are: 1. Its modularity enables the use of any model training algorithm for word alignment, as long as it outputs N-best lists or posteriors. 2. It gives consistent improvements over a multitude of datasets (2 tasks and 11 language pairs). We have shown how alignment inf"
2011.mtsummit-papers.11,D10-1044,0,0.0138276,"oldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A promising aspect about the latter two papers, in particular, is that they are able to incorporate supervised information (likelihood or expected TER on the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marc"
2011.mtsummit-papers.11,N04-1035,0,0.0363765,"l we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in-domain and general-domain bitexts. The contribution of this paper i"
2011.mtsummit-papers.11,2005.eamt-1.19,0,0.0322357,"the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marcu, 2009). There are also adaptation methods that do not target a particular step in the training pipeline. For example, the information retrieval approach (Hildebrand et al., 2005) begins by identifying a subset of out-of-domain bitext most similar to in-domain; this data subset can be used for any (or all) steps of the training pipeline. An alternative approach is to train separate translation models for in-domain and out-of-domain data, then combine the final models log-linearly (Koehn and Schroeder, 2007) or dynamically (Finch and Sumita, 2008; L¨u et al., 2007). 5 Conclusions and Future Work We proposed a flexible and efficient method for domain adaptation in machine translation. The idea is to decompose the word alignment process into model training and alignment i"
2011.mtsummit-papers.11,W07-0733,0,0.0180875,"nslation performance, some work have shown weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marcu, 2009). There are also adaptation methods that do not target a particular step in the training pipeline. For example, the information retrieval approach (Hildebrand et al., 2005) begins by identifying a subset of out-of-domain bitext most similar to in-domain; this data subset can be used for any (or all) steps of the training pipeline. An alternative approach is to train separate translation models for in-domain and out-of-domain data, then combine the final models log-linearly (Koehn and Schroeder, 2007) or dynamically (Finch and Sumita, 2008; L¨u et al., 2007). 5 Conclusions and Future Work We proposed a flexible and efficient method for domain adaptation in machine translation. The idea is to decompose the word alignment process into model training and alignment inference, and view the latter as a sequential Bayesian update problem. The advantages of our approach are: 1. Its modularity enables the use of any model training algorithm for word alignment, as long as it outputs N-best lists or posteriors. 2. It gives consistent improvements over a multitude of datasets (2 tasks and 11 language"
2011.mtsummit-papers.11,N03-1017,0,0.00872408,"d to particular model formalisms (e.g. phrase vs. hierarchical rules) required for the extraction/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively b"
2011.mtsummit-papers.11,2005.mtsummit-papers.11,0,0.0204779,"ple is counted equally. It may be beneficial to have a parameter if it can be tuned well without of overfitting, but we do not consider it here. 3 Experiments 3.1 Datasets and Setup We evaluate our proposed method under two tasks: The EMEA task involves the translation of medical texts from the European Medical Agency (Tiedemann, 2009). We test on ten language pairs–Danish (da), German (de), Greek (el), Spanish (es), Finnish (fi), French (fr), Italian (it), Dutch (nl), Portuguese (pt), and Swedish (sv)–all translating into English. The out-of-domain data are parliamentary texts from Europarl (Koehn, 2005). The NIST task involves translating newswire text using Chinese-to-English NIST OpenMT 2008 data. da in-domain 45.3 general 46.1 bayes 47.1* de 35.5 36.1 36.4* el 41.3 41.6 42.5* es 45.0 46.9 46.2 EMEA fi fr 33.6 46.8 34.0 47.8 34.6* 47.9 it 47.7 49.2 49.3* nl 45.6 46.1 46.2* pt 46.3 47.0 47.5* sv 45.3 45.2 45.9* NIST mt06 mt08 27.7 24.4 28.7 24.6 28.7 25.0* Table 2: Main Results: Test BLEU for EMEA (da,de,...,sv) and NIST (mt06,08): Best results are in bold-font. Statistical significant improvement over general-domain model is indicated by asterisk (*). Language Pair In-domain #sent train #w"
2011.mtsummit-papers.11,D09-1106,0,0.0176991,"m might acquire new phrases, which reduces out-of-vocabulary rate and confounds the analysis of alignment inference results. Further, from preliminary experiments, we found that adding out-of-domain bitext to all four steps actually degrade results sometimes, due to increased ambiguity of additional translation options on the target side. For all systems, we use the Moses decoder, adapted SRILM 3gram (EMEA) and 4gram (NIST), MERT for weight optimization, and GIZA++ (Model4) as the underlying word alignment training tool. Phrase tables are extracted from alignment matrices using the method of (Liu et al., 2009). 3.2 Main Results Table 2 summarizes all our main results. We see that bayes gives robust improvements in testset BLEU. For example, for Danish-to-English translation, using in-domain data by itself achieves 45.3 BLEU (in-domain). This can be improved to 46.1 BLEU by concating out-of-domain data (general). The proposed method, however, further improves the result to 47.1 BLEU (bayes). For the NIST dataset, we see that bayes improves upon general and indomain for the MT08 testset, and ties with general for the MT06 testset. On average, bayes improves over in-domain model by 1.1 BLEU points. Fu"
2011.mtsummit-papers.11,D07-1036,0,0.0487968,"Missing"
2011.mtsummit-papers.11,D09-1040,0,0.0212628,"s it as a hidden parameter in a mixture model. These methods are similar to ours in that the motivation is to improve alignments, but differs in that the focus is on training (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to"
2011.mtsummit-papers.11,D09-1074,0,0.0179112,"on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A promising aspect about the latter two papers, in particular, is that they are able to incorporate supervised information (likelihood or expected TER on the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between the two (Ayan and Dorr,"
2011.mtsummit-papers.11,P08-1023,0,0.0144565,"ndard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in-domain and general-domain bitexts. The contribution of this paper is two-fold: • We i"
2011.mtsummit-papers.11,J04-4002,0,0.0486609,"l formalisms (e.g. phrase vs. hierarchical rules) required for the extraction/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertain"
2011.mtsummit-papers.11,P05-1034,0,0.039829,"on/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in-domain and general-domain bitexts. The contrib"
2011.mtsummit-papers.11,W10-1759,0,0.0106112,"e translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A promising aspect about the latter two papers, in particular, is that they are able to incorporate supervised information (likelihood or expected TER on the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between"
2011.mtsummit-papers.11,D08-1090,0,0.0212051,"meter in a mixture model. These methods are similar to ours in that the motivation is to improve alignments, but differs in that the focus is on training (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation invol"
2011.mtsummit-papers.11,P07-1004,0,0.0168471,"ent inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A promising aspect about"
2011.mtsummit-papers.11,P05-1058,0,0.0309943,"3, except we use scores from the general model M gen rather than M in ; βij is calculated analogously to Eq. 4:  βij = A ∈N 2.4 lgen (A )δ(Aij = 0)/Z (8) I J gen (e1 ,f1 ) Summary and Caveat The pseudocode for the overall algorithm is presented in Algorithm 1. Basically, the alignment matrix posteriors are computed for each sentence pair by combining statistics from in-domain and generaldomain models. It is worth noting two caveats: • Our Bayesian view is that there is a prior alignment matrix, which is updated by in-domain model statistics. This differs from previous work in Step 1, e.g. (Wu et al., 2005), which adopts a prior for alignment model parameters. The distinction between adapting inference results and model parameters is an important one, and this is what gives us a flexible generalpurpose method. • Eq. 6 does not contain a tuning parameter between likelihood aij and prior αij . This arises from the sequential Bayesian update perspective, where each additional sample is counted equally. It may be beneficial to have a parameter if it can be tuned well without of overfitting, but we do not consider it here. 3 Experiments 3.1 Datasets and Setup We evaluate our proposed method under two"
2011.mtsummit-papers.11,J97-3002,0,0.110319,") required for the extraction/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in-domain and"
2011.mtsummit-papers.11,zhang-etal-2004-interpreting,0,0.0273166,"LEU. For example, for Danish-to-English translation, using in-domain data by itself achieves 45.3 BLEU (in-domain). This can be improved to 46.1 BLEU by concating out-of-domain data (general). The proposed method, however, further improves the result to 47.1 BLEU (bayes). For the NIST dataset, we see that bayes improves upon general and indomain for the MT08 testset, and ties with general for the MT06 testset. On average, bayes improves over in-domain model by 1.1 BLEU points. Further, in 9 of 12 cases, bayes also outperforms general-domain model by statistically significant margins, p <0.05 (Zhang et al., 2004). We thus conclude that the proposed method is robust under adaptation scenarios. 3.3 Analyses of Alignments We are also interested in checking if BLEU improvements correlate with quantifiable alignment improvements. This evaluation is possible since the NIST dataset contains some manual alignment annotations (LDC2006E93). We identified 892 sentence-pairs in our in-domain bitext that have manual alignments. Note that this supervised information is never used in any part of our method. Figure 1 shows alignment precision/recall. The curve is computed by thresholding the estimated weighted aligme"
2011.mtsummit-papers.34,2008.iwslt-papers.1,0,0.0222347,"presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited. Bertoldi et al. (2008) evaluated several methods of pivot languages but did apply the global corpus ﬁltering i.e. compatibility measure to control the quality of data. Our purpose is not only to improve the translation quality but also to provide useful linguistic resources for other NLP tasks. 5 Conclusion and Future Work Thousands of human languages are recognized in the world, and building up millions of translation systems between these language pairs suffers greatly on the scarce resource, such as parallel data. We introduced the idea of compatibility, where all languages can be mapped to the same semantic mea"
2011.mtsummit-papers.34,J93-2003,0,0.0230862,"f each sentence pair and choose a certain percentage of the best scored sentences for training. In order to include information from various resources, the quality of a sentence pair is measured using a log-linear model combining different sub-models. Let (f1J , eI1 ) be a bilingual sentence, the evaluation is performed using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and e"
2011.mtsummit-papers.34,2002.tmi-tutorials.2,0,0.0607427,"n the idea of compatibility. Generally speaking, the quality of compatible predictions provided by multiple systems is more reliable. For simple classiﬁcation problems, it is reasonable to take a prediction as good which the multiple systems agree on. This idea is widely used in ensemble learning and semi-supervised learning. Take Bootstrap aggregating, a meta-algorithm for ensemble learning as an example, multiple models are separately trained on randomly generated sub-samples, and then vote to achieve ﬁnal predictions. Another example closely related to our method is co-training such as in (Callison-Burch, 2002). One way to select automatic predictions for re-training in co-training is to choose the agreed ones. Different from simple classiﬁcation problems, even complex structured prediction problems such as parsing, the output of MT is in human languages, which may be the most complicated way to represent the meaning of another human language. It is too strict to ask multiple systems to provide exactly the same translated sentence for an input. We extend the agreement idea to the compatibility idea. Informally, two sentences are called compatible if they express the same meaning to some extent. We c"
2011.mtsummit-papers.34,P07-1092,0,0.0141798,"language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited."
2011.mtsummit-papers.34,P08-1010,0,0.0304425,"Missing"
2011.mtsummit-papers.34,2008.eamt-1.6,0,0.0108565,"d Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by"
2011.mtsummit-papers.34,W09-0431,0,0.013434,"e, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical b"
2011.mtsummit-papers.34,P07-2045,0,0.00524468,"d using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a statistical 5-gram model with modiﬁed Kneser-Ney smoothing estimated using SRI-LM toolkit (Stolcke, 2002). E"
2011.mtsummit-papers.34,2005.mtsummit-papers.11,0,0.0225646,"ll scarce resourced language pairs. The generated virtual parallel corpus can not only be applied into MT but also other NLP tasks. 2 2.1 Generating Virtual Parallel Data Background and Motivation There are only a few parallel corpora publicly available for some languages we work on. The JRC-Acquis(JRC) is a huge collection of European Union legislative documents translated into more than twenty ofﬁcial European languages (Steinberger et al., 2006). The European Parliament Proceedings Parallel Corpus (Europarl corpus) was extracted from the proceedings of the European Parliament (1996-today) (Koehn, 2005). News Commentary(NC) (SMT, 2011) and SETimes (SETIMES, 2011) are corpora collected from the news domains. In this paper, we are concerned with generating high-quality, virtual parallel data for machine translation. To do this, we exploit multiple parallel corpora in different language pairs. In particular, we generate parallel corpora for scarce resourced languages, taking Romanian to German as a case study for simplicity. We can also take German to Romanian or other language directions. In order to ﬁnd out the gap between the translation quality on better studied language pairs and that on l"
2011.mtsummit-papers.34,D07-1005,0,0.0133387,"Section 1 and Section 2.3. Uefﬁng et al. (2009) explored model adaptation methods to use the monolingual data from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation sys"
2011.mtsummit-papers.34,2010.iwslt-papers.12,0,0.0117409,"les obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited. Bertoldi et al. (2008) evaluated several methods of pivot languages but did apply the global corpus ﬁltering i.e. compatibility measure to control the quality of data. Our purpose is not only to improve the translation quality but also to provide useful linguistic resources for other NLP tasks. 5 Conclusion and Future Work Thousands of human languages are recognized in the world, and building up millions o"
2011.mtsummit-papers.34,N01-1020,0,0.0187021,"rget languages. Callison-Burch (2002) presented a co-training method for SMT, the agreement of multiple translation systems is explored to ﬁnd the best translation for re-training. We applied compatibility instead of agreement based approach, detailed description on the difference between compatibility and agreement is referred to Section 1 and Section 2.3. Uefﬁng et al. (2009) explored model adaptation methods to use the monolingual data from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 20"
2011.mtsummit-papers.34,N04-1034,0,0.0782549,"Missing"
2011.mtsummit-papers.34,J03-1002,0,0.00497387,"model combining different sub-models. Let (f1J , eI1 ) be a bilingual sentence, the evaluation is performed using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a s"
2011.mtsummit-papers.34,P02-1040,0,0.0841189,"Missing"
2011.mtsummit-papers.34,N10-1063,0,0.039881,"Missing"
2011.mtsummit-papers.34,W11-2100,0,0.0454475,"The generated virtual parallel corpus can not only be applied into MT but also other NLP tasks. 2 2.1 Generating Virtual Parallel Data Background and Motivation There are only a few parallel corpora publicly available for some languages we work on. The JRC-Acquis(JRC) is a huge collection of European Union legislative documents translated into more than twenty ofﬁcial European languages (Steinberger et al., 2006). The European Parliament Proceedings Parallel Corpus (Europarl corpus) was extracted from the proceedings of the European Parliament (1996-today) (Koehn, 2005). News Commentary(NC) (SMT, 2011) and SETimes (SETIMES, 2011) are corpora collected from the news domains. In this paper, we are concerned with generating high-quality, virtual parallel data for machine translation. To do this, we exploit multiple parallel corpora in different language pairs. In particular, we generate parallel corpora for scarce resourced languages, taking Romanian to German as a case study for simplicity. We can also take German to Romanian or other language directions. In order to ﬁnd out the gap between the translation quality on better studied language pairs and that on less studied language pairs, we co"
2011.mtsummit-papers.34,steinberger-etal-2006-jrc,0,0.0697896,"Missing"
2011.mtsummit-papers.34,C96-2141,0,0.280846,"ature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a statistical 5-gram model with modiﬁed Kneser-Ney smoothing estimated using SRI-LM toolkit (Stolcke, 2002). Each language model is trained with the target side of the parallel data. We do not apply any zmert tuning in EMS because it does not improve our translation results on the evaluation set. Imp"
2011.mtsummit-papers.34,P07-1108,0,0.0135407,"from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including"
2011.mtsummit-papers.36,2009.mtsummit-posters.1,0,0.0193042,"rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete translation process in the post-editing. 3 Proposed method This section presents the propos"
2011.mtsummit-papers.36,J93-2003,0,0.0259478,"ainder of this paper is organized as follows. Section 2 brieﬂy reviews related studies on the reordering problem and another related technology called post-editing. Section 3 presents the proposed method in detail taking Japanese-to-English translation as a test case. Section 4 reports our experiments and discusses the results. Section 5 concludes this paper with our prospects for future work. 2 Related Work Reordering is a both theoretically and practically challenging problem in SMT. In the early period of SMT studies, reordering is modeled by distancebased constraints in translation model (Brown et al., 1993; Koehn et al., 2003). This reordering model is easy to compute and also works well in relatively similar language pair like French-to-English. The distance-based reordering constraint is not reasonable in some language pair such as English-toJapanese, because they have very different word ordering and appropriate reordering distances of words and phrases highly depend on their syntactic roles and contexts. Tillmann (2004) proposed a lexicalized reordering model that models orientation of phrases by monotone, swap, and discontinuous. This can directly model reordering of adjacent phrases but m"
2011.mtsummit-papers.36,J07-2003,0,0.106973,"Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is convincing alternative in recent SMT research. The reordering models mentioned above are applied in SMT decoding and solved simultaneously with phrase translation. Xiong et al. extended the hierarchical SMT by lexicalized reordering (Xiong et al., 2006; Xiong et al., 2008). However, the integrated search requires a large computational cost both in time and space. To keep the search tractable, we constrain reordering search by its reordering distance, as so-called distortion limit (or maximum span in tree-based decoder). It effectively reduces the computational cost but it also give up"
2011.mtsummit-papers.36,P05-1066,0,0.719105,"Missing"
2011.mtsummit-papers.36,W06-1609,0,0.0517088,"Missing"
2011.mtsummit-papers.36,W07-0732,0,0.0604907,"k for Japanese-to-English translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete translation process in th"
2011.mtsummit-papers.36,N10-1128,0,0.0291627,"ore phrase translation options efﬁciently. The pre-ordering is based on syntactic parse and can be regarded as a sub-problem of tree-to-string translation. On the other hand, there are several studies on pre-ordering without syntactic parsing. Costa-juss`a and Fonollosa (2006) tackled the pre-ordering problem as SMT, using reordering tables derived from phrase tables. Tromble and Eisner (2009) applied linear ordering models to preordering. Their techniques can be applied to any language pairs but rely on noisy automatic word alignment results as the reference of the reordering model training. Dyer and Resnik (2010) advanced such a pre-ordering-based translation to a novel uniﬁed approach of long-distance pre-ordering and decoding, with discriminative context-free reordering and ﬁnite-state phrase translation. In this paper, we reverse the pre-ordering SMT framework for Japanese-to-English translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into t"
2011.mtsummit-papers.36,2007.mtsummit-wpt.4,0,0.0178607,"lish translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete translation process in the post-editing"
2011.mtsummit-papers.36,N04-1035,0,0.0641016,"rdering constraint is not reasonable in some language pair such as English-toJapanese, because they have very different word ordering and appropriate reordering distances of words and phrases highly depend on their syntactic roles and contexts. Tillmann (2004) proposed a lexicalized reordering model that models orientation of phrases by monotone, swap, and discontinuous. This can directly model reordering of adjacent phrases but may not work for long distance reordering, because discontinuous supplies few constraints for reordering. On the other hand, syntaxbased SMT (Yamada and Knight, 2001; Galley et al., 2004; Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT b"
2011.mtsummit-papers.36,C10-1043,0,0.193651,"ed English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with"
2011.mtsummit-papers.36,N04-1014,0,0.0802119,"not reasonable in some language pair such as English-toJapanese, because they have very different word ordering and appropriate reordering distances of words and phrases highly depend on their syntactic roles and contexts. Tillmann (2004) proposed a lexicalized reordering model that models orientation of phrases by monotone, swap, and discontinuous. This can directly model reordering of adjacent phrases but may not work for long distance reordering, because discontinuous supplies few constraints for reordering. On the other hand, syntaxbased SMT (Yamada and Knight, 2001; Galley et al., 2004; Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is con"
2011.mtsummit-papers.36,W10-1736,1,0.779822,"d into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase"
2011.mtsummit-papers.36,N03-1017,0,0.171751,"is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with phrase reordering models (Koehn et al., 2003; Tillmann, 2004), and tree-based (or syntax-based) SMT (Yamada and Knight, 2001; Galley et al., 316 In contrast, what can we do in the translation in the opposite direction? This is a non-trivial problem because the pre-ordering techniques are usually language dependent. Even if we have a good preordering technique in A-to-B translation such as reordering rules for syntactic parse trees, it cannot be used directly in B-to-A translation. Developing Bto-A pre-ordering is a different problem from A-toB, which may require a syntactic parser and/or linguistic insights. For example in Japanese-to-E"
2011.mtsummit-papers.36,P07-1091,0,0.280387,"rst translated into foreign-ordered English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering."
2011.mtsummit-papers.36,J08-1002,0,0.0320343,"node. The determiners “the” and “a” are eliminated by the rules, and a pseudo-particle “ va0” is inserted after the subject. 4 • Japanese tokenizer: Mecab3 (with ipadic-2.7.0) Experiment We investigated the advantage of our post-ordering method by the following Japanese-to-English translation experiment with the post-ordering and baseline SMTs. 4.1 Setup We used NTCIR-9 PatentMT (NTCIR-9, 2011) English and Japanese dataset for this experiment. Some statistics of this dataset are shown in Table 1. We preprocessed the dataset by the following softwares: • English syntactic (HPSG) parser: Enju2 (Miyao and Tsujii, 2008) • English tokenizer: stepp (included in Enju package) 2 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 320 Word alignment was automatically estimated using MGIZA++4 using bitexts of 64 or less words in the training set to avoid a problematic underﬂow. Language models are word 5-gram models of English and HFE, trained with SRILM5 . 4.2 Compared methods We compared the proposed post-ordering with three baseline SMTs: a standard phrase-based SMT (PBMT) with lexicalized reordering, a hierarchical phrase-based SMT (HPBMT), and a string-to-tree syntax-based SMT (SBMT), included in Moses6 . 3"
2011.mtsummit-papers.36,P03-1021,0,0.0766118,"DQGDVHFRQGOHQV Figure 5: An example of the post-ordering translation. English parse trees used in SBMT were identical to the ones used for generating HFE sentences in the post-ordering. The post-ordering used two Moses phrase-based decoders, one for Japanese-to-HFE and the other for HFE-to-English. The models for these decoders were trained in the standard manner with Moses, grow-diag-final-and heuristics for symmetric word alignment, msd-bidirectional-fe lexicalized reordering (in PBMT and the postordering). The parameter values are optimized by minimum error rate training (MERT) (Och, 2003) with mert-moses.pl. One difference among conﬁgurations of the decoders was distortion limit. The Japanese-to-HFE decoder did not require long distance reordering, so we compared two conditions with the values of 0 (monotone) and 6. The HFE-to-English and PBMT decoders had to drastically reorder phrases so we used the values of 12 and 20. In the HPBMT and SBMT decoders, we used 15 for its max-chart-span option. The other 321 Table 2 shows the results in BLEU (Papineni et al., 2002) in case-insensitive evaluation and average decoding times7 (on a Xeon 7460 2.66GHz computer) with the compared me"
2011.mtsummit-papers.36,P02-1040,0,0.102487,"icalized reordering (in PBMT and the postordering). The parameter values are optimized by minimum error rate training (MERT) (Och, 2003) with mert-moses.pl. One difference among conﬁgurations of the decoders was distortion limit. The Japanese-to-HFE decoder did not require long distance reordering, so we compared two conditions with the values of 0 (monotone) and 6. The HFE-to-English and PBMT decoders had to drastically reorder phrases so we used the values of 12 and 20. In the HPBMT and SBMT decoders, we used 15 for its max-chart-span option. The other 321 Table 2 shows the results in BLEU (Papineni et al., 2002) in case-insensitive evaluation and average decoding times7 (on a Xeon 7460 2.66GHz computer) with the compared methods. The proposed post-ordering translation (with monotone Japaneseto-HFE translation) achieved 0.2963 in BLEU, better than the best HPBMT baseline (0.2887) by 0.76 points and the standard PBMT baseline (0.2806) by 1.57 points. The differences were statistically signiﬁcant according to the bootstrap sampling test (p &lt; 0.05 with HPBMT and p &lt; 0.01 with PBMT, 1,000 samples) (Zhang et al., 2004), and it was consistent among all post-ordering conditions. In the Japanese-to-HFE transl"
2011.mtsummit-papers.36,N07-1064,0,0.0204361,"ordering SMT framework for Japanese-to-English translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete tran"
2011.mtsummit-papers.36,N04-4026,0,0.440185,"source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with phrase reordering models (Koehn et al., 2003; Tillmann, 2004), and tree-based (or syntax-based) SMT (Yamada and Knight, 2001; Galley et al., 316 In contrast, what can we do in the translation in the opposite direction? This is a non-trivial problem because the pre-ordering techniques are usually language dependent. Even if we have a good preordering technique in A-to-B translation such as reordering rules for syntactic parse trees, it cannot be used directly in B-to-A translation. Developing Bto-A pre-ordering is a different problem from A-toB, which may require a syntactic parser and/or linguistic insights. For example in Japanese-to-English translatio"
2011.mtsummit-papers.36,D09-1105,0,0.249521,"English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel techn"
2011.mtsummit-papers.36,D07-1077,0,0.0514298,"Missing"
2011.mtsummit-papers.36,J97-3002,0,0.389798,"e reordering, because discontinuous supplies few constraints for reordering. On the other hand, syntaxbased SMT (Yamada and Knight, 2001; Galley et al., 2004; Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is convincing alternative in recent SMT research. The reordering models mentioned above are applied in SMT decoding and solved simultaneously with phrase translation. Xiong et al. extended the hierarchical SMT by lexicalized reordering (Xiong et al., 2006; Xiong et al., 2008). However, the integrated search requires a large computational cost both in time and space. To keep the search tractable, we constrain reordering search by"
2011.mtsummit-papers.36,C04-1073,0,0.779624,"in the opposite direction and proposes post-ordering; foreign sentences are ﬁrst translated into foreign-ordered English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of t"
2011.mtsummit-papers.36,P06-1066,0,0.0589586,"Missing"
2011.mtsummit-papers.36,I08-1066,0,0.0191332,"e errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is convincing alternative in recent SMT research. The reordering models mentioned above are applied in SMT decoding and solved simultaneously with phrase translation. Xiong et al. extended the hierarchical SMT by lexicalized reordering (Xiong et al., 2006; Xiong et al., 2008). However, the integrated search requires a large computational cost both in time and space. To keep the search tractable, we constrain reordering search by its reordering distance, as so-called distortion limit (or maximum span in tree-based decoder). It effectively reduces the computational cost but it also give up long distance reordering exceeding the speciﬁed distortion limit. A novel alternative to the reordering problem, called pre-ordering, has been studied over recent years (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Genzel, 2010). Xia and McCord (2004) proposed auto"
2011.mtsummit-papers.36,P01-1067,0,0.583016,"ng rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with phrase reordering models (Koehn et al., 2003; Tillmann, 2004), and tree-based (or syntax-based) SMT (Yamada and Knight, 2001; Galley et al., 316 In contrast, what can we do in the translation in the opposite direction? This is a non-trivial problem because the pre-ordering techniques are usually language dependent. Even if we have a good preordering technique in A-to-B translation such as reordering rules for syntactic parse trees, it cannot be used directly in B-to-A translation. Developing Bto-A pre-ordering is a different problem from A-toB, which may require a syntactic parser and/or linguistic insights. For example in Japanese-to-English translation, pre-ordering of Japanese parse trees into English word order"
2011.mtsummit-papers.36,zhang-etal-2004-interpreting,0,0.0319715,"ed 15 for its max-chart-span option. The other 321 Table 2 shows the results in BLEU (Papineni et al., 2002) in case-insensitive evaluation and average decoding times7 (on a Xeon 7460 2.66GHz computer) with the compared methods. The proposed post-ordering translation (with monotone Japaneseto-HFE translation) achieved 0.2963 in BLEU, better than the best HPBMT baseline (0.2887) by 0.76 points and the standard PBMT baseline (0.2806) by 1.57 points. The differences were statistically signiﬁcant according to the bootstrap sampling test (p &lt; 0.05 with HPBMT and p &lt; 0.01 with PBMT, 1,000 samples) (Zhang et al., 2004), and it was consistent among all post-ordering conditions. In the Japanese-to-HFE translation, the monotone conﬁguration was slightly better than the reordering with the distortion limit of 6 but the difference was not signiﬁcant. In the HFE-to-English translation, the difference in the distortion limit did not affect the ﬁnal results. Among the baseline methods, HPBMT was better than other baselines by 0.5 points. 4.4 Discussion The proposed post-ordering method was consistently better than the baseline methods in the experiment. To investigate the results in detail, we analyzed the Japanese"
2012.iwslt-evaluation.5,2012.iwslt-evaluation.1,0,0.0315298,"D task, exploring issues such as out-of-domain data ﬁltering, minimum Bayes risk decoding, MERT vs. PRO tuning, word alignment combination, and morphology. Decoding Baseline NAIST Submission dev2010 26.02 27.05 tst2010 29.75 31.81 Table 1: The scores for systems with and without the proposed improvements. ogy processing and large language models, which resulted in an average gain of 1.18 BLEU points over all languages. Section 4 describes these results in further detail. 2. English-French System 1. Introduction This paper describes the NAIST participation in the IWSLT 2012 evaluation campaign [1]. We participated in all 11 TED tasks, dividing our efforts in half between the ofﬁcial English-French track and the 10 other unofﬁcial ForeignEnglish tracks. For all tracks we used the Moses decoder [2] and its experiment management system to run a large number of experiments with different settings over many language pairs. For the English-French system we experimented with a number of techniques, settling on a combination that provided signiﬁcant accuracy improvements without introducing unnecessary complexity into the system. In the end, we chose a four-pronged approach consisting of using"
2012.iwslt-evaluation.5,P07-2045,0,0.0162664,"2 27.05 tst2010 29.75 31.81 Table 1: The scores for systems with and without the proposed improvements. ogy processing and large language models, which resulted in an average gain of 1.18 BLEU points over all languages. Section 4 describes these results in further detail. 2. English-French System 1. Introduction This paper describes the NAIST participation in the IWSLT 2012 evaluation campaign [1]. We participated in all 11 TED tasks, dividing our efforts in half between the ofﬁcial English-French track and the 10 other unofﬁcial ForeignEnglish tracks. For all tracks we used the Moses decoder [2] and its experiment management system to run a large number of experiments with different settings over many language pairs. For the English-French system we experimented with a number of techniques, settling on a combination that provided signiﬁcant accuracy improvements without introducing unnecessary complexity into the system. In the end, we chose a four-pronged approach consisting of using the web data with ﬁltering to remove noisy sentences, phrase table smoothing, language model interpolation, and minimum Bayes risk decoding. This led to a score of 31.81 BLEU on the tst2010 data set, a"
2012.iwslt-evaluation.5,N03-1017,0,0.111297,"urposes, in Section 3, we also present additional experiments that gave negative results, which were not included in our ofﬁcial submission. For the 10 translation tasks into English, we focused on techniques that could be used widely across all languages. In particular, we experimented with unsupervised approaches to handling source-side morphology, minimum Bayes risk decoding, and large language models. In the end, most of our systems used a combination of unsupervised morpholThe NAIST English-French translation system for IWSLT 2012 was based on phrase-based statistical machine translation [3] using the Moses decoder [2] and its corresponding training regimen. Overall, we made four enhancements over the standard Moses setup to improve the translation accuracy: Large-scale Data with Filtering: In order to use the large, but noisy parallel training data in the English-French Giga Corpus, we implemented a technique to ﬁlter out noisy translated text. Phrase Table Smoothing: We performed phrase table smoothing to improve the probability estimates of low-frequency phrases. Language Model Interpolation: In order to adapt to the domain of the task, we interpolated language models trained"
2012.iwslt-evaluation.5,J05-4003,0,0.0924584,"Missing"
2012.iwslt-evaluation.5,2011.iwslt-evaluation.9,0,0.0502184,"Missing"
2012.iwslt-evaluation.5,J93-2003,0,0.0294894,"Missing"
2012.iwslt-evaluation.5,W99-0604,0,0.275234,"Missing"
2012.iwslt-evaluation.5,P02-1038,0,0.0218946,"s comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smoothing of the phrase table improves results by a signiﬁcant amount. In initial research on MBR, the space of possible hypotheses E was deﬁned as the n-best list output by the decoder. This was further expanded by [14], who deﬁned MBR over lattices. We tested both of these approaches (as implemented in the Moses decoder). Finally, one ﬁne point about MBR is that it requires a good estimate of the probability P (E  |F ) of hypotheses. In the discriminative training framework of [15], which is used in most modern SMT systems, scores of machine translation hypotheses are generally deﬁned as a log-linear combination of feature functions such as language model or translation model probabilities P (E  |F ) = 2.3. Language Model Interpolation One of the characteristics of the IWSLT TED task is that, as shown in Table 2, we have several heterogeneous corpora. In addition, the in-domain TED data is relatively small, so it can be expected that we will beneﬁt from using data outside of the TED domain. In order to effectively utilize out-ofdomain data in language modeling, we buil"
2012.iwslt-evaluation.5,P03-1021,0,0.0174134,"ns. 1 i wi φi (E  ,F ) e Z (4) where φi indicates feature functions such as the language model, translation model, and reordering model log probabilities, wi is the weight measuring the relative importance of this feature, and Z is a partition function that ensures that the probabilities add to 1. Choosing the weights wi for each feature such that the answer with highest probability ˆ = argmax P (E|F ) E E (5) is the best possible translation is a process called “tuning,” and essential to modern SMT systems. However, in most tuning methods, including the standard minimum error rate training [16] that was used in the proposed system, while the relative weight of each feature wi is adjusted, the overall sum of the weights i wi is generally set ﬁxed at 1. While this is not a problem when ﬁnding the highest probability hypothesis in 5, it will affect the probability estimates P (E  |F ), with 56 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Decoding Viterbi MBR (λ = 1) Lattice MBR (λ = 1) Lattice MBR (λ = 5) dev2010 27.59 27.29 26.70 27.05 tst2010 31.01 31.24 31.25 31.81 Table 6: BLEU Results using Minimum Bayes Risk decoding. larger s"
2012.iwslt-evaluation.5,2010.iwslt-papers.5,1,0.897262,"Missing"
2012.iwslt-evaluation.5,W06-1607,0,0.01659,"pothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we use one minus sentence-wise BLEU+1 score [13] as our loss function L(E  , E) = 1 − BLEU+1(E  , E). (3) 2.2. Phrase Table Smoothing We also performed experiments that used smoothing of the statistics used in calculating translation model probabilities [9]. The motivation behind this method is that the statistics used to train the phrase table are generally sparse, and tend to over-estimate the probabilities of rare events. In the submitted system we used Good-Turing smoothing for the phrase table probabilities. Results comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smoothing of the phrase table improves results by a signiﬁcant amount. In initial research on MBR, the space of possible hypotheses E was deﬁned as the n-best list output by the decoder. This was further expanded by ["
2012.iwslt-evaluation.5,P12-1018,1,0.830308,"ch TED task. 3.2. Word Alignment & Phrase Table Combination We investigated different alignment tools and ways to combine them, as shown in Table 8. Observations are as follows: • GIZA++ and BerkeleyAligner achieve similar BLEU on this task. • Concatenating GIZA++ and BerkeleyAligner word alignment results, prior to phrase extraction, achieves a small boost (29.57 to 29.89 BLEU). We experimented with the simplest approach to exploiting out-of-domain bitext in translation models: data concatenation. This can be seen as adaptation at the earliest stage of the • We also experimented with pilaign [19], a Bayesian phrasal alignment toolkit. This tool directly extracts phrases without resorting to the preliminary step of word alignments, and achieves extremely compact phrase table sizes (0.8M entries) without signiﬁcantly sacriﬁcing BLEU (29.24). 3 It should be noted that due to constraints in the available data for these MBR experiments we are both tuning on testing on tst2010, but the tuning of λ also demonstrated gains in accuracy on the ofﬁcial blind test on tst2011 and tst2012 (37.33→37.90 and 38.92→39.47 respectively). • Combining the GIZA++ and pialign phrase tables by Moses’ multiple"
2012.iwslt-evaluation.5,D11-1125,0,0.0529106,", the number of random restarts was set to 20. 3.3. Lexical Reordering Models Several reordering models available in the Moses decoder were tried. In general, we found the full “msd-bidir-fe” option to perform best, despite the small number of word order differences between English and French. Results are shown in Table 9. Reordering model msd-bidir-fe msd-bidir-f monotonicity-bidir-fe msd-backward-fe distance msd-bidir-fe-collapse BLEU 29.57 29.43 29.29 29.22 28.99 28.86 Table 9: Comparison of Reordering models on tst2010. 3.4. MERT vs. PRO tuning We compared two tuning methods: MERT and PRO [20]. We used the implementations distributed with Moses. For both MERT and PRO, we set the size of k-best list to k = 100, used 14 standard features, and removed duplicates in k-best lists when merging previously generated k-best lists. We ran MERT in multi-threaded setting until convergence. Since the number of random restarts in MERT greatly affects on the translation accuracy [21], we tried various number of random restarts for 1, 10, 20, and 50.4 For PRO, we used MegaM5 as a binary classiﬁer with the default setting. We ran PRO for 25 iterations. We tried two kinds of PRO: [20] interpolated t"
2012.iwslt-evaluation.5,C08-1074,0,0.0166288,"ir-fe msd-backward-fe distance msd-bidir-fe-collapse BLEU 29.57 29.43 29.29 29.22 28.99 28.86 Table 9: Comparison of Reordering models on tst2010. 3.4. MERT vs. PRO tuning We compared two tuning methods: MERT and PRO [20]. We used the implementations distributed with Moses. For both MERT and PRO, we set the size of k-best list to k = 100, used 14 standard features, and removed duplicates in k-best lists when merging previously generated k-best lists. We ran MERT in multi-threaded setting until convergence. Since the number of random restarts in MERT greatly affects on the translation accuracy [21], we tried various number of random restarts for 1, 10, 20, and 50.4 For PRO, we used MegaM5 as a binary classiﬁer with the default setting. We ran PRO for 25 iterations. We tried two kinds of PRO: [20] interpolated the weights with previously learned weights to improve the stability (henceforth “PRO-interpolated”)6 , and 4 Currently, Moses’s default setting is 20. ˜hal/megam/ 6 We set the same interpolation coefﬁcient value of 0.1 as [20] noted. 5 http://www.cs.utah.edu/ the version that do not use such a interpolation (henceforth “PRO-basic”). We ﬁrst investigate the effect of the number of"
2012.iwslt-evaluation.5,N04-1022,0,0.0390465,"ing. LM TED Only Without Interp. With Interp. dev2010 24.80 26.30 27.05 tst2010 29.44 31.15 31.81 Table 5: Results training the language model on only TED data, and when other data is used without and with language model interpolation. results are shown in Table 3. As a result, we can see that using the data from the Giga corpus has a positive effect on the results, but ﬁltering does not have a clear signiﬁcant effect on the results. 2.4. Minimum Bayes Risk Decoding Finally, we experimented with improved decoding strategies for translation, particularly using minimum Bayes risk decoding (MBR, [12]). In normal translation, the decoder attempts to simply ﬁnd the answer with the highest probability among the translation candidates ˆ = argmax P (E|F ) E E (1) in a process called Viterbi decoding. As an alternative to this, MBR attempts to ﬁnd the hypothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we"
2012.iwslt-evaluation.5,C04-1072,0,0.0264753,"empts to simply ﬁnd the answer with the highest probability among the translation candidates ˆ = argmax P (E|F ) E E (1) in a process called Viterbi decoding. As an alternative to this, MBR attempts to ﬁnd the hypothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we use one minus sentence-wise BLEU+1 score [13] as our loss function L(E  , E) = 1 − BLEU+1(E  , E). (3) 2.2. Phrase Table Smoothing We also performed experiments that used smoothing of the statistics used in calculating translation model probabilities [9]. The motivation behind this method is that the statistics used to train the phrase table are generally sparse, and tend to over-estimate the probabilities of rare events. In the submitted system we used Good-Turing smoothing for the phrase table probabilities. Results comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smooth"
2012.iwslt-evaluation.5,E03-1076,0,0.149639,"e baseline results. First, adding additional out-of-domain data (nc=News Commentary, ep=Europarl, un=UN Multitext) to the language model increased results uniformly for all language pairs (line (b) of Table 12). We used an interpolated language model, trained in the same fashion as in our English-French system. Next, we tried two strategies for handling rich morphology in the input. The “CompoundSplit” program in the Moses package was developed for languages with extensive noun compounding, e.g. German, and breaks apart words if sub-parts are seen in the training data over a certain frequency [22]. The alternate “Morfessor” program [23] is an unsupervised morphological analyzer based on the Minimum Description Length principle – it tries to ﬁnd the the smallest set of morphemes that parsimoniously cover the training set. Morfessor is expected to segment more aggressively than CompoundSplit, especially because it can ﬁnd both bound and free morphemes. However, we empirically found that Morfessor segments too aggressively for unknown words (i.e. each character becomes a morpheme), so we do not segment OOV words in dev/test.8 The results in line (c) of Table 12 shows that German beneﬁt mo"
2012.iwslt-evaluation.5,W02-0603,0,0.0311216,"onal out-of-domain data (nc=News Commentary, ep=Europarl, un=UN Multitext) to the language model increased results uniformly for all language pairs (line (b) of Table 12). We used an interpolated language model, trained in the same fashion as in our English-French system. Next, we tried two strategies for handling rich morphology in the input. The “CompoundSplit” program in the Moses package was developed for languages with extensive noun compounding, e.g. German, and breaks apart words if sub-parts are seen in the training data over a certain frequency [22]. The alternate “Morfessor” program [23] is an unsupervised morphological analyzer based on the Minimum Description Length principle – it tries to ﬁnd the the smallest set of morphemes that parsimoniously cover the training set. Morfessor is expected to segment more aggressively than CompoundSplit, especially because it can ﬁnd both bound and free morphemes. However, we empirically found that Morfessor segments too aggressively for unknown words (i.e. each character becomes a morpheme), so we do not segment OOV words in dev/test.8 The results in line (c) of Table 12 shows that German beneﬁt most from CompoundSplit, while Arabic, Rus"
2012.iwslt-evaluation.5,D08-1065,0,\N,Missing
2013.iwslt-evaluation.12,N03-1017,0,0.0375042,"an compounds; and system combination of different types of SMT systems based on generalized minimum Bayes risk (GMBR) framework. This paper presents details of our systems and reports the results in German-English and English-German MT tasks in the evaluation campaign. 2. Translation Methods The main feature of our system for this evaluation is that we perform translation using three different translation models and combine the results through system combination. Each of the three methods is described briefly below. 2.1. Phrase-based Machine Translation Phrase-based machine translation (PBMT; [2]) models the translation process by splitting the source sentence into phrases, translating the phrases into target phrases, and reordering the phrases into the target language order. PBMT is currently the most widely used method in SMT as it is robust, does not require the availability of linguistic analysis tools, and achieves high accuracy, particularly for languages with similar syntactic structure. 2.2. Hierarchical Phrase-based Machine Translation Hierarchical phrase-based machine translation (Hiero; [3]) expands the class of translation rules that can be used in phrase-based machine tra"
2013.iwslt-evaluation.12,J07-2003,0,0.106892,"below. 2.1. Phrase-based Machine Translation Phrase-based machine translation (PBMT; [2]) models the translation process by splitting the source sentence into phrases, translating the phrases into target phrases, and reordering the phrases into the target language order. PBMT is currently the most widely used method in SMT as it is robust, does not require the availability of linguistic analysis tools, and achieves high accuracy, particularly for languages with similar syntactic structure. 2.2. Hierarchical Phrase-based Machine Translation Hierarchical phrase-based machine translation (Hiero; [3]) expands the class of translation rules that can be used in phrase-based machine translation by further allowing rules with gaps that can be filled in a hierarchical fashion. Hiero is generally considered to be more accurate than PBMT on language pairs that are less monotonic, but also requires a significantly larger amount of memory and decoding time. As the German-English pair has a significant amount of reordering, particularly with movement of verbs, we can expect that Hiero will be able to handle these reorderings more appropriately in some cases. 2.3. Forest-to-string Machine Translatio"
2013.iwslt-evaluation.12,P06-1077,0,0.131208,"at can be used in phrase-based machine translation by further allowing rules with gaps that can be filled in a hierarchical fashion. Hiero is generally considered to be more accurate than PBMT on language pairs that are less monotonic, but also requires a significantly larger amount of memory and decoding time. As the German-English pair has a significant amount of reordering, particularly with movement of verbs, we can expect that Hiero will be able to handle these reorderings more appropriately in some cases. 2.3. Forest-to-string Machine Translation Tree-to-string machine translation (T2S; [4]) performs translation by first syntactically parsing the source sentence, then translating from sub-structures of the parse to a string in the target language. Forest-to-string machine translation (F2S; [5]) generalizes this framework, making it possible to not only translate the single one-best syntactic parse, but a packed forest that encodes many possible parses, helping to pass along some of the ambiguity of parsing to be resolved during translation. While there are a number of proposed methods for incorporating source-side syntax into the translation process, here we use a method based o"
2013.iwslt-evaluation.12,D08-1022,0,0.0240076,"irs that are less monotonic, but also requires a significantly larger amount of memory and decoding time. As the German-English pair has a significant amount of reordering, particularly with movement of verbs, we can expect that Hiero will be able to handle these reorderings more appropriately in some cases. 2.3. Forest-to-string Machine Translation Tree-to-string machine translation (T2S; [4]) performs translation by first syntactically parsing the source sentence, then translating from sub-structures of the parse to a string in the target language. Forest-to-string machine translation (F2S; [5]) generalizes this framework, making it possible to not only translate the single one-best syntactic parse, but a packed forest that encodes many possible parses, helping to pass along some of the ambiguity of parsing to be resolved during translation. While there are a number of proposed methods for incorporating source-side syntax into the translation process, here we use a method based on tree-to-string transducers [6]. Syntax-driven methods such as T2S and F2S are particularly useful for language pairs with extremely large amounts of reordering, as the syntactic parse can help guide the ac"
2013.iwslt-evaluation.12,N04-1014,0,0.0445956,"st syntactically parsing the source sentence, then translating from sub-structures of the parse to a string in the target language. Forest-to-string machine translation (F2S; [5]) generalizes this framework, making it possible to not only translate the single one-best syntactic parse, but a packed forest that encodes many possible parses, helping to pass along some of the ambiguity of parsing to be resolved during translation. While there are a number of proposed methods for incorporating source-side syntax into the translation process, here we use a method based on tree-to-string transducers [6]. Syntax-driven methods such as T2S and F2S are particularly useful for language pairs with extremely large amounts of reordering, as the syntactic parse can help guide the accurate re-ordering of entire phrases or clauses. On the other hand, these methods are highly dependent on parsing accuracy, and also have limits on the rules that can be extracted, and are somewhat less robust than the previous two methods. different syntactic parser that did not provide this information. 3. SMT Technologies 3.2.2. English-to-German 3.1. Training data selection The target TED domain is different in both s"
2013.iwslt-evaluation.12,P13-2119,1,0.927042,"these methods are highly dependent on parsing accuracy, and also have limits on the rules that can be extracted, and are somewhat less robust than the previous two methods. different syntactic parser that did not provide this information. 3. SMT Technologies 3.2.2. English-to-German 3.1. Training data selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data1 ). To address this domain adaption problem, we performed adaptation training data selection using the method of [7].2 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [8]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. By taking a sub-sample (same size as the target-domain data), we reduce training t"
2013.iwslt-evaluation.12,D11-1033,0,0.0655671,"mation. 3. SMT Technologies 3.2.2. English-to-German 3.1. Training data selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data1 ). To address this domain adaption problem, we performed adaptation training data selection using the method of [7].2 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [8]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. By taking a sub-sample (same size as the target-domain data), we reduce training time and avoid training and testing language models on the same general-domain data. Similarly, INF (f ) and GENF (f ) are the cross-entropies of f on Foreign-side LM. Finally, sentence pairs are ranked according to Eq. 1 and those w"
2013.iwslt-evaluation.12,P05-1066,0,0.146142,") are added together with the in-domain bitext for translation model training. Here, the LMs are Recurrent Neural Network Language Models (RNNLMs), which have been shown to outperform n-gram LMs in this problem [7]. 3.2. Syntactic Rule-based Pre-ordering Preordering is a method that attempts to first re-order the source sentence into a word order that is closer to the target. As German and English have significantly different word order, we can imagine that this will help our accuracy for this language pair. 3.2.1. German-to-English We applied the clause restructuring method of Collins et al. [9] for German pre-ordering. The method is mainly based on moving German verbs in the end of clause structures towards the beginning of the clause. We re-implemented the method for German parse trees created using the Berkeley parser trained on TIGER corpus. We ignored some additional syntactic information such as subject markers and heads implemented in the original method of [9], because we used a 1 To give a sense of the domain difference, a 4-gram LM trained with Kneser-Ney smoothing on TED data gives a perplexity of 355 on the general domain data, compared to a perplexity of 99 on held-out T"
2013.iwslt-evaluation.12,E03-1076,0,0.18382,"ional feature to each translation hypothesis. We then re-run a single MERT optimization to find ideal weights for this new feature, and then extract the 1-best result from the 10,000-best list for the test set according to these new weights. The parameters for RNNLM training are tuned on the dev set to maximize perplexity, resulting in 300 hidden layers, 300 classes, and 4 steps of back-propogation through time. 3.4. German compound word splitting German compound words present sparsity challenges for machine translation. To address this, we split German words following the general approach of [11]. The idea is to split a word if the geometric average of its subword frequencies is larger than whole word frequency. In our implementation, for each word, we searched for all possible decompositions into two sub-words, considering the possibility of deleting common German fillers “e”, “es”, and “s” (as in ”Arbeit+s+tier”). For simplicity, we did not experiment with splitting into three or more sub-words as done in the compound-splitter.perl script distributed with the Moses package. The unigram frequencies for the subwords and whole word is computed from the German part of the bitext. This s"
2013.iwslt-evaluation.12,I11-1153,1,0.737728,"vial to handle recombination of German split words after reordering and translation. To ensure that the uniform hypotheses space gives the same decision as the original loss in the true space p(e|f ), we use a small development set to tune the parameter θ as follows. For any two hypotheses e1 , e2 , and a reference translation er (possibly not in N (f )) we first compute the true loss: L(e1 |er ) and L(e2 |er ). If L(e1 |er ) &lt; L(e2 |er ), then we would want θ such that: K ∑ ∑ 3.5. GMBR system combination We used a system combination method based on Generalized Minimum Bayes Risk optimization [12], which has been successfully applied to different types of SMT systems for patent translation [13]. Note that our system combination only picks one hypothesis from an N-best list and does not generate a new hypothesis by mixing partial hypotheses among the N-best. θk Lk (e1 |e) &lt; e∈N (f ) k=1 Minimum Bayes Risk (MBR) is a decision rule to choose hypotheses that minimize the expected loss. In the task of SMT from a French sentence (f ) to an English sentence (e), the MBR decision rule on δ(f ) → e′ with the loss function L over the possible space of sentence pairs (p(e, f )) is denoted as: ∑ a"
2013.iwslt-evaluation.12,P10-1017,0,0.0236144,"akes the problem amendable to solutions in “learning to rank” literature [15]. We used BLEU as the objective function and the subcomponents of BLEU as features (system identity feature was not used). There is one regularization hyperparameter for the Ranking SVM, which we set by cross-validation over the development set (dev2010). 3.6. What Didn’t Work Immediately We also tried several other methods that did not have a clear positive effect and were thus omitted from the final system. For example, we attempted to improve alignment accuracy using the discriminative alignment method proposed by [16] training on the 300 hand-aligned sentences.4 However, while this provided small gains in alignment accuracy on a held-out set, the gains were likely not enough, and MT results were inconclusive. We also attempted to use the reordering method of [17] as implemented in lader,5 again trained on the same 300 hand-aligned sentences, but increases in reordering accuracy on a held-out set were minimal. We believe that both of these techniques are promising, but require a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/"
2013.iwslt-evaluation.12,D12-1077,1,0.836426,"Ranking SVM, which we set by cross-validation over the development set (dev2010). 3.6. What Didn’t Work Immediately We also tried several other methods that did not have a clear positive effect and were thus omitted from the final system. For example, we attempted to improve alignment accuracy using the discriminative alignment method proposed by [16] training on the 300 hand-aligned sentences.4 However, while this provided small gains in alignment accuracy on a held-out set, the gains were likely not enough, and MT results were inconclusive. We also attempted to use the reordering method of [17] as implemented in lader,5 again trained on the same 300 hand-aligned sentences, but increases in reordering accuracy on a held-out set were minimal. We believe that both of these techniques are promising, but require a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/ tosch/ ˜ downloads.html 5 http://phontron.com/lader 4. Experiments 4.1. Setup 4.1.1. System overview We used three individual SMT systems for each language pairs: forest-to-string (F2S), hierarchical phrase-based (Hiero), and phrase-based with pre-or"
2013.iwslt-evaluation.12,P13-4016,1,0.84637,"believe that both of these techniques are promising, but require a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/ tosch/ ˜ downloads.html 5 http://phontron.com/lader 4. Experiments 4.1. Setup 4.1.1. System overview We used three individual SMT systems for each language pairs: forest-to-string (F2S), hierarchical phrase-based (Hiero), and phrase-based with pre-ordering (Preorder). In some of our comparisons we also use simple phrase-based translation without preordering (PBMT). F2S was implemented with Travatar [18] and Preorder, PBMT, and Hiero were implemented using Moses [19]. For the Moses models, we generally used the default settings, but with Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of comp"
2013.iwslt-evaluation.12,P07-2045,0,0.0147657,"e a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/ tosch/ ˜ downloads.html 5 http://phontron.com/lader 4. Experiments 4.1. Setup 4.1.1. System overview We used three individual SMT systems for each language pairs: forest-to-string (F2S), hierarchical phrase-based (Hiero), and phrase-based with pre-ordering (Preorder). In some of our comparisons we also use simple phrase-based translation without preordering (PBMT). F2S was implemented with Travatar [18] and Preorder, PBMT, and Hiero were implemented using Moses [19]. For the Moses models, we generally used the default settings, but with Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All s"
2013.iwslt-evaluation.12,P02-1040,0,0.0871245,"t settings, but with Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All systems were evaluated using the standard BLEU score [21] and also RIBES [22], a metric designed specifically to show whether reordering is being performed properly. All systems were optimized towards BLEU score. We measure statistical significance between results with bootstrap resampling with p &gt; 0.05. Bold numbers in each table indicate the best system, and all systems that do not show a statistically significant difference from the best system [23]. All words were lowercased prior to translation, and finally recased by a SMT-based recaser as implemented in Moses. 4.1.2. Translation models We trained the translation models using WIT3 training dat"
2013.iwslt-evaluation.12,D10-1092,1,0.836115,"Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All systems were evaluated using the standard BLEU score [21] and also RIBES [22], a metric designed specifically to show whether reordering is being performed properly. All systems were optimized towards BLEU score. We measure statistical significance between results with bootstrap resampling with p &gt; 0.05. Bold numbers in each table indicate the best system, and all systems that do not show a statistically significant difference from the best system [23]. All words were lowercased prior to translation, and finally recased by a SMT-based recaser as implemented in Moses. 4.1.2. Translation models We trained the translation models using WIT3 training data (138,499 sentences"
2013.iwslt-evaluation.12,W04-3250,0,0.0816664,"lt settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All systems were evaluated using the standard BLEU score [21] and also RIBES [22], a metric designed specifically to show whether reordering is being performed properly. All systems were optimized towards BLEU score. We measure statistical significance between results with bootstrap resampling with p &gt; 0.05. Bold numbers in each table indicate the best system, and all systems that do not show a statistically significant difference from the best system [23]. All words were lowercased prior to translation, and finally recased by a SMT-based recaser as implemented in Moses. 4.1.2. Translation models We trained the translation models using WIT3 training data (138,499 sentences) and 1,000,000 sentences selected over other bitexts (Europarl, News Commentary, and Common Crawl) by the method described in 3.1. 4.1.3. Language models We used two types of word n-gram language models of German and English: interpolated 6-gram and Google 5-gram. The interpolated 6-gram LMs were from linear interpolation of several 6-gram LMs on different data sources (WIT3"
2013.iwslt-evaluation.12,P12-1001,1,0.853229,"at the F2S system did a significantly better job of accurately generating verbs at the end of the German sentence, demonstrating its superior capability for reordering. For German-English, on the other hand, F2S achieved a somewhat counter-intuitive low score on the reordering-based measure RIBES. Upon an analysis of the results, we found that the F2S system was largely getting the reordering right, but occasionally making big changes in reordering large clauses that were not reflected in the German reference. It is likely that if we optimized towards RIBES, or a combination of BLEU and RIBES [25] we might get better results. 4.4. Translation Method Comparison 4.5. Effect of Compound Splitting In this section, we provide a brief comparison of the three translation methods mentioned in Section 2 on tst2010 data. For all systems we used the TED data and 1M selected sentences for training, and used the language model described Next, we examine the effect of compound splitting for German-English translation. From the results in Table 7, we can see that compound splitting provides a gain for all systems, and particularly so for F2S translation. PBMT Hiero F2S en-de n-gram +RNNLM 23.11 23.81"
2013.iwslt-evaluation.12,federico-etal-2012-iwslt,0,\N,Missing
2014.iwslt-evaluation.18,P08-1023,0,0.0240241,"in the F2S system, and compare it to the more traditional method of cube pruning. The second is that this year we attempted to extract pre-ordering rules automatically from parallel corpora, as opposed to hand-designing preordering rules based on linguistic intuition. This paper presents details of our systems and reports the official results together with some detailed discussions on contributions of the techniques involved. 2.1. Forest-to-String Machine Translation In our previous year’s submission to IWSLT, we achieved promising results using the forest-to-string machine translation (F2S; [3]) framework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven me"
2014.iwslt-evaluation.18,P06-1077,0,0.0368401,"The second is that this year we attempted to extract pre-ordering rules automatically from parallel corpora, as opposed to hand-designing preordering rules based on linguistic intuition. This paper presents details of our systems and reports the official results together with some detailed discussions on contributions of the techniques involved. 2.1. Forest-to-String Machine Translation In our previous year’s submission to IWSLT, we achieved promising results using the forest-to-string machine translation (F2S; [3]) framework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT"
2014.iwslt-evaluation.18,W14-7002,1,0.835139,"o-String Machine Translation In our previous year’s submission to IWSLT, we achieved promising results using the forest-to-string machine translation (F2S; [3]) framework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT this year, we experimented with two different search algorithms to measure the effect that search has on the GermanEnglish and English-German pairs. As the first algorithm, we use the standard method for search in tree-based methods of translation: cube pruning [7]. For each edge to be expanded, cube pruning sorts the child hypotheses in descending order of p"
2014.iwslt-evaluation.18,P14-2024,1,0.898714,"ework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT this year, we experimented with two different search algorithms to measure the effect that search has on the GermanEnglish and English-German pairs. As the first algorithm, we use the standard method for search in tree-based methods of translation: cube pruning [7]. For each edge to be expanded, cube pruning sorts the child hypotheses in descending order of probability, and at every step pops the highest-scoring hypothesis off the stack, calculates its language model scores, and adds the popped, scored edge to the hyperg"
2014.iwslt-evaluation.18,J07-2003,0,0.082081,"for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT this year, we experimented with two different search algorithms to measure the effect that search has on the GermanEnglish and English-German pairs. As the first algorithm, we use the standard method for search in tree-based methods of translation: cube pruning [7]. For each edge to be expanded, cube pruning sorts the child hypotheses in descending order of probability, and at every step pops the highest-scoring hypothesis off the stack, calculates its language model scores, and adds the popped, scored edge to the hypergraph. It should be noted that the LM scores are not calculated until after the edge is popped, and thus the order of visiting edges is based on only an LMfree approximation of the true edge score, resulting in search errors. In our F2S system this year, we test a new method of hypergraph search [2], which aims to achieve better search ac"
2014.iwslt-evaluation.18,P05-1066,0,0.0488395,"Lake Tahoe, December 4th and 5th, 2014 ally, refining the probability estimates until the limit on number of stack pops is reached. In our previous work [6] we have found that hypergraph search achieved superior results to cube pruning, and we hypothesize that these results will carry over to German-English and English-German as well. 2.2. Syntax-based Pre-ordering Pre-ordering is a method that attempts to first reorder the source sentence into a word order that is closer to the target, then translate using a standard method such as PBMT. We used hand-crafted German-English pre-ordering rules [8] in our submission last year. This year’s system uses an automatic method to extract domain-dependent pre-ordering rules, avoiding the time-consuming effort required for creating hand-crafted rules. The pre-ordering method is basically similar to [9], but is limited to reordering of child nodes in syntactic parse trees rather than rewriting and word insertion. Since the pre-ordering does not work perfectly in all cases, we allow for further reordering in the PBMT system that translates the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (d"
2014.iwslt-evaluation.18,C04-1073,0,0.0293599,"esize that these results will carry over to German-English and English-German as well. 2.2. Syntax-based Pre-ordering Pre-ordering is a method that attempts to first reorder the source sentence into a word order that is closer to the target, then translate using a standard method such as PBMT. We used hand-crafted German-English pre-ordering rules [8] in our submission last year. This year’s system uses an automatic method to extract domain-dependent pre-ordering rules, avoiding the time-consuming effort required for creating hand-crafted rules. The pre-ordering method is basically similar to [9], but is limited to reordering of child nodes in syntactic parse trees rather than rewriting and word insertion. Since the pre-ordering does not work perfectly in all cases, we allow for further reordering in the PBMT system that translates the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (dev. set BLEU in this paper). 2.2.1. Reordering Pattern Extraction A reordering pattern represents a reordering of child nodes in a source language parse tree, determined by word alignment. The reordering pattern is similar to a tree-based translation"
2014.iwslt-evaluation.18,N04-1035,0,0.060307,"rd insertion. Since the pre-ordering does not work perfectly in all cases, we allow for further reordering in the PBMT system that translates the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (dev. set BLEU in this paper). 2.2.1. Reordering Pattern Extraction A reordering pattern represents a reordering of child nodes in a source language parse tree, determined by word alignment. The reordering pattern is similar to a tree-based translation pattern called frontier graph fragments, which form the most basic unit in tree-based translation [10], but only holds reordering information on the non-terminal child nodes. A reordering pattern can be extracted from an admissible node [11] in the parse tree that covers a distinct contiguous spans in the corresponding target language sentences. Since such a reordering pattern only is constrained by the syntactic labels on the parent and child nodes, we consider several attributes of reordering patterns: syntactic labels of its grand-parent, left and right siblings of the parent, and surface forms of its child nodes (only when the child is a part-of-speech node). 2.2.2. Deterministic Pre-order"
2014.iwslt-evaluation.18,D07-1078,0,0.0251997,"es the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (dev. set BLEU in this paper). 2.2.1. Reordering Pattern Extraction A reordering pattern represents a reordering of child nodes in a source language parse tree, determined by word alignment. The reordering pattern is similar to a tree-based translation pattern called frontier graph fragments, which form the most basic unit in tree-based translation [10], but only holds reordering information on the non-terminal child nodes. A reordering pattern can be extracted from an admissible node [11] in the parse tree that covers a distinct contiguous spans in the corresponding target language sentences. Since such a reordering pattern only is constrained by the syntactic labels on the parent and child nodes, we consider several attributes of reordering patterns: syntactic labels of its grand-parent, left and right siblings of the parent, and surface forms of its child nodes (only when the child is a part-of-speech node). 2.2.2. Deterministic Pre-ordering In order to make the pre-ordering deterministic, we use reordering rules from dominant reordering patterns that agree with more than 75"
2014.iwslt-evaluation.18,N03-1017,0,0.058736,"Missing"
2014.iwslt-evaluation.18,P13-2119,1,0.840496,"icularly for languages with similar syntactic structure. 3. Additional System Enhancements Here we review techniques that were used in our submission last year [1] and also describe some of our new attempts that were not effective in our pilot test and not included in the final system. 3.1. Training Data Selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data). To address this domain adaption problem, we performed adaptation training data selection using the method of [13].1 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [14]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. Similarly, INF (f ) and GENF (f ) are the crossentropies of f on Foreign-side LM."
2014.iwslt-evaluation.18,D11-1033,0,0.0295922,"our pilot test and not included in the final system. 3.1. Training Data Selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data). To address this domain adaption problem, we performed adaptation training data selection using the method of [13].1 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [14]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. Similarly, INF (f ) and GENF (f ) are the crossentropies of f on Foreign-side LM. Finally, sentence pairs are ranked according to Eq. 1 and those with scores lower than some empirically-chosen threshold are added together with the in-domain bitext for translation model training. Here, the LMs are Recurrent Neural"
2014.iwslt-evaluation.18,E03-1076,0,0.0546193,"milarly, INF (f ) and GENF (f ) are the crossentropies of f on Foreign-side LM. Finally, sentence pairs are ranked according to Eq. 1 and those with scores lower than some empirically-chosen threshold are added together with the in-domain bitext for translation model training. Here, the LMs are Recurrent Neural Network Language Models (RNNLMs), which have been shown to outperform n-gram LMs in this problem [13]. 3.2. German Compound Word Splitting German compound words present sparsity challenges for machine translation. To address this, we split German words following the general approach of [15]. The idea is to split a word if the geometric average of its subword frequencies is larger than whole word frequency. In our implementation, for each word, we searched for all possible decompositions into two sub-words, considering the possibility of deleting common German fillers “e”, “es”, and “s” (as in ”Arbeit+s+tier”). The unigram frequencies for the subwords and 1 Code/scripts available at http://cl.naist.jp/∼kevinduh/a/acl2013 128 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 whole word is computed from the German p"
2014.iwslt-evaluation.18,I11-1153,1,0.863598,"e the RNNLM log probabilities and add them as an additional feature to each translation hypothesis. We then re-run a single MERT optimization to find ideal weights for this new feature, and then extract the 1-best result from the 10,000-best list for the test set according to these new weights. The parameters for RNNLM training are tuned on the dev set to maximize perplexity, resulting in 300 nodes in the hidden layer, 300 classes, and 4 steps of back-propagation through time. 3.4. GMBR System Combination We used a system combination method based on Generalized Minimum Bayes Risk optimization [17], which has been successfully applied to different types of SMT systems for patent translation [18]. Note that our system combination only picks one hypothesis from an N-best list and does not generate a new hypothesis by mixing partial hypotheses among the N-best. 3.4.1. Theory Minimum Bayes Risk (MBR) is a decision rule to choose hypotheses that minimize the expected loss. In the task of SMT from a French sentence (f ) to an English sentence (e), the MBR decision rule on δ(f ) → e′ with the loss function L over the possible space of sentence pairs (p(e, f )) is denoted as: ∑ L(δ(f )|e)p(e|f"
2014.iwslt-evaluation.18,2011.mtsummit-papers.36,1,0.801283,"e improvements from the baseline 1-best in our pilot test, but they were much smaller than those resulting from RNNLM, and when the NNJM was combined with RNNLM we saw no significant gains. One possible reason is the small training data size; the model is very sparse and needs large training data because of its large contexts of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Jou"
2014.iwslt-evaluation.18,D13-1139,1,0.77246,"1-best in our pilot test, but they were much smaller than those resulting from RNNLM, and when the NNJM was combined with RNNLM we saw no significant gains. One possible reason is the small training data size; the model is very sparse and needs large training data because of its large contexts of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treeban"
2014.iwslt-evaluation.18,P13-4016,1,0.849598,"parse and needs large training data because of its large contexts of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank5 for English, and TIGER corpus [26] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 with Kneser-Ney smoothing. For search in the F2S m"
2014.iwslt-evaluation.18,P07-2045,0,0.00853439,"s of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank5 for English, and TIGER corpus [26] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 with Kneser-Ney smoothing. For search in the F2S models, we used the previously described hypergraph search method"
2014.iwslt-evaluation.18,W06-1607,0,0.0253342,"uning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank5 for English, and TIGER corpus [26] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 with Kneser-Ney smoothing. For search in the F2S models, we used the previously described hypergraph search method. For the Moses phrase tables, we used standard training settings with Kneser-Ney smoothing of phrase translation probabilities [27]. 4.1.2. Translation Models We trained the translation models using WIT3 training data (178,526 sentences) and 1,000,000 sentences selected over other bitexts (Europarl, News Commentary, and Common Crawl) by the method described in Section 3.1. 4.1.3. Language Models We used word 5-gram language models of German and English that were linearly interpolated from several word 5-gram language models trained on different data sources (WIT3 , Europarl, News Commentary, and Common Crawl). The interpolation weights were optimized to minimize perplexity on the development set, using interpolate-lm.perl"
2014.iwslt-evaluation.18,P11-1132,0,0.0600041,"Missing"
2014.iwslt-evaluation.18,P14-1129,0,0.0304397,"where a difference exists in the true loss. Then we optimize θ in a formulation similar to a Ranking SVM [19]. The pair-wise nature of Eqs. 6 and 7 makes the problem amendable to solutions in “learning to rank” literature [20]. We used BLEU as the objective function and the subcomponents of BLEU as features (system identity feature was not used). There is one regularization hyperparameter for the Ranking SVM, which we set by cross-validation over the development set (dev2010). 3.5. What Didn’t Work Immediately This year we tried to include a state-of-the-art Neural Network Joint Model (NNJM) [21] to improve the accuracy of translation probability estimation. The model is used to predict a target language word using its three preceding target language words and eleven source language words surrounding its affiliation (the non-NULL source language word aligned to the target language word to be predicted). We used top 16,000 source and target vocabularies in the model and mapped the other words into a single OOV symbol, while the original paper[21] used part-of-speech classes. Although the original paper presented a method for integrating the model with decoding, we used the NNJM for rer"
2014.iwslt-evaluation.18,N13-1116,0,\N,Missing
2014.iwslt-evaluation.18,2013.iwslt-evaluation.12,1,\N,Missing
2014.iwslt-papers.16,P01-1067,0,0.149499,"lly annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and these speechrelated applications. The first version of the corpus includes 1,217 sentences and 23,158 words manually annotated with parse trees, and aligned with translations in 26-43 different languages. In this paper we describe the collection of the corpus, and an analysis of its various characteristics. 1. Introduction Syntactic parsing is widely considered as a useful component of natural language processing systems, not the least of which being machine translation [1, 2]. While a large part of the work on these applications has focused on the written word, we can assume that the fundamental principles behind syntax’s success in these applications will also carry over to spoken language as well. The great majority of recent work on syntactic parsing has been based on the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers"
2014.iwslt-papers.16,P14-2024,1,0.799534,"lly annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and these speechrelated applications. The first version of the corpus includes 1,217 sentences and 23,158 words manually annotated with parse trees, and aligned with translations in 26-43 different languages. In this paper we describe the collection of the corpus, and an analysis of its various characteristics. 1. Introduction Syntactic parsing is widely considered as a useful component of natural language processing systems, not the least of which being machine translation [1, 2]. While a large part of the work on these applications has focused on the written word, we can assume that the fundamental principles behind syntax’s success in these applications will also carry over to spoken language as well. The great majority of recent work on syntactic parsing has been based on the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers"
2014.iwslt-papers.16,J93-2004,0,0.047231,"the least of which being machine translation [1, 2]. While a large part of the work on these applications has focused on the written word, we can assume that the fundamental principles behind syntax’s success in these applications will also carry over to spoken language as well. The great majority of recent work on syntactic parsing has been based on the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers. However, as there are large differences between written language and spoken language, there have also been some efforts to create resources for spoken language, including the Penn Treebank annotations of ATIS travel conversation and Switchboard telephone conversation data, as well as the OntoNotes [4] annotation of broadcast news and commentary. While these corpora mainly focus on informal speech or news, spoken monologue in the form of talks presented to an audience is also an attractive target for speech processing applications. In particular"
2014.iwslt-papers.16,N06-2015,0,0.00919081,"the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers. However, as there are large differences between written language and spoken language, there have also been some efforts to create resources for spoken language, including the Penn Treebank annotations of ATIS travel conversation and Switchboard telephone conversation data, as well as the OntoNotes [4] annotation of broadcast news and commentary. While these corpora mainly focus on informal speech or news, spoken monologue in the form of talks presented to an audience is also an attractive target for speech processing applications. In particular, the talk data from TED1 has been used as a target for much research, most notably the IWSLT evaluation campaigns [5]. In this work, we present the NAIST-NTT TED Talk Treebank, a new manually annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and speech-related applications such as sp"
2014.iwslt-papers.16,2012.eamt-1.60,0,0.282225,"License at http://ahclab.naist.jp/resource/tedtreebank 2. Corpus Data In this section, we describe the data used as material for the corpus. 2.1. English Data Table 1: Details of the annotated data. Set All Train Test Talk 10 7 3 Min. 125.07 87.23 37.84 Sent. 1,217 822 395 Word 23,158 16,063 7,095 The English text and speech data were gathered from TED Talks. Specifically, we gathered data starting with the beginning of 1 http://www.ted.com 265 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the May 2012 version of the WIT3 [6] training corpus for EnglishJapanese. From this data, for the first version of the treebank we chose 10 talks, the details of which are shown in Table 1.2 As the original TED data is subtitles, it is necessary to group these subtitles into sentences before performing annotation. In the creation of the corpus, we used the standard English sentence segmentation provided by the WIT3 data.3 In addition, when using a corpus for experiments, it is desirable to have a “standard” split between the training and testing data. As this standard, we designated a split of the first 7 talks as training data,"
2014.iwslt-papers.16,P06-1055,0,0.0347901,"lso the first multilingually aligned treebank of the spoken word. We hope that this data will be of use for investigations into the effect of syntax on speech translation and other cross-lingual tasks. Treebank annotation is an extremely time consuming process, particularly when the entirety of the tree has to be created from scratch. Fortunately, relatively accurate treebank parsers already exist, allowing us to create an initial parse first using an off-the-shelf parser, then have annotators spend their time fixing the errors of the existing parser. In this case, we use the Berkeley Parser6 [8] to create an initial parse. After this, we hired annotators to go through the trees and annotated them based on the standard described in the previous section. The annotators are well versed in annotation of linguistic data, and were given the standard and asked to follow it closely. After receiving this initial annotation result, the first author of the paper went through the entirety of the corpus, checking once more for any remaining errors. Finally, the trees were automatically checked for inconsistencies such as duplicated unary rules, or trees that were judged as a warning or error acco"
2014.iwslt-papers.16,W07-2416,0,0.0146745,"ugh the trees and annotated them based on the standard described in the previous section. The annotators are well versed in annotation of linguistic data, and were given the standard and asked to follow it closely. After receiving this initial annotation result, the first author of the paper went through the entirety of the corpus, checking once more for any remaining errors. Finally, the trees were automatically checked for inconsistencies such as duplicated unary rules, or trees that were judged as a warning or error according to the phrase structure conversion tools of Johansson and Nugues [9]. 3. Creation of Parse Trees 4. Speech Time Alignment The first, and most labor-intensive annotation task was the creation of manual parse trees for the English sentences. Because the treebank described in this paper is of spoken language, the correspondence between syntactic trees and features of the speech is of particular interest. For example, it has been previously noted that prosody and syntax have a close relationship [10], and this corpus could be used to perform further investigations into these and other issues. In order to create the time alignment of each word in the speech, Table"
2014.iwslt-papers.16,2013.iwslt-evaluation.23,1,0.804544,"Missing"
2014.iwslt-papers.16,W07-1001,0,0.0638861,"Missing"
2020.acl-demos.18,E06-1032,0,0.145414,"translators. The desire for fast and consistent evaluation has led to the emergence of a plethora of automatic evaluation metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METOR (Banerjee and Lavie, 2005) and Kevin Duh Johns Hopkins University kevinduh@cs.jhu.edu BEER (Stanojevi´c and Sima’an, 2014). Out of the aforementioned metrics, BLEU has become the de facto evaluation metric for machine translation. It calculates the weighted average of n-gram precision between a translated sentence and a reference sentence. Nevertheless, BLEU, too, has its problems. For example, Callison-Burch et al. (2006) showed that an improved BLEU score does not represent an actual improvement in translation quality. There are also some proposals to evaluate the quality of translations with the help of extrinsic proxy tasks. Berka et al. (2011) collected short English documents from various domains and created yes and no questions in Czech. They then translated the English documents into Czech and evaluated the quality of the MT systems based on human performances on the documents and questions in Czech. Scarton and Specia (2016) translated a dataset of German reading comprehension tests into English with v"
2020.acl-demos.18,N18-2073,1,0.902123,"Missing"
2020.acl-demos.18,W19-5302,0,0.0999325,"tweight python-based MT evaluation toolkit that consumes the same inputs as other automatic MT evaluation tools such as multibleu.perl and SacreBLEU (Post, 2018) and does not require any additional annotated CLIR data. Instead, it automatically transforms inputs into a synthetic CLIR dataset on the fly with the help of an Information Retrieval (IR) system. It implements the document translation approach to CLIR, where MT translations are viewed as documents and indexed using a commonly-used search engine (Elasticsearch). As a case study, we test CLIReval on the metrics shared task of WMT2019 (Ma et al., 2019), which measures the Pearson correlations (r) between automatically generated MT metrics and human judgments. Results show that CLIReval consistently performs at the level of r ≥ 0.9 and is on par or even outperforms popular metrics such as BLEU on multiple language directions. Further, this is achieved without using external data or doing domain-based parameter tuning. These promising results highlight the potential of CLIR as a proxy task for MT evaluation, and we hope CLIReval can facilitate future research in this area. Our key contributions in this work can be summarized as follows: 1. We"
2020.acl-demos.18,P99-1027,0,0.443036,"and consistency issues as manual evaluation. One downstream task that relies heavily on MT but has not been used as a method to evaluate MT systems is the task of Cross-Lingual Information Retrieval (CLIR). CLIR is a task in which search queries are issued in one language, and the retrieved relevant documents are written in a different language. Two commonly used methods in CLIR are query translation, where queries are translated into the same language as the documents and document translation where documents are translated into the same language as the queries (Zhou et al., 2012; Oard, 1998; McCarley, 1999). A monolingual IR system is then used to obtain search results. CLIR is an active field of research, and previ134 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 134–141 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ous works suggest that the performance of CLIR correlates highly with the quality of the MT (Zhu and Wang, 2006; Nie, 2010; Yarmohammadi et al., 2019). Therefore, we expect IR metrics to be good indicators of the quality of translations. Unfortunately, there is currently no publicly available tool to facilit"
2020.acl-demos.18,D18-1211,0,0.0212219,"language. Analyzers are Elasticsearch modules that preprocess and tokenize queries and documents according to language-specific rules. It also implements stopwords removal and stemming. These are important operations that affect the quality of search results. Second, Elasticsearch implements many competitive retrieval models used by IR researchers and practitioners. By default, CLIReval uses the Okapi BM25 (Robertson et al., 2009) score to measure the degree of similarity of documents to a given search query. Note that BM25 shows strong performances on many datasets (Chapelle and Chang, 2011; McDonald et al., 2018) and frequently outperforms newer “state of the art” methods (Guo et al., 2016). It is also fast to compute, allowing CLIReval to run in a highly efficient manner. Third, Elasticsearch is a widely used search engine solution that is supported on various platforms. This increases the ease of installation for users of CLIReval. CLIReval separately indexes the documents from MT and REF files into two instances of Elasticsearch. It then queries the Elasticsearch instances with the generated query strings. For every query, Elasticsearch returns the top 100 documents ranked by BM25 scores. Since tre"
2020.acl-demos.18,L16-1579,0,0.0185807,"reference sentence. Nevertheless, BLEU, too, has its problems. For example, Callison-Burch et al. (2006) showed that an improved BLEU score does not represent an actual improvement in translation quality. There are also some proposals to evaluate the quality of translations with the help of extrinsic proxy tasks. Berka et al. (2011) collected short English documents from various domains and created yes and no questions in Czech. They then translated the English documents into Czech and evaluated the quality of the MT systems based on human performances on the documents and questions in Czech. Scarton and Specia (2016) translated a dataset of German reading comprehension tests into English with various MT systems such as Google Translate and Bing Translate and judged the quality of translations based on human performances on the translated reading comprehension datasets. Unfortunately, these external tasks suffer from the same scalability and consistency issues as manual evaluation. One downstream task that relies heavily on MT but has not been used as a method to evaluate MT systems is the task of Cross-Lingual Information Retrieval (CLIR). CLIR is a task in which search queries are issued in one language,"
2020.acl-demos.18,P14-2080,0,0.0236797,"l. CLIReval separately indexes the documents from MT and REF files into two instances of Elasticsearch. It then queries the Elasticsearch instances with the generated query strings. For every query, Elasticsearch returns the top 100 documents ranked by BM25 scores. Since trec eval only accepts discrete relevance judgment labels, the relevance label converter module is used to convert search scores from REF-IR into discrete labels. 3.4 Relevance Label Converter We implement three ways of converting raw BM25 scores of REF-IR into discrete relevance judgment labels: The query in document method (Schamoni et al., 2014; Sasaki et al., 2018) assigns 1 to a document if and only if the given search query 137 7 https://www.elastic.co/ CLIReval is flexible and users can easily replace Elasticsearch with their own IR system. 8 retrieved documents to the range [0, 1] and use Jenks natural breaks optimization to convert the BM25 scores into discrete relevance judgment labels. Users can specify the number of classes with the jenks nb class argument. Figure 4 illustrates an example of how relevance labels are generated for each query-document pair using the generated query Q (see Section 3.2 and the reference documen"
2020.acl-demos.18,2006.amta-papers.25,0,0.0917585,"nguage to a target language. A natural question that arises is how do we determine whether an MT system is translating sentences well? One answer is that we can engage human translators to evaluate the translated sentences manually. Unfortunately, evaluating translations can be relatively time-consuming and worse, the fact that the quality of translation is inherently subjective can lead to variations among different human translators. The desire for fast and consistent evaluation has led to the emergence of a plethora of automatic evaluation metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METOR (Banerjee and Lavie, 2005) and Kevin Duh Johns Hopkins University kevinduh@cs.jhu.edu BEER (Stanojevi´c and Sima’an, 2014). Out of the aforementioned metrics, BLEU has become the de facto evaluation metric for machine translation. It calculates the weighted average of n-gram precision between a translated sentence and a reference sentence. Nevertheless, BLEU, too, has its problems. For example, Callison-Burch et al. (2006) showed that an improved BLEU score does not represent an actual improvement in translation quality. There are also some proposals to evaluate the quality of translat"
2020.acl-demos.18,W14-3354,0,0.0659595,"Missing"
2020.acl-demos.18,W19-6602,1,0.814834,"ted into the same language as the documents and document translation where documents are translated into the same language as the queries (Zhou et al., 2012; Oard, 1998; McCarley, 1999). A monolingual IR system is then used to obtain search results. CLIR is an active field of research, and previ134 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 134–141 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ous works suggest that the performance of CLIR correlates highly with the quality of the MT (Zhu and Wang, 2006; Nie, 2010; Yarmohammadi et al., 2019). Therefore, we expect IR metrics to be good indicators of the quality of translations. Unfortunately, there is currently no publicly available tool to facilitate research in this area, and this motivates us to design and implement CLIReval. CLIReval is a lightweight python-based MT evaluation toolkit that consumes the same inputs as other automatic MT evaluation tools such as multibleu.perl and SacreBLEU (Post, 2018) and does not require any additional annotated CLIR data. Instead, it automatically transforms inputs into a synthetic CLIR dataset on the fly with the help of an Information Retr"
2020.acl-demos.18,oard-1998-comparative,0,0.492139,"scalability and consistency issues as manual evaluation. One downstream task that relies heavily on MT but has not been used as a method to evaluate MT systems is the task of Cross-Lingual Information Retrieval (CLIR). CLIR is a task in which search queries are issued in one language, and the retrieved relevant documents are written in a different language. Two commonly used methods in CLIR are query translation, where queries are translated into the same language as the documents and document translation where documents are translated into the same language as the queries (Zhou et al., 2012; Oard, 1998; McCarley, 1999). A monolingual IR system is then used to obtain search results. CLIR is an active field of research, and previ134 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 134–141 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ous works suggest that the performance of CLIR correlates highly with the quality of the MT (Zhu and Wang, 2006; Nie, 2010; Yarmohammadi et al., 2019). Therefore, we expect IR metrics to be good indicators of the quality of translations. Unfortunately, there is currently no publicly availabl"
2020.acl-demos.18,C12-1164,0,0.0245963,"ffer from the same scalability and consistency issues as manual evaluation. One downstream task that relies heavily on MT but has not been used as a method to evaluate MT systems is the task of Cross-Lingual Information Retrieval (CLIR). CLIR is a task in which search queries are issued in one language, and the retrieved relevant documents are written in a different language. Two commonly used methods in CLIR are query translation, where queries are translated into the same language as the documents and document translation where documents are translated into the same language as the queries (Zhou et al., 2012; Oard, 1998; McCarley, 1999). A monolingual IR system is then used to obtain search results. CLIR is an active field of research, and previ134 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 134–141 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ous works suggest that the performance of CLIR correlates highly with the quality of the MT (Zhu and Wang, 2006; Nie, 2010; Yarmohammadi et al., 2019). Therefore, we expect IR metrics to be good indicators of the quality of translations. Unfortunately, there is currently no publi"
2020.acl-demos.18,P02-1040,0,0.119555,"ng sentences from a source language to a target language. A natural question that arises is how do we determine whether an MT system is translating sentences well? One answer is that we can engage human translators to evaluate the translated sentences manually. Unfortunately, evaluating translations can be relatively time-consuming and worse, the fact that the quality of translation is inherently subjective can lead to variations among different human translators. The desire for fast and consistent evaluation has led to the emergence of a plethora of automatic evaluation metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METOR (Banerjee and Lavie, 2005) and Kevin Duh Johns Hopkins University kevinduh@cs.jhu.edu BEER (Stanojevi´c and Sima’an, 2014). Out of the aforementioned metrics, BLEU has become the de facto evaluation metric for machine translation. It calculates the weighted average of n-gram precision between a translated sentence and a reference sentence. Nevertheless, BLEU, too, has its problems. For example, Callison-Burch et al. (2006) showed that an improved BLEU score does not represent an actual improvement in translation quality. There are also some proposals to evalu"
2020.acl-demos.18,P06-1075,0,0.062896,"tion, where queries are translated into the same language as the documents and document translation where documents are translated into the same language as the queries (Zhou et al., 2012; Oard, 1998; McCarley, 1999). A monolingual IR system is then used to obtain search results. CLIR is an active field of research, and previ134 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 134–141 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ous works suggest that the performance of CLIR correlates highly with the quality of the MT (Zhu and Wang, 2006; Nie, 2010; Yarmohammadi et al., 2019). Therefore, we expect IR metrics to be good indicators of the quality of translations. Unfortunately, there is currently no publicly available tool to facilitate research in this area, and this motivates us to design and implement CLIReval. CLIReval is a lightweight python-based MT evaluation toolkit that consumes the same inputs as other automatic MT evaluation tools such as multibleu.perl and SacreBLEU (Post, 2018) and does not require any additional annotated CLIR data. Instead, it automatically transforms inputs into a synthetic CLIR dataset on the f"
2020.acl-demos.18,W18-6319,0,0.0186263,"sociation for Computational Linguistics ous works suggest that the performance of CLIR correlates highly with the quality of the MT (Zhu and Wang, 2006; Nie, 2010; Yarmohammadi et al., 2019). Therefore, we expect IR metrics to be good indicators of the quality of translations. Unfortunately, there is currently no publicly available tool to facilitate research in this area, and this motivates us to design and implement CLIReval. CLIReval is a lightweight python-based MT evaluation toolkit that consumes the same inputs as other automatic MT evaluation tools such as multibleu.perl and SacreBLEU (Post, 2018) and does not require any additional annotated CLIR data. Instead, it automatically transforms inputs into a synthetic CLIR dataset on the fly with the help of an Information Retrieval (IR) system. It implements the document translation approach to CLIR, where MT translations are viewed as documents and indexed using a commonly-used search engine (Elasticsearch). As a case study, we test CLIReval on the metrics shared task of WMT2019 (Ma et al., 2019), which measures the Pearson correlations (r) between automatically generated MT metrics and human judgments. Results show that CLIReval consiste"
2020.acl-demos.34,N18-1008,0,0.052679,". E2E-ST has several advantages over the cascaded approach: (1) a single E2E-ST model can reduce latency at inference time, which is useful for time-critical use cases like simultaneous interpretation. (2) A single model enables back-propagation training in an end-to-end fashion, which mitigates the risk of error propagation by cascaded modules. (3) In certain use cases such as endangered language documentation (Bird et al., 2014), source speech and target text translation (without the intermediate source text transcript) might be easier to obtain, necessitating the adoption of E2E-ST models (Anastasopoulos and Chiang, 2018). Nevertheless, the verdict is still out on the comparison of translation quality between E2E-ST and CascadeST. Some empirical results favor E2E (Weiss et al., 2017) while others favor Cascade (Niehues et al., 2019); the conclusion also depends on the nuances of the training data condition (Sperber et al., 2019). We believe the time is ripe to develop a unified toolkit that facilitates research in both E2E and cascaded approaches. We present ESPnet-ST, a toolkit that implements many of the recent models for E2E-ST, as well as the ASR and MT modules for Cascade-ST. Our goal is to provide a tool"
2020.acl-demos.34,N19-1006,0,0.376734,"al., 2019). We believe the time is ripe to develop a unified toolkit that facilitates research in both E2E and cascaded approaches. We present ESPnet-ST, a toolkit that implements many of the recent models for E2E-ST, as well as the ASR and MT modules for Cascade-ST. Our goal is to provide a toolkit where researchers can easily incorporate and test new ideas under different approaches. Recent research suggests that pre-training, multi-task learning, and transfer learning are important techniques for achieving improved results for E2E-ST (B´erard et al., 2018; Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019). Thus, a unified toolkit that enables researchers to seamlessly mix-and-match different ASR and MT models in training both E2E-ST and Cascade-ST systems would facilitate research in the field.1 ESPnet-ST is especially designed to target the ST task. ESPnet was originally developed for the 1 There exist many excellent toolkits that support both ASR and MT tasks (see Table 1). However, it is not always straightforward to use them for E2E-ST and Cascade-ST, due to incompatible training/inference pipelines in different modules or lack of detailed preprocessing/training scri"
2020.acl-demos.34,C14-1096,0,0.0214064,"d speech translation (E2E-ST) systems, where a single S2S directly maps speech in a source language to its translation in the target language (B´erard et al., 2016; Weiss et al., 2017). E2E-ST has several advantages over the cascaded approach: (1) a single E2E-ST model can reduce latency at inference time, which is useful for time-critical use cases like simultaneous interpretation. (2) A single model enables back-propagation training in an end-to-end fashion, which mitigates the risk of error propagation by cascaded modules. (3) In certain use cases such as endangered language documentation (Bird et al., 2014), source speech and target text translation (without the intermediate source text transcript) might be easier to obtain, necessitating the adoption of E2E-ST models (Anastasopoulos and Chiang, 2018). Nevertheless, the verdict is still out on the comparison of translation quality between E2E-ST and CascadeST. Some empirical results favor E2E (Weiss et al., 2017) while others favor Cascade (Niehues et al., 2019); the conclusion also depends on the nuances of the training data condition (Sperber et al., 2019). We believe the time is ripe to develop a unified toolkit that facilitates research in b"
2020.acl-demos.34,N19-1202,0,0.548931,"Missing"
2020.acl-demos.34,L18-1531,0,0.0850367,"Missing"
2020.acl-demos.34,P17-4012,0,0.0337563,"T ST X X X X X X X X X X X X X X X X♣ X♣ X X♣ X X – – X – – X X – – X X X X – – X – X X X – – X X X X – – X – X X X X – X – – – – – – – X X – X X X – X – X X X – X X X – – X – X X – – X – X X X – – X – – – – – X – X♦ X X – – X – – – – – – – X X X – – – – X X – – – – X X X – – – – X X – – – – X Table 1: Framework comparison on supported tasks in January, 2020. ♣ Not publicly available. ♦ Available only in Google Cloud storage. 1 (Shen et al., 2019) 2 (Kuchaiev et al., 2018) 3 (Kuchaiev et al., 2019) 4 (Zeyer et al., 2018) 5 (Zenkel et al., 2018) 6 (Ott et al., 2019) 7 (Vaswani et al., 2018) 8 (Klein et al., 2017) 9 (Povey et al., 2011) 10 (Pratap et al., 2019) ASR task (Watanabe et al., 2018), and recently extended to the text-to-speech (TTS) task (Hayashi et al., 2020). Here, we extend ESPnet to ST tasks, providing code for building translation systems and recipes (i.e., scripts that encapsulate the entire training/inference procedure for reproducibility purposes) for a wide range of ST benchmarks. This is a non-trivial extension: with a unified codebase for ASR/MT/ST and a wide range of recipes, we believe ESPnet-ST is an all-in-one toolkit that should make it easier for both ASR and MT researchers"
2020.acl-demos.34,L18-1001,0,0.308977,"ide range of recipes, we believe ESPnet-ST is an all-in-one toolkit that should make it easier for both ASR and MT researchers to get started in ST research. The contributions of ESPnet-ST are as follows: • To the best of our knowledge, this is the first toolkit to include ASR, MT, TTS, and ST recipes and models in the same codebase. Since our codebase is based on the unified framework with a common stage-by-stage processing (Povey et al., 2011), it is very easy to customize training data and models. • We provide recipes for ST corpora such as Fisher-CallHome (Post et al., 2013), Libri-trans (Kocabiyikoglu et al., 2018), How2 (Sanabria et al., 2018), and MustC (Di Gangi et al., 2019a)2 . Each recipe contains a single script (run.sh), which covers all experimental processes, such as corpus preparation, data augmentations, and transfer learning. • We provide the open-sourced toolkit and the pre-trained models whose hyper-parameters are intensively tuned. Moreover, we provide interactive demo of speech-to-speech translation hosted by Google Colab.3 2 Design 2.1 Installation All required tools are automatically downloaded and built under tools (see Figure 1) by a make command. The tools include (1) neural networ"
2020.acl-demos.34,P07-2045,0,0.036373,"s, such as corpus preparation, data augmentations, and transfer learning. • We provide the open-sourced toolkit and the pre-trained models whose hyper-parameters are intensively tuned. Moreover, we provide interactive demo of speech-to-speech translation hosted by Google Colab.3 2 Design 2.1 Installation All required tools are automatically downloaded and built under tools (see Figure 1) by a make command. The tools include (1) neural network libraries such as PyTorch (Paszke et al., 2019), (2) ASR-related toolkits such as Kaldi (Povey et al., 2011), and (3) MT-related toolkits such as Moses (Koehn et al., 2007) and sentencepiece (Kudo, 2018). ESPnet-ST is implemented with Pytorch backend. 2.2 Recipes for reproducible experiments We provide various recipes for all tasks in order to quickly and easily reproduce the strong baseline systems with a single script. The directory structure is depicted as in Figure 1. egs contains corpus directories, in which the corresponding task directories (e.g., st1) are included. To run experiments, we simply execute run.sh under the desired task directory. Configuration yaml files for feature extraction, data augmentation, model training, and decoding etc. are include"
2020.acl-demos.34,W18-2507,0,0.0893277,"Missing"
2020.acl-demos.34,P18-1007,0,0.0203546,"mentations, and transfer learning. • We provide the open-sourced toolkit and the pre-trained models whose hyper-parameters are intensively tuned. Moreover, we provide interactive demo of speech-to-speech translation hosted by Google Colab.3 2 Design 2.1 Installation All required tools are automatically downloaded and built under tools (see Figure 1) by a make command. The tools include (1) neural network libraries such as PyTorch (Paszke et al., 2019), (2) ASR-related toolkits such as Kaldi (Povey et al., 2011), and (3) MT-related toolkits such as Moses (Koehn et al., 2007) and sentencepiece (Kudo, 2018). ESPnet-ST is implemented with Pytorch backend. 2.2 Recipes for reproducible experiments We provide various recipes for all tasks in order to quickly and easily reproduce the strong baseline systems with a single script. The directory structure is depicted as in Figure 1. egs contains corpus directories, in which the corresponding task directories (e.g., st1) are included. To run experiments, we simply execute run.sh under the desired task directory. Configuration yaml files for feature extraction, data augmentation, model training, and decoding etc. are included in conf. Model directories in"
2020.acl-demos.34,N19-4009,0,0.0282508,"scademodel ASR LM MT TTS ASR LM MT TTS ST ST ST ST X X X X X X X X X X X X X X X X♣ X♣ X X♣ X X – – X – – X X – – X X X X – – X – X X X – – X X X X – – X – X X X X – X – – – – – – – X X – X X X – X – X X X – X X X – – X – X X – – X – X X X – – X – – – – – X – X♦ X X – – X – – – – – – – X X X – – – – X X – – – – X X X – – – – X X – – – – X Table 1: Framework comparison on supported tasks in January, 2020. ♣ Not publicly available. ♦ Available only in Google Cloud storage. 1 (Shen et al., 2019) 2 (Kuchaiev et al., 2018) 3 (Kuchaiev et al., 2019) 4 (Zeyer et al., 2018) 5 (Zenkel et al., 2018) 6 (Ott et al., 2019) 7 (Vaswani et al., 2018) 8 (Klein et al., 2017) 9 (Povey et al., 2011) 10 (Pratap et al., 2019) ASR task (Watanabe et al., 2018), and recently extended to the text-to-speech (TTS) task (Hayashi et al., 2020). Here, we extend ESPnet to ST tasks, providing code for building translation systems and recipes (i.e., scripts that encapsulate the entire training/inference procedure for reproducibility purposes) for a wide range of ST benchmarks. This is a non-trivial extension: with a unified codebase for ASR/MT/ST and a wide range of recipes, we believe ESPnet-ST is an all-in-one toolkit that should"
2020.acl-demos.34,P02-1040,0,0.11071,"2019) 2 (Bahar et al., 2019a) 3 (Bahar et al., 2019b) 4 (Wang et al., 2020) translation corpora: Fisher-CallHome Spanish En→Es, Libri-trans En→Fr, How2 En→Pt, and Must-C En→8 languages. Moreover, we also performed experiments on IWSLT16 En-De to validate the performance of our MT modules. All sentences were tokenized with the tokenizer.perl script in the Moses toolkit (Koehn et al., 2007). We used the joint source and target vocabularies based on byte pair encoding (BPE) (Sennrich et al., 2016) units. ASR vocabularies were created with English sentences only with lc.rm. We report 4-gram BLEU (Papineni et al., 2002) scores with the multi-bleu.perl script in Moses. For speech features, we extracted 80-channel log-mel filterbank coefficients with 3-dimensional pitch features using Kaldi, resulting 83-dimensional features per frame. Detailed training and decoding configura4.1 Fisher-CallHome Spanish (Es→En) Fisher-CallHome Spanish corpus contains 170hours of Spanish conversational telephone speech, the corresponding transcription, as well as the English translations (Post et al., 2013). All punctuation marks except for apostrophe were removed (Post et al., 2013; Kumar et al., 2014; Weiss et al., 2017). We r"
2020.acl-demos.34,2011.iwslt-papers.7,0,0.0330805,"et al., 2019). Decoding with the external LSTM-based LM trained in the Stage 3 is also conducted (Kannan et al., 2017). The transformer uses 12 self-attention blocks stacked on the two VGG blocks in the speech encoder and 6 self-attention blocks in the transcription decoder; see (Karita et al., 2019) for implementation details. Machine translation (MT) The MT model consists of the source text encoder and translation decoder, implemented as a transformer with 6 selfattention blocks. For simplicity, we train the MT model by feeding lowercased source sentences without punctuation marks (lc.rm) (Peitz et al., 2011). There are options to explore characters and different subword units in the MT component. End-to-end speech translation (E2E-ST) Our E2E-ST model is composed of the speech encoder and translation decoder. Since the definition of parameter names is exactly same as in the ASR and MT components, it is quite easy to copy parameters from the pre-trained models for transfer learning. After ASR and MT models are trained as described above, their parameters are extracted and used to initialize the E2E-ST model. The model is then trained on ST data, with the option of incorporating multi-task objectiv"
2020.acl-demos.34,2013.iwslt-papers.14,0,0.594204,"ed codebase for ASR/MT/ST and a wide range of recipes, we believe ESPnet-ST is an all-in-one toolkit that should make it easier for both ASR and MT researchers to get started in ST research. The contributions of ESPnet-ST are as follows: • To the best of our knowledge, this is the first toolkit to include ASR, MT, TTS, and ST recipes and models in the same codebase. Since our codebase is based on the unified framework with a common stage-by-stage processing (Povey et al., 2011), it is very easy to customize training data and models. • We provide recipes for ST corpora such as Fisher-CallHome (Post et al., 2013), Libri-trans (Kocabiyikoglu et al., 2018), How2 (Sanabria et al., 2018), and MustC (Di Gangi et al., 2019a)2 . Each recipe contains a single script (run.sh), which covers all experimental processes, such as corpus preparation, data augmentations, and transfer learning. • We provide the open-sourced toolkit and the pre-trained models whose hyper-parameters are intensively tuned. Moreover, we provide interactive demo of speech-to-speech translation hosted by Google Colab.3 2 Design 2.1 Installation All required tools are automatically downloaded and built under tools (see Figure 1) by a make co"
2020.acl-demos.34,P16-1162,0,0.079723,"ble 3: BLEU of ST systems on Libri-trans corpus. ♣ Implemented w/ ESPnet. 4 Pre-training. ♦ w/ SpecAugment. 1 (Liu et al., 2019) 2 (Bahar et al., 2019a) 3 (Bahar et al., 2019b) 4 (Wang et al., 2020) translation corpora: Fisher-CallHome Spanish En→Es, Libri-trans En→Fr, How2 En→Pt, and Must-C En→8 languages. Moreover, we also performed experiments on IWSLT16 En-De to validate the performance of our MT modules. All sentences were tokenized with the tokenizer.perl script in the Moses toolkit (Koehn et al., 2007). We used the joint source and target vocabularies based on byte pair encoding (BPE) (Sennrich et al., 2016) units. ASR vocabularies were created with English sentences only with lc.rm. We report 4-gram BLEU (Papineni et al., 2002) scores with the multi-bleu.perl script in Moses. For speech features, we extracted 80-channel log-mel filterbank coefficients with 3-dimensional pitch features using Kaldi, resulting 83-dimensional features per frame. Detailed training and decoding configura4.1 Fisher-CallHome Spanish (Es→En) Fisher-CallHome Spanish corpus contains 170hours of Spanish conversational telephone speech, the corresponding transcription, as well as the English translations (Post et al., 2013)."
2020.acl-demos.34,Q19-1020,0,0.51616,"on by cascaded modules. (3) In certain use cases such as endangered language documentation (Bird et al., 2014), source speech and target text translation (without the intermediate source text transcript) might be easier to obtain, necessitating the adoption of E2E-ST models (Anastasopoulos and Chiang, 2018). Nevertheless, the verdict is still out on the comparison of translation quality between E2E-ST and CascadeST. Some empirical results favor E2E (Weiss et al., 2017) while others favor Cascade (Niehues et al., 2019); the conclusion also depends on the nuances of the training data condition (Sperber et al., 2019). We believe the time is ripe to develop a unified toolkit that facilitates research in both E2E and cascaded approaches. We present ESPnet-ST, a toolkit that implements many of the recent models for E2E-ST, as well as the ASR and MT modules for Cascade-ST. Our goal is to provide a toolkit where researchers can easily incorporate and test new ideas under different approaches. Recent research suggests that pre-training, multi-task learning, and transfer learning are important techniques for achieving improved results for E2E-ST (B´erard et al., 2018; Anastasopoulos and Chiang, 2018; Bansal et a"
2020.acl-demos.34,W18-1819,0,0.0409443,"TTS ASR LM MT TTS ST ST ST ST X X X X X X X X X X X X X X X X♣ X♣ X X♣ X X – – X – – X X – – X X X X – – X – X X X – – X X X X – – X – X X X X – X – – – – – – – X X – X X X – X – X X X – X X X – – X – X X – – X – X X X – – X – – – – – X – X♦ X X – – X – – – – – – – X X X – – – – X X – – – – X X X – – – – X X – – – – X Table 1: Framework comparison on supported tasks in January, 2020. ♣ Not publicly available. ♦ Available only in Google Cloud storage. 1 (Shen et al., 2019) 2 (Kuchaiev et al., 2018) 3 (Kuchaiev et al., 2019) 4 (Zeyer et al., 2018) 5 (Zenkel et al., 2018) 6 (Ott et al., 2019) 7 (Vaswani et al., 2018) 8 (Klein et al., 2017) 9 (Povey et al., 2011) 10 (Pratap et al., 2019) ASR task (Watanabe et al., 2018), and recently extended to the text-to-speech (TTS) task (Hayashi et al., 2020). Here, we extend ESPnet to ST tasks, providing code for building translation systems and recipes (i.e., scripts that encapsulate the entire training/inference procedure for reproducibility purposes) for a wide range of ST benchmarks. This is a non-trivial extension: with a unified codebase for ASR/MT/ST and a wide range of recipes, we believe ESPnet-ST is an all-in-one toolkit that should make it easier for both"
2020.acl-demos.34,P18-4022,0,0.0956641,"Missing"
2020.acl-demos.34,2012.eamt-1.60,0,\N,Missing
2020.amta-research.5,W17-4712,0,0.0162546,"a stream of sentences from a mixture of domains unknown to the model (Farajian et al., 2017; Huck et al., 2015). Typically using a single model, various extensions allow the system to cater more specifically to different types of source sentences. Such extensions may include data concatenation, model stacking, data selection and multi-model ensemble (Sajjad et al., 2017). One way is to pre-compute a domain label for each sentence using a dedicated classifier or model (Kobus et al., 2017; Tars and Fishel, 2018). In neural models such adaptations can also be done in the learned representations. Britz et al. (2017) use a discriminator network on top of the encoder to distinguish between domains and pretend a domain token to the target sequence. Gu et al. (2019) employ a shared encoder-decoder and also private models to capture both domain-invariant and domain-specific knowledge, which are then combined to generate the target sequence. Such approaches can also be more nuanced, such as focusing on domain-specific words, and adjusting the training objective to emphasize their importance (Zeng et al., 2018). 5 https://github.com/google-research/bert Proceedings of the 14th Conference of the Association for"
2020.amta-research.5,2012.eamt-1.60,0,0.0180951,"Specifically, we include OpenSubtitles2018 (Lison and Tiedemann, 2016) and WMT 2017 (Bojar et al., 2017), which contains data from e.g. parliamentary proceedings (Europarl, UN), political/economic news, and web-crawled parallel corpus (Common Crawl). After filtering out long sentences (>80 tokens), we obtain a training set of 28 million sentence pairs. 2. The TED task focuses on translating captions from TED Talks, which contains specialized vocabulary in various professional fields (e.g. technology, entertainment, design) in the form of monologue speeches. We use the WIT3 data distribution (Cettolo et al., 2012) with the train/dev/test splits provided by Duh (2018). 3. The WIPO task focuses on patent translation, which contains even more specialized jargon, written in a formal style. We use the COPPA V2.0 distribution (Junczys-Dowmunt et al., 2016). We held out 3000 random sentences each for dev and test, leaving 821 thousand sentences as training data. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 53 GENERAL TED WIPO ALL BLEU↑ TER↓ BLEU↑ TER↓ BLEU↑ TER↓ BLEU↑ TER↓ nmt-general smt-general 29.4 23.9 5"
2020.amta-research.5,N19-1423,0,0.0107043,"only the length feature for L IN UCB in earlier in-domain experiments (Table 2). Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 58 SCALE epsilon-greedy ucb linucb R↓ 19.3 13.3 14.1 B↑ 35.0 50.1 48.9 VARIANCE T↓ 42 38 39 R↓ 21.6 13.3 13.5 B↑ 37.5 50.7 48.9 T↓ 44 38 39 GRANULAR R↓ 19.0 13.1 13.6 B↑ 37.3 50.2 48.5 T↓ 42 38 39 SKEW R↓ 19.7 12.3 15.6 B↑ 33.1 51.0 45.2 T↓ 43 38 40 Table 4: Effects of simulated feedback method on performance. • BERT, features taken from a pre-trained language model (Devlin et al., 2019). Specifically, we ran the multilingual BERT base-size model out-of-the-box5 in inference mode and extract the final layer of the transformer encoder. These embeddings are averaged across all tokens in the sentence. Since the evaluation datasets used in this study are small, on the order of thousands of examples, L IN UCB has difficulty learning quickly when using large feature vectors. Therefore we take only the first 50 embedding dimensions as features. A constant bias feature is used to establish a baseline. Table 5 shows the results. As evident by comparing against the B IAS feature result"
2020.amta-research.5,W17-4713,0,0.0205447,"nst the B IAS feature results, all feature types provide useful information to the system selection task, though length and BERT features prove to be much more effective than vocabulary-based features. All OOV LEN BERT BIAS R↓ B↑ T↓ 13.6 47.7 40 19.2 16.8 13.3 19.0 33.0 45.9 48.1 31.8 42 41 40 42 Table 5: Ablation of contextual bandit features, on the randomly mixed-domain data. 6 Related Work Multi-domain machine translation Our problem setting is closely related to multi-domain machine translation, where the data comes as a stream of sentences from a mixture of domains unknown to the model (Farajian et al., 2017; Huck et al., 2015). Typically using a single model, various extensions allow the system to cater more specifically to different types of source sentences. Such extensions may include data concatenation, model stacking, data selection and multi-model ensemble (Sajjad et al., 2017). One way is to pre-compute a domain label for each sentence using a dedicated classifier or model (Kobus et al., 2017; Tars and Fishel, 2018). In neural models such adaptations can also be done in the learned representations. Britz et al. (2017) use a discriminator network on top of the encoder to distinguish betwee"
2020.amta-research.5,N19-1312,0,0.0180177,"extensions allow the system to cater more specifically to different types of source sentences. Such extensions may include data concatenation, model stacking, data selection and multi-model ensemble (Sajjad et al., 2017). One way is to pre-compute a domain label for each sentence using a dedicated classifier or model (Kobus et al., 2017; Tars and Fishel, 2018). In neural models such adaptations can also be done in the learned representations. Britz et al. (2017) use a discriminator network on top of the encoder to distinguish between domains and pretend a domain token to the target sequence. Gu et al. (2019) employ a shared encoder-decoder and also private models to capture both domain-invariant and domain-specific knowledge, which are then combined to generate the target sequence. Such approaches can also be more nuanced, such as focusing on domain-specific words, and adjusting the training objective to emphasize their importance (Zeng et al., 2018). 5 https://github.com/google-research/bert Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 59 A natural question is what advantage bandit system selec"
2020.amta-research.5,2020.eamt-1.50,0,0.0465131,"Missing"
2020.amta-research.5,2015.mtsummit-papers.19,0,0.0257789,"esults, all feature types provide useful information to the system selection task, though length and BERT features prove to be much more effective than vocabulary-based features. All OOV LEN BERT BIAS R↓ B↑ T↓ 13.6 47.7 40 19.2 16.8 13.3 19.0 33.0 45.9 48.1 31.8 42 41 40 42 Table 5: Ablation of contextual bandit features, on the randomly mixed-domain data. 6 Related Work Multi-domain machine translation Our problem setting is closely related to multi-domain machine translation, where the data comes as a stream of sentences from a mixture of domains unknown to the model (Farajian et al., 2017; Huck et al., 2015). Typically using a single model, various extensions allow the system to cater more specifically to different types of source sentences. Such extensions may include data concatenation, model stacking, data selection and multi-model ensemble (Sajjad et al., 2017). One way is to pre-compute a domain label for each sentence using a dedicated classifier or model (Kobus et al., 2017; Tars and Fishel, 2018). In neural models such adaptations can also be done in the learned representations. Britz et al. (2017) use a discriminator network on top of the encoder to distinguish between domains and preten"
2020.amta-research.5,kobus-etal-2017-domain,0,0.0315941,"domain machine translation Our problem setting is closely related to multi-domain machine translation, where the data comes as a stream of sentences from a mixture of domains unknown to the model (Farajian et al., 2017; Huck et al., 2015). Typically using a single model, various extensions allow the system to cater more specifically to different types of source sentences. Such extensions may include data concatenation, model stacking, data selection and multi-model ensemble (Sajjad et al., 2017). One way is to pre-compute a domain label for each sentence using a dedicated classifier or model (Kobus et al., 2017; Tars and Fishel, 2018). In neural models such adaptations can also be done in the learned representations. Britz et al. (2017) use a discriminator network on top of the encoder to distinguish between domains and pretend a domain token to the target sequence. Gu et al. (2019) employ a shared encoder-decoder and also private models to capture both domain-invariant and domain-specific knowledge, which are then combined to generate the target sequence. Such approaches can also be more nuanced, such as focusing on domain-specific words, and adjusting the training objective to emphasize their impo"
2020.amta-research.5,P07-2045,0,0.0134671,"ystems which constitute the arms of the bandit. Three architectures (nmt, nmt-cont, and smt) are trained and evaluated across three different domains. Typically NMT with continued training (nmt-cont) is the highest performing system on in-domain data, but other systems offer more consistent performance. 4.2 MT Systems The training and development data described above are used to build machine translation systems. Bandit experiments are run on the test data, which has 1982, 3000, and 5504 sentences for the TED, WIPO, and General tasks respectively. All data is tokenized by the Moses tokenizer (Koehn et al., 2007), then split into subwords by BPE (Sennrich et al., 2016) independently with 30k merge operations for the English and German sides. Neural machine translation Models are built with Sockeye (Hieber et al., 2017), using common settings for LSTM seq2seq models (Bahdanau et al., 2015): 2 layer encoder, 2 layer decoder, 512 hidden nodes, 512 word embedding sizes. Statistical machine translation Models are built with Joshua (Post et al., 2013). This represents a strong phrase-based SMT model with GIZA++ alignments, 4-gram language model, and MIRA-based discriminative tuning. Systems For each task, w"
2020.amta-research.5,P17-1138,0,0.0183689,"ch as its architecture or optimization strategy, may become more important. This is a situation the bandit approach is well-suited for. Bandits in NLP Due to the high cost of sourcing human annotations for NLP tasks, developing tractable training methods for learning from simple feedback has long been a desirable goal. Learning NLP tasks (machine translation, sequence labeling, text classification) from bandit feedback has been studied previously (Lawrence et al., 2017; Sokolov et al., 2016), and has been extended to train typical NLP architectures, such as neural sequence-to-sequence models (Kreutzer et al., 2017). Bandits have also been applied previously in MT, even as the topic of a dedicated shared task (Sokolov et al., 2017). Within the context of bandit-driven MT, the focus has been on adapting an existing system, limited to simulated bandit feedback. Sokolov et al. (2016) used actual losses (BLEU) and pairwise ranking. Nguyen et al. (2017), also used bandit learning but to adapt a single neural MT system. Our approach is significantly different, in that it focuses on the use of a bandit-trained policy for selection, rather than adaptation or in-domain training. 7 Conclusion and Future Work As MT"
2020.amta-research.5,D17-1272,0,0.0208017,"ntal setup. Imagine a specialized domain that is not similar to the training domain of any system. In these cases, other attributes of the model, such as its architecture or optimization strategy, may become more important. This is a situation the bandit approach is well-suited for. Bandits in NLP Due to the high cost of sourcing human annotations for NLP tasks, developing tractable training methods for learning from simple feedback has long been a desirable goal. Learning NLP tasks (machine translation, sequence labeling, text classification) from bandit feedback has been studied previously (Lawrence et al., 2017; Sokolov et al., 2016), and has been extended to train typical NLP architectures, such as neural sequence-to-sequence models (Kreutzer et al., 2017). Bandits have also been applied previously in MT, even as the topic of a dedicated shared task (Sokolov et al., 2017). Within the context of bandit-driven MT, the focus has been on adapting an existing system, limited to simulated bandit feedback. Sokolov et al. (2016) used actual losses (BLEU) and pairwise ranking. Nguyen et al. (2017), also used bandit learning but to adapt a single neural MT system. Our approach is significantly different, in"
2020.amta-research.5,L16-1147,0,0.0199406,"learning. 4 Experiments We aim to study the effectiveness of bandit algorithms on the task of MT system selection, across a variety of domains (and domain mixtures). Here we introduce the MT systems, the datasets used to train them, and the bandit algorithms used to learn the system selection policy. 4.1 Datasets Our experiment data consists of three different tasks, translating from German to English: 1. The General-Domain task includes data from a range of domains, and is meant to be reflective of the kind of data used in public deployed systems. Specifically, we include OpenSubtitles2018 (Lison and Tiedemann, 2016) and WMT 2017 (Bojar et al., 2017), which contains data from e.g. parliamentary proceedings (Europarl, UN), political/economic news, and web-crawled parallel corpus (Common Crawl). After filtering out long sentences (>80 tokens), we obtain a training set of 28 million sentence pairs. 2. The TED task focuses on translating captions from TED Talks, which contains specialized vocabulary in various professional fields (e.g. technology, entertainment, design) in the form of monologue speeches. We use the WIT3 data distribution (Cettolo et al., 2012) with the train/dev/test splits provided by Duh (2"
2020.amta-research.5,2015.iwslt-evaluation.11,0,0.0298651,"m language model, and MIRA-based discriminative tuning. Systems For each task, we train SMT and NMT models from scratch using only the training data in the respective domains, resulting in 6 models: {nmt,smt}-{general,ted,wipo}. Additionally we include two improved NMT models for WIPO and TED (nmt-cont-{ted,wipo}), which starts with nmt-general as initializaton and fine-tunes on WIPO or TED training data. This continued training process usually achieves strong translation performance in the target domain, but shows increased risk of catastrophic forgetting in the original general domain task (Luong and Manning, 2015; Thompson et al., 2019). This brings the total number of systems (also the number of bandit arms, K) to 8. The baseline performance of each system on the test sets in each domain is shown in Table 1. Metrics The bandit learning agents are updated on-the-fly on the aforementioned test sets, using perturbed/granularized sentence-level BLEU as feedback. However, for final evaluation we collect all the resulting translations and compute corpus-level BLEU (Papineni et al., 2002) (implemented via SacreBLEU (Post, 2018)) and TER (Snover et al., 2006). We experiment with different ways to mix the tes"
2020.amta-research.5,D17-1153,0,0.0396886,"Missing"
2020.amta-research.5,W18-6319,0,0.0130687,"risk of catastrophic forgetting in the original general domain task (Luong and Manning, 2015; Thompson et al., 2019). This brings the total number of systems (also the number of bandit arms, K) to 8. The baseline performance of each system on the test sets in each domain is shown in Table 1. Metrics The bandit learning agents are updated on-the-fly on the aforementioned test sets, using perturbed/granularized sentence-level BLEU as feedback. However, for final evaluation we collect all the resulting translations and compute corpus-level BLEU (Papineni et al., 2002) (implemented via SacreBLEU (Post, 2018)) and TER (Snover et al., 2006). We experiment with different ways to mix the test sets to illustrate different scenarios for bandit learning. For error analysis, we computed regret as in Eq. 1. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 54 GENERAL R↓ B↑ T↓ TED R↓ B↑ WIPO T↓ R↓ B↑ AVG T↓ R↓ B↑ T↓ random 16.9 17.9 70.9 20.8 24.7 59.3 32.2 36.7 52.4 23.3 26.4 60.9 best-arm-oracle 5.9 29.4 50.5 6.2 39.3 38.2 4.5 62.3 25.0 5.5 43.7 37.9 oracle 2.2 31.6 49.9 1.8 42.1 37.2 2.3 61.7 24.1 2.1 45.1"
2020.amta-research.5,W13-2226,0,0.0332126,"n on the test data, which has 1982, 3000, and 5504 sentences for the TED, WIPO, and General tasks respectively. All data is tokenized by the Moses tokenizer (Koehn et al., 2007), then split into subwords by BPE (Sennrich et al., 2016) independently with 30k merge operations for the English and German sides. Neural machine translation Models are built with Sockeye (Hieber et al., 2017), using common settings for LSTM seq2seq models (Bahdanau et al., 2015): 2 layer encoder, 2 layer decoder, 512 hidden nodes, 512 word embedding sizes. Statistical machine translation Models are built with Joshua (Post et al., 2013). This represents a strong phrase-based SMT model with GIZA++ alignments, 4-gram language model, and MIRA-based discriminative tuning. Systems For each task, we train SMT and NMT models from scratch using only the training data in the respective domains, resulting in 6 models: {nmt,smt}-{general,ted,wipo}. Additionally we include two improved NMT models for WIPO and TED (nmt-cont-{ted,wipo}), which starts with nmt-general as initializaton and fine-tunes on WIPO or TED training data. This continued training process usually achieves strong translation performance in the target domain, but shows"
2020.amta-research.5,E17-2045,0,0.0424335,".8 42 41 40 42 Table 5: Ablation of contextual bandit features, on the randomly mixed-domain data. 6 Related Work Multi-domain machine translation Our problem setting is closely related to multi-domain machine translation, where the data comes as a stream of sentences from a mixture of domains unknown to the model (Farajian et al., 2017; Huck et al., 2015). Typically using a single model, various extensions allow the system to cater more specifically to different types of source sentences. Such extensions may include data concatenation, model stacking, data selection and multi-model ensemble (Sajjad et al., 2017). One way is to pre-compute a domain label for each sentence using a dedicated classifier or model (Kobus et al., 2017; Tars and Fishel, 2018). In neural models such adaptations can also be done in the learned representations. Britz et al. (2017) use a discriminator network on top of the encoder to distinguish between domains and pretend a domain token to the target sequence. Gu et al. (2019) employ a shared encoder-decoder and also private models to capture both domain-invariant and domain-specific knowledge, which are then combined to generate the target sequence. Such approaches can also be"
2020.amta-research.5,P16-1162,0,0.0266981,"architectures (nmt, nmt-cont, and smt) are trained and evaluated across three different domains. Typically NMT with continued training (nmt-cont) is the highest performing system on in-domain data, but other systems offer more consistent performance. 4.2 MT Systems The training and development data described above are used to build machine translation systems. Bandit experiments are run on the test data, which has 1982, 3000, and 5504 sentences for the TED, WIPO, and General tasks respectively. All data is tokenized by the Moses tokenizer (Koehn et al., 2007), then split into subwords by BPE (Sennrich et al., 2016) independently with 30k merge operations for the English and German sides. Neural machine translation Models are built with Sockeye (Hieber et al., 2017), using common settings for LSTM seq2seq models (Bahdanau et al., 2015): 2 layer encoder, 2 layer decoder, 512 hidden nodes, 512 word embedding sizes. Statistical machine translation Models are built with Joshua (Post et al., 2013). This represents a strong phrase-based SMT model with GIZA++ alignments, 4-gram language model, and MIRA-based discriminative tuning. Systems For each task, we train SMT and NMT models from scratch using only the tr"
2020.amta-research.5,2006.amta-papers.25,0,0.0804747,"orgetting in the original general domain task (Luong and Manning, 2015; Thompson et al., 2019). This brings the total number of systems (also the number of bandit arms, K) to 8. The baseline performance of each system on the test sets in each domain is shown in Table 1. Metrics The bandit learning agents are updated on-the-fly on the aforementioned test sets, using perturbed/granularized sentence-level BLEU as feedback. However, for final evaluation we collect all the resulting translations and compute corpus-level BLEU (Papineni et al., 2002) (implemented via SacreBLEU (Post, 2018)) and TER (Snover et al., 2006). We experiment with different ways to mix the test sets to illustrate different scenarios for bandit learning. For error analysis, we computed regret as in Eq. 1. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 54 GENERAL R↓ B↑ T↓ TED R↓ B↑ WIPO T↓ R↓ B↑ AVG T↓ R↓ B↑ T↓ random 16.9 17.9 70.9 20.8 24.7 59.3 32.2 36.7 52.4 23.3 26.4 60.9 best-arm-oracle 5.9 29.4 50.5 6.2 39.3 38.2 4.5 62.3 25.0 5.5 43.7 37.9 oracle 2.2 31.6 49.9 1.8 42.1 37.2 2.3 61.7 24.1 2.1 45.1 37.0 epsilon-greedy ucb linucb"
2020.amta-research.5,P16-1152,0,0.0130159,"pecialized domain that is not similar to the training domain of any system. In these cases, other attributes of the model, such as its architecture or optimization strategy, may become more important. This is a situation the bandit approach is well-suited for. Bandits in NLP Due to the high cost of sourcing human annotations for NLP tasks, developing tractable training methods for learning from simple feedback has long been a desirable goal. Learning NLP tasks (machine translation, sequence labeling, text classification) from bandit feedback has been studied previously (Lawrence et al., 2017; Sokolov et al., 2016), and has been extended to train typical NLP architectures, such as neural sequence-to-sequence models (Kreutzer et al., 2017). Bandits have also been applied previously in MT, even as the topic of a dedicated shared task (Sokolov et al., 2017). Within the context of bandit-driven MT, the focus has been on adapting an existing system, limited to simulated bandit feedback. Sokolov et al. (2016) used actual losses (BLEU) and pairwise ranking. Nguyen et al. (2017), also used bandit learning but to adapt a single neural MT system. Our approach is significantly different, in that it focuses on the"
2020.amta-research.5,W17-4756,0,0.336413,"Missing"
2020.amta-research.5,N19-1209,1,0.823526,"A-based discriminative tuning. Systems For each task, we train SMT and NMT models from scratch using only the training data in the respective domains, resulting in 6 models: {nmt,smt}-{general,ted,wipo}. Additionally we include two improved NMT models for WIPO and TED (nmt-cont-{ted,wipo}), which starts with nmt-general as initializaton and fine-tunes on WIPO or TED training data. This continued training process usually achieves strong translation performance in the target domain, but shows increased risk of catastrophic forgetting in the original general domain task (Luong and Manning, 2015; Thompson et al., 2019). This brings the total number of systems (also the number of bandit arms, K) to 8. The baseline performance of each system on the test sets in each domain is shown in Table 1. Metrics The bandit learning agents are updated on-the-fly on the aforementioned test sets, using perturbed/granularized sentence-level BLEU as feedback. However, for final evaluation we collect all the resulting translations and compute corpus-level BLEU (Papineni et al., 2002) (implemented via SacreBLEU (Post, 2018)) and TER (Snover et al., 2006). We experiment with different ways to mix the test sets to illustrate dif"
2020.amta-research.5,D18-1041,0,0.0164751,"nd Fishel, 2018). In neural models such adaptations can also be done in the learned representations. Britz et al. (2017) use a discriminator network on top of the encoder to distinguish between domains and pretend a domain token to the target sequence. Gu et al. (2019) employ a shared encoder-decoder and also private models to capture both domain-invariant and domain-specific knowledge, which are then combined to generate the target sequence. Such approaches can also be more nuanced, such as focusing on domain-specific words, and adjusting the training objective to emphasize their importance (Zeng et al., 2018). 5 https://github.com/google-research/bert Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 59 A natural question is what advantage bandit system selection has over the alternative strategy of using of a domain classifier to determine which system to use. One advantage of bandit selection is that it provides a unifying framework for online learning all aspects of problem. If a domain classifier is useful, the classifier’s predictions can be used as a feature in the bandit algorithm, and the exte"
2020.amta-research.5,W17-4717,0,\N,Missing
2020.amta-research.5,P18-2050,0,\N,Missing
2020.amta-research.5,W18-2708,0,\N,Missing
2020.emnlp-main.340,1999.mtsummit-1.31,0,0.143794,"Missing"
2020.emnlp-main.340,N19-1423,0,0.0190525,"en optimize the parameters with pairwise hinge loss and Adam optimizer. We trained all models for 20 epochs and sampled around 1,000 training pairs for each epoch. At inference time, we rerank documents based on the output scores from the BERT ranker model. Figure 6: Neural architecture of our baseline CLIR model. Modules in the dotted rectangle share weights. Baseline neural CLIR model We follow the implementation of the vanilla BERT ranker model (MacAvaney et al., 2019), which obtained strong results in monolingual IR. As shown in Figure 6, the model encodes a query-document pair with BERT (Devlin et al., 2019) and stacks a linear combination layer on top of the [CLS] token. We extended the ranker model to use multilingual BERT11 . At training time, we sample documents pairs in which the positive documents have higher relevance judgment labels than the negative documents. For each document pair, we obtain scores for both docu11 Evaluation metric We report all results in NDCG (normalized discounted cumulative gain), an IR metric that measures the usefulness of documents based on their ranks in the search results (J¨arvelin and Kek¨al¨ainen, 2002). Following a common practice from the IR community, we"
2020.emnlp-main.340,2020.clssts-1.5,0,0.0176861,"s and documents to the same language, then employ a monolingual information retrieval (IR) engine to find relevant documents. Recently, the research community has been actively looking at end-to-end solutions that tackle the CLIR task without the need to build MT systems. This line of work builds upon recent advances in Neural Information Retrieval in the monolingual setting, c.f. (Mitra and Craswell, 2018; Craswell et al., 2020). There are proposals to directly train end-to-end neural retrieval models on CLIR datasets (Sasaki et al., 2018; Zhang et al., 2019) or MT bitext (Zbib et al., 2019; Jiang et al., 2020). One can also exploit cross-lingual word embeddings to train a CLIR model on disjoint monolingual corpora (Litschko et al., 2018). Despite the growing interest in end-to-end CLIR, the lack of a large-scale, easily-accessible CLIR dataset covering many language directions in high-, mid- and low-resource settings has detrimentally affected the CLIR community’s capability to replicate and compare with previously published work. For example, among the widely-used datasets, the CLEF collection (Ferro and Silvello, 2015) covers many languages but is not large enough for training neural models. The"
2020.emnlp-main.340,P99-1027,0,0.629424,"aluate MULTI-8 in both singlelanguage retrieval and mix-language retrieval settings. 1 Introduction Cross-Lingual Information Retrieval (CLIR) is a retrieval task in which search queries and candidate documents are written in different languages. CLIR can be very useful in some scenarios. For example, a reporter may want to search foreignlanguage news to obtain different perspectives for her story; an inventor may explore the patents in another country to understand prior art. Traditionally, translation-based approaches are commonly used to tackle the CLIR task (Zhou et al., 2012; Oard, 1998; McCarley, 1999): the query translation approach translates the query into the same language of the documents, whereas the document translation approach translates the document into the same language as the query. Both approaches rely on a machine translation (MT) system or bilingual dictionary to map queries and documents to the same language, then employ a monolingual information retrieval (IR) engine to find relevant documents. Recently, the research community has been actively looking at end-to-end solutions that tackle the CLIR task without the need to build MT systems. This line of work builds upon rece"
2020.emnlp-main.340,D18-1211,0,0.0284549,"arch is an open-source, highly optimized search engine software based on Apache Lucene8 . It has built-in analyzers that handle language-specific preprocessing such as tokenization and stemming. By default, Elasticsearch implements the BM25 weighting scheme (Robertson et al., 2009), a bag-of-word retrieval function that calculates similarity scores between queries and documents based on term frequencies and inverse document frequencies. BM25 is a strong baseline that frequently outperforms existing neural IR models on multiple benchmark IR datasets (Chapelle and Chang, 2011; Guo et al., 2016; McDonald et al., 2018). We used Elasticsearch 6.5.4 and imported the same settings as the official search indices from Wikipedia9 . For every query, we configured Elasticsearch to search both document titles and document bodies, with twice the weight given to document titles. We limit Elasticsearch to return only the top 100 documents for each query and assume documents not returned by the search engine are irrelevant. We parallelized the retrieval processes by running multiple Elasticsearch instances on numerous servers and dedicated one Elasticsearch instance to every language. Discrete relevance judgment labels"
2020.emnlp-main.340,oard-1998-comparative,0,0.685218,"I139, and evaluate MULTI-8 in both singlelanguage retrieval and mix-language retrieval settings. 1 Introduction Cross-Lingual Information Retrieval (CLIR) is a retrieval task in which search queries and candidate documents are written in different languages. CLIR can be very useful in some scenarios. For example, a reporter may want to search foreignlanguage news to obtain different perspectives for her story; an inventor may explore the patents in another country to understand prior art. Traditionally, translation-based approaches are commonly used to tackle the CLIR task (Zhou et al., 2012; Oard, 1998; McCarley, 1999): the query translation approach translates the query into the same language of the documents, whereas the document translation approach translates the document into the same language as the query. Both approaches rely on a machine translation (MT) system or bilingual dictionary to map queries and documents to the same language, then employ a monolingual information retrieval (IR) engine to find relevant documents. Recently, the research community has been actively looking at end-to-end solutions that tackle the CLIR task without the need to build MT systems. This line of work"
2020.emnlp-main.340,N18-2073,1,0.887758,"Missing"
2020.emnlp-main.340,P14-2080,0,0.13492,"titles as search queries for two reasons: (1) They are readily available in large amounts for each of the 139 languages, which enables us to build large datasets (i.e., I is large). (2) In certain real-world search settings, queries are typically short, spanning only two to three tokens (Belkin et al., 2003) and informational, covering a wide variety of topics (Jansen et al., 2008). We leave the investigation of complex queries to future work. We want to emphasize that our mining pipeline is compatible with all query types; for example, we can use the first sentences of documents as queries (Schamoni et al., 2014; Sasaki et al., 2018) if desired. 5 Note that documents in different languages do not share document IDs. This means that document N in language X does not refer to the same entity as document N in language Y. 6 We acknowledge that there are potentially missing interlanguage links in Wikidata. This implies that our method may miss the labeling of some relevant documents. Wikidata has several policies to improve its data quality, such as requests for editors to link new Wikipedia articles to entities in Wikidata. There are also automated auditing tools that periodically identify articles with"
2020.emnlp-main.340,P19-1306,0,0.0627045,"ion (MT) system or bilingual dictionary to map queries and documents to the same language, then employ a monolingual information retrieval (IR) engine to find relevant documents. Recently, the research community has been actively looking at end-to-end solutions that tackle the CLIR task without the need to build MT systems. This line of work builds upon recent advances in Neural Information Retrieval in the monolingual setting, c.f. (Mitra and Craswell, 2018; Craswell et al., 2020). There are proposals to directly train end-to-end neural retrieval models on CLIR datasets (Sasaki et al., 2018; Zhang et al., 2019) or MT bitext (Zbib et al., 2019; Jiang et al., 2020). One can also exploit cross-lingual word embeddings to train a CLIR model on disjoint monolingual corpora (Litschko et al., 2018). Despite the growing interest in end-to-end CLIR, the lack of a large-scale, easily-accessible CLIR dataset covering many language directions in high-, mid- and low-resource settings has detrimentally affected the CLIR community’s capability to replicate and compare with previously published work. For example, among the widely-used datasets, the CLEF collection (Ferro and Silvello, 2015) covers many languages but"
2020.emnlp-main.340,C12-1164,0,0.15295,"model results on BI139, and evaluate MULTI-8 in both singlelanguage retrieval and mix-language retrieval settings. 1 Introduction Cross-Lingual Information Retrieval (CLIR) is a retrieval task in which search queries and candidate documents are written in different languages. CLIR can be very useful in some scenarios. For example, a reporter may want to search foreignlanguage news to obtain different perspectives for her story; an inventor may explore the patents in another country to understand prior art. Traditionally, translation-based approaches are commonly used to tackle the CLIR task (Zhou et al., 2012; Oard, 1998; McCarley, 1999): the query translation approach translates the query into the same language of the documents, whereas the document translation approach translates the document into the same language as the query. Both approaches rely on a machine translation (MT) system or bilingual dictionary to map queries and documents to the same language, then employ a monolingual information retrieval (IR) engine to find relevant documents. Recently, the research community has been actively looking at end-to-end solutions that tackle the CLIR task without the need to build MT systems. This"
2020.emnlp-main.340,W19-6602,1,0.825536,"e-art performances on benchmark datasets by incorporating BERT’s context vectors into existing baseline neural IR models (McDonald et al., 2018). Training on synthetic is also a common practice, e.g., Dehghani et al. (2017) show that supervised neural ranking models can greatly benefit from pre-training on BM25 labels. Cross-lingual Information Retrieval (CLIR) is a sub-field of IR that is becoming increasingly important as new documents in different languages are being generated every day. The field has progressed from translation-based methods (Zhou et al., 2012; Oard, 1998; McCarley, 1999; Yarmohammadi et al., 2019) to recent neural CLIR models (Vuli´c and Moens, 2015; Litschko et al., 2018; Zhang et al., 2019) that rely on cross-lingual word embeddings. In contrast to the wide availability of monolingual IR datasets (Voorhees, 2005; Craswell et al., 2020), cross-lingual and multilingual IR 4167 Dataset #Lang Manual? Multilingual? #query #document #triplets (CLEF 2000-2003) (MATERIAL, 2017) (Schamoni et al., 2014) (Sasaki et al., 2018) 10 7 2 25 yes yes no no yes no no no 2.2K 11.5K 245K 10.9M 1.1M 90K 1.2M 23.9M 33K ∼20K 3.2M 40.1M CLIRMatrix BI-139 raw CLIRMatrix BI-139 base CLIRMatrix MULTI-8 139 139"
2020.lrec-1.325,P19-1310,0,0.0559617,"Missing"
2020.lrec-1.325,1983.tc-1.13,0,0.560974,"Missing"
2020.lrec-1.325,P19-1019,0,0.0163,"neural model (Arivazhagan et al., 2019; Johnson et al., 2017; Gu et low-resource languages; for future work, it will be promising to include this in our analysis of found bitext in Table 3. 2671 al., 2018; Wang et al., 2019b) or design stage-wise transfer learning mechanisms (Zoph et al., 2016; Dabre et al., 2019; Kocmi and Bojar, 2018). MT performance may also be improved by using domain adaptation techniques (Thompson et al., 2019a) to deal with the problem of training on heterogeneous resource types and generalizing to new domains. Recent interest in unsupervised machine translation (c.f. (Artetxe et al., 2019)) promises to reduce the requirement for bitext, training only on monolingual data. Finally, general modeling improvements in NMT architectures can also help (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). For example, results in other settings suggest that paraphrasing the English side of a bitext is a promising approach when translating into English in low-resource settings (Hu et al., 2019). MT for African Languages We focused on Somali and Swahili in this paper. Other African languages have been explored in the context of both SMT and NMT. Hausa, a Chadic (Afro"
2020.lrec-1.325,N19-1006,0,0.0114413,"ave been explored in the context of both SMT and NMT. Hausa, a Chadic (Afroasiatic) language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we believe it is worth continuing to push the frontier of discovering and curating exploitable bitext for low-resource languages. Acknowledgments We thank Dr. Carl Rubino and his team at the IARPA MATERIAL for providing this data. We also thank Philipp Koehn for help with the Paracrawl data. Appendix: Example Translations Example outputs of our SMT and NMT systems (under the baseline + dictionary + found-bi"
2020.lrec-1.325,W16-4507,0,0.01355,"nts in NMT architectures can also help (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). For example, results in other settings suggest that paraphrasing the English side of a bitext is a promising approach when translating into English in low-resource settings (Hu et al., 2019). MT for African Languages We focused on Somali and Swahili in this paper. Other African languages have been explored in the context of both SMT and NMT. Hausa, a Chadic (Afroasiatic) language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we beli"
2020.lrec-1.325,D19-1146,0,0.0816199,"arios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of ge"
2020.lrec-1.325,P17-2090,0,0.0854973,"with example values. 1. We find that SMT and NMT perform similarly in these scenarios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical"
2020.lrec-1.325,L18-1531,0,0.0228742,"language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we believe it is worth continuing to push the frontier of discovering and curating exploitable bitext for low-resource languages. Acknowledgments We thank Dr. Carl Rubino and his team at the IARPA MATERIAL for providing this data. We also thank Philipp Koehn for help with the Paracrawl data. Appendix: Example Translations Example outputs of our SMT and NMT systems (under the baseline + dictionary + found-bitext + paracrawl condition) are shown here for the Text and Transcripts test sets. We"
2020.lrec-1.325,N18-1032,0,0.0204356,"rly in these scenarios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set"
2020.lrec-1.325,E17-3017,0,0.0233731,"the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolkit (Post et al., 2013) and NMT systems using the AWS Sockeye toolkit (Hieber et al., 2017).3 Our Joshua system is a phrase-based model that represents the state of the art in SMT, with 4-gram KenLM language model and MIRA-based tuning. Our Sockeye system is a transformer model (Vaswani et al., 2017), which is among the strongest performers in the field of NMT. We vary the following hyperparameters: • Transformer Architecture: number of layers (1, 2, 4, 6); embedding size (256, 512, 1024), number of hidden units in each layer (1024, 2048), number of heads in self-attention (8, 16). • Preprocessing: number of Byte Pair Encoding (BPE) operations (1k, 2k, 4k, 8k, 16k, 32k) • Training c"
2020.lrec-1.325,N19-1090,1,0.893793,"Missing"
2020.lrec-1.325,Q17-1024,0,0.0773216,"Missing"
2020.lrec-1.325,W18-6325,0,0.100594,"tantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of genres, but is most simila"
2020.lrec-1.325,W17-3204,0,0.201334,". Introduction Cross-over point Commercial SMT and NMT systems are often trained on millions to tens of millions of sentence pairs, if not more (Wu et al., 2016). It is unclear how these systems perform when the training data contains significantly fewer sentence pairs. For many languages in the world, and in particular for languages in the African continent, at present we cannot reasonably expect such a large amount of training data. While there is no established convention, we might consider systems that are trained on less than 100 thousand sentence pairs to be low-resource. Previous work (Koehn and Knowles, 2017; Sennrich and Zhang, 2019) has established the idea that there is a crossover point between NMT and SMT performance depending on the amount of training data. See Figure 1. The intuition is that NMT is data-hungry, so may perform worse than SMT in low-resource settings, but begins to excel when there is sufficient training data. With recent advances in NMT, the cross-over point has gradually decreased. Nevertheless, in general it is difficult to predict a priori whether we are on the left or right side of the cross-over point until we actually build the systems. In this work, we perform a deta"
2020.lrec-1.325,W19-5404,0,0.0432609,"Missing"
2020.lrec-1.325,D19-5625,0,0.0708507,"can also help (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). For example, results in other settings suggest that paraphrasing the English side of a bitext is a promising approach when translating into English in low-resource settings (Hu et al., 2019). MT for African Languages We focused on Somali and Swahili in this paper. Other African languages have been explored in the context of both SMT and NMT. Hausa, a Chadic (Afroasiatic) language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we believe it is worth continu"
2020.lrec-1.325,D18-1103,0,0.0953219,"onal data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of genres, but is most similar to Validation and Test1. In the following, we first describe our low-resource condition for Swahili and Somali (Section"
2020.lrec-1.325,N18-1031,0,0.0739693,"nd NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of genres, but is most similar to Validation and Test1. In the following, we first describe our low-resource condition for Swahi"
2020.lrec-1.325,P02-1040,0,0.10688,"data is uniformly preprocessed with the same Joshua tokenizer and then lower-cased. For NMT, we additionally segment words into subwords via Byte Pair Encoding (Sennrich et al., 2016). For a fair evaluation, all translation outputs are mapped backed to the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolkit (Post et al., 2013) and NMT systems using the AWS Sockeye toolkit (Hieber et al., 2017).3 Our Joshua system is a phrase-based model that represents the state of the art in SMT, with 4-gram KenLM language model and MIRA-based tuning. Our Sockeye system is a transformer model (Vaswani et al., 2017), which is among the strongest performers in the field of NMT. We vary the following hyperparameters: • Transformer Architecture: number of"
2020.lrec-1.325,W13-2226,1,0.713881,"r a fair evaluation, all translation outputs are mapped backed to the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolkit (Post et al., 2013) and NMT systems using the AWS Sockeye toolkit (Hieber et al., 2017).3 Our Joshua system is a phrase-based model that represents the state of the art in SMT, with 4-gram KenLM language model and MIRA-based tuning. Our Sockeye system is a transformer model (Vaswani et al., 2017), which is among the strongest performers in the field of NMT. We vary the following hyperparameters: • Transformer Architecture: number of layers (1, 2, 4, 6); embedding size (256, 512, 1024), number of hidden units in each layer (1024, 2048), number of heads in self-attention (8, 16). • Preprocessing: number of Byte Pa"
2020.lrec-1.325,W18-6319,1,0.854147,"Missing"
2020.lrec-1.325,N18-4016,0,0.0160834,"ters, since neural models are sensitive in low-resource conditions. While hyperparameter tuning can be expensive, it can be feasible when training data is scarce, which is exactly our low-resource scenario. 4. Exploiting Additional Data There are two main research directions for solving lowresource problems: (a) develop new modeling techniques that require less data, and (b) devise ways to exploit additional opportunistic data sources. In this work we first focus on the latter. We explore three types of resources: 1. Dictionary: Pre-existing dictionaries may be available from various sources (Ramesh and Sankaranarayanan, 2018; Thompson et al., 2019b). We define dictionaries as word-by-word or phrase-by-phrase translations, which are different in format from the sentence-bysentence parallel data in that there may be less contex2. Found Bitext: Pre-existing parallel sentences may be found via various sources (Tiedemann, 2012; Christodouloupoulos and Steedman, 2015), such as the Bible. These are relatively clean datasets that contain useful sentence-by-sentence translations, but may be in a different domain/genre from our baseline training set and testset.5 3. Mined Bitext: Parallel sentences can be mined by crawling"
2020.lrec-1.325,W18-1902,0,0.0249852,"st overfitting). For both SMT and NMT, the data is uniformly preprocessed with the same Joshua tokenizer and then lower-cased. For NMT, we additionally segment words into subwords via Byte Pair Encoding (Sennrich et al., 2016). For a fair evaluation, all translation outputs are mapped backed to the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolkit (Post et al., 2013) and NMT systems using the AWS Sockeye toolkit (Hieber et al., 2017).3 Our Joshua system is a phrase-based model that represents the state of the art in SMT, with 4-gram KenLM language model and MIRA-based tuning. Our Sockeye system is a transformer model (Vaswani et al., 2017), which is among the strongest performers in the field of NMT. We vary the following hy"
2020.lrec-1.325,P19-1021,0,0.291128,"point Commercial SMT and NMT systems are often trained on millions to tens of millions of sentence pairs, if not more (Wu et al., 2016). It is unclear how these systems perform when the training data contains significantly fewer sentence pairs. For many languages in the world, and in particular for languages in the African continent, at present we cannot reasonably expect such a large amount of training data. While there is no established convention, we might consider systems that are trained on less than 100 thousand sentence pairs to be low-resource. Previous work (Koehn and Knowles, 2017; Sennrich and Zhang, 2019) has established the idea that there is a crossover point between NMT and SMT performance depending on the amount of training data. See Figure 1. The intuition is that NMT is data-hungry, so may perform worse than SMT in low-resource settings, but begins to excel when there is sufficient training data. With recent advances in NMT, the cross-over point has gradually decreased. Nevertheless, in general it is difficult to predict a priori whether we are on the left or right side of the cross-over point until we actually build the systems. In this work, we perform a detailed evaluation of lowresou"
2020.lrec-1.325,P16-1162,0,0.0714187,"ripts, and evaluates how our systems tuned on text might perform with speech data. The validation set is used for MIRA tuning in SMT (for finding weights that tradeoff e.g., language model, translation model, and length penalty), and for early-stopping in NMT (for stopping the training run when perplexity fails to improve after a several consecutive checkpoint updates, which is effective against overfitting). For both SMT and NMT, the data is uniformly preprocessed with the same Joshua tokenizer and then lower-cased. For NMT, we additionally segment words into subwords via Byte Pair Encoding (Sennrich et al., 2016). For a fair evaluation, all translation outputs are mapped backed to the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolk"
2020.lrec-1.325,N19-1209,1,0.89183,"tive in low-resource conditions. While hyperparameter tuning can be expensive, it can be feasible when training data is scarce, which is exactly our low-resource scenario. 4. Exploiting Additional Data There are two main research directions for solving lowresource problems: (a) develop new modeling techniques that require less data, and (b) devise ways to exploit additional opportunistic data sources. In this work we first focus on the latter. We explore three types of resources: 1. Dictionary: Pre-existing dictionaries may be available from various sources (Ramesh and Sankaranarayanan, 2018; Thompson et al., 2019b). We define dictionaries as word-by-word or phrase-by-phrase translations, which are different in format from the sentence-bysentence parallel data in that there may be less contex2. Found Bitext: Pre-existing parallel sentences may be found via various sources (Tiedemann, 2012; Christodouloupoulos and Steedman, 2015), such as the Bible. These are relatively clean datasets that contain useful sentence-by-sentence translations, but may be in a different domain/genre from our baseline training set and testset.5 3. Mined Bitext: Parallel sentences can be mined by crawling the web, for example v"
2020.lrec-1.325,D19-1142,1,0.914182,"tive in low-resource conditions. While hyperparameter tuning can be expensive, it can be feasible when training data is scarce, which is exactly our low-resource scenario. 4. Exploiting Additional Data There are two main research directions for solving lowresource problems: (a) develop new modeling techniques that require less data, and (b) devise ways to exploit additional opportunistic data sources. In this work we first focus on the latter. We explore three types of resources: 1. Dictionary: Pre-existing dictionaries may be available from various sources (Ramesh and Sankaranarayanan, 2018; Thompson et al., 2019b). We define dictionaries as word-by-word or phrase-by-phrase translations, which are different in format from the sentence-bysentence parallel data in that there may be less contex2. Found Bitext: Pre-existing parallel sentences may be found via various sources (Tiedemann, 2012; Christodouloupoulos and Steedman, 2015), such as the Bible. These are relatively clean datasets that contain useful sentence-by-sentence translations, but may be in a different domain/genre from our baseline training set and testset.5 3. Mined Bitext: Parallel sentences can be mined by crawling the web, for example v"
2020.lrec-1.325,tiedemann-2012-parallel,0,0.353587,"new modeling techniques that require less data, and (b) devise ways to exploit additional opportunistic data sources. In this work we first focus on the latter. We explore three types of resources: 1. Dictionary: Pre-existing dictionaries may be available from various sources (Ramesh and Sankaranarayanan, 2018; Thompson et al., 2019b). We define dictionaries as word-by-word or phrase-by-phrase translations, which are different in format from the sentence-bysentence parallel data in that there may be less contex2. Found Bitext: Pre-existing parallel sentences may be found via various sources (Tiedemann, 2012; Christodouloupoulos and Steedman, 2015), such as the Bible. These are relatively clean datasets that contain useful sentence-by-sentence translations, but may be in a different domain/genre from our baseline training set and testset.5 3. Mined Bitext: Parallel sentences can be mined by crawling the web, for example via Paracrawl6 . We exploit the fact that various websites exist in multiple languages and devise methods to discover and extract these parallel sentences. Depending on the languagepair, large paracrawl corpora may be possible. The challenge with using this crawled data is that it"
2020.lrec-1.325,P15-2021,0,0.0146504,"the English side of a bitext is a promising approach when translating into English in low-resource settings (Hu et al., 2019). MT for African Languages We focused on Somali and Swahili in this paper. Other African languages have been explored in the context of both SMT and NMT. Hausa, a Chadic (Afroasiatic) language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we believe it is worth continuing to push the frontier of discovering and curating exploitable bitext for low-resource languages. Acknowledgments We thank Dr. Carl Rubino and his team at the"
2020.lrec-1.325,D19-1073,0,0.0938521,"g, 2019) for figures with example values. 1. We find that SMT and NMT perform similarly in these scenarios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of"
2020.lrec-1.325,D19-5618,0,0.109279,"e. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of genres, but is most similar to Validation and Test1. In the following, we first describe our low-res"
2020.lrec-1.325,D16-1163,0,0.103973,"NMT perform similarly in these scenarios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony."
2020.ngt-1.12,W17-4712,0,0.0574895,"less expensive way. 6 Table 9: Best configurations for each setting. Scores within 0.1 BLEU of the best are also listed. Configuration 9 generally performs best, while configuration 6 is best for those medium-sized models which were not improved by distillation in the general-domain. with the results from Table 3 which shows adaptation does not improve teachers, either. We suspect this is because the German-English WIPO dataset is the biggest out of any in-domain dataset, making adaptation unnecessary. Future work might also benefit from a quantification of domain similarity between datasets (Britz et al., 2017), which would guide the use of domain adaptation in cases like these. 5.4 In-Domain 2-4 days 2-4 days 1-2 days 1-24 hrs 1-2 days Table 10: Estimates of the computation time required for training randomly initialized models on just generaldomain data or just in-domain data. We also show the time required for adapting general-domain models and distilling data using teachers. Table 8: Development scores for models initialized from a model trained on general-domain data. The indomain data is pre-processed with a teacher adapted from the general-domain (config 6). Domain Gen-Domain 2-4 days 2-4 day"
2020.ngt-1.12,W17-3205,0,0.0191055,"Adapted Teacher 1 2 3 GD Baseline 4 5 6 GD Student 7 8 9 Figure 1: There are 9 possible configurations for training small, in-domain models with knowledge distillation and domain adaptation. Models trained on general-domain data are shown on the left, and in-domain models are shown on the right. Solid arrows represent domain adaptation via continued training. Dashed arrows represent improved optimization via sequence-level knowledge distillation. Configuration 1 is the model which is trained on in-domain data with random initializations and without the assistance of a teacher. cost weighting (Chen et al., 2017)), continued training is most common and the focus of this paper. Knowledge Distillation is a method for improving the performance of under-parameterized “Student” models by exploiting the probability distribution of a more computationally complex “Teacher” network. Kim and Rush (2016) presented an extension of knowledge distillation to machine translation in two flavors: word-level and sequence-level knowledge distillation. Sequence-level knowledge distillation, which is more general, involves three steps: 1. A large Teacher network is randomly initialized and trained until convergence on the"
2020.ngt-1.12,P17-2061,0,0.0657846,"Manning, 2015; Zoph et al., 2016), which involves two steps: 1. A model is randomly initialized and trained until convergence on the general-domain data. 2. A new model is initialized with the parameters resulting from Step 1 and trained until convergence on the in-domain dataset. We can consider domain adaptation as extracting a useful inductive-bias from the general-domain dataset, which is encoded and passed along to the in-domain model as a favorable weight initialization. While there are other methods of extracting inductive bias from general-domain datasets (including mixed fine-tuning (Chu et al., 2017) and 110 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 110–118 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d GD Teacher In-Domain Baseline Teacher Adapted Teacher 1 2 3 GD Baseline 4 5 6 GD Student 7 8 9 Figure 1: There are 9 possible configurations for training small, in-domain models with knowledge distillation and domain adaptation. Models trained on general-domain data are shown on the left, and in-domain models are shown on the right. Solid arrows represent domain adaptation via continue"
2020.ngt-1.12,D16-1139,0,0.0732788,"the right. Solid arrows represent domain adaptation via continued training. Dashed arrows represent improved optimization via sequence-level knowledge distillation. Configuration 1 is the model which is trained on in-domain data with random initializations and without the assistance of a teacher. cost weighting (Chen et al., 2017)), continued training is most common and the focus of this paper. Knowledge Distillation is a method for improving the performance of under-parameterized “Student” models by exploiting the probability distribution of a more computationally complex “Teacher” network. Kim and Rush (2016) presented an extension of knowledge distillation to machine translation in two flavors: word-level and sequence-level knowledge distillation. Sequence-level knowledge distillation, which is more general, involves three steps: 1. A large Teacher network is randomly initialized and trained until convergence on the data. include dark knowledge (Furlanello et al., 2018), mode reduction (Zhou et al., 2019), and regularization (Gordon and Duh, 2019; Dong et al., 2019), but no definitive evidence has been given. Sequence-level knowledge distillation is widely used in both industry (Xia et al., 2019)"
2020.ngt-1.12,L16-1147,0,0.0253855,"of each model are performed by decoding the appropriate development set with a beam-search size of 10 and comparing to the reference using multi-bleu.perl from the Moses toolkit. The tokenization used during multi-bleu.perl evaluation is the same as the one provided in (Duh, 2019a). General-Domain Data We train models in multiple settings: 3 language pairs (German-English, Russian-English, and Chinese-English) each with 1 general-domain dataset and 2 different in-domain datasets. The general-domain datasets for each language are a concatenation of data from OpenSubtitles2018 (Tiedemann, 2016; Lison and Tiedemann, 2016) (which contains translated movie subtitles) and the WMT 2017 datasets (Ondrej et al., 2017) (which includes a variety of sources, including news commentary, parliamentary proceedings, and web-crawled data). Data Statistics The size of each training dataset is presented in Table 1. General-domain datasets contain tens of millions of sentences, while indomain datasets contain much less. GermanEnglish WIPO has an exceptional amount of training data (4.5 times more than the next biggest indomain dataset) and helps qualify how our results TED 152 k 180 k 169 k might change when more in-domain data"
2020.ngt-1.12,2015.iwslt-evaluation.11,0,0.0614117,"nfigurations, we find that two stages of knowledge distillation give the best performance: one using general-domain data and another using in-domain data with an adapted teacher. We perform experiments on multiple language pairs (Russian-English, German-English, ChineseEnglish), domains (patents, subtitles, news, TED talks), and student sizes. https://git.io/Jf2t8 2 Background Domain Adaptation helps overcome a lack of quality training data in niche domains by leveraging large amounts of data in a more accessible general-domain. Domain adaptation is usually accomplished by continued training (Luong and Manning, 2015; Zoph et al., 2016), which involves two steps: 1. A model is randomly initialized and trained until convergence on the general-domain data. 2. A new model is initialized with the parameters resulting from Step 1 and trained until convergence on the in-domain dataset. We can consider domain adaptation as extracting a useful inductive-bias from the general-domain dataset, which is encoded and passed along to the in-domain model as a favorable weight initialization. While there are other methods of extracting inductive bias from general-domain datasets (including mixed fine-tuning (Chu et al., 2"
2020.ngt-1.12,P16-1162,0,0.0697467,"(Zhou et al., 2019). Language De-En Ru-En Zh-En Distilling In-Domain Data How is in-domain data pre-processed using knowledge distillation? Some models are trained with no pre-processing (configurations 1, 4, and 7), while others use a teacher to pre-process the in-domain training data. This teacher might be a baseline trained on indomain data only (configurations 2, 5, and 8) or it can be trained on general-domain data and then adapted to in-domain via continued training (configurations 3, 6, and 9). 4.1 Pre-processing All datasets are tokenized using the Moses4 tokenizer. A BPE vocabulary (Sennrich et al., 2016) of 30,000 tokens is constructed for each language using the training set of the general-domain data. This BPE vocabulary is then applied to both in-domain and general-domain datasets. This mimics the typical scenario of a single, general-domain model being trained and then adapted to new domains as they are encountered. Note that re-training BPE on in-domain data to produce a different vocabulary would force us to re-build the model, making adaptation impossible. Data Evaluation The general-domain development set for each language contains newstest2016 concatenated with the last 2500 lines of"
2020.ngt-1.12,D19-1441,0,0.0607557,"Missing"
2020.ngt-1.12,L16-1559,0,0.0129731,"ach. Evaluations of each model are performed by decoding the appropriate development set with a beam-search size of 10 and comparing to the reference using multi-bleu.perl from the Moses toolkit. The tokenization used during multi-bleu.perl evaluation is the same as the one provided in (Duh, 2019a). General-Domain Data We train models in multiple settings: 3 language pairs (German-English, Russian-English, and Chinese-English) each with 1 general-domain dataset and 2 different in-domain datasets. The general-domain datasets for each language are a concatenation of data from OpenSubtitles2018 (Tiedemann, 2016; Lison and Tiedemann, 2016) (which contains translated movie subtitles) and the WMT 2017 datasets (Ondrej et al., 2017) (which includes a variety of sources, including news commentary, parliamentary proceedings, and web-crawled data). Data Statistics The size of each training dataset is presented in Table 1. General-domain datasets contain tens of millions of sentences, while indomain datasets contain much less. GermanEnglish WIPO has an exceptional amount of training data (4.5 times more than the next biggest indomain dataset) and helps qualify how our results TED 152 k 180 k 169 k might cha"
2020.ngt-1.12,D19-1078,0,0.0190762,"zed in-domain models. There is, however, similar work which is not directly comparable but uses knowledge distillation to adapt to new domains. Knowledge Adaptation uses knowledge distillation to transfer knowledge from multiple, labeled source domains to un-labeled target domains. This is in contrast to our setting, which has labels for both general-domain and in-domain data. Ruder et al. (2017) introduced this idea as “Knowledge Adaptation,” using multi-layer perceptrons to provide sentiment analysis labels for unlabeled indomain data. Similar work includes Iterative Dual Domain Adaptation (Zeng et al., 2019) and Domain Transformation Networks (Wang et al., 2019). These ideas are not limited to machine translation; recent work by Meng et al. (2020) trains in-domain speech recognition systems with knowledge distillation, while Orbes-Arteaga et al. (2019) does similar work on segmentation of magnetic resonance imaging scans. Compressing Pre-trained Language Models Domain adaptation via continued training in NMT is closely related to the idea of pre-training a language model and fine-tuning to different tasks, which might come from different data distributions than the pre-training data. Because lang"
2020.ngt-1.12,D16-1163,0,0.0174512,"t two stages of knowledge distillation give the best performance: one using general-domain data and another using in-domain data with an adapted teacher. We perform experiments on multiple language pairs (Russian-English, German-English, ChineseEnglish), domains (patents, subtitles, news, TED talks), and student sizes. https://git.io/Jf2t8 2 Background Domain Adaptation helps overcome a lack of quality training data in niche domains by leveraging large amounts of data in a more accessible general-domain. Domain adaptation is usually accomplished by continued training (Luong and Manning, 2015; Zoph et al., 2016), which involves two steps: 1. A model is randomly initialized and trained until convergence on the general-domain data. 2. A new model is initialized with the parameters resulting from Step 1 and trained until convergence on the in-domain dataset. We can consider domain adaptation as extracting a useful inductive-bias from the general-domain dataset, which is encoded and passed along to the in-domain model as a favorable weight initialization. While there are other methods of extracting inductive bias from general-domain datasets (including mixed fine-tuning (Chu et al., 2017) and 110 Proceed"
2020.ngt-1.12,W17-4717,0,\N,Missing
2020.repl4nlp-1.18,P19-1337,0,0.0409847,"Missing"
2020.repl4nlp-1.18,N18-1202,0,\N,Missing
2020.repl4nlp-1.18,P19-1355,0,\N,Missing
2020.repl4nlp-1.18,N19-1423,0,\N,Missing
2020.repl4nlp-1.18,D19-1276,0,\N,Missing
2020.tacl-1.26,N16-1100,0,0.0304273,"Missing"
2020.tacl-1.26,D17-1151,0,0.0178182,"llenge with table-lookup is that sufficient coverage of the hyperparameter grid is assumed. Eggensperger et al. (2015) and Klein et al. (2019) propose using a predictive metamodel trained on a table-lookup benchmark to approximate hyperparameters that are not in the table. This is an interesting avenue for future work. Studies on HPO for NMT are scarce. Qin et al. (2017) propose an evolution strategy–based HPO method for NMT. So et al. (2019) apply NAS to Transformer on NMT tasks. There is also work on empirically exploring hyperparameters and architectures of NMT systems (Bahar et al., 2017; Britz et al., 2017; Lim et al., 2018), though the focus is on finding general best-practice configurations. This differs from the goal of HPO, which aims to find the best configuration specific to a given dataset. 9 Conclusions In this paper, we presented a benchmark dataset for hyperparameter optimization of neural machine translation systems. We provided multiple evaluation protocols and analysis approaches for comparing HPO methods. We benchmarked Bayesian optimization and a novel graph-based semi-supervised learning method on the dataset for both single-objective and multiobjective optimization. Our hope is"
2020.tacl-1.26,E17-3017,0,0.016374,"six datasets form a good representative set. It ranges from high-to-low resource; it contains both noisy and clean settings. These datasets also have different levels of similarity—for example, zh-en and ru-en TED talks use the same multiway parallel Dvalid , so one could ask whether the optimal hyperparameters transfer. The text is tokenized by Jieba for Chinese, by Kytea for Japanese, and by the Moses tokenizer for the rest. Byte pair encoding (BPE) segmentation (Sennrich et al., 2016) is learned and applied separately for each side of bitext. We train Transformer NMT models with Sockeye3 (Hieber et al., 2017), focusing on these hyperparameters: 3 https://github.com/awslabs/sockeye. 396 • preprocessing configurations: number of BPE symbols4 (bpe) • training settings: initial learning rate (init lr) for the Adam optimizer • architecture designs:5 number of layers (#layers), embedding size (#embed), number of hidden units in each layer (#hidden), number of heads in self-attention (#att heads). These hyperparameters are chosen because they significantly affect both accuracy and speed of the resulting NMT. Other hyperparameters are kept at their Sockeye defaults.6 Table 1 shows our overall hyperparamet"
2020.tacl-1.26,P02-1040,0,0.115875,"767 604 Best BLEU 14.66 20.23 16.41 20.74 26.09 11.23 bpe 30k 10k 30k 10k 1k 8k #layers 4 4 4 4 2 2 #embed 512 256 512 1024 256 512 #hidden 1024 2048 2048 2048 1024 1024 #att heads 16 8 8 8 8 8 init lr 3e-4 3e-4 3e-4 3e-4 6e-4 3e-4 Table 2: For each language pair, we report the number of NMT systems trained on it, the oracle best BLEU we obtained, and its corresponding hyperparameter configuration. 3.4 Objectives: Accuracy and Cost We train all models on Dtrain until they converge in terms of perplexity on Dvalid . We then record various performance measurements: • Translation accuracy: BLEU (Papineni et al., 2002) and perplexity on Dvalid . • Computational cost: GPU wall clock time for decoding Dvalid , number of updates for the model to converge, GPU memory used for training, total number of model parameters. In this paper, we use BLEU on Dvalid for single-objective experiments; we use BLEU and decoding time for multiobjective experiments. Figure 4: Correlation of hyperparameter rankings across MT datasets.8 3.5 Hyperparameter Importance/Correlation We might be interested in seeing whether good configurations are always good across datasets. This can be done by ranking configurations by BLEU for each"
2020.tacl-1.26,P16-1162,0,0.0672943,"alid consists of 2675 lines (ANALYSIS2 set). Although there are many potential MT datasets we could choose from, we believe these six datasets form a good representative set. It ranges from high-to-low resource; it contains both noisy and clean settings. These datasets also have different levels of similarity—for example, zh-en and ru-en TED talks use the same multiway parallel Dvalid , so one could ask whether the optimal hyperparameters transfer. The text is tokenized by Jieba for Chinese, by Kytea for Japanese, and by the Moses tokenizer for the rest. Byte pair encoding (BPE) segmentation (Sennrich et al., 2016) is learned and applied separately for each side of bitext. We train Transformer NMT models with Sockeye3 (Hieber et al., 2017), focusing on these hyperparameters: 3 https://github.com/awslabs/sockeye. 396 • preprocessing configurations: number of BPE symbols4 (bpe) • training settings: initial learning rate (init lr) for the Adam optimizer • architecture designs:5 number of layers (#layers), embedding size (#embed), number of hidden units in each layer (#hidden), number of heads in self-attention (#att heads). These hyperparameters are chosen because they significantly affect both accuracy an"
2020.tacl-1.4,W16-2301,1,0.841313,"Missing"
2020.tacl-1.4,N19-1423,0,0.0160164,"in Section 6.1. The results are shown in Table 7. We can see that this extra feature did not provide any significant influence to the accuracy. In a more detailed analysis, we find that the reason is that our in and out probes both contain a range of translations from low to high quality translations, and our QE model may not be sufficiently finegrained to tease apart any potential differences. In fact, this may be difficult even for a human estimator. Another approach to exploit external resources is to use a language model pre-trained on a large amount of text. In particular, we used BERT (Devlin et al., 2019), which has shown competitive results in many NLP tasks. We used BERT directly as a classifier, and followed a fine-tuning setup similar to paraphrase detection: For our case the inputs are the English translation and reference 60 ings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, pages 169–214, Copenhagen, Denmark. Association for Computational Linguistics. Our attack approach was a simple one, using shadow models to mimic the target model. Bob can attempt more complex strategies, for example, by using the translation AP"
2020.tacl-1.4,E17-3017,0,0.0210062,"e, and combines in-probe and the rest (Ball minus 10 sets) as a training sets. We use notations 4.2 Alice MT Architecture Alice uses her dataset Atrain (consisting of four subcorpora and ParaCrawl) to train her own MT model. Because Paracrawl is noisy, Alice first applies dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018), retaining the top 4.5 million lines. Alice then trains a joint BPE subword model (Sennrich et al., 2016) using 32,000 merge operations. No recasing is applied. Alice’s model is a six-layer Transformer (Vaswani et al., 2017) using default parameters in Sockeye (Hieber et al., 2017).7 The model was 6 We prepared two different pairs of Ain probe and Aout probe . Thus Ball has 10k fewer samples than Atrain , and not 5k fewer. For the experiment we used only one pair, and kept the other for future use. 7 Three-way tied embeddings, model and embedding size 512, eight attention heads, 2,048 hidden states in the feed forward layers, layer normalization applied before each selfattention layer, and dropout and residual connections applied afterward, word-based batch size of 4,096. 8 Version 1.2.12, case-sensitive, ‘‘13a’’ tokenization for comparability with WMT. 54 Aout probe Ai"
2020.tacl-1.4,W18-6478,0,0.022156,"label to train a binary classifier g(f, e, eˆ). If Bob’s shadow models are sufficiently similar to Alice’s in behavior, this attack can work. Bob first selects 10 sets of 5,000 sentences per subcorpus in Ball . He then chooses two sets and uses one as in-probe and the other as outprobe, and combines in-probe and the rest (Ball minus 10 sets) as a training sets. We use notations 4.2 Alice MT Architecture Alice uses her dataset Atrain (consisting of four subcorpora and ParaCrawl) to train her own MT model. Because Paracrawl is noisy, Alice first applies dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018), retaining the top 4.5 million lines. Alice then trains a joint BPE subword model (Sennrich et al., 2016) using 32,000 merge operations. No recasing is applied. Alice’s model is a six-layer Transformer (Vaswani et al., 2017) using default parameters in Sockeye (Hieber et al., 2017).7 The model was 6 We prepared two different pairs of Ain probe and Aout probe . Thus Ball has 10k fewer samples than Atrain , and not 5k fewer. For the experiment we used only one pair, and kept the other for future use. 7 Three-way tied embeddings, model and embedding size 512, eight attention heads, 2,048 hidden"
2020.tacl-1.4,P19-3020,0,0.0532227,"Missing"
2020.tacl-1.4,P02-1040,0,0.108594,"n decisions that balance between simple experimentation vs. realistic condition. Carol doing a common tokenization removes some of the MT-specific complexity for researchers who want to focus on the Alice or Bob models. However, in a real-world public API, Alice’s tokenization is likely to be unknown to Bob. We decided on a middle ground to have Carol perform a common tokenization, but Alice and Bob do their own subword segmentation. 53 trained until perplexity on newstest2017 (Bojar et al., 2017) had not improved for five consecutive checkpoints, computed every 5,000 batches. The BLEU score (Papineni et al., 2002) on newstest2018 was 42.6, computed using sacreBLEU (Post, 2018) with the default settings.8 4.3 Evaluation Protocol To evaluate membership inference attacks on Alice’s MT models, we use the following procedure: First, Bob asks Alice to translate f . Alice returns her result eˆ to Bob. Bob also has access to the reference e and use his classifier g(f, e, eˆ) to infer whether (e, f ) was in Alice’s training data. The classification is reported to Carol, who computes ‘‘attack accuracy’’. Given a probe set P containing a list of (f, e, eˆ, l), where l is the label (in or out), this accuracy is de"
2020.tacl-1.4,W17-4763,0,0.0142409,"We believe this attests to the inherent difficulty of the sentence-level membership inference problem. Alice Bob:train Bob:valid Bob:test P DT NB NN MLP BERT 50.0 50.3 50.4 49.8 50.4 50.0 49.9 51.4 51.2 66.1 51.0 50.0 50.0 51.1 51.1 50.0 51.0 50.0 50.0 51.1 51.0 50.1 50.8 50.0 Table 7: Membership inference accuracies for classifiers with Quality Estimation sentence score as an extra feature, and a BERT classifier. system. Our models are NMT, so the estimation quality may not be optimally matched, but we believe this is the best data available at this time. We applied the Predictor-Estimator (Kim et al., 2017) implemented in an open source QE framework OpenKiwi (Kepler et al., 2019). It consists of a predictor that predicts each token of the target sentence given the target context and the source, and estimator that takes features produced by the predictor to estimate the labels; both are made of LSTMs. We used this model as this is one of the best models seen in the shared tasks, and it does not require alignment information. The model metrics on the WMT18 dev set, namely, Pearson’s correlation, Mean Average Error, and Root Mean Squared Error for sentence-level scores, are 0.6238, 0.1276, and 0.17"
2020.tacl-1.4,W18-6319,1,0.852908,"tion. Carol doing a common tokenization removes some of the MT-specific complexity for researchers who want to focus on the Alice or Bob models. However, in a real-world public API, Alice’s tokenization is likely to be unknown to Bob. We decided on a middle ground to have Carol perform a common tokenization, but Alice and Bob do their own subword segmentation. 53 trained until perplexity on newstest2017 (Bojar et al., 2017) had not improved for five consecutive checkpoints, computed every 5,000 batches. The BLEU score (Papineni et al., 2002) on newstest2018 was 42.6, computed using sacreBLEU (Post, 2018) with the default settings.8 4.3 Evaluation Protocol To evaluate membership inference attacks on Alice’s MT models, we use the following procedure: First, Bob asks Alice to translate f . Alice returns her result eˆ to Bob. Bob also has access to the reference e and use his classifier g(f, e, eˆ) to infer whether (e, f ) was in Alice’s training data. The classification is reported to Carol, who computes ‘‘attack accuracy’’. Given a probe set P containing a list of (f, e, eˆ, l), where l is the label (in or out), this accuracy is defined as: Figure 3: Illustration of actual MT data splits. Atrai"
2020.tacl-1.4,W17-3204,0,0.0171121,"n MT models, in order to mimic Alice and design his attacks. This data could either be disjoint from Atrain , or contain parts of Atrain . We choose the latter, which assumes that there might be some public data that is accessible to both Alice and Bob. This scenario slightly favors Bob. In the case of MT, parallel data can be hard to come by, and datasets Both Aout probe and Aood should be classified as out by Bob’s classifier. However, it has been known that sequence-to-sequence models behave very differently on data from domains/genre that is significantly different from the training data (Koehn and Knowles, 2017). The goal of having two out probes is to quantify the difficulty or ease of membership inference in different situations. 2.2 Summary and Alternative Definitions Figure 2 summarizes the problem definition. The probes Aout probe and Aood are by construction outside of Alice’s training data Atrain , while the probe Ain probe is included. Bob’s goal is to produce a classifier that can make this distinction. 51 providers may support customized engines if users upload their own bitext training data. The provider promises that the user-supplied data will not be used in the customized engines of oth"
2020.tacl-1.4,P04-1077,0,0.0355638,"notate this as Bin probe , Bout probe , and 1− Btrain . With 10 sets of 5,000 sentences, Bob can create 10 different groups of in-probe, out-probe, and training sets. Figure 4 illustrates the data splits. For each group of data, Bob first trains a shadow MT model using the training set. He then uses this model to translate sentences in the in-probe and out-probe sets. Bob has now a list of (f, e, eˆ) from different shadow models, and he knows for each Bob extracts features from (f, e, eˆ) for a binary classifier. He uses modified 1- to 4-gram precisions and smoothed sentence-level BLEU score (Lin and Och, 2004) as features. Bob’s intuition is that if an unusually large number of n-grams in eˆ matches e, then it could be a sign that this was in the training data and Alice memorized it. Bob calculates n-gram precision by counting the number of n-grams in translation that appear in the reference sentence. In the later investigation Bob also considers the MT model score as an extra feature. 55 Algorithm 1: Construction of A Membership Inference Classifier Data: Ball Result: g(·) i Split Ball into multiples groups of (Bin probe , i i Bout probe , Btrain ) ; foreach i in 1+, 1−, 2+, 2−, 3+, 3− do i Train"
2020.tacl-1.4,tiedemann-2012-parallel,0,0.086097,"Missing"
2020.tacl-1.4,P16-1162,0,0.00956918,"in behavior, this attack can work. Bob first selects 10 sets of 5,000 sentences per subcorpus in Ball . He then chooses two sets and uses one as in-probe and the other as outprobe, and combines in-probe and the rest (Ball minus 10 sets) as a training sets. We use notations 4.2 Alice MT Architecture Alice uses her dataset Atrain (consisting of four subcorpora and ParaCrawl) to train her own MT model. Because Paracrawl is noisy, Alice first applies dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018), retaining the top 4.5 million lines. Alice then trains a joint BPE subword model (Sennrich et al., 2016) using 32,000 merge operations. No recasing is applied. Alice’s model is a six-layer Transformer (Vaswani et al., 2017) using default parameters in Sockeye (Hieber et al., 2017).7 The model was 6 We prepared two different pairs of Ain probe and Aout probe . Thus Ball has 10k fewer samples than Atrain , and not 5k fewer. For the experiment we used only one pair, and kept the other for future use. 7 Three-way tied embeddings, model and embedding size 512, eight attention heads, 2,048 hidden states in the feed forward layers, layer normalization applied before each selfattention layer, and dropou"
2020.tacl-1.4,W17-4717,1,\N,Missing
2020.wmt-1.68,D18-1214,0,0.112511,"Missing"
2020.wmt-1.68,P18-1073,0,0.0618534,"tion Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising for real-world low-resource scenarios. Un"
2020.wmt-1.68,D18-1399,0,0.0621742,"tion Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising for real-world low-resource scenarios. Un"
2020.wmt-1.68,P19-1019,0,0.323171,"e crosslingual language model pretraining (Conneau and Lample, 2019), masked sequence-to-sequence pretraining (Song et al., 2019), and multilingual denoising pretraining (Liu et al., 2020), and have shown promise. For instance, Liu et al. (2020) record the first good results on the low-resource Sinhala-English and Nepali-English pairs. While pretraining and multilingual methods are not the subject of this work, they warrant future evaluation. Figure 1 depicts the basic training process. It is the publicly-available SMT setup of Artetxe et al. (2018b)2 , plus the “NMT hybridization” steps from Artetxe et al. (2019).3 572 2 3 https://github.com/artetxem/monoses Shared with us by Mikel Artetxe. √ Figure 1: The unsupervised MT architecture used in this work. This model is a replication of Artetxe et al. (2018b) [steps before NMT] and Artetxe et al. (2019) [NMT component]. Mz is sorted (they find that using the square root works better empirically), and length-normalized, mean-centered, and length-normalized again. For √ each row√i in sorted( Mx ), they find the row j of sorted( Mz ) that is its nearest neighbor, and assign Xi = Zj in the initial translation dictionary D. A cell Dij = 1 if words Xi , and Zj"
2020.wmt-1.68,J82-2005,0,0.761934,"Missing"
2020.wmt-1.68,W95-0114,0,0.212136,"ystems. 571 Proceedings of the 5th Conference on Machine Translation (WMT), pages 571–583 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics and 6. Section 7 presents our findings, and Section 8 discusses the results. We conclude in Section 9. 2 Related Work Bilingual Lexicon Induction Unsupervised MT methods can be thought of as an end-to-end extension of work inducing bilingual lexicons from monolingual corpora. Bilingual lexicon induction (BLI) using non-parallel data has a rich history, beginning with corpus statistic and decipherment methods (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008), continuing to modern neural methods to create crosslingual word embeddings (e.g. Mikolov et al., 2013a; Conneau et al., 2018, see Ruder et al. (2019) for a survey) which form a critical component of stateof-the-art unsupervised MT systems. Evaluation of Embedding Spaces Søgaard et al. (2018) determine that monolingual embedding spaces of similar languages are not typically isomorphic as was previously believed, and that bilingual dictionary induction “depends heavily on... the language pair, the comparability of the monolingual corpora, a"
2020.wmt-1.68,P19-1070,0,0.0656921,"Missing"
2020.wmt-1.68,D19-1632,1,0.90559,"Missing"
2020.wmt-1.68,P08-1088,0,0.0684869,"nce on Machine Translation (WMT), pages 571–583 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics and 6. Section 7 presents our findings, and Section 8 discusses the results. We conclude in Section 9. 2 Related Work Bilingual Lexicon Induction Unsupervised MT methods can be thought of as an end-to-end extension of work inducing bilingual lexicons from monolingual corpora. Bilingual lexicon induction (BLI) using non-parallel data has a rich history, beginning with corpus statistic and decipherment methods (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008), continuing to modern neural methods to create crosslingual word embeddings (e.g. Mikolov et al., 2013a; Conneau et al., 2018, see Ruder et al. (2019) for a survey) which form a critical component of stateof-the-art unsupervised MT systems. Evaluation of Embedding Spaces Søgaard et al. (2018) determine that monolingual embedding spaces of similar languages are not typically isomorphic as was previously believed, and that bilingual dictionary induction “depends heavily on... the language pair, the comparability of the monolingual corpora, and the parameters of the word embedding algorithms.” V"
2020.wmt-1.68,W11-2123,0,0.0189269,"the goal is to find the linear transformations Wx and Wz which maximize the cosine similarity of the words that are translations of one another as defined by the dictionary D, over the entire dictionary: arg max Wx ,Wz Training begins with two monolingual corpora which are not necessarily related in any way (i.e. they are not assumed to be parallel nor comparable text). First, word embeddings are trained independently for each corpus, resulting in a source and a target embedding space. Specifically, after preprocessing, Artetxe et al. (2018b) train two statistical language models using KenLM (Heafield, 2011), one for the source language and one for the target. They use phrase2vec4 (Artetxe et al., 2018b), an extension of Mikolov et al. (2013b)’s skip-gram model,5 to generate phrase embeddings for 200,000 unigrams, 400,000 bigrams, and 400,000 trigrams. Next, source and target word embeddings are aligned into a common cross-lingual embedding space. They run VecMap6 (Artetxe et al., 2018a) which calculates a linear mapping of one space to another based on the intuition that phrases with similar meaning should have similar neighbors regardless of language. Given a matrix of source word embeddings X"
2020.wmt-1.68,D18-1043,0,0.0289312,"n (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising for real-world low-resource scenarios. Unsupervised MT methods mu"
2020.wmt-1.68,D13-1176,0,0.0503125,"asets. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that stochasticity during embedding training can dramatically affect downstream results. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms. Towards this goal, we release our preprocessed dataset to stress-test systems under multiple data conditions. 1 Introduction Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear"
2020.wmt-1.68,2020.eamt-1.5,0,0.200863,"Missing"
2020.wmt-1.68,P07-2045,1,0.03092,"cts an initial phrase-table for use in a SMT system. They use the softmax over the cosine similarity of the 100 nearest-neighbors of each source phrase embedding as the phrase translation probabilities. This is done in both directions: e(cos(e,f )/τ ) (f |e) = P (cos(e,f 0 )/τ ) f0 e For the target embedding with the highest cosine similarity, the phrases are aligned, and unigram translation probabilities are multiplied to become the lexical weighting. Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase-based SMT system (Koehn et al., 2007). The SMT model weights 573 are tuned using a variant of MERT (Och, 2003) designed for unsupervised scenarios, which uses 10,000 parallel sentences generated via backtranslation (Sennrich et al., 2016a). The SMT model then undergoes three rounds of iterative backtranslation. Artetxe et al. (2019) extend their 2018 work by adding a critical “NMT hybridization” final step, which achieves significant gains over SMT alone.7 An NMT system is trained using backtranslated output from SMT for one epoch. On the next epoch, a small number of sentences are backtranslated with the newly-trained NMT system"
2020.wmt-1.68,W02-0902,1,0.670726,"Missing"
2020.wmt-1.68,P02-1040,0,0.114773,"w. Namely, we: To elucidate the performance gap due to the unsupervised architecture, we build a standard supervised NMT system using the same neural architecture described above. We train until performance on the development set ceases to improve for 10 epochs. To parallel the unsupervised setup, we translate the test set using an ensemble of 6 models; We perform ensemble selection by performance on a validation set, selecting the best-performing checkpoint along with 5 previous checkpoints. 7 Readers are directed to Artetxe et al. (2019) for additional changes that resulted in sizable BLEU (Papineni et al., 2002) gains before the NMT phase. 574 1. Choose 2 language pairs, at least one of which where the source and target languages utilize different scripts. 2. Choose 3 datasets of different domains, at least one of which is parallel bitext. 3. Perform at least one experiment for each language pair under each of the following data conditions: • Originally parallel • Not originally parallel • Different domain for source and target. 4. Choose 2 true low-resource language pairs. Condition Repro Fr-En En-Fr Supervised Fr-En Ru-En Parallel Fr-En Ru-En Disjoint Fr-En Ru-En Diff. Dom. Fr-En Ru-En News Fr-En R"
2020.wmt-1.68,P19-1018,0,0.0304513,"highlight the importance of evaluating unsupervised MT under varying realistic data conditions. Our evaluation is a step towards this goal, and identifies multiple areas for improvement. A critical step in state-of-the-art unsupervised MT is methods for creating CLEs. Several authors have pointed out that “mapping” methods like VecMap assume that monolingual vector spaces are structurally similar, but that this “approximate isomorphism assumption” is increasingly tenuous as languages and domains diverge (Søgaard et al., 2018; Ormazabal et al., 2019; Glavaˇs et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). Patra et al. (2019) find this for Fr-En and Ru-En specifically, 579 the languages examined in this work. Nakashole and Flauger (2018) argue that while linearity may hold within local “neighborhoods” of the vector space, the global mapping is non-linear. Søgaard et al. (2018) use their eigenvector similarity metric to show a strong correlation between vector space similarity and BLI performance. Analysis of the CLEs from our experiments demonstrate a relationship between BLI performance and downstream BLEU on the translation task. Coupled with our empirical evidence, the works cited in this s"
2020.wmt-1.68,2020.tacl-1.47,0,0.148042,"ith supervised baselines were inequitable. While a modest body of literature has examined the quality of cross-lingual word embeddings (CLEs) by measuring performance on BLI, Glavaˇs et al. (2019) evaluate on downstream natural language tasks, underlining the importance of fullsystem evaluation. The authors conclude that “the quality of CLE models is largely task-dependent and that overfitting the models to the BLI task can result in deteriorated performance in downstream tasks.” Similarly, Doval et al. (2019) investigate cross-lingual natural language inference. Evaluation of Unsupervised MT Liu et al. (2020) helpfully re-define unsupervised machine translation into three distinct categories: (1) no bitext whatsoever, (2) the target language pair is linked through bitext via a pivot language, and (3) no linkage through a pivot language, but bitexts exists for *some* language and the target language. The authors analyze their multilingual pretraining method with respect to other similar training paradigms (Conneau and Lample, 2019; Song et al., 2019) and evaluate unsupervised MT performance when using backtranslation (Definition 1) or language transfer after finetuning on related bitext (Definition"
2020.wmt-1.68,P18-2036,0,0.0501533,"rds this goal, and identifies multiple areas for improvement. A critical step in state-of-the-art unsupervised MT is methods for creating CLEs. Several authors have pointed out that “mapping” methods like VecMap assume that monolingual vector spaces are structurally similar, but that this “approximate isomorphism assumption” is increasingly tenuous as languages and domains diverge (Søgaard et al., 2018; Ormazabal et al., 2019; Glavaˇs et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). Patra et al. (2019) find this for Fr-En and Ru-En specifically, 579 the languages examined in this work. Nakashole and Flauger (2018) argue that while linearity may hold within local “neighborhoods” of the vector space, the global mapping is non-linear. Søgaard et al. (2018) use their eigenvector similarity metric to show a strong correlation between vector space similarity and BLI performance. Analysis of the CLEs from our experiments demonstrate a relationship between BLI performance and downstream BLEU on the translation task. Coupled with our empirical evidence, the works cited in this section suggest that nonisometric vector spaces lead to poor quality translation. Factors observed in our experiments that lead to lower"
2020.wmt-1.68,P03-1021,0,0.0706128,"cosine similarity of the 100 nearest-neighbors of each source phrase embedding as the phrase translation probabilities. This is done in both directions: e(cos(e,f )/τ ) (f |e) = P (cos(e,f 0 )/τ ) f0 e For the target embedding with the highest cosine similarity, the phrases are aligned, and unigram translation probabilities are multiplied to become the lexical weighting. Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase-based SMT system (Koehn et al., 2007). The SMT model weights 573 are tuned using a variant of MERT (Och, 2003) designed for unsupervised scenarios, which uses 10,000 parallel sentences generated via backtranslation (Sennrich et al., 2016a). The SMT model then undergoes three rounds of iterative backtranslation. Artetxe et al. (2019) extend their 2018 work by adding a critical “NMT hybridization” final step, which achieves significant gains over SMT alone.7 An NMT system is trained using backtranslated output from SMT for one epoch. On the next epoch, a small number of sentences are backtranslated with the newly-trained NMT system and concatenated with a slightly smaller fraction of SMT-generated bitex"
2020.wmt-1.68,P19-1492,0,0.0313061,"erform on the use cases for which they are needed. These challenges highlight the importance of evaluating unsupervised MT under varying realistic data conditions. Our evaluation is a step towards this goal, and identifies multiple areas for improvement. A critical step in state-of-the-art unsupervised MT is methods for creating CLEs. Several authors have pointed out that “mapping” methods like VecMap assume that monolingual vector spaces are structurally similar, but that this “approximate isomorphism assumption” is increasingly tenuous as languages and domains diverge (Søgaard et al., 2018; Ormazabal et al., 2019; Glavaˇs et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). Patra et al. (2019) find this for Fr-En and Ru-En specifically, 579 the languages examined in this work. Nakashole and Flauger (2018) argue that while linearity may hold within local “neighborhoods” of the vector space, the global mapping is non-linear. Søgaard et al. (2018) use their eigenvector similarity metric to show a strong correlation between vector space similarity and BLI performance. Analysis of the CLEs from our experiments demonstrate a relationship between BLI performance and downstream BLEU on the translation task"
2020.wmt-1.68,P95-1050,0,0.163797,"testing of systems. 571 Proceedings of the 5th Conference on Machine Translation (WMT), pages 571–583 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics and 6. Section 7 presents our findings, and Section 8 discusses the results. We conclude in Section 9. 2 Related Work Bilingual Lexicon Induction Unsupervised MT methods can be thought of as an end-to-end extension of work inducing bilingual lexicons from monolingual corpora. Bilingual lexicon induction (BLI) using non-parallel data has a rich history, beginning with corpus statistic and decipherment methods (e.g., Rapp, 1995; Fung, 1995; Koehn and Knight, 2000, 2002; Haghighi et al., 2008), continuing to modern neural methods to create crosslingual word embeddings (e.g. Mikolov et al., 2013a; Conneau et al., 2018, see Ruder et al. (2019) for a survey) which form a critical component of stateof-the-art unsupervised MT systems. Evaluation of Embedding Spaces Søgaard et al. (2018) determine that monolingual embedding spaces of similar languages are not typically isomorphic as was previously believed, and that bilingual dictionary induction “depends heavily on... the language pair, the comparability of the monolingua"
2020.wmt-1.68,P11-1002,0,0.133892,"under multiple data conditions. 1 Introduction Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising"
2020.wmt-1.68,P16-1009,0,0.438638,"ies. This is done in both directions: e(cos(e,f )/τ ) (f |e) = P (cos(e,f 0 )/τ ) f0 e For the target embedding with the highest cosine similarity, the phrases are aligned, and unigram translation probabilities are multiplied to become the lexical weighting. Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase-based SMT system (Koehn et al., 2007). The SMT model weights 573 are tuned using a variant of MERT (Och, 2003) designed for unsupervised scenarios, which uses 10,000 parallel sentences generated via backtranslation (Sennrich et al., 2016a). The SMT model then undergoes three rounds of iterative backtranslation. Artetxe et al. (2019) extend their 2018 work by adding a critical “NMT hybridization” final step, which achieves significant gains over SMT alone.7 An NMT system is trained using backtranslated output from SMT for one epoch. On the next epoch, a small number of sentences are backtranslated with the newly-trained NMT system and concatenated with a slightly smaller fraction of SMT-generated bitext. The procedure continues for 30 epochs, gradually increasing the percentage of synthetic training data created by the NMT sys"
2020.wmt-1.68,P16-1162,0,0.261177,"ies. This is done in both directions: e(cos(e,f )/τ ) (f |e) = P (cos(e,f 0 )/τ ) f0 e For the target embedding with the highest cosine similarity, the phrases are aligned, and unigram translation probabilities are multiplied to become the lexical weighting. Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase-based SMT system (Koehn et al., 2007). The SMT model weights 573 are tuned using a variant of MERT (Och, 2003) designed for unsupervised scenarios, which uses 10,000 parallel sentences generated via backtranslation (Sennrich et al., 2016a). The SMT model then undergoes three rounds of iterative backtranslation. Artetxe et al. (2019) extend their 2018 work by adding a critical “NMT hybridization” final step, which achieves significant gains over SMT alone.7 An NMT system is trained using backtranslated output from SMT for one epoch. On the next epoch, a small number of sentences are backtranslated with the newly-trained NMT system and concatenated with a slightly smaller fraction of SMT-generated bitext. The procedure continues for 30 epochs, gradually increasing the percentage of synthetic training data created by the NMT sys"
2020.wmt-1.68,P18-1072,0,0.0450552,"Missing"
2020.wmt-1.68,tiedemann-2012-parallel,0,0.047503,"Missing"
2020.wmt-1.68,D19-1449,0,0.0718782,"Missing"
2020.wmt-1.68,P18-1005,0,0.0425578,"ditions. 1 Introduction Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014) and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g., Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b, 2019). Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising for real-world low-"
2020.wmt-1.68,L16-1561,0,0.0315403,"is in BPE tokens. All others are token counts for SMT (pre-BPE). Datasets Training datasets used in our reinvestigation of the unsupervised MT system presented in Artetxe et al. (2019) are shown in Table 1. We focus on RussianEnglish (Ru-En) and French-English (Fr-En) tasks and include as reference Sinhala-English (Si-En) and Nepali-English (Ne-En) as well. Following Section 5, we evaluate the same system under various ablated data setups: Corpus News News UN: A UN: A UN: A UN: A UN: A / B UN: A / B UN: A / CC UN: A / CC News News CC CC United Nations The United Nations Parallel Corpus (UN) (Ziemski et al., 2016) contains official United Nations documents from 1990-2014, human-translated into six languages. The first 10,000 lines of each dataset are held-out. The remaining lines are partitioned into training sets A & B. Training set A on the source side and A on the target side are paired to form the Parallel training set; Training set A on the source side and B on the target side are paired to form the Disjoint training set. 6.2 News Crawl News crawl (News) consists of monolingual data crawled from news websites. Data for each year has been shuffled. Following Artetxe et al. (2018b), we concatenate N"
2020.wmt-1.68,D18-1549,0,\N,Missing
2021.eacl-main.209,D09-1092,0,0.346415,"elies on discrete counts and co-occurrence statistics, and therefore has poorer estimates in low resource scenarios due to data sparsity. word is represented by an M -dimensional vector v ∈ RM and is drawn from a multivariate Gaussian for that topic. That is, for K topics, there are K Gaussian distributions. While there have been extensions to more complex continuous distributions such as von Mises-Fisher (Batmanghelich et al., 2016; Li et al., 2016b)), we opted to work with a simpler distribution to demonstrate the approach, which can subsequently be extended in future work. Polylingual LDA (Mimno et al., 2009) studies LDA across more than two languages using parallel corpora. The model assumes that the documenttopic distribution θd , is shared across languages, and that each language specific topic has a multinomial topic-word distribution, Φ`1 , Φ`2 due to the discrete nature of words. Mimno et al. (2009); Ni et al. (2009) showed that Polylingual topic models can infer topic structure in multilingual corpora. Latent Feature Topic Models A natural extension to discrete only or continuous only representations, is to model a word as being sampled with some probability from its discrete or continuous"
2021.eacl-main.96,L18-1530,0,0.0836761,"tones and glottal stops. Documentation requires accurate transcriptions, a goal yet beyond even the capability of an enthusiastic speaker with many 1134 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1134–1145 April 19 - 23, 2021. ©2021 Association for Computational Linguistics months of training. As noted, ASR has been proposed to mitigate the Transcription Bottleneck and create increasingly extensive EL corpora. Previous studies first investigated HMM-based ASR for EL documenta´ tion (Cavar et al., 2016; Mitra et al., 2016; Adams et al., 2018; Jimerson et al., 2018; Jimerson and Prud’hommeaux, 2018; Michaud et al., 2018; Cruz and Waring, 2019; Thai et al., 2020; Zahrer et al., 2020; Gupta and Boulianne, 2020a). Along with HMM-based ASR, natural language processing and semi-supervised learning have been suggested as a way to produce morphological and syntactic analyses. As HMM-based systems have become more precise, they have been increasingly promoted as a mechanism to bypass the transcription bottleneck. However, ASR’s context for ELs is quite distinct from that of major languages. Endangered languages seldom have sufficient exta"
2021.eacl-main.96,W17-0123,0,0.0266152,"Watanabe1 1 The Johns Hopkins University, Baltimore, Maryland, United States 2 Department of Anthropology, Gettysburg College 3 Secretar´ıa de Educaci´on P´ublica, Estado de Guerrero, Mexico {jiatong shi@, kevinduh@cs.}jhu.edu {jonamith, reyyoloxochitl, estebanyoloxochitl}@gmail.com shinjiw@ieee.org Abstract challenging, primarily because the traditional method to preserve primary data is not simply with audio recordings but also through time-coded transcriptions. In a best-case scenario, texts are presented in interlinear format with aligned parses and glosses along with a free translation (Anastasopoulos and Chiang, 2017). But interlinear transcriptions are difficult to produce in meaningful quantities: (1) ELs often lack a standardized orthography (if written at all); (2) invariably, few speakers can accurately transcribe recordings. Even a highly skilled native speaker or linguist will require a minimum of 30 to 50 hours to simply transcribe one hour of recording (Michaud et al., 2014; Zahrer et al., 2020). Additional time is needed for parse, gloss, and translation. This creates what has been called a “transcription bottleneck”, a situation in which the expert transcribers cannot keep up with the amount of"
2021.eacl-main.96,D18-2012,0,0.0470952,"Missing"
2021.eacl-main.96,2020.lrec-1.307,0,0.342181,"gs of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1134–1145 April 19 - 23, 2021. ©2021 Association for Computational Linguistics months of training. As noted, ASR has been proposed to mitigate the Transcription Bottleneck and create increasingly extensive EL corpora. Previous studies first investigated HMM-based ASR for EL documenta´ tion (Cavar et al., 2016; Mitra et al., 2016; Adams et al., 2018; Jimerson et al., 2018; Jimerson and Prud’hommeaux, 2018; Michaud et al., 2018; Cruz and Waring, 2019; Thai et al., 2020; Zahrer et al., 2020; Gupta and Boulianne, 2020a). Along with HMM-based ASR, natural language processing and semi-supervised learning have been suggested as a way to produce morphological and syntactic analyses. As HMM-based systems have become more precise, they have been increasingly promoted as a mechanism to bypass the transcription bottleneck. However, ASR’s context for ELs is quite distinct from that of major languages. Endangered languages seldom have sufficient extant language lexicons to train an HMM system and invariably suffer from a dearth of skilled transcribers to create these necessary resources (Gupta and Boulianne, 2020b)."
2021.eacl-main.96,2020.sltu-1.51,0,0.203593,"gs of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1134–1145 April 19 - 23, 2021. ©2021 Association for Computational Linguistics months of training. As noted, ASR has been proposed to mitigate the Transcription Bottleneck and create increasingly extensive EL corpora. Previous studies first investigated HMM-based ASR for EL documenta´ tion (Cavar et al., 2016; Mitra et al., 2016; Adams et al., 2018; Jimerson et al., 2018; Jimerson and Prud’hommeaux, 2018; Michaud et al., 2018; Cruz and Waring, 2019; Thai et al., 2020; Zahrer et al., 2020; Gupta and Boulianne, 2020a). Along with HMM-based ASR, natural language processing and semi-supervised learning have been suggested as a way to produce morphological and syntactic analyses. As HMM-based systems have become more precise, they have been increasingly promoted as a mechanism to bypass the transcription bottleneck. However, ASR’s context for ELs is quite distinct from that of major languages. Endangered languages seldom have sufficient extant language lexicons to train an HMM system and invariably suffer from a dearth of skilled transcribers to create these necessary resources (Gupta and Boulianne, 2020b)."
2021.eacl-main.96,2020.lrec-1.319,0,0.25132,"(Gupta and Boulianne, 2020b). As we have confirmed with this present study, end-to-end ASR systems have shown comparable or better results over conventional HMM-based methods (Graves and Jaitly, 2014; Chiu et al., 2018; Pham et al., 2019; Karita et al., 2019a). As endto-end systems directly predict textual units from acoustic information, they save much effort on lexicon construction. Nevertheless, end-to-end ASR systems still suffer from the limitation of training data. Attempts with resource-scarce languages have relatively high character (CER) or word (WER) error rates (Thai et al., 2020; Matsuura et al., 2020; Hjortnaes et al., 2020). It has nevertheless become possible to utilize ASR with ELs to reduce significantly, but not eliminate, the need for human input and annotation to create acceptable (“archival quality”) transcriptions. This Work: This work represents end-to-end ASR efforts on Yolox´ochitl Mixtec (YM), an endangered language from western Mexico. The YMC1 corpus comprises two sub-corpora. The first (“YMC-EXP”, expert transcribed, corpus) includes 100 hours of transcribed speech that have been carefully checked for accuracy. We built a recipe of the ESPNet (Watanabe et al., 2018) that s"
2021.eacl-main.96,2020.iwclul-1.5,0,0.439226,"2020b). As we have confirmed with this present study, end-to-end ASR systems have shown comparable or better results over conventional HMM-based methods (Graves and Jaitly, 2014; Chiu et al., 2018; Pham et al., 2019; Karita et al., 2019a). As endto-end systems directly predict textual units from acoustic information, they save much effort on lexicon construction. Nevertheless, end-to-end ASR systems still suffer from the limitation of training data. Attempts with resource-scarce languages have relatively high character (CER) or word (WER) error rates (Thai et al., 2020; Matsuura et al., 2020; Hjortnaes et al., 2020). It has nevertheless become possible to utilize ASR with ELs to reduce significantly, but not eliminate, the need for human input and annotation to create acceptable (“archival quality”) transcriptions. This Work: This work represents end-to-end ASR efforts on Yolox´ochitl Mixtec (YM), an endangered language from western Mexico. The YMC1 corpus comprises two sub-corpora. The first (“YMC-EXP”, expert transcribed, corpus) includes 100 hours of transcribed speech that have been carefully checked for accuracy. We built a recipe of the ESPNet (Watanabe et al., 2018) that shows the whole process of"
2021.eacl-main.96,L18-1657,0,0.0977716,"ut “code-switching”. Model E2E-RNN E2E-Transformer E2E-Conformer CER dev/test 9.2/9.3 7.8/7.9 7.7/7.7 WER dev/test 19.1/19.2 16.3/16.7 16.0/16.1 Table 3: End-to-End ASR Results on YMC-EXP (corpus with “code-switching”) we cannot conduct HMM-based experiments with either YMC-EXP or YMC-NT corpora. (b) Comparison with Different End-to-End ASR Architectures: We also conducted experiments comparing models with different encoders and decoders on the YMC-EXP corpus. For a Recurrent Neural Network-based (E2E-RNN) model, we followed the best hyper-parameter configuration, as discussed in Zeyer et al. (2018). For a Transformer-based (E2E-Transformer) model, the same configuration from Karita et al. (2019b) was adopted. Both models shared the same data preparation process as the E2E-Conformer model. Table 3 compares different end-to-end ASR architectures on the YMC-EXP corpus.8 The E2E-Conformer obtained the best results, obtaining significant WER improvement as compared to the E2E-RNN and the E2E-Transformer models. The E2E-Conformer’s WER on YMC-EXP(CS) is slightly lower than that obtained for the whole YMC-EXP corpus, despite a significantly smaller training set in the YMC-EXP(-CS) corpus. Sinc"
2021.eacl-main.96,2020.sltu-1.17,0,0.429648,"Missing"
2021.eacl-main.96,2020.lrec-1.353,0,0.410311,"th audio recordings but also through time-coded transcriptions. In a best-case scenario, texts are presented in interlinear format with aligned parses and glosses along with a free translation (Anastasopoulos and Chiang, 2017). But interlinear transcriptions are difficult to produce in meaningful quantities: (1) ELs often lack a standardized orthography (if written at all); (2) invariably, few speakers can accurately transcribe recordings. Even a highly skilled native speaker or linguist will require a minimum of 30 to 50 hours to simply transcribe one hour of recording (Michaud et al., 2014; Zahrer et al., 2020). Additional time is needed for parse, gloss, and translation. This creates what has been called a “transcription bottleneck”, a situation in which the expert transcribers cannot keep up with the amount of recorded material for documentation. “Transcription bottlenecks”, created by a shortage of effective human transcribers are one of the main challenges to endangered language (EL) documentation. Automatic speech recognition (ASR) has been suggested as a tool to overcome such bottlenecks. Following this suggestion, we investigated the effectiveness for EL documentation of end-to-end ASR, which"
2021.emnlp-main.478,W13-2233,0,0.0346701,"vestment (ROI) while avoiding bottlenecks. Recent work has observed that the cross entropy loss of neural language models and other autoregressive generative models scales like a power law in the amount of training data, compute, and number of model parameters over several orders of magnitude (Hestness et al., 2019; Kaplan et al., 2020; Jared Kaplan Johns Hopkins University jaredk@jhu.edu Henighan et al., 2020). Similar intuitions exist in the realm of supervised MT: doubling the amount of parallel training data leads to roughly a fixed improvement in BLEU in both phrase-based statistical MT (Irvine and Callison-Burch, 2013; Turchi et al., 2008) and neural MT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). In Section 2, we show that these MT intuitions can be quantified and explained via cross-entropy power law scaling; using a handful of experiments on small subsets of MT datasets, we precisely predict the performance of large systems trained on orders of magnitude more data. In Section 3, we demonstrate how these trends might be utilized to make ROI predictions when annotating more data for low-resource language pairs. 2 Machine Translation Scaling Laws To investigate the predictability of MT system perfo"
2021.emnlp-main.478,W17-3204,0,0.0214796,"hat the cross entropy loss of neural language models and other autoregressive generative models scales like a power law in the amount of training data, compute, and number of model parameters over several orders of magnitude (Hestness et al., 2019; Kaplan et al., 2020; Jared Kaplan Johns Hopkins University jaredk@jhu.edu Henighan et al., 2020). Similar intuitions exist in the realm of supervised MT: doubling the amount of parallel training data leads to roughly a fixed improvement in BLEU in both phrase-based statistical MT (Irvine and Callison-Burch, 2013; Turchi et al., 2008) and neural MT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). In Section 2, we show that these MT intuitions can be quantified and explained via cross-entropy power law scaling; using a handful of experiments on small subsets of MT datasets, we precisely predict the performance of large systems trained on orders of magnitude more data. In Section 3, we demonstrate how these trends might be utilized to make ROI predictions when annotating more data for low-resource language pairs. 2 Machine Translation Scaling Laws To investigate the predictability of MT system performance as parameters/data increase, we train many Transformer"
2021.emnlp-main.478,L16-1147,0,0.0280027,"Missing"
2021.emnlp-main.478,P02-1040,0,0.10938,"best-fit parameters are shown in Table 1; they are remarkably similar for each language pair, which may be attributed to the similarity of the domains each dataset was sourced from. 5916 Figure 2: (Left) BLEU exponentially decays as crossentropy increases. Different language pairs may have different exponents and constants. (Right) A retrospective analysis of Zhang and Duh (2020) (Appendix A). Even the same development dataset with different BPE applied may have a different constant multiplier. BLEU score, which is more interpretable to humans as a measure of adequacy, fidelity, and fluency (Papineni et al., 2002). Figure 2 shows that the relationship between BLEU and cross-entropy can vary between different language pairs and BPE settings. However, when these factors are fixed, BLEU seems to exponentially increase as crossentropy decreases: BLEU(L) ≈ Ce−kL (3) This relationship is fairly predictable for high BLEU values, but becomes noisier as BLEU drops below 15. Notably, changing the BPE encoding does not seem to affect k, but does change the multiplying constant C.6 Why should this relationship be exponential? We might gain some insight by re-writing Equation 3 in terms of the per-token perplexity"
2021.emnlp-main.478,W18-6319,0,0.0124941,"e insight by re-writing Equation 3 in terms of the per-token perplexity (P): BLEU(P ) ≈ C  1 k P (4) where (1/P ) can intuitively be interpreted as the expected unigram precision of an autoregressively sampled translation with the same length as the reference sentence (Manning and Schutze, 1999). This is only intuition, however: in practice, we do not sample translations but decode using beam search, and BLEU combines multiple modified ngram precisions besides unigram precision.7 6 We evaluate BLEU using multi-bleu.perl from the Moses toolkit. De-bpe-ing, de-tokenizing, and using Sacrebleu (Post, 2018) adds a small amount of noise but does not qualitatively change our results. See Appendix Figure 5. 7 The relationship between precision and perplexity for higher values of n is not clear. In general, expected bigram precision 6= (1/P )2 . Figure 3: (Left) Models trained on <5 MB of data (around 40k lines) fall off-trend when using a BPE vocabulary of 30k, plateauing to an apparent maximum cross-entropy. (Right) When encoding the same dataset with a BPE of 2k, the plateau is rectified and returns to power-law scaling. Similar plots are shown for ru-en and zh-en in Appendix Figure 7. 2.3 Preven"
2021.emnlp-main.478,P19-1021,0,0.0186288,"s of neural language models and other autoregressive generative models scales like a power law in the amount of training data, compute, and number of model parameters over several orders of magnitude (Hestness et al., 2019; Kaplan et al., 2020; Jared Kaplan Johns Hopkins University jaredk@jhu.edu Henighan et al., 2020). Similar intuitions exist in the realm of supervised MT: doubling the amount of parallel training data leads to roughly a fixed improvement in BLEU in both phrase-based statistical MT (Irvine and Callison-Burch, 2013; Turchi et al., 2008) and neural MT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). In Section 2, we show that these MT intuitions can be quantified and explained via cross-entropy power law scaling; using a handful of experiments on small subsets of MT datasets, we precisely predict the performance of large systems trained on orders of magnitude more data. In Section 3, we demonstrate how these trends might be utilized to make ROI predictions when annotating more data for low-resource language pairs. 2 Machine Translation Scaling Laws To investigate the predictability of MT system performance as parameters/data increase, we train many Transformers of various sizes (Table 2"
2021.emnlp-main.478,L16-1559,0,0.0284043,"Missing"
2021.emnlp-main.478,W08-0305,0,0.132636,"Missing"
2021.emnlp-main.478,2020.clssts-1.2,0,0.0416407,"Missing"
2021.emnlp-main.478,2020.tacl-1.26,1,0.755245,"ressive modeling. Power law scaling can arise in complex systems for a variety of reasons (Hanel et al., 2018); Sharma and Kaplan (2020) suggest that scaling exponents may be related to the intrinsic dimension of the data manifold. 5 The best-fit parameters are shown in Table 1; they are remarkably similar for each language pair, which may be attributed to the similarity of the domains each dataset was sourced from. 5916 Figure 2: (Left) BLEU exponentially decays as crossentropy increases. Different language pairs may have different exponents and constants. (Right) A retrospective analysis of Zhang and Duh (2020) (Appendix A). Even the same development dataset with different BPE applied may have a different constant multiplier. BLEU score, which is more interpretable to humans as a measure of adequacy, fidelity, and fluency (Papineni et al., 2002). Figure 2 shows that the relationship between BLEU and cross-entropy can vary between different language pairs and BPE settings. However, when these factors are fixed, BLEU seems to exponentially increase as crossentropy decreases: BLEU(L) ≈ Ce−kL (3) This relationship is fairly predictable for high BLEU values, but becomes noisier as BLEU drops below 15. No"
2021.findings-acl.353,I13-1112,0,0.103983,"or language preservation and revitalization, where models can help coin neologisms for modern terms. Owing to recent successes of machine translation models for similar tasks (Tsvetkov and Dyer, 2015; Gorman et al., 2020; Wu and Yarowsky, 2020a,b), this paper investigates the application of neural sequence-to-sequence models for the task of etymology prediction. Specifically, we focus on word borrowings, where a word enters a language via a non-related donor language.1 Whereas inherited words and cognates tend to follow regular sound shifts and can be modeled well with transliteration models (Beinborn et al., 2013; Wu and Yarowsky, 2018b), words borrowed from unrelated languages undergo various processes (Section 3) that may not preserve the structure or phonetics of the original word. We propose to model borrowings in two tasks (Figure 1), motivated in Section 4. In Task 1, given a donor word and etymological relation, can we predict the form of the incorporated word in the borrowing language? In the opposite direction, in 1 This is in contrast to other etymological relations, such as inheritance, where words enter through a related language, e.g. from Latin to French. 4032 Findings of the Association"
2021.findings-acl.353,P16-1038,0,0.0291965,"Similar approaches have also been applied to the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology information from the English edition of Wiktionary using Yawipa (Wu and Yarowsky, 2020a), a recent Wiktionary parser. We focus on six specific types of borrowings (whose Wiktionary label is in monospaced font below) across a spectrum of semantic and phonetic fidelity: • calque: Also called a loan translation. Components of the original word are literally translated into the target language, e.g. the English brainwash, from the Chinese 洗脑 xi ‘wash’ + nao ‘brain’. • partial calque: A calque where not every component is"
2021.findings-acl.353,2020.sigmorphon-1.2,0,0.0446945,"Missing"
2021.findings-acl.353,2020.amta-research.9,0,0.032981,"sed voting procedure that combines the output of the LSTM-sep, LSTM, and TF models. Each model gives 5 votes for their top prediction, 4 votes for their second place prediction, and so on (1 vote for fifth place). For each test instance, the votes are tallied up, and the prediction with the highest number of votes is the prediction of the ensemble. Ties are broken by picking the prediction with the highest model decoder score among all the models. For Task 2, we experiment with a baseline LSTM model and the same model with copy attention. All models were trained using the OpenNMTpy framework (Klein et al., 2020). The LSTM models are two-layer encoder-decoders with 500dimension hidden state, trained with the ADAM optimizer. The Transformer model has a 6-layer encoder and decoder with 8 heads, trained with ADAM with learning rate scheduling. For reproducibility, we provide the training scripts which include the full model details. Accounting for the extreme imbalance in our dataset, we performed a stratified split of the dataset into a 80-10-10 traindev-test split, where each split contains the same proportion of languages and borrowing relations. 6 To tackle these two tasks, we employ character neural"
2021.findings-acl.353,D18-2012,0,0.0238587,"sk is to reconstruct these missing edges. As Wiktionary is a humanannotated resource, there is much variance in the quality and completeness of annotations, and good performance on this task can help fill in etymology even in high-resource languages like English. 5 Experiments In: abe k a b i j Out: eng c a b b a g e bor For Task 1, we experiment with separate LSTM models trained for each borrowing relation (LSTMsep), a single multi-task LSTM model trained on the combined data (LSTM), the same model trained with both the source and target data preprocessed by the unigram SentencePiece method (Kudo and Richardson, 2018) with a vocabulary size of 4000 (LSTM-spm), the same model with copy attention (See et al., 2017) (LSTM-copy), a Transformer (Vaswani et al., 2017) model (TF), and an ensembling method (Ensemble). This method is a scorebased voting procedure that combines the output of the LSTM-sep, LSTM, and TF models. Each model gives 5 votes for their top prediction, 4 votes for their second place prediction, and so on (1 vote for fifth place). For each test instance, the votes are tallied up, and the prediction with the highest number of votes is the prediction of the ensemble. Ties are broken by picking t"
2021.findings-acl.353,2020.coling-main.387,1,0.754464,"the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology information from the English edition of Wiktionary using Yawipa (Wu and Yarowsky, 2020a), a recent Wiktionary parser. We focus on six specific types of borrowings (whose Wiktionary label is in monospaced font below) across a spectrum of semantic and phonetic fidelity: • calque: Also called a loan translation. Components of the original word are literally translated into the target language, e.g. the English brainwash, from the Chinese 洗脑 xi ‘wash’ + nao ‘brain’. • partial calque: A calque where not every component is translated, e.g. the English apple strudel"
2021.findings-acl.353,2021.naacl-main.353,0,0.0330816,"ata from Wiktionary indicate that modeling borrowings is a challenging task with much room for future research. 2 eng lat fra spa ara san grc deu rus ita Related Work Though the tasks defined in this paper are new, there are several related threads of work. In the task of cognate transliteration, a system is trained to generate cognates in a different language (Beinborn et al., 2013; Wu and Yarowsky, 2018b). This paper uses a multilingual cognate transliteration approach applied specifically to borrowings. Similar approaches have also been applied to the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology informatio"
2021.findings-acl.353,W18-6319,0,0.0126303,"dicting the incorporated word, the input is a sequence containing: the donor language, each character of the donor word, the etymological relation, and the target language. The output is the characters of the incorporated word. In: eng c a b b a g e bor abe Out: k a b i j For Task 2, the input is a sequence containing the word’s language and each character of the word, while the output is the donor language, donor word characters, and relation. 6.1 Results and Analysis Task 1 We evaluate each model on a held-out 15,288 example test set. Table 2 presents character BLEU (computed with SacreBLEU Post (2018)) as well as accuracy and character edit distance from the gold (CED). We also report 5-best results for accuracy (was the correct answer in the top 5 results?) and CED (within the top 5 results, what is the minimum edit distance to the correct answer?) At a cursory glance, the single models trained on all the data performs slightly better compared to the separate relation-specific models, following a 4034 Model BLEU Acc CED 5Acc 5CED LSTM-sep 53.77 20.00 2.42 33.51 1.82 LSTM LSTM-copy LSTM-spm Transformer 55.83 55.90 45.62 61.30 21.43 19.92 10.68 22.19 2.31 2.32 2.85 2.06 34.98 34.46 20.31 41"
2021.findings-acl.353,P17-1099,0,0.0310502,"ce in the quality and completeness of annotations, and good performance on this task can help fill in etymology even in high-resource languages like English. 5 Experiments In: abe k a b i j Out: eng c a b b a g e bor For Task 1, we experiment with separate LSTM models trained for each borrowing relation (LSTMsep), a single multi-task LSTM model trained on the combined data (LSTM), the same model trained with both the source and target data preprocessed by the unigram SentencePiece method (Kudo and Richardson, 2018) with a vocabulary size of 4000 (LSTM-spm), the same model with copy attention (See et al., 2017) (LSTM-copy), a Transformer (Vaswani et al., 2017) model (TF), and an ensembling method (Ensemble). This method is a scorebased voting procedure that combines the output of the LSTM-sep, LSTM, and TF models. Each model gives 5 votes for their top prediction, 4 votes for their second place prediction, and so on (1 vote for fifth place). For each test instance, the votes are tallied up, and the prediction with the highest number of votes is the prediction of the ensemble. Ties are broken by picking the prediction with the highest model decoder score among all the models. For Task 2, we experimen"
2021.findings-acl.353,P15-2021,0,0.0251483,"am applications. Perhaps the most salient is lexicon expansion: more comprehensive dictionaries will enable better communication between cultures as well as better training material for machine translation systems. Computational etymology is also important for historical linguistics, whose focus is on discovering the relationships between languages and their words. An accurate model of word borrowing can also be a boon for language preservation and revitalization, where models can help coin neologisms for modern terms. Owing to recent successes of machine translation models for similar tasks (Tsvetkov and Dyer, 2015; Gorman et al., 2020; Wu and Yarowsky, 2020a,b), this paper investigates the application of neural sequence-to-sequence models for the task of etymology prediction. Specifically, we focus on word borrowings, where a word enters a language via a non-related donor language.1 Whereas inherited words and cognates tend to follow regular sound shifts and can be modeled well with transliteration models (Beinborn et al., 2013; Wu and Yarowsky, 2018b), words borrowed from unrelated languages undergo various processes (Section 3) that may not preserve the structure or phonetics of the original word. We"
2021.findings-acl.353,N12-1039,0,0.0350715,"of cognate transliteration, a system is trained to generate cognates in a different language (Beinborn et al., 2013; Wu and Yarowsky, 2018b). This paper uses a multilingual cognate transliteration approach applied specifically to borrowings. Similar approaches have also been applied to the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology information from the English edition of Wiktionary using Yawipa (Wu and Yarowsky, 2020a), a recent Wiktionary parser. We focus on six specific types of borrowings (whose Wiktionary label is in monospaced font below) across a spectrum of semantic and phonetic fidelity: • calque: Also"
2021.findings-acl.353,L18-1263,1,0.84961,", a system is trained to generate cognates in a different language (Beinborn et al., 2013; Wu and Yarowsky, 2018b). This paper uses a multilingual cognate transliteration approach applied specifically to borrowings. Similar approaches have also been applied to the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology information from the English edition of Wiktionary using Yawipa (Wu and Yarowsky, 2020a), a recent Wiktionary parser. We focus on six specific types of borrowings (whose Wiktionary label is in monospaced font below) across a spectrum of semantic and phonetic fidelity: • calque: Also called a loan tra"
2021.findings-acl.353,L18-1150,1,0.909102,"n and revitalization, where models can help coin neologisms for modern terms. Owing to recent successes of machine translation models for similar tasks (Tsvetkov and Dyer, 2015; Gorman et al., 2020; Wu and Yarowsky, 2020a,b), this paper investigates the application of neural sequence-to-sequence models for the task of etymology prediction. Specifically, we focus on word borrowings, where a word enters a language via a non-related donor language.1 Whereas inherited words and cognates tend to follow regular sound shifts and can be modeled well with transliteration models (Beinborn et al., 2013; Wu and Yarowsky, 2018b), words borrowed from unrelated languages undergo various processes (Section 3) that may not preserve the structure or phonetics of the original word. We propose to model borrowings in two tasks (Figure 1), motivated in Section 4. In Task 1, given a donor word and etymological relation, can we predict the form of the incorporated word in the borrowing language? In the opposite direction, in 1 This is in contrast to other etymological relations, such as inheritance, where words enter through a related language, e.g. from Latin to French. 4032 Findings of the Association for Computational Ling"
2021.findings-acl.353,L18-1538,1,0.92552,"n and revitalization, where models can help coin neologisms for modern terms. Owing to recent successes of machine translation models for similar tasks (Tsvetkov and Dyer, 2015; Gorman et al., 2020; Wu and Yarowsky, 2020a,b), this paper investigates the application of neural sequence-to-sequence models for the task of etymology prediction. Specifically, we focus on word borrowings, where a word enters a language via a non-related donor language.1 Whereas inherited words and cognates tend to follow regular sound shifts and can be modeled well with transliteration models (Beinborn et al., 2013; Wu and Yarowsky, 2018b), words borrowed from unrelated languages undergo various processes (Section 3) that may not preserve the structure or phonetics of the original word. We propose to model borrowings in two tasks (Figure 1), motivated in Section 4. In Task 1, given a donor word and etymological relation, can we predict the form of the incorporated word in the borrowing language? In the opposite direction, in 1 This is in contrast to other etymological relations, such as inheritance, where words enter through a related language, e.g. from Latin to French. 4032 Findings of the Association for Computational Ling"
2021.findings-acl.353,2020.lrec-1.397,1,0.871464,"task is in green and orange, respectively. Introduction Words are borrowed into a language through various processes. For example, the English internet was incorporated into Welsh as rhyngrwyd (rhyng‘between’ + rhwyd ‘net’) through a calqueing process where each component is translated literally. In contrast, the English chimpanzee became the Welsh tsimpansˆı through a process of sound correspondences. Borrowing is prevalent across the world’s languages, and modeling how and from where words enter a language are interesting but understudied tasks under the umbrella of computational etymology (Wu and Yarowsky, 2020a). This is a relatively new field with many downstream applications. Perhaps the most salient is lexicon expansion: more comprehensive dictionaries will enable better communication between cultures as well as better training material for machine translation systems. Computational etymology is also important for historical linguistics, whose focus is on discovering the relationships between languages and their words. An accurate model of word borrowing can also be a boon for language preservation and revitalization, where models can help coin neologisms for modern terms. Owing to recent succes"
2021.findings-acl.353,2020.coling-main.413,1,0.892907,"task is in green and orange, respectively. Introduction Words are borrowed into a language through various processes. For example, the English internet was incorporated into Welsh as rhyngrwyd (rhyng‘between’ + rhwyd ‘net’) through a calqueing process where each component is translated literally. In contrast, the English chimpanzee became the Welsh tsimpansˆı through a process of sound correspondences. Borrowing is prevalent across the world’s languages, and modeling how and from where words enter a language are interesting but understudied tasks under the umbrella of computational etymology (Wu and Yarowsky, 2020a). This is a relatively new field with many downstream applications. Perhaps the most salient is lexicon expansion: more comprehensive dictionaries will enable better communication between cultures as well as better training material for machine translation systems. Computational etymology is also important for historical linguistics, whose focus is on discovering the relationships between languages and their words. An accurate model of word borrowing can also be a boon for language preservation and revitalization, where models can help coin neologisms for modern terms. Owing to recent succes"
2021.findings-emnlp.64,D18-1214,0,0.0169699,"ions that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Work similarity matrices, iteratively extract cliques, and Matc"
2021.findings-emnlp.64,Q17-1010,0,0.089593,"Missing"
2021.findings-emnlp.64,W95-0114,0,0.638172,"paces, and for well-trained embeddings in mismatched domains (e.g., Marchisio et al., 2020). In these cases, SGM might benefit from a different distance metric. A detailed analysis should be performed when data is many-to-many, as translation is naturally a many-to-many task. One might revisit word vectors based on co-occurrence statistics. The size of training and test sets should be increased, as the presence of more synonyms/antonyms and other “distractor"" words may elicit different behavior. There are computational considerations as we scale-up, particularly for SGM. tics. Rapp (1995) and Fung (1995) induce bilingual lexica based on the principle that words that frequently co-occur in one language have translations that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 20"
2021.findings-emnlp.64,P19-1070,0,0.033764,"Missing"
2021.findings-emnlp.64,P19-1143,0,0.062845,"etween points are unchanged by the transform and a closed form solution can be computed by singular value decomposition (Schönemann, 1966). Once languages are mapped to the same space by W, nearest neighbor search finds additional translation pairs. If W is known, one can find translations by optimizing over permutations Π: min ||XW − PY||2F P∈Π (2) P ∈ {0, 1}n×n is permutation matrix that shuffles the rows of Y. If we enforce the 1-to-1 correspondence, this is linear assignment problem that is solvable in polynomial time, e.g. with the Hungarian algorithm (Kuhn, 1955) or Wasserstein methods (Grave et al., 2019a). In the NLP literature, a large number of methods are based on the same underlying idea of linear transform followed by correspondence search/matching (see Related Work). To extract lexicons, one performs nearest neighbor search on the transformed embeddings. To mitigate the hubness problem (where some words are close to too many others) (Radovanovic et al., 2010; Suzuki et al., 2013), Conneau et al. (2018) modifies the similarity using cross-domain similarity local scaling (CSLS) to penalize hubs. For x, y in embedding space V : CSLS(x, y) = 2 cos(x, y) − avg(x, k) − avg(y, k) avg(v, k) ="
2021.findings-emnlp.64,P08-1088,0,0.0348171,"rsus transform that maps the spaces. The graph-based graph-based approaches to BLI under differview works with graphs for each language and ing data conditions and show that they comdirectly performs matching on edge pairs based plement each other when combined. We reon neighborhood information. This view is exemlease our code at https://github.com/ plified by graph matching methods that solve the kellymarchisio/euc-v-graph-bli. quadratic assignment problem from the combina1 Introduction torial optimization literature. Ruder et al. (2018) Bilingual lexicons are useful in many natural lan- and Haghighi et al. (2008) incorporate related techguage processing tasks including constrained de- niques for bilingual lexicon induction. We use coding in machine translation, cross-lingual infor- Seeded Graph Matching (SGM; Fishkind et al., mation retrieval, and unsupervised machine trans- 2019) as representative of this class of approach. lation. There is a large literature inducing bilin- Figure 1 illustrates the differences between the gual lexicons from cross-lingual spaces. “Map- framings; while they both exploit the idea that words with similar neighbors (in Euclidean or ping"" methods based on solving the orth"
2021.findings-emnlp.64,Q19-1007,0,0.0167937,"ne language have translations that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Work similarity matrices, it"
2021.findings-emnlp.64,D18-1330,0,0.0123565,"quently co-occur in one language have translations that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Wor"
2021.findings-emnlp.64,2020.wmt-1.68,1,0.865857,"with or construct two graphs and try to match vertices such that neighborhood structure is preserved. Intuitively, the motivation of preserving neighborhood structure is the same as Procrustes methods, but the absence of linear transform W is an important distinction that potentially makes graph matching more flexible. Indeed, some recent BLI work argue against linear transforms (Mohiuddin et al., 2020) and discuss the failure modes due to lack of isometry (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Glavaš et al., 2019; Vuli´c et al., 2019; Patra et al., 2019; Marchisio et al., 2020). For BLI, we may build the graphs as Gx = XXT and Gy = YYT . For standard graph matching objectives, we restrict the vocabularies of X and Y to equal size, thus Gx , Gy ∈ Rn×n . We find the optimal relabeling of nodes such that: min ||Gx − PGy PT ||2F P∈Π (3) This is an instance of the quadratic assignment problem and is much harder than Eq. 2. It is NPHard (Sahni and Gonzalez, 1976) but various approximation methods exist. Vogelstein et al. (2015) use the Frank-Wolfe method (Frank et al., 1956) to find an approximate doubly-stochastic solution, then project onto the space of permutation matr"
2021.findings-emnlp.64,2020.emnlp-main.215,0,0.0121005,"tion, network science, and computer vision, there exist a large body of related work termed “graph matching."" Rather than assuming the existence of a linear transform between the embedding spaces, these methods start with or construct two graphs and try to match vertices such that neighborhood structure is preserved. Intuitively, the motivation of preserving neighborhood structure is the same as Procrustes methods, but the absence of linear transform W is an important distinction that potentially makes graph matching more flexible. Indeed, some recent BLI work argue against linear transforms (Mohiuddin et al., 2020) and discuss the failure modes due to lack of isometry (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Glavaš et al., 2019; Vuli´c et al., 2019; Patra et al., 2019; Marchisio et al., 2020). For BLI, we may build the graphs as Gx = XXT and Gy = YYT . For standard graph matching objectives, we restrict the vocabularies of X and Y to equal size, thus Gx , Gy ∈ Rn×n . We find the optimal relabeling of nodes such that: min ||Gx − PGy PT ||2F P∈Π (3) This is an instance of the quadratic assignment problem and is much harder than Eq. 2. It is NPHard (Sahni and Gonzalez, 19"
2021.findings-emnlp.64,P18-2036,0,0.0214469,"tching."" Rather than assuming the existence of a linear transform between the embedding spaces, these methods start with or construct two graphs and try to match vertices such that neighborhood structure is preserved. Intuitively, the motivation of preserving neighborhood structure is the same as Procrustes methods, but the absence of linear transform W is an important distinction that potentially makes graph matching more flexible. Indeed, some recent BLI work argue against linear transforms (Mohiuddin et al., 2020) and discuss the failure modes due to lack of isometry (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Glavaš et al., 2019; Vuli´c et al., 2019; Patra et al., 2019; Marchisio et al., 2020). For BLI, we may build the graphs as Gx = XXT and Gy = YYT . For standard graph matching objectives, we restrict the vocabularies of X and Y to equal size, thus Gx , Gy ∈ Rn×n . We find the optimal relabeling of nodes such that: min ||Gx − PGy PT ||2F P∈Π (3) This is an instance of the quadratic assignment problem and is much harder than Eq. 2. It is NPHard (Sahni and Gonzalez, 1976) but various approximation methods exist. Vogelstein et al. (2015) use the Frank-Wolfe method (Frank e"
2021.findings-emnlp.64,P19-1492,0,0.0191284,"the existence of a linear transform between the embedding spaces, these methods start with or construct two graphs and try to match vertices such that neighborhood structure is preserved. Intuitively, the motivation of preserving neighborhood structure is the same as Procrustes methods, but the absence of linear transform W is an important distinction that potentially makes graph matching more flexible. Indeed, some recent BLI work argue against linear transforms (Mohiuddin et al., 2020) and discuss the failure modes due to lack of isometry (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Glavaš et al., 2019; Vuli´c et al., 2019; Patra et al., 2019; Marchisio et al., 2020). For BLI, we may build the graphs as Gx = XXT and Gy = YYT . For standard graph matching objectives, we restrict the vocabularies of X and Y to equal size, thus Gx , Gy ∈ Rn×n . We find the optimal relabeling of nodes such that: min ||Gx − PGy PT ||2F P∈Π (3) This is an instance of the quadratic assignment problem and is much harder than Eq. 2. It is NPHard (Sahni and Gonzalez, 1976) but various approximation methods exist. Vogelstein et al. (2015) use the Frank-Wolfe method (Frank et al., 1956) to find an"
2021.findings-emnlp.64,P19-1018,0,0.177662,"arately trained on monolingual data in language Y. We assume seeds {(x1 , y1 ), (x2 , y2 ), ...(xs , ys )} are given, which are supervised labels indicating translation correspondence between vocabulary items in the languages. We sort the corresponding submatrices of X and Y so each row of X ∈ Rs×d and Y ∈ Rs×d corresponds to the seeds. Usually, s is strictly smaller than both n and m and the goal is to find translation correspondences in the remaining words. Procrustes and linear transforms: The popular Procrustes-based methods for BLI (e.g. Artetxe et al., 2016a, 2019; Conneau et al., 2018; Patra et al., 2019) match seeds by calculating a linear transformation W by a variant of the below: min W∈Rd×d ||XW − Y||2F (1) If W is required to be orthogonal, then distances between points are unchanged by the transform and a closed form solution can be computed by singular value decomposition (Schönemann, 1966). Once languages are mapped to the same space by W, nearest neighbor search finds additional translation pairs. If W is known, one can find translations by optimizing over permutations Π: min ||XW − PY||2F P∈Π (2) P ∈ {0, 1}n×n is permutation matrix that shuffles the rows of Y. If we enforce the 1-to-"
2021.findings-emnlp.64,D19-1449,0,0.0381571,"Missing"
2021.findings-emnlp.64,P95-1050,0,0.386726,"word embedding spaces, and for well-trained embeddings in mismatched domains (e.g., Marchisio et al., 2020). In these cases, SGM might benefit from a different distance metric. A detailed analysis should be performed when data is many-to-many, as translation is naturally a many-to-many task. One might revisit word vectors based on co-occurrence statistics. The size of training and test sets should be increased, as the presence of more synonyms/antonyms and other “distractor"" words may elicit different behavior. There are computational considerations as we scale-up, particularly for SGM. tics. Rapp (1995) and Fung (1995) induce bilingual lexica based on the principle that words that frequently co-occur in one language have translations that co-occur frequently in another. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe e"
2021.findings-emnlp.64,2020.emnlp-main.482,0,0.0264729,"Missing"
2021.findings-emnlp.64,2020.acl-main.318,0,0.0276985,"; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Work similarity matrices, iteratively extract cliques, and Matching words using vector representations be- map with Procrustes. Gutierrez-Vasques and Mijangan with vectors based on co-occurrence statis- gos (2017) create a weighted graph of translation 745 candidates then create word vectors with Node2Vec (Grover and Leskovec, 2016). Wushouer et al. (2013) use graphs for a source, target, and pivot language to iteratively extract translation pairs based on heuristics. Our active learning approach is inspired by Yuan et al. (2020). Mikel Artetxe, Gorka Labaka, and Eneko Agi"
2021.findings-emnlp.64,D18-1042,0,0.084923,"and assumes the existence of a linear we study the behavior of Euclidean versus transform that maps the spaces. The graph-based graph-based approaches to BLI under differview works with graphs for each language and ing data conditions and show that they comdirectly performs matching on edge pairs based plement each other when combined. We reon neighborhood information. This view is exemlease our code at https://github.com/ plified by graph matching methods that solve the kellymarchisio/euc-v-graph-bli. quadratic assignment problem from the combina1 Introduction torial optimization literature. Ruder et al. (2018) Bilingual lexicons are useful in many natural lan- and Haghighi et al. (2008) incorporate related techguage processing tasks including constrained de- niques for bilingual lexicon induction. We use coding in machine translation, cross-lingual infor- Seeded Graph Matching (SGM; Fishkind et al., mation retrieval, and unsupervised machine trans- 2019) as representative of this class of approach. lation. There is a large literature inducing bilin- Figure 1 illustrates the differences between the gual lexicons from cross-lingual spaces. “Map- framings; while they both exploit the idea that words w"
2021.findings-emnlp.64,2020.acl-main.201,0,0.0200584,"other. Diab and Finch (2000) extend this by measuring similarity between words based on co-occurrence vectors and matching words across language by preserving these similarities. Mikolov et al. (2013) are the first to perform BLI over word embeddings, estimating the transformation matrix using stochastic gradient descent. Most recent work solves a variation of the generalized Procrustes problem (e.g., Conneau et al., 2018; Artetxe et al., 2016b, 2017; Patra et al., 2019; Artetxe et al., 2018b; Doval et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019; Alvarez-Melis and Jaakkola, 2018). Zhang et al. (2020) learn a mapping that overfits to training pairs thus enforcing “hard-seeding"", while Ruder et al. (2018) enforce a one-to-one constraint on the output for BLI. Some BLI work uses graph based methods implicitly or explicitly. Artetxe et al. (2018a) form an initial solution with similarity matrices and refine with iterative Procrustes. Grave et al. (2019b) optimize “Procrustes in Wasserstein Distance"", employing a quadratic assignment formulation and the Frank-Wolfe method. Ren et al. (2020) form CSLS 8 Related Work similarity matrices, iteratively extract cliques, and Matching words using vect"
2021.findings-emnlp.64,P19-1307,0,0.0119624,"ays appear in the solution. This is ideal when one is confident about the quality of the seeds, but means that SGM is not robust to errors in the seed set. 3 Experimental Setup Because there are three methods and results sections, we detail the experimental setup first. We evaluate on English→German (En-De) and Russian→English (Ru-En). Monolingual Word Embeddings We use 300dimensional monolingual word embeddings trained on Wikipedia using fastText (Bojanowski et al., 2017).1 We normalize to unit length, mean-center, and renormalize, following Artetxe et al. (2018a) (“iterative normalization"", Zhang et al. (2019)). Data & Software Bilingual dictionaries from MUSE2 are many-to-many lexicons of the 5000 most-frequent words from the source language, paired with one or more target-side translations. We filter each lexicon to be one-to-one for simplicity of analysis. For source words with multiple target words, we keep the first occurrence. This is equivalent to randomly sampling a target sense for polysemous source words because target words are in arbitrary order. En-De originally contains 14667 pairs, and 4903 remain after filtering. Ru-En has 7452 pairs, reduced to 4084. We use 100-4000 pairs as seeds,"
2021.findings-emnlp.64,P18-1072,0,0.0463968,"Missing"
2021.findings-emnlp.64,D13-1058,0,0.0327119,"that shuffles the rows of Y. If we enforce the 1-to-1 correspondence, this is linear assignment problem that is solvable in polynomial time, e.g. with the Hungarian algorithm (Kuhn, 1955) or Wasserstein methods (Grave et al., 2019a). In the NLP literature, a large number of methods are based on the same underlying idea of linear transform followed by correspondence search/matching (see Related Work). To extract lexicons, one performs nearest neighbor search on the transformed embeddings. To mitigate the hubness problem (where some words are close to too many others) (Radovanovic et al., 2010; Suzuki et al., 2013), Conneau et al. (2018) modifies the similarity using cross-domain similarity local scaling (CSLS) to penalize hubs. For x, y in embedding space V : CSLS(x, y) = 2 cos(x, y) − avg(x, k) − avg(y, k) avg(v, k) = 1 k X cos(vn , v) vn ∈Nk (v,V ) Nk (v, V ) returns the k-nearest-neighbors to v ∈ V by cosine similarity (typically k = 10). 739 Graph matching: In fields such as pattern recognition, network science, and computer vision, there exist a large body of related work termed “graph matching."" Rather than assuming the existence of a linear transform between the embedding spaces, these methods s"
2021.hcinlp-1.14,W18-4104,0,0.0308742,"ircumstances) the sentence could be reasonably uttered” (Kruszewski et al., 2016). If the source text is expected to reflect “ordinary real-life,&quot; the output should be plausible to be believable. MT output may also be unbelievable if it violates commonsense reasoning, a challenging element of Natural Language Understanding (Mostafazadeh et al., 2016). Lack of discourse coherence might likewise signal unbelievable translations. For document generation, improving the consistency of generated documents makes it harder for human subjects to distinguish automatically generated text from real text (Karuna et al., 2018). Grammatical errors are related to fluency, a traditional MT quality evaluation feature. Fluency has been defined as a judgment of “whether the translation reads like good English...without knowing the accuracy of the content,” and is typically combined with adequacy, an assessment of “the degree to which the information in a professional translation can be found in an MT (or control) output of the same text” (White et al., 1994). A user who cannot understand the source cannot judge adequacy, but may use expectations based on features like fluency and reasonableness to guess. Believability co"
2021.hcinlp-1.14,J16-4003,0,0.0138203,". The bottom-left translation is not believable so a monolingual user would not be misled by it. However, the more believable top-left translation might mislead a monolingual user. Because these judgments are based on perception, they may be more subjective than traditional MT DA features. We control for some factors that may affect believability (Section 3), resulting in annotations that are similarly reliable to the DA features (Section 4). 2 sibility can be thought of as, “whether in an ordinary real-life situation (not “fairy-tale” circumstances) the sentence could be reasonably uttered” (Kruszewski et al., 2016). If the source text is expected to reflect “ordinary real-life,&quot; the output should be plausible to be believable. MT output may also be unbelievable if it violates commonsense reasoning, a challenging element of Natural Language Understanding (Mostafazadeh et al., 2016). Lack of discourse coherence might likewise signal unbelievable translations. For document generation, improving the consistency of generated documents makes it harder for human subjects to distinguish automatically generated text from real text (Karuna et al., 2018). Grammatical errors are related to fluency, a traditional MT"
2021.hcinlp-1.14,W19-6623,1,0.846607,"between annotators (see Section 4). We note that factors such as foreign language proficiency may affect believability judgments. Further work with a wider variety of annotators is needed to identify and quantify those effects. MT Systems Because our goal is to examine segments annotated for believability, fluency, and adequacy judgments rather than to compare systems, we need MT that will produce outputs across a range of quality. Output that is inadequate but believable is of particular interest, so we rely on estimates of the distribution of “fluently inadequate” translations on MTTT from Martindale et al. (2019) to inform our choice of models. They estimated that fluently inadequate translations were most frequent in the “general” NMT models, trained on out-of-domain data. We use their Arabic, Farsi, and Korean “general” models to capture the range of training data sizes and output quality we believe will provide interesting examples for our analysis. The training data is 49M, 6.2M, and 1.4M segments in Arabic, Farsi, and Korean, respectively. The systems are built in Sockeye (Hieber et al., 2017) using the ‘SockeyeNMT rm1’ settings from the MTTT leaderboard2 . The resulting systems achieved BLEU (Pa"
2021.hcinlp-1.14,N16-1098,0,0.0241076,"ional MT DA features. We control for some factors that may affect believability (Section 3), resulting in annotations that are similarly reliable to the DA features (Section 4). 2 sibility can be thought of as, “whether in an ordinary real-life situation (not “fairy-tale” circumstances) the sentence could be reasonably uttered” (Kruszewski et al., 2016). If the source text is expected to reflect “ordinary real-life,&quot; the output should be plausible to be believable. MT output may also be unbelievable if it violates commonsense reasoning, a challenging element of Natural Language Understanding (Mostafazadeh et al., 2016). Lack of discourse coherence might likewise signal unbelievable translations. For document generation, improving the consistency of generated documents makes it harder for human subjects to distinguish automatically generated text from real text (Karuna et al., 2018). Grammatical errors are related to fluency, a traditional MT quality evaluation feature. Fluency has been defined as a judgment of “whether the translation reads like good English...without knowing the accuracy of the content,” and is typically combined with adequacy, an assessment of “the degree to which the information in a pro"
2021.hcinlp-1.14,W13-2305,0,0.0346815,"label for each feature. We calculate scores following Bojar et al. (2018). Each annotator’s raw scores are converted to z-scores based on their own mean and standard deviation, and the z-scores for each segment are averaged across annotators. Segments with positive z-scores are labeled TRUE and negative z-scores are labeled FALSE. Annotating Believability To understand the relationships between believability and traditional MT quality criteria (fluency and adequacy), we hired professionals to annotate MT output for these characteristics in tasks based on the fluency and adequacy DA methods of Graham et al. (2013) and Bojar et al. (2016). The annotated data sets are available at: https://github.com/ mjmartindale/mt_believability . Test Data We chose a test set that is comparable across three typologically different languages with different amounts of training data. Our test data comes from The Multi-Target TED Talks Task (MTTT)–a collection of bitexts across 20 languages (Duh, 2018). The test set is fully sentence parallel with original talk transcripts as the English and human translations for the other languages. We use the non-English translations in MTTT as “source” and machine translate into Engli"
2021.hcinlp-1.14,E17-3017,0,0.0276681,"rest, so we rely on estimates of the distribution of “fluently inadequate” translations on MTTT from Martindale et al. (2019) to inform our choice of models. They estimated that fluently inadequate translations were most frequent in the “general” NMT models, trained on out-of-domain data. We use their Arabic, Farsi, and Korean “general” models to capture the range of training data sizes and output quality we believe will provide interesting examples for our analysis. The training data is 49M, 6.2M, and 1.4M segments in Arabic, Farsi, and Korean, respectively. The systems are built in Sockeye (Hieber et al., 2017) using the ‘SockeyeNMT rm1’ settings from the MTTT leaderboard2 . The resulting systems achieved BLEU (Papineni et al., 2002) scores of 26.6 for Arabic, 22.2 for Farsi and 11.6 for Korean. Tasks We followed segment-level DA scoring best practices established by WMT (Barrault et al., 2019). The fluency and adequacy questions were taken directly from WMT16 (Bojar et al., 2016). The believability question uses our definition of believability with an introductory phrase to assure the annotator that we understand that it is not possible to truly evaluate the meaning without the source: “Even withou"
2021.hcinlp-1.14,P02-1040,0,0.109997,"9) to inform our choice of models. They estimated that fluently inadequate translations were most frequent in the “general” NMT models, trained on out-of-domain data. We use their Arabic, Farsi, and Korean “general” models to capture the range of training data sizes and output quality we believe will provide interesting examples for our analysis. The training data is 49M, 6.2M, and 1.4M segments in Arabic, Farsi, and Korean, respectively. The systems are built in Sockeye (Hieber et al., 2017) using the ‘SockeyeNMT rm1’ settings from the MTTT leaderboard2 . The resulting systems achieved BLEU (Papineni et al., 2002) scores of 26.6 for Arabic, 22.2 for Farsi and 11.6 for Korean. Tasks We followed segment-level DA scoring best practices established by WMT (Barrault et al., 2019). The fluency and adequacy questions were taken directly from WMT16 (Bojar et al., 2016). The believability question uses our definition of believability with an introductory phrase to assure the annotator that we understand that it is not possible to truly evaluate the meaning without the source: “Even without having seen the source text, I believe the meaning of this translation is likely to match the meaning of the original.” Ann"
2021.hcinlp-1.14,2009.eamt-1.5,0,0.0369201,"ualitative analysis of examples where believability and fluency judgments disagreed suggests that semantic features can overwhelm grammatical features in believability judgments. A full qualitative analysis of the believabilityannotated examples would suggest features that may have influenced annotator’s judgments and could indicate approaches that may be effective in automatically predicting believability. Believability used alone could enable an adversarial MT system to deliberately mask errors and produce misleading output, but believability predictions combined with MT quality estimation (Specia et al., 2009) could be used to flag potentially misleading output. Because believability is a user-centric metric, gaining a complete understanding would require more user-centric methods. The annotator agreement in our results may indicate that believability is less subjective than one might expect, or it may simply indicate that our annotators were fairly homogeneous. A user study could not only tell us exactly what features were most salient in which contexts, but could indicate whether demographic features such as age or education affect perceptions of believability. Qualitative Analysis Based on infor"
2021.hcinlp-1.14,1994.amta-1.25,0,0.874424,"Missing"
2021.iwslt-1.10,2021.naacl-main.151,1,0.916524,"Speech Translation System Hirofumi Inaguma1∗ Brian Yan2∗ Siddharth Dalmia2 Pengcheng Guo3 Jiatong Shi4 Kevin Duh4 Shinji Watanabe2,4 1 Kyoto University, Japan 2 Carnegie Mellon University, USA 3 Northwestern Polytechnical University, China 4 Johns Hopkins University, USA inaguma@sap.ist.i.kyoto-u.ac.jp byan@cs.cmu.edu Abstract also to encourage future research by building strong systems along with the open-sourced project. This year we focused on (1) sequence-level knowledge distillation (SeqKD) (Kim and Rush, 2016), (2) Conformer encoder (Gulati et al., 2020), (3) Multi-Decoder architecture (Dalmia et al., 2021), (4) model ensembling, and (5) better segmentation with a neural network-based voice activity (VAD) system (Bredin et al., 2020) and a novel algorithm to merge multiple short segments for long context modeling. Our primary focus was E2E models, although we also compared them with cascade systems with our best effort. All experiments were conducted with the ESPnet-ST toolkit (Inaguma et al., 2020), and the recipe is publicly available at https://github.com/espnet/ espnet/tree/master/egs/iwslt21. This paper describes the ESPnet-ST group’s IWSLT 2021 submission in the offline speech translation"
2021.iwslt-1.10,N19-1423,0,0.00781703,"23.5 18.4 16.4 15.1 20.8 15.5 11.5 12.2 23.65 19.40 17.08 17.93 WebRTC – 1500 2000 2500 – 200 200 200 35.3 19.4 19.8 22.9 35.1 26.7 27.7 29.5 44.0 27.7 27.1 27.1 22.7 13.8 11.9 11.6 34.28 21.90 21.63 22.78 pyannote – 1500 1500 2000 2000 2000 2000 – 200 100 200 150 100 50 9.5 8.0 7.5 10.3 9.6 8.1 7.3 24.0 23.0 22.2 22.5 21.8 21.5 21.9 15.5 12.4 12.4 12.2 12.3 12.0 12.4 7.3 7.3 6.5 6.5 6.1 5.8 5.9 14.08 12.68 12.15 12.88 12.45 11.90 11.88 5.2 Table 5: Impact of audio segmentation for ASR coder parameters with those of the Conformer ASR. On the decoder side, we initialized parameters like BERT (Devlin et al., 2019), where weight parameters were sampled from N (0, 0.02), biases were set to zero, and layer normalization parameters were set to β = 0, γ = 1. This technique led to better translation performance and faster convergence. 5 5.1 Results ASR 5.1.1 Architecture We compared Transformer and Conformer ASR architectures in Table 4. We observed that Conformer significantly outperformed Transformer. Therefore, we use the Conformer encoder in the following experiments. 5.1.2 Segmentation Next, we investigated the VAD systems and the proposed segment merging algorithm for long context modeling in Table 5."
2021.iwslt-1.10,N19-1202,0,0.0407366,"Missing"
2021.iwslt-1.10,2021.naacl-main.150,1,0.895402,"). The architecture of each block is depicted in Figure 1. It includes a multi-head self-attention module, a convolution module, and a pair of position-wise feed-forward modules in the Macaron-Net style. While the self-attention module learns the long101 Figure 2: The Multi-Decoder (MD) architecture decomposes the overall ST task with ASR and MT subnets while maintaining E2E differentiability. range global context, the convolution module aims to model the local feature patterns synchronously. Recent studies have shown improvements by introducing Conformer in the E2E-ST task (Guo et al., 2020; Inaguma et al., 2021b), which motivated us to adopt this architecture as our system. 3.2 SeqKD Sequence-level knowledge distillation (SeqKD) (Kim and Rush, 2016) is an effective method to transfer knowledge in a teacher model to a student model via discrete symbols. Our recent studies (Inaguma et al., 2021a,b) showed a large improvement in ST performance with this technique. Unlike the previous studies, however, we used more training data than bitext in ST training data to train teacher MT models. We translated source transcripts in the ST training data by the teacher MT models with a beam width of 5 and then rep"
2021.iwslt-1.10,2020.acl-demos.34,1,0.914984,"h the open-sourced project. This year we focused on (1) sequence-level knowledge distillation (SeqKD) (Kim and Rush, 2016), (2) Conformer encoder (Gulati et al., 2020), (3) Multi-Decoder architecture (Dalmia et al., 2021), (4) model ensembling, and (5) better segmentation with a neural network-based voice activity (VAD) system (Bredin et al., 2020) and a novel algorithm to merge multiple short segments for long context modeling. Our primary focus was E2E models, although we also compared them with cascade systems with our best effort. All experiments were conducted with the ESPnet-ST toolkit (Inaguma et al., 2020), and the recipe is publicly available at https://github.com/espnet/ espnet/tree/master/egs/iwslt21. This paper describes the ESPnet-ST group’s IWSLT 2021 submission in the offline speech translation track. This year we made various efforts on training data, architecture, and audio segmentation. On the data side, we investigated sequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech translation. Specifically, we used multi-referenced SeqKD from multiple teachers trained on different amounts of bitext. On the architecture side, we adopted the Conformer encoder and the Multi-D"
2021.iwslt-1.10,D16-1139,0,0.0244625,"position-wise feed-forward modules in the Macaron-Net style. While the self-attention module learns the long101 Figure 2: The Multi-Decoder (MD) architecture decomposes the overall ST task with ASR and MT subnets while maintaining E2E differentiability. range global context, the convolution module aims to model the local feature patterns synchronously. Recent studies have shown improvements by introducing Conformer in the E2E-ST task (Guo et al., 2020; Inaguma et al., 2021b), which motivated us to adopt this architecture as our system. 3.2 SeqKD Sequence-level knowledge distillation (SeqKD) (Kim and Rush, 2016) is an effective method to transfer knowledge in a teacher model to a student model via discrete symbols. Our recent studies (Inaguma et al., 2021a,b) showed a large improvement in ST performance with this technique. Unlike the previous studies, however, we used more training data than bitext in ST training data to train teacher MT models. We translated source transcripts in the ST training data by the teacher MT models with a beam width of 5 and then replaced the original ground-truth translation with the generated translation. We used cased and punctuated transcripts as inputs to the MT teac"
2021.iwslt-1.10,P07-2045,0,0.0195621,"h translation (Inaguma et al., 2020), machine translation (MT), and speech separation/enhancement (Li et al., 2021). The purpose of this submission is not only to show the recent progress on ST researches, but Data preparation In this section, we describe data preparation for each task. The corpus statistics are listed in Table 1. We removed the off-limit talks following previous evaluation campaigns1 . To fit the GPU memory, we excluded utterances having more than 3000 speech frames or more than 400 characters. All sentences were tokenized with the tokenizer.perl script in the Moses toolkit (Koehn et al., 2007). 2.1 ASR We used Must-C (Di Gangi et al., 2019), Must-C v22 , ST-TED (Jan et al., 2018), Librispeech (Panayotov et al., 2015), and TEDLIUM2 (Rousseau et al., 2012) corpora. We used the cleaned version of STTED following (Inaguma et al., 2019). The speech ∗ *Equal contribution 1 https://sites.google.com/ view/iwslt-evaluation-2019/ speech-translation/off-limit-ted-talks 2 https://ict.fbk.eu/ must-c-release-v2-0/ 100 Proceedings of the 18th International Conference on Spoken Language Translation, pages 100–109 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Lin"
2021.iwslt-1.10,P12-3005,0,0.0141132,"ust-C, Must-C v2, and ST-TED only. The shared source and target vocabulary of BPE16k units was constructed using cased and punctuated transcripts and translations. 2.3 MT We used available bitext for WMT204 in addition to the in-domain TED data used for E2E-ST systems. We first performed perplexity-based filtering with an in-domain n-gram language model (LM) (Moore and Lewis, 2010). We controlled the WMT data size by thresholding and obtained three data pools: 5M, 10M, and 20M sentences. Next, we removed non-printing characters and performed language identification with the langid.py toolkit (Lui and Baldwin, 2012)5 and kept sentences whose lan3 https://github.com/google/ sentencepiece 4 Europarl, Commoncrawl, Paracrawl, NewsCommentary, WikiTitles, RAPID and WikiMatrix 5 https://github.com/saffsd/langid.py 3 3.1 System Conformer encoder Conformer encoder (Gulati et al., 2020) is a stacked multi-block architecture and has shown consistent improvement over a wide range of E2E speech processing applications (Guo et al., 2020). The architecture of each block is depicted in Figure 1. It includes a multi-head self-attention module, a convolution module, and a pair of position-wise feed-forward modules in the"
2021.iwslt-1.10,P10-2041,0,0.0237682,"rmed over the WMT data only. For each data size, the joint source and target vocabulary of BPE32k units was constructed using cased and punctuated sentences after the filtering. We did not use additional monolingual data. E2E-ST We used Must-C, Must-C v2, and ST-TED only. The shared source and target vocabulary of BPE16k units was constructed using cased and punctuated transcripts and translations. 2.3 MT We used available bitext for WMT204 in addition to the in-domain TED data used for E2E-ST systems. We first performed perplexity-based filtering with an in-domain n-gram language model (LM) (Moore and Lewis, 2010). We controlled the WMT data size by thresholding and obtained three data pools: 5M, 10M, and 20M sentences. Next, we removed non-printing characters and performed language identification with the langid.py toolkit (Lui and Baldwin, 2012)5 and kept sentences whose lan3 https://github.com/google/ sentencepiece 4 Europarl, Commoncrawl, Paracrawl, NewsCommentary, WikiTitles, RAPID and WikiMatrix 5 https://github.com/saffsd/langid.py 3 3.1 System Conformer encoder Conformer encoder (Gulati et al., 2020) is a stacked multi-block architecture and has shown consistent improvement over a wide range of"
2021.iwslt-1.10,P02-1040,0,0.109478,"ontext modeling in Table 5. We used the same decoding hyperparameters tuned on Must-C. We firstly observed that merging short segments was very effective probably because it alleviated frame classification errors in the VAD systems. Among three audio segmentation methods, we confirmed that pyannote.audio significantly reduced the WER while WebRTC had negative impacts compared to the provided segmentation. Specifically, we found that MT In this section, we show the results of our MT systems used for cascade systems and pseudo labeling in SeqKD. We report case-sensitive detokenized BLEU scores (Papineni et al., 2002) with the multi-bleu-detok.perl script in Moses. We carefully investigated the effective amount of WMT training data to improve the performance of the TED domain. The results are shown in Table 6. We confirmed that adding the WMT data improved the performance by more than 4 BLEU. Regarding the WMT data size, using up to 10M sentences was helpful, but 20M did not show clear improvements, probably because of the undersampling of the TED data. Oversampling as in multilingual NMT (Arivazhagan et al., 2019) could alleviate this problem, but this is beyond our scope. After training with a mix of the"
2021.iwslt-1.10,2020.iwslt-1.9,0,0.476132,"erior probability combination to ensemble models trained with different data and architectures. During inference, we perform a posterior combination at each step of beam search decoding by first computing the softmax normalized posterior probabilities for each model in the ensemble and then taking the mean value. In this ensembling approach, a single unified beam search operates over the combined posteriors of the models to find the most likely decoded sequence. 3.5 Segmentation How to segment audio during inference significantly impacts ST performances (Gaido et al., 2020; Pham et al., 2020; Potapczyk and Przybysz, 2020; Gaido et al., 2021). This is because the ST systems are usually trained with utterances segmented based on punctuation marks (Di Gangi et al., 2019) while the audio segmentation by voice activity detection (VAD) at test time does not access such meta information. Since VAD splits a long speech recording into chunks by silence regions, it would prevent models from extracting semantically coherent contextual information. Therefore, it is very important to seek a better segmentation strategy in order to minimize this gap in training and test conditions and evaluate models correctly. In fact, th"
2021.iwslt-1.10,rousseau-etal-2012-ted,0,0.0859205,"Missing"
2021.iwslt-1.10,P16-1162,0,0.135485,"Missing"
2021.iwslt-1.25,P05-1066,0,0.175046,"Missing"
2021.iwslt-1.25,W19-5314,1,0.354252,"to the target language, which is assumed to be able to recover the ground truth translations accurately (Liu et al., 2021). However, if we perform inference on the training set, many of the predictions are inconsistent with the references. It reflects the distribution shift between the NMT model leaned distribution and the empirical distribution of training corpus, as Figure 1 illustrated. For a training example, a high recovery degree between prediction and ground-truth target sentence means it’s easier to be mastered by the NMT model while a lower recovery degree means it’s more difficult (Ding and Tao, 2019; Wu et al., 2020b). To this end, we employ this recovery degree as the difficulty criterion, where the recovery degree is computed by the sentence-level BLEU. We put forward an analogy of this method that humans can schedule a personal and effective curriculum after skimming over a textbook, namely self-guided curriculum. In this work, we cast the recovery degree of each training example as its learning difficulty, enforcing the NMT model to learn from examples with higher recovery degrees to those with lower degrees. Also, we implement our proposed recovery-based difficulty criterion with fi"
2021.iwslt-1.25,2020.acl-main.153,1,0.392938,"d through a independent word embedding model. In the case of Liu et al. (2020), the norm of word vector on the source side is used as the difficulty criterion. They also use the CDF function to assure the difficulty scores are within [0, 1]. l(z n ;θk )−l(z n ;θk−1 ) , l(z n ;θk−1 ) n n k k − log P (y |x ; θ ), where θ repreNMT Model d(z n ; θk ) = l(z n ; θk ) = sents the NMT model parameters at the kth training phase. The decline of loss is defined as the difficulty criterion in Xu et al. (2020). Besides, the score of cross-lingual patterns may also be a proper difficulty criterion for NMT (Ding et al., 2020a; Zhou et al., 2020a; Wu et al., 2021), which we leave as the future work. We now turn to CURRICULUM SCHEDULING. There are two controlling factors, extraction of training set and training phase duration. In other words, how to split training corpus into subsets and when to load them. Given K mutual exclusive subsets {D1 , . . . , DK } ⊆ D, there are two general regimens loading them as training progresses: one pass and baby steps. In one pass regimen, k subsets Dk are loaded as training set one by one, while in baby steps regimen, these subsets are merged into the current training set one by"
2021.iwslt-1.25,2020.coling-main.389,1,0.399448,"Missing"
2021.iwslt-1.25,2020.emnlp-main.475,0,0.0895643,"t works choose to derive difficulty criteria based on the probability distribution of training examples to approximate the perspective of NMT models. For instance, Platanios et al. (2019) turn discrete numerical difficulty scores into relative probabilities and then construct 206 Proceedings of the 18th International Conference on Spoken Language Translation, pages 206–214 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics the difficulty criterion, while others derive difficulty criterion from independently trained language model (Zhang et al., 2019; Dou et al., 2020; Liu et al., 2020) and word embedding model (Zhou et al., 2020b). Xu et al. (2020) derive difficulty criterion from the NMT model in the training process. And these difficulty criteria are applied to either fixed curriculum schedule (Cirik et al., 2016) or dynamic one (Platanios et al., 2019; Liu et al., 2020; Xu et al., 2020; Zhou et al., 2020b). A well-trained NMT model estimates the optimal probability distribution mapping from the source language to the target language, which is assumed to be able to recover the ground truth translations accurately (Liu et al., 2021). However, if we perfo"
2021.iwslt-1.25,2020.emnlp-main.202,0,0.0623223,"Missing"
2021.iwslt-1.25,P16-1162,0,0.0611772,"n-De) and WMT’17 Chinese⇒English (Zh-En). For En-De, the training set consists of 4.5 million sentence pairs. We use newstest2012 as the validation set and report test results on both newstest2014 and newtest2016 for fair comparison with existing approaches. For Zh-En, we follow (Hassan et al., 2018) to extract 20 million sentence pairs as the training set. We use newsdev2017 as the validation set and newstest2017 as the test set. Chinese sentences are segmented with a word segmentation toolkit Jieba1 . Sentences in other languages are tokenized with Moses2 . We learn Byte-Pair Encoding(BPE) (Sennrich et al., 2016) with 32k merge operations. And we learn BPE with a shared vocabulary for En-De. We use BLEU (Papineni et al., 2002) as the automatic metrics for computing recovery degree and evaluating model performance with statistical significance test (Collins et al., 2005). (7) If oc &gt; ov , training phase will progress to the next one. Otherwise, the current training phase will go on until it reaches the predefined maximum time steps T , and then moves to the next phase. The training process is as described in Algorithm 2. Experiments 4.2 Model Settings We perform proposed CL method with the FAIRSEQ 3 (O"
2021.iwslt-1.25,2020.emnlp-main.152,1,0.759241,"Missing"
2021.iwslt-1.25,2020.wmt-1.34,0,0.276009,"age, which is assumed to be able to recover the ground truth translations accurately (Liu et al., 2021). However, if we perform inference on the training set, many of the predictions are inconsistent with the references. It reflects the distribution shift between the NMT model leaned distribution and the empirical distribution of training corpus, as Figure 1 illustrated. For a training example, a high recovery degree between prediction and ground-truth target sentence means it’s easier to be mastered by the NMT model while a lower recovery degree means it’s more difficult (Ding and Tao, 2019; Wu et al., 2020b). To this end, we employ this recovery degree as the difficulty criterion, where the recovery degree is computed by the sentence-level BLEU. We put forward an analogy of this method that humans can schedule a personal and effective curriculum after skimming over a textbook, namely self-guided curriculum. In this work, we cast the recovery degree of each training example as its learning difficulty, enforcing the NMT model to learn from examples with higher recovery degrees to those with lower degrees. Also, we implement our proposed recovery-based difficulty criterion with fixed and dynamic c"
2021.iwslt-1.25,2020.coling-main.352,0,0.286898,"of training examples to approximate the perspective of NMT models. For instance, Platanios et al. (2019) turn discrete numerical difficulty scores into relative probabilities and then construct 206 Proceedings of the 18th International Conference on Spoken Language Translation, pages 206–214 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics the difficulty criterion, while others derive difficulty criterion from independently trained language model (Zhang et al., 2019; Dou et al., 2020; Liu et al., 2020) and word embedding model (Zhou et al., 2020b). Xu et al. (2020) derive difficulty criterion from the NMT model in the training process. And these difficulty criteria are applied to either fixed curriculum schedule (Cirik et al., 2016) or dynamic one (Platanios et al., 2019; Liu et al., 2020; Xu et al., 2020; Zhou et al., 2020b). A well-trained NMT model estimates the optimal probability distribution mapping from the source language to the target language, which is assumed to be able to recover the ground truth translations accurately (Liu et al., 2021). However, if we perform inference on the training set, many of the predictions are inconsistent with the"
2021.iwslt-1.25,kocmi-bojar-2017-curriculum,0,0.0166346,"entation of training examples based on that rank. ∗ Part of this work was done when the first author visited CLSP, JHU. In the field of neural machine translation (NMT), empirical studies have shown that CL strategies contribute to both convergence speed and model performance (Zhang et al., 2018; Platanios et al., 2019; Zhang et al., 2019; Liu et al., 2020; Zhan et al., 2021; Ruiter et al., 2020). These CL strategies vary by difficulty criteria and curriculum schedules. Early difficulty criterion depends on manually crafted features and prior knowledge such as sentence length and word rarity (Kocmi and Bojar, 2017). The drawback lies in the fact that humans understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples to approximate the perspective of NMT models. For instance, Platanios et al. (2019) turn discrete numerical difficulty scores into relative probabilities and then construct 206 Proceedings of the 18th International Conference on Spoken Language Translation, pages 206–214 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics the difficulty criter"
2021.iwslt-1.25,2020.acl-main.41,0,0.495878,"ost on several tasks by forcing models to learn training examples following an order from “easy” to “difficult”. They further explain CL method with two important constituents: how to rank training examples by learning difficulty and how to schedule the presentation of training examples based on that rank. ∗ Part of this work was done when the first author visited CLSP, JHU. In the field of neural machine translation (NMT), empirical studies have shown that CL strategies contribute to both convergence speed and model performance (Zhang et al., 2018; Platanios et al., 2019; Zhang et al., 2019; Liu et al., 2020; Zhan et al., 2021; Ruiter et al., 2020). These CL strategies vary by difficulty criteria and curriculum schedules. Early difficulty criterion depends on manually crafted features and prior knowledge such as sentence length and word rarity (Kocmi and Bojar, 2017). The drawback lies in the fact that humans understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples to approximate the perspective of NMT models. For instance, Platanios et al. (2019) turn discrete numerical difficulty scor"
2021.iwslt-1.25,N19-4009,0,0.0168981,") with 32k merge operations. And we learn BPE with a shared vocabulary for En-De. We use BLEU (Papineni et al., 2002) as the automatic metrics for computing recovery degree and evaluating model performance with statistical significance test (Collins et al., 2005). (7) If oc &gt; ov , training phase will progress to the next one. Otherwise, the current training phase will go on until it reaches the predefined maximum time steps T , and then moves to the next phase. The training process is as described in Algorithm 2. Experiments 4.2 Model Settings We perform proposed CL method with the FAIRSEQ 3 (Ott et al., 2019) implementation of the 1 https://github.com/fxshy/jieba https://github.com/mosesdecoder 3 https://github.com/pytorch/fairseq 2 210 # Systems 1 2 3 4 Transformer BASE w/ Competence-based CL w/ Norm-based CL w/ Uncertainty-aware CL 5 6 7 Transformer BASE w/ SGCL Fixed w/ SGCL Dynamic WMT14 EnDe BLEU ∆ 27.30 28.19† † 28.81 This work 27.63 ↑ 28.16 0.53 28.62⇑ 0.99 WMT16 EnDe BLEU ∆ ‡ 32.76 32.84‡ 33.93‡ - WMT17 ZhEn BLEU ∆ † 23.69 24.30† † 25.25 25.02‡ - 33.03 33.55↑ 34.07⇑ 23.78 24.65↑ 25.34⇑ 0.52 1.04 0.87 1.56 Table 2: Experiment results on WMT14 En⇒De with newstest2014 and newstest2016, and WM"
2021.iwslt-1.25,P02-1040,0,0.111482,"newstest2012 as the validation set and report test results on both newstest2014 and newtest2016 for fair comparison with existing approaches. For Zh-En, we follow (Hassan et al., 2018) to extract 20 million sentence pairs as the training set. We use newsdev2017 as the validation set and newstest2017 as the test set. Chinese sentences are segmented with a word segmentation toolkit Jieba1 . Sentences in other languages are tokenized with Moses2 . We learn Byte-Pair Encoding(BPE) (Sennrich et al., 2016) with 32k merge operations. And we learn BPE with a shared vocabulary for En-De. We use BLEU (Papineni et al., 2002) as the automatic metrics for computing recovery degree and evaluating model performance with statistical significance test (Collins et al., 2005). (7) If oc &gt; ov , training phase will progress to the next one. Otherwise, the current training phase will go on until it reaches the predefined maximum time steps T , and then moves to the next phase. The training process is as described in Algorithm 2. Experiments 4.2 Model Settings We perform proposed CL method with the FAIRSEQ 3 (Ott et al., 2019) implementation of the 1 https://github.com/fxshy/jieba https://github.com/mosesdecoder 3 https://gi"
2021.iwslt-1.25,N19-1119,0,0.341771,". (2009) achieves significant performance boost on several tasks by forcing models to learn training examples following an order from “easy” to “difficult”. They further explain CL method with two important constituents: how to rank training examples by learning difficulty and how to schedule the presentation of training examples based on that rank. ∗ Part of this work was done when the first author visited CLSP, JHU. In the field of neural machine translation (NMT), empirical studies have shown that CL strategies contribute to both convergence speed and model performance (Zhang et al., 2018; Platanios et al., 2019; Zhang et al., 2019; Liu et al., 2020; Zhan et al., 2021; Ruiter et al., 2020). These CL strategies vary by difficulty criteria and curriculum schedules. Early difficulty criterion depends on manually crafted features and prior knowledge such as sentence length and word rarity (Kocmi and Bojar, 2017). The drawback lies in the fact that humans understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples to approximate the perspective of NMT models. For instance, Platanios et al. (2019) t"
2021.iwslt-1.25,N19-1189,1,0.94104,"icant performance boost on several tasks by forcing models to learn training examples following an order from “easy” to “difficult”. They further explain CL method with two important constituents: how to rank training examples by learning difficulty and how to schedule the presentation of training examples based on that rank. ∗ Part of this work was done when the first author visited CLSP, JHU. In the field of neural machine translation (NMT), empirical studies have shown that CL strategies contribute to both convergence speed and model performance (Zhang et al., 2018; Platanios et al., 2019; Zhang et al., 2019; Liu et al., 2020; Zhan et al., 2021; Ruiter et al., 2020). These CL strategies vary by difficulty criteria and curriculum schedules. Early difficulty criterion depends on manually crafted features and prior knowledge such as sentence length and word rarity (Kocmi and Bojar, 2017). The drawback lies in the fact that humans understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples to approximate the perspective of NMT models. For instance, Platanios et al. (2019) turn discrete numeric"
2021.iwslt-1.25,2020.wmt-1.125,1,0.845338,"ability distribution of training examples to approximate the perspective of NMT models. For instance, Platanios et al. (2019) turn discrete numerical difficulty scores into relative probabilities and then construct 206 Proceedings of the 18th International Conference on Spoken Language Translation, pages 206–214 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics the difficulty criterion, while others derive difficulty criterion from independently trained language model (Zhang et al., 2019; Dou et al., 2020; Liu et al., 2020) and word embedding model (Zhou et al., 2020b). Xu et al. (2020) derive difficulty criterion from the NMT model in the training process. And these difficulty criteria are applied to either fixed curriculum schedule (Cirik et al., 2016) or dynamic one (Platanios et al., 2019; Liu et al., 2020; Xu et al., 2020; Zhou et al., 2020b). A well-trained NMT model estimates the optimal probability distribution mapping from the source language to the target language, which is assumed to be able to recover the ground truth translations accurately (Liu et al., 2021). However, if we perform inference on the training set, many of the predictions are i"
2021.iwslt-1.25,2020.acl-main.620,0,0.400926,"ability distribution of training examples to approximate the perspective of NMT models. For instance, Platanios et al. (2019) turn discrete numerical difficulty scores into relative probabilities and then construct 206 Proceedings of the 18th International Conference on Spoken Language Translation, pages 206–214 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics the difficulty criterion, while others derive difficulty criterion from independently trained language model (Zhang et al., 2019; Dou et al., 2020; Liu et al., 2020) and word embedding model (Zhou et al., 2020b). Xu et al. (2020) derive difficulty criterion from the NMT model in the training process. And these difficulty criteria are applied to either fixed curriculum schedule (Cirik et al., 2016) or dynamic one (Platanios et al., 2019; Liu et al., 2020; Xu et al., 2020; Zhou et al., 2020b). A well-trained NMT model estimates the optimal probability distribution mapping from the source language to the target language, which is assumed to be able to recover the ground truth translations accurately (Liu et al., 2021). However, if we perform inference on the training set, many of the predictions are i"
2021.mtsummit-at4ssl.7,2020.coling-main.304,0,0.0174418,"examples to obtain a good translation performance (Koehn and Knowles, 2017). In this paper, we approach gloss to text translation as a low-resource machine translation task and investigate two methods that are widely explored by the machine translation community to alleviate the need of large corpora, namely hyperparameter search and back-translation. While NMT models, like Transformer (Vaswani et al., 2017) can perform well with default hyperparameter settings on most of the publicly available large corpora, its performance is highly sensitive to hyperparameters under low-resource scenarios (Araabi and Monz, 2020; Duh et al., 2020). The optimal hyperparameter settings for a large corpus might lead to a poor system trained on a small dataset (Zhang and Duh, 2020). In this work, we focus on tuning 4 hyperparameters and find that hyperparameter search is necessary and helpful for gloss translation. Back-translation (Sennrich et al., 2016a) incorporates monolingual data in NMT which can help in low-resource settings (Hoang et al., 2018; Lample et al., 2018; Feldman and CotoSolano, 2020). Our experiments show that it has potential on gloss translation when the additional monolingual data are from the same"
2021.mtsummit-at4ssl.7,C18-1111,0,0.0129966,"interest or in-domain data. We adopt a language model based data selection method to first select examples from TED Talk corpus that are similar to PHOENIX14T. Next, those monolingual examples are back-translated and form a synthetic parallel corpus. A model is then trained on the concatenation of the synthetic data and the parallel PHOENIX14T data. This is not the end. Finally, we continue training the model with only the PHOENIX14T data. This continuedtraining process is also called fine-tuning, which is the conventional way for domain adaptation (Luong et al., 2015; Sennrich et al., 2016a; Chu and Wang, 2018; Zhang et al., 2019). 5.1.1 Data Selection We adopt the data selection method proposed in Moore and Lewis (2010). The main idea is to score the out-of-domain data N using language models trained from the in-domain data I and N and select top n training examples from N by a cut-off threshold on the resulting scores. To be specific, each sentence s in N is assigned a cross-entropy difference score, HI (s) HN (s), (2) where HI (s) is the per-word cross-entropy of s according to a language model trained on 1000 random samples of PHOENIX14T, and HN (s) is the per-word cross-entropy of s according"
2021.mtsummit-at4ssl.7,W19-6620,1,0.847749,"operations, the number of layers, embedding dimensions and initial learning rate. These hyperparameters are recognized as important hyperparameters by Zhang and Duh (2020), where the importance is computed as the variation in BLEU when changing a specific hyperparameter with values of all the other hyperparameters fixed (Klein and Hutter, 2019). BPE is a word segmentation approach that combines frequent sequence of characters so that out-of-vocabulary words are handled. It is expected to improve the translation of rare words and has been a standard preprocessing practice in NMT. According to Ding et al. (2019), although 32k and 90k are popular choices in most machine translation literature, they found 4 github.com/awslabs/sockeye Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 63 best worst random bpe 1k 2k 1k #layer 4 1 2 gloss-text #embed init lr 512 0.00005 512 0.0005 256 0.0002 BLEU 24.385 21.73 23.49 bpe 1k 1k 1k #layer 4 1 2 text-gloss #embed init lr 256 0.0005 512 0.0005 256 0.0002 BLEU 16.43 13.04 15.74 Table 1: Performance of selected Transformers. BLEU sc"
2021.mtsummit-at4ssl.7,2020.lrec-1.325,1,0.757172,"od translation performance (Koehn and Knowles, 2017). In this paper, we approach gloss to text translation as a low-resource machine translation task and investigate two methods that are widely explored by the machine translation community to alleviate the need of large corpora, namely hyperparameter search and back-translation. While NMT models, like Transformer (Vaswani et al., 2017) can perform well with default hyperparameter settings on most of the publicly available large corpora, its performance is highly sensitive to hyperparameters under low-resource scenarios (Araabi and Monz, 2020; Duh et al., 2020). The optimal hyperparameter settings for a large corpus might lead to a poor system trained on a small dataset (Zhang and Duh, 2020). In this work, we focus on tuning 4 hyperparameters and find that hyperparameter search is necessary and helpful for gloss translation. Back-translation (Sennrich et al., 2016a) incorporates monolingual data in NMT which can help in low-resource settings (Hoang et al., 2018; Lample et al., 2018; Feldman and CotoSolano, 2020). Our experiments show that it has potential on gloss translation when the additional monolingual data are from the same domain as the paral"
2021.mtsummit-at4ssl.7,2020.coling-main.351,0,0.0361052,"Missing"
2021.mtsummit-at4ssl.7,2020.eamt-1.50,0,0.0708517,"Missing"
2021.mtsummit-at4ssl.7,W18-2703,0,0.0138692,"ith default hyperparameter settings on most of the publicly available large corpora, its performance is highly sensitive to hyperparameters under low-resource scenarios (Araabi and Monz, 2020; Duh et al., 2020). The optimal hyperparameter settings for a large corpus might lead to a poor system trained on a small dataset (Zhang and Duh, 2020). In this work, we focus on tuning 4 hyperparameters and find that hyperparameter search is necessary and helpful for gloss translation. Back-translation (Sennrich et al., 2016a) incorporates monolingual data in NMT which can help in low-resource settings (Hoang et al., 2018; Lample et al., 2018; Feldman and CotoSolano, 2020). Our experiments show that it has potential on gloss translation when the additional monolingual data are from the same domain as the parallel data. Overall, we conclude that the low-resource machine translation perspective is promising but should not be taken as the ultimate solution for sign language gloss translation. It may be more promising to first focus on creating larger gloss-text datasets. 2 Related Work Most of the Sign Language Processing research has focused on Sign Language Recognition (Yin et al., 2016; Wang et al., 2016; Camg"
2021.mtsummit-at4ssl.7,2020.wmt-1.63,0,0.0134537,"ize are important. A big and complex model is more susceptible to overfitting. On the other hand, if the model is too small and simple, it might struggle to capture the meaningful patterns of data and result in underfitting. Our search space includes 1, 2, 4 layers and embedding size of 256 and 512. The learning rate is another important hyperparameter that scales the gradient in gradient descent training. A small initial learning rate may prolong the training process, whereas a large one may get the model stuck in a sub-optimal solution. It is recommended to start training with a low number (Koehn, 2020). We adjust it among 0.00005, 0.0002 and 0.0005. We tune hyperparameters for NMT systems on both gloss-text and text-gloss directions. This sums up to 72 systems in total. 4.2 Results The BLEU scores obtained on our search space are illustrated in Figure 2, where a wide variance is observed. As shown in Table 1, different choices of hyperparameters can increase the BLEU score by as much as 2.65 on gloss-text and 3.39 on text-gloss. Training is not expensive due to the small data size, so running a wide search over hyperparameters is recommended. 5 Back-translation Back-translation proposed in"
2021.mtsummit-at4ssl.7,P07-2045,0,0.00749749,"el transforms x into a sequence of hidden states, the decoder then generates yj iteratively based on the hidden states and the history decoding states to form the target sentence y. We choose Transformer (Vaswani et al., 2017) as it is the de facto mainstream NMT architecture and has achieved the state-of-the-art performance on many machine translation tasks. Transformer is an encoder-decoder based model with each layer consisting of a multi-head attention mechanism, followed by a feed-forward network. 3.4 Experimental Setup 3.4.1 Data Preprocessing All datasets are tokenized using the Moses (Koehn et al., 2007) tokenizer. We train the BytePair-Encoding (BPE) segmentation (Sennrich et al., 2016b) models separately for gloss and text. For hyperparameter search experiments (Section 4), we learn BPE models from PHOENIX14T. For back-translation tasks (Section 5), on the German side, we learn BPE models from the 3 For the rest of this paper, we will refer to sign language gloss as gloss and the spoken German as German. Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 62 12"
2021.mtsummit-at4ssl.7,W17-3204,0,0.127283,"and Spoken Languages Page 60 SIGN VIDEO Continuous Sign Language Recognition SIGN GLOSSES PAST[long-ago] YOURSELF LITTLE GIRL PRO-2 WANT GROW-UP FUTURE DO-WHAT Neural Machine Translation ENGLISH TRANSLATION When you were a little girl, what did you want to do when you grow up? Figure 1: Cascaded sign language translation system2 - First, CSLR converts sign video to a sequence of sign glosses. Then, NMT converts the sign glosses to text in e.g. English. NMT systems are known to be extremely data-hungry and usually require millions of training examples to obtain a good translation performance (Koehn and Knowles, 2017). In this paper, we approach gloss to text translation as a low-resource machine translation task and investigate two methods that are widely explored by the machine translation community to alleviate the need of large corpora, namely hyperparameter search and back-translation. While NMT models, like Transformer (Vaswani et al., 2017) can perform well with default hyperparameter settings on most of the publicly available large corpora, its performance is highly sensitive to hyperparameters under low-resource scenarios (Araabi and Monz, 2020; Duh et al., 2020). The optimal hyperparameter settin"
2021.mtsummit-at4ssl.7,D18-1549,0,0.0152783,"ameter settings on most of the publicly available large corpora, its performance is highly sensitive to hyperparameters under low-resource scenarios (Araabi and Monz, 2020; Duh et al., 2020). The optimal hyperparameter settings for a large corpus might lead to a poor system trained on a small dataset (Zhang and Duh, 2020). In this work, we focus on tuning 4 hyperparameters and find that hyperparameter search is necessary and helpful for gloss translation. Back-translation (Sennrich et al., 2016a) incorporates monolingual data in NMT which can help in low-resource settings (Hoang et al., 2018; Lample et al., 2018; Feldman and CotoSolano, 2020). Our experiments show that it has potential on gloss translation when the additional monolingual data are from the same domain as the parallel data. Overall, we conclude that the low-resource machine translation perspective is promising but should not be taken as the ultimate solution for sign language gloss translation. It may be more promising to first focus on creating larger gloss-text datasets. 2 Related Work Most of the Sign Language Processing research has focused on Sign Language Recognition (Yin et al., 2016; Wang et al., 2016; Camg¨oz et al., 2016; Vae"
2021.mtsummit-at4ssl.7,D15-1166,0,0.0611251,"domain data and PHOENIX14T is the domain in interest or in-domain data. We adopt a language model based data selection method to first select examples from TED Talk corpus that are similar to PHOENIX14T. Next, those monolingual examples are back-translated and form a synthetic parallel corpus. A model is then trained on the concatenation of the synthetic data and the parallel PHOENIX14T data. This is not the end. Finally, we continue training the model with only the PHOENIX14T data. This continuedtraining process is also called fine-tuning, which is the conventional way for domain adaptation (Luong et al., 2015; Sennrich et al., 2016a; Chu and Wang, 2018; Zhang et al., 2019). 5.1.1 Data Selection We adopt the data selection method proposed in Moore and Lewis (2010). The main idea is to score the out-of-domain data N using language models trained from the in-domain data I and N and select top n training examples from N by a cut-off threshold on the resulting scores. To be specific, each sentence s in N is assigned a cross-entropy difference score, HI (s) HN (s), (2) where HI (s) is the per-word cross-entropy of s according to a language model trained on 1000 random samples of PHOENIX14T, and HN (s) i"
2021.mtsummit-at4ssl.7,P10-2041,0,0.266996,"ontains sign videos, gloss annotations and German translations. The data split for train/dev/test is 7,096/519/642 sentences. The vocabulary size of the training set for glosses and German3 are 1,066 and 2,887 respectively. 3.2 Monolingual Data To the best of our knowledge, there is no publicly available large corpus of weather forecast subtitles in German. Since domain mismatch between the monolingual data and the parallel data might hurt the performance of NMT systems (Koehn and Knowles, 2017), we adopt several domain adaption methods to alleviate this problem. We use Moore-Lewis filtering (Moore and Lewis, 2010) to select sentences similar to PHOENIX14T from a German TED Talk corpus (Duh, 2018), which consists of 151,627 sentences. 3.3 NMT Model Most NMT models in literature follow a encoder-decoder architecture. The conditional probability of generating the target sentence y given the source sentence x is decomposed as: p(y |x) = J Y j=1 p(yj |y<j , x, ✓), (1) where ✓ represents model parameters, yj is the j-th target word, and y<j is the prefix of words before yj . The encoder of an NMT model transforms x into a sequence of hidden states, the decoder then generates yj iteratively based on the hidde"
2021.mtsummit-at4ssl.7,P16-1009,0,0.447228,"rch and back-translation. While NMT models, like Transformer (Vaswani et al., 2017) can perform well with default hyperparameter settings on most of the publicly available large corpora, its performance is highly sensitive to hyperparameters under low-resource scenarios (Araabi and Monz, 2020; Duh et al., 2020). The optimal hyperparameter settings for a large corpus might lead to a poor system trained on a small dataset (Zhang and Duh, 2020). In this work, we focus on tuning 4 hyperparameters and find that hyperparameter search is necessary and helpful for gloss translation. Back-translation (Sennrich et al., 2016a) incorporates monolingual data in NMT which can help in low-resource settings (Hoang et al., 2018; Lample et al., 2018; Feldman and CotoSolano, 2020). Our experiments show that it has potential on gloss translation when the additional monolingual data are from the same domain as the parallel data. Overall, we conclude that the low-resource machine translation perspective is promising but should not be taken as the ultimate solution for sign language gloss translation. It may be more promising to first focus on creating larger gloss-text datasets. 2 Related Work Most of the Sign Language Proc"
2021.mtsummit-at4ssl.7,P16-1162,0,0.814006,"rch and back-translation. While NMT models, like Transformer (Vaswani et al., 2017) can perform well with default hyperparameter settings on most of the publicly available large corpora, its performance is highly sensitive to hyperparameters under low-resource scenarios (Araabi and Monz, 2020; Duh et al., 2020). The optimal hyperparameter settings for a large corpus might lead to a poor system trained on a small dataset (Zhang and Duh, 2020). In this work, we focus on tuning 4 hyperparameters and find that hyperparameter search is necessary and helpful for gloss translation. Back-translation (Sennrich et al., 2016a) incorporates monolingual data in NMT which can help in low-resource settings (Hoang et al., 2018; Lample et al., 2018; Feldman and CotoSolano, 2020). Our experiments show that it has potential on gloss translation when the additional monolingual data are from the same domain as the parallel data. Overall, we conclude that the low-resource machine translation perspective is promising but should not be taken as the ultimate solution for sign language gloss translation. It may be more promising to first focus on creating larger gloss-text datasets. 2 Related Work Most of the Sign Language Proc"
2021.mtsummit-at4ssl.7,P19-1021,0,0.011774,"each batch consists of 2048 words. Training stops when the perplexity on the development set has not improved for 32 checkpoints. All the back-translation experiments in Section 5 adopt the best hyperparameter settings obtained by a hyperparameter search (Section 4). Note that the optimal settings for gloss-text translation is different from text-gloss translation. 4 Hyperparameter Search Hyperparameter selection is crucial to build a good NMT system. It is especially the case for low-resource scenarios when the default hyperparameter settings are very likely to be ineffective. As reported in Sennrich and Zhang (2019) and Zhang and Duh (2020), the NMT systems developed for low-resource translation tasks disagree a lot with those trained on high-resource corpora on the optimal hyperparameter choices. Furthermore, datasets in different domains and language pairs all differ in their hyperparameter preference. They also show that adjusting hyperparameters can cause BLEU differences of more than 20 in some datasets. 4.1 Important Hyperparameters In this work, we focus on 4 hyperparameters of Transformer models: the number of BPE merge operations, the number of layers, embedding dimensions and initial learning r"
2021.mtsummit-at4ssl.7,2020.coling-main.525,0,0.312344,"basic SVO word order. Linguists use glossing to annotate signs, which can be viewed as a written form of sign language. Glosses can be taken as intermediate representations when translating continuous sign utterances to spoken language sentences. Previous work on SLT adopts either an end-to-end system that maps sign language videos directly to spoken languages, or a cascaded system, as shown in Figure 1, that first relies on Continuous Sign Language Recognition (CSLR) to produce sign glosses and then passes the glosses into a Neural Machine Translation (NMT) system (Camgoz et al., 2018, 2020; Yin and Read, 2020). Importantly, Camgoz et al. (2018) reports that the cascaded system outperforms the end-to-end system by a large margin (18.13 vs. 9.58 BLEU). In this work, we focus on improving the NMT component of cascaded systems, which attracts much less attentions compared to the CSLR component of cascaded systems (Cui et al., 2017; Huang et al., 2018; Yang et al., 2019; Orbay and Akarun, 2020). Sign language gloss translation is a challenging problem due to the scarcity of annotated parallel data. The popular continuous SLT dataset, “RWTH-PHOENIX-Weather 2014T” (Camgoz et al., 2018) contains 7,096 glos"
2021.mtsummit-at4ssl.7,2020.tacl-1.26,1,0.894915,"ne translation task and investigate two methods that are widely explored by the machine translation community to alleviate the need of large corpora, namely hyperparameter search and back-translation. While NMT models, like Transformer (Vaswani et al., 2017) can perform well with default hyperparameter settings on most of the publicly available large corpora, its performance is highly sensitive to hyperparameters under low-resource scenarios (Araabi and Monz, 2020; Duh et al., 2020). The optimal hyperparameter settings for a large corpus might lead to a poor system trained on a small dataset (Zhang and Duh, 2020). In this work, we focus on tuning 4 hyperparameters and find that hyperparameter search is necessary and helpful for gloss translation. Back-translation (Sennrich et al., 2016a) incorporates monolingual data in NMT which can help in low-resource settings (Hoang et al., 2018; Lample et al., 2018; Feldman and CotoSolano, 2020). Our experiments show that it has potential on gloss translation when the additional monolingual data are from the same domain as the parallel data. Overall, we conclude that the low-resource machine translation perspective is promising but should not be taken as the ulti"
2021.mtsummit-at4ssl.7,N19-1189,1,0.82812,"n data. We adopt a language model based data selection method to first select examples from TED Talk corpus that are similar to PHOENIX14T. Next, those monolingual examples are back-translated and form a synthetic parallel corpus. A model is then trained on the concatenation of the synthetic data and the parallel PHOENIX14T data. This is not the end. Finally, we continue training the model with only the PHOENIX14T data. This continuedtraining process is also called fine-tuning, which is the conventional way for domain adaptation (Luong et al., 2015; Sennrich et al., 2016a; Chu and Wang, 2018; Zhang et al., 2019). 5.1.1 Data Selection We adopt the data selection method proposed in Moore and Lewis (2010). The main idea is to score the out-of-domain data N using language models trained from the in-domain data I and N and select top n training examples from N by a cut-off threshold on the resulting scores. To be specific, each sentence s in N is assigned a cross-entropy difference score, HI (s) HN (s), (2) where HI (s) is the per-word cross-entropy of s according to a language model trained on 1000 random samples of PHOENIX14T, and HN (s) is the per-word cross-entropy of s according to a language model t"
C04-1022,N03-2002,1,0.811768,"urkish, Russian, or Arabic. Such languages have a large number of word types in relation to the number of word tokens in a given text, as has been demonstrated in a number of previous studies (Geutner, 1995; Kiecza et al., 1999; Hakkani-T¨ ur et al., 2002; Kirchhoff et al., 2003). This in turn results in a high perplexity and in a large number of out-of-vocabulary (OOV) words when applying a trained language model to a new unseen text. 2.1 Factored Word Representations A recently developed approach that addresses this problem is that of Factored Language Models (FLMs) (Kirchhoff et al., 2002; Bilmes and Kirchhoff, 2003), whose basic idea is to decompose words into sets of features (or factors) instead of viewing them as unanalyzable wholes. Probabilistic language models can then be constructed over (sub)sets of word features instead of, or in addition to, the word variables themselves. For instance, words can be decomposed into stems/lexemes and POS tags indicating their morphological features, as shown below: Word: Stem: Tag: Stock Stock Nsg prices price N3pl are be V3pl rising rise Vpart Such a representation serves to express lexical and syntactic generalizations, which would otherwise remain obscured. It"
C04-1022,J92-4003,0,0.339397,"Missing"
C04-1022,C00-1042,0,0.0237462,"models that perform well in sparse data conditions. The factored representation was constructed using linguistic information from the corpus lexicon, in combination with automatic morphological analysis tools. It includes, in addition to the word, the stem, a morphological tag, the root, and the pattern. The latter two are components which when combined form the stem. An example of this factored word representation is shown below: Word:il+dOr/Morph:noun+masc-sg+article/ Stem:dOr/Root:dwr/Pattern:CCC For our Turkish experiments we used a morphologically annotated corpus of Turkish (Hakkani-T¨ ur et al., 2000). The annotation was performed by applying a morphological analyzer, followed by automatic morphological disambiguation as described in (Hakkani-T¨ ur et al., 2002). The morphological tags consist of the initial root, followed by a sequence of inflectional groups delimited by derivation boundaries (ˆDB). A sample annotation (for the word yararlanmak, consisting of the root yarar plus three inflectional groups) is shown below: yararmanlak: yarar+Noun+A3sg+Pnon+Nom ˆDB+Verb+Acquire+Pos ˆDB+Noun+Inf+A3sg+Pnon+Nom We removed segmentation marks (for titles and paragraph boundaries) from the corpus"
C04-1022,N04-4034,0,0.0101525,"ighted mean, product, and maximum of the smoothed probability distributions over all subsets of the conditioning factors. In addition to different choices for g, different discounting parameters can be chosen at different levels in the backoff graph. For instance, at the topmost node, Kneser-Ney discounting might be chosen whereas at a lower node Good-Turing might be applied. FLMs have been implemented as an add-on to the widely-used SRILM toolkit1 and have been used successfully for the purpose of morpheme-based language modeling (Bilmes and Kirchhoff, 2003), multi-speaker language modeling (Ji and Bilmes, 2004), and speech recognition (Kirchhoff et al., 2003). 3 Learning FLM Structure In order to use an FLM, three types of parameters need to be specified: the initial conditioning factors, the backoff graph, and the smoothing options. The goal of structure learning is to find the parameter combinations that create FLMs that achieve a low perplexity on unseen test data. The resulting model space is extremely large: given a factored word representation  with a total of k factors, there are Pk k n=1 n possible subsets of initial conditioning factors. For a set of m conditioning factors, there are up to"
C04-1022,P99-1033,0,0.0132766,"nnotation (for the word yararlanmak, consisting of the root yarar plus three inflectional groups) is shown below: yararmanlak: yarar+Noun+A3sg+Pnon+Nom ˆDB+Verb+Acquire+Pos ˆDB+Noun+Inf+A3sg+Pnon+Nom We removed segmentation marks (for titles and paragraph boundaries) from the corpus but included punctuation. Words may have different numbers of inflectional groups, but the FLM representation requires the same number of factors for each word; we therefore had to map the original morphological tags to a fixed-length factored representation. This was done using linguistic knowledge: according to (Oflazer, 1999), the final inflectional group in each dependent word has a special status since it determines inflectional markings on head words following the dependent word. The final inflectional group was therefore analyzed into separate factors indicating the number (N), case (C), part-of-speech (P) and all other information (O). Additional factors for the word are the root (R) and all remaining information in the original tag not subsumed by the other factors (G). The word itself is used as another factor (W). Thus, the above example would be factorized as follows: W:yararlanmak/R:yarar/P:NounInf-N:A3s"
C04-1022,J03-4001,0,\N,Missing
C04-1022,P98-2240,0,\N,Missing
C04-1022,C98-2235,0,\N,Missing
C10-1050,P06-1090,0,0.0477622,"anguage model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based translation to constrain word order. In this paper, we adopt the reordering model originally proposed by Tromble and Eisner (2009) for the preprocessing approach in phrase-based translation. To integrate the word-based reordering model, we added a reordered source string into the right-hand-side of SCFG’s rules. By this extension, our system can generate the reordered source sentence as well as target sentence and is able to efficiently calculate the score of the reor"
C10-1050,P06-1067,0,0.0327337,"nd the right hand side of Figure 1, allowing us to score both global and local word reorderings. ′ ′ To add γ to rules, we permuted γ into γ after rule extraction based on Grow-diag-final (Koehn et al., 2005) alignment by GIZA++ (Och and Ney, 2003). To do this permutation on rules, we applied two methods. One is the same algorithm as Tromble and Eisner (2009), which reorders aligned source terminals and nonterminals in the same order as that of target side and moves unaligned source terminals to the front of aligned terminals or nonterminals (move-to-front). The other is the same algorithm as AI-Onaizan and Papineni (2006), which differs from Tromble and Eisner’s approach in attaching unaligned source terminals to the closest prealigned source terminals or nonterminals (attach). This extension of ′ adding γ does not increase the number of rules. Table 1 shows a Japanese-to-English example of the representation of rules for our proposed system. Japanese words are romanized. Suppose that source-side string is (X1 wa jinsei no X2 da) and target-side string is (X1 is X2 of life) and their word alignments are a=((jinsei , life) , (no , of) , (da , is)). Source-side aligned words and nonterminal symbols are sorted in"
C10-1050,J93-2003,0,0.0272701,"o handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based translation to constrain word order. In this paper, we adopt the reordering model originally p"
C10-1050,N09-1025,0,0.0662338,"Missing"
C10-1050,niessen-etal-2000-evaluation,0,0.0539958,"o preprocessing approach. PER 39.68 45.27 39.89 39.43 5 Table 5: BLEU and PER scores on the test set. Our training corpus contains about 200.8k sentences. Using the training corpus, we extracted hierarchical phrase rules and trained 4-gram language model and word-based reordering model. Parameters were tuned over 1.0k sentences (development data) with single reference by minimum error rate training (MERT) (Och, 2003). Test data consisted of 1.0k sentences with single reference. Table 4 shows the condition of corpus in detail. 4.3 Results Table 5 shows the BLEU (Papineni et al., 2001) and PER (Niesen et al., 2000) scores obtained by each system. The results clearly indicated that our proposed system with word-based reordering model (move-to-front or attach) outperformed baseline system on BLEU scores. In contrast, there is no significant improvement from baseline on PER. This suggests that the improvement of BLEU mainly comes from reordering. In our experiment, preprocessing approach resulted in very poor scores. 4.4 Discussion Table 6 displays examples showing the cause of the improvements of our system with reordering model (attach) comparing to baseline system. We can see that the outputs of our sys"
C10-1050,P02-1038,0,0.548257,"Missing"
C10-1050,J03-1002,0,0.012804,"equation: ( ) ′ ′ Tˆ = (Sˆ , Tˆ) = (S , T ) argmax w(D) . (6) D:S(D)=S Our system generates the reordered source sen′ tence S as well as target sentence T . Figure 2 ′ shows the generated reordered source sentence S when translating the example of Figure 1. Note ′ that the structure of S is the same as that of target sentence T . The decoder generates both Figure 2 and the right hand side of Figure 1, allowing us to score both global and local word reorderings. ′ ′ To add γ to rules, we permuted γ into γ after rule extraction based on Grow-diag-final (Koehn et al., 2005) alignment by GIZA++ (Och and Ney, 2003). To do this permutation on rules, we applied two methods. One is the same algorithm as Tromble and Eisner (2009), which reorders aligned source terminals and nonterminals in the same order as that of target side and moves unaligned source terminals to the front of aligned terminals or nonterminals (move-to-front). The other is the same algorithm as AI-Onaizan and Papineni (2006), which differs from Tromble and Eisner’s approach in attaching unaligned source terminals to the closest prealigned source terminals or nonterminals (attach). This extension of ′ adding γ does not increase the number"
C10-1050,P05-1066,0,0.562071,"les of hierarchical phrase-based model to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it d"
C10-1050,P03-1021,0,0.214019,"X XXX System Baseline (Hiero) Preprocessing Hiero + move-to-front Hiero + attach 28.09 17.32 28.85 29.25 poorness, our proposed method effectively utilize this reordering model in contrast to preprocessing approach. PER 39.68 45.27 39.89 39.43 5 Table 5: BLEU and PER scores on the test set. Our training corpus contains about 200.8k sentences. Using the training corpus, we extracted hierarchical phrase rules and trained 4-gram language model and word-based reordering model. Parameters were tuned over 1.0k sentences (development data) with single reference by minimum error rate training (MERT) (Och, 2003). Test data consisted of 1.0k sentences with single reference. Table 4 shows the condition of corpus in detail. 4.3 Results Table 5 shows the BLEU (Papineni et al., 2001) and PER (Niesen et al., 2000) scores obtained by each system. The results clearly indicated that our proposed system with word-based reordering model (move-to-front or attach) outperformed baseline system on BLEU scores. In contrast, there is no significant improvement from baseline on PER. This suggests that the improvement of BLEU mainly comes from reordering. In our experiment, preprocessing approach resulted in very poor"
C10-1050,2005.iwslt-1.8,0,0.460908,"n model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based translation to constrain word order. In this paper, we adopt the reordering model originally proposed by Tromble and Eisner (2009) for the preprocessing approach in phrase-based translation. To integrate the word-based reordering model, we added a reordered source string into the right-hand-side of SCFG’s rules. By this extension, our system can generate the reordered source sentence as well as target sentence and is able to efficiently calculate"
C10-1050,W08-0402,0,0.0145236,"on; incorpolated into sorted rules for the proposed implementation. To reveal the effectiveness of integrating the reordering model into decoder, we compared the following setups: Data Training • baseline: a standard hierarchical phrasebased machine translation (Hiero) system. Development Test • preprocessing: applied Tromble and Eisner’s approach, then translate by Hiero system. Word. 2.4M 2.3M 10.3K 9.8K 14.2K 13.5K Avg. leng 12.0 11.5 10.3 9.8 14.2 13.5 Table 4: The Data statistics • Hiero system + reordering model: integrated reordering model into Hiero system. We used the Joshua Decoder (Li and Khudanpur, 2008) as the baseline Hiero system. This decoder uses a log-linear model with seven features, which consist of N -gram language model PLM (T ), lexical translation model Pw (γ|α), Pw (α|γ), rule ja en ja en ja en Sent. 200.8K 200.8K 1.0K 1.0K 1.0K 1.0K For experiments we used a Japanese-English basic travel expression corpus (BTEC). Japanese word order is linguistically very different from English and we think Japanese-English pair is a very good test bed for evaluating reordering model. 443 XXX XXX Metrics BLEU XX XXX System Baseline (Hiero) Preprocessing Hiero + move-to-front Hiero + attach 28.09"
C10-1050,P07-1091,0,0.0761436,"rase-based model to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitl"
C10-1050,2001.mtsummit-papers.68,0,0.0370106,"s reordering model in contrast to preprocessing approach. PER 39.68 45.27 39.89 39.43 5 Table 5: BLEU and PER scores on the test set. Our training corpus contains about 200.8k sentences. Using the training corpus, we extracted hierarchical phrase rules and trained 4-gram language model and word-based reordering model. Parameters were tuned over 1.0k sentences (development data) with single reference by minimum error rate training (MERT) (Och, 2003). Test data consisted of 1.0k sentences with single reference. Table 4 shows the condition of corpus in detail. 4.3 Results Table 5 shows the BLEU (Papineni et al., 2001) and PER (Niesen et al., 2000) scores obtained by each system. The results clearly indicated that our proposed system with word-based reordering model (move-to-front or attach) outperformed baseline system on BLEU scores. In contrast, there is no significant improvement from baseline on PER. This suggests that the improvement of BLEU mainly comes from reordering. In our experiment, preprocessing approach resulted in very poor scores. 4.4 Discussion Table 6 displays examples showing the cause of the improvements of our system with reordering model (attach) comparing to baseline system. We can s"
C10-1050,P08-1066,0,0.063341,"ur approach is similar to preprocessing approach (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009) in that it reorders source sentence in target order. The difference is this sentence reordering is done in decoding rather than in preprocessing. A lot of studies on lexicalized reordering (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) focus on the phrase-based model. These works cannnot be directly applied to hierarchical phrase-based model because of the difference between normal phrases and hierarchical phrases that includes nonterminal symbols. Shen et al. (2008,2009) proposed a way to integrate dependency structure into target and source side string on hierarchical phrase rules. This approach is similar to our approach in extending the formalism of rules on hierarchical phrase-based model in order to consider the constraint of word order. But, our approach differs from (Shen et al., 2008; Shen et al., 2009) in that syntax annotation is not necessary. 6 Conclusion and Future Work We proposed a method to integrate word-based reordering model into hierarchical phrase-based ′ machine translation system. We add γ into the hiero rules, but this does not i"
C10-1050,D09-1008,0,0.0590956,"l., 2005; Nagata et al., 2006) focus on the phrase-based model. These works cannnot be directly applied to hierarchical phrase-based model because of the difference between normal phrases and hierarchical phrases that includes nonterminal symbols. Shen et al. (2008,2009) proposed a way to integrate dependency structure into target and source side string on hierarchical phrase rules. This approach is similar to our approach in extending the formalism of rules on hierarchical phrase-based model in order to consider the constraint of word order. But, our approach differs from (Shen et al., 2008; Shen et al., 2009) in that syntax annotation is not necessary. 6 Conclusion and Future Work We proposed a method to integrate word-based reordering model into hierarchical phrase-based ′ machine translation system. We add γ into the hiero rules, but this does not increase the number of rules. So, this extension itself does not affect the search space of decoding. In this paper we used Tromble and Eisner’s reordering model for our method, but various reordering model can ′ be incorporated to our method, for example S N -gram language model. Our experimental results on Japanese-to-English task showed that our sys"
C10-1050,N04-4026,0,0.216112,"the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based translation to constrain word order. In this paper, we adopt the reordering model originally proposed by Tromble and Eisner (2009) for the preprocessing approach in phrase-based translation. To integrate the word-based reordering model, we added a reordered source string into the right-hand-side of SCFG’s rules. By this extension, our system can generate the reordered source sentence as well as target sentence and is able to e"
C10-1050,D09-1105,0,0.74067,"to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering mode"
C10-1050,P06-1098,1,0.822492,"than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based transl"
C10-1050,C04-1073,0,0.441205,"ontext-free grammar rules of hierarchical phrase-based model to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global re"
C10-1050,W02-1001,0,\N,Missing
C10-1050,P02-1040,0,\N,Missing
C10-1050,J07-2003,0,\N,Missing
cheng-etal-2014-parsing,D07-1013,0,\N,Missing
cheng-etal-2014-parsing,J09-4006,0,\N,Missing
cheng-etal-2014-parsing,W06-2933,0,\N,Missing
cheng-etal-2014-parsing,E09-1100,0,\N,Missing
cheng-etal-2014-parsing,J92-4003,0,\N,Missing
cheng-etal-2014-parsing,P08-1068,0,\N,Missing
cheng-etal-2014-parsing,D12-1132,0,\N,Missing
cheng-etal-2014-parsing,O03-4002,0,\N,Missing
cheng-etal-2014-parsing,P11-1141,0,\N,Missing
cheng-etal-2014-parsing,P13-1013,0,\N,Missing
cheng-etal-2014-parsing,W06-0127,0,\N,Missing
cheng-etal-2014-parsing,I08-4008,1,\N,Missing
cheng-etal-2014-parsing,D07-1101,0,\N,Missing
cheng-etal-2014-parsing,P07-1106,0,\N,Missing
D10-1092,W05-0909,0,0.242163,"Missing"
D10-1092,W10-1749,0,0.0898134,"on of the square root to NKT imply that Chinese word order is close to that of English, and they have to measure subtle word order mistakes. 951 Table 3: NTCIR-7 √ meta-evaluation: Effects of square root (b(x) = 1 − 1 − x) √ NKT NKT b(NKT) Spearman w/ adequacy 0.940 0.940 0.922 Pearson w/ adequacy 0.922 0.817 0.941 Spearman w/ fluency 0.887 0.865 0.858 Pearson w/ fluency 0.931 0.917 0.833 In spite of these differences, the two groups independently recognized the usefulness of rank correlations for automatic evaluation of translation quality for distant language pairs. In their WMT-2010 paper (Birch and Osborne, 2010), they multiplied NKT with the brevity penalty and interpolated it with BLEU for the WMT-2010 shared task. This fact implies that incomprehensible or misleading word order mistakes are rare in translation among European languages. 6 Conclusions When Statistical Machine Translation is applied to distant language pairs such as Japanese and English, word order becomes an important problem. SMT systems often fail to find an appropriate translation because of a large search space. Therefore, they often output misleading or incomprehensible sentences such as “A because B” vs. “B because A.” To penal"
D10-1092,E06-1032,0,0.0197708,"a-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an auto"
D10-1092,W07-0718,0,0.0467588,"f sentence-level scores. We used default settings for conventional metrics, but we tuned GTM (Melamed et al., 2007) with -e option. This option controls preferences on longer word runs. We also used the paraphrase database TERp (http://www.umiacs.umd. 949 edu/˜snover/terp) for METEOR (Banerjee and Lavie, 2005). 3.2 Meta-evaluation with WMT-07 data We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs. Therefore, we also use the WMT07 data (Callison-Burch et al., 2007) that covers only European language pairs. Callison-Burch et al. (2007) tried different human evaluation methods and showed detailed evaluation scores. The Europarl test set has 2,000 sentences, and The News Commentary test set has 2,007 sentences. This data has different language pairs: Spanish, French, German ⇒ English. We exclude CzechEnglish because there were so few systems (See the footnote of p. 146 in their paper). 4 4.1 Results Meta-evaluation with NTCIR-7 data Table 1 shows the main results of this paper. The left part has corpus-level meta-evaluation with adequacy. Error metrics, WE"
D10-1092,I05-2014,0,0.0369692,"l by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers (http://chasenlegacy.sourceforge.jp/) following Fujii et al. (2008) In JE translation, most Statistical Machine Translation (SMT) systems translate the Japanese sentence (J0) kare wa sono hon wo yonda node sekaishi ni kyoumi ga atta which means 944 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944–952, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Associat"
D10-1092,2007.mtsummit-papers.21,0,0.0340785,"eni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers"
D10-1092,W10-1736,1,0.720649,") was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks in the reference with those in the hypothesis. There are two popular rank correlation coefficients: Spearman’s ρ and Kendall’s τ (Kendall, 1975). In Isozaki et al. (2010), we used Kendall’s τ to measure the effectiveness of our Head Finalization rule as a preprocessor for English-to-Japanese translation, but we measured the quality of translation by using conventional metrics. It is not clear how well τ works as an automatic evaluation metric of translation quality. Moreover, Spearman’s ρ might work better than Kendall’s τ . As we discuss later, τ considers only the direction of the rank change, whereas ρ considers the distance of the change. The first objective of this paper is to examine which is the better metric for distant language pairs. The second objec"
D10-1092,N03-1020,0,0.0120162,"tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundar"
D10-1092,P02-1040,0,0.100666,"ese metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and A"
D10-1092,2006.amta-papers.25,0,0.103847,"evity Penalty (BP) min(1, exp(1 − r/h)), where r is the length of the reference and h is the length of the hypothesis. BLEU = BP × (p1 p2 p3 p4 )1/4 . Its range is [0, 1]. The BLEU score of (H0) with reference (R0) is 1.0×(11/11×9/10×6/9×4/8)1/4 = 0.740. Therefore, BLEU gives a very good score to this inadequate translation because it checks only ngrams and does not regard global word order. Since (R0) and (H0) look similar in terms of fluency, adequacy is more important than fluency in the translation between distant language pairs. Similarly, other popular scores such as NIST, PER, and TER (Snover et al., 2006) also give relatively good scores to this translation. NIST also considers only local word orders (n-grams). PER (Position-Independent Word Error Rate) was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks"
D10-1092,N03-2021,0,\N,Missing
D13-1014,D10-1115,0,0.519479,"s addressed the question of how to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in various natural language processing (NLP) tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Turian et al., 2010; Collobert et al., 2011). Recently, modeling of semantic compositionality (Frege, 1892) in vector space has emerged as another important line of research (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013). The goal is to formulate how individual word representations ought to be combined to achieve phrasal or sentential semantics. The main questions for semantic compositionality that we are concerned with are: (1) how can polysemy be handled by a single vector representation per word type, learned by either a distributional or neural model, and (2) how does composition resolve 130 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 130–140, c Seattle, Washington, USA, 18-"
D13-1014,D12-1050,0,0.209522,"subject and object vector, the outer product of the verb vector with itself, and then the element-wise product of both results. 3. Erk and Pad´o’s (2008) model, which adapts the word vectors based on context and is the most similar in terms of motivation to ours. 4. Van de Cruy et al. (2013) multi-way interaction model based on matrix factorization. This achieves the best result for this task to date. A detailed explanation of these models will be provided in Section 7. For the underlying word representations, we experiment with sparse 2000-dim SDS and dense 50-dim NLM. These are provided by Blacoe and Lapata (2012)5 and trained on the British National Corpus (BNC). We are interested in knowing how sensitive each model is to the underlying word representation. In general, this is a challenging task: the upper-bound of ρ = 0.62 is the inter-annotator agreement. 5 http://www.cs.ox.ac.uk/ people/edward.grefenstette/ http://homepages.inf.ed.ac.uk/ s1066731/index.php?page=resources 135 5.3 Implementation details In terms of implementation detail, our model and our re-implementation of Erk and Pado’s model make use of the ukWaC corpus (Baroni et al., 2009).6 This corpus is a two billion word corpus automatical"
D13-1014,W13-3206,0,0.0922558,"ators that combine two word vector representations, u, v ∈ Rn and their learning parameters. Our model only needs two hyper-parameters: the number of prototype words m and dimensional reduction k in SVD verbs, which is the dataset we used here. The previous state-of-the-art result for this task comes from the model of Van de Cruys et al. (2013). They model compositionality as a multi-way interaction between latent factors, which are automatically constructed from corpus data via matrix factorization. Comprehensive evaluation of various existing models are reported in (Blacoe and Lapata, 2012; Dinu et al., 2013). Blacoe and Lapata (2012) highlight the importance of jointly examining word representations and compositionality operators. However, two out of three composition methods they evaluate are parameter-free, so that they can side-step the issue of parameter estimation. Dinu et al. (2013) describe the relation between word vector and compositionality in more detail with free parameters. Table 6 summarizes some ways to compose the meaning of two word vectors (u, v), following (Dinu et al., 2013). These range from simple operators (e.g. Add and Multiply) to expressive models with many free paramete"
D13-1014,D08-1094,0,0.263669,"Missing"
D13-1014,W09-0208,0,0.0282538,"Missing"
D13-1014,D11-1129,0,0.235862,"generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011). Ok Figure 1: Here, we capture the semantics of run in run company by projecting the original word representation of run to the prototype space of company (and vice versa). Introduction Vector space models of words have been very successful in capturing the semantic and syntactic characteristics of individual lexical items (Turney and Pantel, 2010). Much research has addressed the question of how to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in vari"
D13-1014,W10-2805,0,0.0596095,"he first approaches is the vector addition/multiplication idea of Mitchell and Lapata (2008). The appeal of this kind of simple approach is its intuitive geometric interpretation and its robustness to various datasets. However, it may not be sufficiently expressive to represent the various factors involved in compositional semantics, such as syntax and context. To this end, Baroni and Zamparelli (2010) present a compositional model for adjectives and nouns. In their model, an adjective is a matrix operator that modifies the noun vector into an adjective-noun vector. Zanzotto et al. (2010) and Guevara (2010) also proposed linear transformation models for composition and address the issue of estimating large matrices with least squares or regression techniques. Socher et al. (2012) extend this linear transformation approach with the more powerful model of Matrix-Vector Recursive Neural Networks (MV-RNN). Each node in a parse tree is assigned both a vector and a matrix. The vector captures the actual meaning of the word itself, while the matrix is modeled as a operator that modify the meaning of neighboring words and phrases. This model captures semantic change phenomenon like not bad is similar to"
D13-1014,P12-1092,0,0.0574691,"are usually called word embeddings, and it has been shown that such vectors can capture interesting linear relationships, such as king − man + woman ≈ queen (Mikolov et al., 2013). In this work, we adopt the model by Collobert and Weston (2008). The idea is to construct a neural network based on word sequences, where one outputs high scores for n-grams that occur in a large unlabeled corpus and low scores for nonsense n-grams where one word is replaced by a random word. This word representation with NLM has been used to good effect, for example in (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012) where induced word representations are used with sophisticated features to improve performance in various NLP tasks. Specifically, we first represent the word sequence as a vector x = [d(w1 ); d(w2 ); . . . ; d(wm )], where wi is ith word in the sequence, m is the window size, d(w) is the vector representation of word w (an n-dimensional column vector) and [d(w1 ); d(w2 ); . . . ; d(wm )] is the concatenation of word vectors as an input of neural network. Second, we compute the score of the sequence, score(x) = sT (tanh(Wx + b)) (2) where W ∈ Rh×(mn) and s ∈ Rh are the first and second layer"
D13-1014,N13-1090,0,0.0190747,"e target word t, the total count of all word tokens, the frequency of the target word t, and the frequency of the context word ci , respectively. 2.2 Neural Language Model (NLM) word embeddings Another popular way to learn word representations is based on the Neural Language Model (NLM) (Bengio et al., 2003). In comparison with SDS, NLM tend to be low-dimensional (e.g. 50 dimensions) but employ dense features. These dense feature vectors are usually called word embeddings, and it has been shown that such vectors can capture interesting linear relationships, such as king − man + woman ≈ queen (Mikolov et al., 2013). In this work, we adopt the model by Collobert and Weston (2008). The idea is to construct a neural network based on word sequences, where one outputs high scores for n-grams that occur in a large unlabeled corpus and low scores for nonsense n-grams where one word is replaced by a random word. This word representation with NLM has been used to good effect, for example in (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012) where induced word representations are used with sophisticated features to improve performance in various NLP tasks. Specifically, we first represent the word"
D13-1014,P08-1028,0,0.854122,"ical items (Turney and Pantel, 2010). Much research has addressed the question of how to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in various natural language processing (NLP) tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Turian et al., 2010; Collobert et al., 2011). Recently, modeling of semantic compositionality (Frege, 1892) in vector space has emerged as another important line of research (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013). The goal is to formulate how individual word representations ought to be combined to achieve phrasal or sentential semantics. The main questions for semantic compositionality that we are concerned with are: (1) how can polysemy be handled by a single vector representation per word type, learned by either a distributional or neural model, and (2) how does composition resolve 130 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pr"
D13-1014,nivre-etal-2006-maltparser,0,0.065447,"Missing"
D13-1014,N10-1013,0,0.0253016,"el (Section 4.2). 4.1 Compositional Neural Language Model (C-NLM) Compositional Neural Language Model (C-NLM) is a combination of a word representation learning method and compositional rule. In contrast to other compositional models based on machine learning, our model has no complex parameters for modeling composition. Composition is modeled using straightforward vector addition/multiplications; instead, what is learned is the word representation. Figure 3 shows the C-NLM. The learning algorithm is unsupervised, and works by artificially 3 There are works on multiple representations, e.g., (Reisinger and Mooney, 2010); we focus on single representation here. 134 verb v Pverb obj o Figure 4: Co-Compositional Neural Language Model (CoC-NLM) is C-NLM with prototype projection. generating negative examples in a fashion analogous to the NLM learning algorithm of (Collobert and Weston, 2008) and contrastive estimation (Smith and Eisner, 2005). First, given some initial word representations and raw sentences, we compute the compositional vector with function f (in this section, we will assume that we will be using the addition operator). Second, in order to obtain the score of compositional vector, we compute the"
D13-1014,P05-1044,0,0.0282066,"ition is modeled using straightforward vector addition/multiplications; instead, what is learned is the word representation. Figure 3 shows the C-NLM. The learning algorithm is unsupervised, and works by artificially 3 There are works on multiple representations, e.g., (Reisinger and Mooney, 2010); we focus on single representation here. 134 verb v Pverb obj o Figure 4: Co-Compositional Neural Language Model (CoC-NLM) is C-NLM with prototype projection. generating negative examples in a fashion analogous to the NLM learning algorithm of (Collobert and Weston, 2008) and contrastive estimation (Smith and Eisner, 2005). First, given some initial word representations and raw sentences, we compute the compositional vector with function f (in this section, we will assume that we will be using the addition operator). Second, in order to obtain the score of compositional vector, we compute the dot product with vector s ∈ Rn (n is the dimension of the word vector space): verb vector v = d(wv ) and object vector o = d(wo ). score(v, o) = sT f (v, o) = sT (v + o) (8) We also create a corrupted pair by substituting a random verb wverb ′ . The cost function J = max(0, 1 − score(v, o) + score(vc , o)), where vc is the"
D13-1014,D12-1110,0,0.386869,"ow to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in various natural language processing (NLP) tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Turian et al., 2010; Collobert et al., 2011). Recently, modeling of semantic compositionality (Frege, 1892) in vector space has emerged as another important line of research (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013). The goal is to formulate how individual word representations ought to be combined to achieve phrasal or sentential semantics. The main questions for semantic compositionality that we are concerned with are: (1) how can polysemy be handled by a single vector representation per word type, learned by either a distributional or neural model, and (2) how does composition resolve 130 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 130–140, c Seattle, Washington, USA, 18-21 October 2013. 2013"
D13-1014,P10-1097,0,0.164923,"Missing"
D13-1014,I11-1127,0,0.061731,"Missing"
D13-1014,P10-1040,0,0.16006,"pace of company (and vice versa). Introduction Vector space models of words have been very successful in capturing the semantic and syntactic characteristics of individual lexical items (Turney and Pantel, 2010). Much research has addressed the question of how to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in various natural language processing (NLP) tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Turian et al., 2010; Collobert et al., 2011). Recently, modeling of semantic compositionality (Frege, 1892) in vector space has emerged as another important line of research (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013). The goal is to formulate how individual word representations ought to be combined to achieve phrasal or sentential semantics. The main questions for semantic compositionality that we are concerned with are: (1) how can polysemy be handled by a single vector representation per w"
D13-1014,N13-1134,0,0.625416,"Missing"
D13-1014,C10-1142,0,0.0217523,"ded survey papers. One of the first approaches is the vector addition/multiplication idea of Mitchell and Lapata (2008). The appeal of this kind of simple approach is its intuitive geometric interpretation and its robustness to various datasets. However, it may not be sufficiently expressive to represent the various factors involved in compositional semantics, such as syntax and context. To this end, Baroni and Zamparelli (2010) present a compositional model for adjectives and nouns. In their model, an adjective is a matrix operator that modifies the noun vector into an adjective-noun vector. Zanzotto et al. (2010) and Guevara (2010) also proposed linear transformation models for composition and address the issue of estimating large matrices with least squares or regression techniques. Socher et al. (2012) extend this linear transformation approach with the more powerful model of Matrix-Vector Recursive Neural Networks (MV-RNN). Each node in a parse tree is assigned both a vector and a matrix. The vector captures the actual meaning of the word itself, while the matrix is modeled as a operator that modify the meaning of neighboring words and phrases. This model captures semantic change phenomenon like no"
D13-1014,2014.lilt-9.5,0,\N,Missing
D13-1014,L08-1000,0,\N,Missing
D18-1194,P13-1023,0,0.590441,"), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score. 1 Introduction We are concerned here with representing the semantics of multiple natural languages in a single semantic analysis. Renewed interest in semantic analysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languag"
D18-1194,P17-1008,0,0.0755862,"alysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabiliti"
D18-1194,P16-1231,0,0.167334,"en created linearized representations for these sentences using PredPatt based on their gold UD syntax. Meanwhile, the Chinese translations of these sentences were created by crowdworkers on Amazon Mechanical Turk. The test dataset will be released upon publication. For training, we first collected about 1.8M Chinese-English sentence bitexts from the GALE project (Cohen, 2007), then tokenized Chinese sentences with Stanford Word Segmenter (Chang et al., 2008). We created linearized representations for English sentences using PredPatt based on automatic UD syntax generated by SyntaxNet Parser (Andor et al., 2016), and added SPR and factuality annotations using the state-of-the-art models (Rudinger et al., 2018b,c) trained on SPR v2.x and It-happened v2.0 respectively.7 We hold out 20K training sentences for validation and indomain test. Table 2 reports the dataset statistics. 6.2 Variants We evaluate our model described in Section 5 and three variants: (a) We replace the coreference annotating mechanism by randomly choosing an an7 Train Validation In-domain Test Test Both datasets are available at http://decomp.net No. sents Source 1,879,172 10,000 10,000 270 GALE GALE GALE UD Treebank Table 2: Statis"
D18-1194,P02-1041,0,0.0281372,"ted. Unlike AMR, but akin to decisions made in PropBank (Palmer et al., 2005) (which forms the majority of the AMR ontological backbone), we target an analysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig."
D18-1194,W13-2322,0,0.785265,"exity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score. 1 Introduction We are concerned here with representing the semantics of multiple natural languages in a single semantic analysis. Renewed interest in semantic analysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 200"
D18-1194,S16-1176,0,0.141024,"derspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•” are all considered as tokens in this representation. The COREF l"
D18-1194,basile-etal-2012-developing,0,0.030024,"levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score. 1 Introduction We are concerned here with representing the semantics of multiple natural languages in a single semantic analysis. Renewed interest in semantic analysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Che"
D18-1194,C04-1180,0,0.125253,"decisions made in PropBank (Palmer et al., 2005) (which forms the majority of the AMR ontological backbone), we target an analysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by"
D18-1194,E17-2039,0,0.0902485,"Missing"
D18-1194,P13-2131,0,0.390925,"explore endto-end cross-lingual learning. 4 Evaluation Metric S UDS can be represented in three forms. Evaluating such forms is crucial to the development of parsing algorithms. However, there is no method directly available for evaluation. Related methods come from semantic parsing, whose results are mainly evaluated in three ways: (1) task correctness (Tang and Mooney, 2001), which evaluates on a specific NLP task that uses the parsing results; (2) whole-parse correctness (Zettlemoyer and Collins, 2005), which counts the number of parsing results that are completely correct; and (3) Smatch (Cai and Knight, 2013), which computes the number of exactly matched edges between two semantic structures. Nevertheless, our task needs an evaluation metric that can be used regardless of specific tasks or domains, and is able to differentiate two UDS graph representations with similar instances, SPR analysis, or attributes. We design an evaluation metric S that computes the similarity between two graph representations. As described in Section 2.1, the graph representation is a tuple G = (V, E). For two graphs G1 = (V1 , E1 ) and G2 = (V2 , E2 ), we define the score S as the maximum soft edge matching score betwee"
D18-1194,W08-0336,0,0.0144433,"Missing"
D18-1194,D16-1257,0,0.300107,"ty (Saur´ı and Pustejovsky, 2009) and word senses (Miller, 1995). The graph representation can be viewed as an underspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. B"
D18-1194,W07-1210,0,0.0403434,"s to provide a semantic analysis which can be used for various types of deep and shallow processing on the target language side. Many forms of semantic analysis are potentially suitable for this goal, e.g., AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and Universal Decompositional Semantics (White et al., 2016). Here we choose Universal Decompositional Semantics (UDS), but note that our approach is applicable to other potential graph semantic formalisms. The reasons for choosing UDS are three-fold: (1) Compatibility: UDS relates to Robust Minimal Recursion Semantics (RMRS) (Copestake, 2007), aiming for a maximal degree of semantic compatibility. With UDS, shallow analysis, such as predicate-argument extraction (Zhang et al., 2017a), can be regarded as producing a semantics which is underspecified and reusable with respect to deeper analysis, such as lexical semantics and inference (White et al., 2016). (2) Robustness and Speed: There exists a robust framework, PredPatt (White et al., 2016), for automatically creating UDS from raw sentences and their Universal Dependencies. PredPatt has been shown to be fast and accurate enough to process large volumes of text (Zhang et al., 2017"
D18-1194,E09-1001,0,0.144899,"were reportedh ( COREF deadh (in one blockh of flats) ) ] [ was hith (by a storm surgeh) ] Figure 2: UDS linearized representation. Deeper analysis such as SPR and factuality is not shown. edges describe instances of variables in the target language. The subscript “h” indicates the syntactic head of an instance. (3) Attribute edges are unary, which describe various attributes of variables, such as event factuality (Saur´ı and Pustejovsky, 2009) and word senses (Miller, 1995). The graph representation can be viewed as an underspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting lineari"
D18-1194,1995.tmi-1.2,0,0.525065,"ysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languages, semantic analysis cannot be performed directly, owing to a lack of training data. While there is active work in the community focused on rapid construction of resources for low resource Benjamin Van Durme Johns Hopkins University languages (Strassel and Tracey, 2016), it remains an expensive and perhaps infeasible solution to assume in-language annotated resources for developing semantic parsing technologies. In contrast, bitext is easier to get: it occurs often witho"
D18-1194,P16-1004,0,0.0321227,"Miller, 1995). The graph representation can be viewed as an underspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•”"
D18-1194,C16-1056,0,0.0491978,"ediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (201"
D18-1194,N15-1151,0,0.0280355,"hen maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual"
D18-1194,C04-1134,0,0.0308449,"t al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languages, semantic analysis cannot be performed directly, owing to a lack of training data. While there is active work in the community focused on rapid construction of resources for low resource Benjamin Van Durme Johns Hopkins University languages (Strassel and Tracey, 2016), it remains an expensive and perhaps infeasible solution to assume in-language annotated resources for developing semantic parsing technologies. In contrast, bitext is easier to get: it occurs often without researcher involvement,1 and even when not available, it may be"
D18-1194,P09-1042,0,0.0406505,"ic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end cross-lingual learning"
D18-1194,D17-1195,0,0.0187608,"sm5 and decompositional analysis. As illustrated in Fig. 3, Encoder transforms the input sequence into hidden states; Decoder reads the hidden states, and then at each time step generates a token and creates its COREF link; Decompositional Analysis, based on the decoder output, performs SPR analysis for predicate-argument pairs, and factuality analysis for predicates. 4 Future work could consider, e.g., a modified BLEU that considers Levenshtein distance between tokens for a more robust partial-scoring in the face of transliteration errors. 5 Similar coreference mechanism has been proposed by Ji et al. (2017). 1667 Decompostional Analysis AWARENESS SENTIENT INSTIGATION … Decoder 0.8 0.9 0.1 … FACTUAL 1.0 SPR Module Factuality Module y&lt;t and c&lt;t are the preceding tokens and their antecedents. We omit y&lt;t and c&lt;t from the notation when the context is unambiguous. The decoding probability at each time step t is decomposed as P (yt , ct ) = P (yt )P (ct |yt ) COREF peopleh ) were reportedh ( deadh where P (yt ) is the token generation probability, and P (ct |yt ) is the antecedent probability. Token Generation: The probability distribution of the generated token yt is defined as Token Generation & Cor"
D18-1194,P16-1002,0,0.108668,"h representation can be viewed as an underspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•” are all considered as"
D18-1194,P17-4012,0,0.0235319,"preprocess the data by replacing the special symbol “•” with the syntactic head of its antecedent. During training and testing, we replace the coreference annotating mechanism with a heuristic that solves coreference by randomly choosing an antecedent among preceding instances which have the same syntactic head. (c) We remove the decoder-side information in the token representation γ(yt ) defined in Equation (12) and only keep the encoder-side information at . We also include a Pipeline approach where Chinese sentences are first translated into English by a neural machine translation system (Klein et al., 2017) and are then annotated by a UD parser (Andor et al., 2016). The UDS linearized representation of Pipeline are created by PredPatt based the automatic UD parses. 6.3 Results Table 1 reports the experimental results on the test set. Results on the in-domain test set are similar and shown in Appendix D. In Table 1, S metric (defined in Section 4) measures the similarity between predicted and reference graph representations. Based on the optimal variable mapping provided by the S metric, we are able to evaluate our model and the variants in different aspects: BLEUINST measures the BLEU score of a"
D18-1194,D17-1018,0,0.0230523,"s. Coref Link: The probability of yt referring to the preceding token yk , i.e., ct = yk , is defined as SCORE(yt , yk ) where yt is the decoded token at time step t, and ct is the source of the COREF link for yt , i.e., the antecedent of yt . The set of possible antecedents of yt is A(t) = {, y1 , . . . , yt−1 }: a dummy antecedent  and all preceding tokens.  represents a scenario, where the token is not a special symbol “•”, and it refers to none of the preceding tokens. αt,i hi , i = sc (yt ) + sp (yk ) + sa (yt , yk ) (8) There are three factors in this pairwise score, which is akin to Lee et al. (2017): (1) sc (yt ), whether yt should refer to a preceding instance; (2) sp (yk ), whether yk shoud be a candidate source of such a coreference; and (3) sa (yt , yk ), whether yk is an antecedent of yt . Fig. 4 shows the details of the scoring architecture. At the core of the three factors are vector representations γ(yt ) for each token yt , which is described in detail in the following section. Given 1668 COREF link score SCORE(yt, yk) Preceding token score sp(yk) SPR: Given a predicate-argument pair (yi , yj ), we (yi ,yj ) denote the score for SPR property p as DSPR . p As shown in Fig. 3, we"
D18-1194,D15-1166,0,0.0237123,"Missing"
D18-1194,D11-1006,0,0.0774706,"Missing"
D18-1194,S18-2017,1,0.868796,"Missing"
D18-1194,1991.mtsummit-papers.9,0,0.0630016,".3 Embracing underspecification in the name of tractability is exemplified by MRS (Copestake et al., 2005; Copestake, 2009), the so-called slacker semantics, and we draw inspiration from that work. Analyses such as AMR (Banarescu et al., 2013) also make use of underspecification, but usually this is only implicit: certain aspects of meaning are simply not annotated. Unlike AMR, but akin to decisions made in PropBank (Palmer et al., 2005) (which forms the majority of the AMR ontological backbone), we target an analysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to"
D18-1194,P12-1066,0,0.0198033,"es (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end cross-lingual learning. 4 Evaluation Metric S UDS can be represented"
D18-1194,H05-1108,0,0.115905,"Missing"
D18-1194,J05-1004,0,0.288422,"an analysis, we are pursuing a strategy that incrementally increases the complexity of the target analysis in accordance with our ability to fashion models capable of producing it.3 Embracing underspecification in the name of tractability is exemplified by MRS (Copestake et al., 2005; Copestake, 2009), the so-called slacker semantics, and we draw inspiration from that work. Analyses such as AMR (Banarescu et al., 2013) also make use of underspecification, but usually this is only implicit: certain aspects of meaning are simply not annotated. Unlike AMR, but akin to decisions made in PropBank (Palmer et al., 2005) (which forms the majority of the AMR ontological backbone), we target an analysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies ba"
D18-1194,P02-1040,0,0.104017,"Missing"
D18-1194,E17-1035,0,0.355092,"pendency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•” are all considered as tokens in this representation. The COREF links are drawn as a"
D18-1194,P14-2006,0,0.0282414,"has the access to the sentence in the source language during the encoding stage,8 the performance is comparable to the state-of-the-art monolingual model. Table 3: Coreference evaluation (MUC) based on forced decoding. 7 Coreference occurs 589 times in the test set. To evaluate the coreference accuracy of our model, we force the decoder to generate the reference target sequence, and only predict coreference via the copy mechanism, or its variants. In Table 3, we report the precision, recall, and F1 for the standard MUC using the official coreference scorer of the CoNLL-2011/2012 shared tasks (Pradhan et al., 2014). Since coreference in our setup occurs at the sentence level, our model achieves high performance. Variant (a) randomly choosing antecedents performs poorly, whereas variant (b), which solves coreference only based on syntactic heads, achieves a relatively high score. Variant (c) demonstrates that only using encoder-side information in the coreference annotating mechanism leads a significant performance drop. We introduce the task of cross-lingual decompositional semantic parsing, which maps content provided in a source language into decompositional analysis based on a target language. We pre"
D18-1194,D17-1009,0,0.0579582,"Missing"
D18-1194,Q15-1034,1,0.907465,"Missing"
D18-1194,D18-1114,1,0.898981,"Missing"
D18-1194,N18-1067,1,0.885848,"Missing"
D18-1194,W14-2411,0,0.0162147,", and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score. 1 Introduction We are concerned here with representing the semantics of multiple natural languages in a single semantic analysis. Renewed interest in semantic analysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languages, semantic analysis cannot be performed directly, owing to a lack of training data. While there is active work in the community focused on rapid construction of resources for low resource Benjamin"
D18-1194,silveira-etal-2014-gold,0,0.0744862,"Missing"
D18-1194,L16-1521,0,0.0194704,"1; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languages, semantic analysis cannot be performed directly, owing to a lack of training data. While there is active work in the community focused on rapid construction of resources for low resource Benjamin Van Durme Johns Hopkins University languages (Strassel and Tracey, 2016), it remains an expensive and perhaps infeasible solution to assume in-language annotated resources for developing semantic parsing technologies. In contrast, bitext is easier to get: it occurs often without researcher involvement,1 and even when not available, it may be easier to find bilingual speakers that can translate a text, than it is to find experts that will create in-language semantic annotations. In addition, we are simply further along in being able to automatically understand English than we are other languages, resulting from the bias in investment in English-rooted resources. Th"
D18-1194,C04-1127,0,0.0352355,"t al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end cross-lingual learning. 4 Evaluation Metric S UDS can be represented in three forms. Evaluating such forms is crucial to the development of parsing algorithms. However, there is no method directly available for evaluation. Related methods come from semantic parsing, whose results are mainly evaluated in three ways: (1) task correctness (Tang and Mooney, 2001), which evaluates on a specific NLP task that uses the pa"
D18-1194,I17-1084,1,0.895202,"Missing"
D18-1194,Q14-1005,0,0.0228795,"2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end cross-lingual learning. 4 Evaluation Metric S UDS can be represented in three forms. Evaluati"
D18-1194,D16-1177,1,0.853308,"Missing"
D18-1194,N06-1056,0,0.0592432,"RS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•” are all considered as tokens in this representation. The COREF links are drawn as a visual convenience, and the actual linearized representation achieves this via co-indexing, and is thus fully linear. We de"
D18-1194,H01-1035,0,0.100566,"002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al."
D18-1194,I08-3008,0,0.0379404,"tial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end"
D18-1194,E17-2011,1,0.872845,"Missing"
D19-1142,D16-1162,0,0.0233046,"isier lexicon that more closely mimics real-world lexicons (e.g. by adding irrelevant entries or relevant morphological variants, lemmatizing entries, or subsampling). At a high level, our lexicons are created by a twostep process: (1) identifying interesting words on the source side of the test and development sets, and (2) human annotators correcting or validating automatic alignments of the identified words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be u"
D19-1142,2012.eamt-1.60,0,0.0263817,"e general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be particularly challenging for MT, but given time and resources there is nothing to prevent the application of the annotation protocol to other terms. 6 For strong initial alignments, GIZA++ (Och and Ney, 2003) is trained on train, development, and test for all data described in section 4 as well as a TED talk corpus (Cettolo et al., 2012). 7 While we want the dictionary to match the reference, we did not want to train on large phrases from the reference. Evaluation Metrics We evaluate lexicon incorporation approaches using two main metrics: BLEU and recall. For each annotated instance of a source-side lexical entry, we can check whether the system output contains the correct aligned target-side translation. Recall is computed as the percentage of the time that the system produces the correct output, averaged over all annotations. Note that this does not guarantee that the words are placed in a sensible location in the sentence"
D19-1142,W17-4716,0,0.0166719,"words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be used to ensure that target entries from a bilingual lexicon be present in the MT output whenever their corresponding source entries are present in the input. Constrained decoding with multiple target options (e.g. when a source word can be translated into one of several target words) is addressed in Chatterjee et al. (2017) and Hasler et al. (2018). 2.3 Datasets Used in Prior Studies Prior work has used eith"
D19-1142,P17-2090,0,0.216085,"than would be found in most realworld scenarios. However, it could also be used as a standardized starting point to produce a noisier lexicon that more closely mimics real-world lexicons (e.g. by adding irrelevant entries or relevant morphological variants, lemmatizing entries, or subsampling). At a high level, our lexicons are created by a twostep process: (1) identifying interesting words on the source side of the test and development sets, and (2) human annotators correcting or validating automatic alignments of the identified words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies b"
D19-1142,grimes-etal-2012-automatic,0,0.0487356,"Missing"
D19-1142,N18-2081,0,0.0304261,"Missing"
D19-1142,E17-3017,0,0.0316391,"ible effect on BLEU score results. To build strong baseline systems, we first train models on general domain data. As general domain data, we use the OpenSubtitles18 (Lison et al., 2018) corpora for both Ru→En and Ko→En. For Ru→En and Zh→En we also use the parallel portion of the WMT17 news translation task (Bojar et al., 2017). We then fine-tune these generaldomain models on WIPO training data (Luong and Manning, 2015), using the dev set for validation. These domain-adapted models are then used as the initial systems for our lexicon incorporation experiments. We build the systems in Sockeye (Hieber et al., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be particularly challenging for MT, but given time and resources there is nothing to prevent the application of the annotation protocol to other terms. 6 For strong initial alignments, GIZA++ (Och and"
D19-1142,P17-1141,0,0.0221676,"ents of the identified words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be used to ensure that target entries from a bilingual lexicon be present in the MT output whenever their corresponding source entries are present in the input. Constrained decoding with multiple target options (e.g. when a source word can be translated into one of several target words) is addressed in Chatterjee et al. (2017) and Hasler et al. (2018). 2.3 Datasets Used in Prior Studies"
D19-1142,P07-2045,1,0.0239386,"icon integration into neural machine translation. Our data consists of humangenerated alignments of words and phrases in machine translation test sets in three language pairs (Russian→English, Chinese→English, and Korean→English), resulting in clean bilingual lexicons which are well matched to the reference. We also present two simple baselines—constrained decoding and continued training—and an improvement to continued training to address overfitting. 1 Lexical Entry Target Neural machine translation (NMT) is the current state-of-the-art. In contrast with statistical machine translation (SMT; Koehn et al., 2007), where there are several established methods of incorporating external knowledge,1 recent work is still examining how best to incorporate bilingual lexicons into NMT systems. Bilingual lexicon integration is desirable in a number of scenarios: highly technical vocabulary (which might be rare, or require translations of a domain-specific sense), lowerresource settings (where bilingual lexicons might be a significant portion of the available parallel data), translation settings where a client requires particular terms to be used (e.g. brand names), or for improving rare word translation. At pre"
D19-1142,N19-1209,1,0.842185,"ensure high quality, (2) derived from the development and test references so the best-case-scenario impact on translation performance can be directly measured, (3) covering 3 language pairs, and (4) focused on challenging words. We perform exploratory work on our development set, showing two representative baselines to compare incorporating the lexicon at training time (continued training) vs. at decoding time (constrained decoding). We examine the tradeoffs in terms of BLEU, recall, and speed. We also present a novel application of Elastic Weight Consolidation (EWC; Kirkpatrick et al., 2017; Thompson et al., 2019) which significantly improves performance by preventing overfitting during continued training on the bilingual lexicon. 2 Related Work We first review prior work on incorporation of bilingual lexicons into NMT and then discuss the datasets used and explain how our new dataset addresses some shortcomings. Recent work on the incorporation of bilingual lexicons into NMT systems can be loosely clus2 http://www.cs.jhu.edu/~kevinduh/a/ hablex2019/ 1382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language"
D19-1142,W18-2708,1,0.821566,"ng). At a high level, our lexicons are created by a twostep process: (1) identifying interesting words on the source side of the test and development sets, and (2) human annotators correcting or validating automatic alignments of the identified words. Incorporation at Training Time Zhang and Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be used to ensure that target entries from a bilingual lexicon be present in the MT output whenever their corresponding source entries are present in the input. Constr"
D19-1142,L18-1275,0,0.0173633,"alignment, adding or removing source or target side words as needed to complete a valid alignment. Since annotation is cumbersome with very long sentences, we omit sentences of length 100 or more tokens. Filtering out numerical entries and phrases longer than 3 words7 results in bilingual lexicons with sizes shown in Table 2. The data contains a small number of discontiguous alignments; these are so infrequent as to have a negligible effect on BLEU score results. To build strong baseline systems, we first train models on general domain data. As general domain data, we use the OpenSubtitles18 (Lison et al., 2018) corpora for both Ru→En and Ko→En. For Ru→En and Zh→En we also use the parallel portion of the WMT17 news translation task (Bojar et al., 2017). We then fine-tune these generaldomain models on WIPO training data (Luong and Manning, 2015), using the dev set for validation. These domain-adapted models are then used as the initial systems for our lexicon incorporation experiments. We build the systems in Sockeye (Hieber et al., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. W"
D19-1142,2015.iwslt-evaluation.11,0,0.0790031,"ies and phrases longer than 3 words7 results in bilingual lexicons with sizes shown in Table 2. The data contains a small number of discontiguous alignments; these are so infrequent as to have a negligible effect on BLEU score results. To build strong baseline systems, we first train models on general domain data. As general domain data, we use the OpenSubtitles18 (Lison et al., 2018) corpora for both Ru→En and Ko→En. For Ru→En and Zh→En we also use the parallel portion of the WMT17 news translation task (Bojar et al., 2017). We then fine-tune these generaldomain models on WIPO training data (Luong and Manning, 2015), using the dev set for validation. These domain-adapted models are then used as the initial systems for our lexicon incorporation experiments. We build the systems in Sockeye (Hieber et al., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be p"
D19-1142,J03-1002,0,0.0247052,"., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be particularly challenging for MT, but given time and resources there is nothing to prevent the application of the annotation protocol to other terms. 6 For strong initial alignments, GIZA++ (Och and Ney, 2003) is trained on train, development, and test for all data described in section 4 as well as a TED talk corpus (Cettolo et al., 2012). 7 While we want the dictionary to match the reference, we did not want to train on large phrases from the reference. Evaluation Metrics We evaluate lexicon incorporation approaches using two main metrics: BLEU and recall. For each annotated instance of a source-side lexical entry, we can check whether the system output contains the correct aligned target-side translation. Recall is computed as the percentage of the time that the system produces the correct output"
D19-1142,N18-1119,0,0.0605426,"nd Zong (2016) and Fadaee et al. (2017) both propose using bilingual lexicons to create synthetic bitext to augment training data for NMT systems. Arthur et al. (2016) use translation probabilities from a lexicon (like SMT phrase tables) in conjunction with NMT probabilities. 2.2 Incorporation at Decode Time Kothur et al. (2018) perform fine-grained continued training adaptation on very small, documentspecific bilingual lexicons of novel words.3 A popular inference-time approach is constrained decoding (Anderson et al., 2016; Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018), which modifies beam search to require that user-specified words or phrases to be present in the output hypotheses. Constrained decoding can be used to ensure that target entries from a bilingual lexicon be present in the MT output whenever their corresponding source entries are present in the input. Constrained decoding with multiple target options (e.g. when a source word can be translated into one of several target words) is addressed in Chatterjee et al. (2017) and Hasler et al. (2018). 2.3 Datasets Used in Prior Studies Prior work has used either human-generated general purpose bilingual"
D19-1142,P16-1162,0,0.0667478,"the WMT17 news translation task (Bojar et al., 2017). We then fine-tune these generaldomain models on WIPO training data (Luong and Manning, 2015), using the dev set for validation. These domain-adapted models are then used as the initial systems for our lexicon incorporation experiments. We build the systems in Sockeye (Hieber et al., 2017), using a two-layer LSTM network with hidden unit size 512. We use an initial learning rate of 3e-4 both for training the general domain model and adapting to WIPO. We apply the Moses tokenizer (Koehn et al., 2007), lowercasing, and bytepair encoding (BPE; Sennrich et al., 2016) with a vocabulary size of 30k. BPE is trained on the general domain corpus only, then applied to all data. 4.1 them to be particularly challenging for MT, but given time and resources there is nothing to prevent the application of the annotation protocol to other terms. 6 For strong initial alignments, GIZA++ (Och and Ney, 2003) is trained on train, development, and test for all data described in section 4 as well as a TED talk corpus (Cettolo et al., 2012). 7 While we want the dictionary to match the reference, we did not want to train on large phrases from the reference. Evaluation Metrics"
D19-1392,P13-1023,0,0.397933,"based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP. 1 Introduction Broad-coverage semantic parsing aims at mapping any natural language text, regardless of its domain, genre, or even the language itself, into a general-purpose meaning representation. As a long-standing topic of interest in computational linguistics, broad-coverage semantic parsing has targeted a number of meaning representation frameworks, including CCG (Steedman, 1996, 2001), DRS (Kamp and Reyle, 1993; Bos, 2008), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), SDP (Oepen et al., 2014, 2015), and UDS (White et al., 2016).1 Each of these frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptual Cognitive Annotation, Semantic"
D19-1392,S15-2162,0,0.0397619,"Missing"
D19-1392,P13-1104,0,0.034945,"Missing"
D19-1392,D15-1198,0,0.158101,"Missing"
D19-1392,E17-1051,0,0.734714,"arsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptual Cognitive Annotation, Semantic Dependency Parsing, and Universal Decompositional Semantics. Manning, 2018; Peng et al., 2017a) are not directly transferable to AMR and UCCA because of the lack of explicit alignments between tokens in the sentence and nodes in the semantic graph. While transition-based approaches are adaptable to different broad-coverage semantic parsing tasks (Wang et al., 2018; Hershcovich et al., 2018; Damonte et al., 2017), when it comes to representations such as AMR whose nodes are unanchored to tokens in the sentence, a pre-trained aligner has to be used to produce the reference transition sequences (Wang et al., 2015; Damonte et al., 2017; Peng et al., 2017b). In contrast, there are attempts to develop attention-based approaches in a graph-based parsing paradigm (Dozat and Manning, 2018; Zhang et al., 2019), but they lack parsing incrementality, which is advocated in terms of computational efficiency and cognitive modeling (Nivre, 2004; Huang and Sagae, 2010). In this paper, we approach different broadcover"
D19-1392,D17-1130,0,0.140818,"opular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without rel"
D19-1392,W13-2322,0,0.351025,"demonstrate that our attentionbased neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP. 1 Introduction Broad-coverage semantic parsing aims at mapping any natural language text, regardless of its domain, genre, or even the language itself, into a general-purpose meaning representation. As a long-standing topic of interest in computational linguistics, broad-coverage semantic parsing has targeted a number of meaning representation frameworks, including CCG (Steedman, 1996, 2001), DRS (Kamp and Reyle, 1993; Bos, 2008), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), SDP (Oepen et al., 2014, 2015), and UDS (White et al., 2016).1 Each of these frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptu"
D19-1392,P81-1022,0,0.682397,"Missing"
D19-1392,P18-2077,0,0.325854,"plicit alignments between tokens in the sentence and nodes in the semantic graph. While transition-based approaches are adaptable to different broad-coverage semantic parsing tasks (Wang et al., 2018; Hershcovich et al., 2018; Damonte et al., 2017), when it comes to representations such as AMR whose nodes are unanchored to tokens in the sentence, a pre-trained aligner has to be used to produce the reference transition sequences (Wang et al., 2015; Damonte et al., 2017; Peng et al., 2017b). In contrast, there are attempts to develop attention-based approaches in a graph-based parsing paradigm (Dozat and Manning, 2018; Zhang et al., 2019), but they lack parsing incrementality, which is advocated in terms of computational efficiency and cognitive modeling (Nivre, 2004; Huang and Sagae, 2010). In this paper, we approach different broadcoverage semantic parsing tasks under a unified framework of transduction. We propose an attention-based neural transducer that extends the two-stage semantic parser of Zhang et al. (2019) to directly transduce input text into a meaning representation in one stage. This transducer has properties of both transition-based approaches and graph-based approaches: on the one hand, it"
D19-1392,S16-1176,0,0.0606068,"ty. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms – DM (DELPH-IN MRS; Flickinger et al., 2012; Oepen and Lønning, 2006), PAS (Predicate-Argument Structures; Miyao and Tsujii, 2004"
D19-1392,S15-2154,0,0.143571,"Missing"
D19-1392,W08-2222,0,0.0287166,"R, SDP and UCCA – demonstrate that our attentionbased neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP. 1 Introduction Broad-coverage semantic parsing aims at mapping any natural language text, regardless of its domain, genre, or even the language itself, into a general-purpose meaning representation. As a long-standing topic of interest in computational linguistics, broad-coverage semantic parsing has targeted a number of meaning representation frameworks, including CCG (Steedman, 1996, 2001), DRS (Kamp and Reyle, 1993; Bos, 2008), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), SDP (Oepen et al., 2014, 2015), and UDS (White et al., 2016).1 Each of these frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Repr"
D19-1392,P15-1033,0,0.0336696,"are graph-based: Peng et al. (2017a, 2018) use a max-margin classifier on top of a BiLSTM, with the factored score for each graph over predicates, unlabeled arcs, and arc labels. Multi-task learning approaches and disjoint data have been used to improve the parser performance. Dozat and Manning (2018) extend an LSTM-based syntactic dependency parser to produce graph-structured dependencies, and carefully tune it to state of the art performance. Wang et al. (2018) extend the transition system of Choi and McCallum (2013) to produce non-projective trees, and use improved versions of stack-LSTMs (Dyer et al., 2015) to learn representation for key components. All of these are specialized for bi-lexical dependency parsing, whereas our parser can effectively produce both bi-lexical semantics graphs, and graphs that are less anchored to the surface utterance. Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion. Sentence representations in UCCA are directed acyclic graphs (DAG), where terminal nodes correspond to surface lexical tokens, and non-t"
D19-1392,D19-1393,0,0.542525,"Missing"
D19-1392,P13-2131,0,0.446911,"Missing"
D19-1392,P14-1134,0,0.372446,"es, named entities, negation and modality, into a rooted, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015),"
D19-1392,S16-1180,0,0.0403568,"rface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noo"
D19-1392,D18-1198,0,0.663725,"Missing"
D19-1392,P17-1104,0,0.114026,"nce. Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion. Sentence representations in UCCA are directed acyclic graphs (DAG), where terminal nodes correspond to surface lexical tokens, and non-terminal nodes to semantic units that participate in super-ordinate relations. Edges are labeled, indicating the role of a child in the relation the parent represents. Fig. 1(c) shows an example UCCA DAG. The first UCCA parser is proposed by Hershcovich et al. (2017), where they extend a transition system to produce DAGs. To leverage other semantic resources, Hershcovich et al. (2018) is one of the few attempts to present (lossy) conversion from AMR, SDP and Universal Dependencies (UD; Nivre et al., 2016) to a unified UCCA3787 express-01 ARG0 person op1 Pierre expressed ARG1 concern-01 ARG1 Pierre op2 Vinken Pierre (3) op2 Vinken (4) ARG1 concern ARG1 person (2) Term Term top (1) Pierre (3) ARG2 concern (4) Phrase his (5) (e) DM arborescence A (6) H A H (2) A A P P poss-of Pierre (3) Term Pierre Vinken expressed his concern expressed (1) compound-of P Ter"
D19-1392,P18-1035,0,0.158075,"that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion. Sentence representations in UCCA are directed acyclic graphs (DAG), where terminal nodes correspond to surface lexical tokens, and non-terminal nodes to semantic units that participate in super-ordinate relations. Edges are labeled, indicating the role of a child in the relation the parent represents. Fig. 1(c) shows an example UCCA DAG. The first UCCA parser is proposed by Hershcovich et al. (2017), where they extend a transition system to produce DAGs. To leverage other semantic resources, Hershcovich et al. (2018) is one of the few attempts to present (lossy) conversion from AMR, SDP and Universal Dependencies (UD; Nivre et al., 2016) to a unified UCCA3787 express-01 ARG0 person op1 Pierre expressed ARG1 concern-01 ARG1 Pierre op2 Vinken Pierre (3) op2 Vinken (4) ARG1 concern ARG1 person (2) Term Term top (1) Pierre (3) ARG2 concern (4) Phrase his (5) (e) DM arborescence A (6) H A H (2) A A P P poss-of Pierre (3) Term Pierre Vinken expressed his concern expressed (1) compound-of P Term (c) UCCA Vinken (2) (d) AMR arborescence Term (b) DM ARG1 concern-01 (5) A his poss express-01 (1) person (2) P ARG2 c"
D19-1392,P17-1014,0,0.154646,"tion systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms – DM (DELPH-IN MRS; Flickinger et al., 2012; Oepen and Lønning, 2006), PAS (Predicate-Argument Structures; Miyao and Tsujii, 2004), and PSD (Prague Semantic Dependencies; Hajiˇc et al., 2012) – representing predicate-argument relations between content words in a s"
D19-1392,P19-1450,0,0.10329,"Missing"
D19-1392,P18-1037,0,0.15254,"d, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of w"
D19-1392,S16-1166,0,0.0963677,"Abstract Meaning Representation (AMR; Banarescu et al., 2013) encodes sentence-level semantics, such as predicate-argument information, reentrancies, named entities, negation and modality, into a rooted, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produc"
D19-1392,S17-2090,0,0.0512281,"eaning Representation (AMR; Banarescu et al., 2013) encodes sentence-level semantics, such as predicate-argument information, reentrancies, named entities, negation and modality, into a rooted, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions."
D19-1392,C04-1204,0,0.158385,"rzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms – DM (DELPH-IN MRS; Flickinger et al., 2012; Oepen and Lønning, 2006), PAS (Predicate-Argument Structures; Miyao and Tsujii, 2004), and PSD (Prague Semantic Dependencies; Hajiˇc et al., 2012) – representing predicate-argument relations between content words in a sentence. Their annotations have been converted into bi-lexical dependencies, forming directed graphs whose nodes injectively correspond to surface lexical units, and edges represent semantic relations between nodes. In this work, we focus on only the DM formalism. Fig. 1(b) shows an example DM graph. Most recent parsers for SDP are graph-based: Peng et al. (2017a, 2018) use a max-margin classifier on top of a BiLSTM, with the factored score for each graph over p"
D19-1392,P19-1451,0,0.463745,"Missing"
D19-1392,P10-1110,0,0.0867168,"Missing"
D19-1392,S15-2153,0,0.55673,"Missing"
D19-1392,S14-2008,0,0.411139,"the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP. 1 Introduction Broad-coverage semantic parsing aims at mapping any natural language text, regardless of its domain, genre, or even the language itself, into a general-purpose meaning representation. As a long-standing topic of interest in computational linguistics, broad-coverage semantic parsing has targeted a number of meaning representation frameworks, including CCG (Steedman, 1996, 2001), DRS (Kamp and Reyle, 1993; Bos, 2008), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), SDP (Oepen et al., 2014, 2015), and UDS (White et al., 2016).1 Each of these frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptual Cognitive Annotation, Semantic Dependency Parsing, and U"
D19-1392,oepen-lonning-2006-discriminant,0,0.0660154,"anslation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms – DM (DELPH-IN MRS; Flickinger et al., 2012; Oepen and Lønning, 2006), PAS (Predicate-Argument Structures; Miyao and Tsujii, 2004), and PSD (Prague Semantic Dependencies; Hajiˇc et al., 2012) – representing predicate-argument relations between content words in a sentence. Their annotations have been converted into bi-lexical dependencies, forming directed graphs whose nodes injectively correspond to surface lexical units, and edges represent semantic relations between nodes. In this work, we focus on only the DM formalism. Fig. 1(b) shows an example DM graph. Most recent parsers for SDP are graph-based: Peng et al. (2017a, 2018) use a max-margin classifier on t"
D19-1392,P17-1186,0,0.105416,"Missing"
D19-1392,N18-1135,0,0.123138,"Missing"
D19-1392,K15-1004,0,0.0393825,"hou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms –"
D19-1392,E17-1035,0,0.244366,"se frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptual Cognitive Annotation, Semantic Dependency Parsing, and Universal Decompositional Semantics. Manning, 2018; Peng et al., 2017a) are not directly transferable to AMR and UCCA because of the lack of explicit alignments between tokens in the sentence and nodes in the semantic graph. While transition-based approaches are adaptable to different broad-coverage semantic parsing tasks (Wang et al., 2018; Hershcovich et al., 2018; Damonte et al., 2017), when it comes to representations such as AMR whose nodes are unanchored to tokens in the sentence, a pre-trained aligner has to be used to produce the reference transition sequences (Wang et al., 2015; Damonte et al., 2017; Peng et al., 2017b). In contrast, there are attempts"
D19-1392,D14-1162,0,0.083512,"the partial semantic graph, which helps prune the output space for both nodes and edges. Since at each decoding step, we assume the incoming edge is always from a preceding node (see Section 4.3 for the details), the predicted semantic graph is guaranteed to be a valid arborescence, and a MST algorithm is no longer needed. 4.1 Encoder At the encoding stage, we employ an encoder embedding module to convert the input text into vector representations, and a BiLSTM is used to encode vector representations into hidden states. Encoder Embedding Module concatenates word-level embeddings from GloVe (Pennington et al., 2014) and BERT2 (Devlin et al., 2018), char-level embeddings from CharCNN (Kim et al., 2016), and randomly initialized embeddings for POS tags. For AMR, it includes extra randomly initialized embeddings for anonymization indicators that tell the encoder whether a token is an anonymized token from preprocessing. For UCCA, it includes extra randomly initialized embeddings for NER tags, syntactic dependency labels, punctuation indicators, and shapes that are provided in the UCCA official dataset. Multi-layer BiLSTM (Hochreiter and Schmidhuber, 1997) is defined as:  →   −−−→ l−1 l  − s lt LSTM (st"
D19-1392,D15-1136,0,0.123967,"Missing"
D19-1392,P17-1099,0,0.198219,"source node index du , a relation type r, a target node label v, and a target node index dv . Let Y be the output space. The unified transduction problem is to seek the most-likely sequence of semantic relations Yˆ given X: Yˆ = arg max P(Y |X) Y ∈Y = arg max Y ∈Y 4 m Y P(yi |y&lt;i , X) i Transducer To tackle the unified transduction problem, we introduce an attention-based neural transducer that extends Zhang et al. (2019)’s attention-based parser. Their attention-based parser addresses semantic parsing in a two-stage process: it first employs an extended variant of pointer-generator network (See et al., 2017) to convert the input text into a list of nodes, and then uses a deep biaffine graph-based parser (Dozat and Manning, 2016) with a maximum spanning tree (MST) algorithm to create edges. In contrast, our attention-based neural transducer directly transduces the input text into a meaning representation in one stage via a sequence of semantic relations. A high-level model architecture of our transducer is depicted in Fig. 2: an encoder first encodes the input text into hidden states; and then conditioned on the hidden states, at each decoding time step, a decoder takes the previous semantic relat"
D19-1392,J01-1008,0,0.0959536,"Missing"
D19-1392,S16-1181,0,0.147679,"Missing"
D19-1392,D17-1129,0,0.400515,"Missing"
D19-1392,N15-1040,0,0.615577,"Dependency Parsing, and Universal Decompositional Semantics. Manning, 2018; Peng et al., 2017a) are not directly transferable to AMR and UCCA because of the lack of explicit alignments between tokens in the sentence and nodes in the semantic graph. While transition-based approaches are adaptable to different broad-coverage semantic parsing tasks (Wang et al., 2018; Hershcovich et al., 2018; Damonte et al., 2017), when it comes to representations such as AMR whose nodes are unanchored to tokens in the sentence, a pre-trained aligner has to be used to produce the reference transition sequences (Wang et al., 2015; Damonte et al., 2017; Peng et al., 2017b). In contrast, there are attempts to develop attention-based approaches in a graph-based parsing paradigm (Dozat and Manning, 2018; Zhang et al., 2019), but they lack parsing incrementality, which is advocated in terms of computational efficiency and cognitive modeling (Nivre, 2004; Huang and Sagae, 2010). In this paper, we approach different broadcoverage semantic parsing tasks under a unified framework of transduction. We propose an attention-based neural transducer that extends the two-stage semantic parser of Zhang et al. (2019) to directly transd"
D19-1392,D16-1177,1,0.902229,"Missing"
D19-1392,P19-1009,1,0.699796,"Missing"
D19-1392,D18-1194,1,0.890588,"Missing"
D19-1392,D16-1065,0,0.0450789,"ality, into a rooted, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 201"
E14-4030,P13-2107,0,0.075521,"Missing"
E14-4030,J99-2004,0,0.856615,"Missing"
E14-4030,P06-1037,0,0.456152,"ing Dependency Parsers with Supertags Hiroki Ouchi Kevin Duh Yuji Matsumoto Computational Linguistics Laboratory Nara Institute of Science and Technology {ouchi.hiroki.nt6, kevinduh, matsu}@is.naist.jp Abstract mation that imposes complex constraints in a local context (Bangalore and Joshi, 1999). While supertags have been used in frameworks based on lexicalized grammars, e.g. Lexicalized TreeAdjoining Grammar (LTAG), Head-driven Phrase Structure Grammar (HPSG) and Combinatory Categorial Grammar (CCG), they have scarcely been utilized for dependency parsing so far. Previous work by Foth et al (2006) demonstrate that supertags improve German dependency parsing under a Weighted Constraint Dependency Grammar (WCDG). Recent work by Ambati et al (2013) show that supertags based on CCG lexicon improves transition-based dependency parsing for Hindi. In particular, they argue that supertags can improve long distance dependencies (e.g. coordination, relative clause) in a morphologicallyrich free-word-order language. Zhang et. al. (2010) define supertags that incorporate that longdistance dependency information for the purpose of HPSG parsing. All these works suggest the promising synergy between"
E14-4030,N10-1115,0,0.0438009,"left or right dependents. For instance, the word ’Monday’ has a left dependent ’Black’, so we encode it as ’PRD/L+L’, where the part before ’+’ specifies the head information (’PRD/L’) and the part afterwards (’L’) specifies the position of the dependent (’L’ for left, ’R’ for right). When a word has its dependents in both left and right directions, such as the word ’was’ in Figure 1, we combine them using ’ ’, as in: ’ROOT+L R’. On our Penn Treebank data, Model 1 has 79 supertags. 3 Supertags as Features in a Transition-based Dependency Parser In this work, we adopt the Easy-First parser of (Goldberg and Elhadad, 2010), a highly-accurate transition-based dependency parser. We describe how we incorporate supertag features in the EasyFirst framework, though it can be done similarly 155 Model Model1 Model2 for other transition-based frameworks like left-toright arc-eager and arc-standard models (Nivre et al., 2006; Yamada and Matsumoto, 2003). In the Easy-First algorithm, a dependency tree is constructed by two kinds of actions: ATTACH L EFT(i) and ATTACH R IGHT(i) to a list of partial tree structures p1 ,...,pk initialized with the n words of the sentence w1 ,...,wn . ATTACH L EFT(i) attaches (pi , p+1 ) and"
E14-4030,P10-1110,0,0.0230229,"baseline features based on POS/word in Goldberg and Elhadad (2010). # tags 79 312 Dev 87.81 87.22 Test 88.12 87.13 Table 3: Supertag accuracy evaluated on development and test set. Dev = development set, PTB 22; Test = test set, PTB 23 4 Experiments To evaluate the effectiveness of supertags as features, we perform experiments on the Penn Treebank (PTB), converted into dependency format with Penn2Malt1 . Adopting standard approach, we split PTB sections 2-21 for training, section 22 for development and 23 for testing. We assigned POS-tags to the training data by ten-fold jackknifing following Huang and Sagae (2010). Development and test sets are automatically tagged by the tagger trained on the training set. For a partial tree structure p, features are defined based on information in its head: we use wp to refer to the surface word form of the head word of p, tp to refer to the head word’s POS tag, and sp to refer to the head word’s supertag. Further, we not only use a supertag as is, but split each supertag into subparts. For instance, the supertag ’ROOT+SUB/L PRD/R’ is split into ’ROOT’, ’SUB/L’ and ’PRD/R’, a supertag representing the supertag head information shp , supertag left dependent informatio"
E14-4030,J93-2004,0,0.0488395,"Missing"
E14-4030,W03-3023,1,0.690838,"ft and right directions, such as the word ’was’ in Figure 1, we combine them using ’ ’, as in: ’ROOT+L R’. On our Penn Treebank data, Model 1 has 79 supertags. 3 Supertags as Features in a Transition-based Dependency Parser In this work, we adopt the Easy-First parser of (Goldberg and Elhadad, 2010), a highly-accurate transition-based dependency parser. We describe how we incorporate supertag features in the EasyFirst framework, though it can be done similarly 155 Model Model1 Model2 for other transition-based frameworks like left-toright arc-eager and arc-standard models (Nivre et al., 2006; Yamada and Matsumoto, 2003). In the Easy-First algorithm, a dependency tree is constructed by two kinds of actions: ATTACH L EFT(i) and ATTACH R IGHT(i) to a list of partial tree structures p1 ,...,pk initialized with the n words of the sentence w1 ,...,wn . ATTACH L EFT(i) attaches (pi , p+1 ) and removes pi+1 from the partial tree list. ATTACH R IGHT(i) attaches (pi+1 , pi ) and removes pi from the partial tree list. Features are extracted from the attachment point as well as two neighboring structures: pi−2 , pi−1 , pi , pi+1 , pi+2 , pi+3 . Table 2 summarizes the supertag features we extract from this neighborhood;"
E14-4030,P11-2033,0,0.0591563,"Missing"
E14-4030,N10-1090,0,0.0416732,"Missing"
E14-4030,W06-2933,0,\N,Missing
E14-4037,J93-2003,0,0.09062,"on It is generally acknowledged that absolute equivalence between two languages is impossible, since concept lexicalization varies across languages. Major translation theories thus argue that texts should be translated ‘sense-for-sense’ instead of ‘word-for-word’ (Nida, 1964). This suggests that unalignable words may be an issue for the parallel text used to train current statistical machine translation (SMT) systems. Although existing automatic word alignment methods have some mechanism to handle the lack of exact word-for-word alignment (e.g. null probabilities, fertility in the IBM models (Brown et al., 1993)), they may be too coarse-grained to model the ’sense-for-sense’ translations created by professional human translators. For example, the Chinese term ‘tai-yang’ literally means ‘sun’, yet the concept it represents is equivalent to the English term ‘the sun’. Since the concept of a definite article is not incorporated in the morphology of ‘tai yang’, the added ‘the’ is not aligned to any Chinese word. Yet in another context like ’the man’, ‘the’ can be the translation 2 Analysis of Unalignable Words Our manually-aligned data, which we call ORACLE data, is a Chinese-to-English corpus released b"
E14-4037,D10-1062,0,0.0159191,"tatistical significant improvements in BLEU and METEOR. 6 Comparing to the results of PBMT, this suggests our method may be most effective in improving systems where rule extraction is sen5 Conclusion We analyzed in-depth the phenomenon of unalignable words in parallel text, and show that what is unalignable depends on the word’s concept and context. We argue that this is not a trivial problem, but with an unalignable word classifier and a simple modified MT training pipeline, we can achieve small but significant gains in end-to-end translation. In related work, the issue of dropped pronouns (Chung and Gildea, 2010) and function words (Setiawan et al., 2010; Nakazawa and Kurohashi, 2012) have been found important in word alignment, and (Fossum et al., 2008) showed that syntax features are helpful for fixing alignments. An interesting avenue of future work is to integrate these ideas with ours, in particular by exploiting syntax and viewing unalignable words as aligned at a structure above the lexical level. 5 We use the standard MT08 test sets; the training data includes LDC2004T08, 2005E47, 2005T06, 2007T23, 2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15, and 2010T03 (34M English words and 1.1M se"
E14-4037,W08-0306,0,0.0229129,"ving systems where rule extraction is sen5 Conclusion We analyzed in-depth the phenomenon of unalignable words in parallel text, and show that what is unalignable depends on the word’s concept and context. We argue that this is not a trivial problem, but with an unalignable word classifier and a simple modified MT training pipeline, we can achieve small but significant gains in end-to-end translation. In related work, the issue of dropped pronouns (Chung and Gildea, 2010) and function words (Setiawan et al., 2010; Nakazawa and Kurohashi, 2012) have been found important in word alignment, and (Fossum et al., 2008) showed that syntax features are helpful for fixing alignments. An interesting avenue of future work is to integrate these ideas with ours, in particular by exploiting syntax and viewing unalignable words as aligned at a structure above the lexical level. 5 We use the standard MT08 test sets; the training data includes LDC2004T08, 2005E47, 2005T06, 2007T23, 2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15, and 2010T03 (34M English words and 1.1M sentences). Since we do not have access to all OpenMT data, e.g. FBIS, our results may not be directly comparable to other systems in the evaluati"
E14-4037,P07-2045,0,0.00470613,"nd F1 of the resulting alignments, and quality of the final MT outputs. Baseline is the standard MT training pipeline without removal of unaligned words. Our Proposed approach performs better in alignment, phrasebased (PBMT) and hierarchical (Hiero) systems. The results, evaluated by BLEU, METEOR and TER, support our hypothesis that removing gold unalignable words helps improve word alignment and the resulting SMT. 3 We can suppress the NULL probabilities of the model. All experiments are done using standard settings for Moses PBMT and Hiero with 4-gram LM and mslrbidirectional-fe reordering (Koehn et al., 2007). The classifier is trained using LIBSVM (Chang and Lin, 2011). 4 2 We define the list as the top 100 word types with the highest count of unalignable words per language according to the hand annotated data. 192 Align acc. ORACLE P .711 Baseline R .488 F1.579 ORACLE P .802 Proposed R .509 (gold) F1.623 REAL Baseline REAL Proposed (predict) B T M B T M B T M B T M PBMT C-E E-C 11.4 17.4 70.9 69.0 21.8 23.9 11.8+ 18.3+ 71.4− 65.7+ 22.1+ 24.1+ 18.2 18.5 63.4 67.2 22.9 24.6 18.6 18.5 63.8− 66.5+ 23.2+ 24.5 Hiero C-E E-C 10.3 15.8 75.9 72.3 21.08 23.7 11.0+ 17.2+ 74.7+ 68.7+ 22.0+ 24.0+ 17.0 17.2 6"
E14-4037,li-etal-2010-enriching,0,0.0150687,"be too coarse-grained to model the ’sense-for-sense’ translations created by professional human translators. For example, the Chinese term ‘tai-yang’ literally means ‘sun’, yet the concept it represents is equivalent to the English term ‘the sun’. Since the concept of a definite article is not incorporated in the morphology of ‘tai yang’, the added ‘the’ is not aligned to any Chinese word. Yet in another context like ’the man’, ‘the’ can be the translation 2 Analysis of Unalignable Words Our manually-aligned data, which we call ORACLE data, is a Chinese-to-English corpus released by the LDC (Li et al., 2010)1 . It consists of ∼13000 Chinese sentences from news and blog domains and their English translation . English words are manually aligned with the Chinese characters. Characters without an exact counterpart are annotated with categories that state the functions of the words. These characters are either aligned to ‘NULL’, or attached to their dependency heads, if any, and aligned together to form a multi-word alignment. For example, ‘the’ is annotated as [DET], for ‘determiner’, and aligned to ‘tai-yang’ together with ‘sun’. In this work, any English word or Chinese character without an exact c"
E14-4037,C12-1120,0,0.0184062,"to the results of PBMT, this suggests our method may be most effective in improving systems where rule extraction is sen5 Conclusion We analyzed in-depth the phenomenon of unalignable words in parallel text, and show that what is unalignable depends on the word’s concept and context. We argue that this is not a trivial problem, but with an unalignable word classifier and a simple modified MT training pipeline, we can achieve small but significant gains in end-to-end translation. In related work, the issue of dropped pronouns (Chung and Gildea, 2010) and function words (Setiawan et al., 2010; Nakazawa and Kurohashi, 2012) have been found important in word alignment, and (Fossum et al., 2008) showed that syntax features are helpful for fixing alignments. An interesting avenue of future work is to integrate these ideas with ours, in particular by exploiting syntax and viewing unalignable words as aligned at a structure above the lexical level. 5 We use the standard MT08 test sets; the training data includes LDC2004T08, 2005E47, 2005T06, 2007T23, 2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15, and 2010T03 (34M English words and 1.1M sentences). Since we do not have access to all OpenMT data, e.g. FBIS, our"
E14-4037,D10-1052,0,0.0158377,"and METEOR. 6 Comparing to the results of PBMT, this suggests our method may be most effective in improving systems where rule extraction is sen5 Conclusion We analyzed in-depth the phenomenon of unalignable words in parallel text, and show that what is unalignable depends on the word’s concept and context. We argue that this is not a trivial problem, but with an unalignable word classifier and a simple modified MT training pipeline, we can achieve small but significant gains in end-to-end translation. In related work, the issue of dropped pronouns (Chung and Gildea, 2010) and function words (Setiawan et al., 2010; Nakazawa and Kurohashi, 2012) have been found important in word alignment, and (Fossum et al., 2008) showed that syntax features are helpful for fixing alignments. An interesting avenue of future work is to integrate these ideas with ours, in particular by exploiting syntax and viewing unalignable words as aligned at a structure above the lexical level. 5 We use the standard MT08 test sets; the training data includes LDC2004T08, 2005E47, 2005T06, 2007T23, 2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15, and 2010T03 (34M English words and 1.1M sentences). Since we do not have access to a"
E14-4037,N03-1033,0,0.0234339,"Missing"
E17-2011,W08-0336,0,0.0671522,"Missing"
E17-2011,P16-1004,0,0.0643804,"ve the cross-lingual IE task with a joint approach. Further, we focus on Open IE, which allows for an open set of semantic relations between a predicate and its arguments. Open IE in the monolingual setting has shown to be useful in a wide range of tasks, such as question answering (Fader et al., 2014), ontology learning (Suchanek, 2014), and summarization (ChrisInspired by the recent success of neural models in machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), and semantic parsing (Dong and Lapata, 2016), we propose a sequence-to-sequence model that enables end-toend cross-lingual Open IE. Essentially, we recast the problem as structured translation: the model encodes natural-language sentences and decodes predicate-argument forms (Figure 1). We show that the joint approach outperforms the pipeline on various metrics, and that the neural model is critical for the joint approach because of its capability in generating complex open IE patterns. 64 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 64–70, c V"
E17-2011,D11-1142,0,0.761024,"preferred by the user (e.g. English tuples). Conventional pipeline solutions decompose the task as machine translation followed by information extraction (or vice versa). We propose a joint solution with a neural sequence model, and show that it outperforms the pipeline in a cross-lingual open information extraction setting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a"
E17-2011,P15-1034,0,0.286145,"r (e.g. English tuples). Conventional pipeline solutions decompose the task as machine translation followed by information extraction (or vice versa). We propose a joint solution with a neural sequence model, and show that it outperforms the pipeline in a cross-lingual open information extraction setting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese"
E17-2011,N15-1151,0,0.0368238,"f input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negatively impact the IE engine, which may have been trained to expect natural English input (Sudo et al., 2004). Another approach is to first run a Chinese-based IE engin"
E17-2011,P02-1040,0,0.0980418,"03 and LDC2006G05 66 Moses (Koehn et al., 2007), directly on the same data we used to train Joint-Seq2Seq, i.e. pairs of Chinese sentences and English linearized PredPatt. We call this system Joint-Moses. We also train a Pipeline system which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt for predicate-argument identification. Results: We regard the generation of linearized PredPatt or linearized predicates4 as a translation problem, and use BLEU score (Papineni et al., 2002) for evaluation. As shown in Table 1, Joint Seq2Seq achieves the best BLEU scores, with an improvement 1.7 BLEU for linearized PredPatt and improvement of 4.3 BLEU for linearized predicates compared to Pipeline. PredPatt Predicates Pipeline 17.19 17.24 Joint Moses Joint Seq2Seq 18.34 18.94 16.43 21.55 k=150 k=1252 k=9535 Pipeline 32.95 28.73 27.20 Joint Moses Joint Seq2Seq 32.56 33.67 27.94 29.21 25.43 28.03 Table 2: Evaluation results (weighted F1 ) of predicates at different cluster granularities. outputs are empty. In contrast, Joint Seq2Seq generates no empty output and very few malformed"
E17-2011,P15-1001,0,0.0471709,"ized PredPatt. P (yi |y<i , A) = g(yi , sL i , ci ) minimize − Experiments Hyperparameters: Our proposed model (JointSeq2Seq) is trained using the Adam optimiser (Kingma and Ba, 2014), with mini-batch size 64 and step size 200. Both encoder and decoder have 2 layers and hidden state size 512, but different LSTM parameters sampled from U(0.05,0.05). Vocabulary size is 40K for both sides. Dropout (rate=0.5) is applied to non-recurrent connections (Srivastava et al., 2014). Gradients are clipped when their norm is bigger than 5 (Pascanu et al., 2013). We use sampled softmax to speed up training (Jean et al., 2015). Comparisons: As an alternative, we train a phrase-based machine translation system, log P (yi |y<i , A) (3) where D is the batch of training pairs, and P (yi | y<i , A) is computed by Eq.(3). Inference: We use greedy search to decode tokens one by one: yˆi = arg maxyi ∈VB P (yi |yˆ<i , A) 2 The code is available at https://github.com/ sheng-z/cross-lingual-open-ie. 3 The data comes from the GALE project; the largest bitexts are LDC2007E103 and LDC2006G05 66 Moses (Koehn et al., 2007), directly on the same data we used to train Joint-Seq2Seq, i.e. pairs of Chinese sentences and English linear"
E17-2011,P09-1048,0,0.201833,"Missing"
E17-2011,W09-1704,0,0.065801,"action setting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negatively impact the IE engine, whi"
E17-2011,D13-1176,0,0.0267203,"sources in the source language. Such problems with pipeline systems compound when the IE engine relies on parsers or other analytics as features. We propose to solve the cross-lingual IE task with a joint approach. Further, we focus on Open IE, which allows for an open set of semantic relations between a predicate and its arguments. Open IE in the monolingual setting has shown to be useful in a wide range of tasks, such as question answering (Fader et al., 2014), ontology learning (Suchanek, 2014), and summarization (ChrisInspired by the recent success of neural models in machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), and semantic parsing (Dong and Lapata, 2016), we propose a sequence-to-sequence model that enables end-toend cross-lingual Open IE. Essentially, we recast the problem as structured translation: the model encodes natural-language sentences and decodes predicate-argument forms (Figure 1). We show that the joint approach outperforms the pipeline on various metrics, and that the neural model is critical for the joint approach because of its capability in generating complex open IE pattern"
E17-2011,N15-1058,1,0.819327,"Missing"
E17-2011,P07-2045,0,0.00590556,"clipped when their norm is bigger than 5 (Pascanu et al., 2013). We use sampled softmax to speed up training (Jean et al., 2015). Comparisons: As an alternative, we train a phrase-based machine translation system, log P (yi |y<i , A) (3) where D is the batch of training pairs, and P (yi | y<i , A) is computed by Eq.(3). Inference: We use greedy search to decode tokens one by one: yˆi = arg maxyi ∈VB P (yi |yˆ<i , A) 2 The code is available at https://github.com/ sheng-z/cross-lingual-open-ie. 3 The data comes from the GALE project; the largest bitexts are LDC2007E103 and LDC2006G05 66 Moses (Koehn et al., 2007), directly on the same data we used to train Joint-Seq2Seq, i.e. pairs of Chinese sentences and English linearized PredPatt. We call this system Joint-Moses. We also train a Pipeline system which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt for predicate-argument identification. Results: We regard the generation of linearized PredPatt or linearized predicates4 as a translation problem, and use BLEU score (Papineni et al., 2002) for evaluation. As shown"
E17-2011,P16-1159,0,0.0444001,"Missing"
E17-2011,P13-1117,0,0.0582793,") of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negatively impact the IE engine, which may have been trained to expect natural English input (Sudo et al., 2004). Another approach is to first run a Chinese-based IE engine and then translate the resu"
E17-2011,W15-0807,0,0.135885,"q: collection of the specific funds) [(the special funds) related to (people ’s lives)]] Table 4: Example output. Arguments are shown in blue, and predicates shown in purple. Head tokens are underlined in bold. Token labels are omitted. 4 In linearized predicates, arguments are replaced by placeholders. For example, the linearized PredPatt in Fig 2 becomes “[ ?arg wants:ph Sth:= [ ?arg build:ph ?arg ] ]” after replacement. 5 Weighted F1 is the weighted average of individual F1 for each predicate, with weights proportional to predicate frequencies in the test data. We use token-level F1 score (Liu et al., 2015) which gives partial credits to partial matches. 6 Downloaded from: https://github.com/se4u/ mvlsa. 5 Conclusions We focus on the problem of cross-lingual open IE, and propose a joint solution based on a neu67 ral sequence-to-sequence model. Our joint approach outperforms the pipeline solution by 1-4 BLEU and 0.5-0.8 F1 . Future work includes minimum risk training (Shen et al., 2016) for directly optimizing the cross-lingual open IE metrics of interest. Furthermore, as PredPatt works on any language that has UD parsers available, we plan to evaluate cross-lingual Open IE on other target langua"
E17-2011,W11-1215,0,0.127292,"ting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negatively impact the IE engine, which may have been trai"
E17-2011,D15-1166,0,0.202929,"Missing"
E17-2011,C04-1127,0,0.754175,"in a cross-lingual open information extraction setting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negat"
E17-2011,C14-1121,0,0.0987837,"Missing"
E17-2011,D16-1177,1,0.789187,"Missing"
E17-2011,N16-1035,0,0.0149336,"es an L-layer bidirectional RNN (Schuster and Paliwal, 1997) which consists of a forward RNN reading inputs from x1 to x|A |and a backward RNN reading inputs in → − reverse from x|A |to x1 . Let hli ∈ Rn denote Our goal is to learn a model which directly maps a sentence input A in the source language into predicate-argument structures output B in the target language. Formally, we regard the input as a sequence A = x1 , · · · , x|A |, and use a linearized representation of the predicate-argument structure as the output sequence B = y1 , · · · , y|B |. While tree-based decoders are conceivable (Zhang et al., 2016), linearization of structured outputs to sequences simplifies decoding and has been shown 1 |B| Y https://github.com/hltcoe/PredPatt 65 4 the forward hidden state at time step i and layer l; it is computed by states at the previous time→ − → − −−→ −−→ step and at a lower layer: hli = f (hli−1 , hl−1 i ) → − where f is a nonlinear LSTM unit (Hochreiter − → and Schmidhuber, 1997). The lowest layer h0i is the word embedding of the token xi . The back← − ward hidden state hli is computed similarly using another LSTM, and the representation of each token xi is the concatenation of the top-layers: |"
E17-2011,D16-1257,0,\N,Missing
E17-2011,P16-1231,0,\N,Missing
I11-1004,J08-1002,0,0.059746,"show that our proposal can significantly improve BLEU scores of 2.47∼3.15 points compared with using the original English sentences. We finally conclude this paper by summarizing our proposal and the experiment results. Specially, for English-to-Japanese translation, Isozaki et al. (2010b) proposed to move syntactic or semantic heads to the end of corresponding phrases or clauses so that to yield head finalized English (HFE) sentences which follow the word order of Japanese. The head information of an English sentence is detected by a head-driven phrase structure grammar (HPSG) parser, Enju1 (Miyao and Tsujii, 2008). In addition, transformation rules were manually written for appending particle seed words, refining POS tags to be used before parsing, and deleting English determiners. Due to the usage of the same parser, we take this HFE approach as one of our baseline systems. The goal in this paper, however, is to learn preordering rules from parallel data in an automatic way. Under this motivation, pre-ordering rules can be extracted in a language-independent manner. A number of researches follow this automatic way. For example, in (Xia and McCord, 2004), a variety of heuristic rules were applied to bi"
I11-1004,J03-1002,0,0.00568597,"ese sentences into SVO-style English sentences. For comparison, our proposal 1) makes use of not only PASs but also the source syntactic tree structures for preordering rule matching, 2) extracts pre-ordering 1 Figure 1 shows a word-aligned HPSG-tree-tostring pair for English-to-Japanese translation. PASs among lexical nodes and their argument nodes in this HPSG tree are described by arrows in thick-lines. For simplicity, we only draw the identifiers for the signs of the nodes in the HPSG tree. Note that the identifiers that start with ‘c’ 2 These word alignments are gained by running GIZA++ (Och and Ney, 2003) on the original parallel sentences. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 30 c0 &lt;tok id=t0 cat=SC pos=WRB base=when lexentry=[when] pred=conj_arg12 arg1=c16 arg2=c3&gt; c0 c1 c1 c3 &lt;cons id=c16 cat=S xcat= head=c18 sem_head=c18 schema=mod_head&gt; t0 &lt;cons id=c3 cat=S xcat= head=c13 sem_head=c13 schema=subj_head&gt; c18 c6 c21 c8 c13 c10 c7 c5 c3 c2 c16 c4 c2 c16 c9 c11 t0 t1 t2 t3 when the fluid pressure c12 t4 cylinder t6 31 is c23 c15 c17 c20 c14 t5 c19 t7 t8 t9 used , c22 c24 c25 t10 t11 t12 is gradually applied fluid . 流体 圧 シリンダ 31 の 場合 は 流体 が 徐々に 排出 さ れる こと と なる 。 0 1 fluid pressu"
I11-1004,P05-1033,0,0.0598702,"otone or reordering) with probabilities for each bilingual phrase from the training data. For example, by taking lexical information as features, a maximum entropy phrase reordering model was proposed by Xiong et al. (2006). Second, syntax-based models attempt to solve the word ordering problem by employing syntactic structures. For example, linguistically syntaxbased approaches (Galley et al., 2004; Liu et al., 2006) first parse source and/or target sentences and then learn reordering templates from the subtree fragments of the parse trees. In contrast, hierarchical phrase based translation (Chiang, 2005) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu"
I11-1004,P03-1021,0,0.0213618,"topological order do if n is a terminal node then n.srcPhrase ← E[n.srcSpan[0]] else if n.srcPhrase = NULL then n.srcPhrase ← C ONNECT(n.children().srcPhrase) end if end for 3 Experiments 3.1 Setup We test our proposal by translating from English to Japanese. We use the NTCIR-9 English-Japanese patent corpus4 as our experiment set. Since the reference set of the official test set has not been released yet, we instead split the original development set averagely into two parts, named dev.a and dev.b. In our experiments, we first take dev.a as our development set for minimum-error rate tuning (Och, 2003) and then report the final translation accuracies on dev.b. For direct comparison with other systems in the future, we use the configuration of the official baseline system5 : • Moses6 (Koehn et al., 2007): revision = “3717” as the baseline decoder. Note that we also train Moses using HFE sentences (Isozaki et al., 2010b) and the English sentences pre-ordered by PASs; Algorithm 2 sketches the algorithm for applying pre-ordering rules to a given HPSG tree TE . The algorithm contains three parts: rule matching (Lines 4-12), bottom-up rule applying (Lines 13-19), and sentence collecting (Lines 20"
I11-1004,P05-1066,0,0.180454,"jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), and English-to-SOV languages (Xu et al., 2009). Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to"
I11-1004,P02-1040,0,0.0821751,"Missing"
I11-1004,N04-1035,0,0.311955,"mmunication Science Laboratories, NTT Corporation 2-4 Hikaridai Seika-cho, Soraku-gun Kyoto 619-0237 Japan {wu.xianchao,sudoh.katsuhito,kevin.duh,tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp Abstract orientations (monotone or reordering) with probabilities for each bilingual phrase from the training data. For example, by taking lexical information as features, a maximum entropy phrase reordering model was proposed by Xiong et al. (2006). Second, syntax-based models attempt to solve the word ordering problem by employing syntactic structures. For example, linguistically syntaxbased approaches (Galley et al., 2004; Liu et al., 2006) first parse source and/or target sentences and then learn reordering templates from the subtree fragments of the parse trees. In contrast, hierarchical phrase based translation (Chiang, 2005) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et"
I11-1004,C10-1043,0,0.299656,"syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), and English-to-S"
I11-1004,2007.tmi-papers.21,0,0.143543,"Missing"
I11-1004,D10-1092,1,0.904154,"Missing"
I11-1004,N04-4026,0,0.272773,"Missing"
I11-1004,W10-1736,1,0.754264,", in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content which corresponds to PASs. In order to record the relative positions among a predicate word and its argument phrases, we propose a linear-time algorithm to extract preordering rules from word-aligned HPSG-tree-tostring"
I11-1004,C96-2141,0,0.339493,"al., 2004; Liu et al., 2006) first parse source and/or target sentences and then learn reordering templates from the subtree fragments of the parse trees. In contrast, hierarchical phrase based translation (Chiang, 2005) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic b"
I11-1004,D07-1077,0,0.403469,"e far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), and English-to-SOV languages (Xu et al., 2009). Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to extract the pre-ordering rules from word"
I11-1004,P10-1034,1,0.926742,"dering ways. First, in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content which corresponds to PASs. In order to record the relative positions among a predicate word and its argument phrases, we propose a linear-time algorithm to extract preordering rules from word-ali"
I11-1004,P07-2045,0,0.0692904,"eline SMT systems. 1 Introduction Statistical machine translation (SMT) suffers from an essential problem for translating between languages with substantial structural differences, such as between English which is a subject-verbobject (SVO) language and Japanese which is a typical subject-object-verb (SOV) language. Numerous approaches have been consequently proposed to tackle this word-order problem, such as lexicalized reordering methods, syntax-based models, and pre-ordering ways. First, in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) i"
I11-1004,2006.iwslt-evaluation.11,1,0.751145,"is that, predicate-argument structures (PASs) are introduced to extract fine-grained pre-ordering rules. PASs have the following merits for describing reordering phenomena: • predicate words and argument phrases respectively record reordering phenomena in a lexicalized level and an abstract level; • PASs provide a fine-grained classification of the reordering phenomena since they include factored representations of syntactic features of the predicate words and their argument phrases. 2 Pre-ordering Rule Extraction and Application 2.1 An example The idea of using PASs for pre-ordering follows (Komachi et al., 2006). Several reordering operations were manually designed by Komachi et al. (2006) to pre-ordering Japanese sentences into SVO-style English sentences. For comparison, our proposal 1) makes use of not only PASs but also the source syntactic tree structures for preordering rule matching, 2) extracts pre-ordering 1 Figure 1 shows a word-aligned HPSG-tree-tostring pair for English-to-Japanese translation. PASs among lexical nodes and their argument nodes in this HPSG tree are described by arrows in thick-lines. For simplicity, we only draw the identifiers for the signs of the nodes in the HPSG tree."
I11-1004,C04-1073,0,0.419443,"phrase structure grammar (HPSG) parser, Enju1 (Miyao and Tsujii, 2008). In addition, transformation rules were manually written for appending particle seed words, refining POS tags to be used before parsing, and deleting English determiners. Due to the usage of the same parser, we take this HFE approach as one of our baseline systems. The goal in this paper, however, is to learn preordering rules from parallel data in an automatic way. Under this motivation, pre-ordering rules can be extracted in a language-independent manner. A number of researches follow this automatic way. For example, in (Xia and McCord, 2004), a variety of heuristic rules were applied to bilingual parse trees to extract pre-ordering rules for French-English translation. Rottmann and Vogen (2007) learned reordering rules based on sequences of part-of-speech (POS) tags, instead of parse trees. Dependency trees were used by Genzel (2010) to extract source-side reordering rules for translating languages from SVO to SOV, etc.. The novel idea expressed in this paper is that, predicate-argument structures (PASs) are introduced to extract fine-grained pre-ordering rules. PASs have the following merits for describing reordering phenomena:"
I11-1004,P06-1066,0,0.17735,"Missing"
I11-1004,H05-1021,0,0.0214174,"between languages with substantial structural differences, such as between English which is a subject-verbobject (SVO) language and Japanese which is a typical subject-object-verb (SOV) language. Numerous approaches have been consequently proposed to tackle this word-order problem, such as lexicalized reordering methods, syntax-based models, and pre-ordering ways. First, in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure"
I11-1004,N09-1028,0,0.247032,"05) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), a"
I11-1004,J93-2003,0,\N,Missing
I11-1073,W08-0304,0,0.0816198,"faster parameter tuning algorithm would have a positive impact on research on all the components of the SMT system. Imagine a researcher designing a new pruning algorithm for decoding, a new word alignment model, or a new domain adaptation method. Any of these methods need to be evaluated in the context of a full SMT system, which requires parameter tuning. If we can reduce the parameter tuning time from 10 hours to 1 hour, this can greatly increase the pace of innovation. Thus our motivation is orthogonal to recent research on improving MERT, such as efforts to escape local maxima problems (Cer et al., 2008; 649 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 649–657, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP proposed by (Och, 2003). There are several variations for updating weights during the iterative tuning process in MERT. The most commonly used algorithm for MERT is usually called Koehncoordinate descent (KCD), which is used in the MERT utility packaged in the popular Moses statistical machine translation system (Koehn et al., 2007). Another choice is Powell’s method that was advocated when MERT was first introduced for SMT (Och, 2"
I11-1073,W09-0439,0,0.549104,"sentences in the tuning dataset with the current weights to generate N -best lists of translations. Then, the method employs the inner loop procedure to optimize the weights based on those N -best lists instead of decoding the tuning dataset. After obtaining the optimal weights from the inner loop, we repeat the outer loop to generate a new N -best list. Figure 1 shows the system outline, which is described in detail elsewhere (Bertoldi et al., 2009). We note here that the method proposed in this paper essentially involves the replacement of the inner loop algorithm of the conventional MERT. Foster and Kuhn, 2009; Moore and Quirk, 2008), or incorporate lattices (Macherey et al., 2008). 2 Parameter Tuning for SMT Systems Most recently developed SMT systems consist of several model components, such as translation models, language models, and reordering models. To combine evidence obtained from these different components, we often define a discriminative (log-)linear model. Suppose the SMT system has D components. The log probabilities of the components are usually treated as features in the discriminative model. We denote the d-th feature, or log probability of the d-th component given a source sentence"
I11-1073,P07-2045,0,0.0612959,"on is orthogonal to recent research on improving MERT, such as efforts to escape local maxima problems (Cer et al., 2008; 649 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 649–657, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP proposed by (Och, 2003). There are several variations for updating weights during the iterative tuning process in MERT. The most commonly used algorithm for MERT is usually called Koehncoordinate descent (KCD), which is used in the MERT utility packaged in the popular Moses statistical machine translation system (Koehn et al., 2007). Another choice is Powell’s method that was advocated when MERT was first introduced for SMT (Och, 2003). Since KCD tends to be marginally more effective at optimizing the MERT objective, and is much simpler to implement than Powell’s method, this paper focuses only on KCD. KCD is a variant of a coordinate ascent (or descent) algorithm. At each iteration, it moves along the coordinate, which allows for the greatest progress of the maximization. The routine performs a trial line maximization along each coordinate to determine which one should be selected. It then updates the weight vector with"
I11-1073,D08-1076,0,0.0441913,"-best lists of translations. Then, the method employs the inner loop procedure to optimize the weights based on those N -best lists instead of decoding the tuning dataset. After obtaining the optimal weights from the inner loop, we repeat the outer loop to generate a new N -best list. Figure 1 shows the system outline, which is described in detail elsewhere (Bertoldi et al., 2009). We note here that the method proposed in this paper essentially involves the replacement of the inner loop algorithm of the conventional MERT. Foster and Kuhn, 2009; Moore and Quirk, 2008), or incorporate lattices (Macherey et al., 2008). 2 Parameter Tuning for SMT Systems Most recently developed SMT systems consist of several model components, such as translation models, language models, and reordering models. To combine evidence obtained from these different components, we often define a discriminative (log-)linear model. Suppose the SMT system has D components. The log probabilities of the components are usually treated as features in the discriminative model. We denote the d-th feature, or log probability of the d-th component given a source sentence f and its translation e, as φd (e, f ). We also denote the d-th weight a"
I11-1073,C08-1074,0,0.0654341,"g dataset with the current weights to generate N -best lists of translations. Then, the method employs the inner loop procedure to optimize the weights based on those N -best lists instead of decoding the tuning dataset. After obtaining the optimal weights from the inner loop, we repeat the outer loop to generate a new N -best list. Figure 1 shows the system outline, which is described in detail elsewhere (Bertoldi et al., 2009). We note here that the method proposed in this paper essentially involves the replacement of the inner loop algorithm of the conventional MERT. Foster and Kuhn, 2009; Moore and Quirk, 2008), or incorporate lattices (Macherey et al., 2008). 2 Parameter Tuning for SMT Systems Most recently developed SMT systems consist of several model components, such as translation models, language models, and reordering models. To combine evidence obtained from these different components, we often define a discriminative (log-)linear model. Suppose the SMT system has D components. The log probabilities of the components are usually treated as features in the discriminative model. We denote the d-th feature, or log probability of the d-th component given a source sentence f and its translation e"
I11-1073,P03-1021,0,0.718943,"rd alignment model, or a new domain adaptation method. Any of these methods need to be evaluated in the context of a full SMT system, which requires parameter tuning. If we can reduce the parameter tuning time from 10 hours to 1 hour, this can greatly increase the pace of innovation. Thus our motivation is orthogonal to recent research on improving MERT, such as efforts to escape local maxima problems (Cer et al., 2008; 649 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 649–657, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP proposed by (Och, 2003). There are several variations for updating weights during the iterative tuning process in MERT. The most commonly used algorithm for MERT is usually called Koehncoordinate descent (KCD), which is used in the MERT utility packaged in the popular Moses statistical machine translation system (Koehn et al., 2007). Another choice is Powell’s method that was advocated when MERT was first introduced for SMT (Och, 2003). Since KCD tends to be marginally more effective at optimizing the MERT objective, and is much simpler to implement than Powell’s method, this paper focuses only on KCD. KCD is a vari"
I11-1073,P02-1040,0,0.0919199,"Missing"
I11-1153,N09-2019,0,0.0494692,"Missing"
I11-1153,P09-1064,0,0.033943,"4 is the N-best approximation commonly used in practice: N (f ) contains the set of hypotheses in the N-best list, and the argmin and sum is only performed within this finite set. There are two difficulties with Eq. 4: 1. The N-best approximation is much smaller than the true space of all English hypotheses in the argmin and sum of Eq. 3. The approximation in the argmin causes search errors, while the approximation in the sum introduces bias. This problem can be somewhat mitigated by increasing the N-best list size or extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming that the decoder’s sco"
I11-1153,C10-1036,0,0.0320563,"Missing"
I11-1153,P07-2026,0,0.0184181,"extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming that the decoder’s score is an accurate surrogate for the true posterior distribution p(e|f ). The second difficulty poses a particular problem for system combination. Although the assumption in Eq. 5 is reasonable for single-system MT, it becomes P unclear how to compare the model scores i λi hi (e, f ) in a multi-system setting. To illustrate, consider two MT systems, their 2-best lists, and corresponding model scores: • System A: e1 , score=7; e2 , score=3; • System B: e3 , score=90; e4 , score=10; It is unclear what is the ranking of post"
I11-1153,E06-1005,0,0.124381,"Missing"
I11-1153,P08-1023,0,0.0352485,"satisfy the relations (Eq. 10) in its constraints while allowing for some slack ξ, whose amount depends on hyperparameter c. 1358 4 Experiments We experiment with the NTCIR-9 (2011) Englishto-Japanese Patent Translation task1 . This includes 3 million sentences for training individual MT systems; the official dev set is split into 1000 sentences for MERT of individual systems, 500 for system combination optimization (MBR, GMBR), and 500 for final evaluation. We combine three systems: • Phrase-based Moses with lexical reordering, distortion=6 (Koehn and others, 2007) • Forest-to-string system (Mi et al., 2008) • Weighted finite-state Transducer (WFST) (Zhou et al., 2006) with rule-based reordering as preprocessing (Isozaki et al., 2010b). Each system generates a 100-best list, so our system combination task involves hypothesis selection out of 300 hypotheses. As evaluation measure, we focus on BLEU, Normalized Kendall’s Tau (NKT), a metric that has been shown to correlate well with humans on this language pair (Isozaki et al., 2010a)2 , and a combination thereof. The loss function used for MBR is therefore the sum of BLEU and NKT. For GMBR, the sub-components of this loss function are derived from"
I11-1153,N07-1029,0,0.0560152,"Missing"
I11-1153,D08-1065,0,0.0343379,"BR) decision rule. Eq. 4 is the N-best approximation commonly used in practice: N (f ) contains the set of hypotheses in the N-best list, and the argmin and sum is only performed within this finite set. There are two difficulties with Eq. 4: 1. The N-best approximation is much smaller than the true space of all English hypotheses in the argmin and sum of Eq. 3. The approximation in the argmin causes search errors, while the approximation in the sum introduces bias. This problem can be somewhat mitigated by increasing the N-best list size or extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming t"
I11-1153,D08-1011,0,0.0385247,"k e Lk (e k=1 θk Ck (e ), where P|e) =′ Ck (e′ ) = L (e |e) represents the come k ′ bined loss for e . So we first compute Ck (·) for all hypotheses, for an O(|N (f )|2 ) run-time. To find the GMBR P decision then requires a ′ search arg mine′ ∈N (f ) K k=1 θk Ck (e ). So in test, GMBR is on the same order as conventional MBR. To tune θ, we first extract all pairs of hypotheses where a difference exists in the true loss, then optimize θ in a formulation similar to RankSVM (Joachims, 2006). The pair-wise nature of Eq. 10 makes the problem amenable to solutions in “learning to rank” literature (He et al., 2008a). The pseudocode is shown in Algorithm 1. The RankSVM (line 8) tries to satisfy the relations (Eq. 10) in its constraints while allowing for some slack ξ, whose amount depends on hyperparameter c. 1358 4 Experiments We experiment with the NTCIR-9 (2011) Englishto-Japanese Patent Translation task1 . This includes 3 million sentences for training individual MT systems; the official dev set is split into 1000 sentences for MERT of individual systems, 500 for system combination optimization (MBR, GMBR), and 500 for final evaluation. We combine three systems: • Phrase-based Moses with lexical reo"
I11-1153,D10-1092,1,0.905756,"Missing"
I11-1153,W10-1736,1,0.870667,"Missing"
I11-1153,P07-2045,0,0.00573936,"Missing"
I11-1153,N04-1022,0,0.0385613,"e loss function in MBR and allows it to be optimized in the given hypothesis space of multiple systems. This extension better approximates the true Bayes Risk decision rule and empirically improves over MBR, even in cases where the combined systems are of mixed quality. 1 Introduction Minimum Bayes Risk (MBR) is a theoreticallyelegant decision rule that has been used for singlesystem decoding and system combination in machine translation (MT). MBR arose in Bayes decision theory (Duda et al., 2000) and has since been applied to speech recognition (Goel and Byrne, 2000) and machine translation (Kumar and Byrne, 2004). The idea is to choose hypotheses that minimize Bayes Risk as oppose to those that maximize posterior probability. This enables the use of taskspecific loss functions (e.g BLEU). However, the definition of Bayes Risk depends critically on the posterior probability of hypotheses. In single-system decoding, one could approximate this probability using model scores. However, for system combination, the various systems 2 The Difficulty with MBR Consider the task of translation from a French sentence (f ) to an English sentence (e). Our goal is to find a decision rule δ(f ) → e′ , which takes f as"
I11-1153,P09-1019,0,0.0167692,"oximation commonly used in practice: N (f ) contains the set of hypotheses in the N-best list, and the argmin and sum is only performed within this finite set. There are two difficulties with Eq. 4: 1. The N-best approximation is much smaller than the true space of all English hypotheses in the argmin and sum of Eq. 3. The approximation in the argmin causes search errors, while the approximation in the sum introduces bias. This problem can be somewhat mitigated by increasing the N-best list size or extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming that the decoder’s score is an accurate sur"
I13-1094,W95-0107,0,0.16069,"Missing"
I13-1094,W09-1206,0,0.0338762,"Missing"
I13-1094,D11-1001,0,0.0130879,"oundaries and to extract relevant information for training classifiers. However, there have been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. Sun et. al (2009) and Hacioglu et. al (2004) addressed the SRL problem on the basis of shallow syntactic information at the level of phrase chunks. In their approach, SRL is formulated as a sequence labelIn recent years, SRL has become an important component in many kinds of deep natural language processing applications, such as question answering (Narayanan and Harabagiu, 2004), event extraction (Riedel and McCallum, 2011), document categorization (Persson et al., 2009). SRL aims at identifying the semantic relations between predicates in a sentence and their associated arguments, with these relations drawn from a pre-specific list of possible semantic roles for 781 International Joint Conference on Natural Language Processing, pages 781–787, Nagoya, Japan, 14-18 October 2013. ing problem, performing IOB2 decisions on the syntactic chunks of a sentence. However, this method ignores the full syntactic parsing information entirely, and we believe that even the accuracy of full syntactic parsing is not ideal, it i"
I13-1094,P06-2013,0,0.0284832,"ts would be improved. Under this hypothesis, we simply extracted the following features from every parse tree in the Nbest list which are generated using second-order MST parser. These features are also included in the ”standard” feature set when N = 1. 4 Results and Discussion 4.1 Experimental Setting We used the Chinese dataset provided by CoNLL2009 shared task for experiments. For comparison, two kinds of dependency parsing results are provided, the first is from MALT parser, the second is from second-order MST parser. As for chunking information, we used the chunk definition presented in (Chen et al., 2006) to extract chunks from Chinese Tree Bank as training corpus. The line CH in Figure 1 shows the definition of chunks. In this example, ” 金融工作”(finance work) is a noun phrase and is composed by two nouns. 1 783 http://crfpp.sourceforge.net/ WORD 去年 西藏 金融 工作 取得 显著 成绩 POS NN NR NN NN VV JJ NN CH [NP] [NP] [VP] [ADJP] [NP] TAG B-NP B-NP B-NP I-NP B-VP B-ADJP B-NP SRL TMP NONE NONE A0 取得.01 NONE A1 [ NP ] Figure 1: Chunking information for a predicate-argument structure. Feature Name Chunk features Path features Description chunk tag of headword with IB representation (e.g. B − N P ) chunk tag of t"
I13-1094,W04-2416,0,0.0849248,"Missing"
I13-1094,N04-1032,0,0.0384608,"ious dependencybased SRL research (Johansson and Nugues, 2008; Luo et al., 2012). We do not explain ”standard” features, however, we give a detailed description of the features used in this work. 3.3 Error Analysis for Dependency-based SRL 4.2.1 Base Phrase Chunking Related Features In Figure 1, obviously, words in chunks do not have equal importance for SRL. Headwords represent the main meaning of the chunks. The base phrase chunking related features shown in Table 2 are only applied to these headwords. For other words in chunks, only lemma and POS information is used. The rules described in Sun and Jurafsky (2004) are used to extract headwords. Verb class in Table 2 is represented similarly as V erb.C1C2, which means this verb has two senses. For its first sense, it has one core argument and for its second sense, it has two core arguments. These verb classes are extracted from Chinese PropBank (Xue, 2008). Using the gold parse of dependency relations between a predicate and its arguments and according to these relations, we classified SRL errors into following three types. • C: children of a predicate should be arguments but they are tagged incorrectly. • G: grand children of a predicate should be argu"
I13-1094,D09-1153,0,0.0377288,"Missing"
I13-1094,D08-1008,0,0.18199,"hod ignores the full syntactic parsing information entirely, and we believe that even the accuracy of full syntactic parsing is not ideal, it is still helpful for SRL. Moreover, their method is inapplicable to dependency based SRL since a chunk usually consists of successive words. A substantial amount of research has focused on dependency-based SRL (Meza-Ruiz and Riedel, 2009; Luo et al., 2012) since the CoNLL-2009 shared task and rich linguistic features (Zhao et al., 2009) are applied. For dependency related features, most studies focused on extracting them from the best dependency result. Johansson and Nugues (2008) tried to use N-best dependency parsing results. In their work, they applied 16-best dependency trees to generate predicate-argument structures and applied both syntactic trees and predicate-argument structures to a linear model. This model reranks the predicate-argument structures and the top 16 dependency trees at the same time. Though their work suggests that N-best dependency parsing can enhance the SRL, little is known about how the N-best dependency parsing related features perform on SRL. 3.1 Predicate Sense Disambiguation and SRL with a Local Model Since the predicate cannot be an argu"
I13-1094,J08-2002,0,0.0842463,"Missing"
I13-1094,P05-1012,0,0.148457,"Missing"
I13-1094,J08-2004,0,0.0611353,"Missing"
I13-1094,N09-1018,0,0.0167972,"ist of possible semantic roles for 781 International Joint Conference on Natural Language Processing, pages 781–787, Nagoya, Japan, 14-18 October 2013. ing problem, performing IOB2 decisions on the syntactic chunks of a sentence. However, this method ignores the full syntactic parsing information entirely, and we believe that even the accuracy of full syntactic parsing is not ideal, it is still helpful for SRL. Moreover, their method is inapplicable to dependency based SRL since a chunk usually consists of successive words. A substantial amount of research has focused on dependency-based SRL (Meza-Ruiz and Riedel, 2009; Luo et al., 2012) since the CoNLL-2009 shared task and rich linguistic features (Zhao et al., 2009) are applied. For dependency related features, most studies focused on extracting them from the best dependency result. Johansson and Nugues (2008) tried to use N-best dependency parsing results. In their work, they applied 16-best dependency trees to generate predicate-argument structures and applied both syntactic trees and predicate-argument structures to a linear model. This model reranks the predicate-argument structures and the top 16 dependency trees at the same time. Though their work s"
I13-1094,W09-1208,0,0.0254255,"81–787, Nagoya, Japan, 14-18 October 2013. ing problem, performing IOB2 decisions on the syntactic chunks of a sentence. However, this method ignores the full syntactic parsing information entirely, and we believe that even the accuracy of full syntactic parsing is not ideal, it is still helpful for SRL. Moreover, their method is inapplicable to dependency based SRL since a chunk usually consists of successive words. A substantial amount of research has focused on dependency-based SRL (Meza-Ruiz and Riedel, 2009; Luo et al., 2012) since the CoNLL-2009 shared task and rich linguistic features (Zhao et al., 2009) are applied. For dependency related features, most studies focused on extracting them from the best dependency result. Johansson and Nugues (2008) tried to use N-best dependency parsing results. In their work, they applied 16-best dependency trees to generate predicate-argument structures and applied both syntactic trees and predicate-argument structures to a linear model. This model reranks the predicate-argument structures and the top 16 dependency trees at the same time. Though their work suggests that N-best dependency parsing can enhance the SRL, little is known about how the N-best depe"
I13-1094,D07-1100,0,0.0665788,"Missing"
I13-1094,C04-1100,0,0.00959114,"onstituent parse of sentences to define argument boundaries and to extract relevant information for training classifiers. However, there have been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. Sun et. al (2009) and Hacioglu et. al (2004) addressed the SRL problem on the basis of shallow syntactic information at the level of phrase chunks. In their approach, SRL is formulated as a sequence labelIn recent years, SRL has become an important component in many kinds of deep natural language processing applications, such as question answering (Narayanan and Harabagiu, 2004), event extraction (Riedel and McCallum, 2011), document categorization (Persson et al., 2009). SRL aims at identifying the semantic relations between predicates in a sentence and their associated arguments, with these relations drawn from a pre-specific list of possible semantic roles for 781 International Joint Conference on Natural Language Processing, pages 781–787, Nagoya, Japan, 14-18 October 2013. ing problem, performing IOB2 decisions on the syntactic chunks of a sentence. However, this method ignores the full syntactic parsing information entirely, and we believe that even the accurac"
I13-1094,W08-2121,0,\N,Missing
I13-1094,P10-2018,1,\N,Missing
I17-1084,P16-1231,0,0.136093,"bels). To make a baseline comparison, we train an OpenNMT system (Klein et al., 2017) directly on the same data ignoring the labels. Comparisons Our approach (Selective Decoding) is compared against four other approaches: (1) Joint Seq2Seq, which trains a standard encoder-decoder model on the Chinese-English dataset described in Table 1; (2) Joint Moses, which trains a phrase-based machine translation system, Moses (Koehn et al., 2007), directly on the same data; (3) Pipeline-S which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt (White et al., 2016) for predicate-argument identification; and (4) Pipeline-N is the same as Pipeline-S except that the Moses system is replaced by OpenNMT (Klein et al., 2017), a neural machine translation system. 5.2.3 Linearized PredPatt Approach OpenNMT 24.92 Evaluation Metrics Selective Decoding 25.16 Table 3: Evaluation results (BLEU) of sequence generation on the test set. For evaluation, we directly convert the output by our approach (e.g. Fig. 1(e)) to the form of linearized PredPatt (e.g., Fig. 1(d)), and follow the same man"
I17-1084,P15-1034,0,0.0205754,"been used in neural machine translating (Tu et al., 2016) and image caption generation (Xu et al., 2015) to explicitly control the influence from source or target contexts. The experiments in § 5 also confirms our point: our model using the selective decoding mechanism significantly improves the performance, compared to the standard encoder-decoder model. Regarding to open IE systems for generating training data, PredPatt has shown promising performance on large-scale open IE benchmarks (Zhang et al., 2017c). Compared to other existing open IE systems (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), PredPatt uses manual language-agnostic patterns on UD, which makes it a well-founded component across languages. Additionally, the underlying structure constructed by PredPatt has been shown to be a well-formed syntax-semantics interface (Zhang et al., 2017b). The beam is used to increase the search space for the sequence Y in the target language. At each decoding time step, we first greedily select the type of decoder, and then generate candidate tokens from the selected decoder to update the beam. When the special token heosi is generated, we remove the candidate sequence from the beam. 4"
I17-1084,D14-1179,0,0.0249957,"Missing"
I17-1084,P17-4012,0,0.0167177,"embeddings), and without beam search, only using greedy search during inference (-beam search). As shown in Table 2, while the pretrained word embeddings moderately improve the BLEU score of linearized PredPatt, they have slightly negative impact on linearized predicates. Beam search helps improve the BLEU score of both. Selective Decoding explicitly models sequence generation and sequence labeling, which enables a standalone evaluation of the sequence generation process (i.e., the final output without predicate and argument labels). To make a baseline comparison, we train an OpenNMT system (Klein et al., 2017) directly on the same data ignoring the labels. Comparisons Our approach (Selective Decoding) is compared against four other approaches: (1) Joint Seq2Seq, which trains a standard encoder-decoder model on the Chinese-English dataset described in Table 1; (2) Joint Moses, which trains a phrase-based machine translation system, Moses (Koehn et al., 2007), directly on the same data; (3) Pipeline-S which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt (White"
I17-1084,P07-2045,0,0.00408444,"models sequence generation and sequence labeling, which enables a standalone evaluation of the sequence generation process (i.e., the final output without predicate and argument labels). To make a baseline comparison, we train an OpenNMT system (Klein et al., 2017) directly on the same data ignoring the labels. Comparisons Our approach (Selective Decoding) is compared against four other approaches: (1) Joint Seq2Seq, which trains a standard encoder-decoder model on the Chinese-English dataset described in Table 1; (2) Joint Moses, which trains a phrase-based machine translation system, Moses (Koehn et al., 2007), directly on the same data; (3) Pipeline-S which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt (White et al., 2016) for predicate-argument identification; and (4) Pipeline-N is the same as Pipeline-S except that the Moses system is replaced by OpenNMT (Klein et al., 2017), a neural machine translation system. 5.2.3 Linearized PredPatt Approach OpenNMT 24.92 Evaluation Metrics Selective Decoding 25.16 Table 3: Evaluation results (BLEU) of sequence gener"
I17-1084,P16-1004,0,0.022314,"the beam. When the special token heosi is generated, we remove the candidate sequence from the beam. 4 Related Work The model we propose in this paper is adapted from the RNN encoder-decoder architectures which have been successfully applied to a wide range of NLP tasks such as machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b), syntactic parsing (Vinyals et al., 2015a), question answering (Hermann et al., 2015), summarization (Rush et al., 2015), and semantic parsing (Dong and Lapata, 2016). As a novel variation of the encoder-decoder architecture, our model provides a general solution to tasks involving translation and labeling. crosslingual open IE is an example of this kind of task. The end-to-end solution proposed by Zhang et al. (2017a) used a vanilla attention-based encoderdecoder model to achieve results which outperform the traditional pipeline solutions. Compared to the vanilla encoder-decoder model, our model splits the joint task into two concurrent tasks (i.e., labeling and translating), which are jointly learnt by a selector and multiple decoders. This eases the bur"
I17-1084,W15-0807,0,0.0258909,"ty We compute the number of the linearized PredPatt outputs from which the tree structure representation can not be recovered, including the empty outputs and the outputs which have unmatched brackets, or have zero or multiple heads for an argument or a predicate. As shown in Table 5, compared to the previous best Joint Seq2Seq approach, Selective Decoding further reduces the number of unrecoverable outputs by one order of magnitude. Joint Moses 33,178 10 Figure 4: BLEU scores of the linearized PredPatt on the test set w.r.t. the lengths of the references. We compute the token-level F1 score (Liu et al., 2015) of predicates and arguments. As shown in Table 4, Selective Decoding substantially improves the F1 score of both predicates and arguments. In the ablation test, pretrained word embeddings slightly improve F1 of predicates, but have no improvement on F1 of arguments. Beam search helps improve the score of both. PipelineN 6,014 Selective Decoding Joint Seq2Seq 5 Table 4: Evaluation results (F1 ) of predicates and arguments on the test set. PipelineS 5,965 15 Input sentence and its English translation: 我 哪怕 有 千分之一 的 希望 呢 , 我 死 活 都 要 给 他 做 最后 的 (Even if there was only a one thousandth of a hope ,"
I17-1084,D11-1142,0,0.0508307,"ilar mechanisms have been used in neural machine translating (Tu et al., 2016) and image caption generation (Xu et al., 2015) to explicitly control the influence from source or target contexts. The experiments in § 5 also confirms our point: our model using the selective decoding mechanism significantly improves the performance, compared to the standard encoder-decoder model. Regarding to open IE systems for generating training data, PredPatt has shown promising performance on large-scale open IE benchmarks (Zhang et al., 2017c). Compared to other existing open IE systems (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), PredPatt uses manual language-agnostic patterns on UD, which makes it a well-founded component across languages. Additionally, the underlying structure constructed by PredPatt has been shown to be a well-formed syntax-semantics interface (Zhang et al., 2017b). The beam is used to increase the search space for the sequence Y in the target language. At each decoding time step, we first greedily select the type of decoder, and then generate candidate tokens from the selected decoder to update the beam. When the special token heosi is generated, we remove the candidate sequ"
I17-1084,P16-1006,0,0.0356003,"Missing"
I17-1084,D15-1166,0,0.0827738,"Missing"
I17-1084,P15-1001,0,0.0268347,"are sampled from U(−0.1, 0.1). The dropout rate is set to 0.3. The word embedding size is 300 for input tokens on both the encoder side and the decoder side. We use open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions6 to initialize the word embeddings on the decoder side. The mini-batch size is set to 64 and the step size set to 50. Gradients are clipped when their norms are greater than 5 (Pascanu et al., 2013). For simplicity, we use vanilla softmax over the decoder vocabulary as opposed to more efficient alternatives such as sampled softmax (Jean et al., 2015). The vocabulary size is set to 40,000. The number of epochs 6 https://nlp.stanford.edu/projects/ glove/ 836 5.2.4 Evaluation using BLEU Table 2 shows the cased BLEU scores of linearized PredPatt and linearized predicates7 on the test set. Selective Decoding significantly improves the performance on both of them. Compared to the previous best approach (Joint Seq2Seq), Selective Decoding improves the BLEU score of linearized PredPatt to 23.88, and the score of linearized predicates to 25.42. is 20. Early stopping is used to avoid overfitting. The beam size is 5. Before feeding into the encoder,"
I17-1084,D14-1162,0,0.0805412,"-parameters setting for experiments, evaluate our approach in two kinds of scenarios, and compare the results of our approach and the other comparing approaches. 5.1 Hyper-parameters On the encoder side, both the forward RNN and the backward RNN have 2-layer stacked LSTMs with 500 hidden units. On the decoder side, all types of decoders are 2-layer stacked LSTMs with 500 hidden units. All LSTM parameters are sampled from U(−0.1, 0.1). The dropout rate is set to 0.3. The word embedding size is 300 for input tokens on both the encoder side and the decoder side. We use open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions6 to initialize the word embeddings on the decoder side. The mini-batch size is set to 64 and the step size set to 50. Gradients are clipped when their norms are greater than 5 (Pascanu et al., 2013). For simplicity, we use vanilla softmax over the decoder vocabulary as opposed to more efficient alternatives such as sampled softmax (Jean et al., 2015). The vocabulary size is set to 40,000. The number of epochs 6 https://nlp.stanford.edu/projects/ glove/ 836 5.2.4 Evaluation using BLEU Table 2 shows the cased BLEU scores of linearized PredPatt an"
I17-1084,D15-1044,0,0.017442,"tokens from the selected decoder to update the beam. When the special token heosi is generated, we remove the candidate sequence from the beam. 4 Related Work The model we propose in this paper is adapted from the RNN encoder-decoder architectures which have been successfully applied to a wide range of NLP tasks such as machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b), syntactic parsing (Vinyals et al., 2015a), question answering (Hermann et al., 2015), summarization (Rush et al., 2015), and semantic parsing (Dong and Lapata, 2016). As a novel variation of the encoder-decoder architecture, our model provides a general solution to tasks involving translation and labeling. crosslingual open IE is an example of this kind of task. The end-to-end solution proposed by Zhang et al. (2017a) used a vanilla attention-based encoderdecoder model to achieve results which outperform the traditional pipeline solutions. Compared to the vanilla encoder-decoder model, our model splits the joint task into two concurrent tasks (i.e., labeling and translating), which are jointly learnt by a sele"
I17-1084,D13-1176,0,0.014997,"e a well-formed syntax-semantics interface (Zhang et al., 2017b). The beam is used to increase the search space for the sequence Y in the target language. At each decoding time step, we first greedily select the type of decoder, and then generate candidate tokens from the selected decoder to update the beam. When the special token heosi is generated, we remove the candidate sequence from the beam. 4 Related Work The model we propose in this paper is adapted from the RNN encoder-decoder architectures which have been successfully applied to a wide range of NLP tasks such as machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b), syntactic parsing (Vinyals et al., 2015a), question answering (Hermann et al., 2015), summarization (Rush et al., 2015), and semantic parsing (Dong and Lapata, 2016). As a novel variation of the encoder-decoder architecture, our model provides a general solution to tasks involving translation and labeling. crosslingual open IE is an example of this kind of task. The end-to-end solution proposed by Zhang et al. (2017a) used a vanilla attention-based encoderdecoder model"
I17-1084,W09-0441,0,0.0375213,"great importance to solve the cross-lingual portability issues of various NLP systems which are in the support of open information extraction (Sudo et al., 2004). Additionally, there is often a great demand for rapid access to information across languages, especially when a large-scale incident occurs (Lu et al., 2016). Conventional solutions decompose the task as a pipeline of machine translation followed by open information extraction (or vice versa), which causes a deviation since machine translation attaches equal importance to adequacy and fluency of the intermediate translation results (Snover et al., 2009), whereas the final goal focuses more on extracting correct predicates and arguments. Recently Zhang et al. (2017a) proposes an endto-end solution that outperforms the conventional pipeline solutions. They recast cross-lingual open IE as a sequence-to-sequence learning problem by converting the target facts in the tree structure (Fig. 1(c)) into a linear form called linearized PredPatt2 (Fig. 1(d)), and employ a stan1 The predicate-argument information is normally represented by relation tuples. Here, we use a richer representation (i.e., a tree structure) adopted by PredPatt (White et al., 20"
I17-1084,W17-6944,1,0.862332,"Missing"
I17-1084,L16-1521,0,0.0670588,"罪状 ” 的 良 机, (As a result, the Democratic party lost a good opportunity to list the ‘ charges ’ against Bush. Reference: (1) As (a result) , (the Democratic party) lost (a good opportunity) (2) (a good opportunity) list (the ‘ charges ’ against Bush) Selective Decoding: Datasets Task #Train #Valid #Test uzb-eng 31,581 1,373 1,373 tur-eng 20,774 903 903 amh-eng 12,140 527 527 som-eng 10,702 465 465 yor-eng 5,787 251 251 Table 8: Number of data used for cross-lingual open IE in low-resource scenarios. To prepare the experiment datasets, we first collect bitexts from DARPA LORELEI language packs (Strassel and Tracey, 2016). The source languages of the bitexts are Uzbek, Turkish, Amharic, Somali, and Yoruba.10 We then run a process similar to Zhang et al. (2017a) to generate pairs of source-language sentences and English linearized PredPatt: first, we employ SyntaxNet Parser (Andor et al., 2016) to generate Universal Dependency parses for the English sentences, and then run PredPatt (White et al., 2016) to generate English linearized PredPatt from the Universal Dependency parses. We remove empty sequences and very long sequences (length&gt;50) in the pairs, and randomly split them into training, validation and test"
I17-1084,C04-1127,0,0.672647,"English translation (b), English predicate-argument information (c), linearized PredPatt output (d) and output with separated predicate and argument labels (e). Introduction Cross-lingual open information extraction is defined as the task of extracting facts from the source language (e.g., Chinese text in Fig. 1(a)) and representing them in the target language (e.g. English predicate-argument information in Fig. 1(c))1 . It is a challenging task and of great importance to solve the cross-lingual portability issues of various NLP systems which are in the support of open information extraction (Sudo et al., 2004). Additionally, there is often a great demand for rapid access to information across languages, especially when a large-scale incident occurs (Lu et al., 2016). Conventional solutions decompose the task as a pipeline of machine translation followed by open information extraction (or vice versa), which causes a deviation since machine translation attaches equal importance to adequacy and fluency of the intermediate translation results (Snover et al., 2009), whereas the final goal focuses more on extracting correct predicates and arguments. Recently Zhang et al. (2017a) proposes an endto-end sol"
I17-1084,P16-5005,0,0.0126659,"V |Y |× T |Y |is the set of all possible (Y 0 , T 0 ) pairs. And (Yˆ , Tˆ) can be directly converted to the form of linearized PredPatt which is used for evaluation. However, it is impractical to iterate over all these (Y 0 , T 0 ) pairs during inference: here, we use beam search to generate tokens and labels as shown in Algorithm 1. ti ∈T =softmax(Uo0 si + Co0 ci + b0o )[yi ], Inference |V |is the vocabulary size of the target language. 835 whereas the GRU/LSTM gated unit itself learns to memorize long short-term dependencies. Similar mechanisms have been used in neural machine translating (Tu et al., 2016) and image caption generation (Xu et al., 2015) to explicitly control the influence from source or target contexts. The experiments in § 5 also confirms our point: our model using the selective decoding mechanism significantly improves the performance, compared to the standard encoder-decoder model. Regarding to open IE systems for generating training data, PredPatt has shown promising performance on large-scale open IE benchmarks (Zhang et al., 2017c). Compared to other existing open IE systems (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), PredPatt uses manual language-agnost"
I17-1084,D16-1177,1,0.799451,"Missing"
I17-1084,E17-2011,1,0.850608,"Missing"
I17-1096,D14-1162,0,0.0806857,"to handle multiple passages, we incorporated an extra passage ranker component. The architecture is shown in Figure 1. In brief, the embedding/encoder layers first build representations of Q and P . The aggregation layer uses co-attention to fuse information from the QP pair. The output layer is a recurrent net that maintains intermediate state and dynamically decides at which turn to generate the answer. 3.1 Detailed description of ReasoNet++ Embedding Layer: We adopt three types of embeddings to represent input word tokens in Q and P . For word embeddings, we use pre-trained GloVe vectors (Pennington et al., 2014). To address the out-of-vocabulary problem, we also include character and letter 3-gram embeddings. Character embeddings are fed into a convolutional neural network (CNN) as in (Kim, 2014), then max-pooled to form a fixed-size vector for each token. For letter 3-gram embeddings, we follow Huang et al. (2013) by first hashing each word as a bag of letter 3-gram, then feeding them into another CNN. The concatenation of all embeddings are then fed to a two-layer Highway Network (Srivastava et al., 2015). Therefore, we obtain the 959 Pi=m−1 p p H p , H q C q , i=0 H:i c are all 2d by n matrices, U"
I17-1096,D16-1264,0,0.366414,"models have embraced this kind of multiple-turn strategy; they generate predictions by making multiple passes through the same text and integrating intermediate information in the process (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Shen et al., 2016). While state-of-the-art results have been achieved by these models, there has yet to be an in-depth analysis of the impact of the multiple-turn strategy to reasoning. This paper attempts to fill this gap. We provide empirical results and analysis on two challenging RC datasets: the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), and the Microsoft Machine Reading We find that multiple-turn reasoning outperforms single-turn reasoning across the board for various types of question and answer types. Furthermore, the flexibility to dynamically decide the 957 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 957–966, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP query source answer (A) #questions (Q) #passages (P ) SQuAD crowdsourced span of words 100K questions 23K paragraphs MS MARCO user logs free-form text 100K queries 1M paragraphs MS MARCO: MS MARCO is a la"
I17-1096,D13-1020,0,0.0623584,"Missing"
I17-1096,D14-1181,0,0.00420097,"e aggregation layer uses co-attention to fuse information from the QP pair. The output layer is a recurrent net that maintains intermediate state and dynamically decides at which turn to generate the answer. 3.1 Detailed description of ReasoNet++ Embedding Layer: We adopt three types of embeddings to represent input word tokens in Q and P . For word embeddings, we use pre-trained GloVe vectors (Pennington et al., 2014). To address the out-of-vocabulary problem, we also include character and letter 3-gram embeddings. Character embeddings are fed into a convolutional neural network (CNN) as in (Kim, 2014), then max-pooled to form a fixed-size vector for each token. For letter 3-gram embeddings, we follow Huang et al. (2013) by first hashing each word as a bag of letter 3-gram, then feeding them into another CNN. The concatenation of all embeddings are then fed to a two-layer Highway Network (Srivastava et al., 2015). Therefore, we obtain the 959 Pi=m−1 p p H p , H q C q , i=0 H:i c are all 2d by n matrices, U is a 8d by n matrix. Finally, to incorporate the full context, the “memory cells” of the passage are computed by a bidirectional GRU on top of U : final embedding for the words in Q as a"
I17-1096,W04-1013,0,0.0155775,"e down answers in free-form text, and according to the authors of MS MARCO, the complexity of answer varies from a single “yes/no” or entity name (e.g. Q: “What is the capital of Italy”; A: Rome), to long textual answers (e.g. Q: “What is the agenda for Hollande’s state visit to Washington?”). Long textual answers may need to be derived through reasoning across multiple pieces of text. The dataset is partitioned into a 82,430 training, a 10,047 development, and 9,650 test tuples. Since the answer is free-form text, the evaluation metrics of choice are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004). To apply the same RC model to both SQuAD (where answers are text spans in P ) and MS MARCO (where answer are free-form text), we search for spans in MS MARCO’s passages that maximizes the ROUGEL score with the raw free-form answers. Our training data uses these spans as labels, but we evaluate our model with respect to the raw free-form answers; this has an upper bound of 94.23 BLEU and 87.53 ROUGE-L on the dev set. By this construction, there are multiple number of passages to read for each question, but the answer span might only involve a few passages (i.e. the ones that include the max R"
I17-1096,D16-1013,0,0.0289406,"Missing"
I17-1096,P02-1040,0,0.0987244,"on retrieval system. The judges write down answers in free-form text, and according to the authors of MS MARCO, the complexity of answer varies from a single “yes/no” or entity name (e.g. Q: “What is the capital of Italy”; A: Rome), to long textual answers (e.g. Q: “What is the agenda for Hollande’s state visit to Washington?”). Long textual answers may need to be derived through reasoning across multiple pieces of text. The dataset is partitioned into a 82,430 training, a 10,047 development, and 9,650 test tuples. Since the answer is free-form text, the evaluation metrics of choice are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004). To apply the same RC model to both SQuAD (where answers are text spans in P ) and MS MARCO (where answer are free-form text), we search for spans in MS MARCO’s passages that maximizes the ROUGEL score with the raw free-form answers. Our training data uses these spans as labels, but we evaluate our model with respect to the raw free-form answers; this has an upper bound of 94.23 BLEU and 87.53 ROUGE-L on the dev set. By this construction, there are multiple number of passages to read for each question, but the answer span might only involve a few passages (i.e. the one"
I17-1100,W07-1207,0,0.0166474,"o varying degrees. On one end of the spectrum are approaches that utilize declarative logical forms. In such approaches, semantic parsers first convert a sequence into a meaning representation that expresses the semantics needed for inference. In this case, each individual component of the logical form is clearly interpretable. Tremendous energies within computational linguistics have been spent on building declarative, componentwise-interpretable logical forms such as Hobbsian Logic (Hobbs, 1985), Discourse Representation Theory (Kamp et al., 2011), the Rochester Interactive Planning System (Allen et al., 2007), Minimal Recursion Semantics (Copestake et al., 2005), Episodic Logic (Schubert and Hwang, 2000), Combinatory Categorical Grammar (Steedman, 2000), Semantic Role Labeling (Gildea and Jurafsky, 2002), Framenet Parsing (Fillmore et al., 2003) and Abstract Meaning Representation (Banarescu et al., 2013). Opposite the above approaches are methods that utilize vector space-based logical forms. Recent work on word and string embeddings (Mikolov et al., 2013; Pennington et al., 2014) has produced vector space representations that can be induced from large corpora in an unsupervised manner that have"
I17-1100,W13-2322,0,0.0602476,"Missing"
I17-1100,N13-1092,1,0.843593,"Missing"
I17-1100,W07-1401,0,0.381524,"ral strategy to automatically generate one or more sentential hypotheses based on an input sentence and pre-existing manual semantic annotations. The resulting suite of datasets enables us to probe a statistical RTE model’s performance on different aspects of semantics. We demonstrate the value of this approach by investigating the behavior of a popular neural network RTE model. 1 Introduction The Recognizing Textual Entailment (RTE) task aims to assess a system’s ability to do textual inference—i.e. derive valid conclusions from textual clues (Dagan et al., 2006, 2013; Bar-Haim et al., 2006; Giampiccolo et al., 2007, 2009; Bentivogli et al., 2009, 2010, 2011). In this task, a system judges whether “typically, a human reading [the sentential context, or text] T would infer that [the sentential hypothesis] H is most likely true” (Dagan et al., 2006). Recent efforts in textual inference have focused on the Stanford Natural Language Inference (SNLI) dataset. SNLI is made up of hundreds of thousands of text-hypothesis pairs, wherein the texts are image captions drawn from the Flickr30k corpus (Young et al., 2014) and the hypotheses are elicited from crowdsourcing workers based on those captions (but not the c"
I17-1100,D15-1075,0,0.402868,"he FrameNet Plus (FN+) dataset, which contains likelihood judgments about the paraphrase validity of frame triggers (Pavlick et al., 2015), and 1 These recasted datasets are made publicly available at http://decomp.net. 996 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 996–1005, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (iii) the Definite Pronoun Resolution (DPR) dataset, which contains annotations relevant to complex anaphora resolution (Rahman and Ng, 2012). We use these recasted datasets to train a recent neural RTE model (Bowman et al., 2015) and measure its performance. We show that complex anaphora is the most difficult semantic phenomenon for neural RTE models to capture, followed by predicting thematic proto-role properties. Perhaps unsurprisingly, given the nature of the RTE task, paraphrasing seems to be the easiest phenomenon to model. In the next section (§2), we discuss previous work in RTE, focusing in particular on the development of RTE datasets. We then discuss our data creation process (§3) as well as the results of a small validation (§4). Finally, we report on the setup and results of our three experiments (§5) and"
I17-1100,J02-3001,0,0.0290378,"that expresses the semantics needed for inference. In this case, each individual component of the logical form is clearly interpretable. Tremendous energies within computational linguistics have been spent on building declarative, componentwise-interpretable logical forms such as Hobbsian Logic (Hobbs, 1985), Discourse Representation Theory (Kamp et al., 2011), the Rochester Interactive Planning System (Allen et al., 2007), Minimal Recursion Semantics (Copestake et al., 2005), Episodic Logic (Schubert and Hwang, 2000), Combinatory Categorical Grammar (Steedman, 2000), Semantic Role Labeling (Gildea and Jurafsky, 2002), Framenet Parsing (Fillmore et al., 2003) and Abstract Meaning Representation (Banarescu et al., 2013). Opposite the above approaches are methods that utilize vector space-based logical forms. Recent work on word and string embeddings (Mikolov et al., 2013; Pennington et al., 2014) has produced vector space representations that can be induced from large corpora in an unsupervised manner that have been used to initialize the training of neural networks for tasks as complex as English-to-French machine translation (Sutskever et al., 2014). Vector space-based intermediate forms are not commonly"
I17-1100,P16-1223,0,0.0136742,"mena. To construct such datasets we presented a general strategy of converting semantic classification examples to annotated textual inference pairs that can be used to create large datasets for free on which even neural models for RTE can be trained. Further we used these datasets to gain insights into the behavior of a popular neural RTE model and the SNLI dataset itself. The variation in the performance of that model on the three datasets showed that neural models for natural language understanding recognise lexical variations or paraphrasing much better than anaphora resolution. Recently (Chen et al., 2016) also presented a similar conclusion after manually analyzing the errors made by neural systems on a reading comprehenproperties on the SPR test set. The percentage numbers at the bottom are the contribution of the category above to the total errors. Error percentages that are close to each other are omitted for clarity. Acknowledgments This research was supported by the JHU HLTCOE, DARPA DEFT, DARPA LORELEI, and the JHU Science of Learning Institute. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this pu"
I17-1100,D16-1053,0,0.011158,"Missing"
I17-1100,P85-1008,0,0.766062,"m All approaches to natural language understanding utilize intermediate logical forms that are interpretable to varying degrees. On one end of the spectrum are approaches that utilize declarative logical forms. In such approaches, semantic parsers first convert a sequence into a meaning representation that expresses the semantics needed for inference. In this case, each individual component of the logical form is clearly interpretable. Tremendous energies within computational linguistics have been spent on building declarative, componentwise-interpretable logical forms such as Hobbsian Logic (Hobbs, 1985), Discourse Representation Theory (Kamp et al., 2011), the Rochester Interactive Planning System (Allen et al., 2007), Minimal Recursion Semantics (Copestake et al., 2005), Episodic Logic (Schubert and Hwang, 2000), Combinatory Categorical Grammar (Steedman, 2000), Semantic Role Labeling (Gildea and Jurafsky, 2002), Framenet Parsing (Fillmore et al., 2003) and Abstract Meaning Representation (Banarescu et al., 2013). Opposite the above approaches are methods that utilize vector space-based logical forms. Recent work on word and string embeddings (Mikolov et al., 2013; Pennington et al., 2014)"
I17-1100,W11-1902,0,0.0148632,"Missing"
I17-1100,marelli-etal-2014-sick,0,0.0349083,"on top of natural text. 2.2 Approaches to textual entailment Research in textual entailment, at least in its most recent form, was catalyzed by the RTE shared task (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007, 2009; Bentivogli et al., 2009, 2010, 2011). With each iteration of this shared task, manually annotated examples were created for testing competing systems. But even after multiple iterations, the amount of available data for RTE was still small. The Sentences Involving Compositional Knowledge (SICK) corpus was released with the goal of alleviating this problem (Marelli et al., 2014). A significant further contribution was made with the Stanford Natural Language Inference (SNLI) corpus, which uses crowdsourcing to gather two orders of magnitude more examples than all previous datasets (Bowman et al., 2015). SNLI enabled fully supervised training of powerful machine learning models like neural networks. A number of researchers have pursued this direction by applying completely supervised neural models for sequential data to the problem of textual entailment (Rockt¨aschel et al., 2015; Mou et al., 2015; Shuohang and Jing, 2015; Liu et al., 2016; Cheng et al., 2016; Parikh e"
I17-1100,N13-1090,0,0.00783932,"ical forms such as Hobbsian Logic (Hobbs, 1985), Discourse Representation Theory (Kamp et al., 2011), the Rochester Interactive Planning System (Allen et al., 2007), Minimal Recursion Semantics (Copestake et al., 2005), Episodic Logic (Schubert and Hwang, 2000), Combinatory Categorical Grammar (Steedman, 2000), Semantic Role Labeling (Gildea and Jurafsky, 2002), Framenet Parsing (Fillmore et al., 2003) and Abstract Meaning Representation (Banarescu et al., 2013). Opposite the above approaches are methods that utilize vector space-based logical forms. Recent work on word and string embeddings (Mikolov et al., 2013; Pennington et al., 2014) has produced vector space representations that can be induced from large corpora in an unsupervised manner that have been used to initialize the training of neural networks for tasks as complex as English-to-French machine translation (Sutskever et al., 2014). Vector space-based intermediate forms are not commonly recognized as logical forms but in light of recent work (Bouchard et al., 2015) it seems worthwhile to reconsider this view. An argument in favor of declarative, interpretable logical forms is that one can directly observe the specific mistakes made by a sy"
I17-1100,E17-1002,0,0.0126305,"Missing"
I17-1100,N16-1170,0,0.0107298,"Missing"
I17-1100,D17-3004,1,0.869421,"Missing"
I17-1100,J05-1004,0,0.05908,"textual inference examples. This strategy requires only minor effort in developing datasetspecific generation capabilities to recast annotations into a shared universal representation: natural language sentences. We demonstrate the use of this strategy in two steps. First, we construct three recasted datasets from three existing semantic resources that target three distinct semantic phenomena:1 (i) the Semantic Proto-Roles v1 (SPR) dataset (Reisinger et al., 2015), which contains likelihood judgments about the semantic proto-role properties (Dowty, 1991) of verbal arguments found in PropBank (Palmer et al., 2005), (ii) the FrameNet Plus (FN+) dataset, which contains likelihood judgments about the paraphrase validity of frame triggers (Pavlick et al., 2015), and 1 These recasted datasets are made publicly available at http://decomp.net. 996 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 996–1005, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (iii) the Definite Pronoun Resolution (DPR) dataset, which contains annotations relevant to complex anaphora resolution (Rahman and Ng, 2012). We use these recasted datasets to train a recent neural RTE"
I17-1100,D16-1244,0,0.0215936,"Missing"
I17-1100,P15-2067,1,0.597886,"Missing"
I17-1100,D14-1162,0,0.100309,"Missing"
I17-1100,D12-1071,0,0.0655808,"perties (Dowty, 1991) of verbal arguments found in PropBank (Palmer et al., 2005), (ii) the FrameNet Plus (FN+) dataset, which contains likelihood judgments about the paraphrase validity of frame triggers (Pavlick et al., 2015), and 1 These recasted datasets are made publicly available at http://decomp.net. 996 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 996–1005, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (iii) the Definite Pronoun Resolution (DPR) dataset, which contains annotations relevant to complex anaphora resolution (Rahman and Ng, 2012). We use these recasted datasets to train a recent neural RTE model (Bowman et al., 2015) and measure its performance. We show that complex anaphora is the most difficult semantic phenomenon for neural RTE models to capture, followed by predicting thematic proto-role properties. Perhaps unsurprisingly, given the nature of the RTE task, paraphrasing seems to be the easiest phenomenon to model. In the next section (§2), we discuss previous work in RTE, focusing in particular on the development of RTE datasets. We then discuss our data creation process (§3) as well as the results of a small valid"
I17-1100,W14-2901,1,0.917501,"Missing"
I17-1100,Q15-1034,1,0.836899,"Missing"
I17-1100,D16-1177,1,0.475288,"Missing"
I17-1100,Q14-1006,0,0.0927602,"erive valid conclusions from textual clues (Dagan et al., 2006, 2013; Bar-Haim et al., 2006; Giampiccolo et al., 2007, 2009; Bentivogli et al., 2009, 2010, 2011). In this task, a system judges whether “typically, a human reading [the sentential context, or text] T would infer that [the sentential hypothesis] H is most likely true” (Dagan et al., 2006). Recent efforts in textual inference have focused on the Stanford Natural Language Inference (SNLI) dataset. SNLI is made up of hundreds of thousands of text-hypothesis pairs, wherein the texts are image captions drawn from the Flickr30k corpus (Young et al., 2014) and the hypotheses are elicited from crowdsourcing workers based on those captions (but not the corresponding image). While SNLI has led to significant methodological improvements, its collection protocol does not lend itself to understanding the types of semantic knowledge necessary for properly understanding a particular example. Researchers compete on which system achieves the highest score on a test set, but this itself does not lead to an understanding of which linguistic properties are better captured by a quantitatively superior system. In contrast, datasets such as FraCaS (Cooper et a"
I17-2004,D16-1162,0,0.0293866,"sults compared to NMT decoding with standard beam search. Introduction Domain adaptation is a major challenge for neural machine translation (NMT). Although impressive improvements have been achieved in recent years (c.f. Bojar et al. (2016)), NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). In such situations, neural systems often produce fluent output that unfortunately contains words not licensed by the unfamiliar source sentence (Arthur et al., 2016; Tu et al., 2016). Phrase-based systems, in contrast, explicitly model the translation of all source words via coverage vectors, and tend to produce translations that are adequate but less fluent. This situation is depicted in Table 1, which contains examples of PBMT and NMT systems trained on WMT training sets which are then applied to IT texts. We present an approach that combines the best of both worlds by using the lattice output of PBMT to constrain the search space available to an NMT decoder, thereby bringing together the adequacy Search Graph Source sentence PBMT NMT Lattice Search Ta"
I17-2004,D13-1106,0,0.0627969,"Missing"
I17-2004,P17-2061,0,0.0264912,"on? While the answer depends on the amount of in-domain and out-of-domain data, we find that PBMTin × NMTin and PBMTin × NMTout perform the best. This supports previous findings (Koehn and Knowles, 2017) that PBMTin is robust when training data is insufficient. In conclusion, we recommend using lattice search with search graphs from PBMTin , and NMT models can be trained on either in-domain or out-of-domain corpora. Related Work Previous work on domain adaptation in NMT focuses on training methods such as transfer learning or fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017). This strategy begins with a strong model trained on a large out-of-domain corpus and then continuesx training on an in-domain corpus. Our approach is orthogonal in that we focus on search. Conceivably, advances in training methods might be incorporated to improve our individual NMTin models. Our lattice search algorithm is related to previous work in hybrid NMT/PBMT systems, which can be visualized on a spectrum depending on how tightly integrated the two systems are. On one end, NMT can easily be used to rerank N best lists output by PBMT; on the other, NMT can be incorporated as features i"
I17-2004,D11-1103,0,0.0778349,"Missing"
I17-2004,P14-1129,0,0.0327377,"combine in (b). This figure does not demonstrate pruning, descendants of items that fall off the beam would not be explored. 2 This is similar to PBMT stack decoding. However, in PBMT stack decoding, stacks are grouped by the number of translated source words, which is not possible in NMT, since the translation of individual source words is not tracked. 21 Corpus Medical IT Koran Subtitles WMT Words 14,301,472 3,041,677 9,848,539 114,371,754 113,165,079 Sentences 1,104,752 337,817 480,421 13,873,398 4,562,102 W/S 13 9 21 8 25 include German specific processing and Neural Network Joint Models (Devlin et al., 2014), replicating Ding et al. (2016). The PBMTin models are Moses models with standard settings, replicating Koehn and Knowles (2017). The NMT models are trained with Nematus (Sennrich et al., 2017). The NMTout models replicate Sennrich et al. (2016);4 the NMTin models replicate Koehn and Knowles (2017). We use Marian (Junczys-Dowmunt et al., 2016a) to rescore N -best lists. The search graphs are pre-processed by converting them to the OpenFST format (Allauzen et al., 2007) and applying operations to remove epsilon arcs, determinize, minimize and topsort. Since the search graphs may be prohibitive"
I17-2004,E17-3017,0,0.0437165,"Missing"
I17-2004,W16-2323,0,0.0301409,"ed source words, which is not possible in NMT, since the translation of individual source words is not tracked. 21 Corpus Medical IT Koran Subtitles WMT Words 14,301,472 3,041,677 9,848,539 114,371,754 113,165,079 Sentences 1,104,752 337,817 480,421 13,873,398 4,562,102 W/S 13 9 21 8 25 include German specific processing and Neural Network Joint Models (Devlin et al., 2014), replicating Ding et al. (2016). The PBMTin models are Moses models with standard settings, replicating Koehn and Knowles (2017). The NMT models are trained with Nematus (Sennrich et al., 2017). The NMTout models replicate Sennrich et al. (2016);4 the NMTin models replicate Koehn and Knowles (2017). We use Marian (Junczys-Dowmunt et al., 2016a) to rescore N -best lists. The search graphs are pre-processed by converting them to the OpenFST format (Allauzen et al., 2007) and applying operations to remove epsilon arcs, determinize, minimize and topsort. Since the search graphs may be prohibitively large in size, we prune them with a threshold.5 We perform 5fold cross-validation over pruning thresholds (.1, .25, .5) and lattice search beamsizes (1, 10, 100). Very aggressive pruning with a small beam limits the search to be very similar t"
I17-2004,W16-2316,0,0.0613373,"ds is not tracked. 21 Corpus Medical IT Koran Subtitles WMT Words 14,301,472 3,041,677 9,848,539 114,371,754 113,165,079 Sentences 1,104,752 337,817 480,421 13,873,398 4,562,102 W/S 13 9 21 8 25 include German specific processing and Neural Network Joint Models (Devlin et al., 2014), replicating Ding et al. (2016). The PBMTin models are Moses models with standard settings, replicating Koehn and Knowles (2017). The NMT models are trained with Nematus (Sennrich et al., 2017). The NMTout models replicate Sennrich et al. (2016);4 the NMTin models replicate Koehn and Knowles (2017). We use Marian (Junczys-Dowmunt et al., 2016a) to rescore N -best lists. The search graphs are pre-processed by converting them to the OpenFST format (Allauzen et al., 2007) and applying operations to remove epsilon arcs, determinize, minimize and topsort. Since the search graphs may be prohibitively large in size, we prune them with a threshold.5 We perform 5fold cross-validation over pruning thresholds (.1, .25, .5) and lattice search beamsizes (1, 10, 100). Very aggressive pruning with a small beam limits the search to be very similar to the PBMT output. In contrast, a very deep lattice with a large beam begins to approach the uncons"
I17-2004,E17-2058,0,0.0705903,"Missing"
I17-2004,P07-2045,1,0.0125337,"in terms of domain, but the training data is not large (relative to WMT). 3. PBMTin × NMTout : PBMT is trained on small in-domain data while NMT is trained on larger out-of-domain data. 4. PBMTout × NMTin : NMT is trained on small in-domain data while PBMT is trained on larger out-of-domain data. For each training configuration, we are interested in seeing how our proposed NMT lattice search compares to standard NMT beam search. Additionally, we compare the results of PBMT 1best decoding and PBMT N -best lists rescoring (N=500) using the same NMT model. The PBMT models are trained with Moses (Koehn et al., 2007). The PBMTout models 3 Results 4 github.com/rsennrich/wmt16-scripts Pruning removes arcs that do not appear on a lattice path whose score is within than t ⊗ w, where w is the weight of the FST’s shortest path, and t is the pruning threshold. 5 opensubtitles.org 22 Test Domain IT Medical Koran Subtitle Training Configuration PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMT 1-best 25.1 ("
I17-2004,P16-2049,0,0.037377,"us on search. Conceivably, advances in training methods might be incorporated to improve our individual NMTin models. Our lattice search algorithm is related to previous work in hybrid NMT/PBMT systems, which can be visualized on a spectrum depending on how tightly integrated the two systems are. On one end, NMT can easily be used to rerank N best lists output by PBMT; on the other, NMT can be incorporated as features in PBMT (JunczysDowmunt et al., 2016b). In the middle of the spectrum is NMT search (or re-scoring) based on constraints from PBMT. Our algorithm is conceptually very similar to Stahlberg et al. (2016), who rescore a WFSA reformulation of the Hiero formalism. Their 23 References algorithm is a breadth-first search over all the nodes of the lattice, capped by a beam. Other hybrid methods include: constraining the output vocabulary of NMT on a per-sentence basis, using bilingual information provided by PBMT (Mi et al., 2016), Minimum Bayes Risk decoding with PBMT n-gram posteriors (Stahlberg et al., 2017), and incorporating PBMT hypotheses as additional input in a modified NMT architecture (Wang et al., 2017). Related works in lattice search/re-scoring with RNNs (without NMT encoder-decoders)"
I17-2004,W17-3204,1,0.933362,"le stack-based lattice search algorithm for NMT,1 and (2) a set of domain adaptation experiments showing that PBMT lattice constraints are effective in achieving robust results compared to NMT decoding with standard beam search. Introduction Domain adaptation is a major challenge for neural machine translation (NMT). Although impressive improvements have been achieved in recent years (c.f. Bojar et al. (2016)), NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). In such situations, neural systems often produce fluent output that unfortunately contains words not licensed by the unfamiliar source sentence (Arthur et al., 2016; Tu et al., 2016). Phrase-based systems, in contrast, explicitly model the translation of all source words via coverage vectors, and tend to produce translations that are adequate but less fluent. This situation is depicted in Table 1, which contains examples of PBMT and NMT systems trained on WMT training sets which are then applied to IT texts. We present an approach that combines the best of both worlds by using the lattice ou"
I17-2004,tiedemann-2012-parallel,0,0.123673,"Missing"
I17-2004,P16-1008,0,0.025807,"decoding with standard beam search. Introduction Domain adaptation is a major challenge for neural machine translation (NMT). Although impressive improvements have been achieved in recent years (c.f. Bojar et al. (2016)), NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). In such situations, neural systems often produce fluent output that unfortunately contains words not licensed by the unfamiliar source sentence (Arthur et al., 2016; Tu et al., 2016). Phrase-based systems, in contrast, explicitly model the translation of all source words via coverage vectors, and tend to produce translations that are adequate but less fluent. This situation is depicted in Table 1, which contains examples of PBMT and NMT systems trained on WMT training sets which are then applied to IT texts. We present an approach that combines the best of both worlds by using the lattice output of PBMT to constrain the search space available to an NMT decoder, thereby bringing together the adequacy Search Graph Source sentence PBMT NMT Lattice Search Target translation F"
I17-2004,2015.iwslt-evaluation.11,0,0.0598293,"hich training configuration is best for domain adaptation? While the answer depends on the amount of in-domain and out-of-domain data, we find that PBMTin × NMTin and PBMTin × NMTout perform the best. This supports previous findings (Koehn and Knowles, 2017) that PBMTin is robust when training data is insufficient. In conclusion, we recommend using lattice search with search graphs from PBMTin , and NMT models can be trained on either in-domain or out-of-domain corpora. Related Work Previous work on domain adaptation in NMT focuses on training methods such as transfer learning or fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017). This strategy begins with a strong model trained on a large out-of-domain corpus and then continuesx training on an in-domain corpus. Our approach is orthogonal in that we focus on search. Conceivably, advances in training methods might be incorporated to improve our individual NMTin models. Our lattice search algorithm is related to previous work in hybrid NMT/PBMT systems, which can be visualized on a spectrum depending on how tightly integrated the two systems are. On one end, NMT can easily be used to rerank N best lists output by PBMT; on"
I17-2004,P16-2021,0,0.0231845,"o rerank N best lists output by PBMT; on the other, NMT can be incorporated as features in PBMT (JunczysDowmunt et al., 2016b). In the middle of the spectrum is NMT search (or re-scoring) based on constraints from PBMT. Our algorithm is conceptually very similar to Stahlberg et al. (2016), who rescore a WFSA reformulation of the Hiero formalism. Their 23 References algorithm is a breadth-first search over all the nodes of the lattice, capped by a beam. Other hybrid methods include: constraining the output vocabulary of NMT on a per-sentence basis, using bilingual information provided by PBMT (Mi et al., 2016), Minimum Bayes Risk decoding with PBMT n-gram posteriors (Stahlberg et al., 2017), and incorporating PBMT hypotheses as additional input in a modified NMT architecture (Wang et al., 2017). Related works in lattice search/re-scoring with RNNs (without NMT encoder-decoders) (Ladhak et al., 2016; Deoras et al., 2011; Hori et al., 2014) may serve as other interesting comparisons. Specifically, Auli et al. (2013) and Liu et al. (2016) provide alternatives to our approach to the problem of recombination. The former work allows the splitting of previously recombined decoder states (thresholded) whil"
I17-2016,J96-1002,0,0.329562,"Language Processing, pages 91–96, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP to each token: B if the token is the beginning of an entity, or I if the token is inside an entity, or O if the token is outside an entity (see Fig. 1). Following convention, we focus on person (per), location (loc), organization (org), and miscellaneous (misc) entity types, resulting in 9 tags: {B - ORG, I - ORG, B - PER , I - PER , B - LOC , I - LOC , B - MISC , I - MISC }. Conditional Random Fields (CRFs), first introduced in Lafferty et al. (2001), generalize the classical maximum entropy models (Berger et al., 1996) to distributions over structured objects, and are an effective tool for sequence labeling tasks like NER. We briefly overview the formalism here and then discuss its neural parameterization. 2.1 features are conjoined with other indicator features, e.g., is the ith tag I - LOC? We refer the reader to Sha and Pereira (2003) for standard CRF feature functions employed in NER, which we use in this work. The log-linear parameterization yields a convex objective and is extremely efficient to compute as it only involves a sparse dot product, but the representational power of model depends fully on"
I17-2016,Q16-1026,0,0.00598678,". Now, given a low-resource target language τ and a source language σ (potentially, a set of m highresource source languages {σi }m i=1 ). We consider the following training objective X L (θ) = log pθ (t |w, τ ) + (7) X (t,w)∈Dτ µ· log pθ (t |w, σ) , Character-Level Neural Networks. In recent years, many authors have incorporated characterlevel information into taggers using neural networks, e.g., dos Santos and Zadrozny (2014) employed a convolutional network for part-of-speech tagging in morphologically rich languages and Ling et al. (2015) a LSTM for a myriad of different tasks. Relatedly, Chiu and Nichols (2016) approached NER with character-level LSTMs, but without using a CRF. Our work firmly builds upon on this in that we, too, compactly summarize the word form with a recurrent neural component. Neural Transfer Schemes. Previous work has also performed transfer learning using neural networks. The novelty of our work lies in the crosslingual transfer. For example, Peng and Dredze (2017) and Yang et al. (2017), similarly oriented concurrent papers, focus on domain adaptation within the same language. While this is a related problem, cross-lingual transfer is much more involved since the morphology,"
I17-2016,P07-1033,0,0.0792625,"Missing"
I17-2016,P82-1020,0,0.794572,"Missing"
I17-2016,D15-1176,0,0.0395701,"· · c|wi |is shared cross-lingually while e(wi ) is language-specific. Now, given a low-resource target language τ and a source language σ (potentially, a set of m highresource source languages {σi }m i=1 ). We consider the following training objective X L (θ) = log pθ (t |w, τ ) + (7) X (t,w)∈Dτ µ· log pθ (t |w, σ) , Character-Level Neural Networks. In recent years, many authors have incorporated characterlevel information into taggers using neural networks, e.g., dos Santos and Zadrozny (2014) employed a convolutional network for part-of-speech tagging in morphologically rich languages and Ling et al. (2015) a LSTM for a myriad of different tasks. Relatedly, Chiu and Nichols (2016) approached NER with character-level LSTMs, but without using a CRF. Our work firmly builds upon on this in that we, too, compactly summarize the word form with a recurrent neural component. Neural Transfer Schemes. Previous work has also performed transfer learning using neural networks. The novelty of our work lies in the crosslingual transfer. For example, Peng and Dredze (2017) and Yang et al. (2017), similarly oriented concurrent papers, focus on domain adaptation within the same language. While this is a related p"
I17-2016,I13-1183,0,0.0177086,"tures the user selects. 2.3 Modern CRFs, however, try to obviate the handselection of features through deep, non-linear parameterizations of ψ (ti−1 , ti , w; θ). This idea is far from novel and there have been numerous attempts in the literature over the past decade to find effective non-linear parameterizations (Peng et al., 2009; Do and Arti`eres, 2010; Collobert et al., 2011; Vinel et al., 2011; Fujii et al., 2012). Until recently, however, it was not clear that these nonlinear parameterizations of CRFs were worth the non-convexity and the extra computational cost. Indeed, on neural CRFs, Wang and Manning (2013) find that “a nonlinear architecture offers no benefits in a high-dimensional discrete feature space.” However, recently with the application of long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) recurrent neural networks (RNNs) (Elman, 1990) to CRFs, it has become clear that neural feature extractors are superior to the hand-crafted approaches (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). As our starting point, we build upon the architecture of Lample et al. (2016), which is currently competitive with the state of the art for NER. CRFs: A Cursory Overview We star"
I17-2016,P16-1101,0,0.00986082,"til recently, however, it was not clear that these nonlinear parameterizations of CRFs were worth the non-convexity and the extra computational cost. Indeed, on neural CRFs, Wang and Manning (2013) find that “a nonlinear architecture offers no benefits in a high-dimensional discrete feature space.” However, recently with the application of long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) recurrent neural networks (RNNs) (Elman, 1990) to CRFs, it has become clear that neural feature extractors are superior to the hand-crafted approaches (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). As our starting point, we build upon the architecture of Lample et al. (2016), which is currently competitive with the state of the art for NER. CRFs: A Cursory Overview We start with two discrete alphabets Σ and ∆. In the case of sentence-level sequence tagging, Σ is a set of words (potentially infinite) and ∆ is a set of tags (generally finite; in our case |∆ |= 9). Given t = t1 · · · tn ∈ ∆n and w = w1 · · · wn ∈ Σn , where n is the sentence length. A CRF is a globally normalized conditional probability distribution, n pθ (t |w) = 1 Y ψ (ti−1 , ti , w; θ) , (1) Zθ (w) i=1 where ψ (ti−1 ,"
I17-2016,Q14-1005,0,0.0716736,"better at transferring entity-level abstractions cross-linguistically. Projection-based Transfer Schemes. Projection is a common approach to tag low-resource languages. The strategy involves annotating one side of bitext with a tagger for a high-resource language and then project the annotation the over the bilingual alignments obtained through unsupervised learning (Och and Ney, 2003). Using these projected annotations as weak supervision, one then trains a tagger in the target language. This line of research has a rich history, starting with Yarowsky and Ngai (2001). For a recent take, see Wang and Manning (2014) for projecting NER from English to Chinese. We emphasize that projection-based approaches are incomparable to our proposed method as they make an additional bitext assumption, which is generally not present in the case of low-resource languages. 5 5.1 Data We experiment on 15 languages from the crosslingual named entity dataset described in Pan et al. (2017). We focus on 5 typologically diverse3 target languages: Galician, West Frisian, Ukranian, Marathi and Tagalog. As related source languages, we consider Spanish, Catalan, Italian, French, Romanian, Dutch, Russian, Cebuano, Hindi and Urdu."
I17-2016,J03-1002,0,0.0251075,"linear CRF to dominate in the low-resource settings, the neural CRF to dominate in the highresource setting. The novelty of our paper lies in the consideration of the low-resource with transfer case: we show that neural CRFs are better at transferring entity-level abstractions cross-linguistically. Projection-based Transfer Schemes. Projection is a common approach to tag low-resource languages. The strategy involves annotating one side of bitext with a tagger for a high-resource language and then project the annotation the over the bilingual alignments obtained through unsupervised learning (Och and Ney, 2003). Using these projected annotations as weak supervision, one then trains a tagger in the target language. This line of research has a rich history, starting with Yarowsky and Ngai (2001). For a recent take, see Wang and Manning (2014) for projecting NER from English to Chinese. We emphasize that projection-based approaches are incomparable to our proposed method as they make an additional bitext assumption, which is generally not present in the case of low-resource languages. 5 5.1 Data We experiment on 15 languages from the crosslingual named entity dataset described in Pan et al. (2017). We"
I17-2016,P17-1178,0,0.0397626,"ng (Och and Ney, 2003). Using these projected annotations as weak supervision, one then trains a tagger in the target language. This line of research has a rich history, starting with Yarowsky and Ngai (2001). For a recent take, see Wang and Manning (2014) for projecting NER from English to Chinese. We emphasize that projection-based approaches are incomparable to our proposed method as they make an additional bitext assumption, which is generally not present in the case of low-resource languages. 5 5.1 Data We experiment on 15 languages from the crosslingual named entity dataset described in Pan et al. (2017). We focus on 5 typologically diverse3 target languages: Galician, West Frisian, Ukranian, Marathi and Tagalog. As related source languages, we consider Spanish, Catalan, Italian, French, Romanian, Dutch, Russian, Cebuano, Hindi and Urdu. For the language code abbreviations and linguistic families, see Tab. 1. For each of the target languages, we emulate a truly low-resource condition, creating a 100 sentence split for training. We then create a 10000 sentence superset to be able to compare to a high-resource condition in those same Experiments Fundamentally, we want to show that characterleve"
I17-2016,N01-1026,0,0.0335455,"with transfer case: we show that neural CRFs are better at transferring entity-level abstractions cross-linguistically. Projection-based Transfer Schemes. Projection is a common approach to tag low-resource languages. The strategy involves annotating one side of bitext with a tagger for a high-resource language and then project the annotation the over the bilingual alignments obtained through unsupervised learning (Och and Ney, 2003). Using these projected annotations as weak supervision, one then trains a tagger in the target language. This line of research has a rich history, starting with Yarowsky and Ngai (2001). For a recent take, see Wang and Manning (2014) for projecting NER from English to Chinese. We emphasize that projection-based approaches are incomparable to our proposed method as they make an additional bitext assumption, which is generally not present in the case of low-resource languages. 5 5.1 Data We experiment on 15 languages from the crosslingual named entity dataset described in Pan et al. (2017). We focus on 5 typologically diverse3 target languages: Galician, West Frisian, Ukranian, Marathi and Tagalog. As related source languages, we consider Spanish, Catalan, Italian, French, Rom"
I17-2016,W17-2612,0,0.00845038,"networks, e.g., dos Santos and Zadrozny (2014) employed a convolutional network for part-of-speech tagging in morphologically rich languages and Ling et al. (2015) a LSTM for a myriad of different tasks. Relatedly, Chiu and Nichols (2016) approached NER with character-level LSTMs, but without using a CRF. Our work firmly builds upon on this in that we, too, compactly summarize the word form with a recurrent neural component. Neural Transfer Schemes. Previous work has also performed transfer learning using neural networks. The novelty of our work lies in the crosslingual transfer. For example, Peng and Dredze (2017) and Yang et al. (2017), similarly oriented concurrent papers, focus on domain adaptation within the same language. While this is a related problem, cross-lingual transfer is much more involved since the morphology, syntax and semantics change more radically between two languages than (t,w)∈Dσ where µ is a trade-off parameter, Dτ is the set of training examples for the target language and Dσ is the set of training data for the source language σ. In the case of multiple source languages, we add a summand to the set of source languages used, in which case set have multiple training sets Dσi . 93"
I17-2016,W95-0107,0,0.0196601,"ew years, however, neural approaches that jointly learn their own features have surpassed the feature-based approaches in performance. Despite their empirical success, neural networks have remarkably high sample complexity and still only outperform hand-engineered feature approaches when enough supervised training data is available, leaving effective training of neural networks in the low-resource case a challenge. For most of the world’s languages, there is a very 2 Neural Conditional Random Fields Named entity recognition is typically framed as a sequence labeling task using the BIO scheme (Ramshaw and Marcus, 1995; Baldwin, 2009), i.e., given an input sentence, the goal is to assign a label 91 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 91–96, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP to each token: B if the token is the beginning of an entity, or I if the token is inside an entity, or O if the token is outside an entity (see Fig. 1). Following convention, we focus on person (per), location (loc), organization (org), and miscellaneous (misc) entity types, resulting in 9 tags: {B - ORG, I - ORG, B - PER , I - PER , B - LOC , I - LOC ,"
I17-2016,N03-1028,0,0.0497222,"(org), and miscellaneous (misc) entity types, resulting in 9 tags: {B - ORG, I - ORG, B - PER , I - PER , B - LOC , I - LOC , B - MISC , I - MISC }. Conditional Random Fields (CRFs), first introduced in Lafferty et al. (2001), generalize the classical maximum entropy models (Berger et al., 1996) to distributions over structured objects, and are an effective tool for sequence labeling tasks like NER. We briefly overview the formalism here and then discuss its neural parameterization. 2.1 features are conjoined with other indicator features, e.g., is the ith tag I - LOC? We refer the reader to Sha and Pereira (2003) for standard CRF feature functions employed in NER, which we use in this work. The log-linear parameterization yields a convex objective and is extremely efficient to compute as it only involves a sparse dot product, but the representational power of model depends fully on the quality of the features the user selects. 2.3 Modern CRFs, however, try to obviate the handselection of features through deep, non-linear parameterizations of ψ (ti−1 , ti , w; θ). This idea is far from novel and there have been numerous attempts in the literature over the past decade to find effective non-linear parame"
I17-2016,W02-2024,0,0.0641318,"per, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows transfer among the languages, improving F1 by up to 9.8 points over a loglinear CRF baseline. 1 O O B-ORG O B-LOC I-LOC I-LOC Figure 1: Example of an English sentence annotated with its typed named entities. limited amount of training data for NER; CoNLL— the standard dataset in the field—only provides annotations for 4 languages (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Creating similarly sized datasets for other languages has a prohibitive annotation cost, making the lowresource case an important scenario. To get around this barrier, we develop a cross-lingual solution: given a low-resource target language, we additionally offer large amounts of annotated data in a language that is genetically related to the target language. We show empirically that this improves the quality of the resulting model. In terms of neural modeling, we introduce a novel neural conditional random field (CRF) for cross-lingual NER that allows"
I17-2016,W03-0419,0,\N,Missing
I17-2016,N16-1030,0,\N,Missing
I17-2065,P16-1101,0,0.110712,"ay be more easily acquired in the language and domain of interest. However, cross-lingual transfer on comparable corpora is more difficult than on parallel corpora, due to the difficulty in finding high-quality word translation equivalences. Our contributions are two-fold: First, we investigate cross-lingual transfer on an NER task, and found that pre-trained BWE’s do not necessarily help out-of-the-box. This corroborates results in the monolingual setting, where it is widely recognized that training task-specific embeddings is helpful for the downstream tasks like NER (Peng and Dredze, 2015; Ma and Hovy, 2016). Second, we propose a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011;"
I17-2065,N13-1006,0,0.022686,"Second, we propose a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Language Processing,"
I17-2065,D11-1006,0,0.0757057,"Missing"
I17-2065,P11-1061,0,0.0382041,"5; Ma and Hovy, 2016). Second, we propose a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Lan"
I17-2065,P05-1045,0,0.0214667,"ent Tuple #Sentence #Token (6) O Θ = {V0 }, where V0 is the context embedding with size d × v. In the implementation, we use negative sampling to save the computation, since we desire using a large vocabulary to handle as many words as possible for cross-lingual transfer. def 2.3 Data We use the EN-ZH portion of the Wikipedia Comparable Corpora4 . For experiment purposes, we sampled 19K document pairs5 as our comparable corpora C. The NER labeled data on English (S) is obtained by collecting the first paragraph of each English document in C as X (S) , and labeling it with Stanford NER tagger (Finkel et al., 2005) to generate Y (S) .6 For the NER test data in Chinese (T ), we separately sampled 1K documents and collected the first sentence as X (T ) . We ran automatic word segmentation7 and manually labeled X (T ) to generate Y (T ) . The English side of these 1K tuple is treated as held-out data for tuning NER hyperparameters, and is labeled with the same Stanford NER tagger. We use the BIO tagging scheme for 3 basic named-entity types (“LOC” for location, “ORG” for organization and “PER” for person), so the output space is 7 tags. The data statistics are shown in Table 1. The size of BWE’s is about 1"
I17-2065,D16-1135,0,0.0207661,"-task learning approach can help adapt bilingual word embeddings (BWE’s) to improve cross-lingual transfer. Joint training of BWE’s encourages the BWE’s to be taskspecific, and outperforms the baseline of using pre-trained BWE’s. We showed promising results on the challenging task of cross-lingual NER on comparable corpora, where the target language has no labels. Future work will aim to improve the absolute F1 scores by combining limited labels in the low-resource languages, via exploiting document structure in Wikipedia (Richman and Schone, 2008; Steinberger et al., 2011; Tsai et al., 2016; Ni and Florian, 2016; Pan et al., 2017). While we only focus on the most difficult case where the source language and target languages are not in the same family, and a bilingual dictionary is not available in this paper, it is interesting to study how this technique could be applied when the different levels of supervision are available on various language pairs in the future. negative sampling with 25 samples and subsampling rate of value 1e-4. The dropout rate of 0.3 is decided by the best F1 score of the language-specific NER tagger on the English held-out data. The coefficient α for balancing two Ln and Lm s"
I17-2065,P17-1178,0,0.0456547,"h can help adapt bilingual word embeddings (BWE’s) to improve cross-lingual transfer. Joint training of BWE’s encourages the BWE’s to be taskspecific, and outperforms the baseline of using pre-trained BWE’s. We showed promising results on the challenging task of cross-lingual NER on comparable corpora, where the target language has no labels. Future work will aim to improve the absolute F1 scores by combining limited labels in the low-resource languages, via exploiting document structure in Wikipedia (Richman and Schone, 2008; Steinberger et al., 2011; Tsai et al., 2016; Ni and Florian, 2016; Pan et al., 2017). While we only focus on the most difficult case where the source language and target languages are not in the same family, and a bilingual dictionary is not available in this paper, it is interesting to study how this technique could be applied when the different levels of supervision are available on various language pairs in the future. negative sampling with 25 samples and subsampling rate of value 1e-4. The dropout rate of 0.3 is decided by the best F1 score of the language-specific NER tagger on the English held-out data. The coefficient α for balancing two Ln and Lm should presumably be"
I17-2065,N15-1157,0,0.110401,"ord embeddings while optimizing a NER objective. This creates word embeddings that are both shared between languages and fine-tuned for the NER task. As a proof of concept, we demonstrate this model on English-toChinese transfer using Wikipedia. … ct 1 (T ) , cK NER Training Sentence y1 , y2 , · · · , yl x 1 , x2 , · · · , x l Figure 1: Our multi-task framework, which trains bilingual word embeddings from comparable corpora while optimizing an NER objective on the high-resource language. The NER part of the model is then tested on a low-resource language. Blunsom, 2014; Vulic and Moens, 2015; Gouws and Søgaard, 2015). A comparable corpus is a collection of document pairs written in different languages but talking about the same topic (e.g. interconnected Wikipedia articles). The advantage of comparable corpora is that they may be more easily acquired in the language and domain of interest. However, cross-lingual transfer on comparable corpora is more difficult than on parallel corpora, due to the difficulty in finding high-quality word translation equivalences. Our contributions are two-fold: First, we investigate cross-lingual transfer on an NER task, and found that pre-trained BWE’s do not necessarily h"
I17-2065,D15-1064,1,0.751953,"corpora is that they may be more easily acquired in the language and domain of interest. However, cross-lingual transfer on comparable corpora is more difficult than on parallel corpora, due to the difficulty in finding high-quality word translation equivalences. Our contributions are two-fold: First, we investigate cross-lingual transfer on an NER task, and found that pre-trained BWE’s do not necessarily help out-of-the-box. This corroborates results in the monolingual setting, where it is widely recognized that training task-specific embeddings is helpful for the downstream tasks like NER (Peng and Dredze, 2015; Ma and Hovy, 2016). Second, we propose a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; D"
I17-2065,P08-1001,0,0.037514,"when we replaced the uni4 Conclusion & Future Work We show how a multi-task learning approach can help adapt bilingual word embeddings (BWE’s) to improve cross-lingual transfer. Joint training of BWE’s encourages the BWE’s to be taskspecific, and outperforms the baseline of using pre-trained BWE’s. We showed promising results on the challenging task of cross-lingual NER on comparable corpora, where the target language has no labels. Future work will aim to improve the absolute F1 scores by combining limited labels in the low-resource languages, via exploiting document structure in Wikipedia (Richman and Schone, 2008; Steinberger et al., 2011; Tsai et al., 2016; Ni and Florian, 2016; Pan et al., 2017). While we only focus on the most difficult case where the source language and target languages are not in the same family, and a bilingual dictionary is not available in this paper, it is interesting to study how this technique could be applied when the different levels of supervision are available on various language pairs in the future. negative sampling with 25 samples and subsampling rate of value 1e-4. The dropout rate of 0.3 is decided by the best F1 score of the language-specific NER tagger on the Eng"
I17-2065,N16-1030,0,0.0586647,"n different languages into a single document (where S and T words are interspersed), then apply a standard monolingual word embedding algorithm. Evaluation : At test time, given X (T ) – the raw sentences of T , we evaluate the F1 score of Y¯ (T ) predicted by the trained model {V∗ , Λ∗ } against the true label Y (T ) . Note that this model is trained 1 Chinese can be considered a high-resource language for NER, but we use it as a proof-of-concept and do not use any existing Chinese resources. 2 Same word in different languages is treated separately. 3 Besides unigram, we also tried LSTM+CRF (Lample et al., 2016) for longer context. Despite good results in monolingual NER, it did poorly in our cross-lingual experiments. 384 Algorithm 1 Stochastic merging of two documents, where len(c) returns the number of tokens of document c, c[i] is the ith token of document c. Inspired by Lample et al. (2016), for both approaches, words with frequency 1 in the NER data are replaced by OOV with probability 0.5 to so that embedding OOV could be optimized. Input: Comparable Document: c(S) , c(T ) Output: Pseudo-bilingual Document: c 1: c ← []; i1 ← 0; i2 ← 0 len(c(S) ) len(c(S) )+len(c(T ) ) 3: while i1 < len(c(S) )"
I17-2065,R11-1015,0,0.0372364,"Missing"
I17-2065,N12-1052,0,0.0783082,"Missing"
I17-2065,K16-1022,0,0.177793,"We show how a multi-task learning approach can help adapt bilingual word embeddings (BWE’s) to improve cross-lingual transfer. Joint training of BWE’s encourages the BWE’s to be taskspecific, and outperforms the baseline of using pre-trained BWE’s. We showed promising results on the challenging task of cross-lingual NER on comparable corpora, where the target language has no labels. Future work will aim to improve the absolute F1 scores by combining limited labels in the low-resource languages, via exploiting document structure in Wikipedia (Richman and Schone, 2008; Steinberger et al., 2011; Tsai et al., 2016; Ni and Florian, 2016; Pan et al., 2017). While we only focus on the most difficult case where the source language and target languages are not in the same family, and a bilingual dictionary is not available in this paper, it is interesting to study how this technique could be applied when the different levels of supervision are available on various language pairs in the future. negative sampling with 25 samples and subsampling rate of value 1e-4. The dropout rate of 0.3 is decided by the best F1 score of the language-specific NER tagger on the English held-out data. The coefficient α for bal"
I17-2065,P15-2118,0,0.37076,"ntly trains bilingual word embeddings while optimizing a NER objective. This creates word embeddings that are both shared between languages and fine-tuned for the NER task. As a proof of concept, we demonstrate this model on English-toChinese transfer using Wikipedia. … ct 1 (T ) , cK NER Training Sentence y1 , y2 , · · · , yl x 1 , x2 , · · · , x l Figure 1: Our multi-task framework, which trains bilingual word embeddings from comparable corpora while optimizing an NER objective on the high-resource language. The NER part of the model is then tested on a low-resource language. Blunsom, 2014; Vulic and Moens, 2015; Gouws and Søgaard, 2015). A comparable corpus is a collection of document pairs written in different languages but talking about the same topic (e.g. interconnected Wikipedia articles). The advantage of comparable corpora is that they may be more easily acquired in the language and domain of interest. However, cross-lingual transfer on comparable corpora is more difficult than on parallel corpora, due to the difficulty in finding high-quality word translation equivalences. Our contributions are two-fold: First, we investigate cross-lingual transfer on an NER task, and found that pre-trained"
I17-2065,W14-1613,0,0.0209244,"ansfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 383–388, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP corpus, training a NER model from labeled English articles (high-resource) and testing it on Chinese articles (low-resource)1 . The chall"
I17-2065,C16-1045,0,0.071938,"e a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 383–388, c Taip"
I17-2065,P15-2064,0,0.0403858,"xperimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 383–388, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP corpus, training a NER model from labeled English articles (high-resource) and testing i"
I17-2065,H01-1035,0,\N,Missing
I17-3002,E17-2114,1,0.840887,"Missing"
I17-3002,P07-2045,1,0.0098071,"Missing"
J18-1006,P08-1084,0,0.0112536,"nty about θ by a prior distribution p(θ ). 3. Given data, apply Bayes theorem p(θ|x) ∝ p(x|θ )p(θ ) to find the posterior distribution for the quantities of interest. This approach enables an elegant and unified way to incorporate prior knowledge and manage uncertainty over parameters. It can also be used to provide capacity control for complex models as an alternative to smoothing. There have been many successful applications of Bayesian techniques in natural language processing (NLP). Some examples include: word segmentation (Goldwater et al. 2009), syntax (Johnson et al. 2007), morphology (Snyder & Barzilay 2008), coreference resolution (Haghighi & Klein 2007), and machine translation (Blunsom et al. 2009). Cohen’s book provides an accessible yet in-depth introduction to Bayesian techniques. It is aimed at a researcher or student who is already familiar with statistical modeling in natural language (i.e., at the level of introductory books such as Manning ¨ & Schutze [1999], Jurafsky & Martin [2009]). The stated goal of the book is to “cover the methods and algorithms that are needed to fluently read Bayesian learning papers in NLP and to do research in the area.” I believe Cohen successfully achieves"
K16-1030,C12-1163,0,0.0187967,"semantic lexicon from a training corpus. These works focus on the pragmatic use of language, where the informativeness and lexicon of an utterance largely depends on the context (e.g. ‘Red’ is not valid to be used to refer to a blue ball). In this work, we apply RSA to predict the usage of DCs, which is more universal across different contexts (i.e. A DC can be used or dropped given various discourse senses and contexts). Our model is built upon the speaker’s model of RSA to predict speaker’s choice of explicit or implicit DCs. 2.2 and lexicons or psycholinguistic experiments. More recently, Asr and Demberg (2012) presents an analysis of the PDTB, showing that ‘causal’ and ‘continuous’ senses are more often implicit, or marked by less specific DCs. Indeed these senses are presupposed by listeners according to linguistics theories (Segal et al., 1991; Murray, 1997; Levinson, 2000; Sanders, 2005; Kuperberg et al., 2011). On the other hand, Asr and Demberg (2015) finds that DCs are more often dropped for the discourse relation Chosen Alternative (the relation typically signalled by the DC ‘instead’), if the context contains negation words, which are identified cues for this relation. Similarly, contextual"
K16-1030,W13-2610,0,0.2146,"are reported in attempts to train implicit DC classifiers based on explicit DC instances (Sporleder and Lascarides, 2008; Webber, 2009). Asr and Demberg (2012; 2015) attribute the corpus statistics to the UID hypothesis, which explains that expected, predictable relations are more likely to be conveyed implicitly, and thus more ambiguously, to maintain steady information flow. However, there are explicit ‘causal’ and ‘continuous’ relations and some Chosen Alternative are marked even argument 1 is negated. Although markedness measures are proposed to rate the implicitness of a relation sense (Asr and Demberg, 2013; Jin and de Marneffe, 2015), these measures only quantify the general markedness of the sense in the data, but not the speaker’s choice for each particular instance. In contrast, this work specifically measures the predictability of a given relation; generalizes the approach to all discourse senses instead of particular senses or cues; and combines the markedness preference with other language production factors, in order to model each instance of relation. Patterson and Kehler (2013) is the only study we are aware of that predicts the choice of explicit or implicit DCs of each instance of re"
K16-1030,W98-1414,0,0.112878,"s. The predictor measures how easily a candidate utterance can be predicted and the speaker adjusts information density based on the expected predictability. UID is applied to explain a variety of speaker’s options, such as phonetic (Aylett and Turk, 2004), morphological (Frank and Jaeger, 2008) and syntactic (Jaeger, 2010) reductions, and also referring expressions (Tily and Piantadosi, 2009). 2.3 Explicit vs. Implicit DCs The choice of discourse marking strategies has been studied in earlier works as a subtask for natural language generation (Scott and de Souza, 1990; Moser and Moore, 1995; Grote and Stede, 1998; Soria and Ferrari, 1998; Allbritton and Moore, 1999). In the absence of large-scale resources, investigations are based on manually derived rules 3 This is opposite to ‘informativeness’ in RSA, which is defined by negative surprisal (Equation 4). 304 bedded or shared, the previous discourse relation, argument lengths, and content word ratios. The classifier is trained and tested on a subset of relations from the PDTB, after screening away infrequent senses and DCs. An overall high classification accuracy is achieved. Relation-level and discourse-level features are found to be more useful tha"
K16-1030,W15-0205,0,0.0375864,"Missing"
K16-1030,W15-0117,0,0.188885,"A DC can be used or dropped given various discourse senses and contexts). Our model is built upon the speaker’s model of RSA to predict speaker’s choice of explicit or implicit DCs. 2.2 and lexicons or psycholinguistic experiments. More recently, Asr and Demberg (2012) presents an analysis of the PDTB, showing that ‘causal’ and ‘continuous’ senses are more often implicit, or marked by less specific DCs. Indeed these senses are presupposed by listeners according to linguistics theories (Segal et al., 1991; Murray, 1997; Levinson, 2000; Sanders, 2005; Kuperberg et al., 2011). On the other hand, Asr and Demberg (2015) finds that DCs are more often dropped for the discourse relation Chosen Alternative (the relation typically signalled by the DC ‘instead’), if the context contains negation words, which are identified cues for this relation. Similarly, contextual difference in explicit and implicit discourse relations are reported in attempts to train implicit DC classifiers based on explicit DC instances (Sporleder and Lascarides, 2008; Webber, 2009). Asr and Demberg (2012; 2015) attribute the corpus statistics to the UID hypothesis, which explains that expected, predictable relations are more likely to be c"
K16-1030,W15-2505,0,0.0324473,"Missing"
K16-1030,D15-1132,0,0.0241696,"Missing"
K16-1030,P02-1026,0,0.0853376,"tic or textual factors. A classifier is trained to predict whether a candidate DC (i.e. the DC that actually occurs in the text as an explicit DC, or annotated as an implicit DC) is actually present, given the sense of the discourse relation and the arguments. Relatively shallow linguistic features are used, such as whether the relations are emUniform Information Density The UID principle views language communication as a form of information transmission through a noisy channel and a constant rate of information flow is optimal according to Shannon’s Information Theory (Levy and Jaeger, 2006; Genzel and Charniak, 2002; Shannon, 1948). It states that speakers structure utterances by optimizing information density, which is the quantity of information (measured by surprisal3 ) transmitted per unit of utterance, such as word. Information density rises when the utterance is ‘surprising’ and drops when an utterance is highly predictable. To smooth the peaks and troughs, speakers adjust the ambiguity of an utterance by including or reducing linguistic markers. Following the UID principle, linguistic choices made by speakers are predicted more accurately by incorporating an information density predictor on top of"
K16-1030,D09-1036,0,0.0303188,"features in the arguments that signal a particular sense makes the sense more predictable, and thus promote the reduction of a DC. For example, the DC ‘instead’ is less used to present the Chosen Alternative sense if the first argument is negated (Asr and Demberg, 2015). Generalizing this idea to capture various cues in the arguments for various senses, we approximate I(s; arg, C) by the confidence of an automatic discourse parser in predicting the discourse sense. An implicit relation parser uses various features in the arguments to identify the implicit relation sense (Pitler et al., 2009; Lin et al., 2009; Park and Cardi, 2012; Rutherford and Xue, 2014). If the arguments contain much informative features, the parser will predict the sense more confidently. We propose two methods, for comparison, to measure the confidence of the parser prediction. A confident prediction means the parser will assign a high probability to the one output sense. Therefore, we use the negative surprisal of the estimated probability Pp of the parser output sense soutput (Equation 14) to approximate I(s; arg, C). (13) and that s/he will use an implicit DC otherwise. 3.2 Informativeness of arguments Informativeness of"
K16-1030,prasad-etal-2008-penn,0,0.0877943,"Missing"
K16-1030,P14-5010,0,0.00470031,"ge word length of all DCs. A lexicon of possible DC per each discourse sense is derived from the whole corpus. For multi-word DCs, a white space is simply counted The range of values of the cost function depends on the cost definition. We thus adjust the values with a constant weight wc that is tuned on the dev set in the experiments: D(exp) = wc · cost(exp) 8 The implicit DC classifier is trained by Na¨ıve Bayes based on features including syntactic features, polarity, immediately preceding DC, and Brown cluster pairs. Syntactic features are based on automatic parsing using Stanford CoreNLP (Manning et al., 2014). The parser is trained on the same sections of the PDTB as the training set used in our experiment. 9 http://www.cs.brandeis.edu/˜clp/ conll15st/results.html 10 We use the parser’s probability estimates as is; conceivably it may be improved by an additional probabilistic calibration step (Nguyen and O’Connor, 2015). 4 (16) Experiment We apply the model to simulate speaker’s choice of explicit or implicit DC for discourse relations in the PDTB corpus. The aim of the experiment is to answer two questions: (1) Does the model explain the factors affecting speaker’s choice of DC markedness? If the"
K16-1030,J14-4007,0,0.0519489,"it wasn’t enough. (WSJ0097) 3. Before (Explicit; Temporal-AsynchronousPrecedence) becoming a consultant in 1974, Mr. Achenbaum was a senior executive at J. Walter Thompson Co..(WSJ0295) Explicit DCs are labelled with relation senses (Example 1). If an explicit DC is absent between two sentences within the same paragraph and an implicit relation can be inferred, a candidate DC and the relation sense are annotated (Example 2). Our model is based on the assumption that W = {explicit, implicit} for all relations, yet it is notable that intra-sentential implicit DCs are not annotated in the PDTB (Prasad et al., 2014). We thus exclude intra-sentential samples, such that W = {explicit, implicit} is always true and free of grammatical constraints. Also, as a result of the annotation procedure, implicit DCs always occur in between 2 arguments in their original order, i.e. Arg1-DC-Arg2. To preserve the original order of the discourse arguments, which is also part of the communicative structure intended by the speaker but out of the scope of this model, we only use samples in the Arg1-DC-Arg2 order. For example, Example (3) is excluded from our training data. Finally, annotations of other forms of discourse rel"
K16-1030,W13-3303,0,0.0584378,"Missing"
K16-1030,E14-1068,0,0.0127307,"particular sense makes the sense more predictable, and thus promote the reduction of a DC. For example, the DC ‘instead’ is less used to present the Chosen Alternative sense if the first argument is negated (Asr and Demberg, 2015). Generalizing this idea to capture various cues in the arguments for various senses, we approximate I(s; arg, C) by the confidence of an automatic discourse parser in predicting the discourse sense. An implicit relation parser uses various features in the arguments to identify the implicit relation sense (Pitler et al., 2009; Lin et al., 2009; Park and Cardi, 2012; Rutherford and Xue, 2014). If the arguments contain much informative features, the parser will predict the sense more confidently. We propose two methods, for comparison, to measure the confidence of the parser prediction. A confident prediction means the parser will assign a high probability to the one output sense. Therefore, we use the negative surprisal of the estimated probability Pp of the parser output sense soutput (Equation 14) to approximate I(s; arg, C). (13) and that s/he will use an implicit DC otherwise. 3.2 Informativeness of arguments Informativeness of DCs This section explains how we estimate the inf"
K16-1030,P95-1018,0,0.407035,"top of other constraints. The predictor measures how easily a candidate utterance can be predicted and the speaker adjusts information density based on the expected predictability. UID is applied to explain a variety of speaker’s options, such as phonetic (Aylett and Turk, 2004), morphological (Frank and Jaeger, 2008) and syntactic (Jaeger, 2010) reductions, and also referring expressions (Tily and Piantadosi, 2009). 2.3 Explicit vs. Implicit DCs The choice of discourse marking strategies has been studied in earlier works as a subtask for natural language generation (Scott and de Souza, 1990; Moser and Moore, 1995; Grote and Stede, 1998; Soria and Ferrari, 1998; Allbritton and Moore, 1999). In the absence of large-scale resources, investigations are based on manually derived rules 3 This is opposite to ‘informativeness’ in RSA, which is defined by negative surprisal (Equation 4). 304 bedded or shared, the previous discourse relation, argument lengths, and content word ratios. The classifier is trained and tested on a subset of relations from the PDTB, after screening away infrequent senses and DCs. An overall high classification accuracy is achieved. Relation-level and discourse-level features are foun"
K16-1030,D15-1182,0,0.0605133,"Missing"
K16-1030,P15-1158,0,0.0258334,"Missing"
K16-1030,W12-1614,0,0.0166023,"rguments that signal a particular sense makes the sense more predictable, and thus promote the reduction of a DC. For example, the DC ‘instead’ is less used to present the Chosen Alternative sense if the first argument is negated (Asr and Demberg, 2015). Generalizing this idea to capture various cues in the arguments for various senses, we approximate I(s; arg, C) by the confidence of an automatic discourse parser in predicting the discourse sense. An implicit relation parser uses various features in the arguments to identify the implicit relation sense (Pitler et al., 2009; Lin et al., 2009; Park and Cardi, 2012; Rutherford and Xue, 2014). If the arguments contain much informative features, the parser will predict the sense more confidently. We propose two methods, for comparison, to measure the confidence of the parser prediction. A confident prediction means the parser will assign a high probability to the one output sense. Therefore, we use the negative surprisal of the estimated probability Pp of the parser output sense soutput (Equation 14) to approximate I(s; arg, C). (13) and that s/he will use an implicit DC otherwise. 3.2 Informativeness of arguments Informativeness of DCs This section expla"
K16-1030,D13-1094,0,0.0797956,"even argument 1 is negated. Although markedness measures are proposed to rate the implicitness of a relation sense (Asr and Demberg, 2013; Jin and de Marneffe, 2015), these measures only quantify the general markedness of the sense in the data, but not the speaker’s choice for each particular instance. In contrast, this work specifically measures the predictability of a given relation; generalizes the approach to all discourse senses instead of particular senses or cues; and combines the markedness preference with other language production factors, in order to model each instance of relation. Patterson and Kehler (2013) is the only study we are aware of that predicts the choice of explicit or implicit DCs of each instance of relation. They argue that while the decision is related to the ease to infer the relation, it may also depend on other stylistic or textual factors. A classifier is trained to predict whether a candidate DC (i.e. the DC that actually occurs in the text as an explicit DC, or annotated as an implicit DC) is actually present, given the sense of the discourse relation and the arguments. Relatively shallow linguistic features are used, such as whether the relations are emUniform Information D"
K16-1030,W98-0306,0,0.17836,"es how easily a candidate utterance can be predicted and the speaker adjusts information density based on the expected predictability. UID is applied to explain a variety of speaker’s options, such as phonetic (Aylett and Turk, 2004), morphological (Frank and Jaeger, 2008) and syntactic (Jaeger, 2010) reductions, and also referring expressions (Tily and Piantadosi, 2009). 2.3 Explicit vs. Implicit DCs The choice of discourse marking strategies has been studied in earlier works as a subtask for natural language generation (Scott and de Souza, 1990; Moser and Moore, 1995; Grote and Stede, 1998; Soria and Ferrari, 1998; Allbritton and Moore, 1999). In the absence of large-scale resources, investigations are based on manually derived rules 3 This is opposite to ‘informativeness’ in RSA, which is defined by negative surprisal (Equation 4). 304 bedded or shared, the previous discourse relation, argument lengths, and content word ratios. The classifier is trained and tested on a subset of relations from the PDTB, after screening away infrequent senses and DCs. An overall high classification accuracy is achieved. Relation-level and discourse-level features are found to be more useful than argument-level features"
K16-1030,C08-2022,0,0.0318175,"s stateof-the-art approaches, while giving an explanatory account of the speaker’s choice. 1 1. It was a great movie, but I did not like it. 2. It was a great movie, therefore I liked it. 3. It was a great movie. I liked it. The word ‘but’ indicates a Concession relation in Example (1), and ‘therefore’ indicates a Result relation in Example (2). We call ‘but’ and ‘therefore’ explicit discourse connectives (DCs). In Example (3), DCs are absent but a Result relation can be inferred. We say the DC is implicit in this case. Explicit DCs are highly informative cues to identify discourse relations (Pitler et al., 2008) while implicit DCs are more ambiguous. For example, ‘I liked it’ can also be read as a Justification for the first sentence in Example (3). Marking a discourse relation or not is subject to ambiguity and redundancy. On one hand, using an explicit DC avoids ambiguity. For example, if the DC ‘but’ is omitted in Example (1), readers may have problems in inferring the Concession sense. On the other hand, if the intended discourse sense is highly predictable, it is verbose or redundant to insert an explicit DC in the utterance, such as the DC ‘therefore’ in Example (2). Introduction Speakers or au"
K16-1030,P09-1077,0,0.0298011,"referred. Presence of features in the arguments that signal a particular sense makes the sense more predictable, and thus promote the reduction of a DC. For example, the DC ‘instead’ is less used to present the Chosen Alternative sense if the first argument is negated (Asr and Demberg, 2015). Generalizing this idea to capture various cues in the arguments for various senses, we approximate I(s; arg, C) by the confidence of an automatic discourse parser in predicting the discourse sense. An implicit relation parser uses various features in the arguments to identify the implicit relation sense (Pitler et al., 2009; Lin et al., 2009; Park and Cardi, 2012; Rutherford and Xue, 2014). If the arguments contain much informative features, the parser will predict the sense more confidently. We propose two methods, for comparison, to measure the confidence of the parser prediction. A confident prediction means the parser will assign a high probability to the one output sense. Therefore, we use the negative surprisal of the estimated probability Pp of the parser output sense soutput (Equation 14) to approximate I(s; arg, C). (13) and that s/he will use an implicit DC otherwise. 3.2 Informativeness of arguments I"
K16-1030,tonelli-etal-2010-annotation,0,0.028255,"p forward to formalize the idea of the UID theory, that redundant explicit markers are avoided if the discourse relation is clear enough from the context. As future work, we plan to improve the markedness model by making fuller use of the training data, such as learning a more expressive formulation of the context governing the choice of explicit or implicit DCs. We also plan to evaluate the effectiveness of the model in applications, such as natural language generation or machine translation tasks. On the other hand, as discourse presentation differs across genres (Webber, 2009) and mediums (Tonelli et al., 2010), the model can be applied to predict the explicitation of discourse relations from, for example, news articles to spoken dialogues. Another direction is to apply the RSA framework in the opposite direction - to build a listener’s model that simulates a listener’s recognition of a discourse sense given an utterance, as proposed in Yung et al.(2016). Acknowledgments We thank the anonymous reviewers for their valuable feedback on the previous versions of this paper. References 14 When extracting the argument informativeness features from the training set, using the automatic discourse parser, we"
K16-1030,K15-2002,0,0.0283782,"Missing"
K16-1030,P09-1076,0,0.0982275,"steners according to linguistics theories (Segal et al., 1991; Murray, 1997; Levinson, 2000; Sanders, 2005; Kuperberg et al., 2011). On the other hand, Asr and Demberg (2015) finds that DCs are more often dropped for the discourse relation Chosen Alternative (the relation typically signalled by the DC ‘instead’), if the context contains negation words, which are identified cues for this relation. Similarly, contextual difference in explicit and implicit discourse relations are reported in attempts to train implicit DC classifiers based on explicit DC instances (Sporleder and Lascarides, 2008; Webber, 2009). Asr and Demberg (2012; 2015) attribute the corpus statistics to the UID hypothesis, which explains that expected, predictable relations are more likely to be conveyed implicitly, and thus more ambiguously, to maintain steady information flow. However, there are explicit ‘causal’ and ‘continuous’ relations and some Chosen Alternative are marked even argument 1 is negated. Although markedness measures are proposed to rate the implicitness of a relation sense (Asr and Demberg, 2013; Jin and de Marneffe, 2015), these measures only quantify the general markedness of the sense in the data, but not"
K16-1030,W15-2519,1,0.866134,"Missing"
K16-1030,P16-2086,1,0.876576,"Missing"
N06-1021,J93-2004,0,0.0328332,"Missing"
N06-1021,P05-1012,0,0.143593,"sidering the deployment of a new language for machine translation is whether the natural language components available are of sufficient quality to warrant the effort to integrate them into the machine translation system. It is not feasible in every instance to do the integration work first and then to evaluate the output. Table 1 summarizes the data used to train the parsers, giving the number of tokens (excluding traces and other empty elements) and counts of sentences.1 3 Parser Architecture We take as our starting point a re-implementation of McDonald’s state-of-the-art dependency parser (McDonald et al., 2005a). Given a sentence x, the goal of the parser is to find the highest-scoring parse yˆ among all possible parses y ∈ Y : yˆ = arg max s(x, y) y∈Y (1) 1 The files in each partition of the Chinese and Arabic data are given at http://research.microsoft.com/˜simonco/ HLTNAACL2006. Language Arabic Chinese Czech English Total Tokens 116,695 527,242 1,595,247 1,083,159 Training Sentences 2,100 14,735 73,088 39,832 Development Sentences 446 1,961 7,319 1,346 Blind Sentences 449 2,080 7,507 2,416 Table 1: Summary of data used to train parsers. For a given parse y, its score is the sum of the scores of"
N06-1021,P05-1034,0,0.0365991,"Missing"
N06-1021,W02-1001,0,0.556283,"arning by presenting the input samples in some sequential order. For large training set sizes, a batch learner may face computational difficulties since there already exists an exponential number of parses per input sentence. Online learning is more tractable since it works with one input at a time. A popular online learner is the perceptron. It adjusts w by updating it with the feature vector whenever a misclassification on the current input sample occurs. It has been shown that such updates converge in a finite number of iterations if the data is linearly separable. The averaged perceptron (Collins, 2002) is a variant which averages the w across all iterations; it has demonstrated good generalization especially with data that is not linearly separable, as in many natural language processing problems. 2 The Chu-Liu-Edmonds’ decoder, which is based on a maximal spanning tree algorithm, can run in O(N 2 ), but our simpler implementation of O(N 3 ) was sufficient. Recently, the good generalization properties of Support Vector Machines have prompted researchers to develop large margin methods for the online setting. Examples include the margin perceptron (Duda et al., 2001), ALMA (Gentile, 2001), a"
N06-1021,C96-1058,0,0.169288,"3) a feature representation f (i, j). Two decoders will be discussed here; the training algorithm and feature representation are discussed in the following sections. A good decoder should satisfy several properties: ideally, it should be able to search through all valid parses of a sentence and compute the parse scores efficiently. Efficiency is a significant issue since there are usually an exponential number of parses for any given sentence, and the discriminative training methods we will describe later require repeated decoding at each training iteration. We reimplemented Eisner’s decoder (Eisner, 1996), which searches among all projective parse trees, and the Chu-Liu-Edmonds’ decoder (Chu and Liu, 1965; Edmonds, 1967), which searches in the space of both projective and non-projective parses. (A projective tree is a parse with no crossing dependency links.) For the English and Chinese data, the headfinding rules for converting from Penn Treebank analyses to dependency analyses creates trees that are guaranteed to be projective, so Eisner’s algorithm suffices. For the Czech and Arabic corpora, a non-projective decoder is necessary. Both algorithms are O(N 3 ), where N is the number of words 1"
N06-1021,N03-1033,0,0.0160933,"Missing"
N06-1021,W05-1516,0,0.0335081,"r Chinese are the first published results for the dependency parsing of the Chinese Treebank 5.0.4 Since the Arabic and Chinese numbers are well short of the numbers for Czech and English, we attempted to determine what impact the smaller corpora used for training the Arabic and Chinese parsers might have. We performed data reduction experiments, training the parsers on five random samples at each size smaller than the entire training set. Figure 2 shows the dependency accuracy measured on the complete development test set when training with samples of the data. The graph shows the average 4 (Wang et al., 2005) report numbers for undirected dependencies on the Chinese Treebank 3.0. We cannot meaningfully compare those numbers to the numbers here. Language English (exc punc) Czech (inc punc) Algorithm Avg. Perceptron MIRA Bayes Point Machine Avg. Perceptron MIRA Bayes Point Machine DA 90.6 90.9 90.8 82.9 83.3 84.0 RA 94.0 94.2 93.7 88.0 88.6 88.8 CM 36.5 37.5 37.6 30.3 31.3 30.9 Table 3: Comparison to previous best published results reported in (McDonald et al., 2005a). Bayes Point Machine Best averaged perceptron Worst averaged perceptron Arabic 78.4 77.9 77.4 Chinese 83.8 83.1 82.6 Czech 84.5 83.5"
N06-1021,W03-3023,0,0.478632,"Missing"
N06-1021,2005.iwslt-1.12,0,\N,Missing
N13-1115,D11-1031,0,0.0571522,"Missing"
N13-1115,aziz-etal-2012-pet,0,0.0328302,"Missing"
N13-1115,N12-1062,0,0.181062,"imizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 20"
N13-1115,W11-2103,0,0.0418762,"hing a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al"
N13-1115,N10-1080,0,0.492006,"to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et"
N13-1115,P12-1098,0,0.0365651,"Missing"
N13-1115,N12-1047,0,0.069524,"Missing"
N13-1115,D08-1024,0,0.0752244,"oduce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capt"
N13-1115,J07-2003,0,0.0354992,"on that prefers the locally best component. These are simpler to implement and also performed competitively in their domain adaptation experiments. Unless explicitly noted otherwise, the results presented in Section 6 are based on linear mixture operation log-wsum, which empirically performed better than the log-wmax for ensemble tuning. 6 Experiments We evaluate the different methods on ArabicEnglish translation in single as well as multiple references scenario. Corpus statistics are shown in Table 1. For all the experiments in this paper, we use Kriya, our in-house Hierarchical phrasebased (Chiang, 2007) (Hiero) system, and integrated the required changes for ensemble decoding. Kriya performs comparably to the state of the art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets (Sankaran et al., 2012). We use PRO (Hopkins and May, 2011) for optimizing the feature weights and PMO-PRO (Duh et al., 2012) for optimizing meta weights, wherever applicable. In both cases, we use SVMRank (Joachims, 2006) as the optimizer. We used the default parameter settings for different MT tuning metrics. For METEOR, we tried both METEOR-tune and METEOR-ht"
N13-1115,W11-2107,0,0.0297673,"was marginally better in METEOR3 and RIBES scores. We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-hter setting for both optimization and evaluation of all our experiments. 6.1 Evaluation on Tuning Set Unlike conventional tuning methods, PMO (Duh et al., 2012) was originally evaluated on the tuning set to avoid potential mismatch with the test set. In order to ensure robustness of evaluation, they redecode the devset using the optimal weights from the last tuning iteration and report the scores on 13 This behaviour was also noted by Denkowski and Lavie (2011) in their analysis of Urdu-English system for tunable metrics task in WMT11. 952 best candidates. Corpus ISI corpus Training size 1.1 M Tuning/ test set 1664/ 1313 (MTA) 1982/ 987 (ISI) Table 1: Corpus Statistics (# of sentences) for Arabic-English. MTA (4-refs) and ISI (1-ref). We follow the same strategy and compare our PMO-ensemble approach with PMO-PRO (denoted P) and a linear combination4 (denoted L) baseline. Similar to Duh et al. (2012), we use five different BLEU:RIBES weight settings, viz. (0.0, 1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and (1.0, 0.0), marked L1 through L5 or P1 throug"
N13-1115,N12-1059,0,0.0551629,"Missing"
N13-1115,P12-1001,1,0.900409,"l., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et al., 2012) the choice of which option to use rests with the MT system developer and in that sense their approach is an a posteriori method to specify the preference (Marler and Arora, 2004). In contrast to this, our tuning framework provides a principled way of using the Pareto optimal options using ensemble decoding (Razmara et al., 2012). We also introduce a novel method of ensemble tuning for jointly tuning multiple MT evaluation metrics and further combin"
N13-1115,W09-0426,0,0.329084,"Missing"
N13-1115,N12-1023,0,0.115129,"Missing"
N13-1115,D11-1138,0,0.0566372,"Missing"
N13-1115,P12-1031,0,0.119299,"Missing"
N13-1115,2009.mtsummit-posters.8,0,0.106993,"Missing"
N13-1115,D11-1125,0,0.214609,"imizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in tran"
N13-1115,D11-1035,0,0.119177,"Missing"
N13-1115,mauser-etal-2008-automatic,0,0.0487964,"Missing"
N13-1115,P03-1021,0,0.0645367,"is contrasts the traditional tuning where gains are usually limited to a single metric. Our human evaluation results confirm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging a"
N13-1115,P02-1040,0,0.102374,"ross several metrics (BLEU, RIBES), without any significant reduction in other metrics. This contrasts the traditional tuning where gains are usually limited to a single metric. Our human evaluation results confirm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other"
N13-1115,P12-1099,1,0.916893,"was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et al., 2012) the choice of which option to use rests with the MT system developer and in that sense their approach is an a posteriori method to specify the preference (Marler and Arora, 2004). In contrast to this, our tuning framework provides a principled way of using the Pareto optimal options using ensemble decoding (Razmara et al., 2012). We also introduce a novel method of ensemble tuning for jointly tuning multiple MT evaluation metrics and further combine this with the PMO ap947 Proceedings of NAACL-HLT 2013, pages 947–957, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics proach (Duh et al., 2012). We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now. Our ensemble tuning met"
N13-1115,P12-1002,0,0.121243,"Missing"
N13-1115,2006.amta-papers.25,0,0.571545,"., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tu"
N13-1115,D11-1117,0,0.167625,", 2012). We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now. Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan,"
N13-1115,D07-1080,0,0.0904751,"irm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover"
N13-1115,N12-1026,0,0.381172,"uning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic"
N13-1115,D10-1092,1,\N,Missing
N13-1115,2012.eamt-1.31,0,\N,Missing
N13-1115,2012.tc-1.5,0,\N,Missing
N15-1033,C10-3010,0,0.0134665,"n and Hoang, 2007) is also an example where an LM over a second 6 Results for model score, a more direct measure of search errors, were largely similar. stream of factors (for example POS tags, classes, or lemmas) has been shown to increase accuracy. These factors are limited, however, by the strong constraint of being associated with a single word and not allowing reordering, and thus are not applicable to our setting of using multiple languages. There has also been work on using multiple languages to improve the quality of extracted translation lexicons or topic models (Mausam et al., 2009; Baldwin et al., 2010; Mimno et al., 2009). These are not concerned with multi-target translation, but may provide us with useful hints about how to generate more effective multi-target translation models. 8 Conclusion In this paper, we have proposed a method for multitarget translation using a generalization of SCFGs, and proposed methods to learn and perform search over the models. In experiments, we found that these models are effective in the case when a strong LM exists in a second target that is highly related to the first target of interest. As the overall framework of multi-target translation is broad-reac"
N15-1033,D07-1090,0,0.0358374,"ld not decide which is correct. However, if they were additionally given English T2 translations corresponding to each of the Chinese translations, they could easily choose the third as the most natural, even without knowing a word of Chinese. Translating this into MT terminology, this is equivalent to generating two corresponding target sentences E1 and E2 , and using the naturalness of E2 to help decide which E1 to generate. Language models (LMs) are the traditional tool for assessing the naturalness of sentences, and it is widely known that larger and stronger LMs greatly help translation (Brants et al., 2007). It is easy to think of a situation where we can only create a weak LM for T1, but much more easily create a strong LM for T2. For example, T1 could be an under-resourced language, or a new entrant to the EU or UN. As a concrete method to realize multi-target translation, we build upon Chiang (2007)’s framework of synchronous context free grammars (SCFGs), which we first overview in Section 2.2 SCFGs are an extension of context-free grammars that define rules that synchronously generate source and target strings F and E. We expand this to a new formalism of multi-synchronous CFGs (MSCFGs, Sec"
N15-1033,2012.eamt-1.60,0,0.014986,"pair, from the source F to the target E. However, in many actual translation situations, identical documents are translated not from one language to another, but between a large number of different languages. Examples of this abound in commercial translation, and prominent open data sets used widely by the MT community include UN documents in 6 languages (Eisele and Chen, 2010), European Parliament Proceedings in 21 languages 1 Code and data to replicate the experiments can be found at http://phontron.com/project/naacl2015 (Koehn, 2005), and video subtitles on TED in as many as 50 languages (Cettolo et al., 2012). However, despite this abundance of multilingual data, there have been few attempts to take advantage of it. One exception is the multi-source SMT method of Och and Ney (2001), which assumes a situation where we have multiple source sentences, and would like to combine the translations from these sentences to create a better, single target translation. In this paper, we propose a framework of multitarget SMT. In multi-target translation, we translate F to not a single target E, but to a set of sentences E = hE1 , E2 , . . . , E|E |i in multiple target languages (which we will abbreviate T1, T"
N15-1033,J07-2003,0,0.631893,"rating two corresponding target sentences E1 and E2 , and using the naturalness of E2 to help decide which E1 to generate. Language models (LMs) are the traditional tool for assessing the naturalness of sentences, and it is widely known that larger and stronger LMs greatly help translation (Brants et al., 2007). It is easy to think of a situation where we can only create a weak LM for T1, but much more easily create a strong LM for T2. For example, T1 could be an under-resourced language, or a new entrant to the EU or UN. As a concrete method to realize multi-target translation, we build upon Chiang (2007)’s framework of synchronous context free grammars (SCFGs), which we first overview in Section 2.2 SCFGs are an extension of context-free grammars that define rules that synchronously generate source and target strings F and E. We expand this to a new formalism of multi-synchronous CFGs (MSCFGs, Section 3) that simultaneously generate not just two, but an arbitrary number of strings hF, E1 , E2 , . . . , EN i. We describe how to acquire these from data (Section 4), and how to perform search, including calculation of LM probabilities over multiple target language strings (Section 5). To evaluate"
N15-1033,P11-2031,0,0.0115844,"oder, we use the Travatar (Neubig, 2013) toolkit, and implement all necessary extensions to the decoder and rule extraction code to allow for multiple targets. Unless otherwise specified, we use joint search with a pop limit of 2,000, and T1 rule pruning with a limit of 10 rules per source rule. BLEU is used for both tuning and evaluating all models. In particular, we tune and evaluate all models based on T1 BLEU, simulating a situation similar to that in the introduction, where we want to use a large LM in T2 to help translation in T1. In order to control for optimizer instability, we follow Clark et al. (2011)’s recommendation of performing tuning 3 times, and reporting the average of the runs along with statistical significance obtained by pairwise bootstrap resampling (Koehn, 2004). 6.2 Main Experimental Results In this section we first perform experiments to investigate the effectiveness of the overall framework of multi-target translation. We assess four models, starting with standard single-target SCFGs and moving gradually towards our full MSCFG model: SCFG: A standard SCFG grammar with only the source and T1. SCFG+T2Al: SCFG constrained during rule extraction to only extract rules that also"
N15-1033,eisele-chen-2010-multiun,0,0.19766,"ulti-target translation, where a second target language is used to assess the quality of the first target language. Introduction In statistical machine translation (SMT), the great majority of work focuses on translation of a single language pair, from the source F to the target E. However, in many actual translation situations, identical documents are translated not from one language to another, but between a large number of different languages. Examples of this abound in commercial translation, and prominent open data sets used widely by the MT community include UN documents in 6 languages (Eisele and Chen, 2010), European Parliament Proceedings in 21 languages 1 Code and data to replicate the experiments can be found at http://phontron.com/project/naacl2015 (Koehn, 2005), and video subtitles on TED in as many as 50 languages (Cettolo et al., 2012). However, despite this abundance of multilingual data, there have been few attempts to take advantage of it. One exception is the multi-source SMT method of Och and Ney (2001), which assumes a situation where we have multiple source sentences, and would like to combine the translations from these sentences to create a better, single target translation. In t"
N15-1033,D07-1091,0,0.0259752,"n for future work is search algorithms that can combine the advantages of these two approaches. 7 Related Work While there is very little previous work on multitarget translation, there is one line of work by Gonz´alez and Casacuberta (2006) and P´erez et al. (2007), which adapts a WFST-based model to output multiple targets. However, this purely monotonic method is unable to perform non-local reordering, and thus is not applicable most language pairs. It is also motivated by efficiency concerns, as opposed to this work’s objective of learning from a T2 language. Factored machine translation (Koehn and Hoang, 2007) is also an example where an LM over a second 6 Results for model score, a more direct measure of search errors, were largely similar. stream of factors (for example POS tags, classes, or lemmas) has been shown to increase accuracy. These factors are limited, however, by the strong constraint of being associated with a single word and not allowing reordering, and thus are not applicable to our setting of using multiple languages. There has also been work on using multiple languages to improve the quality of extracted translation lexicons or topic models (Mausam et al., 2009; Baldwin et al., 20"
N15-1033,N03-1017,0,0.00891142,"of having a strong T2 LM to help with T1 translation, we perform experiments on translation of United Nations documents (Section 6). These experiments, and our subsequent analysis, show that the framework of multi-target translation can, indeed, provide significant gains in accuracy (of up to 1.5 BLEU points), particularly when the two target languages in question are similar. 2 Synchronous Context-Free Grammars We first briefly cover SCFGs, which are widely used in MT, most notably in the framework of hierarchi2 One could also consider a multi-target formulation of phrase-based translation (Koehn et al., 2003), but generating multiple targets while considering reordering in phrase-based search is not trivial. We leave this to future work. 294 (a) SCFG Grammar r1: X → <X1 of the X2, X1 des X2> r2: X → <activity, activités> r3: X → <chambers, chambres> <X1, X1> r1 Derivation <X2 of the X3, X2 des X3> r2 <activity of the X3, activités des X3> r3 <activity of the chambers, activités des chambres> (b) MSCFG Grammar r1: X → <X1 of the X2, X1 des X2, X2 的 X1> r2: X → <activity, activités, 活动 > r3: X → <chambers, chambres, 分庭 > <X1, X1, X1> r1 Derivation <X2 of the X3, X2 des X3, X3 的 X2> r2 <activity of t"
N15-1033,W04-3250,0,0.0404655,"ed, we use joint search with a pop limit of 2,000, and T1 rule pruning with a limit of 10 rules per source rule. BLEU is used for both tuning and evaluating all models. In particular, we tune and evaluate all models based on T1 BLEU, simulating a situation similar to that in the introduction, where we want to use a large LM in T2 to help translation in T1. In order to control for optimizer instability, we follow Clark et al. (2011)’s recommendation of performing tuning 3 times, and reporting the average of the runs along with statistical significance obtained by pairwise bootstrap resampling (Koehn, 2004). 6.2 Main Experimental Results In this section we first perform experiments to investigate the effectiveness of the overall framework of multi-target translation. We assess four models, starting with standard single-target SCFGs and moving gradually towards our full MSCFG model: SCFG: A standard SCFG grammar with only the source and T1. SCFG+T2Al: SCFG constrained during rule extraction to only extract rules that also match the T2 alignments. This will help measure the effect, if any, of being limited by T2 alignments in rule extraction. MSCFG-T2LM: The MSCFG, without using the T2 LM. Compare"
N15-1033,2005.mtsummit-papers.11,0,0.0103944,"he great majority of work focuses on translation of a single language pair, from the source F to the target E. However, in many actual translation situations, identical documents are translated not from one language to another, but between a large number of different languages. Examples of this abound in commercial translation, and prominent open data sets used widely by the MT community include UN documents in 6 languages (Eisele and Chen, 2010), European Parliament Proceedings in 21 languages 1 Code and data to replicate the experiments can be found at http://phontron.com/project/naacl2015 (Koehn, 2005), and video subtitles on TED in as many as 50 languages (Cettolo et al., 2012). However, despite this abundance of multilingual data, there have been few attempts to take advantage of it. One exception is the multi-source SMT method of Och and Ney (2001), which assumes a situation where we have multiple source sentences, and would like to combine the translations from these sentences to create a better, single target translation. In this paper, we propose a framework of multitarget SMT. In multi-target translation, we translate F to not a single target E, but to a set of sentences E = hE1 , E2"
N15-1033,R09-1040,0,0.0448042,"Missing"
N15-1033,P09-1030,0,0.0128469,"ine translation (Koehn and Hoang, 2007) is also an example where an LM over a second 6 Results for model score, a more direct measure of search errors, were largely similar. stream of factors (for example POS tags, classes, or lemmas) has been shown to increase accuracy. These factors are limited, however, by the strong constraint of being associated with a single word and not allowing reordering, and thus are not applicable to our setting of using multiple languages. There has also been work on using multiple languages to improve the quality of extracted translation lexicons or topic models (Mausam et al., 2009; Baldwin et al., 2010; Mimno et al., 2009). These are not concerned with multi-target translation, but may provide us with useful hints about how to generate more effective multi-target translation models. 8 Conclusion In this paper, we have proposed a method for multitarget translation using a generalization of SCFGs, and proposed methods to learn and perform search over the models. In experiments, we found that these models are effective in the case when a strong LM exists in a second target that is highly related to the first target of interest. As the overall framework of multi-target tra"
N15-1033,P04-1084,0,0.0432955,"ciency, and limit the number of terminals to limit model size (in our experiments, we set this limit to five). 4 4.2 X → hγ, α1 , ..., αN i. (4) Training Multi-Synchronous Grammars This section describes how, given a set of parallel sentences in N languages, we can create translation models (TMs) using MSCFGs. 4 We will also make the restriction that indices are linear and non-deleting, indicating that each non-terminal index present in any of the strings will appear exactly once in all of the strings. Thus, MSCFGs can also be thought of as a subset of the “generalized multi-text grammars” of Melamed et al. (2004). 295 MSCFG Rule Extraction In this section, we generalize the rule extraction process in the previous section to accommodate multiple targets. We do so by first independently creating alignments between the source corpus F, and each of N target corpora {E1 , . . . , EN }. Given a particular sentence we now have source F , N target strings {E1 , . . . , EN }, and N alignments {A1 , . . . , AN }. We next independently extract initial phrases for each of the N languages using the standard bilingual phrase-extract algorithm, yielding initial phrase sets {BP1 , . . . , BPN }. Finally, we convert t"
N15-1033,D09-1092,0,0.0299287,"also an example where an LM over a second 6 Results for model score, a more direct measure of search errors, were largely similar. stream of factors (for example POS tags, classes, or lemmas) has been shown to increase accuracy. These factors are limited, however, by the strong constraint of being associated with a single word and not allowing reordering, and thus are not applicable to our setting of using multiple languages. There has also been work on using multiple languages to improve the quality of extracted translation lexicons or topic models (Mausam et al., 2009; Baldwin et al., 2010; Mimno et al., 2009). These are not concerned with multi-target translation, but may provide us with useful hints about how to generate more effective multi-target translation models. 8 Conclusion In this paper, we have proposed a method for multitarget translation using a generalization of SCFGs, and proposed methods to learn and perform search over the models. In experiments, we found that these models are effective in the case when a strong LM exists in a second target that is highly related to the first target of interest. As the overall framework of multi-target translation is broad-reaching, there are a sti"
N15-1033,P13-4016,1,0.739364,"milarity. We use English as our source sentence in all cases, as it is the most common actual source language for UN documents. To prepare the data, we first deduplicate the sentences in the corpus, then hold out 1,500 sentences each for tuning and test. In our basic training setup, we use 100k sentences for training both the TM and the T1 LM. This somewhat small number is to simulate a T1 language that has relatively few resources. For the T2 language, we assume we have a large language model trained on all of the UN data, amounting to 3.5M sentences total. As a decoder, we use the Travatar (Neubig, 2013) toolkit, and implement all necessary extensions to the decoder and rule extraction code to allow for multiple targets. Unless otherwise specified, we use joint search with a pop limit of 2,000, and T1 rule pruning with a limit of 10 rules per source rule. BLEU is used for both tuning and evaluating all models. In particular, we tune and evaluate all models based on T1 BLEU, simulating a situation similar to that in the introduction, where we want to use a large LM in T2 to help translation in T1. In order to control for optimizer instability, we follow Clark et al. (2011)’s recommendation of"
N15-1033,2001.mtsummit-papers.46,0,0.587206,"number of different languages. Examples of this abound in commercial translation, and prominent open data sets used widely by the MT community include UN documents in 6 languages (Eisele and Chen, 2010), European Parliament Proceedings in 21 languages 1 Code and data to replicate the experiments can be found at http://phontron.com/project/naacl2015 (Koehn, 2005), and video subtitles on TED in as many as 50 languages (Cettolo et al., 2012). However, despite this abundance of multilingual data, there have been few attempts to take advantage of it. One exception is the multi-source SMT method of Och and Ney (2001), which assumes a situation where we have multiple source sentences, and would like to combine the translations from these sentences to create a better, single target translation. In this paper, we propose a framework of multitarget SMT. In multi-target translation, we translate F to not a single target E, but to a set of sentences E = hE1 , E2 , . . . , E|E |i in multiple target languages (which we will abbreviate T1, T2, etc.). This, in a way, can be viewed as the automated version of the multi-lingual dissemination of content performed by human translators when creating data for the UN, Eur"
N15-1033,J03-1002,0,0.00528918,"erminal and non-terminal symbols.4 In this paper, for notational convenience, we will use a specialized version of Equation 3 in which we define a single γ as the source side string, and α1 , ...αN as an arbitrary number N of target side strings: 4.1 SCFG Rule Extraction First, we briefly outline rule extraction for SCFGs in the standard two-language case, as proposed by Chiang (2007). We first start by preparing two corpora in the source and target language, F and E, and obtaining word alignments for each sentence automatically, using a technique such as the IBM models implemented by GIZA++ (Och and Ney, 2003). We then extract initial phrases for each sentence. Given a source f1J , target eI1 , and alignment A = {hi1 , i0 1 i, . . . , hi|A |, i0 |A |i} where i and i0 represent indices of aligned words in F and E respectively. First, based on this alignment, we extract all pairs of j0 j0 j |BP | |BP | phrases BP = {hfij11 , ei0 11 i, . . . , hfi|BP , ei0 |BP i}, | | where fij11 is a substring of f1J spanning from i1 to j0 j1 , and ei0 11 is analogous for the target side. The 0 Therefore, at each derivation step, one non-terminal in γ is chosen and all the nonterminals with same indices in α1 , ...,"
N15-1033,W07-0708,0,0.0133987,"Missing"
N15-1033,P12-1001,1,\N,Missing
N15-1092,P14-1023,0,0.0130009,"milar concepts; e.g., feature generation, dimensionality reduction, and vector space models. The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics. Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Sch¨olkopf et al., 1998). Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014). Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised o"
N15-1092,D14-1082,0,0.226541,"odong Liu†∗, Jianfeng Gao‡ , Xiaodong He‡ , Li Deng‡ , Kevin Duh† and Ye-yi Wang‡ Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192, Japan ‡ Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA xiaodong-l@is.naist.jp, {jfgao,xiaohe,deng}@microsoft.com kevinduh@is.naist.jp, yeyiwang@microsoft.com † Abstract representations learned from large corpora. Further, since these representations are usually in a lowdimensional vector space, they result in more compact models than those built from surface-form features. A recent successful example is the parser by (Chen and Manning, 2014), which is not only accurate but also fast. Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect"
N15-1092,P14-1066,1,0.29944,"n. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 201"
N15-1092,D14-1002,1,0.736534,"n. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 201"
N15-1092,D14-1070,0,0.0132056,"Missing"
N15-1092,P14-1062,0,0.00783704,"yyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 2014b), parse tree structure (Irsoy and Cardie, 2014), and run-time inference (Le and Mikolov, 2014). The synergy between multi-task learning and neural nets is quite natural; the general idea dates back to (Caruana, 1997). The main challenge is in designing the tasks and the network structure. For example, (Collobert et al., 2011) defined part-of-speech tagging, chunking, and named entity recognition as multiple tasks in a single sequence labeler; (Bordes et al., 2012) defined multiple data sources as tasks in their relation extraction system. While conceptual"
N15-1092,N13-1090,0,0.587375,"g large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations uni"
N15-1092,D14-1079,0,0.0296496,"feature generation, dimensionality reduction, and vector space models. The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics. Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Sch¨olkopf et al., 1998). Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014). Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing a"
N15-1092,D14-1162,0,0.118051,"s-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks. Introd"
N15-1092,D13-1170,0,0.00800131,"tions to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks. Introduction Recent advances in deep neural networks (DNNs) have demonstrated the importance of learn"
N15-1092,P10-1040,0,0.0115602,"et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other meth"
N15-1092,2014.lilt-9.5,0,\N,Missing
N18-2073,I08-6014,0,0.0964496,"Missing"
N18-2073,P14-2080,1,0.726088,"rge-Scale CLIR Dataset Table 1: CLIR dataset statistics. For each language X, we show the total number of documents in language X and the number of English queries. The number of ”most relevant” documents is by definition equal to #Query. The number of ”slightly relevant” documents is shown in the column #SR. We construct a large-scale CLIR data from Wikipedia. The idea is to exploit inter-language links: from an English page, we extract a sentence as query, and label the linked foreign-document pages as relevant. See Figure 1 for an illustration. This data construction process is similar to (Schamoni et al., 2014) who made an EnglishGerman CLIR dataset, but ours is at a larger scale. Specifically, we use Wikipedia dumps released on August 23, 2017. English queries are obtained by extracting the first sentence of every English Wikipedia article. The intuition is that the first sentence is usually a well-defined summary of its corresponding article and should be thematically related for articles linked to it from another language. Similar to (Schamoni et al., 2014), title words from the query sentences are removed, because they may be present across different language editions. This deletion prevents the"
N18-2073,D13-1175,0,0.254848,"Missing"
N19-1189,P07-2045,0,0.0156253,"for patent. Unlabeled-domain Data For additional unlabeled-domain data, we use web-crawled bitext from the Paracrawl project.4 We filter the data using the Zipporah cleaning tool (Xu and Koehn, 2017), with a threshold score of 1. After filtering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English. Using different data selection methods, we include up to the 4096k and 2048k sentence-pairs for our German and Russian experiments, respectively. Data Preprocessing All datasets are tokenized using the Moses (Koehn et al., 2007) tokenizer. We learn byte pair encoding (BPE) segmentation 2 https://github.com/kevinduh/ sockeye-recipes/tree/master/egs/ curriculum 3 Appendix A explains our choice of 15k in detail. 4 https://www.paracrawl.eu/ models (Sennrich et al., 2016) from general domain data. The BPE models are trained separately for each language, and the number of BPE symbols is set to 30k. We then apply the BPE models to in-domain and Paracrawl data, so that the parameters of the generic model can be applied as an initialization for continued training. Once we have a converged generic NMT model, which is very expe"
N19-1189,W17-3204,0,0.029107,"o the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs. 1 Domain Data Generic Model In-Domain Data Continued Training Initialization Domain Specific Model Unlabeled-Domain Data Figure 1: Workflow of our domain adaptation system. Introduction Neural machine translation (NMT) performance often drops when training and test domains do not match and when in-domain training data is scarce (Koehn and Knowles, 2017). Tailoring the NMT system to each domain could improve performance, but unfortunately high-quality parallel data does not exist for all domains. Domain adaptation techniques address this problem by exploiting diverse data sources to improve indomain translation, including general domain data that does not match the domain of interest, and unlabeled domain data whose domain is unknown (e.g. webcrawl like Paracrawl). One approach to exploit unlabeled-domain bitext is to apply data selection techniques (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) to find bitext that are simila"
N19-1189,N19-1208,1,0.732218,"Missing"
N19-1189,L16-1147,0,0.0208309,"ange from adding it to the already selected subset of N . The selected sentence is the one which most decreases Hn , the cross-entropy between previously selected n-sentence corpus and I. Curriculum Learning Training Strategy 1 https://github.com/kevinduh/ sockeye-recipes/tree/master/egs/ curriculum 1904 3 Experiments and Results We evaluate on four domain adaptation tasks. The code base is provided to ensure reproducibility.2 3.1 Data and Setup General Domain Data We have two general domain datasets, Russian-English (ru) and German-English (de). Both are a concatenation of OpenSubtitles2018 (Lison and Tiedemann, 2016) and WMT 2017 (Bojar et al., 2017), which contains data from several domains, e.g. parliamentary proceedings (Europarl, UN Parallel Corpus), political/economic news (news commentary, Rapid corpus), and web-crawled parallel corpus (Common Crawl, Yandex, Wikipedia titles). We performed sentence length filtering (up to 80 words) after tokenization, ending up with 28 million sentence pairs for German and 51 million sentence pairs for Russian. In-domain Data We evaluate our proposed methods on two distinct domains per language pair: • TED talks: data-split from Duh (2018). • Patents: from the World"
N19-1189,D15-1166,0,0.0533833,"by exploiting diverse data sources to improve indomain translation, including general domain data that does not match the domain of interest, and unlabeled domain data whose domain is unknown (e.g. webcrawl like Paracrawl). One approach to exploit unlabeled-domain bitext is to apply data selection techniques (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) to find bitext that are similar to in-domain data. This selected data can additionally be combined with in-domain bitext and trained in a continued training framework, as shown in Figure 1. Continued training or fine-tuning (Luong et al., 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017) is an adaptation technique where a model is first trained on the large general domain data, then used as initialization of a new model which is further trained on in-domain bitext. In our framework, the selected samples are concatenated with in-domain data, then used for continued training. This effectively increases the in-domain training size with “pseudo” in-domain samples, and is helpful in continued training (Koehn et al., 2018). A challenge with employing data selection in continued training is that there exists no clear-cut way to define"
N19-1189,2012.iwslt-papers.7,0,0.0213726,"les and less similar ones are adjusted dynamically during the training of NMT models based on the curriculum training strategy. In NMT, instance weighting is usually implemented by modifying the objective function (Chen and Huang, 2016; Wang et al., 2017; Chen et al., 2017). In statistical machine translation, Matsoukas et al. (2009) extract features from sentences to capture their domains and then use a classifier to map features to sentence weights. Foster et al. extend this method by weighting at the level of phrase pairs. Shah et al. (2010) use resampling to weight corpora and alignments. Mansour and Ney (2012) focus on sentence-level weighting for phrase extraction. Zhou et al. (2015) weight examples based on their word distributions. For model-centric domain adaptation methods, our work is related to van der Wees et al. (2017). They adopt gradual fine-tuning, which does the opposite of our method: training starts from the whole dataset, and the training set gradually decreases by removing less similar sentences. Wang et al. (2018) use a similar approach, where the NMT model is trained on progressively noisereduced data batches. However, such schedules have the risk of wasting computation on non-re"
N19-1189,D09-1074,0,0.0379615,"c domain adaptation methods, our curriculum learning approach has connections to instance weighting. In our work, the presentation of certain examples at specific training phases is equivalent to up-weighting those examples and down-weight the others at that time. Weights of similar samples and less similar ones are adjusted dynamically during the training of NMT models based on the curriculum training strategy. In NMT, instance weighting is usually implemented by modifying the objective function (Chen and Huang, 2016; Wang et al., 2017; Chen et al., 2017). In statistical machine translation, Matsoukas et al. (2009) extract features from sentences to capture their domains and then use a classifier to map features to sentence weights. Foster et al. extend this method by weighting at the level of phrase pairs. Shah et al. (2010) use resampling to weight corpora and alignments. Mansour and Ney (2012) focus on sentence-level weighting for phrase extraction. Zhou et al. (2015) weight examples based on their word distributions. For model-centric domain adaptation methods, our work is related to van der Wees et al. (2017). They adopt gradual fine-tuning, which does the opposite of our method: training starts fr"
N19-1189,P10-2041,0,0.891363,"est domains do not match and when in-domain training data is scarce (Koehn and Knowles, 2017). Tailoring the NMT system to each domain could improve performance, but unfortunately high-quality parallel data does not exist for all domains. Domain adaptation techniques address this problem by exploiting diverse data sources to improve indomain translation, including general domain data that does not match the domain of interest, and unlabeled domain data whose domain is unknown (e.g. webcrawl like Paracrawl). One approach to exploit unlabeled-domain bitext is to apply data selection techniques (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) to find bitext that are similar to in-domain data. This selected data can additionally be combined with in-domain bitext and trained in a continued training framework, as shown in Figure 1. Continued training or fine-tuning (Luong et al., 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017) is an adaptation technique where a model is first trained on the large general domain data, then used as initialization of a new model which is further trained on in-domain bitext. In our framework, the selected samples are concatenated with in-domain data, then us"
N19-1189,N19-1119,0,0.0486853,"ests that CDS dominates ML for distant domains such as Patent, while ML can do slightly better than CDS for domains that are not that distant such as TED. 5 Related Work Curriculum learning has shown its potential to improve sample efficiency for neural models (Graves et al., 2017; Weinshall and Cohen, 2018) by guiding the order of presented samples, usually from easier-to-learn samples to difficult samples. Although there is no single criterion to measure difficulty for general neural machine translation tasks (Kocmi and Bojar, 2017; Wang et al., 2018; Zhang et al., 2018; Kumar et al., 2019; Platanios et al., 2019), for the domain adaptation scenario, we measure difficulty based on the distance from indomain data. Compared to previous work, our application of curriculum learning mainly focuses on improvements on translation quality without consideration of convergence speed. 12 In Figure 8, for the purpose of fair comparison, each distribution is defined on the same vocabulary, consisting of the source side vocabulary of TED, patent and Paracrawl data. Chu and Wang (2018) surveyed recent domain adaptation methods for NMT. In their taxonomy, our workflow in Figure 1 can be considered a hybrid that uses b"
N19-1189,P16-1162,0,0.142635,"ering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English. Using different data selection methods, we include up to the 4096k and 2048k sentence-pairs for our German and Russian experiments, respectively. Data Preprocessing All datasets are tokenized using the Moses (Koehn et al., 2007) tokenizer. We learn byte pair encoding (BPE) segmentation 2 https://github.com/kevinduh/ sockeye-recipes/tree/master/egs/ curriculum 3 Appendix A explains our choice of 15k in detail. 4 https://www.paracrawl.eu/ models (Sennrich et al., 2016) from general domain data. The BPE models are trained separately for each language, and the number of BPE symbols is set to 30k. We then apply the BPE models to in-domain and Paracrawl data, so that the parameters of the generic model can be applied as an initialization for continued training. Once we have a converged generic NMT model, which is very expensive to train, we can adapt it to different domains, without building up a new vocabulary and retraining the model. NMT Setup Our NMT models are developed in Sockeye5 (Hieber et al., 2017). The generic model and continued training model are t"
N19-1189,W10-1759,0,0.0296606,"les and down-weight the others at that time. Weights of similar samples and less similar ones are adjusted dynamically during the training of NMT models based on the curriculum training strategy. In NMT, instance weighting is usually implemented by modifying the objective function (Chen and Huang, 2016; Wang et al., 2017; Chen et al., 2017). In statistical machine translation, Matsoukas et al. (2009) extract features from sentences to capture their domains and then use a classifier to map features to sentence weights. Foster et al. extend this method by weighting at the level of phrase pairs. Shah et al. (2010) use resampling to weight corpora and alignments. Mansour and Ney (2012) focus on sentence-level weighting for phrase extraction. Zhou et al. (2015) weight examples based on their word distributions. For model-centric domain adaptation methods, our work is related to van der Wees et al. (2017). They adopt gradual fine-tuning, which does the opposite of our method: training starts from the whole dataset, and the training set gradually decreases by removing less similar sentences. Wang et al. (2018) use a similar approach, where the NMT model is trained on progressively noisereduced data batches"
N19-1189,N19-1209,1,0.789476,"till used, so the model will not forget what it just learned. (3) It builds on a strong continued training baseline, which continues on in-domain data. (4) The method implements best practices that have shown to be helpful in NMT, e.g. bucketing, mini-batching, and data shuffling. For future work, it would be interesting to measure how curriculum learning models perform on the general domain test set (rather than the indomain test set we focus on in this work); do they suffer more or less from catastrophic forgetting (Goodfellow et al., 2014; Kirkpatrick et al., 2017; Khayrallah et al., 2018; Thompson et al., 2019)? Acknowledgments This work is supported in part by a AWS Machine Learning Research Award and a grant from the Office of the Director of National Intelligence, Intelligence Advanced Research Projects Activity (IARPA), via contract FA8650-17-C-9115. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors. We thank the organizers and participants of the 2018 Machine Translation Marathon for providing a productive environment to start this project. We also than"
N19-1189,D17-1155,0,0.0543658,"a modified training procedure based for continued training. For data-centric domain adaptation methods, our curriculum learning approach has connections to instance weighting. In our work, the presentation of certain examples at specific training phases is equivalent to up-weighting those examples and down-weight the others at that time. Weights of similar samples and less similar ones are adjusted dynamically during the training of NMT models based on the curriculum training strategy. In NMT, instance weighting is usually implemented by modifying the objective function (Chen and Huang, 2016; Wang et al., 2017; Chen et al., 2017). In statistical machine translation, Matsoukas et al. (2009) extract features from sentences to capture their domains and then use a classifier to map features to sentence weights. Foster et al. extend this method by weighting at the level of phrase pairs. Shah et al. (2010) use resampling to weight corpora and alignments. Mansour and Ney (2012) focus on sentence-level weighting for phrase extraction. Zhou et al. (2015) weight examples based on their word distributions. For model-centric domain adaptation methods, our work is related to van der Wees et al. (2017). They ado"
N19-1189,W18-6314,0,0.289006,"distant from the Paracrawl data than TED is. Figure 2 suggests that CDS dominates ML for distant domains such as Patent, while ML can do slightly better than CDS for domains that are not that distant such as TED. 5 Related Work Curriculum learning has shown its potential to improve sample efficiency for neural models (Graves et al., 2017; Weinshall and Cohen, 2018) by guiding the order of presented samples, usually from easier-to-learn samples to difficult samples. Although there is no single criterion to measure difficulty for general neural machine translation tasks (Kocmi and Bojar, 2017; Wang et al., 2018; Zhang et al., 2018; Kumar et al., 2019; Platanios et al., 2019), for the domain adaptation scenario, we measure difficulty based on the distance from indomain data. Compared to previous work, our application of curriculum learning mainly focuses on improvements on translation quality without consideration of convergence speed. 12 In Figure 8, for the purpose of fair comparison, each distribution is defined on the same vocabulary, consisting of the source side vocabulary of TED, patent and Paracrawl data. Chu and Wang (2018) surveyed recent domain adaptation methods for NMT. In their taxonomy"
N19-1189,D17-1147,0,0.148955,"Missing"
N19-1189,D17-1319,0,0.0153486,"sian. In-domain Data We evaluate our proposed methods on two distinct domains per language pair: • TED talks: data-split from Duh (2018). • Patents: from the World International Property Organization COPPA-V2 dataset (Junczys-Dowmunt et al., 2016). We randomly sample 15k parallel sentences from the original corpora as our in-domain bitext.3 We also have around 2k sentences of development and test data for TED and 3k for patent. Unlabeled-domain Data For additional unlabeled-domain data, we use web-crawled bitext from the Paracrawl project.4 We filter the data using the Zipporah cleaning tool (Xu and Koehn, 2017), with a threshold score of 1. After filtering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English. Using different data selection methods, we include up to the 4096k and 2048k sentence-pairs for our German and Russian experiments, respectively. Data Preprocessing All datasets are tokenized using the Moses (Koehn et al., 2007) tokenizer. We learn byte pair encoding (BPE) segmentation 2 https://github.com/kevinduh/ sockeye-recipes/tree/master/egs/ curriculum 3 Appendix A explains our choice of 15k in det"
N19-1209,P17-2061,0,0.0516862,"June 7, 2019. 2019 Association for Computational Linguistics 2 Related Work A few prior studies address the drop in generaldomain NMT performance during continued training. Freitag and Al-Onaizan (2016) found that ensembling general- and in-domain models provides most of the in-domain gain from continued training while retaining most of the generaldomain performance. Ensembling doubles memory and computational requirements at translation time, which may be impractical for some applications and does not address our more fundamental goal of building a single model that is robust across domains. Chu et al. (2017) found that mixing general-domain data with the in-domain data used for continued training improved generaldomain performance of the resulting models, at the expense of training time. Dakwale and Monz (2017) share our goal of improving the general-domain performance of continued training. They introduce two novel approaches which use the initial, general-domain model to supervise the in-domain model during continued training. The first, multi-objective fine-tuning, which they denote MCL, trains the network with a joint objective of standard loglikelihood loss plus a second term based on knowle"
N19-1209,W18-2705,1,0.841876,"the general-domain model. The second, multiple-output layer fine tuning, adds new parameters to the output layer during continued training that are specific to the new domain. They found both methods performed similarly, significantly outperforming ensembling in the more challenging case where domain shift is significant, so we select the simpler MCL as our baseline. We do not assume that the domain of input sentences is known, thus we do not compare to methods such as LHUC (Vilar, 2018). Our work applies a regularization term to continued training, similar to Miceli Barone et al. (2017) and Khayrallah et al. (2018), but for the purpose of retaining generaldomain performance as opposed to improving indomain performance. 3 to-sequence models with large vocabularies. Instead we propose to approximate it with the diagonal of the empirical Fisher (Martens, 2014), which can be computed efficiently using gradients from back-propagation. At a high level, our method works as follows: 1. Train on the general-domain data, resulting in parameters θˆG . 2. Compute the diagonal of the empirical Fisher matrix F¯ . F¯i,i estimates how important the ith parameter θˆiG is to the general-domain translation task. 3. Initia"
N19-1209,D16-1139,0,0.0470448,"with the in-domain data used for continued training improved generaldomain performance of the resulting models, at the expense of training time. Dakwale and Monz (2017) share our goal of improving the general-domain performance of continued training. They introduce two novel approaches which use the initial, general-domain model to supervise the in-domain model during continued training. The first, multi-objective fine-tuning, which they denote MCL, trains the network with a joint objective of standard loglikelihood loss plus a second term based on knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) of the general-domain model. The second, multiple-output layer fine tuning, adds new parameters to the output layer during continued training that are specific to the new domain. They found both methods performed similarly, significantly outperforming ensembling in the more challenging case where domain shift is significant, so we select the simpler MCL as our baseline. We do not assume that the domain of input sentences is known, thus we do not compare to methods such as LHUC (Vilar, 2018). Our work applies a regularization term to continued training, similar to Miceli Barone et al. (2017) a"
N19-1209,P07-2045,1,0.0158311,"Model Continued Training MCL 25 40 20 20 15 General-Domain BLEU EWC (this work) 30 15 20 En→De 10 En→Ru En→Zh 10 10 5 25 30 35 40 20 25 30 35 10 20 30 40 30 30 40 25 30 20 20 De→En 10 20 Ru→En 15 10 10 35 40 45 50 Zh→En 30 25 35 40 10 20 30 40 In-Domain BLEU Figure 1: Performance trade-off for MCL and EWC: Convex hull of grid search over learning rate and regularization amount. x-axis is in-domain BLEU and y-axis is general-domain BLEU, so the desired operating point is the top right corner. Initial general-domain model (GD) and continued training (CT) points are shown for comparison. enizer (Koehn et al., 2007) and byte-pair encoding (BPE) (Sennrich et al., 2016). We train separate BPE models for the source and target languages, each with a vocabulary size of approximately 30k. BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets. Token counts for corpora are shown in Table 1. We implemented5 both EWC and MCL in Sockeye (Hieber et al., 2017). To avoid floating point issues, we normalize the empirical Fisher diagonal to have a mean value of 1.0 instead of dividing by the number of sentences. For effi"
N19-1209,W17-3204,1,0.881107,"erformance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)—a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading indomain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable. 1 Introduction Neural Machine Translation (NMT) performs poorly without large training corpora (Koehn and Knowles, 2017). Domain adaptation is required when there is sufficient data in the desired language pair but insufficient data in the desired domain (the topic, genre, style or level of formality). This work focuses on the supervised domain adaptation problem where a small in-domain parallel corpus is available for training. Continued training (Luong and Manning, 2015; Sennrich et al., 2015) (also called fine-tuning), where a model is first trained on general-domain data and then domain adapted by training on in-domain data, is a popular approach in this setting as it leads to empirical improvements in the"
N19-1209,E17-3017,0,0.0341829,"U, so the desired operating point is the top right corner. Initial general-domain model (GD) and continued training (CT) points are shown for comparison. enizer (Koehn et al., 2007) and byte-pair encoding (BPE) (Sennrich et al., 2016). We train separate BPE models for the source and target languages, each with a vocabulary size of approximately 30k. BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets. Token counts for corpora are shown in Table 1. We implemented5 both EWC and MCL in Sockeye (Hieber et al., 2017). To avoid floating point issues, we normalize the empirical Fisher diagonal to have a mean value of 1.0 instead of dividing by the number of sentences. For efficiency, we compute gradients for a batch of sentences prior to squaring and accumulating them. Fisher regularization is implemented as weight decay (towards θˆG ) in Adam (Kingma and Ba, 2014). Preliminary experiments in Ru→En found no meaningful difference in general-domain or indomain performance when computing the diagonal of F¯ on varying amounts of data ranging from 500k sentences to the full dataset. We also tried computing the d"
N19-1209,L18-1275,0,0.0213701,"Where LSN LL (θ) is the standard NLL loss on DS and λ is a hyper-parameter which weights the importance of the general-domain task. Note that the left-hand side of Equation 3 is still the loss over both the general- and in-domain translation tasks, but the right-hand side is based only on indomain data. All information from the generaldomain data has been collapsed into the second term, which is in the form of a regularizer. 4 Experiments Our general-domain training data is the concatenation of the parallel portions of the WMT17 news translation task (Bojar et al., 2017) and OpenSubtitles18 (Lison et al., 2018) corpora. For De↔En and Ru↔En, we use newstest2017 and the final 2500 lines of OpenSubtitles as our test set. We use newstest2016 and the penultimate 2500 lines of OpenSubtitles as the development set. For Zh↔En, we use the final and penultimate 4000 lines of the UN portion of the WMT data and the final and penultimate 2500 lines of OpenSubtitles as our test and development sets, respectively. We use the World Intellectual Property Organization (WIPO) COPPA-V2 corpus (JunczysDowmunt et al., 2016) as our in-domain dataset. The WIPO data consist of parallel sentences from international patent ap"
N19-1209,2015.iwslt-evaluation.11,0,0.172589,"e previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable. 1 Introduction Neural Machine Translation (NMT) performs poorly without large training corpora (Koehn and Knowles, 2017). Domain adaptation is required when there is sufficient data in the desired language pair but insufficient data in the desired domain (the topic, genre, style or level of formality). This work focuses on the supervised domain adaptation problem where a small in-domain parallel corpus is available for training. Continued training (Luong and Manning, 2015; Sennrich et al., 2015) (also called fine-tuning), where a model is first trained on general-domain data and then domain adapted by training on in-domain data, is a popular approach in this setting as it leads to empirical improvements in the targeted domain. One downside of continued training is that the adapted model’s ability to translate generaldomain sentences is severely degraded during adaptation (Freitag and Al-Onaizan, 2016). We interpret this drop in general-domain performance as catastrophic forgetting (Goodfellow et al., 2013) of general-domain translation knowledge. Degradation o"
N19-1209,D17-1156,0,0.141688,"5; Kim and Rush, 2016) of the general-domain model. The second, multiple-output layer fine tuning, adds new parameters to the output layer during continued training that are specific to the new domain. They found both methods performed similarly, significantly outperforming ensembling in the more challenging case where domain shift is significant, so we select the simpler MCL as our baseline. We do not assume that the domain of input sentences is known, thus we do not compare to methods such as LHUC (Vilar, 2018). Our work applies a regularization term to continued training, similar to Miceli Barone et al. (2017) and Khayrallah et al. (2018), but for the purpose of retaining generaldomain performance as opposed to improving indomain performance. 3 to-sequence models with large vocabularies. Instead we propose to approximate it with the diagonal of the empirical Fisher (Martens, 2014), which can be computed efficiently using gradients from back-propagation. At a high level, our method works as follows: 1. Train on the general-domain data, resulting in parameters θˆG . 2. Compute the diagonal of the empirical Fisher matrix F¯ . F¯i,i estimates how important the ith parameter θˆiG is to the general-domai"
N19-1209,P16-1009,0,0.167347,"Missing"
N19-1209,P16-1162,0,0.179336,"l-Domain BLEU EWC (this work) 30 15 20 En→De 10 En→Ru En→Zh 10 10 5 25 30 35 40 20 25 30 35 10 20 30 40 30 30 40 25 30 20 20 De→En 10 20 Ru→En 15 10 10 35 40 45 50 Zh→En 30 25 35 40 10 20 30 40 In-Domain BLEU Figure 1: Performance trade-off for MCL and EWC: Convex hull of grid search over learning rate and regularization amount. x-axis is in-domain BLEU and y-axis is general-domain BLEU, so the desired operating point is the top right corner. Initial general-domain model (GD) and continued training (CT) points are shown for comparison. enizer (Koehn et al., 2007) and byte-pair encoding (BPE) (Sennrich et al., 2016). We train separate BPE models for the source and target languages, each with a vocabulary size of approximately 30k. BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets. Token counts for corpora are shown in Table 1. We implemented5 both EWC and MCL in Sockeye (Hieber et al., 2017). To avoid floating point issues, we normalize the empirical Fisher diagonal to have a mean value of 1.0 instead of dividing by the number of sentences. For efficiency, we compute gradients for a batch of sentences"
N19-1209,W18-6313,1,0.869182,"tles as our test set. We use newstest2016 and the penultimate 2500 lines of OpenSubtitles as the development set. For Zh↔En, we use the final and penultimate 4000 lines of the UN portion of the WMT data and the final and penultimate 2500 lines of OpenSubtitles as our test and development sets, respectively. We use the World Intellectual Property Organization (WIPO) COPPA-V2 corpus (JunczysDowmunt et al., 2016) as our in-domain dataset. The WIPO data consist of parallel sentences from international patent application abstracts. WIPO De↔En data are large enough to train strong indomain systems (Thompson et al., 2018), so we truncate to 100k lines to simulate a more interesting domain adaptation scenario. We reserve 3000 lines each for in-domain development and test sets. We apply the Moses tok2064 General-Domain Model Continued Training MCL 25 40 20 20 15 General-Domain BLEU EWC (this work) 30 15 20 En→De 10 En→Ru En→Zh 10 10 5 25 30 35 40 20 25 30 35 10 20 30 40 30 30 40 25 30 20 20 De→En 10 20 Ru→En 15 10 10 35 40 45 50 Zh→En 30 25 35 40 10 20 30 40 In-Domain BLEU Figure 1: Performance trade-off for MCL and EWC: Convex hull of grid search over learning rate and regularization amount. x-axis is in-domain"
N19-1209,N18-2080,0,0.053336,"dard loglikelihood loss plus a second term based on knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) of the general-domain model. The second, multiple-output layer fine tuning, adds new parameters to the output layer during continued training that are specific to the new domain. They found both methods performed similarly, significantly outperforming ensembling in the more challenging case where domain shift is significant, so we select the simpler MCL as our baseline. We do not assume that the domain of input sentences is known, thus we do not compare to methods such as LHUC (Vilar, 2018). Our work applies a regularization term to continued training, similar to Miceli Barone et al. (2017) and Khayrallah et al. (2018), but for the purpose of retaining generaldomain performance as opposed to improving indomain performance. 3 to-sequence models with large vocabularies. Instead we propose to approximate it with the diagonal of the empirical Fisher (Martens, 2014), which can be computed efficiently using gradients from back-propagation. At a high level, our method works as follows: 1. Train on the general-domain data, resulting in parameters θˆG . 2. Compute the diagonal of the emp"
P05-2004,W01-0701,0,0.0174528,"Workshop, pages 19–24, c Ann Arbor, Michigan, June 2005. 2005 Association for Computational Linguistics variables (e.g. tags, chunks). Then we define the FHMM as the probabilistic model: p(x1:T , y1:T , z1:T ) = π0 T Y (1) p(xt |yt , zt )p(yt |yt−1 , zt )p(zt |zt−1 ) t=2 Figure 1: Baseline FHMM. The two hidden sequences y1:t and z1:t can represent tags and chunks, respectively. Together they generate x 1:t , the observed word sequence. al. (2000) uses a POS tagger to output an N-best list of tags, then a Viterbi search to find the chunk sequence that maximizes the joint tag/chunk probability. Florian and Ngai (2001) extends transformationbased learning tagger to a joint tagger/chunker by modifying the objective function such that a transformation rule is evaluated on the classification of all simultaneous subtasks. Our work is most similar in spirit to Dynamic Conditional Random Fields (DCRF) (Sutton et al., 2004), which also models tagging and chunking in a factorial framework. Some main differences between our model and DCRF may be described as 1) directed graphical model vs. undirected graphical model, and 2) generative model vs. conditional model. The main advantage of FHMM over DCRF is that FHMM req"
P05-2004,N01-1025,0,0.0844344,"Missing"
P05-2004,J93-2004,0,0.0276077,"Missing"
P05-2004,W95-0107,0,0.0759145,"Missing"
P05-2004,W96-0213,0,0.590238,"Missing"
P05-2004,W00-0726,0,0.10142,"Missing"
P05-2004,N03-1028,0,0.106843,"Missing"
P05-2004,N03-1033,0,0.0932046,"Missing"
P05-2004,P00-1015,0,0.0585599,"Missing"
P05-2004,W04-3242,0,\N,Missing
P05-2004,N03-2002,0,\N,Missing
P05-2004,J95-4004,0,\N,Missing
P05-2004,A00-1031,0,\N,Missing
P08-2010,J05-1003,0,0.027915,"when selecting the number of boosting iterations based on Dev. Last column indicates absolute improvements. BoostedMERT outperforms MERT by 0.8 points on Eval. 4 Related Work Various methods are used to optimize log-linear models in re-ranking (Shen et al., 2004; Venugopal et al., 2005; Smith and Eisner, 2006). Although this line of work is worthwhile, we believe more gain is possible if we go beyond log-linear models. For example, Shen’s method (2004) produces largemargins but observed little gains in performance. Our BoostedMERT should not be confused with other boosting algorithms such as (Collins and Koo, 2005; Kudo et al., 2005). These algorithms are called boosting because they iteratively choose features (weak learners) and optimize the weights for the boost/exponential loss. They do not, however, maintain a distribution over N-best lists. The idea of maintaining a distribution over Nbest lists is novel. To the best of our knowledge, the most similar algorithm is AdaRank (Xu and Li, 2007), developed for document ranking in information retrieval. Our main difference lies in Lines 4-7 in Algorithm 1: AdaRank proposes a simple closed form solution for α and combines only weak features, not full lea"
P08-2010,P05-1024,0,0.0287612,"er of boosting iterations based on Dev. Last column indicates absolute improvements. BoostedMERT outperforms MERT by 0.8 points on Eval. 4 Related Work Various methods are used to optimize log-linear models in re-ranking (Shen et al., 2004; Venugopal et al., 2005; Smith and Eisner, 2006). Although this line of work is worthwhile, we believe more gain is possible if we go beyond log-linear models. For example, Shen’s method (2004) produces largemargins but observed little gains in performance. Our BoostedMERT should not be confused with other boosting algorithms such as (Collins and Koo, 2005; Kudo et al., 2005). These algorithms are called boosting because they iteratively choose features (weak learners) and optimize the weights for the boost/exponential loss. They do not, however, maintain a distribution over N-best lists. The idea of maintaining a distribution over Nbest lists is novel. To the best of our knowledge, the most similar algorithm is AdaRank (Xu and Li, 2007), developed for document ranking in information retrieval. Our main difference lies in Lines 4-7 in Algorithm 1: AdaRank proposes a simple closed form solution for α and combines only weak features, not full learners (as in MERT)."
P08-2010,N04-1021,0,0.110436,"Missing"
P08-2010,P03-1021,0,0.213836,"Missing"
P08-2010,N04-1023,0,0.175171,"estigate this in future work. Train, Best BLEU Dev, Best BLEU Eval, Best BLEU Eval, Selected BLEU MERT 40.3 24.0 41.2 41.2 BOOST 41.0 25.0 43.7 42.0 ∆ 0.7 1.0 2.5 0.8 5 Table 1: The first three rows show the BLEU score for Train, Dev, and Eval from 30 iterations of BoostedMERT or 30 random re-restarts of MERT. The last row shows the actual BLEU on Eval when selecting the number of boosting iterations based on Dev. Last column indicates absolute improvements. BoostedMERT outperforms MERT by 0.8 points on Eval. 4 Related Work Various methods are used to optimize log-linear models in re-ranking (Shen et al., 2004; Venugopal et al., 2005; Smith and Eisner, 2006). Although this line of work is worthwhile, we believe more gain is possible if we go beyond log-linear models. For example, Shen’s method (2004) produces largemargins but observed little gains in performance. Our BoostedMERT should not be confused with other boosting algorithms such as (Collins and Koo, 2005; Kudo et al., 2005). These algorithms are called boosting because they iteratively choose features (weak learners) and optimize the weights for the boost/exponential loss. They do not, however, maintain a distribution over N-best lists. The"
P08-2010,P06-2101,0,0.0539644,"LEU Dev, Best BLEU Eval, Best BLEU Eval, Selected BLEU MERT 40.3 24.0 41.2 41.2 BOOST 41.0 25.0 43.7 42.0 ∆ 0.7 1.0 2.5 0.8 5 Table 1: The first three rows show the BLEU score for Train, Dev, and Eval from 30 iterations of BoostedMERT or 30 random re-restarts of MERT. The last row shows the actual BLEU on Eval when selecting the number of boosting iterations based on Dev. Last column indicates absolute improvements. BoostedMERT outperforms MERT by 0.8 points on Eval. 4 Related Work Various methods are used to optimize log-linear models in re-ranking (Shen et al., 2004; Venugopal et al., 2005; Smith and Eisner, 2006). Although this line of work is worthwhile, we believe more gain is possible if we go beyond log-linear models. For example, Shen’s method (2004) produces largemargins but observed little gains in performance. Our BoostedMERT should not be confused with other boosting algorithms such as (Collins and Koo, 2005; Kudo et al., 2005). These algorithms are called boosting because they iteratively choose features (weak learners) and optimize the weights for the boost/exponential loss. They do not, however, maintain a distribution over N-best lists. The idea of maintaining a distribution over Nbest li"
P08-2010,W05-0836,0,\N,Missing
P11-2075,D08-1014,0,0.30279,"Missing"
P11-2075,W06-1615,0,0.0877732,"(x, y) = ps (y|x)ps (x) be the corresponding source distribution. We assume that one (or both) of the following distributions differ between source and target: • Instance mismatch: ps (x) 6= pt (x). • Labeling mismatch: ps (y|x) 6= pt (y|x). Instance mismatch implies that the input feature vectors have different distribution (e.g. one dataset uses the word “excellent” often, while the other uses the word “awesome”). This degrades performance because classifiers trained on “excellent” might not know how to classify texts with the word “awesome.” The solution is to tie together these features (Blitzer et al., 2006) or re-weight the input distribution (Sugiyama et al., 2008). Under some assumptions (i.e. covariate shift), oracle accuracy can be achieved theoretically (Shimodaira, 2000). Labeling mismatch implies the same input has different labels in different domains. For example, the JP word meaning “excellent” may be mistranslated as “bad” in English. Then, positive JP 5 See “Adapt by Language” columns of Table 2. Note JP+FR+DE condition has 6000 labeled samples, so is not directly comparable to other adaptation scenarios (2000 samples). Nevertheless, mixing languages seem to give good results. 6 See"
P11-2075,N09-1068,0,0.0223374,"he same market and same language domain as the target. “JP+FR+DE” indicates the concatenation of JP, FR, DE as source data. Boldface shows the winner of Supervised vs. Adapted. reviews will be associated with the word “bad”: ps (y = +1|x = bad) will be high, whereas the true conditional distribution should have high pt (y = −1|x = bad) instead. There are several cases for labeling mismatch, depending on how the polarity changes (Table 3). The solution is to filter out these noisy samples (Jiang and Zhai, 2007) or optimize loosely-linked objectives through shared parameters or Bayesian priors (Finkel and Manning, 2009). Which mismatch is responsible for accuracy degradations in cross-lingual adaptation? To measure instance mismatch, we compute statistics between ps (x) and pt (x), or approximations thereof: First, we calculate a (normalized) average feature from all samples of source S, which represents the unigram distribution of MT output. Similarly, the average feature vector for target T approximates the unigram distribution of English reviews pt (x). Then we measure: • Instance mismatch: Systematic MT bias generates word distributions different from naturallyoccurring English. (Translation may be valid"
P11-2075,P07-1034,0,0.247664,"d language mismatch are comparable in several cases (e.g. MUSIC-DE and DVD-EN perform similarly for target MUSIC-EN). Observation 2: The ranking of source language by decreasing accuracy is DE &gt; FR &gt; JP. Does this mean JP-EN is a more difficult language pair for MT? The next section will show that this is not necessarily the case. Certainly, the domain mismatch for JP is larger than DE, but this could be due to phenomenon other than MT errors. 4 Where exactly is the domain mismatch? 4.1 Theory of Domain Adaptation We analyze domain adaptation by the concepts of labeling and instance mismatch (Jiang and Zhai, 2007). Let pt (x, y) = pt (y|x)pt (x) be the target distribution of samples x (e.g. unigram feature vector) and labels y (positive / negative). Let ps (x, y) = ps (y|x)ps (x) be the corresponding source distribution. We assume that one (or both) of the following distributions differ between source and target: • Instance mismatch: ps (x) 6= pt (x). • Labeling mismatch: ps (y|x) 6= pt (y|x). Instance mismatch implies that the input feature vectors have different distribution (e.g. one dataset uses the word “excellent” often, while the other uses the word “awesome”). This degrades performance because"
P11-2075,P10-1114,0,0.0821553,"Missing"
P11-2075,P09-1027,0,0.423747,"Missing"
P11-2075,P10-2048,0,0.0762766,"Missing"
P12-1001,P07-1111,0,0.0175966,"tion of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunabi"
P12-1001,W11-2103,0,0.162556,"These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good t"
P12-1001,N10-1080,0,0.0600026,"in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint"
P12-1001,N09-1025,0,0.0389566,", so the method not only learns to discriminate pareto vs. non-pareto but also also learns to discriminate among competing non-pareto points. Also, like other MT works, in line 5 the N-best list is concatenated to N-best lists from previous iterations, so {h} is a set with i · N elements. General PMO Approach: The strategy we outlined in Section 3.2 can be easily applied to other MT optimization techniques. For example, by replacing the optimization subroutine (line 10, Algorithm 2) with a Powell search (Och, 2003), one can get PMO-MERT4 . Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4 A difference with traditional MERT is the necessity of sentence-BLEU (Liang et al., 2006) in line 6. We use sentenceBLEU for optimization but corpus-B"
P12-1001,W09-0426,0,0.266662,"ts. In this case, we recommend the following trick: Set up a multi-objective problem where one metric is BLEU and the other is 3/4BLEU+1/4RIBES. This encourages PMO to explore the joint metric space but avoid solutions that sacrifice too much BLEU, and should also outperform Linear Combination that searches only on the (3/4,1/4) direction. 5 How many Pareto points? The number of pareto 7 Related Work Multi-objective optimization for MT is a relatively new area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogona"
P12-1001,I08-1042,0,0.0478355,"Missing"
P12-1001,D11-1138,0,0.0324528,"auser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications. (Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models. (Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however. 6 Opportunities and Limitations We introduce a new approach (PMO) for training MT systems on multiple metrics. Leveraging the diverse perspectives of different evaluation metrics ha"
P12-1001,2009.mtsummit-posters.8,0,0.158214,"and the other is 3/4BLEU+1/4RIBES. This encourages PMO to explore the joint metric space but avoid solutions that sacrifice too much BLEU, and should also outperform Linear Combination that searches only on the (3/4,1/4) direction. 5 How many Pareto points? The number of pareto 7 Related Work Multi-objective optimization for MT is a relatively new area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). I"
P12-1001,D11-1125,0,0.225156,"Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluat"
P12-1001,D10-1092,1,0.861646,"nce & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective optimization method that avoids “overfitting to a single metric”. We want to build a MT system"
P12-1001,P07-2045,0,0.0130597,"Missing"
P12-1001,W07-0734,0,0.0337259,"s BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is"
P12-1001,P06-1096,0,0.0438141,"RT4 . Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4 A difference with traditional MERT is the necessity of sentence-BLEU (Liang et al., 2006) in line 6. We use sentenceBLEU for optimization but corpus-BLEU for evaluation here. 5 abstracts. As metrics we use BLEU and RIBES (which demonstrated good human correlation in this language pair (Goto et al., 2011)). (2) The NIST task is Chinese-to-English translation with OpenMT08 training data and MT06 as devset. As metrics we use BLEU and NTER. • BLEU = BP × (Πprecn )1/4 . BP is brevity penality. precn is precision of n-gram matches. 1/4 • RIBES = (τ + 1)/2 × prec1 , with Kendall’s τ computed by measuring permutation between matching words in reference and hypothesis5 . • NTER=max(1−TER,"
P12-1001,N07-1006,0,0.0190011,"w area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question"
P12-1001,D11-1035,0,0.262905,"a choice between picking the best weight according to BLEU (BLEU=.265,RIBES=.665) vs. another weight with higher RIBES but poorer BLEU, e.g. (.255,.675). Nevertheless, both the PMO and Linear-Combination with various (p1 , p2 ) samples this joint-objective space broadly. 3. Interestingly, a multi-objective approach can sometimes outperform a single-objective optimizer in its own metric. In Figure 2, singleobjective PRO focusing on optimizing RIBES only achieves 0.68, but PMO-PRO using both BLEU and RIBES outperforms with 0.685. The third observation relates to the issue of metric tunability (Liu et al., 2011). We found that RIBES can be difficult to tune directly. It is an extremely non-smooth objective with many local optima–slight changes in word ordering causes large changes in RIBES. So the best way to improve RIBES is to 6 0.694 0.146 0.148 0.15 0.152 0.154 0.156 0.158 0.16 0.162 0.164 bleu Figure 3: NIST Results not to optimize it directly, but jointly with a more tunable metric BLEU. The learning curve in Figure 4 show that single-objective optimization of RIBES quickly falls into local optimum (at iteration 3) whereas PMO can zigzag and sacrifice RIBES in intermediate iterations (e.g. iter"
P12-1001,D08-1076,0,0.0416377,"f combination weights. Further we observe that multiobjective approaches can be helpful for optimizing difficult-to-tune metrics; this is beneficial for quickly introducing new metrics developed in MT evaluation into MT optimization, especially when good {pk } are not yet known. We conclude by drawing attention to some limitations and opportunities raised by this work: Limitations: (1) The performance of PMO is limited by the size of the Pareto set. Small N-best lists lead to sparsely-sampled Pareto Frontiers, and a much better approach would be to enlarge the hypothesis space using lattices (Macherey et al., 2008). How to compute Pareto points directly from lattices is an interesting open research question. (2) The binary distinction between pareto vs. non-pareto points ignores the fact that 2nd-place non-pareto points may also lead to good practical solutions. A better approach may be to adopt a graded definition of Pareto optimality as done in some multi-objective works (Deb et al., 2002). (3) A robust evaluation methodology that enables significance testing for multi-objective problems is sorely needed. This will make it possible to compare multi-objective methods on more than 2 metrics. We also nee"
P12-1001,mauser-etal-2008-automatic,0,0.138462,"ts {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011)"
P12-1001,P03-1021,0,0.424427,"e diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While ma"
P12-1001,W07-0714,0,0.0433798,"Missing"
P12-1001,P02-1040,0,0.0840642,"e for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that"
P12-1001,2010.iwslt-evaluation.1,0,0.022396,"ecause they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. T"
P12-1001,2006.amta-papers.25,0,0.0542521,"ect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective o"
P12-1001,D11-1117,0,0.0468041,"not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications. (Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models. (Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however. 6 Opportunities and Limit"
P12-2020,W06-2920,0,0.0329395,"ee of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003)"
P12-2020,J07-4004,0,0.0657609,"tsuhito@lab.ntt.co.jp, kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp Abstract tures. For example, using the constituent-todependency conversion approach proposed by Johansson and Nugues (2007), we can easily yield dependency trees from PCFG style trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2 (Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency struc"
P12-2020,P97-1003,0,0.248621,"typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 4 http://nlp.cs.lth.se/software/treebank converter/ http://www.cs.columbia.edu/ mcollins/papers/heads 101 a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predicate are designated by the arrows from the argument features in a leaf node to"
P12-2020,W07-2416,0,0.0959405,"ng. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 4 http://nlp.cs.lth.se/software/treebank converter/ http://www.cs.columbia.edu/ mcollins/papers/heads 101 a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predica"
P12-2020,P07-2045,0,0.00633941,"icate types to the gold-standard grammatical relations can be found in Table 13 in (Clark and 102 Curran, 2007). The post-processing is like that described for HPSG parsing, except we greedily use the MST’s sentence root when we can not determine it based on the CCG parser’s PASs. 3 Experiments 3.1 Setup We re-implemented the string-to-dependency decoder described in (Shen et al., 2008). Dependency structures from non-isomorphic syntactic/semantic parsers are separately used to train the transfer rules as well as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007). For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm1.0b35 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gi"
P12-2020,P95-1037,0,0.136107,"2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 4 http://nlp.cs.lth.se/software/treebank converter/ http://www.cs.columbia.edu/ mcollins/papers/heads 101 a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predicate are designated by the arrows from the argument features"
P12-2020,H94-1020,0,0.0614407,"Missing"
P12-2020,J11-1007,0,0.0363269,"age side dependency structures have been successfully used in statistical machine translation (SMT) by Shen et al. (2008) and achieved state-of-the-art results as reported in the NIST 2008 Open MT Evaluation workshop and the NTCIR-9 Chinese-to-English patent translation task (Goto et al., 2011; Ma and Matsoukas, 2011). A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to longdistance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order (McDonald and Nivre, 2011). It is known that dependency-style structures can be transformed from a number of linguistic struc∗ † Now at Baidu Inc. Now at Nara Institute of Science & Technology (NAIST) 2 Gaining Dependency Structures 2.1 Dependency tree We follow the definition of dependency graph and dependency tree as given in (McDonald and Nivre, 2011). A dependency graph G for sentence s is called a dependency tree when it satisfies, (1) the nodes cover all the words in s besides the ROOT; (2) one node can have one and only one head (word) with a determined syntactic role; and (3) the ROOT of the graph is reachable"
P12-2020,P05-1012,0,0.0580344,"_ aux_ verb_ punct_ noun_ arg0 arg1 arg12 arg12 arg1 when the fluid pressure cylinder 31 * c19 c13 c22 c24 c25 t10 t11 t12 aux_ arg12 adj_ arg1 verb_ arg12 is gradually applied . Figure 1: HPSG tree of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head find"
P12-2020,J08-1002,0,0.0193457,"o, Soraku-gun Kyoto 619-0237 Japan wuxianchao@gmail.com,sudoh.katsuhito@lab.ntt.co.jp, kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp Abstract tures. For example, using the constituent-todependency conversion approach proposed by Johansson and Nugues (2007), we can easily yield dependency trees from PCFG style trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2 (Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using"
P12-2020,W03-3017,0,0.0491047,"g12 arg1 when the fluid pressure cylinder 31 * c19 c13 c22 c24 c25 t10 t11 t12 aux_ arg12 adj_ arg1 verb_ arg12 is gradually applied . Figure 1: HPSG tree of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerm"
P12-2020,J03-1002,0,0.00290707,"pendency decoder described in (Shen et al., 2008). Dependency structures from non-isomorphic syntactic/semantic parsers are separately used to train the transfer rules as well as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007). For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm1.0b35 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. We report the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). 3.2 Statistics of dependencies We compare the similarity of the dependencies with each other, as shown in Table 2. Ba"
P12-2020,P02-1040,0,0.0827304,"Missing"
P12-2020,P11-1027,0,0.01549,"as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007). For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm1.0b35 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. We report the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). 3.2 Statistics of dependencies We compare the similarity of the dependencies with each other, as shown in Table 2. Basically, we investigate (1) if two dependency graphs of one sentence share the same root word and (2) if the head of one word in one sentence are identical in two dependency graph"
P12-2020,P08-1066,0,0.377419,"le trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2 (Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation a"
P12-2020,N07-1051,0,\N,Missing
P13-2119,D10-1092,1,0.436516,"Missing"
P13-2119,W12-3139,0,0.0208124,"Missing"
P13-2119,N06-2001,0,0.028358,"Missing"
P13-2119,W04-3250,0,0.464764,"Missing"
P13-2119,W12-2703,0,0.060629,"Missing"
P13-2119,D11-1033,0,0.191828,"vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams. 1 2 Data Selection Method We employ the data selection method of (Axelrod et al., 2011), which builds upon (Moore and Lewis, 2010). The intuition is to select general-domain sentences that are similar to indomain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large g"
P13-2119,P10-2041,0,0.175474,"language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams. 1 2 Data Selection Method We employ the data selection method of (Axelrod et al., 2011), which builds upon (Moore and Lewis, 2010). The intuition is to select general-domain sentences that are similar to indomain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentenc"
P13-2119,C90-3038,0,0.351908,"with English (en) as source and German (de), Spanish (es), French (fr), Russian (ru) as target. This is the in-domain corpus, and consists of TED Talk transcripts covering topics in technology, entertainment, and design. As general-domain corpora, we collected bitext from the WMT2013 campaign, including CommonCrawl and NewsCommentary for all 4 languages, Europarl for de/es/fr, UN for es/fr, Gigaword for fr, and Yandex for ru. The indomain data is divided into a training set (for SMT 1 Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. 2 The recurrent states are unrolled for several time-steps, then stochastic gradient descent is applied. 679 en-de en-es In-domain Training Set #sentence 129k 140k #t"
P13-2119,2012.eamt-1.60,0,0.0186427,"ontext as an identity (n-gram hit-or-miss) function on [w(t − 1), w(t − 2), . . .], neural LMs summarize the context by a hidden state vector s(t). This is a continuous vector of dimension |S |whose elements are predicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 3 Experiment Setup We experimented with four language pairs in the WIT3 corpus (Cettolo et al., 2012), with English (en) as source and German (de), Spanish (es), French (fr), Russian (ru) as target. This is the in-domain corpus, and consists of TED Talk transcripts covering topics in technology, entertainment, and design. As general-domain corpora, we collected bitext from the WMT2013 campaign, including CommonCrawl and NewsCommentary for all 4 languages, Europarl for de/es/fr, UN for es/fr, Gigaword for fr, and Yandex for ru. The indomain data is divided into a training set (for SMT 1 Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007"
P13-2119,2012.amta-papers.19,0,0.122261,"Missing"
P13-2119,2010.iwslt-papers.5,1,0.84743,"one defines the score of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized crossentropy of e on the English in-domain LM. GENE (e) is the length-normalized cross-entropy 678 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678–683, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0 (t), . . . , wk (t), . . . w|W |(t)]  Figure"
P13-2119,P02-1040,0,0.0858846,"Missing"
P13-2119,W12-2702,0,0.0173223,"Missing"
P13-2119,W12-3154,0,0.0129056,"core of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized crossentropy of e on the English in-domain LM. GENE (e) is the length-normalized cross-entropy 678 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678–683, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0 (t), . . . , wk (t), . . . w|W |(t)]  Figure 1: Recurrent neural LM. w"
P13-2119,2006.amta-papers.25,0,0.0371213,"Missing"
P13-2119,C12-1173,0,0.0163768,"Missing"
P13-2119,I08-2088,0,0.0198878,"Missing"
P13-2119,D10-1044,0,\N,Missing
P14-2024,2008.amta-srw.1,0,0.0300649,"-mentioned elements must be taken seriously in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first step in examining and codifying the more important elements that make it possible to construct a highly accurate T2S MT system. To do so, we perform an empirical study of the effect of parsing accuracy, packed forest input, alignment accuracy, and search. The reason why we"
P14-2024,P06-1002,0,0.0273261,"http://plata.ar.media.kyoto-u.ac.jp/tool/EDA 145 Name GIZA++ Nile/16 Nile/4 Nile crease in BLEU at the cost of an increase in decoding time. Interestingly, the increases in BLEU did not show any sign of saturating even when setting the n-best cutoff to 200, although larger cutoffs resulted in exceedingly large translation forests that required large amounts of memory. Alignment Overview The second element that we investigate is alignment accuracy. It has been noted in many previous works that significant gains in alignment accuracy do not make a significant difference in translation results (Ayan and Dorr, 2006; Ganchev et al., 2008). However, none of these works have explicitly investigated the effect on T2S translation, so it is not clear whether these results carry over to our current situation. As our baseline aligner, we use the GIZA++ implementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the discriminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit.6 This method has the ability to use source- and targetside syntactic information, and has been shown to improve the accuracy of S2"
P14-2024,J07-2003,0,0.0123811,"0.71 1.75 2.96 4.80 1.75 4.34 8.73 en-ja HS en-ja CP 0.09 0.08 0.14 0.13 0.25 0.37 0.57 0.24 0.44 0.64 1.21 2.22 3.97 1.60 3.83 5.74 ja-en HS ja-en CP 30 0.0 29 100 1000 100 10000 1000 Pop Limit 10000 Figure 3: Hypergraph search (HS) and cube pruning (CP) results for F2S and T2S. Numbers above and below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripher"
P14-2024,P05-1066,0,0.0379111,"nt element of statistical machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see large increases of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods based on syntactic pre- or post-ordering. 143 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143"
P14-2024,D12-1109,0,0.0120695,"3: Hypergraph search (HS) and cube pruning (CP) results for F2S and T2S. Numbers above and below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripheral elements that contribute greatly to the accuracy of T2S machine translation: parsing, alignment, and search. Put together, a T2S system that uses the more effective settings for these three elements greatly out"
P14-2024,I11-1087,1,0.67766,"le model of the Berkeley parser tends to have the higher accuracy of the two, so if the accuracy of a system using this model is higher then it is likely that parsing accuracy is important for T2S translation. Instead of the Berkeley Parser itself, we use a clone Egret,4 which achieves nearly identical accuracy, and is able to output packed forests for use in MT, as mentioned below. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other element in the phrase. For Japanese, our first method uses the MSTbased pointwise dependency parser of Flannery et al. (2011), as implemented in the Eda toolkit.5 In order to convert dependencies into phrasestructure trees typically used in T2S translation, we use the head rules implemented in the Travatar toolkit. In addition, we also train a latent variable CFG using the Berkeley Parser and use Egret for parsing. Both models are trained on the Japanese Word Dependency Treebank (Mori et al., 2014). In addition, Mi et al. (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. By doing so, it is possible to resolve some of the ambigu"
P14-2024,J07-3002,0,0.0225012,"Missing"
P14-2024,P06-1121,0,0.0606722,"nment accuracy, and search. The reason why we choose these elements is that past work that has reported low accuracy for T2S systems has often neglected to consider one or all of these elements. 1 Introduction In recent years, syntactic parsing is being viewed as an ever-more important element of statistical machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see l"
P14-2024,P08-1112,0,0.0227997,"Missing"
P14-2024,P12-2061,0,0.0149337,"ison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which are state-ofthe-art methods for the language pairs. Specifically, for en-ja translation we use the head finalization pre-ordering method of (Isozaki et al., 2010b), and for ja-en translation, we use the syntactic postordering method of (Goto et al., 2012). For all systems, T2S or otherwise, the language model is a Kneser-Ney 5-gram, and tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 2.2 ja-en BLEU RIBES 30.49 69.80 29.41 69.51 29.42 73.85 31.15 72.87 33.70 75.94 Table 1: Overall results for five systems. we compare the 3 non-T2S baselines with two T2S systems that vary the settings of the parser, alignment, and search, as described in the following Sections 3, 4, and 5. The first system “T2Sall” is a system that uses the worst settings1 for each of these elements, while the second system “T2S+all” use"
P14-2024,N04-1014,0,0.0156771,"ever, after accounting for all these elements we see large increases of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods based on syntactic pre- or post-ordering. 143 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143–149, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Experimental Setup 2.1 System PBMT Hiero Pre/Post T2S-all T2S+all Systems Compared In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight, 2004), constructed using the Travatar toolkit (Neubig, 2013). Rules are extracted using the GHKM algorithm (Galley et al., 2006), and rules with up to 5 composed minimal rules, up to 2 nonterminals, and up to 10 terminals are used. We also prepare 3 baselines not based on T2S to provide a comparison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two me"
P14-2024,N13-1116,0,0.029048,"d below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripheral elements that contribute greatly to the accuracy of T2S machine translation: parsing, alignment, and search. Put together, a T2S system that uses the more effective settings for these three elements greatly outperforms a system that uses more standard settings, as well as the current state-of-the-art o"
P14-2024,D10-1092,1,0.0713085,"Missing"
P14-2024,W10-1736,1,0.771966,"nd up to 10 terminals are used. We also prepare 3 baselines not based on T2S to provide a comparison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which are state-ofthe-art methods for the language pairs. Specifically, for en-ja translation we use the head finalization pre-ordering method of (Isozaki et al., 2010b), and for ja-en translation, we use the syntactic postordering method of (Goto et al., 2012). For all systems, T2S or otherwise, the language model is a Kneser-Ney 5-gram, and tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 2.2 ja-en BLEU RIBES 30.49 69.80 29.41 69.51 29.42 73.85 31.15 72.87 33.70 75.94 Table 1: Overall results for five systems. we compare the 3 non-T2S baselines with two T2S systems that vary the settings of the parser, alignment, and search, as described in the following Sections 3, 4, and 5. The first system “T2Sall” is a system t"
P14-2024,2012.amta-papers.27,0,0.0218767,"y in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first step in examining and codifying the more important elements that make it possible to construct a highly accurate T2S MT system. To do so, we perform an empirical study of the effect of parsing accuracy, packed forest input, alignment accuracy, and search. The reason why we choose these elements is that past work"
P14-2024,P07-2045,0,0.00710243,"imental Setup 2.1 System PBMT Hiero Pre/Post T2S-all T2S+all Systems Compared In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight, 2004), constructed using the Travatar toolkit (Neubig, 2013). Rules are extracted using the GHKM algorithm (Galley et al., 2006), and rules with up to 5 composed minimal rules, up to 2 nonterminals, and up to 10 terminals are used. We also prepare 3 baselines not based on T2S to provide a comparison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which are state-ofthe-art methods for the language pairs. Specifically, for en-ja translation we use the head finalization pre-ordering method of (Isozaki et al., 2010b), and for ja-en translation, we use the syntactic postordering method of (Goto et al., 2012). For all systems, T2S or otherwise, the language model is a Kneser-Ney 5-gram, and tuning is performed to maximize BLEU score usi"
P14-2024,W04-3250,0,0.0191526,"n translation over data from the NTCIR PatentMT task (Goto et al., 2011), the most standard benchmark task for these language pairs. We use the training data from NTCIR 7/8, a total of approximately 3.0M sentences, and perform tuning on the NTCIR 7 dry run, testing on the NTCIR 7 formal run data. As evaluation measures, we use the standard BLEU (Papineni et al., 2002) as well as RIBES (Isozaki et al., 2010a), a reorderingbased metric that has been shown to have high correlation with human evaluations on the NTCIR data. We measure significance of results using bootstrap resampling at p &lt; 0.05 (Koehn, 2004). In tables, bold numbers indicate the best system and all systems that were not significantly different from the best system. 2.3 en-ja BLEU RIBES 35.84 72.89 34.45 72.94 36.69 77.05 36.23 76.60 40.84 80.15 3 Parsing 3.1 Parsing Overview As T2S translation uses parse trees both in training and testing of the system, an accurate syntactic parser is required. In order to test the extent that parsing accuracy affects translation, we use two 1 Stanford/Eda, GIZA++, pop-limit 5000 cube pruning. forests, Nile, pop-limit 5000 hypergraph search. 3 We have also observed similar trends on other genres"
P14-2024,D12-1096,0,0.010034,"s of en-ja 24.55→30.81, ja-en 19.28→22.46, zh-ja 15.22→20.67, ja-zh 30.88→33.89. 2 Egret Motivational Experiment Before going into a detailed analysis, we first present results that stress the importance of the elements described in the introduction. To do so, 144 different syntactic parsers and examine the translation accuracy realized by each parser. For English, the two most widely referenced parsers are the Stanford Parser and Berkeley Parser. In this work, we compare the Stanford Parser’s CFG model, with the Berkeley Parser’s latent variable model. In previous reports, it has been noted (Kummerfeld et al., 2012) that the latent variable model of the Berkeley parser tends to have the higher accuracy of the two, so if the accuracy of a system using this model is higher then it is likely that parsing accuracy is important for T2S translation. Instead of the Berkeley Parser itself, we use a clone Egret,4 which achieves nearly identical accuracy, and is able to output packed forests for use in MT, as mentioned below. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other element in the phrase. For Japanese, our first method uses the MSTbased pointwis"
P14-2024,P06-1077,0,0.0394828,"ms has often neglected to consider one or all of these elements. 1 Introduction In recent years, syntactic parsing is being viewed as an ever-more important element of statistical machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see large increases of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods ba"
P14-2024,P11-1128,0,0.026314,"Missing"
P14-2024,P08-1023,0,0.495906,"nd JapaneseEnglish pairs, we show how a basic T2S system that performs on par with phrasebased systems can be improved by 2.6-4.6 BLEU, greatly exceeding existing stateof-the-art methods. These results indicate that T2S systems indeed hold much promise, but the above-mentioned elements must be taken seriously in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first"
P14-2024,mori-etal-2014-japanese,0,0.0216529,"ow. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other element in the phrase. For Japanese, our first method uses the MSTbased pointwise dependency parser of Flannery et al. (2011), as implemented in the Eda toolkit.5 In order to convert dependencies into phrasestructure trees typically used in T2S translation, we use the head rules implemented in the Travatar toolkit. In addition, we also train a latent variable CFG using the Berkeley Parser and use Egret for parsing. Both models are trained on the Japanese Word Dependency Treebank (Mori et al., 2014). In addition, Mi et al. (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. By doing so, it is possible to resolve some of the ambiguity in syntactic interpretation at translation time, potentially increasing translation accuracy. However, the great majority of recent works on T2S translation do not consider multiple syntactic parses (e.g. Liu et al. (2011), Zhang et al. (2011)), and thus it is important to confirm the potential gains that could be acquired by taking ambiguity into account. 3.2 en-ja BLEU"
P14-2024,W13-4604,1,0.812763,"ntly different from the best system. 2.3 en-ja BLEU RIBES 35.84 72.89 34.45 72.94 36.69 77.05 36.23 76.60 40.84 80.15 3 Parsing 3.1 Parsing Overview As T2S translation uses parse trees both in training and testing of the system, an accurate syntactic parser is required. In order to test the extent that parsing accuracy affects translation, we use two 1 Stanford/Eda, GIZA++, pop-limit 5000 cube pruning. forests, Nile, pop-limit 5000 hypergraph search. 3 We have also observed similar trends on other genres and language pairs. For example, in a Japanese-Chinese/English medical conversation task (Neubig et al., 2013), forests, alignment, and search resulted in BLEU increases of en-ja 24.55→30.81, ja-en 19.28→22.46, zh-ja 15.22→20.67, ja-zh 30.88→33.89. 2 Egret Motivational Experiment Before going into a detailed analysis, we first present results that stress the importance of the elements described in the introduction. To do so, 144 different syntactic parsers and examine the translation accuracy realized by each parser. For English, the two most widely referenced parsers are the Stanford Parser and Berkeley Parser. In this work, we compare the Stanford Parser’s CFG model, with the Berkeley Parser’s laten"
P14-2024,P07-1019,0,0.0233075,"a-en HS ja-en CP 30 0.0 29 100 1000 100 10000 1000 Pop Limit 10000 Figure 3: Hypergraph search (HS) and cube pruning (CP) results for F2S and T2S. Numbers above and below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripheral elements that contribute greatly to the accuracy of T2S machine translation: parsing, alignment, and search. Put together, a T2S system that"
P14-2024,P13-4016,1,0.241763,"es of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods based on syntactic pre- or post-ordering. 143 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143–149, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Experimental Setup 2.1 System PBMT Hiero Pre/Post T2S-all T2S+all Systems Compared In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight, 2004), constructed using the Travatar toolkit (Neubig, 2013). Rules are extracted using the GHKM algorithm (Galley et al., 2006), and rules with up to 5 composed minimal rules, up to 2 nonterminals, and up to 10 terminals are used. We also prepare 3 baselines not based on T2S to provide a comparison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which a"
P14-2024,D10-1027,0,0.0170447,"Limit 10000 Figure 3: Hypergraph search (HS) and cube pruning (CP) results for F2S and T2S. Numbers above and below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripheral elements that contribute greatly to the accuracy of T2S machine translation: parsing, alignment, and search. Put together, a T2S system that uses the more effective settings for these three"
P14-2024,J03-1002,0,0.00756703,"ted in exceedingly large translation forests that required large amounts of memory. Alignment Overview The second element that we investigate is alignment accuracy. It has been noted in many previous works that significant gains in alignment accuracy do not make a significant difference in translation results (Ayan and Dorr, 2006; Ganchev et al., 2008). However, none of these works have explicitly investigated the effect on T2S translation, so it is not clear whether these results carry over to our current situation. As our baseline aligner, we use the GIZA++ implementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the discriminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit.6 This method has the ability to use source- and targetside syntactic information, and has been shown to improve the accuracy of S2T translation. We trained Nile and tested both methods on the Japanese-English alignments provided with the Kyoto Free Translation Task (Neubig, 2011) (430k parallel sentences, 1074 manually aligned training sentences, and 120 manually aligned test sentences).7 As creating manual alignm"
P14-2024,P03-1021,0,0.0210137,"t for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which are state-ofthe-art methods for the language pairs. Specifically, for en-ja translation we use the head finalization pre-ordering method of (Isozaki et al., 2010b), and for ja-en translation, we use the syntactic postordering method of (Goto et al., 2012). For all systems, T2S or otherwise, the language model is a Kneser-Ney 5-gram, and tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 2.2 ja-en BLEU RIBES 30.49 69.80 29.41 69.51 29.42 73.85 31.15 72.87 33.70 75.94 Table 1: Overall results for five systems. we compare the 3 non-T2S baselines with two T2S systems that vary the settings of the parser, alignment, and search, as described in the following Sections 3, 4, and 5. The first system “T2Sall” is a system that uses the worst settings1 for each of these elements, while the second system “T2S+all” uses the best settings.2 The results for the systems are shown in Table 1. The most striking result is that T2S+all significantly exceeds all of the baselines, even including"
P14-2024,P02-1040,0,0.0960476,"will investigate the contribution of each of these elements in detail in the following sections. In the remainder of the paper settings follow T2S+all except when otherwise noted. Data and Evaluation We perform all of our experiments on en-ja and ja-en translation over data from the NTCIR PatentMT task (Goto et al., 2011), the most standard benchmark task for these language pairs. We use the training data from NTCIR 7/8, a total of approximately 3.0M sentences, and perform tuning on the NTCIR 7 dry run, testing on the NTCIR 7 formal run data. As evaluation measures, we use the standard BLEU (Papineni et al., 2002) as well as RIBES (Isozaki et al., 2010a), a reorderingbased metric that has been shown to have high correlation with human evaluations on the NTCIR data. We measure significance of results using bootstrap resampling at p &lt; 0.05 (Koehn, 2004). In tables, bold numbers indicate the best system and all systems that were not significantly different from the best system. 2.3 en-ja BLEU RIBES 35.84 72.89 34.45 72.94 36.69 77.05 36.23 76.60 40.84 80.15 3 Parsing 3.1 Parsing Overview As T2S translation uses parse trees both in training and testing of the system, an accurate syntactic parser is require"
P14-2024,P10-1017,0,0.0695133,"is alignment accuracy. It has been noted in many previous works that significant gains in alignment accuracy do not make a significant difference in translation results (Ayan and Dorr, 2006; Ganchev et al., 2008). However, none of these works have explicitly investigated the effect on T2S translation, so it is not clear whether these results carry over to our current situation. As our baseline aligner, we use the GIZA++ implementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the discriminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit.6 This method has the ability to use source- and targetside syntactic information, and has been shown to improve the accuracy of S2T translation. We trained Nile and tested both methods on the Japanese-English alignments provided with the Kyoto Free Translation Task (Neubig, 2011) (430k parallel sentences, 1074 manually aligned training sentences, and 120 manually aligned test sentences).7 As creating manual alignment data is costly, we also created two training sets that consisted of 1/4 and 1/16 of the total data to test if we can achieve an effect with sm"
P14-2024,P08-1066,0,0.00443162,"earch. The reason why we choose these elements is that past work that has reported low accuracy for T2S systems has often neglected to consider one or all of these elements. 1 Introduction In recent years, syntactic parsing is being viewed as an ever-more important element of statistical machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see large increases of ac"
P14-2024,2011.mtsummit-papers.36,1,0.958722,"cal machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see large increases of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods based on syntactic pre- or post-ordering. 143 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143–149, c Baltimore, Ma"
P14-2024,D11-1020,0,0.00523042,"be taken seriously in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first step in examining and codifying the more important elements that make it possible to construct a highly accurate T2S MT system. To do so, we perform an empirical study of the effect of parsing accuracy, packed forest input, alignment accuracy, and search. The reason why we choose these elem"
P14-2024,P12-2062,0,0.0174251,"4 2.05 10 Forest n-best Cutoff 100 Figure 1: BLEU scores using various levels of forest pruning. Numbers in the graph indicate decoding time in seconds/sentence. Egret achieves greater accuracy than that using the other two parsers. This improvement is particularly obvious in RIBES, indicating that an increase in parsing accuracy has a larger effect on global reordering than on lexical choice. When going from T2S to F2S translation using Egret, we see another large gain in accuracy, although this time with the gain in BLEU being more prominent. We believe this is related to the observation of Zhang and Chiang (2012) that F2S translation is not necessarily helping fixing parsing errors, but instead giving the translation system the freedom to ignore the parse somewhat, allowing for less syntactically motivated but more fluent translations. As passing some degree of syntactic ambiguity on to the decoder through F2S translation has proven useful, a next natural question is how much of this ambiguity we need to preserve in our forest. The pruning criterion that we use for the forest is based on including all edges that appear in one or more of the n-best parses, so we perform translation setting n to 1 (tree"
P14-2024,P11-1084,0,0.0742786,"ow a basic T2S system that performs on par with phrasebased systems can be improved by 2.6-4.6 BLEU, greatly exceeding existing stateof-the-art methods. These results indicate that T2S systems indeed hold much promise, but the above-mentioned elements must be taken seriously in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first step in examining and codifying the m"
P15-1093,W02-1001,0,0.0740395,"Missing"
P15-1093,P10-1160,0,0.100864,"Missing"
P15-1093,I11-1023,1,0.841898,"Missing"
P15-1093,P08-1067,0,0.0786277,"Missing"
P15-1093,W07-1522,1,0.769102,"Missing"
P15-1093,P09-2022,0,0.517951,"Missing"
P15-1093,P13-1116,0,0.0315645,"Missing"
P15-1093,E06-1011,0,0.0563756,"Missing"
P15-1093,I11-1085,0,0.409695,"Missing"
P15-1093,D08-1055,0,0.319631,"Missing"
P15-1093,D14-1041,0,0.0607383,"Missing"
P15-1093,D14-1109,0,0.0485623,"Missing"
P15-2043,I05-3017,0,0.276786,"Missing"
P15-2043,D12-1132,0,0.199707,"Missing"
P15-2043,W03-1719,0,0.0991892,"Missing"
P15-2043,D11-1090,0,0.358815,"Missing"
P15-2043,N09-1007,0,0.392543,"Missing"
P15-2043,P12-1027,0,0.0930338,"is.naist.jp Abstract words (IVs). By running a synthetic word parser on each of the words in a CWS training set, we can generate a fine-grained segmentation standard that contains more IVs. Since the current conditional random field (CRF) word segmenters (Tseng et al., 2005; Sun and Xu, 2011) perform well on IVs, this transforming process can conceivably improve the handling of pseudo-OOV words, as long as we can recover the original word segmentation standard from the fine-grained sub-word segmentation. In recent years, some related works about improving OOV problem in CWS have been ongoing. Sun et al. (2012) presented a joint model for Chinese word segmentation and OOVs detection. Their models achieved fast training speed, high accuracies and increase on OOV recall. Sun (2011) proposed a similar sub-word structure which is generated by merging the segmentations provided by different segmenters (a word-based segmenter, a character-based segmenter and a local character classifier). However, her models does not predict the sub-words of all the synthetic words, but those words with different segmented results of the three segmenters. Her work maximizes the agreement of different models to improve CWS"
P15-2043,I05-3027,0,0.0428039,"a graph-based parser (McDonald, 2006) on the data released by Cheng et al. (2014) and include the dictionary (NAIST Chinese Dictionary1 ) features and Brown clustering features extracted from a large unlabeled corpus (Chinese Gigaword Second Edition2 ) as described in Cheng et al. (2014). For native Chinese speakers, single character and two character words are usually treated as the 2.2 CRF-based Word Segmenter Xue et al. (2003) proposed a method which treated Chinese word segmentation as a character-based sequential labeling problem and exploited several discriminative learning algorithms. Tseng et al. (2005) adopted the CRFs as the learning method and obtained the best results in the second international Chinese word segmentation bakeoff2005. Moreover, Sun and Xu (2011) attempted to extract information from large unlabeled data to enhance the Chinese word segmentation results. In this work, we train a traditional CRF-based supervised model on the fine-grained training data, include the dictionary (NAIST Chinese Dictionary) features and access variety features extracted from a large unlabeled corpus (Chinese Gigaword Second Edition) as described in Sun and Xu (2011). 1 http://cl.naist.jp/index.php"
P15-2043,O03-4002,0,0.361957,"Missing"
P15-2043,P07-1106,0,0.241264,"Missing"
P15-2043,D13-1031,0,0.600326,"Missing"
P15-2043,J09-4006,0,\N,Missing
P15-2043,P11-1139,0,\N,Missing
P15-2043,W03-1726,0,\N,Missing
P16-2086,P14-5010,0,0.00758309,"onsiders if the speaker would prefer a particular DC, explicit or implicit, when expressing the intended sense. In this experiment, we integrate the output of an automatic discourse parser with the probability prediction by the pragmatic listener L1 . We employ the winning parser of the CONLL shared task (Wang and Lan, 2015). The parser is also trained on Sections 2-22 of PDTB, and thus does not overlap with our test set. The sense classification of the parser is based on a pool of lexicosyntactic features drawn from gold standard arguments, DCs and automatic parsed trees produced by CoreNLP (Manning et al., 2014). For each test sample, the parser outputs a probability estimate for each sense. We use these estimates to replace the salience measure (PL (s)) (in Eq. 8) and deduce PL0 1 (s|d, C), where C is the previous relation form. Non-Explicit .2616 .2616 .2507 .2692 .2616 .2616 .2698* .2671 .2616 .2616 .2616 .2616 PS (d|s, C)Pparser (s) PL0 1 (s|d, C) = P 1 PS1 (d|s0 , C)Pparser (s0 ) (9) s0 ∈S Table 2: Accuracy of prediction by L0 , L1 and L2 . Improvements above the baseline are bolded. * means significant at p &lt; 0.02 by McNemar Test. Table 3 compares the performance of the original parser output a"
P16-2086,P15-1158,0,0.104697,"Missing"
P16-2086,prasad-etal-2008-penn,0,0.14344,"Missing"
P16-2086,K15-2002,0,0.118809,"helps automatic discourse sense classification. A full discourse parser typically consists of a pipeline of classifiers: explicit and implicit DCs are first classified and then processed separately by 2 classifiers (Xue et al., 2015). On the contrary, the pragmatic listener of the RSA model considers if the speaker would prefer a particular DC, explicit or implicit, when expressing the intended sense. In this experiment, we integrate the output of an automatic discourse parser with the probability prediction by the pragmatic listener L1 . We employ the winning parser of the CONLL shared task (Wang and Lan, 2015). The parser is also trained on Sections 2-22 of PDTB, and thus does not overlap with our test set. The sense classification of the parser is based on a pool of lexicosyntactic features drawn from gold standard arguments, DCs and automatic parsed trees produced by CoreNLP (Manning et al., 2014). For each test sample, the parser outputs a probability estimate for each sense. We use these estimates to replace the salience measure (PL (s)) (in Eq. 8) and deduce PL0 1 (s|d, C), where C is the previous relation form. Non-Explicit .2616 .2616 .2507 .2692 .2616 .2616 .2698* .2671 .2616 .2616 .2616 .2"
P16-2086,K15-2001,0,\N,Missing
P18-1157,P16-1223,0,0.0737543,"Missing"
P18-1157,P17-1171,0,0.12555,"rforming a kind of stochastic ensemble over the model’s successive predic1694 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1694–1704 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ants. A typical technique to obtain lexicon embedding is concatenation of its word embedding with other linguistic embedding such as those derived from Part-Of-Speech (POS) tags. For word embeddings, we use the pre-trained 300-dimensional GloVe vectors (Pennington et al., 2014) for the both Q and P . Following Chen et al. (2017), we use three additional types of linguistic features for each token pi in the passage P : x st-1 st st+1 • 9-dimensional POS tagging embedding for total 56 different types of the POS tags. Figure 1: Illustration of “stochastic prediction dropout” in the answer module during training. At each reasoning step t, the model combines memory (bottom row) with hidden states st−1 to generate a prediction (multinomial distribution). Here, there are three steps and three predictions, but one prediction is dropped and the final result is an average of the remaining distributions. • 8-dimensional named-e"
P18-1157,D17-1215,0,0.0159621,"provement as we increase T = 1 to T = 5, but after 5 steps the improvements have saturated. In fact, the EM/F1 scores drop slightly, but considering that the random initialization results in Table 3 show a standard deviation of 0.142 and a spread of 0.426 (for EM), we believe that the T = 10 result does not statistically differ from the T = 5 result. In summary, we think it is useful to perform some approximate hyper-parameter tuning for the number of steps, but it is not necessary to find the exact optimal value. Finally, we test SAN on two Adversarial SQuAD datasets, AddSent and AddOneSent (Jia and Liang, 2017), where the passages contain 1699 Seed# EM F1 Seed# EM F1 Seed 1 76.24 84.06 Seed 6 76.23 83.99 Seed 2 76.30 84.13 Seed 7 76.35 84.09 Seed 3 75.92 83.90 Seed 8 76.07 83.71 Seed 4 76.00 83.95 Seed 9 75.93 83.85 Seed 5 76.12 83.99 Seed 10 76.15 84.11 Mean: 76.131, Std. deviation: 0.142 (EM) Mean: 83.977, Std. deviation: 0.126 (F1) Table 3: Robustness of SAN (5-step) on different random seeds for initialization: best and worst scores are boldfaced. Note that our official submit is trained on seed 1. (a) EM comparison on different systems. SAN 1 step 2 step 3 step 4 step 5 step (b) F1 score compar"
P18-1157,W04-1013,0,0.0153045,"esults on MS MARCO MS MARCO (Nguyen et al., 2016) is a large scale real-word RC dataset which contains 100,100 (100K) queries collected from anonymized user logs from the Bing search engine. The characteristic of MS MARCO is that all the questions are real user queries and passages are extracted from real web documents. For each query, approximate 10 passages are extracted from public web documents. The answers are generated by humans. The data is partitioned into a 82,430 training, a 10,047 development and 9,650 test tuples. The evaluation metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style. To apply the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple passages per query. Our model as shown in Figure 2 is developed to generate answer from a single passage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1 , ..., P J . First, we run SAN on ev1701"
P18-1157,D17-1085,0,0.140626,"rk (prediction from final step) Fixed 5-step with Memory Network (prediction averaged from all steps) Dynamic steps (max 5) with ReasoNet Stochastic Answer Network (SAN ), Fixed 5-step EM 75.139 75.033 75.256 75.355 76.235 F1 83.367 83.327 83.215 83.360 84.056 Table 1: Main results—Comparison of different answer module architectures. Note that SAN performs best in both Exact Match and F1 metrics. Ensemble model results: BiDAF + Self Attention + ELMo (Peters et al., 2018) SAN (Ensemble model) AIR-FusionNet (Huang et al., 2017) DCN+ (Xiong et al., 2017) M-Reader (Hu et al., 2017) Conductor-net (Liu et al., 2017b) r-net (Wang et al., 2017) ReasoNet++ (Shen et al., 2017) Individual model results: BiDAF + Self Attention + ELMo(Peters et al., 2018) SAN (single model) AIR-FusionNet(Huang et al., 2017) RaSoR + TR (Salant and Berant, 2017) DCN+(Xiong et al., 2017) r-net(Wang et al., 2017) ReasoNet++(Shen et al., 2017) BiDAF (Seo et al., 2016) Human Performance Dev Set (EM/F1) -/78.619/85.866 -/-/-/74.8 / 83.3 77.7/83.7 75.4/82.9 Test Set (EM/F1) 81.003/87.432 79.608/86.496 78.978/86.016 78.852/85.996 77.678/84.888 76.996/84.630 76.9/84.0 75.0/82.6 -/76.235/84.056 75.3/83.6 -/74.5/83.1 72.3/80.6 70.8/79.4 6"
P18-1157,P02-1040,0,0.10049,"form by question type? Experiments results on MS MARCO MS MARCO (Nguyen et al., 2016) is a large scale real-word RC dataset which contains 100,100 (100K) queries collected from anonymized user logs from the Bing search engine. The characteristic of MS MARCO is that all the questions are real user queries and passages are extracted from real web documents. For each query, approximate 10 passages are extracted from public web documents. The answers are generated by humans. The data is partitioned into a 82,430 training, a 10,047 development and 9,650 test tuples. The evaluation metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style. To apply the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple passages per query. Our model as shown in Figure 2 is developed to generate answer from a single passage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1 , ..., P J . Fir"
P18-1157,D14-1162,0,0.0841458,"ep is still trained to generate the same answer; we are performing a kind of stochastic ensemble over the model’s successive predic1694 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1694–1704 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ants. A typical technique to obtain lexicon embedding is concatenation of its word embedding with other linguistic embedding such as those derived from Part-Of-Speech (POS) tags. For word embeddings, we use the pre-trained 300-dimensional GloVe vectors (Pennington et al., 2014) for the both Q and P . Following Chen et al. (2017), we use three additional types of linguistic features for each token pi in the passage P : x st-1 st st+1 • 9-dimensional POS tagging embedding for total 56 different types of the POS tags. Figure 1: Illustration of “stochastic prediction dropout” in the answer module during training. At each reasoning step t, the model combines memory (bottom row) with hidden states st−1 to generate a prediction (multinomial distribution). Here, there are three steps and three predictions, but one prediction is dropped and the final result is an average of"
P18-1157,N18-1202,0,0.08263,"Missing"
P18-1157,D16-1264,0,0.781631,"ial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO). 1 Introduction Machine reading comprehension (MRC) is a challenging task: the goal is to have machines read a text passage and then answer any question about the passage. This task is an useful benchmark to demonstrate natural language understanding, and also has important applications in e.g. conversational agents and customer service support. It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning. For instance, the following example from the MRC dataset SQuAD (Rajpurkar et al., 2016) illustrates the need for synthesis of information across sentences and multiple steps of reasoning: Q: What collection does the V&A Theator & Performance galleries hold? P : The V&A Theator & Performance galleries opened in March 2009. ... They hold the UK’s biggest national collection of This kind of iterative process can be viewed as a form of multi-step reasoning. Several recent MRC models have embraced this kind of multistep strategy, where predictions are generated after making multiple passes through the same text and integrating intermediate information in the process. The first models"
P18-1157,D13-1020,0,0.0320734,"andidate answer span with its relevance score r(P j , Q) assigned by a passage ranker, and output the span with the maximum score as the answer. In our experiments, we use the passage ranker described in (Liu et al., 2018)5 . The ranker is trained on the same MS MARCO training data, and achieves 37.1 p@1 on the development set. The results in Table 7 show that SAN outperforms V-Net (Wang et al., 2018) and becomes the new state of the art6 . 6 Related Work The recent big progress on MRC is largely due to the availability of the large-scale datasets (Rajpurkar et al., 2016; Nguyen et al., 2016; Richardson et al., 2013; Hill et al., 2016), since it is possible to train large end-to-end neural network models. In spite of the variety of model structures and attenion types (Bahdanau et al., 2015; Chen et al., 2016; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017; Wang et al., 2017), a typical neural network MRC model first maps the symbolic representation of the documents and questions into a neural space, then search answers on top of it. We categorize these models into two groups based on the difference of the answer module: singlestep and multi-step reasoning. The key difference between the two is w"
P18-1157,I17-1096,1,0.832902,"his kind of iterative process can be viewed as a form of multi-step reasoning. Several recent MRC models have embraced this kind of multistep strategy, where predictions are generated after making multiple passes through the same text and integrating intermediate information in the process. The first models employed a predetermined fixed number of steps (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Kumar et al., 2015). Later, Shen et al. (2016) proposed using reinforcement learning to dynamically determine the number of steps based on the complexity of the question. Further, Shen et al. (2017) empirically showed that dynamic multi-step reasoning outperforms fixed multi-step reasoning, which in turn outperforms single-step reasoning on two distinct MRC datasets (SQuAD and MS MARCO). In this work, we derive an alternative multi-step reasoning neural network for MRC. During training, we fix the number of reasoning steps, but perform stochastic dropout on the answer module (final layer predictions). During decoding, we generate answers based on the average of predictions in all steps, rather than the final step. We call this a stochastic answer network (SAN) because the stochastic drop"
P18-1157,P17-1018,0,0.243393,"step) Fixed 5-step with Memory Network (prediction averaged from all steps) Dynamic steps (max 5) with ReasoNet Stochastic Answer Network (SAN ), Fixed 5-step EM 75.139 75.033 75.256 75.355 76.235 F1 83.367 83.327 83.215 83.360 84.056 Table 1: Main results—Comparison of different answer module architectures. Note that SAN performs best in both Exact Match and F1 metrics. Ensemble model results: BiDAF + Self Attention + ELMo (Peters et al., 2018) SAN (Ensemble model) AIR-FusionNet (Huang et al., 2017) DCN+ (Xiong et al., 2017) M-Reader (Hu et al., 2017) Conductor-net (Liu et al., 2017b) r-net (Wang et al., 2017) ReasoNet++ (Shen et al., 2017) Individual model results: BiDAF + Self Attention + ELMo(Peters et al., 2018) SAN (single model) AIR-FusionNet(Huang et al., 2017) RaSoR + TR (Salant and Berant, 2017) DCN+(Xiong et al., 2017) r-net(Wang et al., 2017) ReasoNet++(Shen et al., 2017) BiDAF (Seo et al., 2016) Human Performance Dev Set (EM/F1) -/78.619/85.866 -/-/-/74.8 / 83.3 77.7/83.7 75.4/82.9 Test Set (EM/F1) 81.003/87.432 79.608/86.496 78.978/86.016 78.852/85.996 77.678/84.888 76.996/84.630 76.9/84.0 75.0/82.6 -/76.235/84.056 75.3/83.6 -/74.5/83.1 72.3/80.6 70.8/79.4 67.7/77.3 80.3/90.5 78.580/85"
P18-1157,P18-1178,0,0.0770301,"the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple passages per query. Our model as shown in Figure 2 is developed to generate answer from a single passage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1 , ..., P J . First, we run SAN on ev1701 SingleM odel ReasoNet++(Shen et al., 2017) V-Net(Wang et al., 2018) Standard 1-step in Table 1 SAN ROUGE BLEU 38.01 38.62 45.65 42.30 42.39 46.14 43.85 Table 7: MS MARCO devset results. ery (P j , Q) pair, generating J candidate answer spans, one from each passage. Then, we multiply the SAN score of each candidate answer span with its relevance score r(P j , Q) assigned by a passage ranker, and output the span with the maximum score as the answer. In our experiments, we use the passage ranker described in (Liu et al., 2018)5 . The ranker is trained on the same MS MARCO training data, and achieves 37.1 p@1 on the development set. The results in Table 7 show th"
P19-1009,P13-1023,0,0.223892,"ent 70.2 61.9 64.3 76.3 68.3 71.3 Average Pooling Max Pooling AMR 1.0 AMR 2.0 70.2±0.1 70.0±0.1 76.3±0.1 76.2±0.1 Table 6: S MATCH scores based different pooling functions. Standard deviation is over 3 runs on the test data. 8 Conclusion We proposed an attention-based model for AMR parsing where we introduced a series of novel components into a transductive setting that extend beyond what a typical NMT system would do on this task. Our model achieves the best performance on two AMR corpora. For future work, we would like to extend our model to other semantic parsing tasks (Oepen et al., 2014; Abend and Rappoport, 2013). We are also interested in semantic parsing in cross-lingual settings (Zhang et al., 2018; Damonte and Cohen, 2018). Acknowledgments We thank the anonymous reviewers for their valuable feedback. This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE), and DARPA LORELEI and AIDA. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U"
P19-1009,P13-2131,0,0.587459,"Missing"
P19-1009,S16-1186,0,0.080467,"Missing"
P19-1009,P14-1134,0,0.603011,"ioning their incoming edges. Introduction Abstract Meaning Representation (AMR, Banarescu et al., 2013) parsing is the task of transducing natural language text into AMR, a graphbased formalism used for capturing sentence-level semantics. Challenges in AMR parsing include: (1) its property of reentrancy – the same concept can participate in multiple relations – which leads to graphs in contrast to trees (Wang et al., 2015); (2) the lack of gold alignments between nodes (concepts) in the graph and words in the text which limits attempts to rely on explicit alignments to generate training data (Flanigan et al., 2014; Wang et al., 2015; Damonte et al., 2017; Foland and Martin, 2017; Peng et al., 2017b; Groschwitz et al., 2018; Guo and Lu, 2018); and (3) relatively limited amounts of labeled data (Konstas et al., 2017). Recent attempts to overcome these challenges include: modeling alignments as latent variables (Lyu and Titov, 2018); leveraging external semantic resources (Artzi et al., 2015; Bjerva et al., 2016); data augmentation (Konstas et al., 2017; van Noord and Bos, 2017b); and employing attention-based sequence-to-sequence models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos,"
P19-1009,Q16-1023,0,0.0320924,"ning time, we obtain the reference list of nodes and their indices using a pre-order traversal over the reference AMR tree. We also evaluate other traversal strategies, and will discuss their difference in Section 7.2. Edge Prediction Given a input sentence w, a node list u, and indices d, we look for the highest scoring parse tree y in the space Y(u) of valid trees over u with the constraint of d. A parse tree y is a set of directed head-modifier edges y = {(ui , uj ) |1 ≤ i, j ≤ m}. In order to make the search tractable, we follow the arcfactored graph-based approach (McDonald et al., 2005; Kiperwasser and Goldberg, 2016), decomposing the score of a tree to the sum of the score of its head-modifier edges: If we consider the AMR tree with indexed nodes as the prediction target, then our approach to parsing is formalized as a two-stage process: node prediction and edge prediction.1 An example of the parsing process is shown in Figure 2. Node Prediction Given a input sentence w = hw1 , ..., wn i, each wi a word in the sentence, our approach sequentially decodes a list of nodes u = hu1 , ..., um i and deterministically assigns their indices d = hd1 , ..., dm i. P (u) = help himself. Node Prediction Task Formalizat"
P19-1009,P17-1014,0,0.423126,"xternal semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing with Seq2Seq-style models have been considered (Barzdins and Gosko, 2016; Peng et al., 2017b), but are limited by the relatively small amount of labeled AMR data. Konstas et al. (2017) overcame this by making use of millions of unlabeled data through self-training, while van Noord and Bos (2017b) showed significant gains via a characterlevel Seq2Seq model and a large amount of silverstandard AMR training data. In contrast, our approach supported by extended pointer generator can be effectively trained on the limited amount of labeled AMR data, with no data augmentation. Prediction For node prediction, based on the final probability distribution P (node) (ut ) at each decoding time step, we implement both greedy search and beam search to sequentially decode a node list u and"
P19-1009,P17-1043,0,0.0960249,"Missing"
P19-1009,S16-1180,0,0.0359894,"a coverage P loss to penalize repetitive nodes: covlosst = i min(atsrc [i], covt [i]), where covt is the sum of source attention distributions over all previous decoding time steps: covt = P t−1 t0 t0 =0 asrc . See See et al. (2017) for full details. 4.4 quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention. Transition-based approaches began with Wang et al. (2015, 2016), who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015)"
P19-1009,Q16-1037,0,0.0286727,"Missing"
P19-1009,P18-1170,0,0.112378,"Missing"
P19-1009,D18-1264,0,0.269007,"ecoding time steps: covt = P t−1 t0 t0 =0 asrc . See See et al. (2017) for full details. 4.4 quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention. Transition-based approaches began with Wang et al. (2015, 2016), who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing with Seq2Seq-style"
P19-1009,P16-1154,0,0.0549117,"work (See et al., 2017) for node prediction. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016), we found the pointer-generator network very effective at reducing data sparsity in 82 previous node ut−1 (while training, ut−1 is the previous node of the reference node list; at test time it is the previous node emitted by the decoder), and the attentional vector set−1 from the previous step (explained later in this section). sl0 is the concatenation of last encoder hidden states → − ← − from f l and f l respectively. Source attention distribution atsrc is calculated by additive attention (Bahdanau et al., 2014): of generating word-level embe"
P19-1009,D15-1166,0,0.158373,"Missing"
P19-1009,P16-1014,0,0.0153774,"(2018), we extend the pointer-generator network (See et al., 2017) for node prediction. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016), we found the pointer-generator network very effective at reducing data sparsity in 82 previous node ut−1 (while training, ut−1 is the previous node of the reference node list; at test time it is the previous node emitted by the decoder), and the attentional vector set−1 from the previous step (explained later in this section). sl0 is the concatenation of last encoder hidden states → − ← − from f l and f l respectively. Source attention distribution atsrc is calculated by additive attention (Bahdanau et al."
P19-1009,P18-1037,0,0.555127,"can participate in multiple relations – which leads to graphs in contrast to trees (Wang et al., 2015); (2) the lack of gold alignments between nodes (concepts) in the graph and words in the text which limits attempts to rely on explicit alignments to generate training data (Flanigan et al., 2014; Wang et al., 2015; Damonte et al., 2017; Foland and Martin, 2017; Peng et al., 2017b; Groschwitz et al., 2018; Guo and Lu, 2018); and (3) relatively limited amounts of labeled data (Konstas et al., 2017). Recent attempts to overcome these challenges include: modeling alignments as latent variables (Lyu and Titov, 2018); leveraging external semantic resources (Artzi et al., 2015; Bjerva et al., 2016); data augmentation (Konstas et al., 2017; van Noord and Bos, 2017b); and employing attention-based sequence-to-sequence models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017b). In this paper, we introduce a different way to handle reentrancy, and propose an attention-based model that treats AMR parsing as sequence-tograph transduction. The proposed model, supported by an extended pointer-generator network, is aligner-free and can be effectively trained with limited amount of labeled AMR"
P19-1009,P14-5010,0,0.00933403,"Missing"
P19-1009,K15-1004,0,0.0416047,"ransform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing with Seq2Seq-style models have been considered (Barzdins and Gosko, 2016; Peng et al., 2017b), but are limited by the relatively small amount of labeled AMR data. Konstas et al. (2017) overcame this by making use of millions of unlabeled data through self-training, while van Noord and Bos (2017b"
P19-1009,P05-1012,0,0.650777,"f the 57th Annual Meeting of the Association for Computational Linguistics, pages 80–94 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics The ARG1 of “help-01”. While efforts have gone into developing graph-based algorithms for AMR parsing (Chiang et al., 2013; Flanigan et al., 2014), it is more challenging to parse a sentence into an AMR graph rather than a tree as there are efficient off-the-shelf tree-based algorithms, e.g., Chu and Liu (1965); Edmonds (1968). To leverage these tree-based algorithms as well as other structured prediction paradigms (McDonald et al., 2005), we introduce another view of reentrancy. AMR reentrancy is employed when a node participates in multiple semantic relations. We convert an AMR graph into a tree by duplicating nodes that have reentrant relations; that is, whenever a node has a reentrant relation, we make a copy of that node and use the copy to participate in the relation, thereby resulting in a tree. Next, in order to preserve the reentrancy information, we add an extra layer of annotation by assigning an index to each node. Duplicated nodes are assigned the same index as the original node. Figure 1(b) shows a resultant AMR"
P19-1009,E17-1035,0,0.658672,"et al., 2013) parsing is the task of transducing natural language text into AMR, a graphbased formalism used for capturing sentence-level semantics. Challenges in AMR parsing include: (1) its property of reentrancy – the same concept can participate in multiple relations – which leads to graphs in contrast to trees (Wang et al., 2015); (2) the lack of gold alignments between nodes (concepts) in the graph and words in the text which limits attempts to rely on explicit alignments to generate training data (Flanigan et al., 2014; Wang et al., 2015; Damonte et al., 2017; Foland and Martin, 2017; Peng et al., 2017b; Groschwitz et al., 2018; Guo and Lu, 2018); and (3) relatively limited amounts of labeled data (Konstas et al., 2017). Recent attempts to overcome these challenges include: modeling alignments as latent variables (Lyu and Titov, 2018); leveraging external semantic resources (Artzi et al., 2015; Bjerva et al., 2016); data augmentation (Konstas et al., 2017; van Noord and Bos, 2017b); and employing attention-based sequence-to-sequence models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017b). In this paper, we introduce a different way to handle reentrancy, and propose"
P19-1009,D14-1162,0,0.0825124,"stributions as well as the vocabulary distribution are weighted by these probabilities respectively, and then summed to obtain the final distribution, from which we make our prediction. Best viewed in color. 4 Model AMR parsing, which will be shown in Section 7.2. As depicted in Figure 3, the extended pointergenerator network consists of four major components: an encoder embedding layer, an encoder, a decoder embedding layer, and a decoder. Encoder Embedding Layer This layer converts words in input sentences into vector representations. Each vector is the concatenation of embeddings of GloVe (Pennington et al., 2014), BERT (Devlin et al., 2018), POS (part-of-speech) tags and anonymization indicators, and features learned by a character-level convolutional neural network (CharCNN, Kim et al., 2016). Anonymization indicators are binary indicators that tell the encoder whether the word is an anonymized word. In preprocessing, text spans of named entities in input sentences will be replaced by anonymized tokens (e.g. person, country) to reduce sparsity (see the Appendix for details). Except BERT, all other embeddings are fetched from their corresponding learned embedding lookup tables. BERT takes subword unit"
P19-1009,D16-1031,0,0.0267743,"2017) for node prediction. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016), we found the pointer-generator network very effective at reducing data sparsity in 82 previous node ut−1 (while training, ut−1 is the previous node of the reference node list; at test time it is the previous node emitted by the decoder), and the attentional vector set−1 from the previous step (explained later in this section). sl0 is the concatenation of last encoder hidden states → − ← − from f l and f l respectively. Source attention distribution atsrc is calculated by additive attention (Bahdanau et al., 2014): of generating word-level embeddings from BERT. BERT E"
P19-1009,D14-1048,0,0.351871,"ibutions over all previous decoding time steps: covt = P t−1 t0 t0 =0 asrc . See See et al. (2017) for full details. 4.4 quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention. Transition-based approaches began with Wang et al. (2015, 2016), who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing"
P19-1009,K16-1028,0,0.0170043,"on. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016), we found the pointer-generator network very effective at reducing data sparsity in 82 previous node ut−1 (while training, ut−1 is the previous node of the reference node list; at test time it is the previous node emitted by the decoder), and the attentional vector set−1 from the previous step (explained later in this section). sl0 is the concatenation of last encoder hidden states → − ← − from f l and f l respectively. Source attention distribution atsrc is calculated by additive attention (Bahdanau et al., 2014): of generating word-level embeddings from BERT. BERT Embeddings Average Pooling"
P19-1009,D15-1136,0,0.151725,"dman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing with Seq2Seq-style models have been considered (Barzdins and Gosko, 2016; Peng et al., 2017b), but are limited by the relatively small amount of labeled AMR data. Konstas et al. (2017) overcame this by making use of millions of unlabeled data through self-training, while van Noord and Bos (2017b) showed significant gains via a characterlevel Seq2Seq model and a large amount of silverstandard AMR training data. In contrast, our approach supported by extende"
P19-1009,P19-1451,0,0.33754,"Missing"
P19-1009,W17-7306,0,0.304179,"Missing"
P19-1009,S16-1178,0,0.0176029,") + log Pk,t (l) + λcovlosst ] 84 covlosst is a coverage P loss to penalize repetitive nodes: covlosst = i min(atsrc [i], covt [i]), where covt is the sum of source attention distributions over all previous decoding time steps: covt = P t−1 t0 t0 =0 asrc . See See et al. (2017) for full details. 4.4 quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention. Transition-based approaches began with Wang et al. (2015, 2016), who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches convertin"
P19-1009,S14-2008,0,0.106925,"lignment Pure Alignment 70.2 61.9 64.3 76.3 68.3 71.3 Average Pooling Max Pooling AMR 1.0 AMR 2.0 70.2±0.1 70.0±0.1 76.3±0.1 76.2±0.1 Table 6: S MATCH scores based different pooling functions. Standard deviation is over 3 runs on the test data. 8 Conclusion We proposed an attention-based model for AMR parsing where we introduced a series of novel components into a transductive setting that extend beyond what a typical NMT system would do on this task. Our model achieves the best performance on two AMR corpora. For future work, we would like to extend our model to other semantic parsing tasks (Oepen et al., 2014; Abend and Rappoport, 2013). We are also interested in semantic parsing in cross-lingual settings (Zhang et al., 2018; Damonte and Cohen, 2018). Acknowledgments We thank the anonymous reviewers for their valuable feedback. This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE), and DARPA LORELEI and AIDA. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or en"
P19-1009,P17-1099,0,0.0359169,"tiple hidden states of BERT. In order to accurately use these hidden states to represent each word, we apply an average pooling function to the outputs of BERT. Figure 4 illustrates the process Our model has two main modules: (1) an extended pointer-generator network for node prediction; and (2) a deep biaffine classifier for edge prediction. The two modules correspond to the two-stage process for AMR parsing, and they are jointly learned during training. 4.1 Extended Pointer-Generator Network Inspired by the self-copy mechanism in Zhang et al. (2018), we extend the pointer-generator network (See et al., 2017) for node prediction. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao a"
P19-1009,P17-1186,0,0.144991,"Missing"
P19-1009,S16-1181,0,0.202215,"Missing"
P19-1009,D17-1129,0,0.450804,"Missing"
P19-1009,N15-1040,0,0.385945,"himself.” (a) A standard AMR graph. (b) An AMR tree with node indices as an extra layer of annotation, where the corresponding graph can be recovered by merging nodes of the same index and unioning their incoming edges. Introduction Abstract Meaning Representation (AMR, Banarescu et al., 2013) parsing is the task of transducing natural language text into AMR, a graphbased formalism used for capturing sentence-level semantics. Challenges in AMR parsing include: (1) its property of reentrancy – the same concept can participate in multiple relations – which leads to graphs in contrast to trees (Wang et al., 2015); (2) the lack of gold alignments between nodes (concepts) in the graph and words in the text which limits attempts to rely on explicit alignments to generate training data (Flanigan et al., 2014; Wang et al., 2015; Damonte et al., 2017; Foland and Martin, 2017; Peng et al., 2017b; Groschwitz et al., 2018; Guo and Lu, 2018); and (3) relatively limited amounts of labeled data (Konstas et al., 2017). Recent attempts to overcome these challenges include: modeling alignments as latent variables (Lyu and Titov, 2018); leveraging external semantic resources (Artzi et al., 2015; Bjerva et al., 2016);"
P19-1009,P15-1095,0,0.110026,"Missing"
P19-1009,D18-1194,1,0.889516,"Missing"
P19-1009,D16-1065,0,0.401352,"X score(ui , uj ) y∈Y(u) (u ,u )∈y i j Based on the scores of the edges, the highest scoring parse tree (i.e., maximum spanning arborescence) can be efficiently found using the Chu-Liu-Edmonnds algorithm. We further incorporate indices as constraints in the algorithm, which is described in Section 4.4. After obtaining the parse tree, we merge identically indexed nodes to recover the standard AMR graph. P (ui |u&lt;i , d&lt;i , w) i=1 Note that we allow the same node to occur multi1 The two-stage process is similar to “concept identification” and “relation identification” in Flanigan et al. (2014); Zhou et al. (2016); Lyu and Titov (2018); inter alia. 81 Final Distribution &lt;latexit sha1_base64=""LBJIGmiRa8y7fNwyIgzxQ26kqoY="">AAAB9XicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi2GPBi8cK9gPaWDbbTbt0dxN2J2oJ/R9ePCji1f/izX/jts1BWx8MPN6bYWZemAhu0PO+ncLa+sbmVnG7tLO7t39QPjxqmTjVlDVpLGLdCYlhgivWRI6CdRLNiAwFa4fj65nffmDa8Fjd4SRhgSRDxSNOCVrpPun3kD2hlpnRdNovV7yqN4e7SvycVCBHo1/+6g1imkqmkApiTNf3EgwyopFTwaalXmpYQuiYDFnXUkUkM0E2v3rqnlll4EaxtqXQnau/JzIijZnI0HZKgiOz7M3E/7xuilEtyLhKUmSKLhZFqXAxdmcRuAOuGUUxsYRQze2tLh0RTSjaoEo2BH/55VXSuqj6XtW/vazUa3kcRTiBUzgHH66gDjfQgCZQ0PAMr/DmPDovzrvzsWgtOPnMMfyB8/kDY1qTCw==&lt;/latexit> &lt;latexit sha1_base64=""R"
P19-1009,P13-1091,0,\N,Missing
P19-1009,D15-1198,0,\N,Missing
P19-1009,W13-2322,0,\N,Missing
P19-1009,S16-1179,0,\N,Missing
P19-1009,S16-1182,0,\N,Missing
P19-1009,N18-1104,0,\N,Missing
P19-1009,P18-2077,0,\N,Missing
P19-1009,D18-1198,0,\N,Missing
P19-1009,E17-1051,0,\N,Missing
Q17-1027,S12-1051,0,0.016845,"coin. H: That flip comes up heads. No human reading T should infer that H is true. A model trained to make ordinal predictions should say: “plausible, with probability 1.0”, whereas a model trained to make binary entailed/not-entailed predictions should say: “not entailed, with probability 1.0”. The following example exhibits the same property: T: An animal eats food. H: A person eats food. Again, with high confidence, H is plausible; and, with high confidence, it is also not entailed. Non-entailing Inference Of the various non“entailment” textual inference tasks, a few are most salient here. Agirre et al. (2012) piloted a Textual Similarity evaluation which has been refined in subsequent years. Systems produce scalar values corresponding to predictions of how similar the meaning is between two provided sentences, e.g., the following pair from SICK was judged very similar (4.2 out of 5), while also being a contradiction: There is no biker jumping in the air and A lone biker is jumping in the air. The ordinal approach we advocate for relies on a graded notion, like textual similarity. The Choice of Plausible Alternative (COPA) task (Roemmele et al., 2011) was a reaction to RTE, similarly motivated to p"
Q17-1027,P98-1013,0,0.0627839,"se inference examples, judged to hold with varying levels of subjective likelihood (§5). We provide baseline results (§6) for prediction on the JOCI corpus.5 4 For further background see discussions by Van Durme (2010), Gordon and Van Durme (2013), Rudinger et al. (2015) and Misra et al. (2016). 5 The JOCI corpus is released freely at: http://decomp. net/. 380 2 Background Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovere"
Q17-1027,P11-1062,0,0.00914887,"ning Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge"
Q17-1027,D15-1075,0,0.196309,"rk et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge, e.g., the Winograd Schema Challenge discussed by Levesque et al. (2011). The data for these tasks are either smaller, carefully constructed evaluation sets by professionals, following efforts like the F RAC A S test suite (Cooper et al., 1996), or they rely on crowdsourced elicitation (Bowman et al., 2015). Crowdsourcing is scalable, but elicitation protocols can lead to biased responses unlikely to contain a wide range of possible common-sense inferences. Humans can generally agree on the plausibility of a wide range of possible inference pairs, but they are not likely to generate them from an initial prompt.6 The construction of SICK (Sentences Involving Compositional Knowledge) made use of existing paraphrastic sentence pairs (descriptions by differ6 McRae et al. (2005): Features such as &lt;is larger than a tulip&gt; or &lt;moves faster than an infant&gt;, for example; although logically possible, do n"
Q17-1027,P08-1090,0,0.0281911,"ere is no biker jumping in the air and A lone biker is jumping in the air. The ordinal approach we advocate for relies on a graded notion, like textual similarity. The Choice of Plausible Alternative (COPA) task (Roemmele et al., 2011) was a reaction to RTE, similarly motivated to probe a system’s ability to understand inferences that are not strictly entailed. A single context was provided, with two alternative inferences, and a system had to judge which was more plausible. The COPA dataset was manually elicited, and is not large; we discuss this data further in §5. The Narrative Cloze task (Chambers and Jurafsky, 2008) requires a system to score candidate inferences as to how likely they are to appear in a document that also included the provided context. Many such inferences are then not strictly entailed by the context. Further, the Cloze task gives the benefit of being able to generate very large numbers of examples automatically by simply occluding parts of existing documents and asking a system to predict what is missing. The LAMBADA dataset (Paperno et al., 2016) is akin to our strategy for automatic generation followed by human filtering, but for Cloze examples. As our concern is with inferences that"
Q17-1027,W99-0631,0,0.0911746,"ugh the measurement of human reading times when using prompts derived from our ordinal common-sense inference examples. Computational modeling of (unconditional) semantic plausibility has been explored by those such as Pad´o et al. (2009), Erk et al. (2010) and Sayeed et al. (2015). Textual Entailment A multi-year source of textual inference examples were generated under the Recognizing Textual Entailment (RTE) Challenges, introduced by Dagan et al. (2006): 7 This notion of thematic plausibility is then related to the notion of verb-argument selectional preference (Zernik, 1992; Resnik, 1993; Clark and Weir, 1999), and sortal (in)correctness (Thomason, 1972). 8 Thanks to the anonymous reviewer for this connection. 381 We say that T entails H if, typically, a human reading T would infer that H is most likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge. This definition strayed from the more strict notion of entailment as used by linguistic semanticists, such as those involved with F RAC A S. While Giampiccolo et al. (2008) extended binary RTE with an “unknown” category, the entailment community has primari"
Q17-1027,W03-0901,0,0.154374,"2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge, e.g., the Winograd Schema Challenge discussed by Levesque et al. (2011). The data for these tasks are either smaller, carefully constructed evaluation sets by professionals, following efforts like the F RAC A S test suite (Cooper et al., 1996), or they rely on crowdsourced elicitation (Bowman et al.,"
Q17-1027,de-marneffe-etal-2014-universal,0,0.0235901,"Missing"
Q17-1027,J10-4007,0,0.103965,"Missing"
Q17-1027,W11-0112,0,0.0204984,"ce, i.e. the new posterior on the world after reading the sentence. A new sentence in a discourse is almost never entailed by another sentence in the discourse, because such a sentence would add no new information. In order to successfully process a discourse, there needs to be some understanding of what new information can be, possibly or plausibly, added to the discourse. Collecting sentence pairs with ordinal entailment connections is potentially useful for improving and testing these language understanding capabilities that would be needed by algorithms for applications like storytelling. Garrette et al. (2011) and Beltagy et al. (2017) treated textual entailment as probabilistic logical inference in Markov Logic Networks (Richardson and Domingos, 2006). However, the notion of probability in their entailment task has a subtle distinction from our problem of common-sense inference. The probability of being an entailment given by a probabilistic model trained for a binary classification (being an entailment or not) is not necessarily the same as the likelihood of an inference being true. For example: T: A person flips a coin. H: That flip comes up heads. No human reading T should infer that H is true."
Q17-1027,S13-1036,0,0.0204765,"for ordinal regression model on A-test and B-test. (*p-value&lt;.01 for ρ) We also run a feature ablation test. Table 6 shows that the most useful features differ for Atest and B-test. On A-test, where the inferences are elicited from humans, removal of similarity- and bow-based features together results in the largest performance drop. On B-test, by contrast, removing similarity and bow features results in a comtions in ordinal prediction tasks (Baccianella et al., 2009; Bennett and Lanning, 2007; Gaudette and Japkowicz, 2009; Agresti and Kateri, 2011; Popescu and Dinu, 2009; Liu et al., 2015; Gella et al., 2013). parable performance drop to removing seq2seq features. These observations point to statistical differences between human-elicited and auto-generated hypotheses, a motivating point of the JOCI corpus. 7 Conclusions and Future Work In motivating the need for automatically building collections of common-sense knowledge, Clark et al. (2003) wrote: Acknowledgments Thank you to action editor Mark Steedman and the anonymous reviewers for their feedback, as well as colleagues including Lenhart Schubert, Kyle Rawlins, Aaron White, and Keisuke Sakaguchi. This work was supported in part by DARPA LORELE"
Q17-1027,C92-2082,0,0.222733,"(2016). 5 The JOCI corpus is released freely at: http://decomp. net/. 380 2 Background Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks"
Q17-1027,T87-1006,0,0.349843,".2 Common-sense inference – inferences based on common-sense knowledge – is possibilistic: things everyone more or less would expect to hold in a given context, but without the necessary strength of logical entailment.3 Because natural language corpora exhibits human reporting bias (Gordon and Van Durme, 2013), systems that derive knowledge exclusively from such corpora may be more accurately considered models of language, rather than of the Introduction We use words to talk about the world. Therefore, to understand what words mean, we must have a prior explication of how we view the world. – Hobbs (1987) Researchers in Artificial Intelligence and (Computational) Linguistics have long-cited the requirement of common-sense knowledge in language understanding.1 This knowledge is viewed as a key 1 Schank (1975): It has been apparent ... within ... natural language understanding ... that the eventual limit to our solution ... would be our ability to characterize world knowledge. 2 McCarthy (1959): a program has common sense if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows. 3 Many of the bridging inferences o"
Q17-1027,C02-1105,0,0.00837765,"pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge, e.g., the Winograd Schema Challenge discussed by Levesque et al. (2011). The data for these tasks are either smaller, carefully constructed evaluation sets by professionals, following efforts like the F RAC A S test suite (Cooper et al., 1996), or they rely on crowdsourced elicitat"
Q17-1027,P15-1145,0,0.0153587,": Ablation results for ordinal regression model on A-test and B-test. (*p-value&lt;.01 for ρ) We also run a feature ablation test. Table 6 shows that the most useful features differ for Atest and B-test. On A-test, where the inferences are elicited from humans, removal of similarity- and bow-based features together results in the largest performance drop. On B-test, by contrast, removing similarity and bow features results in a comtions in ordinal prediction tasks (Baccianella et al., 2009; Bennett and Lanning, 2007; Gaudette and Japkowicz, 2009; Agresti and Kateri, 2011; Popescu and Dinu, 2009; Liu et al., 2015; Gella et al., 2013). parable performance drop to removing seq2seq features. These observations point to statistical differences between human-elicited and auto-generated hypotheses, a motivating point of the JOCI corpus. 7 Conclusions and Future Work In motivating the need for automatically building collections of common-sense knowledge, Clark et al. (2003) wrote: Acknowledgments Thank you to action editor Mark Steedman and the anonymous reviewers for their feedback, as well as colleagues including Lenhart Schubert, Kyle Rawlins, Aaron White, and Keisuke Sakaguchi. This work was supported in"
Q17-1027,W07-1431,0,0.0403768,"entails H if, typically, a human reading T would infer that H is most likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge. This definition strayed from the more strict notion of entailment as used by linguistic semanticists, such as those involved with F RAC A S. While Giampiccolo et al. (2008) extended binary RTE with an “unknown” category, the entailment community has primarily focused on issues such as “paraphrase” and “monotonicity”. An example of this is the Natural Logic implementation of MacCartney and Manning (2007). Language understanding in context is not only understanding the entailments of a sentence, but also the plausible inferences of the sentence, i.e. the new posterior on the world after reading the sentence. A new sentence in a discourse is almost never entailed by another sentence in the discourse, because such a sentence would add no new information. In order to successfully process a discourse, there needs to be some understanding of what new information can be, possibly or plausibly, added to the discourse. Collecting sentence pairs with ordinal entailment connections is potentially useful"
Q17-1027,marelli-etal-2014-sick,0,0.0295091,"ce pairs, but they are not likely to generate them from an initial prompt.6 The construction of SICK (Sentences Involving Compositional Knowledge) made use of existing paraphrastic sentence pairs (descriptions by differ6 McRae et al. (2005): Features such as &lt;is larger than a tulip&gt; or &lt;moves faster than an infant&gt;, for example; although logically possible, do not occur in [human responses] [...] Although people are capable of verifying that a &lt;dog is larger than a pencil&gt;. ent people of the same image), which were modified through a series of rule-based transformations then judged by humans (Marelli et al., 2014). As with SICK, we rely on humans only for judging provided examples, rather than elicitation of text. Unlike SICK, our generation is based on a process targeted specifically at common sense (see §4.1.1). Plausibility Researchers in psycholinguistics have explored a notion of plausibility in human sentence processing, where, for instance, arguments to predicates are intuitively more or less “plausible” as fillers to different thematic roles, as reflected in human reading times. For example, McRae et al. (1998) looked at manipulations such as: (a) The boss hired by the corporation was perfect f"
Q17-1027,H92-1116,0,0.627619,"Missing"
Q17-1027,N16-1098,0,0.0825906,"lso included the provided context. Many such inferences are then not strictly entailed by the context. Further, the Cloze task gives the benefit of being able to generate very large numbers of examples automatically by simply occluding parts of existing documents and asking a system to predict what is missing. The LAMBADA dataset (Paperno et al., 2016) is akin to our strategy for automatic generation followed by human filtering, but for Cloze examples. As our concern is with inferences that are often true but never stated in a document, this approach is not viable here. The ROCStories corpus (Mostafazadeh et al., 2016) elicited a more “plausible” collection of documents in order to retain the narrative Cloze in the context of common-sense inference. The ROCStories corpus can be viewed as an extension of the idea behind 382 the COPA corpus, done at a larger scale with crowdsourcing, and with multi-sentence contexts; we consider this dataset in §5. Alongside the narrative Cloze, Pichotta and Mooney (2016) made use of a 5-point Likert scale (very likely to very unlikely) as a secondary evaluation of various script induction techniques. While they were concerned with measuring their ability to generate very lik"
Q17-1027,N07-1071,0,0.0310536,"sponding argument with a placeholder, similar to Van Durme et al. (2009) (see Fig 3 (b)). We remove any template associated with a sense if it occurs less than two times for that sense, 12 https://pypi.python.org/pypi/ PyStanfordDependencies 13 Using English glosses of the logical representations, abstraction of “a long, dark corridor” would yield “corridor” for example; “a small office at the end of a long dark corridor” would yield “office”; and “Mrs. MacReady” would yield “person”. See Schubert (2002) for detail. 14 In order to avoid too general senses, we set cut points at the depth of 4 (Pantel et al., 2007) to truncate the hierarchy and consider all 81,861 senses below these points. 384 leaving 38 million unique templates. (c) Deriving properties via WordNet: At this step, we want to associate with each WordNet sense a set of possible properties. We employ three strategies. The first strategy is to use a decision tree to pick out highly discriminative properties for each WordNet sense. Specifically, for each set of cohyponyms,15 we train a decision tree using the associated templates as features. For example, in Fig 3 (c), we train a decision tree over the cohyponyms of publication.n.01. Then th"
Q17-1027,P16-1144,0,0.0232549,"re plausible. The COPA dataset was manually elicited, and is not large; we discuss this data further in §5. The Narrative Cloze task (Chambers and Jurafsky, 2008) requires a system to score candidate inferences as to how likely they are to appear in a document that also included the provided context. Many such inferences are then not strictly entailed by the context. Further, the Cloze task gives the benefit of being able to generate very large numbers of examples automatically by simply occluding parts of existing documents and asking a system to predict what is missing. The LAMBADA dataset (Paperno et al., 2016) is akin to our strategy for automatic generation followed by human filtering, but for Cloze examples. As our concern is with inferences that are often true but never stated in a document, this approach is not viable here. The ROCStories corpus (Mostafazadeh et al., 2016) elicited a more “plausible” collection of documents in order to retain the narrative Cloze in the context of common-sense inference. The ROCStories corpus can be viewed as an extension of the idea behind 382 the COPA corpus, done at a larger scale with crowdsourcing, and with multi-sentence contexts; we consider this dataset"
Q17-1027,R09-1063,0,0.0142342,".00 .00 .00 .05 Table 6: Ablation results for ordinal regression model on A-test and B-test. (*p-value&lt;.01 for ρ) We also run a feature ablation test. Table 6 shows that the most useful features differ for Atest and B-test. On A-test, where the inferences are elicited from humans, removal of similarity- and bow-based features together results in the largest performance drop. On B-test, by contrast, removing similarity and bow features results in a comtions in ordinal prediction tasks (Baccianella et al., 2009; Bennett and Lanning, 2007; Gaudette and Japkowicz, 2009; Agresti and Kateri, 2011; Popescu and Dinu, 2009; Liu et al., 2015; Gella et al., 2013). parable performance drop to removing seq2seq features. These observations point to statistical differences between human-elicited and auto-generated hypotheses, a motivating point of the JOCI corpus. 7 Conclusions and Future Work In motivating the need for automatically building collections of common-sense knowledge, Clark et al. (2003) wrote: Acknowledgments Thank you to action editor Mark Steedman and the anonymous reviewers for their feedback, as well as colleagues including Lenhart Schubert, Kyle Rawlins, Aaron White, and Keisuke Sakaguchi. This wor"
Q17-1027,H93-1054,0,0.106892,", perhaps through the measurement of human reading times when using prompts derived from our ordinal common-sense inference examples. Computational modeling of (unconditional) semantic plausibility has been explored by those such as Pad´o et al. (2009), Erk et al. (2010) and Sayeed et al. (2015). Textual Entailment A multi-year source of textual inference examples were generated under the Recognizing Textual Entailment (RTE) Challenges, introduced by Dagan et al. (2006): 7 This notion of thematic plausibility is then related to the notion of verb-argument selectional preference (Zernik, 1992; Resnik, 1993; Clark and Weir, 1999), and sortal (in)correctness (Thomason, 1972). 8 Thanks to the anonymous reviewer for this connection. 381 We say that T entails H if, typically, a human reading T would infer that H is most likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge. This definition strayed from the more strict notion of entailment as used by linguistic semanticists, such as those involved with F RAC A S. While Giampiccolo et al. (2008) extended binary RTE with an “unknown” category, the entailmen"
Q17-1027,P98-2180,0,0.106775,"mon-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge, e.g., the Winograd Schema Challenge discussed by Lev"
Q17-1027,D15-1195,1,0.837784,"Missing"
Q17-1027,T75-2023,0,0.585373,"entailment.3 Because natural language corpora exhibits human reporting bias (Gordon and Van Durme, 2013), systems that derive knowledge exclusively from such corpora may be more accurately considered models of language, rather than of the Introduction We use words to talk about the world. Therefore, to understand what words mean, we must have a prior explication of how we view the world. – Hobbs (1987) Researchers in Artificial Intelligence and (Computational) Linguistics have long-cited the requirement of common-sense knowledge in language understanding.1 This knowledge is viewed as a key 1 Schank (1975): It has been apparent ... within ... natural language understanding ... that the eventual limit to our solution ... would be our ability to characterize world knowledge. 2 McCarthy (1959): a program has common sense if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows. 3 Many of the bridging inferences of Clark (1975) make use of common-sense knowledge, such as the following example of “Probable part”: I walked into the room. The windows looked out to the bay. To resolve the definite reference the windows,"
Q17-1027,P06-1101,0,0.0142468,"JOCI corpus is released freely at: http://decomp. net/. 380 2 Background Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual infere"
Q17-1027,W08-2219,1,0.802414,"Missing"
Q17-1027,E09-1092,1,0.874567,"Missing"
Q17-1027,D16-1177,1,0.0444107,"Missing"
Q17-1027,Q14-1006,0,0.0321308,") and ROCStories (Mostafazadeh et al., 2016), paired with hypotheses generated via methods described in §4.1. These pairs are then annotated with ordinal labels using crowdsourcing (§4.2). We also include context-hypothesis pairs directly taken from SNLI and other corpora (e.g., as premise-hypothesis pairs), and re-annotate them with ordinal labels. 5.1 Data sources for Context-Hypothesis Pairs In order to compare with existing inference corpora, we choose contexts from two resources: (1) the first sentence in the sentence pairs of the SNLI corpus which are captions from the Flickr30k corpus (Young et al., 2014), and (2) the first sentence in the stories of the ROCStories corpus. We then collect candidates of automatically generated common-sense inferences (AGCI) against these contexts. Specifically, in the SNLI train set, there are over 150K different first sentences, involving 7,414 different arguments according to predicate-argument extraction. We randomly choose 4,600 arguments. For each argument, we sample one first sentence that has the argument and collect candidates of AGCI against this as context. We also do the same generation for the SNLI development set and test set. We also collect candi"
Q17-1027,C92-4212,0,0.0392934,"al future work, perhaps through the measurement of human reading times when using prompts derived from our ordinal common-sense inference examples. Computational modeling of (unconditional) semantic plausibility has been explored by those such as Pad´o et al. (2009), Erk et al. (2010) and Sayeed et al. (2015). Textual Entailment A multi-year source of textual inference examples were generated under the Recognizing Textual Entailment (RTE) Challenges, introduced by Dagan et al. (2006): 7 This notion of thematic plausibility is then related to the notion of verb-argument selectional preference (Zernik, 1992; Resnik, 1993; Clark and Weir, 1999), and sortal (in)correctness (Thomason, 1972). 8 Thanks to the anonymous reviewer for this connection. 381 We say that T entails H if, typically, a human reading T would infer that H is most likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge. This definition strayed from the more strict notion of entailment as used by linguistic semanticists, such as those involved with F RAC A S. While Giampiccolo et al. (2008) extended binary RTE with an “unknown” category,"
Q17-1027,E17-2011,1,0.766207,"Missing"
S10-1086,D07-1050,1,0.257026,"D) task (Task 16 (Okumura et al., 2010)), which has two new characteristics: (1) Both training and test data across 3 or 4 domains. The training data include books or magazines (called PB), newspaper articles (PN), and white papers (OW). The test data also include documents from a Q&A site on the WWW (OC); (2) Test data include new senses (called X) that are not defined in dictionary. There is much previous research on WSD. In the case of Japanese, unsupervised approaches such as extended Lesk have performed well (Baldwin et al., 2010), although they are outperformed by supervised approaches (Tanaka et al., 2007; Murata et al., 2003). Therefore, we selected a supervised approach and constructed Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers using common features and topic features. We performed extensive experiments to investigate the best combinations of domains for training. We describe the data in Section 2, and our system in Section 3. Then in Section 4, we show the results and provide some discussion. 2.2 Data Pre-processing We performed two preliminary pre-processing steps. First, we restored the base forms because the given training and test data have no information about"
S10-1086,W10-4001,0,0.0904494,"data. We participated in the SemEval-2010 Japanese Word Sense Disambiguation (WSD) task (Task 16 (Okumura et al., 2010)), which has two new characteristics: (1) Both training and test data across 3 or 4 domains. The training data include books or magazines (called PB), newspaper articles (PN), and white papers (OW). The test data also include documents from a Q&A site on the WWW (OC); (2) Test data include new senses (called X) that are not defined in dictionary. There is much previous research on WSD. In the case of Japanese, unsupervised approaches such as extended Lesk have performed well (Baldwin et al., 2010), although they are outperformed by supervised approaches (Tanaka et al., 2007; Murata et al., 2003). Therefore, we selected a supervised approach and constructed Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers using common features and topic features. We performed extensive experiments to investigate the best combinations of domains for training. We describe the data in Section 2, and our system in Section 3. Then in Section 4, we show the results and provide some discussion. 2.2 Data Pre-processing We performed two preliminary pre-processing steps. First, we restored the"
S10-1086,D07-1108,0,\N,Missing
S10-1086,S07-1053,0,\N,Missing
S10-1086,S10-1012,0,\N,Missing
S18-2017,W09-1704,0,0.0193066,"tanh(ct ) ∈ (−1, 1)D . Therefore, extra work is needed to ensure h0t ∈ (−1, 1)D . For this purpose, we follow the recipe3 : • Sample h00t ∈ (−1, 1)D by independently sampling each entry from an uniform distribution over (−1, 1); • Sample a scalar λt ∈ (0, 1) from a Beta distribution B(α, β) where α and β are hyperparameters to be tuned; • Compute h0t = ht + λt (h00t − ht ) such that h0t ∈ (−1, 1)D lies on the line segment between ht and h00t . 4 Related Work Cross-lingual information extraction has drawn a great deal of attention from researchers. Some (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji and Nothman, 2016) worked in closed domains, i.e. on a predefined set of events and/or entities, Zhang et al. (2017b) explored this problem in open domain and their attentional encoder-decoder model significantly outperformed a baseline system that does translation and parsing in a pipeline. Zhang et al. (2017c) further improved the results by inventing a hierarchical architecture that learns to first predict the next semantic structure tag and then select a tagdependent decoder for token generation. Orthogonal to these efforts, Halo aims to help all neural models on"
S18-2017,P07-2045,0,0.00961903,"Missing"
S18-2017,D15-1166,0,0.0628326,"016). Halo differs from these methods because 1) it makes use of the task-specific information— vocabulary is partitioned by semantic structure tags; and 2) it makes use of the human belief that the hidden representations of tokens with the same semantic structure tag should stay close to each other. Some 5 ers are low resourced, meaning they have much fewer samples and lower token/type ratios. 5.2 Before applying our Halo technique, we first improved the current state-of-the-art neural model of Zhang et al. (2017c) by using residual connections (He et al., 2016) and multiplicative attention (Luong et al., 2015), which effectively improved the model performance. We refer to the model of Zhang et al. (2017c) and our improved version as ModelZ and ModelP respectively5 . Experiments We evaluate our method on several real-world CLIE datasets measured by BLEU (Papineni et al., 2002) and F1, as proposed by Zhang et al. (2017b). For the generated linearized PredPatt outputs and their references, the former metric4 measures their n-gram similarity, and the latter measures their token-level overlap. In fact, F1 is computed separately for predicate and argument, as F1 P RED and F1 A RG respectively. 5.1 Model"
S18-2017,P02-1040,0,0.102246,"should stay close to each other. Some 5 ers are low resourced, meaning they have much fewer samples and lower token/type ratios. 5.2 Before applying our Halo technique, we first improved the current state-of-the-art neural model of Zhang et al. (2017c) by using residual connections (He et al., 2016) and multiplicative attention (Luong et al., 2015), which effectively improved the model performance. We refer to the model of Zhang et al. (2017c) and our improved version as ModelZ and ModelP respectively5 . Experiments We evaluate our method on several real-world CLIE datasets measured by BLEU (Papineni et al., 2002) and F1, as proposed by Zhang et al. (2017b). For the generated linearized PredPatt outputs and their references, the former metric4 measures their n-gram similarity, and the latter measures their token-level overlap. In fact, F1 is computed separately for predicate and argument, as F1 P RED and F1 A RG respectively. 5.1 Model Implementation 5.3 Experimental Details In experiments, instead of using the full vocabularies shown in table 1, we set a minimum count threshold for each dataset, to replace the rare words by a special out-of-vocabulary symbol. These thresholds were tuned on dev sets. T"
S18-2017,P09-1048,0,0.01758,"Missing"
S18-2017,D16-1177,1,0.847053,"Missing"
S18-2017,E17-2011,1,0.885386,"Missing"
S18-2017,W11-1215,0,0.0126728,"∈ (−1, 1)D . Therefore, extra work is needed to ensure h0t ∈ (−1, 1)D . For this purpose, we follow the recipe3 : • Sample h00t ∈ (−1, 1)D by independently sampling each entry from an uniform distribution over (−1, 1); • Sample a scalar λt ∈ (0, 1) from a Beta distribution B(α, β) where α and β are hyperparameters to be tuned; • Compute h0t = ht + λt (h00t − ht ) such that h0t ∈ (−1, 1)D lies on the line segment between ht and h00t . 4 Related Work Cross-lingual information extraction has drawn a great deal of attention from researchers. Some (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji and Nothman, 2016) worked in closed domains, i.e. on a predefined set of events and/or entities, Zhang et al. (2017b) explored this problem in open domain and their attentional encoder-decoder model significantly outperformed a baseline system that does translation and parsing in a pipeline. Zhang et al. (2017c) further improved the results by inventing a hierarchical architecture that learns to first predict the next semantic structure tag and then select a tagdependent decoder for token generation. Orthogonal to these efforts, Halo aims to help all neural models on this task, rather tha"
S18-2017,I17-1084,1,0.828392,"Missing"
S18-2017,L16-1521,0,0.0664641,"thresholds were tuned on dev sets. The Beta distribution is very flexible. In general, its variance is a decreasing function of α + β, and when α + β is fixed, the mean is an increasing function of α. In our experiments, we fixed α + β = 20 and only lightly tuned α on dev sets. Optimal values of α stay close to 1. Datasets Multiple datasets were used to demonstrate the effectiveness of our proposed method, where one sample in each dataset is a source language sentence paired with its linearized English PredPatt output. These datasets were first introduced as the DARPA LORELEI Language Packs (Strassel and Tracey, 2016), and then used for this task by Zhang et al. (2017b,c). As shown in table 1, the C HINESE dataset has almost one million training samples and a high token/type ratio, while the oth5.4 Results As shown in Table 2, ModelP outperforms ModelZ on all the datasets measured by all the metrics, except for F1 P RED on C HINESE dataset. Our Halo technique consistently boosts the model performance of M ODEL P except for F1 P RED on T URKISH. 4 The MOSES implementation (Koehn et al., 2007) was used as in all the previous work on this task. 5 145 Z stands for Zhang and P for Plus. Additionally, experiment"
S18-2017,C04-1127,0,0.815218,"te tokens with the same semantic structure tag (either predicate or argument) as the centroid. We call this technique Halo, because the process of each hidden state taking up its surroundings is analogous to how the halo is formed around the sun. The method is believed to help the model generalize better, by learning more semantics-aware and noise-insensitive hidden states without introducing extra parameters. Introduction Cross-lingual information extraction (CLIE) is the task of distilling and representing factual information in a target language from the textual input in a source language (Sudo et al., 2004; Zhang et al., 2017b). For example, Fig. 1 illustrates a pair of input Chinese sentence and its English predicateargument information1 , where predicate and argument are well used semantic structure tags. It is of great importance to solve the task, as to provide viable solutions to extracting information from the text of languages that suffer from no or little existing information extraction tools. Neural models have empirically proven successful in this task (Zhang et al., 2017b,c), but still remain unsatisfactory in low resource (i.e. small number of training samples) settings. These neura"
S18-2022,E17-1075,0,0.0994966,"Missing"
S18-2022,D15-1103,0,0.284133,"Missing"
S18-2022,D17-1284,0,0.0693452,"ncode entity e and its context x into feature vectors, and we consider both sentence-level context xs and document-level context xd in contrast to prior work which only takes sentence-level context (Gillick et al., 2014; Shimaoka et al., 2017). 1 gd (xd ) = relu(Wd1 tanh(Wd2 DM(xd ))), (7) where DM is a pretrained distributed memory model (Le and Mikolov, 2014) which converts the document-level context into a distributed representation. Wd1 and Wd2 are weight matrices. 1 Document-level context has also been exploited in Yaghoobzadeh and Sch¨utze (2015); Yang et al. (2016); Karn et al. (2017); Gupta et al. (2017). 174 2.3 Adaptive Thresholds Loose Macro: In prior work, a fixed threshold (rt = 0.5) is used for classification of all types (Ling and Weld, 2012; Shimaoka et al., 2017). We instead assign a different threshold to each type that is optimized to maximize the overall strict F1 on the dev set. We show the definition of strict F1 in Section 3.1. 3 N 1 X |Tˆi ∩ Ti | P = N |Tˆi | i=1 R= i=1 Loose Micro: Experiments Train Dev Test Types 251,039 84,078 2,000,000 2,202 2,000 10,000 8,963 13,766 563 89 93 113 3.2 Hyperparameters We use open-source GloVe vectors (Pennington et al., 2014) trained on Com"
S18-2022,E17-2119,0,0.0245922,"Missing"
S18-2022,D16-1144,0,0.651474,"ˆ i=1 |Ti ∩ Ti | P N ˆ i=1 |Ti | PN ˆ i=1 |Ti ∩ Ti | R= P N i=1 |Ti | P = We conduct experiments on three publicly available datasets.2 Table 1 shows the statistics of these datasets. OntoNotes: Gillick et al. (2014) sampled sentences from OntoNotes (Weischedel et al., 2011) and annotated entities in these sentences using 89 types. We use the same train/dev/test splits in Shimaoka et al. (2017). Document-level contexts are retrieved from the original OntoNotes corpus. BBN: Weischedel and Brunstein (2005) annotated entities in Wall Street Journal using 93 types. We use the train/test splits in Ren et al. (2016b) and randomly hold out 2,000 pairs for dev. Document contexts are retrieved from the original corpus. FIGER: Ling and Weld (2012) sampled sentences from 780k Wikipedia articles and 434 news reports to form the train and test data respectively, and annotated entities using 113 types. The splits we use are the same in Shimaoka et al. (2017). OntoNotes BBN FIGER N 1 X |Tˆi ∩ Ti | N |Ti | N 1 X ˆ δ(Ti = Ti ) N i=1 3 For PLE (Ren et al., 2016b), we were unable to replicate the performance benefits reported in their work, so we report the results after running their codebase. 2 We made the source"
S18-2022,W16-1313,0,0.332997,"Missing"
S18-2022,D15-1166,0,0.172852,"Missing"
S18-2022,E17-1119,0,0.104033,"application to downstream tasks, recent work on entity typing has moved beyond standard coarse types towards finer-grained semantic types with richer ontologies (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015). Rather than assuming an entity can be uniquely categorized into a single type, the task has been approached as a multi-label classification problem: e.g., in “... became a top seller ... Monopoly is played in 114 countries. ...” (Figure 1), “Monopoly” is considered both a game as well as a product. The state-of-the-art approach (Shimaoka et al., 2017) for fine-grained entity typing employs an attentive neural architecture to learn representations of the entity mention as well as its context. These representations are then combined with hand-crafted features (e.g., lexical and syntactic features), and fed into a linear classifier with a fixed threshold. While this approach outperforms previous approaches which only use sparse binary features (Ling and Weld, 2012; Gillick et al., 2014) or distributed representations (Yogatama et al., 2015), it has a few drawbacks: (1) the representations of left and right contexts are learnt independently, i"
S18-2022,C16-1017,0,0.503215,"... /other /other /other/health /other/health/treatment /organization /organization/company /organization /organization/company B Bozell joins Backer Spielvogel Bates and Ogilvy Group as U.S. agencies with interests in Korean agencies. Table 2: Examples showing the improvement brought by document-level contexts and dot-product attention. Entities are shown in the green box. The gray boxes visualize attention weights (darkness) on context tokens. Approach Approach Strict Macro Micro B INARY(Gillick et al., 2014) K WSABIE(Yogatama et al., 2015) N/A N/A N/A N/A 70.01 72.98 PLE(Ren et al., 2016b) Ma et al. (2016) AFET(Ren et al., 2016a) F NET(Abhishek et al., 2017) N EURAL(Shimaoka et al., 2017) w/o Hand-crafted features 51.61 49.30 55.10 52.20 51.74 47.15 67.39 68.23 71.10 68.50 70.98 65.53 62.38 61.27 64.70 63.30 64.91 58.25 O UR A PPROACH w/o Adaptive thresholds w/o Document-level contexts w/ Hand-crafted features 55.52 53.49 53.17 54.40 73.33 73.11 72.14 73.13 67.61 66.78 66.51 66.89 Strict Macro Micro K WSABIE(Yogatama et al., 2015) N/A Attentive(Shimaoka et al., 2016) 58.97 F NET(Abhishek et al., 2017) 65.80 N/A 77.96 81.20 72.25 74.94 77.40 Ling and Weld (2012) PLE(Ren et al., 2016b) Ma et al."
S18-2022,D15-1083,0,0.107698,"Missing"
S18-2022,N16-1174,0,0.0372348,"ur model contains three encoders which encode entity e and its context x into feature vectors, and we consider both sentence-level context xs and document-level context xd in contrast to prior work which only takes sentence-level context (Gillick et al., 2014; Shimaoka et al., 2017). 1 gd (xd ) = relu(Wd1 tanh(Wd2 DM(xd ))), (7) where DM is a pretrained distributed memory model (Le and Mikolov, 2014) which converts the document-level context into a distributed representation. Wd1 and Wd2 are weight matrices. 1 Document-level context has also been exploited in Yaghoobzadeh and Sch¨utze (2015); Yang et al. (2016); Karn et al. (2017); Gupta et al. (2017). 174 2.3 Adaptive Thresholds Loose Macro: In prior work, a fixed threshold (rt = 0.5) is used for classification of all types (Ling and Weld, 2012; Shimaoka et al., 2017). We instead assign a different threshold to each type that is optimized to maximize the overall strict F1 on the dev set. We show the definition of strict F1 in Section 3.1. 3 N 1 X |Tˆi ∩ Ti | P = N |Tˆi | i=1 R= i=1 Loose Micro: Experiments Train Dev Test Types 251,039 84,078 2,000,000 2,202 2,000 10,000 8,963 13,766 563 89 93 113 3.2 Hyperparameters We use open-source GloVe vectors"
S18-2022,D14-1162,0,0.0810725,"Karn et al. (2017); Gupta et al. (2017). 174 2.3 Adaptive Thresholds Loose Macro: In prior work, a fixed threshold (rt = 0.5) is used for classification of all types (Ling and Weld, 2012; Shimaoka et al., 2017). We instead assign a different threshold to each type that is optimized to maximize the overall strict F1 on the dev set. We show the definition of strict F1 in Section 3.1. 3 N 1 X |Tˆi ∩ Ti | P = N |Tˆi | i=1 R= i=1 Loose Micro: Experiments Train Dev Test Types 251,039 84,078 2,000,000 2,202 2,000 10,000 8,963 13,766 563 89 93 113 3.2 Hyperparameters We use open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions to initialize word embeddings used in all encoders. All weight parameters are sampled from U(−0.01, 0.01). The encoder for sentence-level context is a 2-layer bidirectional RNN with 200 hidden units. The DM output size is 50. Sizes of Wa , Wd1 and Wd2 are 200×300, 70×50, and 50×70 respectively. Adam optimizer (Kingma and Ba, 2014) and mini-batch gradient is used for optimization. Batch size is 200. Dropout (rate=0.5) is applied to three feature functions. To avoid overfitting, we choose models which yield the best strict F1 on dev sets. 3.3 Res"
S18-2022,N13-1071,0,0.0648779,"Missing"
S18-2022,P15-2048,0,0.649898,"” (Figure 1), “Monopoly” is considered both a game as well as a product. The state-of-the-art approach (Shimaoka et al., 2017) for fine-grained entity typing employs an attentive neural architecture to learn representations of the entity mention as well as its context. These representations are then combined with hand-crafted features (e.g., lexical and syntactic features), and fed into a linear classifier with a fixed threshold. While this approach outperforms previous approaches which only use sparse binary features (Ling and Weld, 2012; Gillick et al., 2014) or distributed representations (Yogatama et al., 2015), it has a few drawbacks: (1) the representations of left and right contexts are learnt independently, ignoring their mutual connection; (2) the attention on context is computed solely upon the context, considering no alignment to the entity; (3) document-level contexts which could be useful in classification are not exploited; and (4) hand-crafted features heavily rely on system or human annotations. To overcome these drawbacks, we propose a neural architecture (Figure 1) which learns more context-aware representations by using a better attention mechanism and taking advantage of semantic dis"
S18-2022,C12-2133,0,0.207282,"Missing"
W05-0708,C04-1080,0,0.0403864,"Missing"
W05-0708,A00-1031,0,0.0638306,"by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 1 Introduction Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagg"
W05-0708,W95-0101,0,0.165377,"on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 1 Introduction Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtain"
W05-0708,J92-4003,0,0.132836,"Missing"
W05-0708,N03-2003,0,0.0143299,"ty p(w0:M , t0:M ) becomes: M Y pE (wi |ti )(λpE (ti |hi ) + (1 − λ)pL (ti |hi )) (2) i=0 Here λ defines the interpolation weights for the ECA contextual model pE (ti |hi ) and the LCA contextual model pL (ti |hi ). pE (wn |tn ) is the ECA lexi60 cal model. The interpolation weight λ is estimated by maximizing the likelihood of a held-out data set given the combined model. As an extension, we allow the interpolation weights to be a function of the current tag: λ(ti ), since class-dependent interpolation has shown improvements over basic interpolation in applications such as language modeling (Bulyko et al., 2003). 5.2 Joint Training of Contextual Model As an alternative to model interpolation, we consider training a single model jointly from the two different data sets. The underlying assumption of this technique is that tag sequences in LCA and ECA are generated by the same process, whereas the observations (the words) are generated from the tag by two different processes in the two different dialects. The HMM model for joint training is expressed as: M Y (αi pE (wi |ti ) + (1 − αi )pL (wi |ti ))pE+L (ti |hi ) i=0 where αi =  1 if word wi is ECA 0 otherwise (3) A single conditional probability table"
W05-0708,A92-1018,0,0.170834,"Missing"
W05-0708,W96-0102,0,0.0143507,"task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagging is a straightforward task. The dialects of Arabic, by contrast, are spoken rather than written languages. Apart from small amounts of written dialectal material in e.g."
W05-0708,P00-1026,0,0.014269,"Missing"
W05-0708,N04-4038,0,0.120933,"oaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagging is a straightforward task. The dialects of Arabic, by contrast, are spoken rather than written languages. Apart from small amounts of written dialectal material in e.g. plays, novels, chat rooms, etc., data can only be obtained by recording and manually transcribing actual conversations. Moreover, there is no universally agreed upon writing standard for dialects (though several standardization efforts are underway); any largescale data collection and transcription effort therefore req"
W05-0708,W04-3229,0,0.0124355,"alectal Data Sharing Next we examine whether unannotated corpora in other dialects (LCA) can be used to further improve the ECA tagger. The problem of data sparseness for Arabic dialects would be less severe if we were able to exploit the commonalities between similar dialects. In natural language processing, Kim & Khudanpur (2004) have explored techniques for using parallel Chinese/English corpora for language modeling. Parallel corpora have also been used to infer morphological analyzers, POS taggers, and noun phrase bracketers by projections via word alignments (Yarowsky et al., 2001). In (Hana et al., 2004), Czech data is used to develop a morphological analyzer for Russian. In contrast to these works, we do not require parallel/comparable corpora or a bilingual dictionary, which may be difficult to obtain. Our goal is to develop general algorithms for utilizing the commonalities across dialects for developing a tool for a specific dialect. Although dialects can differ very strongly, they are similar in that they exhibit morphological simplifications and a different word order compared to MSA (e.g. SVO rather than VSO order), and close dialects share some vocabulary. Each of the tagger component"
W05-0708,W00-0729,0,0.01355,"Standard Arabic. No dialect-specific tools are used. We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 1 Introduction Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all poss"
W05-0708,W00-0731,0,0.0291547,"yzer for Modern Standard Arabic. No dialect-specific tools are used. We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 1 Introduction Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon"
W05-0708,N03-1033,0,0.0051223,"S) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagging is a straightforward task. The dialects of Arabic, by contrast, are spoken rather than written languages. Apart from sma"
W05-0708,N01-1026,0,0.0147868,"training lexicon. 5 Cross-Dialectal Data Sharing Next we examine whether unannotated corpora in other dialects (LCA) can be used to further improve the ECA tagger. The problem of data sparseness for Arabic dialects would be less severe if we were able to exploit the commonalities between similar dialects. In natural language processing, Kim & Khudanpur (2004) have explored techniques for using parallel Chinese/English corpora for language modeling. Parallel corpora have also been used to infer morphological analyzers, POS taggers, and noun phrase bracketers by projections via word alignments (Yarowsky et al., 2001). In (Hana et al., 2004), Czech data is used to develop a morphological analyzer for Russian. In contrast to these works, we do not require parallel/comparable corpora or a bilingual dictionary, which may be difficult to obtain. Our goal is to develop general algorithms for utilizing the commonalities across dialects for developing a tool for a specific dialect. Although dialects can differ very strongly, they are similar in that they exhibit morphological simplifications and a different word order compared to MSA (e.g. SVO rather than VSO order), and close dialects share some vocabulary. Each"
W06-1647,C04-1080,0,0.0521538,"Missing"
W06-1647,W95-0101,0,0.137353,"Missing"
W06-1647,N04-4038,0,0.0548523,"Missing"
W06-1647,W05-0708,1,0.940065,"und will achieve a lower expected test risk. Therefore, one can use the bound as a principled way of choosing the parameters in the Transductive Clustering algorithm: First, a large number of different clusterings is created; then the one that achieves the lowest PACBayesian bound is chosen. The pseudo-code is given in Figure 2. (El-Yaniv and Gerzon, 2005) has applied the Transductive Clustering algorithm successfully to binary classification problems and demonstrated improvements over the current state-of-the-art Spectral Graph Transducers (Section 3.4). We use the algorithm as described in (Duh and Kirchhoff, 2005b), which adapts the algorithm to structured output problems. In particular, the modification involves a different estimate of the priors p(h), which was assumed to be uniform in (El-Yaniv and Gerzon, 2005). Since there are many possible h, adopting a uniform prior will lead to small values of p(h) and thus a loose bound for all h. Probability mass should only be spent on POS-sets that are possible, and as such, we calculate p(h) based on frequencies of compound-labels in the training data (i.e. an empirical prior). Transductive Clustering How does a transductive algorithm effectively utilize"
W06-1647,P05-1071,0,0.0547921,"Missing"
W06-1647,E06-1047,0,\N,Missing
W08-0314,atserias-etal-2006-freeling,0,0.030555,"Missing"
W08-0314,P08-2010,1,0.78181,"used to train the language model. The second language model used for rescoring was a 5-gram model over part-of-speech (POS) tags. This model was built using the Spanish side of the English-Spanish parallel training corpus. The POS tags were obtained from the corpus using Freeling v2.0 (Atserias et al, 2006). We selected the language models for our translation system were selected based on performance on the English-to-Spanish task, and reused them for the German-to-Spanish task. 4 Boosted Reranking We submitted an alternative system, based on a different re-ranking method, called BoostedMERT (Duh and Kirchhoff, 2008), for each task. BoostedMERT is a novel boosting algorithm that uses Minimum Error Rate Training (MERT) as a weak learner to build a re-ranker that is richer than the standard log-linear models. This is motivated by the observation that log-linear models, as trained by MERT, often do not attain the oracle BLEU scores of the Nbest lists in the development set. While this may be due to a local optimum in MERT, we hypothesize that log-linear models based on our K re-ranking features are also not sufficiently expressive. BoostedMERT is inspired by the idea of Boosting (for classification), which h"
W08-0314,N03-1017,0,0.00598649,"task. For German-to-Spanish translation we additionally investigated simplifications of German morphology, which is known to be fairly complex due to a large number of compounds and inflections. In the following sections we first describe the data, baseline system and postprocessing steps before describing boosted N-best list reranking and morphology-based preprocessing for German. 3 3.1 System Overview Translation model The system developed for this year’s shared task is a state-of-the-art, two-pass phrase-based statistical machine translation system based on a log-linear translation model (Koehn et al, 2003). The translation models and training method follow the standard Moses (Koehn et al, 2007) setup distributed as part of the shared task. We used the training method suggested in the Moses documentation, with lexicalized reordering (the msd-bidirectional-fe option) enabled. The system was tuned via Minimum Error Rate Training (MERT) on the first 500 sentences of the devtest2006 dataset. 123 Proceedings of the Third Workshop on Statistical Machine Translation, pages 123–126, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 3.2 Decoding Our system used the Moses de"
W08-0314,2005.mtsummit-papers.11,0,0.034177,"Missing"
W08-0314,E06-1006,1,0.854058,"ed by a high number of noun compounds and rich inflectional paradigms. Simplification of morphology can produce better word alignment, and thus better phrasal translations, and can also significantly reduce the out-of-vocabulary rate. We therefore applied two operations: (a) splitting of compound words and (b) stemming. After basic preprocessing, the German half of the training corpus was first tagged by the German version of TreeTagger (Schmid, 1994), to identify partof-speech tags. All nouns were then collected into a noun list, which was used by a simple compound splitter, as described in (Yang and Kirchhoff, 2006). This splitter scans the compound word, hypothesizing segmentations, and selects the first segmentation that produces two nouns that occur individually in the corpus. After splitting the compound nouns in the filtered corpus, we used the TreeTagger again, only this time to lemmatize the (filtered) training corpus. The stemmed version of the German text was used to train the translation system’s word alignments (through the end of step 3 in the Moses training script). After training the alignments, they were projected back onto the unstemmed corpus. The parallel 125 phrases were then extracted"
W08-0314,W96-0213,0,\N,Missing
W08-0314,N04-1023,0,\N,Missing
W08-0314,E06-1005,0,\N,Missing
W08-0314,N03-2002,1,\N,Missing
W08-0314,W05-0821,1,\N,Missing
W08-0314,P05-1071,0,\N,Missing
W08-0314,D08-1076,0,\N,Missing
W08-0314,C04-1022,1,\N,Missing
W08-0314,N06-4004,0,\N,Missing
W08-0314,J92-4003,0,\N,Missing
W08-0314,P07-2045,0,\N,Missing
W08-0314,W08-0336,0,\N,Missing
W08-0314,J03-1002,0,\N,Missing
W08-0314,N04-1021,0,\N,Missing
W08-0331,P07-1111,0,0.0849514,"comparing different techniques in cases when human judgments for each MT output are expensivie to obtain. Many automatic evaluation methods have been proposed to date. Successful methods such as BLEU ∗ Work supported by an NSF Graduate Research Fellowship. Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminate between human-like translations and automatic translations, using features from the aforementioned metrics (e.g. n-gram precisions). In contrast, (Albrecht and Hwa, 2007) argues for a regression approach that directly predicts human adequecy/fluency scores. All the above methods are score-based in the sense that they generate a score for each MT system output. When the evaluation goal is to compare multiple MT systems, scores are first generated independently for each system, then systems are ranked by their respective scores. We think that this twostep process may be unnecessarily complex. Why solve a more difficult problem of predicting the quality of MT system outputs, when the goal is simply 191 Proceedings of the Third Workshop on Statistical Machine Tran"
W08-0331,W07-0718,0,0.0471762,"f MT outputs, and (b) a machine learning algorithm for learning and predicting rankings.1 The advantages of a ranking approach are: • It is often easier for human judges to rank MT outputs by preference than to assign absolute scores (Vilar et al., 2007). This is because it is difficult to quantify the quality of a translation accurately, but relative easy to tell which one of several translations is better. Thus humanannotated data based on ranking may be less costly to acquire. • The inter- and intra-annotator agreement for ranking is much more reasonable than that of scoring. For instance, Callison-Burch (2007) found the inter-annotator agreement (Kappa) for scoring fluency/adequency to be around .22-.25, whereas the Kappa for ranking is around .37-.56. Thus human-annotated data based on ranking may be more reliable to use. • As mentioned earlier, when the final goal of the evaluation is comparing systems, ranking more directly solves the problem. A scoring approach essentially addresses a more difficult problem of estimating MT output quality. Nevertheless, we note that score-based approaches remain important in cases when the absolute difference between MT quality is desired. For instance, one mig"
W08-0331,P01-1020,0,0.0603242,"tems is an important research topic for the advancement of MT technology, since automatic evaluation methods can be used to quickly determine the (approximate) quality of MT system outputs. This is useful for tuning system parameters and for comparing different techniques in cases when human judgments for each MT output are expensivie to obtain. Many automatic evaluation methods have been proposed to date. Successful methods such as BLEU ∗ Work supported by an NSF Graduate Research Fellowship. Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminate between human-like translations and automatic translations, using features from the aforementioned metrics (e.g. n-gram precisions). In contrast, (Albrecht and Hwa, 2007) argues for a regression approach that directly predicts human adequecy/fluency scores. All the above methods are score-based in the sense that they generate a score for each MT system output. When the evaluation goal is to compare multiple MT systems, scores are first generated independently for each system, then systems are ranked by their respective scores."
W08-0331,N06-1058,0,0.034309,"Missing"
W08-0331,2004.tmi-1.8,0,0.189419,"c for the advancement of MT technology, since automatic evaluation methods can be used to quickly determine the (approximate) quality of MT system outputs. This is useful for tuning system parameters and for comparing different techniques in cases when human judgments for each MT output are expensivie to obtain. Many automatic evaluation methods have been proposed to date. Successful methods such as BLEU ∗ Work supported by an NSF Graduate Research Fellowship. Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminate between human-like translations and automatic translations, using features from the aforementioned metrics (e.g. n-gram precisions). In contrast, (Albrecht and Hwa, 2007) argues for a regression approach that directly predicts human adequecy/fluency scores. All the above methods are score-based in the sense that they generate a score for each MT system output. When the evaluation goal is to compare multiple MT systems, scores are first generated independently for each system, then systems are ranked by their respective scores. We think that this twostep proce"
W08-0331,P04-1077,0,0.0577017,"Missing"
W08-0331,W05-0904,0,0.125203,"Missing"
W08-0331,P02-1040,0,0.105132,"Missing"
W08-0331,2006.amta-papers.25,0,0.0764046,"Missing"
W08-0331,W07-0713,0,0.0749494,"Translation, pages 191–194, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics to compare systems? In this regard, we propose a ranking-based approach that directly ranks a set of MT systems without going through the intermediary of system-specific scores. Our approach requires (a) training data in terms of human ranking judgments of MT outputs, and (b) a machine learning algorithm for learning and predicting rankings.1 The advantages of a ranking approach are: • It is often easier for human judges to rank MT outputs by preference than to assign absolute scores (Vilar et al., 2007). This is because it is difficult to quantify the quality of a translation accurately, but relative easy to tell which one of several translations is better. Thus humanannotated data based on ranking may be less costly to acquire. • The inter- and intra-annotator agreement for ranking is much more reasonable than that of scoring. For instance, Callison-Burch (2007) found the inter-annotator agreement (Kappa) for scoring fluency/adequency to be around .22-.25, whereas the Kappa for ranking is around .37-.56. Thus human-annotated data based on ranking may be more reliable to use. • As mentioned"
W08-0331,W07-0736,0,0.14397,"Missing"
W08-0331,W05-0909,0,\N,Missing
W08-0331,C04-1046,0,\N,Missing
W10-1736,P07-1091,0,0.326335,"Missing"
W10-1736,2006.amta-papers.16,0,0.0396519,"Missing"
W10-1736,J03-1002,0,0.00303273,", 3, 2, 6] from this alignment. When the same word (or derivative words) appears twice or more in a single English sentence, two or more non-consecutive words in the English sentence are aligned to a single Japanese word: 3.1 Rough evaluation of reordering rate of change of speed NULL ({}) sokudo ({5}) henka ({3}) no ({2 4}) wariai ({1}) First, we examined rank correlation between Head Final English sentences produced by the Head Finalization rule and Japanese reference sentences. Since we do not have handcrafted word alignment data for an English-to-Japanese bilingual corpus, we used GIZA++ (Och and Ney, 2003) to get automatic word alignment. Based on this automatic word alignment, we measured Kendall’s τ for the word order between HFE sentences and Japanese sentences. Kendall’s τ is a kind of rank correlation measure defined as follows. Suppose a list of integers such as L = [2, 1, 3, 4]. The number of all integer pairs in this list is 4 C2 = 4 × 3/(2 × 1) = 6. The number of increasing pairs is five: (2, 3), (2, 4), (1, 3), (1, 4), and (3, 4). Kendall’s τ is defined by τ= We excluded the ambiguously aligned words (2 from the calculation of τ . We use only [5, 3, 1] and get τ = −1.0. The exclusion"
W10-1736,P05-1022,0,0.0389233,"Missing"
W10-1736,P02-1040,0,0.107213,"ecided to stop swapping them at coordination nodes, which are indicated cat and xcat attributes of the Enju output. We call this the coordination exception rule. In addition, we avoid Enju’s splitting of numerical expressions such as “12,345” and “(1)” because this splitting leads to inappropriate word orders. 246 3 Experiments John va1 a ball va2 hit . NULL ({3}) jon ({1}) wa ({2}) bohru ({4}) wo ({5}) utta ({6}) . ({7}) In order to show how closely our Head Finalization makes English follow Japanese word order, we measured Kendall’s τ , a rank correlation coefficient. We also measured BLEU (Papineni et al., 2002) and other automatic evaluation scores to show that Head Finalization can actually improve the translation quality. We used NTCIR7 PAT-MT’s Patent corpus (Fujii et al., 2008). Its training corpus has 1.8 million sentence pairs. We used MeCab (http:// mecab.sourceforge.net/) to segment Japanese sentences. Then, we get [1, 2, 4, 5, 6, 7] and τ = 1.0. We use τ or the average of τ over all training sentences to observe the tendency. Sometimes, one Japanese word corresponds to an English phrase: John went to Costa Rica . NULL ({}) jon ({1}) wa ({}) kosutarika ({4 5}) ni ({3}) itta ({2}) . ({6}) We"
W10-1736,P05-1066,0,0.769816,"Missing"
W10-1736,P05-1034,0,0.166265,"Missing"
W10-1736,de-marneffe-etal-2006-generating,0,0.00411069,"Missing"
W10-1736,2006.amta-papers.25,0,0.0277013,"it (http:// 5 Discussion Our method used an HPSG parser, which gives rich information, but it is not easy to build such a parser. It is much easier to build word dependency parsers and Penn Treebank-style parsers. In order use these parsers, we have to add some heuristic rules. www.mibel.cs.tsukuba.ac.jp/norimatsu/ bleu kit/) following the PATMT’s overview paper (Fujii et al., 2008). The table shows that dl=6 gives the best result, and even dl=0 (no reordering in Moses) gives better scores than the organizers’ Moses. Table 2 also shows Word Error Rates (WER) and Translation Error Rates (TER) (Snover et al., 2006). Since they are error rates, smaller is better. Although the improvement of BLEU is not very impressive, the score of WER is greatly reduced. This difference comes from the fact that BLEU measures only local word order, while WER mea5.1 Word Dependency Parsers At first, we thought that we could substitute a word dependency parser for Enju by simply rephrasing a head with a modified word. Xu et al. (2009) used a semantic head-based dependency parser for a similar purpose. Even when we use a syntactic head-based dependency parser instead, we encountered their ‘excessive movement’ problem. A str"
W10-1736,N07-1007,0,0.0105596,"ad c4 appears before c5, so c4 and c5 are swapped. The lower picture shows the swapped result. Then we get John a ball hit, which has the same word order as its Japanese translation jon wa bohru wo utta except for the functional words a, wa, and wo. We have to add Japanese particles wa (topic marker) or ga (nominative case marker) for John and wo (objective case marker) for ball to get an acceptable Japanese sentence. It is well known that SMT is not good at generating appropriate particles from English, whitch does not have particles. Particle generation was tackled by a few research groups (Toutanova and Suzuki, 2007; Hong et al., 2009). Here, we use Enju’s output to generate seeds Figure 1: Enju’s XML output (some attributes are removed for readability). c0 ? c1 ? c2 ? t0 John c3 ? c6 ? t2 a c4 ? t1 hit c0 c1 ? c2 ? t0 John jon (wa) Original English ? c5 ? c5 ? c7 ? t3 ball Head Final English c3 ? c7 c6 c4 ? ? ? t3 t2 t1 a ball hit – bohru (wo) utta Figure 2: Head Finalization of a simple sentence (? indicates a head). 245 0 Original English ? ? 4 ? 1? 2 John 5 went 6 ? 7 to 8 9 the 0 13 3 ? 10 police 11 ? 12 because 13 14 ? 15 Mary ? 16 ? 19 his 17 lost 18 ? 20 wallet Head Final English ? 3 11 ? ? ? 16"
W10-1736,C04-1073,0,0.835753,"Missing"
W10-1736,N09-1028,0,0.772582,"ral level. Why do we think this works? The reason is simple: Japanese is a typical head-final language. That is, a syntactic head word comes after nonhead (dependent) words. SOV is just one aspect of head-final languages. In order to implement this idea, we need a parser that outputs syntactic heads. Enju is such a parser from the University of Tokyo (http://www-tsujii.is.s. u-tokyo.ac.jp/enju). We discuss other parsers in section 5. There is another kind of head: semantic heads. Hong et al. (2009) used Stanford parser (de Marneffe et al., 2006), which outputs semantic headbased dependencies; Xu et al. (2009) also used the same representation. The use of syntactic heads and the number of dependents are essential for the simplicity of English is a typical SVO (Subject-VerbObject) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently, a few groups have proposed rulebased preprocessing methods to mitigate this problem (Xu et al., 2009; Hong et al."
W10-1736,P09-2059,0,0.540137,"ider part-of-speech tags or rule weights because the powerful Enju parser allows us to implement the rule at a general level. Why do we think this works? The reason is simple: Japanese is a typical head-final language. That is, a syntactic head word comes after nonhead (dependent) words. SOV is just one aspect of head-final languages. In order to implement this idea, we need a parser that outputs syntactic heads. Enju is such a parser from the University of Tokyo (http://www-tsujii.is.s. u-tokyo.ac.jp/enju). We discuss other parsers in section 5. There is another kind of head: semantic heads. Hong et al. (2009) used Stanford parser (de Marneffe et al., 2006), which outputs semantic headbased dependencies; Xu et al. (2009) also used the same representation. The use of syntactic heads and the number of dependents are essential for the simplicity of English is a typical SVO (Subject-VerbObject) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently,"
W10-1736,P01-1067,0,0.267073,"Missing"
W10-1757,W06-1615,0,0.0749079,"for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsity leads to overfitting. We addressed this by re-casting N-best lists as multitask learning data. Our MT experiments show consistent statistically significant improvements. From the Bayesian view, multitask formulation of N-best"
W10-1757,W09-2201,0,0.0321418,"r better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsi"
W10-1757,N09-1025,0,0.167697,"tanabe et al., 2007) defines feature templates based on bilingual word alignments, which lead to extraction of heavilylexicalized features of the form: 2. The input (f ) has high variability (e.g. large vocabulary size), so that features for different inputs are rarely shared. 3. The N-best list output also exhibits high variability (e.g. many different word reorderings). Larger N may improve reranking performance, but may also increase feature sparsity. When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail. Our goal here is to address this situation. 3 Proposed Reranking Framework h(e, f ) =  1       0 if foreign word “Monsieur” and English word “Mr.” co-occur in e,f otherwise In the following, we first give an intuitive comparison between single vs. multiple task learning (Section 3.1), before presenting the general metaalgorithm (Section 3.2) and particular instantiations (Section 3.3). (2) One can imagine that such features are sparse because it may only fire for input sentences that contain the word “Monsieur”. For all other input sentences, it is an useless, inactive featur"
W10-1757,J07-2003,0,0.0561179,"Missing"
W10-1757,J05-1003,0,0.264845,"f features. 1. We introduce the idea of viewing N-best reranking as a multitask learning problem. This view is particularly apt to any general reranking problem with sparse feature sets. 2. We propose a simple meta-algorithm that first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker. Thus it is easily applicable to existing systems. 1 Introduction Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (Shen et al., 2004; Collins and Koo, 2005; Roark et al., 2007). The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover. In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels. Given a new N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list. Existing research have focused on training a single reranker directly on the 3. We demonstrate that"
W10-1757,N09-1068,0,0.0391788,"gularizer to ensure that the learned functions of related tasks are close to each other. The popular ℓ1 /ℓ2 objective can be optimized by various methods, such as boosting (Obozinski et al., 2009) and convex programming (Argyriou et al., 2008). Yet another regularizer is the ℓ1 /ℓ∞ norm (Quattoni et al., 2009), which replaces the 2-norm with a max. One could also define a regularizer to ensure i that each task-specific to some average P wi is close parameter, e.g. i ||w − wavg ||2 . If we interpret wavg as a prior, we begin to see links to Hierarchical Bayesian methods for multitask learning (Finkel and Manning, 2009; Daume, 2009). 2. Shared Subspace: This approach assumes that there is an underlying feature subspace that is common to all tasks. Early works on multitask learning implement this by neural networks, where different tasks have different output layers but share the same hidden layer (Caruana, 1997). Another method is to write the weight vector as two parts w = [u; v] and let the task-specific function be uT · h(e, f ) + vT · Θ · h(e, f ) (Ando and Zhang, 2005). Θ is a D ′ × D matrix that maps the original features to a subspace common to all tasks. The new feature representation is computed by"
W10-1757,W08-0804,0,0.0274275,"res are shared: Wa : » – 4 0 0 4 0 3 3 0 4 4 3 3 → 14 Wb : » – 4 0 3 4 0 3 0 0 4 5 3 0 → 12 2 In MT, evaluation metrics like BLEU do not exactly decompose across sentences, so for some training algorithms this loss is an approximation. [optional] RandomHashing({Hi }) W = MultitaskLearn({(Hi , yi )}) hc = ExtractCommonFeature(W) {Hic } = RemapFeature({Hi }, hc ) wc = ConventionalReranker({(Hic , yi )}) The first step, random hashing, is optional. Random hashing is an effective trick for reducing the dimension of sparse feature sets without suffering losses in fidelity (Weinberger et al., 2009; Ganchev and Dredze, 2008). It works by collapsing random subsets of features. This step can be performed to speed-up multitask learning later. In some cases, the original feature dimension may be so large that hashed representations may be necessary. The next two steps are key. A multitask learning algorithm is run on the N-best lists, and a common feature space shared by all lists is extracted. For example, if one uses the multitask objective of Eq. 5, the result of step 2 is a set of weights W. ExtractCommonFeature(W) then returns the feature id’s (either from original or hashed representation) that receive nonzero"
W10-1757,P09-1114,0,0.0254945,"help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfor"
W10-1757,P05-1024,1,0.783566,"engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jo"
W10-1757,N04-1022,0,0.0383443,"Missing"
W10-1757,P06-1096,0,0.0716939,"., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on p"
W10-1757,P05-1012,0,0.149052,", 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Evaluation campaigns like WMT (Callison-Burch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within m"
W10-1757,N04-1021,0,0.102794,"Missing"
W10-1757,P02-1040,0,0.0791554,"Missing"
W10-1757,2009.iwslt-evaluation.1,0,0.0128431,"feature sets, with corresponding feature size and train/test BLEU/PER. All multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone. method. Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Evaluation campaigns like WMT (Callison-Burch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Ch"
W10-1757,P08-1098,0,0.0306633,"me applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsity leads to overfitting. We addressed this by re-casting N-best lists as multitask learning data. Our MT experiments show con"
W10-1757,N04-1023,0,0.65709,"nvolving millions of features. 1. We introduce the idea of viewing N-best reranking as a multitask learning problem. This view is particularly apt to any general reranking problem with sparse feature sets. 2. We propose a simple meta-algorithm that first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker. Thus it is easily applicable to existing systems. 1 Introduction Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (Shen et al., 2004; Collins and Koo, 2005; Roark et al., 2007). The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover. In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels. Given a new N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list. Existing research have focused on training a single reranker directly on the"
W10-1757,P09-1054,0,0.143227,"Missing"
W10-1757,D07-1080,1,0.936069,") is a D-dimensional feature vector, w is the weight vector to be trained, and N (f ) is the set of likely translations of f , i.e. the N-best list. The feature h(e, f ) can be any quantity defined in terms of the sentence pair, such as translation model and language model probabilities. Here we are interested in situations where the feature definitions can be quite sparse. A common methodology in reranking is to first design feature templates based on linguistic intuition and domain knowledge. Then, numerous features are instantiated based on the training data seen. For example, the work of (Watanabe et al., 2007) defines feature templates based on bilingual word alignments, which lead to extraction of heavilylexicalized features of the form: 2. The input (f ) has high variability (e.g. large vocabulary size), so that features for different inputs are rarely shared. 3. The N-best list output also exhibits high variability (e.g. many different word reorderings). Larger N may improve reranking performance, but may also increase feature sparsity. When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009)"
W10-1757,zhang-etal-2004-interpreting,0,0.0226462,"“de”) or special characters (such as numeral symbol and punctuation). These are features that can be expected to be widely applicable, and it is promising that multitask learning is able to recover these from the millions of potential features. 10 3. All three multitask methods obtained features that outperformed the baseline. The BLEU scores are 28.8, 28.9, 29.1 for Unsupervised Feature Selection, Joint Regularization, and Shared Subspace, respectively, which all outperform the 28.6 baseline. All improvements are statistically significant by bootstrap sampling test (1000 samples, p &lt; 0.05) (Zhang et al., 2004). 300 4. Shared Subspace performed the best. We conjecture this is because its feature projection can create new feature combinations that is more expressive than the feature selection used by the two other methods. Bootstrap samples 250 50 0 −0.2 Wabbit 1.2 5 Related Work in NLP Previous reranking work in NLP can be classified into two different research focuses: 1. Engineering better features: In MT, (Och and others, 2004) investigates features extracted from a wide variety of syntactic representations, such as parse tree probability on the outputs. Although their results show that the propo"
W10-1757,W09-0401,0,\N,Missing
W10-1757,P05-1022,0,\N,Missing
W10-1757,W09-0438,0,\N,Missing
W10-1762,J93-2003,0,0.011344,"ated studies on reordering. Section 3 describes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007)"
W10-1762,J07-2003,0,0.366382,"et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse trees. Xia and Our"
W10-1762,P05-1066,0,0.134623,"Missing"
W10-1762,P98-1070,0,0.140627,"ur method can be seen as a variant of tree-to-string translation that focuses only on the clause structure in parse trees and independently translates the clauses. Although previous syntax-based methods can theoretically model this kind of derivation, it is practically difﬁcult to decode long multi-clause sentences as described above. Our approach is also related to sentence simpliﬁcation and is intended to obtain simple and short source sentences for better translation. Kim and Ehara (1994) proposed a rule-based method for splitting long Japanese sentences for Japaneseto-English translation; Furuse et al. (1998) used a syntactic structure to split ill-formed inputs in speech translation. Their splitting approach splits a sentence sequentially to obtain short segments, and does not undertake their reordering. Another related ﬁeld is clause identiﬁcation (Tjong et al., 2001). The proposed method is not limited to a speciﬁc clause identiﬁcation method and any method can be employed, if their clause deﬁnition matches the proposed method where clauses are independently translated. 3 Bilingual source Corpus (Training) target parse & clause segmentation Source Sentences (clause-segmented) word alignment Wor"
W10-1762,P02-1040,0,0.0837809,"est sentences are multi-clause sentences. Training Corpus Type Parallel (no-clause-seg.) Parallel (auto-aligned) (oracle-aligned) Dictionary Development Corpus Type Parallel (oracle-aligned) Test Corpus Type Parallel (clause-seg.) E J E J J E J #words 690,536 942,913 135,698 183,043 183,147 263,175 291,455 #words E 34,417 J 46,480 E J #words 34,433 45,975 decoders employed two language models: a word 5-gram language model from the Japanese sentences in the parallel corpus and a word 4-gram language model from the Japanese entries in the dictionary. The feature weights were optimized for BLEU (Papineni et al., 2002) by MERT, using the development sentences. 4.4 Results Table 3 shows the results in BLEU, Translation Edit Rate (TER) (Snover et al., 2006), and Position-independent Word-error Rate (PER) (Och et al., 2001), obtained with Moses and our hierarchical phrase-based SMT, respectively. Bold face results indicate the best scores obtained with the compared methods (excluding oracles). The proposed method consistently outperformed the baseline. The BLEU improvements with the proposed method over the baseline and comparison methods were statistically signiﬁcant according to the bootstrap sampling test ("
W10-1762,N04-1035,0,0.0349554,"many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reorderi"
W10-1762,2006.amta-papers.25,0,0.0712603,"Missing"
W10-1762,N04-1014,0,0.0158515,"typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed s"
W10-1762,N04-4026,0,0.0472133,"ur thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically"
W10-1762,W01-0708,0,0.0411713,"Missing"
W10-1762,D09-1105,0,0.0200396,"nguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between languages with large syntactic differences, the rules are usually unsuitable for other language groups. On the other hand, statistical methods can be applied to any language pai"
W10-1762,J97-3002,0,0.192686,"sed SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse"
W10-1762,N03-1017,0,0.0275575,"ribes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be ext"
W10-1762,C04-1073,0,0.0479303,"Missing"
W10-1762,2005.iwslt-1.8,0,0.0209809,"uture studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering ove"
W10-1762,N09-1028,0,0.052652,"Computational Linguistics McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between l"
W10-1762,P01-1067,0,0.0503465,"ce-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previou"
W10-1762,P07-2045,0,0.00356861,"inal symbol s0 with the second clause and obtain the Japanese sentence: watashi wa tom ga kino susume ta zasshi o kat ta . 4 Experiment We conducted the following experiments on the English-to-Japanese translation of research paper abstracts in the medical domain. Such technical documents are logically and formally written, and sentences are often so long and syntactically complex that their translation needs long distance reordering. We believe that the medical domain is suitable as regards evaluating the proposed method. 4.2 Model and Decoder We used two decoders in the experiments, Moses9 (Koehn et al., 2007) and our inhouse hierarchical phrase-based SMT (almost equivalent to Hiero (Chiang, 2007)). Moses used a phrase table with a maximum phrase length of 7, a lexicalized reordering model with msd-bidirectional-fe, and a distortion limit of 1210 . Our hierarchical phrase-based SMT used a phrase table with a maximum rule length of 7 and a window size (Hiero’s Λ) of 12 11 . Both 4.1 Resources Our bilingual resources were taken from the medical domain. The parallel corpus consisted of research paper abstracts in English taken from PubMed4 and the corresponding Japanese translations. The training port"
W10-1762,zhang-etal-2004-interpreting,0,0.0423294,"Missing"
W10-1762,P07-1091,0,0.327459,"ally follows the Penn Treebank II scheme but also includes SINV, SQ, SBAR. See http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enjuoutput-spec.html#correspondence for details. 418 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418–427, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem ove"
W10-1762,P06-1077,0,0.0402884,"related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-"
W10-1762,P06-1004,0,0.0185365,"in our corpora. 421 John lost the book that was borrowed ... clause(1) clause(2) p(that |kara) + p(was |kara ) + ... p(John |john ) + p(lost |john ) + ... john John k|fm ) between each Japanese word fm and English clause k. Theoretically, we can simply output the clause id k ′ for each fm by ﬁnding k ′ = arg maxk t(lm = k|fm ). In practice, this may sometimes lead to Japanese clauses that have too many gaps, so we employ a two-stage procedure to extract clauses that are more contiguous. First, we segment the Japanese sentence into K clauses based on a dynamic programming algorithm proposed by Malioutov and Barzilay (2006). We deﬁne an M × M similarity matrix S = [sij ] with sij = exp(−||li −lj ||) where li is (K + i)-th row vector in the label matrix L. sij represents the similarity between the i-th and j-th Japanese words with respect to their clause alignment score distributions; if the score distributions are similar then sij is large. The details of this algorithm can be found in (Malioutov and Barzilay, 2006). The clause segmentation gives us contiguous Japanese clauses f˜1 , f˜2 , ..., f˜K , thus minimizing inter-segment similarity and maximizing intra-segment similarity. Second, we determine the clause"
W10-1762,J08-1002,0,0.0605287,"Missing"
W10-1762,W01-1408,0,0.038533,"Missing"
W10-1762,C98-1067,0,\N,Missing
W10-1762,J08-3004,0,\N,Missing
W12-4207,W08-0336,0,0.0779511,"Missing"
W12-4207,P05-1066,0,0.323609,"Missing"
W12-4207,C10-1043,0,0.0945607,"oached the reordering problem in multiple ways. The most basic idea is preordering (Xia and McCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2004) used a lexical reordering model, and Galley et al. (2004) followed a syntactic-based model. In this work, however, we are centered in the design of manual rules inspired by the Head Finalization (HF) reordering (Isozaki et al., 2010b). HF reordering is one of the simplest methods for preordering that significantly improves word alignments and leads to a better translation quality. Al59 though the method is limited to translation where the target language is head-final, it requires neither training"
W12-4207,D10-1092,1,0.926453,"Missing"
W12-4207,W10-1736,1,0.0700138,"me Tsukada‡ Masaaki Nagata‡ + The Graduate University For Advanced Studies, Tokyo, Japan ‡ NTT Communication Science Laboratories, NTT Corporation + handan@nii.ac.jp, ∗ wuxianchao@baidu.com, † kevinduh@is.naist.jp ‡ {sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp Abstract In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chineseto-English (Wang et al., 2007) and Englishto-Japanese (Isozaki et al., 2010b) translation with significant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules. 1 Introduction In state-of-the-art Statistical Machine Translation (SMT) systems, bilingual phrases are the main building blocks for constructing a tra"
W12-4207,N03-1017,0,0.0605813,"ranslation (SMT) systems, bilingual phrases are the main building blocks for constructing a translation given a sentence from a source language. To extract those bilingual phrases from a parallel corpus, the first step is to discover the implicit wordto-word correspondences between bilingual sentences (Brown et al., 1993). Then, a symmetrization matrix is built (Och and Ney, 2004) by using word-to-word alignments, and a wide variety ∗ Now at Baidu Japan Inc. Now at Nara Institute of Science and Technology (NAIST) † of heuristics can be used to extract the bilingual phrases (Zens et al., 2002; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation quality diminishes. In the latter scenario, a simple but effective strategy to cope with this problem is to reorder the words of sentences in one language so that it resembles the word order of another language (Wu et al., 2011; Isozak"
W12-4207,P07-2045,0,0.0344953,"Missing"
W12-4207,J08-1002,0,0.110732,"h languages with different phrase structures like English and Japanese. Head Finalization is a successful syntax-based reordering method designed to reorder sentences from a head-initial language to resemble the word order in sentences from a headfinal language (Isozaki et al., 2010b). The essence 58 of this rule is to move the syntactic heads to the end of its dependency by swapping child nodes in a phrase structure tree when the head child appears before the dependent child. Isozaki et al. (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. The score results from several mainstream evaluation methods indicated that the translation quality had been improved; the scores of Word Error Rate (WER) and Translation Edit Rate (TER) (Snover et al., 2006) had especially been greatly reduced. 2.2 Chinese Deep Parsing Syntax-based reordering methods need parsed sentences as input. Isozaki et al. (2010b) used Enju, an HPSG-based deep parser for English, but they also discussed using other types of parsers, such as word dependency parsers and Penn Treebankstyle parsers. However, to use word de"
W12-4207,J03-1002,0,0.0108902,"reebank. Chinese Enju requires segmented and POS-tagged sentences to do parsing. We used the Stanford Chinese segmenter (Chang et al., 2008) and Stanford POStagger (Toutanova et al., 2003) to obtain the segmentation and POS-tagging of the Chinese side of the training, development, and test sets. The baseline system was trained following the instructions of recent SMT evaluation campaigns (Callison-Burch et al., 2010) by using the MT toolkit Moses (Koehn et al., 2007) in its default configuration. Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics “grow-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the parallel corpora by using the modified Kneser-Ney smoothing (Chen and Goodman, 1999) implemented in the SRILM (Stolcke, 2002) toolkit. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. The effectiveness of the reorderings proposed in Section 3.3 was assessed by using two precision metrics and two error metrics on translation quali"
W12-4207,J04-4002,0,0.081366,"ally improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules. 1 Introduction In state-of-the-art Statistical Machine Translation (SMT) systems, bilingual phrases are the main building blocks for constructing a translation given a sentence from a source language. To extract those bilingual phrases from a parallel corpus, the first step is to discover the implicit wordto-word correspondences between bilingual sentences (Brown et al., 1993). Then, a symmetrization matrix is built (Och and Ney, 2004) by using word-to-word alignments, and a wide variety ∗ Now at Baidu Japan Inc. Now at Nara Institute of Science and Technology (NAIST) † of heuristics can be used to extract the bilingual phrases (Zens et al., 2002; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation"
W12-4207,P03-1021,0,0.0679623,"2010) by using the MT toolkit Moses (Koehn et al., 2007) in its default configuration. Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics “grow-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the parallel corpora by using the modified Kneser-Ney smoothing (Chen and Goodman, 1999) implemented in the SRILM (Stolcke, 2002) toolkit. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. The effectiveness of the reorderings proposed in Section 3.3 was assessed by using two precision metrics and two error metrics on translation quality. The first evaluation metric is BLEU (Papineni et al., 2002), a very common accuracy metric in SMT that measures N -gram precision, with a penalty for too short sentences. The second evaluation metric was RIBES (Isozaki et al., 2010a), a recent precision metric used to evaluate translation quality between structurally different languages. It uses notions on rank correlation coefficients and precision"
W12-4207,P02-1040,0,0.0870379,"w-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the parallel corpora by using the modified Kneser-Ney smoothing (Chen and Goodman, 1999) implemented in the SRILM (Stolcke, 2002) toolkit. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. The effectiveness of the reorderings proposed in Section 3.3 was assessed by using two precision metrics and two error metrics on translation quality. The first evaluation metric is BLEU (Papineni et al., 2002), a very common accuracy metric in SMT that measures N -gram precision, with a penalty for too short sentences. The second evaluation metric was RIBES (Isozaki et al., 2010a), a recent precision metric used to evaluate translation quality between structurally different languages. It uses notions on rank correlation coefficients and precision measures. The third evaluation metric is TER (Snover et al., 2006), another error metric that computes the minimum number of edits required to convert translated sentences into its corresponding references. Possible edits include insertion, deletion, subst"
W12-4207,2006.amta-papers.25,0,0.130258,"(Isozaki et al., 2010b). The essence 58 of this rule is to move the syntactic heads to the end of its dependency by swapping child nodes in a phrase structure tree when the head child appears before the dependent child. Isozaki et al. (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. The score results from several mainstream evaluation methods indicated that the translation quality had been improved; the scores of Word Error Rate (WER) and Translation Edit Rate (TER) (Snover et al., 2006) had especially been greatly reduced. 2.2 Chinese Deep Parsing Syntax-based reordering methods need parsed sentences as input. Isozaki et al. (2010b) used Enju, an HPSG-based deep parser for English, but they also discussed using other types of parsers, such as word dependency parsers and Penn Treebankstyle parsers. However, to use word dependency parsers, they needed an additional heuristic rule to recover phrase structures, and Penn Treebank-style parsers are problematic because they output flat phrase structures (i.e. a phrase may have multiple dependents, which causes a problem of reorderi"
W12-4207,N04-4026,0,0.0537842,"cCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2004) used a lexical reordering model, and Galley et al. (2004) followed a syntactic-based model. In this work, however, we are centered in the design of manual rules inspired by the Head Finalization (HF) reordering (Isozaki et al., 2010b). HF reordering is one of the simplest methods for preordering that significantly improves word alignments and leads to a better translation quality. Al59 though the method is limited to translation where the target language is head-final, it requires neither training data nor fine-tuning. To our knowledge, HF is the best method to reorder languages when translat"
W12-4207,N03-1033,0,0.00850002,"and extended CWMT Chinese-Japanese corpus. Dev. stands for Development, OoV for “Out of Vocabulary” words, K for thousands of elements, and M for millions of elements. Data statistics were collected after tokenizing. methods. Detailed Corpus statistics can be found in Table 6. To parse Chinese sentences, we used Chinese Enju (Yu et al., 2010), an HPSG-based parser trained with the Chinese HPSG treebank converted from Penn Chinese Treebank. Chinese Enju requires segmented and POS-tagged sentences to do parsing. We used the Stanford Chinese segmenter (Chang et al., 2008) and Stanford POStagger (Toutanova et al., 2003) to obtain the segmentation and POS-tagging of the Chinese side of the training, development, and test sets. The baseline system was trained following the instructions of recent SMT evaluation campaigns (Callison-Burch et al., 2010) by using the MT toolkit Moses (Koehn et al., 2007) in its default configuration. Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics “grow-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the p"
W12-4207,D07-1077,0,0.101287,"uhito Sudoh‡ Xianchao Wu‡∗ Kevin Duh‡† Hajime Tsukada‡ Masaaki Nagata‡ + The Graduate University For Advanced Studies, Tokyo, Japan ‡ NTT Communication Science Laboratories, NTT Corporation + handan@nii.ac.jp, ∗ wuxianchao@baidu.com, † kevinduh@is.naist.jp ‡ {sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp Abstract In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chineseto-English (Wang et al., 2007) and Englishto-Japanese (Isozaki et al., 2010b) translation with significant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules. 1 Introduction In state-of-the-art Statistical Machine Translation (SMT) systems, bilingual phrases are th"
W12-4207,I11-1004,1,0.873372,"02; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation quality diminishes. In the latter scenario, a simple but effective strategy to cope with this problem is to reorder the words of sentences in one language so that it resembles the word order of another language (Wu et al., 2011; Isozaki et al., 2010b). The advantages of this strategy are two fold. The first advantage is at the decoding stage, since it enables the translation to be constructed almost monotonically. The second advantage is at the training stage, since automatically estimated word-to-word alignments are likely to be more accurate and symmetrization matrices reveal more evident bilingual phrases, leading to the extraction of better quality bilingual phrases and cleaner phrase tables. In this work, we focus on Chinese-to-Japanese translation, motivated by the increasing interaction between these two coun"
W12-4207,C04-1073,0,0.304241,"ute “head” indicates which child node is the syntactic head. In this figure, &lt;head=“c4” id=“c3”> means that the node that has id=“c4” is the syntactic head of the node that has id=“c3”. Figure 1: An XML output for a Chinese sentence from Chinese Enju. For clarity, we only draw information related to the phrase structure and the heads. 2.3 Related Work Reordering is a popular strategy for improving machine translation quality when source and target languages are structurally very different. Researchers have approached the reordering problem in multiple ways. The most basic idea is preordering (Xia and McCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2"
W12-4207,N09-1028,0,0.298949,"the phrase structure and the heads. 2.3 Related Work Reordering is a popular strategy for improving machine translation quality when source and target languages are structurally very different. Researchers have approached the reordering problem in multiple ways. The most basic idea is preordering (Xia and McCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2004) used a lexical reordering model, and Galley et al. (2004) followed a syntactic-based model. In this work, however, we are centered in the design of manual rules inspired by the Head Finalization (HF) reordering (Isozaki et al., 2010b). HF reordering is one of the simplest methods for preorde"
W12-4207,C10-2162,0,0.0590977,"Missing"
W12-4207,W11-2907,0,0.172115,"word dependency parsers and Penn Treebankstyle parsers. However, to use word dependency parsers, they needed an additional heuristic rule to recover phrase structures, and Penn Treebank-style parsers are problematic because they output flat phrase structures (i.e. a phrase may have multiple dependents, which causes a problem of reordering within a phrase). Consequently, compared to different types of parsers, Head-Final English performs the best on the basis of English Enju’s parsing result. In this paper, we follow their observation, and use the HPSG-based parser for Chinese (Chinese Enju) (Yu et al., 2011) for Chinese syntactic parsing. Since Chinese Enju is based on the same parsing model as English Enju, it provides rich syntactic information including phrase structures and syntactic/semantic heads. Figure 1 shows an example of an XML output from Chinese Enju for the sentence “wo (I) qu (go to) dongjing (Tokyo) he (and) jingdu (Kyoto).” The label &lt;cons> and &lt;tok> represent the non-terminal nodes and terminal nodes, respectively. Each node is identified by a unique “id” and has several attributes. The attribute “head” indicates which child node is the syntactic head. In this figure, &lt;head=“c4”"
W12-4207,2002.tmi-tutorials.2,0,0.0338415,"atistical Machine Translation (SMT) systems, bilingual phrases are the main building blocks for constructing a translation given a sentence from a source language. To extract those bilingual phrases from a parallel corpus, the first step is to discover the implicit wordto-word correspondences between bilingual sentences (Brown et al., 1993). Then, a symmetrization matrix is built (Och and Ney, 2004) by using word-to-word alignments, and a wide variety ∗ Now at Baidu Japan Inc. Now at Nara Institute of Science and Technology (NAIST) † of heuristics can be used to extract the bilingual phrases (Zens et al., 2002; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation quality diminishes. In the latter scenario, a simple but effective strategy to cope with this problem is to reorder the words of sentences in one language so that it resembles the word order of another language (Wu"
W12-4207,J93-2003,0,\N,Missing
W12-4207,D08-1076,0,\N,Missing
W13-2263,N10-1015,0,0.0436995,"Missing"
W13-2263,de-marneffe-etal-2006-generating,0,0.00926866,"Missing"
W13-2263,P07-1003,0,0.7654,"an alignment vector such that aj = i indicates the j-th target word aligns to the i-th source word and aj = 0 means the j-th target word is null-aligned. j is the index of the last non null-aligned target word before the index j. In both models, pt (fj |eaj ) is the lexical translation probability and can be defined as conditional probability distributions. As for the distortion probability pd (aj |aj ), pd (aj = 0|aj = i0 ) = p0 where p0 is NULL probability in both models. pd (aj = i|aj = i0 ) is uniform in the Model 1 and proportional to the relative count c(i − i0 ) in the HMM for i 6= 0. DeNero and Klein (2007) proposed a syntax-sensitive distortion model for the HMM alignment, in which the distortion probability depends on the path from the i-th word to the i0 -th word on the source-side phrase-structure tree, instead of the linear distance between the two words. These models can be trained efficiently using the EM algorithm. In practice, models in two directions (source to target and target to source) are trained and then symmetrized by taking their intersection, union or using other heuristics. Liang et al. (2006) proposed a joint objective of alignment models in both directions and the probabili"
W13-2263,P08-1066,0,0.0343647,"ich improves translation rule extraction for tree-to-string transducers. Both models as508 (a) Precision/Recall Curve with Soft-Union. (b) Precision/Recall Curve with Soft-Union + Competitive Thresholding. (c) Precision/Recall Curve with the Best Strategy. (d) Alignment Error Rate with Soft-Union. (e) Precision/Recall Curve with Soft-Union + Competi- (f) Alignment Error Rate with with the Best Strategy. tive Thresholding. Figure 2: Precision/Recall Curve and Alignment Error Rate with Different Models and Strategies. 509 directly incorporate dependency structures, such as string-to-dependency (Shen et al., 2008) and dependency-to-string (Xie et al., 2011) models, would be especially interesting. Last but not least, though the dependency structures don’t pose a hard restriction on the alignment in our model, it is highly likely that parse errors have negative effects on the alignment accuracy. One way to estimate the effect of parse errors on the accuracy is to parse the input sentences with inferior models, for example trained on a limited amount of training data. Moreover, preserving some ambiguities using k-best trees or shared forests might help mitigate the effect of 1best parse errors. sume a ch"
W13-2263,N06-2015,0,0.141282,"Missing"
W13-2263,C96-2141,0,0.837224,"s a tree structure of the hidden variables, which fits well with the notion of word-toword dependency, and it can be trained from unlabeled data via the EM algorithm with the same order of time complexity as HMMs. Introduction Automatic word alignment is the first step in the pipeline of statistical machine translation. Translation models are usually extracted from wordaligned bilingual corpora, and lexical translation probabilities based on word alignment models are also used for translation. The most widely used models are the IBM Model 4 (Brown et al., 1993) and Hidden Markov Models (HMM) (Vogel et al., 1996). These models assume that alignments are largely monotonic, possibly with a few jumps. While such assumption might be adequate for alignment between similar languages, it does not necessarily hold between a pair of distant languages like English and Japanese. Recently, several models have focused on incorporating syntactic structures into word alignment. As an extension to the HMM alignment, Lopez and Resnik (2005) present a distortion model conditioned on the source-side dependency In this paper, we propose a novel word alignment model based on the HMT model and show that it naturally enable"
W13-2263,P07-2045,0,0.00516379,"ave a considerable amount of posterior probability. If that is true, too many links will be above the threshold when it is set low, and too few links can exceed the threshold when it is set high. More sophisticated distortion model may help mitigate such sensitivity to the posterior threshold. Table 2: Alignment error rates (AER) based on each model’s peak performance. with window size w = 4 for the distortion model. The entire training procedure takes around 4 hours on a 3.3 GHz Xeon CPU. We train the IBM Model 4 using GIZA++ (Och and Ney, 2003) with the training script of the Moses toolkit (Koehn et al., 2007). The HMM and S-HMM alignment models are initialized with jointly trained IBM Model 1 parameters (5 iterations) and trained independently for 5 iterations using the Berkeley Aligner. We find that though initialization with jointly trained IBM Model 1 parameters is effective, joint training of HMM alignment models harms the performance on this dataset (results not shown). 5.3 Result We use posterior thresholding for the HMT and HMM alignment models, and the grow-diag-finaland heuristic for the IBM Model 4. Table 2 and Figure 2 show the result. As the Soft-Union criterion performed best, we don’"
W13-2263,1983.tc-1.13,0,0.677602,"Missing"
W13-2263,P09-2037,0,0.211469,"bined weights c and a threshold τ , it choose a link (j, i) only if its weight cji ≥ τ and it is connected to the link with the maximum weight both in row j and column i. 3 Hidden Markov Tree Model The Hidden Markov Tree (HMT) model was first introduced by Crouse et al. (1998). Though it has been applied successfully to various applications such as image segmentation (Choi and Baraniuk, 2001), denoising (Portilla et al., 2003) and biology (Durand et al., 2005), it is largely unnoticed in the field of natural language processing. To the best of our knowledge, the only exception is ˇ Zabokrtsk` y and Popel (2009) who used a variant of the Viterbi algorithm for HMTs in the transfer phase of a deep-syntax based machine translation system. An HMT model consists of an observed random tree X = {x1 , ..., xN } and a hidden random tree S = {s1 , ..., sN }, which is isomorphic to the observed tree. The parameters of the model are • P (s1 = j), the initial hidden state prior • P (st = j|sρ(t) = i), transition probabilities • P (xt = h|st = j), emission probabilities, a = {(i, j) : p(aj = i|f , e) > τ } where ρ() is a function that maps the index of a hidden node to the index of its parent node. These parameter"
W13-2263,W02-2016,1,0.6706,"Berkeley Aligner’s syntactic distortion model, and convert them to dependency trees for our dependency-based distortion model5 . As the Berkeley Parser couldn’t parse 7 (out of about 330K) sentences in the training data, we removed those lines from both sides of the data. All the sentences in the other sets were parsed successfully. For the Japanese side of the data, we first concatenate the function words in the tokenized sentences using a script6 published by the author of the dataset. Then we re-segment and POStag them using MeCab7 version 0.996 and parse them using CaboCha8 version 0.66 (Kudo and Matsumoto, 2002), both with UniDic. Finally, we modify the CoNLL-format output of CaboCha where some kind of symbols such as punctuation marks and parentheses have dependent words. We chose this procedure for a reasonable compromise between the dataset’s default tokenization and the dependency parser we use. As we cannot use the default gold alignment due to the difference in preprocessing, we use a script9 published by the author of the dataset to modify the gold alignment so that it better matches the new tokenization. just like the IBM Models and HMM alignments, where c(f, e) and c(e) are the count of the"
W13-2263,N06-1014,0,0.420834,"odel 1 and proportional to the relative count c(i − i0 ) in the HMM for i 6= 0. DeNero and Klein (2007) proposed a syntax-sensitive distortion model for the HMM alignment, in which the distortion probability depends on the path from the i-th word to the i0 -th word on the source-side phrase-structure tree, instead of the linear distance between the two words. These models can be trained efficiently using the EM algorithm. In practice, models in two directions (source to target and target to source) are trained and then symmetrized by taking their intersection, union or using other heuristics. Liang et al. (2006) proposed a joint objective of alignment models in both directions and the probability of agreement between them, and an EM-like algorithm for training. They also proposed posterior thresholding for decoding and symmetrization, which take where pf (aj = i|f , e) is the alignment probability under the source-to-target model and pr (ai = j|f , e) is the one under the target-to-source model. They also propose a posterior decoding heuristic called competitive thresholding. Given a j × i matrix of combined weights c and a threshold τ , it choose a link (j, i) only if its weight cji ≥ τ and it is co"
W13-2263,W05-0812,0,0.151445,"slation probabilities based on word alignment models are also used for translation. The most widely used models are the IBM Model 4 (Brown et al., 1993) and Hidden Markov Models (HMM) (Vogel et al., 1996). These models assume that alignments are largely monotonic, possibly with a few jumps. While such assumption might be adequate for alignment between similar languages, it does not necessarily hold between a pair of distant languages like English and Japanese. Recently, several models have focused on incorporating syntactic structures into word alignment. As an extension to the HMM alignment, Lopez and Resnik (2005) present a distortion model conditioned on the source-side dependency In this paper, we propose a novel word alignment model based on the HMT model and show that it naturally enables unsupervised training based on both source and target dependency trees in a tractable manner. We also compare our HMT word alignment model with the IBM Model 4 and the HMM alignment models in terms of the standard alignment error rates on a publicly available English-Japanese dataset. 2 IBM Model 1 and HMM Alignment We briefly review the IBM Model 1 (Brown et al., 1993) and the Hidden Markov Model (HMM) word align"
W13-2263,I11-1089,0,0.038217,"Missing"
W13-2263,J03-1002,0,0.190331,"model, the initial hidden state prior described in Section 3 can be defined by assuming an artificial ROOT node for both dependency trees, forcing the target ROOT node to be aligned only to the source ROOT c(d(i0 , i)) 0 00 i00 6=0 c(d(i , i )) = (1 − p0 ) · P if the j-th word is aligned. Note that we must artificially normalize pd (aj = i|aρ(j) = i0 ), because unlike in the case of the linear distance, multiple words can have the same distance from the j-th word on a dependency tree. 1 This dependence on aj can be implemented as a firstorder HMT, analogously to the case of the HMM alignment (Och and Ney, 2003). 505 a0 ↓ −ROOT−f0 a1 ↓ Thatf1 a2 ↓ wasf2 a3 ↓ Mountf3 a4 ↓ Kuramaf4 a5 ↓ .f5 (a) Target sentence with its dependency/alignment tree. Target words {f0 , ..., f5 } are emitted from alignment variables {a0 , ..., a5 }. Ideally, a0 = 0, a1 = 1, a2 = 7, a3 = 5, a4 = 4 and a5 = 9. −ROOT −e0 そのe1 that 山e2 mountain がe3 鞍馬e4 Kurama 山 e5 mountain でe6 あっe7 be たe8 。 e9 . (b) Source sentence with its dependency tree. None of the target words are aligned to e2 , e3 , e6 and e8 . Figure 1: An example of sentence pair under the Hidden Markov Tree word alignment model. If we ignore the source words to which"
W13-2263,P06-1055,0,0.0519743,"Y { βj,j 0 (i)}pt (fj |ei )p(aj = i) i = j 0 ∈c(j) 506 βj (i)pd (aj = i|aρ(j) = i0 )ξρ(j) (i0 ) , p(aj = i)βρ(j),j (i0 ) which is used for the estimation of distortion probabilities, can be extracted during the downward recursion. In the M-step, the lexical translation model can be updated with The tuning set of the KFTT has manual alignments. As the KFTT doesn’t distinguish between sure and possible alignments, F-measure equals 1 − AER on this dataset. 5.1 c(f, e) pt (f |e) = , c(e) We tokenize the English side of the data using the Stanford Tokenizer3 and parse it with the Berkeley Parser4 (Petrov et al., 2006). We use the phrasestructure trees for the Berkeley Aligner’s syntactic distortion model, and convert them to dependency trees for our dependency-based distortion model5 . As the Berkeley Parser couldn’t parse 7 (out of about 330K) sentences in the training data, we removed those lines from both sides of the data. All the sentences in the other sets were parsed successfully. For the Japanese side of the data, we first concatenate the function words in the tokenized sentences using a script6 published by the author of the dataset. Then we re-segment and POStag them using MeCab7 version 0.996 an"
W13-2263,P10-1017,0,0.0190787,"ests might help mitigate the effect of 1best parse errors. sume a chain structure for hidden variables (alignment) as opposed to a tree structure as in our model, and condition distortions on the syntactic structure only in one direction. Nakazawa and Kurohashi (2011) propose a dependency-based phrase-to-phrase alignment model with a sophisticated generative story, which leads to an increase in computational complexity and requires parallel sampling for training. Several supervised, discriminative models use syntax structures to generate features and to guide the search (Burkett et al., 2010; Riesa and Marcu, 2010; Riesa et al., 2011). Such efforts are orthogonal to ours in the sense that discriminative alignment models generally use statistics obtained by unsupervised, generative models as features and can benefit from their improvement. It would be interesting to incorporate statistics of the HMT word alignment model into such discriminative models. ˇ Zabokrtsk` y and Popel (2009) use HMT models for the transfer phase in a tree-based MT system. While our model assumes that the tree structure of alignment variables is isomorphic to target side’s dependency tree, they assume that the deep-syntactic tre"
W13-2263,D11-1046,0,0.0358705,"Missing"
W13-2263,J93-2003,0,\N,Missing
W13-2263,D11-1020,0,\N,Missing
W13-3523,P08-1088,0,0.104927,"Science and Technology 8916-5 Takayama, Ikoma, Nara 630-0192, Japan {xiaodong-l,kevinduh,matsu}@is.naist.jp Abstract One approach for building a bilingual dictionary resource uses parallel sentence-aligned corpora. This is often done in the context of Statistical MT, using word alignment algorithms such as the IBM models (Brown et al., 1993; Och and Ney, 2003). Unfortunately, parallel corpora may be scarce for certain language-pairs or domains of interest (e.g., medical and microblog). Thus, the use of comparable corpora for bilingual dictionary extraction has become an active research topic (Haghighi et al., 2008; Vuli´c et al., 2011). Here, a comparable corpus is defined as collections of document pairs written in different languages but talking about the same topic (Koehn, 2010), such as interconnected Wikipedia articles. The challenge with bilingual dictionary extraction from comparable corpus is that existing word alignment methods developed for parallel corpus cannot be directly applied. We believe there are several desiderata for bilingual dictionary extraction algorithms: We propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora. Our approach is"
W13-3523,W11-0801,0,0.0288397,"translated as “panfry“). Table 3 shows the distribution of error types by a manual classification. Incorrect Alignment errors are most frequent, implying the topic models are doing a reasonable job of generating the topicaligned corpus. The amount of Incorrect Topic is not trivial, though, so we would still imagine more advanced topic models to help. Segmentation errors are in general hard to solve, even with a better word segmenter, since in general one-to-one crosslingual word correspondence is not consistent–we believe the solution is a system that naturally handles multi-word expressions (Baldwin, 2011). 5 10 15 20 Topic Count Figure 5: Power-law distribution of number of word types with X number of topics. topic model of (Mimno et al., 2009) assumes one topic distribution per document pair. For lowlevels of comparability, a small number of topics may not sufficiently model the differences in topical content. This suggests the use of hierarchical topic models (Haffari and Teh, 2009) or other variants in future work. 3. What are the statistical characteristics of topic-aligned corpora? First, we show the word-topic distribution from multilingual topic modeling in the K = 400 scenario (first s"
W13-3523,W02-0902,0,0.16114,"ceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 212–221, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c"
W13-3523,J93-2003,0,0.0904013,"Missing"
W13-3523,J10-4005,0,0.0165542,"parallel sentence-aligned corpora. This is often done in the context of Statistical MT, using word alignment algorithms such as the IBM models (Brown et al., 1993; Och and Ney, 2003). Unfortunately, parallel corpora may be scarce for certain language-pairs or domains of interest (e.g., medical and microblog). Thus, the use of comparable corpora for bilingual dictionary extraction has become an active research topic (Haghighi et al., 2008; Vuli´c et al., 2011). Here, a comparable corpus is defined as collections of document pairs written in different languages but talking about the same topic (Koehn, 2010), such as interconnected Wikipedia articles. The challenge with bilingual dictionary extraction from comparable corpus is that existing word alignment methods developed for parallel corpus cannot be directly applied. We believe there are several desiderata for bilingual dictionary extraction algorithms: We propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora. Our approach is based on a novel combination of topic modeling and word alignment techniques. Intuitively, our approach works by converting a comparable document-aligned corpus into a pa"
W13-3523,C10-1070,0,0.118909,"Missing"
W13-3523,P11-2071,0,0.0551353,"Missing"
W13-3523,P09-1030,0,0.0301144,"Missing"
W13-3523,C02-1166,0,0.107487,"Missing"
W13-3523,P11-2032,0,0.0177225,"k ). While this kind of topic-dependent translation has been proposed for the parallel corpus (Zhao and Xing, 2007), we are the first to enable it for comparable corpora. Our large-scale experiments demonstrated that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. We further show that our topicdependent translation models can capture some of the polysemy phenomenon important in dictionary construction. Future work includes: 1. Exploring other topic models (Haffari and Teh, 2009) and word alignment techniques (DeNero and Macherey, 2011; Mermer and Saraclar, 2011; Moore, 2004) in our framework. 2. Extract lexicon from massive multilingual collections. Mausum (2009) and Shezaf (2010) show that language pivots significantly improve the precision of distribution-based approaches. Since multilingual topic models can easily be trained on more than 3 languages, we expect it will give a big boost to our approach. Acknowledgments We thank Mamoru Komachi, Shuhei Kondo and the anonymous reviewers for valuable discussions and comments. Part of this research was executed under the Commissioned Research of National Institute of Information and Communications Techn"
W13-3523,D09-1092,0,0.679415,"uli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explained in Section 4.2). They do not use word alignments like we do and thus cannot model polysemy. Further, their approach requires training topic models with a large number of topics, which may limit the scalability of the approach. Recently, there has been much interest in multilingual topic models (MLTM) (Jagarlamudi and Daume, 2010; Mimno et al., 2009; Ni et al., 2009; Boyd-Graber and Blei, 2009). Many of these models give p(t|e) and p(t|f ), but stop short of extracting a bilingual lexicon. Although topic models can group related e and f in the same topic cluster, the extraction of a high-precision dictionary requires additional effort. One of our contributions here is an effective way to do this extraction using word alignment methods. Figure 1: Proposed Framework allel topic-aligned corpus, then apply word alignment methods to model co-occurence within topics. By employing topic models, we avoid the need for seed lexicon and operate pur"
W13-3523,P11-1043,0,0.0170978,"ls of the form p(we |wf , tk ). While this kind of topic-dependent translation has been proposed for the parallel corpus (Zhao and Xing, 2007), we are the first to enable it for comparable corpora. Our large-scale experiments demonstrated that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. We further show that our topicdependent translation models can capture some of the polysemy phenomenon important in dictionary construction. Future work includes: 1. Exploring other topic models (Haffari and Teh, 2009) and word alignment techniques (DeNero and Macherey, 2011; Mermer and Saraclar, 2011; Moore, 2004) in our framework. 2. Extract lexicon from massive multilingual collections. Mausum (2009) and Shezaf (2010) show that language pivots significantly improve the precision of distribution-based approaches. Since multilingual topic models can easily be trained on more than 3 languages, we expect it will give a big boost to our approach. Acknowledgments We thank Mamoru Komachi, Shuhei Kondo and the anonymous reviewers for valuable discussions and comments. Part of this research was executed under the Commissioned Research of National Institute of Informati"
W13-3523,P04-1066,0,0.0348541,"ic-dependent translation has been proposed for the parallel corpus (Zhao and Xing, 2007), we are the first to enable it for comparable corpora. Our large-scale experiments demonstrated that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. We further show that our topicdependent translation models can capture some of the polysemy phenomenon important in dictionary construction. Future work includes: 1. Exploring other topic models (Haffari and Teh, 2009) and word alignment techniques (DeNero and Macherey, 2011; Mermer and Saraclar, 2011; Moore, 2004) in our framework. 2. Extract lexicon from massive multilingual collections. Mausum (2009) and Shezaf (2010) show that language pivots significantly improve the precision of distribution-based approaches. Since multilingual topic models can easily be trained on more than 3 languages, we expect it will give a big boost to our approach. Acknowledgments We thank Mamoru Komachi, Shuhei Kondo and the anonymous reviewers for valuable discussions and comments. Part of this research was executed under the Commissioned Research of National Institute of Information and Communications Technology (NICT),"
W13-3523,P11-2093,0,0.0235328,"Missing"
W13-3523,W04-3208,0,0.0316964,"ning, pages 212–221, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explaine"
W13-3523,P04-1067,0,0.342167,"that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explained in Section 4.2). They do not use word alignments like we do and thus cannot model polysemy. Further, their approach requires training topic models with a large number of topics, which m"
W13-3523,J03-1002,0,0.0122219,"e known. For each document pair di = [dei , dfi ] consisting of English document dei and Foreign document dfi (where i ∈ {1, . . . , D}, D is number of document pairs), we know that dei and dfi talk about the same topics. While the monolingual topic model lets each document have its own so-called documentspecific distribution over topics, the multilingual topic model assumes that documents in each tuple share the same topic prior (thus the comparable corpora assumption) and each topic consists of several language-specific word distributions. The generative story is shown in Algorithm 1. 1993; Och and Ney, 2003), which proposes the following probabilistic model for alignment: p(e, a, |f ) ≈ f p(wie |wa(i) ) i=1 f p(wie |wa(i) ) (1) Here, captures the translation probability of the English word at position i from the foreign word at position j = a(i), where the actual alignment a is a hidden variable, and training can be done via EM. Although this model does not incorporate much linguistic knowledge, it enables us to find correspondence between distinct objects from paired sets. In machine translation, the distinct objects are words from different languages while the paired sets are sentence-aligned c"
W13-3523,P95-1050,0,0.492305,"s also rely on bilingual dictionaries as integral components. 3. Scalability: The approach should run efficiently an massively large-scale datasets. Our framework addresses the above desired points by exploiting a novel combination of topic models and word alignment, as shown in Figure 1. Intuitively, our approach works by first converting a comparable document-aligned corpus into a par212 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 212–221, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related"
W13-3523,N09-1020,0,0.0892861,"general hard to solve, even with a better word segmenter, since in general one-to-one crosslingual word correspondence is not consistent–we believe the solution is a system that naturally handles multi-word expressions (Baldwin, 2011). 5 10 15 20 Topic Count Figure 5: Power-law distribution of number of word types with X number of topics. topic model of (Mimno et al., 2009) assumes one topic distribution per document pair. For lowlevels of comparability, a small number of topics may not sufficiently model the differences in topical content. This suggests the use of hierarchical topic models (Haffari and Teh, 2009) or other variants in future work. 3. What are the statistical characteristics of topic-aligned corpora? First, we show the word-topic distribution from multilingual topic modeling in the K = 400 scenario (first step of Proposed, Cue, and JS). For each word type w, we count the number of topics it may appear in, i.e. nonzero probabilities according to p(w|t). Fig. 5 shows the number of word types that have x number of topics. This powerlaw is expected since we are modeling all words.9 Next we compute the statistics after constructing the topic-aligned corpora (Step 3 of Fig. 2). For each part"
W13-3523,P10-1011,0,0.163841,"Missing"
W13-3523,D12-1003,0,0.177235,"Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explained in Section 4.2). The"
W13-3523,P11-2084,0,0.222493,"Missing"
W13-3523,H01-1033,0,\N,Missing
W13-4409,P00-1032,0,0.304001,"Missing"
W13-4409,O03-4002,0,0.0907584,"uency of a correct phrase is higher than the corresponding error template. Wu et al. (2010) proposed a system which implemented a translate model and a template module. Then the system merged the output of the two single models and reached a balanced performance on precision and recall. 3 Figure 1: System structure. 3.1 Language Model Based Method To generate the correction candidates, firstly we segment the sentence into words and then find all possible corrections based on the confusion set and a Chinese dictionary. In this study, we use the character based Chinese word segmentation model2 (Xue, 2003), which outperforms the word based word segmentation model in out-of-vocabulary recall. The model is trained on the Academia Sinica corpus, released under the Chinese word segmentation bake-off 20053 and the feature templates are the same in Sun (2011). For example, given the following Chinese sentence (here, the Chinese character in red indicates an error character): ”我 看 過 許 多 勇 敢 的 人 ， 不 怕措折 地 奮 鬥。”. Firstly, we segment the sentence into words separated by a slash as follows. ”我/看過/許多/勇敢/的/人/，/不怕/措折/的/ 奮鬥/。” . Secondly, we build a lattice, as shown in Figure 2, based on the following rules:"
W13-4409,O09-2007,0,0.0386347,"Missing"
W13-4409,W10-4107,0,0.157669,"detect Chinese spelling errors. They used CKIP word segmentation toolkit to generate correction candidates (CKIP, 1999). By incorporating a dictionary and confusion sets, the system can detect whether a segmented word contains error or not. Hung et al. (2008) proposed a system which was based on manually edited error templates (short phrases with one error). For the cost of editing error templates manually, Cheng et al. (2008) proposed an automatic error template generation system. The basic assumption is that the frequency of a correct phrase is higher than the corresponding error template. Wu et al. (2010) proposed a system which implemented a translate model and a template module. Then the system merged the output of the two single models and reached a balanced performance on precision and recall. 3 Figure 1: System structure. 3.1 Language Model Based Method To generate the correction candidates, firstly we segment the sentence into words and then find all possible corrections based on the confusion set and a Chinese dictionary. In this study, we use the character based Chinese word segmentation model2 (Xue, 2003), which outperforms the word based word segmentation model in out-of-vocabulary r"
W13-4409,J93-2003,0,\N,Missing
W13-4409,P11-1139,0,\N,Missing
W14-0819,C10-3010,0,0.0310546,"gual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, c Gothenburg, Sweden, 26-27 April 2014. 2014 Association for Computational Linguistics Salehi and Cook (2013) predict the degree of compositionality using the string distance between the automatic translation into multiple languages of an expression and the individual translation of its components. They use an online database called Panlex (Baldwin et al., 2010), that can translate words and expressions from English into many languages. Tsvetkov and Wintner (2013) is probably the closest work to ours. They trained a Bayesian Network for identfying MWE’s and one of the features used is a binary feature that assumes value is 1 if the literal translation of the MWE candidate occurs more than 5 times in a large English corpus. 3 Identifying Collocations In this research, we predict whether the expression(s) resulted from the translation of the components of a Japanese collocation candidate is/are also commonly used in English. For instance, if we transla"
W14-0819,N03-1017,0,0.0222043,"our proposed method, for each candidate in the test set, we find all the possible literally translated expressions (as described in Section 3). In the phrase-table generated after the training step, we look for all the entries that contain the original candidate string and check if at least one of the possible literal translations appear as their corresponding translation. For the entries found, we compute the average of the sum of the candidate’s direct and inverse phrase translation probability scores. The direct phrase translation probability and the inverse phrase translation probability (Koehn et al., 2003) are respectively defined as: count(f , e) Φ(e|f ) = P f count(f , e) (1) count(f , e) Φ(f |e) = P e count(f , e) (2) Where f and e indicate a foreign phrase and a source phrase, independently. The candidates are ranked according to the average score as described previously. 5 Evaluation In our evaluation, we average the precision considering all true collocations and idioms as threshold points, obtaining the mean average precision (MAP). Differently from the traditional approach used to evaluate an association measure, using MAP we do not need to set a hard threshold. 112 Table 2 presents the"
W14-0819,P07-2045,0,0.0045739,"ee combinations). After that, we obtained a new Kappa coefficient of 0.5427, which is also considered as showing moderate agreement (Fleiss, 1971). 4.3 Baseline We compare our proposed method with two baselines: an association measure based system and a Phrase-Based Statistical Machine Translation (SMT) based system. Monolingual Association Measure: The system ranks the candidates in the test set according to their Dice score calculated using the Hiragana Times Japanese data. Phrase-Based SMT system: a standard nonfactored phrase-based SMT system was built using the open source Moses toolkit (Koehn et al., 2007) with parameters set similar to those of Neubig (2011), who provides a baseline system previously applied to a Japanese-English corpus built from Wikipedia articles. For training, we used Hiragana Times bilingual corpus. The Japanese sentences were word-segmented and the English sentences were tokenized and lowercased. All sentences with size greater than 60 tokens were previously eliminated. The whole English corpus was used as training data for a 5-gram language model built with the SRILM toolkit (Stolcke, 2002). Similar to what we did for our proposed method, for each candidate in the test"
W14-0819,W02-2016,1,0.833863,"is more prone to appear in an English corpus, since it corresponds to the translation of the expression as well. In our work, we focus on noun-verb expressions in Japanese. Our proposed method consists of three steps: 1) Candidate Extraction: We focus on nounverb constructions in Japanese. We work with three construction types: object-verb, subject-verb and dative-verb constructions, represented respectively as “noun wo verb (noun-を-verb)”, “noun ga verb (noun-が-verb)” and “noun ni verb (noun-にverb)”, respectively. The candidates are extracted from a Japanese corpus using a dependency parser (Kudo and Matsumoto, 2002) and ranked by frequency. 2) Translation of the component words: for each noun-verb candidate, we automatically obtain all the possible English literal translations of the noun and the verb using a Japanese/English dictionary. Using that information, all the possible verb-noun combinations in English are then generated. For instance, for the candidate 本を 1 In Japanese, を is a case marker that indicates the objectverb dependency relation. 110 買う hon-wo-kau ”to buy a book” (buy-を-book), we take the noun 本 hon and the verb 買う kau and check their translation given in the dictionary. 本 has translat"
W14-0819,W97-0311,0,0.150133,"asures are applied to rank the candidates based on association scores and consequently remove noise. One drawback of such method is that association measures might not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual association measure ameliorates this problem by exploiting both corpora in two languages, one of which may be large. A few studies have attempted to identify noncompositional MWE’s using parallel corpora and dictionaries. Melamed (1997) investigates how non-compositional compounds can be detected from parallel corpus by identifying translation divergences in the component words. Pichotta and DeNero (2013) analyses the frequency statistics of an expression and its component words, using many bilingual corpora to identifying phrasal verbs in English. The disadvantage of such approach is that large-scale parallel corpora is available for only a few language pairs. On the other hand, monolingual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual"
W14-0819,D13-1060,0,0.0192611,"ght not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual association measure ameliorates this problem by exploiting both corpora in two languages, one of which may be large. A few studies have attempted to identify noncompositional MWE’s using parallel corpora and dictionaries. Melamed (1997) investigates how non-compositional compounds can be detected from parallel corpus by identifying translation divergences in the component words. Pichotta and DeNero (2013) analyses the frequency statistics of an expression and its component words, using many bilingual corpora to identifying phrasal verbs in English. The disadvantage of such approach is that large-scale parallel corpora is available for only a few language pairs. On the other hand, monolingual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, c Gothenburg, Sweden, 26-27 April 2014. 2014 Associatio"
W14-0819,W12-3311,0,0.346846,"meaning of the expression cannot be predicted from the meaning of the parts, i.e. they are characterized by limited compositionality (Manning and Sch¨utze, 1999). Given an expression, we predict whether the expression(s) resulted from the word by word translation is also commonly used in another language. If not, that might be evidence that the original expression is a collocation (or an idiom). This can be captured by the ratio of association scores, assigned Related Work Most previous works on MWEs and, more specifically, collocation identification (Evert, 2008; Seretan, 2011; Pecina, 2010; Ramisch, 2012) employ a standard methodology consisting of two steps: 1) candidate extraction, where candidates are extracted based on n-grams or morphosyntactic patterns and 2) candidate filtering, where association measures are applied to rank the candidates based on association scores and consequently remove noise. One drawback of such method is that association measures might not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual association measu"
W14-0819,S13-1039,0,0.0197324,"cs of an expression and its component words, using many bilingual corpora to identifying phrasal verbs in English. The disadvantage of such approach is that large-scale parallel corpora is available for only a few language pairs. On the other hand, monolingual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, c Gothenburg, Sweden, 26-27 April 2014. 2014 Association for Computational Linguistics Salehi and Cook (2013) predict the degree of compositionality using the string distance between the automatic translation into multiple languages of an expression and the individual translation of its components. They use an online database called Panlex (Baldwin et al., 2010), that can translate words and expressions from English into many languages. Tsvetkov and Wintner (2013) is probably the closest work to ours. They trained a Bayesian Network for identfying MWE’s and one of the features used is a binary feature that assumes value is 1 if the literal translation of the MWE candidate occurs more than 5 times in"
W14-0819,D11-1077,0,\N,Missing
W15-2519,W11-2022,0,0.11599,"oceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 142–152, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 3.1 2014). On the other hand, it is also reported that certain English explicit DCs are not translated explicitly in French or German (Meyer and Webber, 2013). We hypothesize that explicitation is more common in Chinese-to-English translation. To incorporate DC translation in SMT, explicit DCs are annotated in French-English parallel corpus and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of discourse units. Syntactic parsing of Chinese complex sentences (CCS) (Zhou, 2004) covers certain intersentential discourse relations, including both explicit and implicit relations. Tu et al. (2014) presents a CCS-tree-to-string translation model in which translation rules and"
W15-2519,P07-2045,0,0.00633869,"translate the implicit DCs. We insert the most frequent fine sense of the annotated coarse sense to the source text4 . Referring to the same example, ‘而且’ (‘and’) is inserted at position [1] because it is the most frequent fine sense under the coarse sense Expansion. MT Settings We train baseline MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use t"
W15-2519,2012.amta-papers.20,0,0.0375184,"Missing"
W15-2519,P13-4016,0,0.0135362,"nse of the annotated coarse sense to the source text4 . Referring to the same example, ‘而且’ (‘and’) is inserted at position [1] because it is the most frequent fine sense under the coarse sense Expansion. MT Settings We train baseline MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use these SMT models to translate the source text in which impl"
W15-2519,P03-1056,0,0.0196722,"line MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use these SMT models to translate the source text in which implicit DCs are explicitated by the methods described in Section 4.1. 1178 sentences and 1175 sentences of the manually annotated parallel corpus are used as the tuning and test sets respectively. The systems are tuned with the tuning set prep"
W15-2519,J03-1002,0,0.00481282,"nt fine sense under the coarse sense Expansion. MT Settings We train baseline MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use these SMT models to translate the source text in which implicit DCs are explicitated by the methods described in Section 4.1. 1178 sentences and 1175 sentences of the manually annotated parallel corpus are used as the tun"
W15-2519,P14-2047,0,0.06343,"native lexicalization’ of a discourse relation that cannot be isolated from context as an explicit DC, e.g. ‘it was followed by’ for a Temporal relation. Prepositions that mark discourse To investigate how DCs are translated from Chinese to English, we manually align DCs in the source to their translations on a parallel corpus. The DCs are further annotated with their nature and senses. This section describes the strategy and findings of our annotation. 1 Our annotation is independent of existing monolingual discourse annotation on the Chinese Treebank such as the CDTB(Zhou and Xue, 2015) and Li et al. (2014b) 143 relations are also labeled ‘AltLex’, such as ‘through’ for a Contingency relation. This label is defined on English side only. Each pair of aligned DCs are thus tagged with 8 labels. Some annotation examples are shown below.   Example 1 • Coarse sense: We first group the DCs under the 4 top-level discourse senses defined in PDTB, namely Expansion, Contingency, Comparison and Temporal. 中国必须对国有企业进行改革, [1]加强本身的竞争 力。 China must implement reforms on state-owned enterprises so as to [1] improve its own competitiveness. . [1]nature: actual DC: fine sense: coarse sense: • Fine sense: The sens"
W15-2519,P03-1021,0,0.00958229,"cause it is the most frequent fine sense under the coarse sense Expansion. MT Settings We train baseline MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use these SMT models to translate the source text in which implicit DCs are explicitated by the methods described in Section 4.1. 1178 sentences and 1175 sentences of the manually annotated"
W15-2519,C14-1055,0,0.428382,"native lexicalization’ of a discourse relation that cannot be isolated from context as an explicit DC, e.g. ‘it was followed by’ for a Temporal relation. Prepositions that mark discourse To investigate how DCs are translated from Chinese to English, we manually align DCs in the source to their translations on a parallel corpus. The DCs are further annotated with their nature and senses. This section describes the strategy and findings of our annotation. 1 Our annotation is independent of existing monolingual discourse annotation on the Chinese Treebank such as the CDTB(Zhou and Xue, 2015) and Li et al. (2014b) 143 relations are also labeled ‘AltLex’, such as ‘through’ for a Contingency relation. This label is defined on English side only. Each pair of aligned DCs are thus tagged with 8 labels. Some annotation examples are shown below.   Example 1 • Coarse sense: We first group the DCs under the 4 top-level discourse senses defined in PDTB, namely Expansion, Contingency, Comparison and Temporal. 中国必须对国有企业进行改革, [1]加强本身的竞争 力。 China must implement reforms on state-owned enterprises so as to [1] improve its own competitiveness. . [1]nature: actual DC: fine sense: coarse sense: • Fine sense: The sens"
W15-2519,D14-1224,0,0.0530394,"native lexicalization’ of a discourse relation that cannot be isolated from context as an explicit DC, e.g. ‘it was followed by’ for a Temporal relation. Prepositions that mark discourse To investigate how DCs are translated from Chinese to English, we manually align DCs in the source to their translations on a parallel corpus. The DCs are further annotated with their nature and senses. This section describes the strategy and findings of our annotation. 1 Our annotation is independent of existing monolingual discourse annotation on the Chinese Treebank such as the CDTB(Zhou and Xue, 2015) and Li et al. (2014b) 143 relations are also labeled ‘AltLex’, such as ‘through’ for a Contingency relation. This label is defined on English side only. Each pair of aligned DCs are thus tagged with 8 labels. Some annotation examples are shown below.   Example 1 • Coarse sense: We first group the DCs under the 4 top-level discourse senses defined in PDTB, namely Expansion, Contingency, Comparison and Temporal. 中国必须对国有企业进行改革, [1]加强本身的竞争 力。 China must implement reforms on state-owned enterprises so as to [1] improve its own competitiveness. . [1]nature: actual DC: fine sense: coarse sense: • Fine sense: The sens"
W15-2519,W12-1614,0,0.0922438,"Missing"
W15-2519,D09-1036,0,0.162578,"Missing"
W15-2519,D13-1094,0,0.195558,"Missing"
W15-2519,P09-1077,0,0.18546,"Missing"
W15-2519,P02-1047,0,0.119414,"Missing"
W15-2519,W12-0117,0,0.20856,"ond Workshop on Discourse in Machine Translation (DiscoMT), pages 142–152, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 3.1 2014). On the other hand, it is also reported that certain English explicit DCs are not translated explicitly in French or German (Meyer and Webber, 2013). We hypothesize that explicitation is more common in Chinese-to-English translation. To incorporate DC translation in SMT, explicit DCs are annotated in French-English parallel corpus and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of discourse units. Syntactic parsing of Chinese complex sentences (CCS) (Zhou, 2004) covers certain intersentential discourse relations, including both explicit and implicit relations. Tu et al. (2014) presents a CCS-tree-to-string translation model in which translation rules and language model are conditioned"
W15-2519,prasad-etal-2008-penn,0,0.0309732,"Missing"
W15-2519,W13-3303,0,0.143615,"course connectives’ (DCs) or implicitly inferred. The markedness of discourse relations varies across languages. For 2 Related Work In translation studies, explicitation of implicit DCs is observed in translations between European languages (Becher, 2011; Zuffery and Cartoni, 142 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 142–152, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 3.1 2014). On the other hand, it is also reported that certain English explicit DCs are not translated explicitly in French or German (Meyer and Webber, 2013). We hypothesize that explicitation is more common in Chinese-to-English translation. To incorporate DC translation in SMT, explicit DCs are annotated in French-English parallel corpus and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of"
W15-2519,J14-4007,0,0.0436237,"label is defined on English side only. Each pair of aligned DCs are thus tagged with 8 labels. Some annotation examples are shown below.   Example 1 • Coarse sense: We first group the DCs under the 4 top-level discourse senses defined in PDTB, namely Expansion, Contingency, Comparison and Temporal. 中国必须对国有企业进行改革, [1]加强本身的竞争 力。 China must implement reforms on state-owned enterprises so as to [1] improve its own competitiveness. . [1]nature: actual DC: fine sense: coarse sense: • Fine sense: The sense hierarchy of PDTB is always modified in comparable discourse corpora of different languages (Prasad et al., 2014). Instead of defining a list of senses that cover discourse relations of both languages, we group interchangeable explicit DCs under the same category, and the category serves as the ‘fine sense’ label. For example, ‘besides’ ,‘moreover’ and ‘in addition’ are all annotated with the fine sense ‘in addition’. Similar to DC identification, the list of fine senses is built in the course of annotation. In total, there are 74 Chinese and 75 English fine senses (See Table 2). Chinese implicit nil 来 Contingency English explicit so as to in order to Contingency   Example 2   [1] 在投资项目上比上年减少四百四十四件,但"
W15-2519,E14-1068,0,0.0350057,"the English translation. To correctly model the translation of implicit relations, do we need a discourse parser that classifies an implicit source DC to its fine sense or coarse sense? Or will SMT robustly handle implicit-toexplicit DC translation without any discourse preprocessing? We seek to answer these questions in the next section. 4 Total 227(74) 156(61) Total −(54) −(39) With an automatic discoure parser, a discoursetree-to-string translation model can be built. Nonetheless, state-of-the-art accuracy of implicit discourse sense classification is still low for downstream application (Rutherford and Xue, 2014). In this work, we design oracle experiments to evaluate the MT of implicit DCs assuming that the gold discourse sense is given. 4.1 Table 2: Number of unique DCs and DC fine senses (in brackets)3 Explicitating implicit DCs for MT based on manual annotation Method In our annotation scheme, implicit DCs senses are defined by DCs that are identified during explicit DC annotation. In other words, the implicit DCs are represented by explicit DC that acturally occur in Chinese discourse. We hypothize that explicitating implicit DCs in the source based on manual annotation will improve implicit-to-e"
W15-2519,N15-1081,0,0.0114875,"ly In addition, we compare the contexts in which added DC to the reference translation (Li et al., a source implicit DC is translated into an explicit 2014a). Therefore, to improve implicit-to-explicit DC or by other means (by implicit DC or alternaDC translation, an additional task should be detive lexicalization). If the contexts are similar, it fined to identify whether a source implicit DC is suggests that the translation strategy could be an kept implicit, explicitly translated to an ambigous option independent of the context. DC such as ‘and’, or explicitly translated to other Following Rutherford and Xue (2015), we deunambiguous DCs. fine the context of a discourse relation as the uniGenerally, it is pragmatically correct to use gram distribution of words in the 2 arguments con‘and’ to translate an implicit discourse relation, nected by the relation. The context of a particular or to keep the relation implicit as in the source. discourse usage is thus the sum of the unigram disNonetheless, repetatively using this stragegy will tributions of all discourse relations associated with result in excessively long sentences, as in the exthat usage. We also use the Jensen-Shannon Diverample below. In this ca"
W15-2519,P13-2066,0,0.13941,"other hand, it is also reported that certain English explicit DCs are not translated explicitly in French or German (Meyer and Webber, 2013). We hypothesize that explicitation is more common in Chinese-to-English translation. To incorporate DC translation in SMT, explicit DCs are annotated in French-English parallel corpus and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of discourse units. Syntactic parsing of Chinese complex sentences (CCS) (Zhou, 2004) covers certain intersentential discourse relations, including both explicit and implicit relations. Tu et al. (2014) presents a CCS-tree-to-string translation model in which translation rules and language model are conditioned by automatic CCS parse. Improved BLEU scores are reported, but it is not clear how much the translation of implicit DCs has been improved. The parallel corpus comes f"
W15-2519,P14-1080,0,0.11848,"s and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of discourse units. Syntactic parsing of Chinese complex sentences (CCS) (Zhou, 2004) covers certain intersentential discourse relations, including both explicit and implicit relations. Tu et al. (2014) presents a CCS-tree-to-string translation model in which translation rules and language model are conditioned by automatic CCS parse. Improved BLEU scores are reported, but it is not clear how much the translation of implicit DCs has been improved. The parallel corpus comes from 325 newswire articles (2353 sentences) of the the Chinese Treebank and their English translation (Palmer et al., 2005; Bies et al., 2007)1 . The annotation was carried out by 1 professional Chinese-English translator. We use translation spotting technique (Meyer et al., 2011) to align the DCs crosslingually, consideri"
W15-2519,W15-3101,1,0.506415,"ditioned by automatic CCS parse. Improved BLEU scores are reported, but it is not clear how much the translation of implicit DCs has been improved. The parallel corpus comes from 325 newswire articles (2353 sentences) of the the Chinese Treebank and their English translation (Palmer et al., 2005; Bies et al., 2007)1 . The annotation was carried out by 1 professional Chinese-English translator. We use translation spotting technique (Meyer et al., 2011) to align the DCs crosslingually, considering both explicit and implicit DCs. Annotation is carried out on the raw texts. Readers are refered to Yung et al. (2015) for details concerning the Chinese side annotation, such as definition of discourse units and annotation policy for parallel connectives. The labels used in the crosslingual annotation are defined as follows: • Explicit DC: An explicit DC is a lexical expression that connects two discourse units with a relation. We do not define a close set of explicit DCs to be annotated. The list is constructed in the course of annotation. We also do not limit the syntactic categories of the DCs. In total, 227 Chinese and 152 English DCs are identified. (See Table 2) • Implicit DC: An implicit DC is an impl"
W15-3101,D14-1224,0,0.414648,"Missing"
W15-3101,P11-1100,0,0.0557413,"Missing"
W15-3101,W01-1605,0,0.139506,"a discourse argument. Above the clause level, Chinese sentences (marked by ‘。’) are also units of discourse (Chu, 1998). When presented with texts where periods and commas are removed, native Chinese speakers disagree with where to restore them (Bittner, 2013). The actual sentence segmentation of the text thus represents the spans of discourse arguments intended by the writer and should be taken into account. Secondly, it is well known that syntactical structure is presented by word order in Chinese - so is Related Work Major discourse annotated resources in English include the RST Treebank (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). The RST Treebank represents discourse relations in a tree structure, where a satellite text span is related to a nucleus text span. 1 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 1–6, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing discourse. While the Arg1 can occur before or after Arg2 in English, arguments predominantly occur in fixed order in Chinese, depending on the logical relation. For example"
W15-3101,P09-1077,0,0.0852398,"rguments of each relation. We evaluate the accuracy of each component and the overall accuracy of the final output, classifying up to the 4 main senses. The pipeline consists of 5 classifiers, as shown in Figure 1, each of which is trained with the relevant samples, e.g. only arguments annotated with explicit DCs are used to train the explicit DC classifier. 289 and 36 articles are used as training and testing data respectively. Features include lexical and syntactical features (bag of words, bag of POS, word pairs and production rules) that have been used in classifying implicit English DCs (Pitler et al., 2009; Lin et al., 2010), and probability distribution of senses for explicit DC classification. The extraction of features is based on automatic parsing by the Stanford Parser (Levy and Manning, 2003). We also use the surrounding discourse relations as features, hypothesizing that certain relation sequences are more likely than others. The classifiers are trained by SVM with a linear kernel using the LIBSVM package(Chang and Lin, 2011). • Cause-result v.s. result-cause order: 因为...所以... (yinwei...suoyi..., because... therefore...) and 之 所 以...是 因 为... (zhisuoyi...shiyinwei..., the reason why...is"
W15-3101,prasad-etal-2008-penn,0,0.103922,"es (marked by ‘。’) are also units of discourse (Chu, 1998). When presented with texts where periods and commas are removed, native Chinese speakers disagree with where to restore them (Bittner, 2013). The actual sentence segmentation of the text thus represents the spans of discourse arguments intended by the writer and should be taken into account. Secondly, it is well known that syntactical structure is presented by word order in Chinese - so is Related Work Major discourse annotated resources in English include the RST Treebank (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). The RST Treebank represents discourse relations in a tree structure, where a satellite text span is related to a nucleus text span. 1 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 1–6, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing discourse. While the Arg1 can occur before or after Arg2 in English, arguments predominantly occur in fixed order in Chinese, depending on the logical relation. For example, the same concession relation can be expressed by both const"
W15-3101,N13-1100,0,0.0413613,"Missing"
W15-3101,D14-1196,0,0.0528809,"Missing"
W15-3101,C12-2138,0,0.0449565,"Missing"
W15-3101,zhou-etal-2014-cuhk,0,0.0179967,"aul, 1996; Chu and Ji, 1999). The clauses are semantically arranged in a topic-comment sequence following the writer’s conceptual mind (Tai, 1985; Bittner, 2013). When the arguments are not arranged in the standard order, the sense of the DC is altered. For example, when ‘虽 然’ (suiran, although’ is used in construction (2), it represents an ‘expansion’ relation (Huang et al., 2014). Therefore, discourse relations should be defined given the order of the arguments. Lastly, parallel DCs are frequent in Chinese discourse, yet usually either one DC of the pair occurs to signify the same relation (Zhou et al., 2014). For example, (3) and (4) are grammatical alternatives to (1). Arguments Each clause separated by punctuations except quotation marks is treated as a candidate argument. Clauses that do not function as discourse units are classified into 3 types - attribution, optional punctuation and non-discourse adverbial. The main difference of our annotation scheme is that the the order of the arguments for each DC is defined by default. Since the arguments of a particular discourse relation occur in fixed order and are always adjacent, each argument is related to the immediately preceding argument by a"
W15-3101,W12-1636,0,\N,Missing
W15-3101,P03-1056,0,\N,Missing
W15-3101,I11-1170,0,\N,Missing
W15-3101,C14-1060,0,\N,Missing
W16-2310,P14-1129,0,0.0580228,"Missing"
W16-2310,P13-2071,1,0.895955,"e investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS"
W16-2310,D08-1023,0,0.228814,"tion, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language"
W16-2310,D08-1089,0,0.0787452,"ll 12 language pairs of this year’s evaluation campaign. Novel research directions we investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize"
W16-2310,buck-etal-2014-n,0,0.124801,"Missing"
W16-2310,W15-3013,1,0.821897,"l language pairs, the use of all fea275 Language Turkish Turkish Turkish Turkish Turkish Turkish English Threshold none 0 2 5 10 20 none Token count 8806 9606 9935 10169 10416 10720 11514 Method baseline Byte-Pair Chipmunk Chipmunk Chipmunk Chipmunk Morfessor Morfessor Morfessor Morfessor Morfessor Table 5: Token counts for different thresholds for Morfessor segmentation consecutive bytes with a symbol that does not occur elsewhere. Each such replacement is called a merge, and the number of merges is a tunable parameter. The original text can be recovered using a lookup-table. Sennrich et al. (2015) applied this to word segmentation, and demonstrate its success at solving the large vocabulary problem in neural machine translation. To create our training data for the Morfessor and ChipMunk experiments, we augment the original training data with a second copy that has been segmented. For the tuning and test data, we only segment words that occur infrequently. This allows frequent words to be translated directly, but also allows the system to learn from the subword units of all words, including frequent ones. The number and type of subword units in each word segmented by byte pair encoding"
W16-2310,N12-1047,0,0.270491,"ce model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphol"
W16-2310,W11-2123,0,0.143406,"phrase-based, hierarchical phrase-based and syntax-based systems for all 12 language pairs of this year’s evaluation campaign. Novel research directions we investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and th"
W16-2310,P07-1019,0,0.143673,"nments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of"
W16-2310,N09-1025,0,0.0734239,"Missing"
W16-2310,2012.eamt-1.58,0,0.410346,"Missing"
W16-2310,P05-1066,1,0.655469,"isk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in the WMT 2016 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon strong baselines of the Edinburgh-JHU joint WMT submissions from the last year (Haddow et al., 2015), t"
W16-2310,K15-1017,0,0.0842479,"Missing"
W16-2310,D07-1091,1,0.808965,"gth, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in t"
W16-2310,E03-1076,1,0.762597,"ny, August 11-12, 2016. 2016 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Romanian–English Russian-English Turkish–English Language Czech German English Finnish Romanian Russian Turkish Sentences 19,074 19,074 1,500 943 9,006 500 Tokens 6.7 billion 65.2 billion 65.1 billion 2.9 billion 8.1 billion 23.3 billion 11.9 billion LM Size 13GB 107GB 89GB 8GB 13GB 41GB 23GB Table 1: Tuning set sizes for phrase-based system Table 2: Sizes of the language model trained on the monomlingual corpora extracted from Common Crawl. and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for any other language. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2014, with a total of 19,074 sentences (see Table 1). We used news-test 2015 as development test set. Significantly less tuning data was available for Finnish, Romanian, and Turkish. 2.2 2.3 Huge Language Model This year, large corpora of monolingual data were extracted from Common Crawl (Buck et al., 2014). We used this data to train 5-gram KneserNey smoothed language models, pruning out 3– 5 gram singletons."
W16-2310,P07-2045,1,0.0124021,"the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in the WMT 2016 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon strong baselines of the Edinburgh-JHU joint WMT submissions from the last year (Haddow et al., 2015), the Edinburgh syntax-based system submissions from the last year (Williams et al., 2015) as well as recent research in the field (Vaswani et al., 2013; Devlin et al., 2014). We also used the Apache Joshua translation toolkit (Post et al., 2015) to build hierarchical systems for two language tasks. 1 Moses Phrase-Based Systems http://www.statmt.org/wmt16 272 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 272–280, c Berlin, Germany"
W16-2310,N04-1022,0,0.203242,"inal-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction T"
W16-2310,E99-1010,0,0.314445,"lz, as we did all other language models. We compressed the language models with KenLM with 4-bit quantization and use of the trie data structure. The resulting size of the language model is listed in Table 2. The largest language model is the German model at 107GB, trained on 65.2 billion tokens, about an order of magnitude larger than previous data. Och Clusters As in last year’s system, we use word classes in four feature functions: (i) the language model, (ii) the operation sequence model, (iii) the reordering model, and the (iv) sparse word translation features. We generated Och clusters (Och, 1999) — a variant of Brown clusters — using mkcls. We have to choose a hyper parameter: the number of clusters. Our experiments and also prior work (Stewart et al., 2014) suggest that instead of committing to a single value, it is beneficial to use multiple numbers and use them in multiple feature functions concurrently. We used 50, 200, 600, and 2000 clusters, hence having 4 additional interpolated language models, 4 additional operation sequence models, 4 additional lexicalized reordering models, and 4 additional sets of sparse features. The feature functions for word classes were trained exactly"
W16-2310,2014.amta-researchers.3,0,0.0345605,"lting size of the language model is listed in Table 2. The largest language model is the German model at 107GB, trained on 65.2 billion tokens, about an order of magnitude larger than previous data. Och Clusters As in last year’s system, we use word classes in four feature functions: (i) the language model, (ii) the operation sequence model, (iii) the reordering model, and the (iv) sparse word translation features. We generated Och clusters (Och, 1999) — a variant of Brown clusters — using mkcls. We have to choose a hyper parameter: the number of clusters. Our experiments and also prior work (Stewart et al., 2014) suggest that instead of committing to a single value, it is beneficial to use multiple numbers and use them in multiple feature functions concurrently. We used 50, 200, 600, and 2000 clusters, hence having 4 additional interpolated language models, 4 additional operation sequence models, 4 additional lexicalized reordering models, and 4 additional sets of sparse features. The feature functions for word classes were trained exactly the same way as the corresponding feature functions for words. For instance, this means that the word class language model required training of individual models on"
W16-2310,D13-1140,0,0.0599603,"ystems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in the WMT 2016 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon strong baselines of the Edinburgh-JHU joint WMT submissions from the last year (Haddow et al., 2015), the Edinburgh syntax-based system submissions from the last year (Williams et al., 2015) as well as recent research in the field (Vaswani et al., 2013; Devlin et al., 2014). We also used the Apache Joshua translation toolkit (Post et al., 2015) to build hierarchical systems for two language tasks. 1 Moses Phrase-Based Systems http://www.statmt.org/wmt16 272 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 272–280, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Romanian–English Russian-English Turkish–English Language Czech German English Finnish Romanian Russian Turkish Sentences 19,074 19,074 1,50"
W16-2310,W09-0429,1,\N,Missing
W16-2310,P03-1020,0,\N,Missing
W16-2310,P16-1162,0,\N,Missing
W16-2310,W14-3324,1,\N,Missing
W17-4724,W16-2310,1,0.682889,"tional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT),"
W17-4724,P13-2071,1,0.854174,"phrase-based, syntax-based and/or neural machine translation systems for all 14 language pairs of this year’s evaluation campaign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS"
W17-4724,D08-1089,0,0.046349,"n task of EMNLP 2017 Second Conference on Machine Translation (WMT 2017). We set up phrase-based, syntax-based and/or neural machine translation systems for all 14 language pairs of this year’s evaluation campaign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize"
W17-4724,D08-1023,0,0.193388,"ign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language"
W17-4724,W11-2123,0,0.0496754,"ibes the Johns Hopkins University submissions to the shared translation task of EMNLP 2017 Second Conference on Machine Translation (WMT 2017). We set up phrase-based, syntax-based and/or neural machine translation systems for all 14 language pairs of this year’s evaluation campaign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and th"
W17-4724,P07-1019,0,0.0412146,"nments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for t"
W17-4724,buck-etal-2014-n,0,0.0365342,"Missing"
W17-4724,W08-0336,0,0.0357741,"rs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 276–282 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Latvian–English Russian-English Turkish–English Chinese–English Sentences 21,243 21,730 2,870 984 11,824 1,001 1,000 For English data, we used the scripts from Moses (Koehn et al., 2007) to tokenize our data, while for Chinese data we carried out word segmentation with Stanford word segmenter (Chang et al., 2008). We also normalized all the Chinese punctuations to their English counterparts to avoid disagreement across sentences. We parsed the tokenized data with Berkeley Parser (Petrov and Klein, 2007) using the pre-trained grammar provided with the toolkit, followed by right binarization of the parse. Finally, truecasing was performed on all the English texts. Due to the lack of casing system, we did not perform truecasing for any Chinese texts. We performed word alignment with fast-align (Dyer et al., 2013) due to the huge scale of this year’s training data and grow-diag-final-and heuristic for ali"
W17-4724,2012.eamt-1.58,0,0.0622113,"Missing"
W17-4724,N12-1047,0,0.032334,"ce model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT su"
W17-4724,N09-1025,0,0.0519466,"Missing"
W17-4724,D07-1091,1,0.65067,"gth, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this"
W17-4724,P16-1162,0,0.735349,"of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 276–282 c Copenhagen, Denmark, September 711, 2017. 2017 A"
W17-4724,P07-2045,1,0.0439969,"r other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a bas"
W17-4724,E03-1076,1,0.540903,"ning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the"
W17-4724,N04-1022,0,0.0775843,"inal-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound s"
W17-4724,N16-1046,0,0.0530386,"scores). Bold scores indicate best and submitted systems. 2 million sentences from news crawl 2016 monolingual corpus and 1.5 million sentences from preprocessed CWMT Chinese monolingual corpus from our syntax-based system run and backtranslated them with our trained base system. These back-translated pseudo-parallel data were then mixed with an equal amount of random samples from real parallel training data and used as the data for continued training. All the hyperparameters used for the continued training are exactly the same as those in the initial training stage. Following the effort of (Liu et al., 2016) and (Sennrich et al., 2016a), we also trained right-toleft (r2l) models with a random sample of 4 million sentence pairs for both translation directions of Chinese-English language pairs, in the hope that they could lead to better reordering on the target side. But they were not included in the final submission because they turned out to hurt the performance on development set. We conjecture that our r2l model is too weak compared to both base and back-trans models to yield good reordering hypotheses. We performed model averaging over the 4-best models for both base and back-trans systems as"
W17-4724,E99-1010,0,0.227759,"systems (Blunsom and Osborne, 2008)(Chiang et al., 2009). We used the same language model and tuning settings as the phrase-based systems. While BLEU score was used both for tuning and our development experiments, it is ambiguous when applied for Chinese outputs because Chinese does not have explicit word boundaries. For discriminative training and development tests, we evaluate the Chinese output against the automatically-segmented Chinese reference with multi-bleu.perl scripts in Moses (Koehn et al., 2007). Table 1: Tuning set sizes for phrase and syntaxbased system 500, and 2000 clusters (Och, 1999) using mkcls. In addition, we included a large language model based on the CommonCrawl monolingual data (Buck et al., 2014). The systems were tuned on a very large tuning set consisting of the test sets from 2008-2015, with a total of up to 21,730 sentences (see Table 1). We used newstest2016 as development test set. Significantly less tuning data was available for Finnish, Latvian, and Turkish. 2.2 Results Table 2 shows results for all language pairs, except for Chinese–English, for which we did not built phrase-based systems. Our phrase-based systems were clearly outperformed by NMT systems"
W17-4724,N07-1051,0,0.0711363,"1, 2017. 2017 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Latvian–English Russian-English Turkish–English Chinese–English Sentences 21,243 21,730 2,870 984 11,824 1,001 1,000 For English data, we used the scripts from Moses (Koehn et al., 2007) to tokenize our data, while for Chinese data we carried out word segmentation with Stanford word segmenter (Chang et al., 2008). We also normalized all the Chinese punctuations to their English counterparts to avoid disagreement across sentences. We parsed the tokenized data with Berkeley Parser (Petrov and Klein, 2007) using the pre-trained grammar provided with the toolkit, followed by right binarization of the parse. Finally, truecasing was performed on all the English texts. Due to the lack of casing system, we did not perform truecasing for any Chinese texts. We performed word alignment with fast-align (Dyer et al., 2013) due to the huge scale of this year’s training data and grow-diag-final-and heuristic for alignment symmetrization. We used the GHKM rule extractor implemented in Moses to extract SCFG rules from the parallel corpus. We set the maximum number of nodes (except target words) in the rules"
W17-4724,E17-3017,0,0.0705342,"Missing"
W17-4724,W16-2323,0,0.530034,"of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 276–282 c Copenhagen, Denmark, September 711, 2017. 2017 A"
W17-6936,W14-4012,0,0.0412872,"Missing"
W17-6936,N16-1024,0,0.0233907,"scussed in §1, it has been observed that NLP approaches that embed an entire sentence into a single, fixed-size vector may degrade in performance on longer sentences. One answer to this problem is to use finer-grained, multi-vector sentence representations that can grow with sentence complexity. Indeed, most neural (or otherwise continuous-space) models of sentences provide some finer-grained vector representations, most notably at the token level (i.e., standard RNN implementations), sub-token level (Sennrich et al., 2016), character-level (Kim et al., 2016), and syntactic constituent level (Dyer et al., 2016), and are often used in task-specific attention mechanisms. For many tasks involving search or attention, however, such as open question-answering or document-level analysis, preserving each such intermediate representation may be prohibitively expensive. In comparison to other fine-grained, multi-vector representations, skip-prop offers two advantages: (1) the number of propositions (and hence vectors) per sentence is relatively few, and (2) the proposition is its own interpretable unit of meaning. Figs. 1 and 2 illustrate the granularity-expense tradeoff between one-vector-per sentence, prop"
W17-6936,P02-1026,0,0.0255171,"ically useful level of granularity. We demonstrate the feasibility of training skip-prop vectors, introducing a method adapted from skip-thought vectors, and compare skip-prop with “one vector per sentence” and “one vector per token” approaches. 1 Introduction The length and complexity of written natural language sentences is highly variable. Sentences from New York Times (NYT) stories (August 1997), for example, contain on average 23 tokens, with a standard deviation of 12. By information-theoretic measures, too, natural language sentences convey differing amounts of information (Hale, 2003; Genzel and Charniak, 2002). It is natural to suppose, then, that methods in computational linguistics that aim to learn fixed-size semantic representations of sentences, i.e., with vectors of fixed dimension, may be limited in their expressiveness or efficiency. Indeed, on many NLP tasks for which neural sentence embedding methods have been adapted, degraded performance on longer input sentences is commonly observed: in machine translation (Cho et al., 2014), question-answering (Kumar et al., 2016), and semantic role labeling (Zhou and Xu, 2015), for example. Motivated by these observations, we introduce skip-prop vect"
W17-6936,marelli-etal-2014-sick,0,0.224409,"les from Sept. 1997; and test is 5K random sentence triples from Oct. 1997. The vocabulary is approximately 39K tokens from Sept. 1997 NYT with minimum frequency of 15. Each model is trained for one epoch on the entire train set using mini-batches of size 1. As described in §4, a sentence triple (sl , sc , sr ) consists of a contiguous set of three sentences from a news story: a “left,” “center,” and “right” sentence. For the qualitative nearest-neighbor experiments, two datasets are used: (1) a 100K superset of the NYT development set (Sept. 1997), and (2) all sentences from the SICK corpus (Marelli et al., 2014). Results As a preliminary evaluation of skip-prop vectors, we present both quantitative and qualitative results. These results show that (1) it is feasible to train skip-prop vectors with our proposed method, and (2) some notion of semantic similarity over propositions is preserved in this representation. Table 1 shows the perplexity attained by each model. Here, perplexity is computed either from the two decoders’ predictions of the left and right context sentences (skip-thought objective), or one decoder’s prediction of the original sentence (autoencoder objective). In all cases, the skip-p"
W17-6936,N06-1020,0,0.0360103,"hows typical NYT documents contain more propositions than sentences, and many more tokens than propositions. (Log scale x-axis.) 3 Figure 2: Scatter plot shows longer sentences contain more propositions. Most sentences contain fewer than 50 tokens, and fewer than 8 propositions. Background Sequence-to-Sequence Models Sequence-to-sequence (seq-to-seq) models are a class of neural networks that compute the conditional probability of an output sequence given an input sequence, i.e., P (y1 ...yn |x1 ...xm ). They have been applied to many tasks in NLP (Bahdanau et al., 2014; Vinyals et al., 2015; McClosky et al., 2006), though here we train them to encode multi-vector sentence representations. Typical seq-to-seq models consist of two recurrent neural networks (RNNs): an encoder and decoder, which iterate over the input and output sequences, respectively. The final hidden state of the encoder RNN, hm , is passed as the initial state to the decoder RNN. Thus, the vector hm is a representation of the entireQ input, and the decoder computes the conditional distribution: P (y1 ...yn |x1 ...xm ) = P (y1 ...yn |hm ) = ni=1 P (yi |y&lt;i , hm ). We train skip-prop with a multi-encoder, multi-decoder variant of seq-to-"
W17-6936,P16-1162,0,0.0081757,"king a “one vector per proposition” approach to representing the meaning of a sentence. As discussed in §1, it has been observed that NLP approaches that embed an entire sentence into a single, fixed-size vector may degrade in performance on longer sentences. One answer to this problem is to use finer-grained, multi-vector sentence representations that can grow with sentence complexity. Indeed, most neural (or otherwise continuous-space) models of sentences provide some finer-grained vector representations, most notably at the token level (i.e., standard RNN implementations), sub-token level (Sennrich et al., 2016), character-level (Kim et al., 2016), and syntactic constituent level (Dyer et al., 2016), and are often used in task-specific attention mechanisms. For many tasks involving search or attention, however, such as open question-answering or document-level analysis, preserving each such intermediate representation may be prohibitively expensive. In comparison to other fine-grained, multi-vector representations, skip-prop offers two advantages: (1) the number of propositions (and hence vectors) per sentence is relatively few, and (2) the proposition is its own interpretable unit of meaning. Figs."
W17-6936,P15-1109,0,0.027445,"age sentences convey differing amounts of information (Hale, 2003; Genzel and Charniak, 2002). It is natural to suppose, then, that methods in computational linguistics that aim to learn fixed-size semantic representations of sentences, i.e., with vectors of fixed dimension, may be limited in their expressiveness or efficiency. Indeed, on many NLP tasks for which neural sentence embedding methods have been adapted, degraded performance on longer input sentences is commonly observed: in machine translation (Cho et al., 2014), question-answering (Kumar et al., 2016), and semantic role labeling (Zhou and Xu, 2015), for example. Motivated by these observations, we introduce skip-prop vectors, a method for learning multi-vector sentence representations following a “one vector per proposition” strategy. Our approach is based on the skip-thought method of Kiros et al. (2015), which combines neural sequence-to-sequence models (Sutskever et al., 2014) with a skip-gram-like training objective (Mikolov et al., 2013) to obtain generalpurpose sentence representations as a fixed-size vector. Skip-prop capitalizes on the idea that a complex sentence may be represented in terms of the simpler sentences, or proposit"
W17-6936,W12-3018,1,\N,Missing
W18-1201,P11-2031,0,0.0865757,"ng with word2vec results in a 1.8 BLEU point gain over randomly initialized word embeddings. morph results in a 0.4 BLEU point gain over word2vec, and fastText a 0.7 BLEU point gain. Fixing the embeddings consistently performs worse than allowing backpropagation. However, this gap narrows as the BLEU scores of both improve. We also compare to running a NMT system with a CNN over character embeddings in the encoder from Costa-juss`a and Fonollosa (2016), which results in a BLEU score of 26.46. 7 We also perform statistical significance testing via bootstrap resampling, using the multeval tool (Clark et al., 2011). The best BLEU are 6 Note that when attempting to handle OOVs, in the case where we are only normalizing diacritics, we can only recover a lemma vector for 1 of the 3 OOVs while fastText is using n-grams to recover something for all 3. In the case of full normalization, both are able to recover a vector. 7 We use the code from https://github.com/ harvardnlp/seq2seq-attn, modifying hyperparameters to match our word-level models as closely as possible and using character-level default settings. 6 28.76 for morph and 29.10 for fastText. Both morph and fastText improve upon word2vec (28.38) with"
W18-1201,P16-2058,0,0.0367661,"Missing"
W18-1201,I17-2016,1,0.887335,"Missing"
W18-1201,D15-1041,0,0.0249853,"vlI )) p(wO |wI , lI ) = PW 0T w=1 exp(vw concat(vwI , vlI )) Without the lemma part, this objective corresponds to word2vec. Because there may be multiple lemmas associated with a word type, we use a weighted average over lemma vectors in the final vector: The usefulness of word embeddings in downstream applications is a question that often needs to be revisited. Many types of morphological or character-level embedding models have been evaluated under various extrinsic metrics, in applications such as language modeling (Kim et al., 2016; Botha and Blunsom, 2014; Sperr et al., 2013), parsing (Ballesteros et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), and named-entity recognition (dos Santos and Guimar˜aes, 2015; CotwI∗ = concat(vwI , 1 X c(wI : lI ) ∗ vlI ) c(wI ) lI where c(·) is the count of a word or word-lemma pair. When the morphological analyzer cannot produce a lemma, we use the word form itself. We output the vectors associated with individual lemmas as well, which can be used to handle OOV words. 3 The lemma simplifies a word, removing clitics and some inflectional morphology. While it reduces sparsity of infrequent stems, it also removes potentially useful information. The"
W18-1201,N15-1140,0,0.0409415,"Missing"
W18-1201,P17-1080,0,0.0245476,"in neural machine translation, using a standard bi-directional LSTM encoderdecoder model with the attention mechanism from Luong et al. (2015). We describe below other work in NMT that has tried to address some of the same issues dealing with settings with limited parallel data, improving translation of morphological complexity, and Arabic NMT. 1 https://github.com/pamelashapiro/ word2vec_morph 1 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 1–11 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics 2.1 Low-Resource Settings from Belinkov et al. (2017) that showed that the encoder already learns more morphological information than the decoder. Our work differs in that we are focusing on incorporating morphological information into the source side. Moreover, Belinkov et al. (2017) works with higher-resource datasets. It is possible that in lower-resource settings, it will still be helpful to incorporate morphological information into the encoder. Some success has been achieved applying neural machine translation to low-resource settings. Zoph et al. (2016) use transfer learning to improve NMT from low-resource languages into English. They in"
W18-1201,P16-1156,0,0.0386597,"Missing"
W18-1201,Q17-1010,0,0.631052,"abic, a morphologically rich language with available linguistic resources, and perform Ar-to-En MT experiments on a small corpus of TED subtitles. We find that word embeddings utilizing subword information consistently outperform standard word embeddings on a word similarity task and as initialization of the source word embeddings in a low-resource NMT system. 1 1. We adapt word2vec to utilize lemmas from a morphological analyzer,1 and show improvements on a word similarity task over a stateof-the-art unsupervised approach to incorporating morphological information based on character n-grams (Bojanowski et al., 2017). 2. We experiment with Arabic-to-English NMT on the TED Talks corpus. Our results demonstrate that incorporating some form of morphological word embeddings into NMT improves BLEU scores and outperforms the conventional approaches of using standard word embeddings, random initialization, or bytepair encoding (BPE). Introduction Neural machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) has recently become the dominant approach to machine translation. However, the standard encoder-decoder models with attention have been shown to perform poorly in low-resource settings (Koehn and"
W18-1201,I17-1015,0,0.0227563,"rich et al., 2016b), and these sequences of word pieces are translated. BPE become standard practice. However, it is unclear how much data is necessary for it to be beneficial. In our experiments, BPE performs worse than initializing with any of the word embeddings for our dataset. Character-level NMT has recently become popular as well (Ling et al., 2015b; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017). Their work aims to implicitly learn morphology by building neural network architectures over characters. We also compare to a character-level NMT system in our experiments. Additionally, Dalvi et al. (2017) add morphological information into the decoder, following work 3 Morphological Word Embeddings Morphological word embeddings help improve the quality of pretrained word embeddings for less frequent morphological variants, which is important for morphologically rich and low-resource languages. We outline related work in this section and describe an additional approach of our own. Some related work has used morphological resources to guide word embeddings. Cotterell and Sch¨utze (2015) use a multi-task objective to encourage word embeddings to reflect morphological tags, working within the log-"
W18-1201,2012.eamt-1.60,0,0.0242585,"Missing"
W18-1201,W15-3904,0,0.0517565,"Missing"
W18-1201,W16-2506,0,0.0545565,"Missing"
W18-1201,W17-3204,0,0.0280986,"l., 2017). 2. We experiment with Arabic-to-English NMT on the TED Talks corpus. Our results demonstrate that incorporating some form of morphological word embeddings into NMT improves BLEU scores and outperforms the conventional approaches of using standard word embeddings, random initialization, or bytepair encoding (BPE). Introduction Neural machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) has recently become the dominant approach to machine translation. However, the standard encoder-decoder models with attention have been shown to perform poorly in low-resource settings (Koehn and Knowles, 2017), a problem which can be alleviated by initialization of parameters from an NMT system trained on higher-resource languages (Zoph et al., 2016). An alternative way to initialize parameters in a low-resource NMT setup is to use pretrained monolingual word embeddings, which are quick to train and readily available for many languages. There is a large body of work on word embeddings. Popular approaches include word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014). These 2 Neural Machine Translation We follow recent work in neural machine translation, using a standard bi-directional"
W18-1201,P13-1149,0,0.0310209,"rated in Figure 1. We learn word vectors and lemma vectors, using their concatenation in the dot product with a context vector in the skipgram objective. So the modified objective we are approximating with negative sampling is now For simplicity and efficiency, we consider only embeddings in the skipgram family—fastText, word2vec skipgram, and our modification of the word2vec skipgram objective, described in 3.1. There is a large literature on exploiting characters, morphology, and composition for embedding models (Chen et al., 2015; Ling et al., 2015a; Qiu et al., 2014; Wieting et al., 2016; Lazaridou et al., 2013), and a comparison with these different models may be interesting future work. 0 exp(vwTO concat(vwI , vlI )) p(wO |wI , lI ) = PW 0T w=1 exp(vw concat(vwI , vlI )) Without the lemma part, this objective corresponds to word2vec. Because there may be multiple lemmas associated with a word type, we use a weighted average over lemma vectors in the final vector: The usefulness of word embeddings in downstream applications is a question that often needs to be revisited. Many types of morphological or character-level embedding models have been evaluated under various extrinsic metrics, in applicatio"
W18-1201,N16-1101,0,0.0220356,"al. (2017) works with higher-resource datasets. It is possible that in lower-resource settings, it will still be helpful to incorporate morphological information into the encoder. Some success has been achieved applying neural machine translation to low-resource settings. Zoph et al. (2016) use transfer learning to improve NMT from low-resource languages into English. They initialize parameters in the low-resource setting with parameters from an NMT model trained on a high-resource language. Nguyen and Chiang (2017) extend this by exploiting vocabulary overlap in related languages. Similarly, Firat et al. (2016) share parameters between high and low resource languages via multi-way, multilingual NMT. Other work aims to exploit monolingual data via back-translation (Sennrich et al., 2016a). Imankulova et al. (2017) aim to improve this technique for low-resource settings by filtering generated back-translations with quality estimation. Meanwhile, He et al. (2016) use a reinforcement learning approach to learn from monolingual data. Our approach is similar to those utilizing transfer learning, but we initialize on the source side with monolingual word embeddings, which is relatively simple to implement"
W18-1201,Q17-1026,0,0.0457616,"ating Morphology Some research has aimed to incorporate morphological information into NMT systems. BytePair Encoding (BPE) segments words into pieces by merging character sequences based on frequency (Sennrich et al., 2016b), and these sequences of word pieces are translated. BPE become standard practice. However, it is unclear how much data is necessary for it to be beneficial. In our experiments, BPE performs worse than initializing with any of the word embeddings for our dataset. Character-level NMT has recently become popular as well (Ling et al., 2015b; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017). Their work aims to implicitly learn morphology by building neural network architectures over characters. We also compare to a character-level NMT system in our experiments. Additionally, Dalvi et al. (2017) add morphological information into the decoder, following work 3 Morphological Word Embeddings Morphological word embeddings help improve the quality of pretrained word embeddings for less frequent morphological variants, which is important for morphologically rich and low-resource languages. We outline related work in this section and describe an additional approach of our own. Some rela"
W18-1201,D09-1124,0,0.0381425,"in Section 3.1. We experiment with normalizing only diacritics as well as additionally normalizing inconsistently typed characters as in Almahairi et al. (2016), referred to here as “full normalization.” We normalize the word similarity dataset accordingly. To ensure that dimensionality is not a major factor, we experiment with various dimensions. We also experiment with just using lemmas to predict, which performs slightly worse than using both word and lemma and taking the lemma part of the vector, though still better than word2vec and fastText. We evaluate on an Arabic dataset developed by Hassan and Mihalcea (2009) based on the classic WordSim353 (Finkelstein et al., 2001), as is evaluated on by Bojanowski et al. (2017). We re-run on word2vec and fastText and obtain similar, though not identical, results to Bojanowski et al. (2017). We suspect the differences are due to differences in cleaning and tokenizing Arabic Wikipedia. As is standard for these evaluations, we report Spearman rank coefficient in Table 1. There are 3 OOV words when normalizing diacritics, and 1 with full normalization, out of 353 Word Similarity Results Before running NMT, we first experiment on a word similarity dataset to test th"
W18-1201,D15-1176,0,0.129431,"dings in settings with limited parallel data. Incorporating Morphology Some research has aimed to incorporate morphological information into NMT systems. BytePair Encoding (BPE) segments words into pieces by merging character sequences based on frequency (Sennrich et al., 2016b), and these sequences of word pieces are translated. BPE become standard practice. However, it is unclear how much data is necessary for it to be beneficial. In our experiments, BPE performs worse than initializing with any of the word embeddings for our dataset. Character-level NMT has recently become popular as well (Ling et al., 2015b; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017). Their work aims to implicitly learn morphology by building neural network architectures over characters. We also compare to a character-level NMT system in our experiments. Additionally, Dalvi et al. (2017) add morphological information into the decoder, following work 3 Morphological Word Embeddings Morphological word embeddings help improve the quality of pretrained word embeddings for less frequent morphological variants, which is important for morphologically rich and low-resource languages. We outline related work in this section and"
W18-1201,W17-5704,0,0.0209602,"achieved applying neural machine translation to low-resource settings. Zoph et al. (2016) use transfer learning to improve NMT from low-resource languages into English. They initialize parameters in the low-resource setting with parameters from an NMT model trained on a high-resource language. Nguyen and Chiang (2017) extend this by exploiting vocabulary overlap in related languages. Similarly, Firat et al. (2016) share parameters between high and low resource languages via multi-way, multilingual NMT. Other work aims to exploit monolingual data via back-translation (Sennrich et al., 2016a). Imankulova et al. (2017) aim to improve this technique for low-resource settings by filtering generated back-translations with quality estimation. Meanwhile, He et al. (2016) use a reinforcement learning approach to learn from monolingual data. Our approach is similar to those utilizing transfer learning, but we initialize on the source side with monolingual word embeddings, which is relatively simple to implement and low-cost to train. Di Gangi and Marcello (2017) experiment with monolingual word embeddings as we do, but they merge external monolingual word embeddings with the embeddings learned by an NMT system. We"
W18-1201,D15-1166,0,0.0592178,"meters from an NMT system trained on higher-resource languages (Zoph et al., 2016). An alternative way to initialize parameters in a low-resource NMT setup is to use pretrained monolingual word embeddings, which are quick to train and readily available for many languages. There is a large body of work on word embeddings. Popular approaches include word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014). These 2 Neural Machine Translation We follow recent work in neural machine translation, using a standard bi-directional LSTM encoderdecoder model with the attention mechanism from Luong et al. (2015). We describe below other work in NMT that has tried to address some of the same issues dealing with settings with limited parallel data, improving translation of morphological complexity, and Arabic NMT. 1 https://github.com/pamelashapiro/ word2vec_morph 1 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 1–11 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics 2.1 Low-Resource Settings from Belinkov et al. (2017) that showed that the encoder already learns more morphological information than the decoder. Our work differs in that we"
W18-1201,P17-4012,0,0.0320722,"tilizing the other morphological information provided. 4.2 Experiments • morph: the modified skip-gram word embeddings described in Section 3.1, which rely on a morphological analyzer and lemma embeddings. The word embeddings inserted into the NMT system are always of dimension 300, and in word similarity experiments, we experiment with dimensions of different sizes. All word embeddings are trained with negative sampling (5 samples), with a window size of 5, a 10−4 rejection threshold for subsampling, and 5 iterations. Additional fastText parameters are left at the default. We use OpenNMT-py (Klein et al., 2017) for all NMT experiments, with a max sentence size of 80. We use word-level prediction accuracy for model selection. For the BPE baseline, the number of BPE merge operations is 30,000. The hidden layer size is 1024, trained with batch size 80, with Adadelta (Zeiler, 2012) and a dropout rate of 0.2 for 20 epochs with a learning rate of 1.0. When initializing the encoder with word embeddings, we experiment both with locking the word Arabic Morphology One prominent feature of Arabic morphology is that it is rich with clitics, morphemes that syntactically function as words but phonologically funct"
W18-1201,W13-3512,0,0.0388935,"n of work in that it uses morphological resources, but it works within the popu2 Figure 1: Modified skipgram objective for training morph embeddings. Here, w(t) is the current word, l(t) is its lemma, and w(t-2), w(t-1),w(t+1),w(t+2) are neighboring words. lar word2vec skipgram objective (Mikolov et al., 2013a), adding a simple modification to consider a lemma in addition to a word form. terell and Duh, 2017). Besides the Arabic word similarity dataset, here we also focus on evaluating embeddings at the source side of a machine translation task. Other work uses purely unsupervised techniques. Luong et al. (2013) segment words using Morfessor (Creutz and Lagus, 2007), and use recursive neural networks to build word embeddings from morph embeddings. Instead of explicit segmentation, fastText (Bojanowski et al., 2017) incorporates subword information into the skipgram model by treating a word as a bag of character ngrams. They represent each n-gram of sizes 3-6 with a vector, and each word as a sum of its n-gram vectors. While fastText is not explicitly learning morphology, it can be viewed as potentially incorporating morpheme-like subwords. 3.1 Modified Skipgram Objective We assume the availability of"
W18-1201,P16-1009,0,0.196987,"er. Some success has been achieved applying neural machine translation to low-resource settings. Zoph et al. (2016) use transfer learning to improve NMT from low-resource languages into English. They initialize parameters in the low-resource setting with parameters from an NMT model trained on a high-resource language. Nguyen and Chiang (2017) extend this by exploiting vocabulary overlap in related languages. Similarly, Firat et al. (2016) share parameters between high and low resource languages via multi-way, multilingual NMT. Other work aims to exploit monolingual data via back-translation (Sennrich et al., 2016a). Imankulova et al. (2017) aim to improve this technique for low-resource settings by filtering generated back-translations with quality estimation. Meanwhile, He et al. (2016) use a reinforcement learning approach to learn from monolingual data. Our approach is similar to those utilizing transfer learning, but we initialize on the source side with monolingual word embeddings, which is relatively simple to implement and low-cost to train. Di Gangi and Marcello (2017) experiment with monolingual word embeddings as we do, but they merge external monolingual word embeddings with the embeddings"
W18-1201,I17-2050,0,0.0160974,"we are focusing on incorporating morphological information into the source side. Moreover, Belinkov et al. (2017) works with higher-resource datasets. It is possible that in lower-resource settings, it will still be helpful to incorporate morphological information into the encoder. Some success has been achieved applying neural machine translation to low-resource settings. Zoph et al. (2016) use transfer learning to improve NMT from low-resource languages into English. They initialize parameters in the low-resource setting with parameters from an NMT model trained on a high-resource language. Nguyen and Chiang (2017) extend this by exploiting vocabulary overlap in related languages. Similarly, Firat et al. (2016) share parameters between high and low resource languages via multi-way, multilingual NMT. Other work aims to exploit monolingual data via back-translation (Sennrich et al., 2016a). Imankulova et al. (2017) aim to improve this technique for low-resource settings by filtering generated back-translations with quality estimation. Meanwhile, He et al. (2016) use a reinforcement learning approach to learn from monolingual data. Our approach is similar to those utilizing transfer learning, but we initia"
W18-1201,pasha-etal-2014-madamira,0,0.0662396,"Missing"
W18-1201,P16-1162,0,0.297111,"er. Some success has been achieved applying neural machine translation to low-resource settings. Zoph et al. (2016) use transfer learning to improve NMT from low-resource languages into English. They initialize parameters in the low-resource setting with parameters from an NMT model trained on a high-resource language. Nguyen and Chiang (2017) extend this by exploiting vocabulary overlap in related languages. Similarly, Firat et al. (2016) share parameters between high and low resource languages via multi-way, multilingual NMT. Other work aims to exploit monolingual data via back-translation (Sennrich et al., 2016a). Imankulova et al. (2017) aim to improve this technique for low-resource settings by filtering generated back-translations with quality estimation. Meanwhile, He et al. (2016) use a reinforcement learning approach to learn from monolingual data. Our approach is similar to those utilizing transfer learning, but we initialize on the source side with monolingual word embeddings, which is relatively simple to implement and low-cost to train. Di Gangi and Marcello (2017) experiment with monolingual word embeddings as we do, but they merge external monolingual word embeddings with the embeddings"
W18-1201,W13-3204,0,0.0225196,"work. 0 exp(vwTO concat(vwI , vlI )) p(wO |wI , lI ) = PW 0T w=1 exp(vw concat(vwI , vlI )) Without the lemma part, this objective corresponds to word2vec. Because there may be multiple lemmas associated with a word type, we use a weighted average over lemma vectors in the final vector: The usefulness of word embeddings in downstream applications is a question that often needs to be revisited. Many types of morphological or character-level embedding models have been evaluated under various extrinsic metrics, in applications such as language modeling (Kim et al., 2016; Botha and Blunsom, 2014; Sperr et al., 2013), parsing (Ballesteros et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), and named-entity recognition (dos Santos and Guimar˜aes, 2015; CotwI∗ = concat(vwI , 1 X c(wI : lI ) ∗ vlI ) c(wI ) lI where c(·) is the count of a word or word-lemma pair. When the morphological analyzer cannot produce a lemma, we use the word form itself. We output the vectors associated with individual lemmas as well, which can be used to handle OOV words. 3 The lemma simplifies a word, removing clitics and some inflectional morphology. While it reduces sparsity of infrequent stems, it also removes"
W18-1201,D16-1157,0,0.0132001,"ntext words, as illustrated in Figure 1. We learn word vectors and lemma vectors, using their concatenation in the dot product with a context vector in the skipgram objective. So the modified objective we are approximating with negative sampling is now For simplicity and efficiency, we consider only embeddings in the skipgram family—fastText, word2vec skipgram, and our modification of the word2vec skipgram objective, described in 3.1. There is a large literature on exploiting characters, morphology, and composition for embedding models (Chen et al., 2015; Ling et al., 2015a; Qiu et al., 2014; Wieting et al., 2016; Lazaridou et al., 2013), and a comparison with these different models may be interesting future work. 0 exp(vwTO concat(vwI , vlI )) p(wO |wI , lI ) = PW 0T w=1 exp(vw concat(vwI , vlI )) Without the lemma part, this objective corresponds to word2vec. Because there may be multiple lemmas associated with a word type, we use a weighted average over lemma vectors in the final vector: The usefulness of word embeddings in downstream applications is a question that often needs to be revisited. Many types of morphological or character-level embedding models have been evaluated under various extrins"
W18-1201,D14-1162,0,0.0821444,"Missing"
W18-1201,D16-1163,0,0.115567,"word embeddings into NMT improves BLEU scores and outperforms the conventional approaches of using standard word embeddings, random initialization, or bytepair encoding (BPE). Introduction Neural machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) has recently become the dominant approach to machine translation. However, the standard encoder-decoder models with attention have been shown to perform poorly in low-resource settings (Koehn and Knowles, 2017), a problem which can be alleviated by initialization of parameters from an NMT system trained on higher-resource languages (Zoph et al., 2016). An alternative way to initialize parameters in a low-resource NMT setup is to use pretrained monolingual word embeddings, which are quick to train and readily available for many languages. There is a large body of work on word embeddings. Popular approaches include word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014). These 2 Neural Machine Translation We follow recent work in neural machine translation, using a standard bi-directional LSTM encoderdecoder model with the attention mechanism from Luong et al. (2015). We describe below other work in NMT that has tried to address"
W18-1201,C14-1015,0,0.0186052,"emma to predict context words, as illustrated in Figure 1. We learn word vectors and lemma vectors, using their concatenation in the dot product with a context vector in the skipgram objective. So the modified objective we are approximating with negative sampling is now For simplicity and efficiency, we consider only embeddings in the skipgram family—fastText, word2vec skipgram, and our modification of the word2vec skipgram objective, described in 3.1. There is a large literature on exploiting characters, morphology, and composition for embedding models (Chen et al., 2015; Ling et al., 2015a; Qiu et al., 2014; Wieting et al., 2016; Lazaridou et al., 2013), and a comparison with these different models may be interesting future work. 0 exp(vwTO concat(vwI , vlI )) p(wO |wI , lI ) = PW 0T w=1 exp(vw concat(vwI , vlI )) Without the lemma part, this objective corresponds to word2vec. Because there may be multiple lemmas associated with a word type, we use a weighted average over lemma vectors in the final vector: The usefulness of word embeddings in downstream applications is a question that often needs to be revisited. Many types of morphological or character-level embedding models have been evaluated"
W18-1201,P17-2095,0,0.0170974,"machine translation on Arabic. They find that preprocessing of Arabic as used in statistical machine translation is helpful. They normalize the text, removing diacritics and normalizing inconsistently typed characters, and they tokenize according the Penn Arabic Treebank (ATB) scheme (Maamouri et al., 2004), separating all clitics except for definite articles. We normalize as such, but do not use ATB tokenization, instead using the default tokenization in Moses (Koehn et al., 2007). We do this to focus on embeddings for words and to facilitate generalization to other languages. Additionally, Sajjad et al. (2017) explore alternatives to language-specific segmentation in Arabic, finding that BPE performs the best in their scenario. Note that unlike the previously described work, we are using a dataset of only 2.9 million tokens for training. This is to assess the use of morphological word embeddings in settings with limited parallel data. Incorporating Morphology Some research has aimed to incorporate morphological information into NMT systems. BytePair Encoding (BPE) segments words into pieces by merging character sequences based on frequency (Sennrich et al., 2016b), and these sequences of word piece"
W18-2705,D16-1139,0,0.132757,"Missing"
W18-2705,2012.eamt-1.60,0,0.0122739,"corpus), and the EU Press Releases. We use newstest2015 as the out-of-domain development set and newstest2016 as the outof-domain test set. These consist of professionally translated news articles released by the WMT shared task. We perform adaptation into two different domains: EMEA (descriptions of medicines) and TED Talks (rehearsed presentations). For EMEA, we use the data split from (Koehn and Knowles, 2017),7 which was extracted from from OPUS (Tiedemann, 2009, 2012).8 For TED, we use the data split from the Multitarget TED Talks Task (MTTT) (Duh, 2018).9 which was extracted from WIT3 (Cettolo et al., 2012).10 Tables 1–3 give the number of words and sentences of each of the corpora in the train, dev, and test sets, respectively. In addition to experiments on the full training sets, we also conduct experiments adapting to each given domain using only the first 2,000 sentences of each in-domain training set to simulate adaptation into a low-resource domain. For all experiments we translate from English to German as well as from German to English. Table 3: Tokenized test set sizes. 3.2 NMT settings Our neural machine translation systems are trained using a modified version of OpenNMT-py (Klein et a"
W18-2705,P17-2061,0,0.0679878,"Missing"
W18-2705,2005.mtsummit-papers.11,1,0.0896777,"domain model (which replaces the parent) on in-domain training data that the outof-domain model was not trained on. 3 Table 1: Tokenized training set sizes. corpus de words en words sentences EMEA 26479 28838 2000 TED 37509 38717 1958 newstest15 44869 47569 2169 Experiments 3.1 Table 2: Tokenized development set sizes. Data corpus de words en words sentences EMEA 31737 33884 2000 TED 35516 36857 1982 newstest16 64379 65647 2999 For our large, out-of-domain corpus we utilize bitext from WMT2017 (Bojar et al., 2017),4 which contains data from several sources: Europarl parliamentary proceedings (Koehn, 2005),5 News Commentary (political and economic news commentary),6 Common Crawl (web-crawled parallel corpus), and the EU Press Releases. We use newstest2015 as the out-of-domain development set and newstest2016 as the outof-domain test set. These consist of professionally translated news articles released by the WMT shared task. We perform adaptation into two different domains: EMEA (descriptions of medicines) and TED Talks (rehearsed presentations). For EMEA, we use the data split from (Koehn and Knowles, 2017),7 which was extracted from from OPUS (Tiedemann, 2009, 2012).8 For TED, we use the dat"
W18-2705,W17-4123,0,0.0210706,"old-standard distribution 1{yi = v} (which is simply a one-hot vector that indicates if the correct word was produced), and the model’s distribution p(yi = v |x; θ; yj&lt;i ). 2.2 Continued Training L(θ) = (1 − α) LNLL (θ) + α Lreg (θ) Continued training is a simple yet effective technique for domain adaptation. It consists of three steps: (3) The added regularization term is formulated in the spirit of knowledge distillation (Kim and Rush, 2 For a detailed explanation of attention based NMT see Bahdanau et al. (2015) (the original paper), or for a gentle introduction see the textbook chapter by Koehn (2017). 3 The out-of-domain model is fixed while training the indomain model. 37 corpus de words en words sentences EMEA 13,572,552 14,774,808 1,104,752 TED 2,966,837 3,161,544 152,609 WMT 139,449,418 146,569,151 5,919,142 2016), where a student model is trained to match the output distribution of a parent model. In wordlevel knowledge distillation, the student model’s output distribution is trained on the same data that the parent model was trained. In contrast, our domain specific model (which replaces the student) is trained with a loss term that encourages it to match the out-of-domain model (wh"
W18-2705,W17-3204,1,0.93447,"distribution and that of the out-of-domain model.1 This prevents the distribution of words produced from differing too much from the original distribution. Introduction Neural Machine Translation (NMT) (Bahdanau et al., 2015) is currently the state-of-the art paradigm for machine translation. It dominated the recent WMT shared task (Bojar et al., 2017), and is used commercially (Wu et al., 2016; Crego et al., 2016; Junczys-Dowmunt et al., 2016). Despite their successes, NMT systems require a large amount of training data and do not perform well in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). Domain adaptation is required when there is sufficient data to train an NMT system in the desired language pair, but the domain (the topic, genre, style or level of We show that this method improves upon standard continued training by as much as 1.5 BLEU. 1 The code is available: github.com/khayrallah/OpenNMT-py-reg 36 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 36–44 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics 2 Method 1. Train a model until convergence on large outof-domain bitext using LNLL as the training o"
W18-2705,2015.iwslt-evaluation.11,0,0.137405,"out-of-domain model to prevent the model’s output from differing too much from the original out-ofdomain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU. 1 In this paper, we focus on the supervised domain adaptation problem, where in addition to a large out-of-domain corpus, we also have a smaller in-domain parallel corpus available for training. A technique commonly applied in this situation is continued training (Luong and Manning, 2015), where a model is first trained on the out-of-domain corpus, and then that model is used to initialize a new model that is trained on the in-domain corpus. This simple method leads to empirical improvements on in-domain test sets. However, we hypothesize that some knowledge available in the out-of-domain data—which is not observed in the smaller in-domain data but would be useful at test time—is being forgotten during continued training, due to overfitting. (This phenomena can be viewed as a version of catastrophic forgetting (Goodfellow et al., 2013)). For this reason, we add an additional t"
W18-2705,D17-1156,0,0.175941,"Missing"
W18-2705,P16-1162,0,0.291622,"Missing"
W18-2705,tiedemann-2012-parallel,0,0.0777348,"Missing"
W18-6313,D11-1033,0,0.0803148,"M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training h"
W18-6313,P10-2016,0,0.057732,"Missing"
W18-6313,P17-2061,0,0.0587364,"model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Sim"
W18-6313,2010.iwslt-papers.5,1,0.80886,"ain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see §2). Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance. In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training. Motivated by domain adaptation analysis in PBMT (Haddow and Koehn, 2012; Duh et al., 2010; Irvine et al., 2013), this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training. We segment the model into five subnetworks, which we refer to as components, denoted in Figure 1: the source embeddings, encoder, decoder (which includes the attention mechanism), the softmax (used to denote the decoder output embeddings and biases), and the target embeddings. We freeze components one at a time during continued training to see how much the adaptation depends on each component. We also e"
W18-6313,W18-2705,1,0.827312,"Missing"
W18-6313,W17-4713,0,0.124642,"method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the t"
W18-6313,D10-1044,0,0.0664357,"Missing"
W18-6313,W07-0733,1,0.69704,"t. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) usin"
W18-6313,W12-3154,1,0.843127,"has necessitated new domain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see §2). Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance. In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training. Motivated by domain adaptation analysis in PBMT (Haddow and Koehn, 2012; Duh et al., 2010; Irvine et al., 2013), this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training. We segment the model into five subnetworks, which we refer to as components, denoted in Figure 1: the source embeddings, encoder, decoder (which includes the attention mechanism), the softmax (used to denote the decoder output embeddings and biases), and the target embeddings. We freeze components one at a time during continued training to see how much the adaptation depends on each co"
W18-6313,E17-3017,0,0.0649422,"Missing"
W18-6313,Q13-1035,0,0.0453818,"Missing"
W18-6313,W18-2708,1,0.893091,"tation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the test sets. 125 cross-li"
W18-6313,W17-2620,0,0.0293601,"field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the test sets. 125 cross-lingual transfer learning approaches (Gr´ezl et al., 2014; Kunze et al., 2017). Usually, the lower layers of the network, which perform acoustic modeling, are frozen and only the upper layers are updated. In a similar vein, other works (Swietojanski and Renals, 2014; Vilar, 2018) adapt a network to a new domain by learning additional weights that re-scale the hidden units. 3 Data Our experiments are carried out across three language pairs, from Russian, Korean, and German into English. Basic statistics on the datasets used for our experiments are summarized in Table 2. The three languages represent three different domain adaptation scenarios: • In German, both the in- a"
W18-6313,L18-1146,0,0.0799127,"Missing"
W18-6313,L16-1147,0,0.0169409,"fied communication and sharing of information between the project partners enables the transfer of expertise in rural tourism. WIPO The films coated therewith, in particular polycarbonate films coated therewith, have improved properties with regard to scratch resistance, solvent resistance, and reduced oiling effect, said films thus being especially suitable for use in producing plastic parts in film insert molding methods. Table 3: Example sentences to illustrate domain differences. 3.1 Out-of-domain Data For our out-of-domain dataset we utilize the OpenSubtitles2018 corpus (Tiedemann, 2016; Lison and Tiedemann, 2016), which consists of translated movie subtitles.1 For De–En and Ru– En, we also use data from WMT 2017 (Bojar et al., 2017),2 which contains data from several sources: Europarl (parliamentary proceedings) (Koehn, 2005),3 News Commentary (political and economic news commentary),4 Common Crawl (web-crawled parallel corpus), and the EU Press Releases. We use the final 2500 lines of OpenSubtitles2018 for the development set. For German and Russian we also concatenate newstest2016 as part of the development set. newstest2016 consists of translated news articles released by WMT for its shared task. I"
W18-6313,D07-1036,0,0.042488,"Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zop"
W18-6313,2015.iwslt-evaluation.11,0,0.369812,"t the out-ofdomain model can provide a good generic initialization for the new domain. 1 hands your Wash Decoder Encoder Source Embedding Wasch dir die Hände Figure 1: Visualization of an NMT system segmented into components. Introduction Neural Machine Translation (NMT) has supplanted Phrase-Based Machine Translation (PBMT) as the standard for high-resource machine translation. This has necessitated new domain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see §2). Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance. In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training. Motivated by domain adaptation analysis in PBMT (Haddow and Koehn, 2012; Duh et al., 2010; Irvine et al., 2013), this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training. We"
W18-6313,D15-1166,0,0.13368,"Missing"
W18-6313,N18-2080,0,0.103829,"2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the test sets. 125 cross-lingual transfer learning approaches (Gr´ezl et al., 2014; Kunze et al., 2017). Usually, the lower layers of the network, which perform acoustic modeling, are frozen and only the upper layers are updated. In a similar vein, other works (Swietojanski and Renals, 2014; Vilar, 2018) adapt a network to a new domain by learning additional weights that re-scale the hidden units. 3 Data Our experiments are carried out across three language pairs, from Russian, Korean, and German into English. Basic statistics on the datasets used for our experiments are summarized in Table 2. The three languages represent three different domain adaptation scenarios: • In German, both the in- and out-of-domain datasets are large. • In Russian, the in-domain dataset is large but the out-of-domain dataset is small. • In Korean, both in- and out-of-domain datasets are small. OpenSubtitles You’re"
W18-6313,2014.eamt-1.6,0,0.0183159,"M 179.8 M De–En WMT 5.8 M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domain"
W18-6313,D16-1163,0,0.0343017,"007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO"
W18-6313,D09-1074,0,0.0328423,"M Ru–En Subtitles 25.9 M 179.8 M De–En WMT 5.8 M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to a"
W18-6313,D17-1156,0,0.0510244,"data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentatio"
W18-6313,P18-2050,0,0.0137192,"hat is, continued training is able to adapt the overall system to a new domain by modifying only parameters in a single component. This finding goes against the intuitive hypothesis that source embeddings must account for domain changes in the source vocabulary, target embeddings must account for changes in the target vocabulary, etc. We note that the encoder and decoder, despite having the least parameters (3.7M and 6.8M, respectively, out of 56M), perform strongly across all languages. This suggests further work on adapting only a subset of parameters may be warranted (see also Vilar, 2018; Michel and Neubig, 2018). 129 Acknowledgements Softmax Encoder Decoder Source Embed Target Embed 30 20 The authors would like to thank Lane Schwartz and Graham Neubig for their roles in organizing the MT Marathon in the Americas (MTMA), where this work began. The authors would also like to thank Michael Denkowski and David Vilar for assistance with S OCKEYE. This material is based upon work supported in part by the DARPA LORELEI and IARPA MATERIAL programs. Brian Thompson is supported by the Department of Defense through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program. Antonios Anastaso"
W18-6313,P10-2041,0,0.0713404,"rce Out-of-domain training sets Ru–En WMT 25.2 M 563.9 M Ru–En Subtitles 25.9 M 179.8 M De–En WMT 5.8 M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-"
W18-6313,I17-2050,0,0.0190351,"0; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k"
W18-6313,P16-1162,0,0.0613931,"based on the WIPO development set perplexity and report results on the WIPO test sets. In-domain Data We perform adaptation into the World International Property Organization (WIPO) COPPA-V2 dataset (Junczys-Dowmunt et al., 2016).5 The WIPO data consist of parallel sentences from international patent application abstracts. We reserve 3000 lines each for the in-domain development and test sets. See Table 3 for an example WIPO sentence. 3.3 mented into words using the KoNLPy wrapper of the Mecab-Ko segmenter.7 As a final preprocessing step, we train Byte Pair Encoding (BPE) segmentation models (Sennrich et al., 2016) on the out-of-domain training corpus. We train separate BPE models for each language, each with a vocabulary size of 30,000. For each language, BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets. This mimics the realistic setting where a generic, computationally-expensive-to-train NMT model is trained once. This NMT model is then adapted to new domains as they emerge, without retraining on the out-of-domain corpus. Training BPE on the in-domain data would change the vocabulary and thus requ"
W18-6313,L16-1559,0,0.0130136,"boat. WMT Intensified communication and sharing of information between the project partners enables the transfer of expertise in rural tourism. WIPO The films coated therewith, in particular polycarbonate films coated therewith, have improved properties with regard to scratch resistance, solvent resistance, and reduced oiling effect, said films thus being especially suitable for use in producing plastic parts in film insert molding methods. Table 3: Example sentences to illustrate domain differences. 3.1 Out-of-domain Data For our out-of-domain dataset we utilize the OpenSubtitles2018 corpus (Tiedemann, 2016; Lison and Tiedemann, 2016), which consists of translated movie subtitles.1 For De–En and Ru– En, we also use data from WMT 2017 (Bojar et al., 2017),2 which contains data from several sources: Europarl (parliamentary proceedings) (Koehn, 2005),3 News Commentary (political and economic news commentary),4 Common Crawl (web-crawled parallel corpus), and the EU Press Releases. We use the final 2500 lines of OpenSubtitles2018 for the development set. For German and Russian we also concatenate newstest2016 as part of the development set. newstest2016 consists of translated news articles released b"
W18-6313,2005.mtsummit-papers.11,1,\N,Missing
W18-6417,W17-4724,1,0.846102,"Missing"
W18-6417,W18-1820,0,0.0303837,"Missing"
W18-6417,P16-1009,0,0.044916,"lish–German), but fell short against this year’s best performing systems (45.3 vs. 48.4 (-3.1) and 43.4 vs. 48.3 (-4.9), respectively)1 . The best models this year used the Transformer model instead of the recurrent neural networks that our models are based on. Our novel contributions are iterative back-translation and fine-tuning on prior test sets. For Russian–English, we carried out extensive hyperparameter search, with different numbers of layers, embedding and hidden state sizes, and drop-out settings. 2 2.1 We started with shallow systems similar to Edinburgh’s submission two years ago (Sennrich et al., 2016a). It uses byte pair encoding with a vocabulary of 50,000 (Sennrich et al., 2016c) and backtranslation of the news2016 monolingual corpus (Sennrich et al., 2016b), about twice the size of the original training data. For each training run, we compare different ways to obtain a single best model. • Use the single model that performed best on the dev set (newstest2016). German–English and English–German The systems for the German–English language pairs were developed with the Marian toolkit (Junczys-Dowmunt et al., 2018). We developed models with both shallow and deep architectures, based on rec"
W18-6417,P16-1162,0,0.0980537,"lish–German), but fell short against this year’s best performing systems (45.3 vs. 48.4 (-3.1) and 43.4 vs. 48.3 (-4.9), respectively)1 . The best models this year used the Transformer model instead of the recurrent neural networks that our models are based on. Our novel contributions are iterative back-translation and fine-tuning on prior test sets. For Russian–English, we carried out extensive hyperparameter search, with different numbers of layers, embedding and hidden state sizes, and drop-out settings. 2 2.1 We started with shallow systems similar to Edinburgh’s submission two years ago (Sennrich et al., 2016a). It uses byte pair encoding with a vocabulary of 50,000 (Sennrich et al., 2016c) and backtranslation of the news2016 monolingual corpus (Sennrich et al., 2016b), about twice the size of the original training data. For each training run, we compare different ways to obtain a single best model. • Use the single model that performed best on the dev set (newstest2016). German–English and English–German The systems for the German–English language pairs were developed with the Marian toolkit (Junczys-Dowmunt et al., 2018). We developed models with both shallow and deep architectures, based on rec"
W18-6417,W18-2703,1,0.880545,"Missing"
W18-6417,P18-4020,0,0.0578053,"Missing"
W18-6417,N16-1046,0,0.0700336,"Missing"
W18-6417,2015.iwslt-evaluation.11,0,0.236565,"Missing"
W18-6417,W17-4739,0,0.0416451,"Missing"
W18-6417,W16-2323,0,0.0284616,"lish–German), but fell short against this year’s best performing systems (45.3 vs. 48.4 (-3.1) and 43.4 vs. 48.3 (-4.9), respectively)1 . The best models this year used the Transformer model instead of the recurrent neural networks that our models are based on. Our novel contributions are iterative back-translation and fine-tuning on prior test sets. For Russian–English, we carried out extensive hyperparameter search, with different numbers of layers, embedding and hidden state sizes, and drop-out settings. 2 2.1 We started with shallow systems similar to Edinburgh’s submission two years ago (Sennrich et al., 2016a). It uses byte pair encoding with a vocabulary of 50,000 (Sennrich et al., 2016c) and backtranslation of the news2016 monolingual corpus (Sennrich et al., 2016b), about twice the size of the original training data. For each training run, we compare different ways to obtain a single best model. • Use the single model that performed best on the dev set (newstest2016). German–English and English–German The systems for the German–English language pairs were developed with the Marian toolkit (Junczys-Dowmunt et al., 2018). We developed models with both shallow and deep architectures, based on rec"
W19-1424,W18-3931,0,0.0909609,"Missing"
W19-1424,W14-3911,0,0.0285096,"ch conditions it is beneficial to perform dialect identification for Arabic neural machine translation versus using a general system for all dialects. 1 Introduction Arabic exhibits a linguistic phenomenon called diglossia—speakers use Modern Standard Arabic (MSA) for formal settings and local dialects for informal settings. There are broad categories of dialects by region, such as Levantine or Maghrebi. However, dialects also vary at a finer-grained level, even within individual countries. An additional complication is that code-switching, i.e. mixing MSA and dialect, is a common occurrence (Elfardy et al., 2014). To put the importance of handling Arabic dialects in perspective, Ethnologue lists Arabic as having the 5th highest number of L1 speakers, spread over 21 regional dialects.1 The bulk of work on translating Arabic dialects uses rule-based and statistical machine translation, and much of it is translating between dialects and MSA. Generally, this work builds systems for specific dialects, with substantial amounts of information about the dialects themselves built in (Harrat et al., 2017). In the meantime, neural machine translation has become the dominant paradigm, and with it multi1 2 In the"
W19-1424,N16-1101,0,0.0589038,"Missing"
W19-1424,W14-3612,0,0.0227621,"he dual form. The dialects also display lexical and phonetic divergences, with some modification of grammatical structures such as tense markers. (Versteegh, 2014; Watson, 2007) One major challenge of working with dialects in an NLP setting is that they have not been historically written down. However, with the rise of informal texts on the internet and social media, it is more common for dialectal Arabic to appear on the internet, but without having formalized orthography. In fact, it is common on social media to use Latin script including numerals to represent Arabic sounds, dubbed Arabizi (Bies et al., 2014). We work with data which is in the Arabic script only, but Arabizi is an important phenomenon to keep in mind for future work. The exact regional groupings of regional dialects are not entirely consistent, but here are a few major groupings (including the two which we work with in this paper): 2. Egyptian: unusual in the amount of media available for NLP, such as Egyptian Arabic Wikipedia. Egypt has produced cinema in Egyptian Arabic that is distributed across the Arab world, increasing the reach of the dialect. 3. Levantine: spoken in parts of Lebanon, Jordan, Syria, Palestine, Israel, and T"
W19-1424,E17-3017,0,0.0304688,"ectal fashion. The simplest variant, introduced in Johnson et al. (2017), uses a shared wordpiece vocabulary and trains with data from several languages, adding a tag indicating the language at the beginning of each sentence. We follow this approach, but removing the tag, as in (Lee et al., 2017), and using a Transformer. We use a shared subword vocabulary by applying Byte-Pair Encoding (BPE) to the data for all variants concatenated (Sennrich et al., 2016). However, here we are not dealing with completely different languages, but rather variants of a lan5.2 Implementation We use the Sockeye (Hieber et al., 2017) implementation of a Transformer (Vaswani et al., 2017) for all of our experiments. We used 6 layers, 512dimensional embeddings, 8 attention heads, and 3 By normalize the orthography, we mean that we removed diacritics and tatweels and normalized alefs and yas. For tokenization, we used the Moses tokenizer for English, since it does not have one for Arabic. We did not apply Arabicspecific tokenization that segments clitics as well. 217 MSA EGY LEV 10k BPE 38.23 22.44 22.31 Multidialectal 30k BPE 50k BPE 38.49 38.22 21.93 21.12 21.89 21.47 10k BPE 36.42 22.79 23.78 Dialect-Tuned 30k BPE 50k BPE"
W19-1424,C12-3048,0,0.0208531,"ting dialects and varieties with NMT recently. Costa-juss`a et al. (2018) find that NMT improves over SMT for translating between Brazilian and European Portuguese, though that is a higher resource setting. Lakew et al. (2018b) use a multilingual Transformer for language varieties, as we do. However, their focus is translating into the different varieties rather than from an unknown variety, and they do not work with Arabic. Harrat et al. (2017) provide a survey of machine translation for Arabic dialects. There has been a lot of work translating between dialects and MSA, primarily rule-based (Salloum and Habash, 2012), with some statistical machine translation approaches (Meftouh et al., 2015), which also translates between different dialects. More recently, Erdmann et al. (2017) translate between dialects with statistical MT, additionally modeling morphology and syntax. Translating between Arabic dialects and other languages has dealt primarily with English as the other language, as we do here. Most work on this has been done with statistical machine translation systems, and generally involves pivoting through MSA or rule-based conversions to MSA. Sawaf (2010) use a hybrid rule-based, SMT system to transl"
W19-1424,2010.amta-papers.5,0,0.0624247,"s and MSA, primarily rule-based (Salloum and Habash, 2012), with some statistical machine translation approaches (Meftouh et al., 2015), which also translates between different dialects. More recently, Erdmann et al. (2017) translate between dialects with statistical MT, additionally modeling morphology and syntax. Translating between Arabic dialects and other languages has dealt primarily with English as the other language, as we do here. Most work on this has been done with statistical machine translation systems, and generally involves pivoting through MSA or rule-based conversions to MSA. Sawaf (2010) use a hybrid rule-based, SMT system to translate dialectal Arabic. Zbib et al. (2012) explore the effects of different amounts of dialectal bitext versus MSA for SMT and try pivoting through MSA. Sajjad et al. (2013) adapts Egyptian Arabic to look like MSA with character-level transformations and uses SMT with phrase table merging to incorporate MSA-to-English data. We model our data setup after this paper, additionally using the Levantine data from the LDC corpus they use for Egyptian data (LDC2012T09). Meanwhile, Salloum et al. (2014) develop several variants of MSA and DA using SMT, and le"
W19-1424,P16-1162,0,0.0685412,"in Table 2. Multidialectal Model One approach to being able to translate sentences of unknown dialect is to train a system in a “multilingual,” or here multidialectal fashion. The simplest variant, introduced in Johnson et al. (2017), uses a shared wordpiece vocabulary and trains with data from several languages, adding a tag indicating the language at the beginning of each sentence. We follow this approach, but removing the tag, as in (Lee et al., 2017), and using a Transformer. We use a shared subword vocabulary by applying Byte-Pair Encoding (BPE) to the data for all variants concatenated (Sennrich et al., 2016). However, here we are not dealing with completely different languages, but rather variants of a lan5.2 Implementation We use the Sockeye (Hieber et al., 2017) implementation of a Transformer (Vaswani et al., 2017) for all of our experiments. We used 6 layers, 512dimensional embeddings, 8 attention heads, and 3 By normalize the orthography, we mean that we removed diacritics and tatweels and normalized alefs and yas. For tokenization, we used the Moses tokenizer for English, since it does not have one for Arabic. We did not apply Arabicspecific tokenization that segments clitics as well. 217 M"
W19-1424,C18-1054,0,0.0867252,"ts. Finally, lexical differences are noticeable in text as well. 1. Maghrebi: spoken in Morocco, Algeria, Tunisia, Libya, Western Sahara, and Mauritania. Maghrebi has French and Berber in215 3 3.1 Related Work 3.2 Translating Arabic Dialects Neural Machine Translation for Dialects and Varieties While NMT for Arabic dialects has not been extensively explored, there has been some work translating dialects and varieties with NMT recently. Costa-juss`a et al. (2018) find that NMT improves over SMT for translating between Brazilian and European Portuguese, though that is a higher resource setting. Lakew et al. (2018b) use a multilingual Transformer for language varieties, as we do. However, their focus is translating into the different varieties rather than from an unknown variety, and they do not work with Arabic. Harrat et al. (2017) provide a survey of machine translation for Arabic dialects. There has been a lot of work translating between dialects and MSA, primarily rule-based (Salloum and Habash, 2012), with some statistical machine translation approaches (Meftouh et al., 2015), which also translates between different dialects. More recently, Erdmann et al. (2017) translate between dialects with st"
W19-1424,W18-6316,0,0.291018,"ts. Finally, lexical differences are noticeable in text as well. 1. Maghrebi: spoken in Morocco, Algeria, Tunisia, Libya, Western Sahara, and Mauritania. Maghrebi has French and Berber in215 3 3.1 Related Work 3.2 Translating Arabic Dialects Neural Machine Translation for Dialects and Varieties While NMT for Arabic dialects has not been extensively explored, there has been some work translating dialects and varieties with NMT recently. Costa-juss`a et al. (2018) find that NMT improves over SMT for translating between Brazilian and European Portuguese, though that is a higher resource setting. Lakew et al. (2018b) use a multilingual Transformer for language varieties, as we do. However, their focus is translating into the different varieties rather than from an unknown variety, and they do not work with Arabic. Harrat et al. (2017) provide a survey of machine translation for Arabic dialects. There has been a lot of work translating between dialects and MSA, primarily rule-based (Salloum and Habash, 2012), with some statistical machine translation approaches (Meftouh et al., 2015), which also translates between different dialects. More recently, Erdmann et al. (2017) translate between dialects with st"
W19-1424,Q17-1026,0,0.0266361,"d BPE with 10k, 30k, and 50k merge operations, training on the concatenation of all of the training data. The final counts of sentences for our data are shown in Table 2. Multidialectal Model One approach to being able to translate sentences of unknown dialect is to train a system in a “multilingual,” or here multidialectal fashion. The simplest variant, introduced in Johnson et al. (2017), uses a shared wordpiece vocabulary and trains with data from several languages, adding a tag indicating the language at the beginning of each sentence. We follow this approach, but removing the tag, as in (Lee et al., 2017), and using a Transformer. We use a shared subword vocabulary by applying Byte-Pair Encoding (BPE) to the data for all variants concatenated (Sennrich et al., 2016). However, here we are not dealing with completely different languages, but rather variants of a lan5.2 Implementation We use the Sockeye (Hieber et al., 2017) implementation of a Transformer (Vaswani et al., 2017) for all of our experiments. We used 6 layers, 512dimensional embeddings, 8 attention heads, and 3 By normalize the orthography, we mean that we removed diacritics and tatweels and normalized alefs and yas. For tokenizatio"
W19-1424,P12-3005,0,0.0340472,"understand which combinations of test sentences and models are least and most compatible, we also present a matrix of all combinations of model and test set in 3. We can see that EGY and LEV test sets are much more harmed by the MSA model than the LEV- and EGY- tuned models respectively. It is possible that there is some shared vocabulary between EGY and LEV that it is learning, or that the EGY and LEV training sets are just a closer domain to each other than to MSA. Finally, we use a very simple baseline for dialect ID to see how it performs. We train a model for language ID with langid.py (Lui and Baldwin, 2012), which uses naive Bayes classifier with a multinomial event model. Training langid.py on our data does not work well for dialect ID—in particular, the system is very sensitive to data size. It would probably be better to provide larger quantities of monolingual data for this if avaiable. However, we report results here to give a sense of how a very basic language ID system might perform. We try training it in two ways: (1) with the data proportions left as-is and (2) upsampling the EGY and LEV data sizes to match the MSA data size. (1) results in predicting almost all sentences as MSA, and (2"
W19-1424,J14-1006,0,0.015413,"the two which we work with in this paper): 2. Egyptian: unusual in the amount of media available for NLP, such as Egyptian Arabic Wikipedia. Egypt has produced cinema in Egyptian Arabic that is distributed across the Arab world, increasing the reach of the dialect. 3. Levantine: spoken in parts of Lebanon, Jordan, Syria, Palestine, Israel, and Turkey. 4. Arabian Peninsula: includes subcategories such as Gulf (spoken along the Persian Gulf) and Hejazi (spoken in parts of Saudi Arabia including Mecca). 5. Iraqi: spoken in Iraq and parts of neighboring countries, also called Mesopotamian Arabic. Zaidan and Callison-Burch (2014) detail in particular the ways in which dialectal varieties might manifest in their written form, from an NLP perspective. For instance, with respect to morphology, they note that the disappearance of grammatical case in dialects mostly only appears in the accusative when a suffix is added, because case in MSA generally are denoted by short vowels which are usually omitted from text. The disappearance of duals and feminine plurals is also noticeable, as well as the addition of more complex cliticization (such as circumfix negation). With respect to syntax, they note that VSO word order is more"
W19-1424,Y15-1004,0,0.018017,"at NMT improves over SMT for translating between Brazilian and European Portuguese, though that is a higher resource setting. Lakew et al. (2018b) use a multilingual Transformer for language varieties, as we do. However, their focus is translating into the different varieties rather than from an unknown variety, and they do not work with Arabic. Harrat et al. (2017) provide a survey of machine translation for Arabic dialects. There has been a lot of work translating between dialects and MSA, primarily rule-based (Salloum and Habash, 2012), with some statistical machine translation approaches (Meftouh et al., 2015), which also translates between different dialects. More recently, Erdmann et al. (2017) translate between dialects with statistical MT, additionally modeling morphology and syntax. Translating between Arabic dialects and other languages has dealt primarily with English as the other language, as we do here. Most work on this has been done with statistical machine translation systems, and generally involves pivoting through MSA or rule-based conversions to MSA. Sawaf (2010) use a hybrid rule-based, SMT system to translate dialectal Arabic. Zbib et al. (2012) explore the effects of different amo"
W19-1424,N12-1006,0,0.0286866,"l machine translation approaches (Meftouh et al., 2015), which also translates between different dialects. More recently, Erdmann et al. (2017) translate between dialects with statistical MT, additionally modeling morphology and syntax. Translating between Arabic dialects and other languages has dealt primarily with English as the other language, as we do here. Most work on this has been done with statistical machine translation systems, and generally involves pivoting through MSA or rule-based conversions to MSA. Sawaf (2010) use a hybrid rule-based, SMT system to translate dialectal Arabic. Zbib et al. (2012) explore the effects of different amounts of dialectal bitext versus MSA for SMT and try pivoting through MSA. Sajjad et al. (2013) adapts Egyptian Arabic to look like MSA with character-level transformations and uses SMT with phrase table merging to incorporate MSA-to-English data. We model our data setup after this paper, additionally using the Levantine data from the LDC corpus they use for Egyptian data (LDC2012T09). Meanwhile, Salloum et al. (2014) develop several variants of MSA and DA using SMT, and learn a Naive Bayes Classifier to determine which system would be best suited to transla"
W19-1424,N12-4000,0,0.228233,"anslation approaches (Meftouh et al., 2015), which also translates between different dialects. More recently, Erdmann et al. (2017) translate between dialects with statistical MT, additionally modeling morphology and syntax. Translating between Arabic dialects and other languages has dealt primarily with English as the other language, as we do here. Most work on this has been done with statistical machine translation systems, and generally involves pivoting through MSA or rule-based conversions to MSA. Sawaf (2010) use a hybrid rule-based, SMT system to translate dialectal Arabic. Zbib et al. (2012) explore the effects of different amounts of dialectal bitext versus MSA for SMT and try pivoting through MSA. Sajjad et al. (2013) adapts Egyptian Arabic to look like MSA with character-level transformations and uses SMT with phrase table merging to incorporate MSA-to-English data. We model our data setup after this paper, additionally using the Levantine data from the LDC corpus they use for Egyptian data (LDC2012T09). Meanwhile, Salloum et al. (2014) develop several variants of MSA and DA using SMT, and learn a Naive Bayes Classifier to determine which system would be best suited to transla"
W19-1424,P13-2001,0,0.021767,"et al. (2017) translate between dialects with statistical MT, additionally modeling morphology and syntax. Translating between Arabic dialects and other languages has dealt primarily with English as the other language, as we do here. Most work on this has been done with statistical machine translation systems, and generally involves pivoting through MSA or rule-based conversions to MSA. Sawaf (2010) use a hybrid rule-based, SMT system to translate dialectal Arabic. Zbib et al. (2012) explore the effects of different amounts of dialectal bitext versus MSA for SMT and try pivoting through MSA. Sajjad et al. (2013) adapts Egyptian Arabic to look like MSA with character-level transformations and uses SMT with phrase table merging to incorporate MSA-to-English data. We model our data setup after this paper, additionally using the Levantine data from the LDC corpus they use for Egyptian data (LDC2012T09). Meanwhile, Salloum et al. (2014) develop several variants of MSA and DA using SMT, and learn a Naive Bayes Classifier to determine which system would be best suited to translate data of unknown dialect. This is similar to our work in considering the possibility of the dialect being unknown, though we cons"
W19-1424,C18-1113,0,0.0301211,"Missing"
W19-1424,D16-1163,0,0.0261993,"e finer-grained case in future work. 3.4 Multilingual NMT One of the benefits of neural machine translation is the ease of sharing parameters across models, lending itself well to multilingual machine translation (Firat et al., 2016; Johnson et al., 2017; Lee et al., 2017). A multilingual approach uses all of the training data together (possibly up-sampling low-resource languages) to build one model with a single set of parameters. On the other hand, people have also found transfer learning by simple fine-tuning to work well, especially between related high-resource and lowresource languages (Zoph et al., 2016). The multilingual approach has the benefit of not requiring us to know which dialect we are translating. Meanwhile, with enough training data in the correct dialect, we may be able to do better than 216 guage. that with the fine-tuning approach. This is the trade-off we explore here. We use a Transformer model (Vaswani et al., 2017), as it has seen to do perform better in general as well as in the multilingual setting (Lakew et al., 2018a). 4 4.2 On the other hand, dialect identification is an active area of research, and an alternative approach is to design a dialect-specific model for each"
W19-1424,P14-2125,0,0.01974,"y involves pivoting through MSA or rule-based conversions to MSA. Sawaf (2010) use a hybrid rule-based, SMT system to translate dialectal Arabic. Zbib et al. (2012) explore the effects of different amounts of dialectal bitext versus MSA for SMT and try pivoting through MSA. Sajjad et al. (2013) adapts Egyptian Arabic to look like MSA with character-level transformations and uses SMT with phrase table merging to incorporate MSA-to-English data. We model our data setup after this paper, additionally using the Levantine data from the LDC corpus they use for Egyptian data (LDC2012T09). Meanwhile, Salloum et al. (2014) develop several variants of MSA and DA using SMT, and learn a Naive Bayes Classifier to determine which system would be best suited to translate data of unknown dialect. This is similar to our work in considering the possibility of the dialect being unknown, though we consider Neural Machine Translation (NMT) approaches. As for using NMT on dialectal Arabic, Guellil et al. (2017) try using NMT on transliterated Algerian data and find that SMT outperforms it. Meanwhile, Hassan et al. (2017) generate synthetic Levantine data using monolingual word embeddings and add that to MSA-English data, br"
W19-4634,Q17-1010,0,0.0301507,"nal penalty and further recursive backoff for ‘z’ using the context of the single symbol (‘i’). To use PPM for classification rather than compression, models M1 , M2 , ..., Mn are trained for each discrete class. Then for a given textual sample t, choose the model that encodes t in the least 3.2 Word Embeddings For the word-based neural models, we use 300dimensional word embeddings trained on different amounts of data as input representations. First, we use randomly initialized embeddings. Then, we train fastText continuous bag of words (cbow) models with default parameters on the MADAR data (Bojanowski et al., 2017).3 Finally, we utilize additional data, training on MADAR in addition to the datasets mentioned above (MADAR+). We provide final results (Macro-Average F1) from the ensemble model using each of these variants in Table 3. We see that utilizing additional data provided marginal performance gains, helping more in Subtask 2 where much of our additional data was also Twitter data, making it in-domain. Embedding Random MADAR MADAR+ Subtask 1 0.632 0.626 0.634 Subtask 2 0.399 0.397 0.411 Table 3: Effect of different word embeddings, Macro-Average F1 for final ensemble models on dev data. 3 265 https:"
W19-4634,W19-4622,0,0.018077,"fficulty of classifying short documents, highly-correlated modalities like topic and proper names can lead to overfitting, particularly for userdirected content like Twitter. Our method attempts to address the former by using a language modeling technique that has empirically been found to perform well on extremely short documents. For the latter, we employ ensembles of heterogeneous neural architectures and aggressive dropout, with the goal of finding a broad range of features that support the task without overfitting. Our submission to the MADAR shared task on Arabic dialect identification (Bouamor et al., 2019) employed a language modeling technique called Prediction by Partial Matching, an ensemble of neural architectures, and sources of additional data for training word embeddings and auxiliary language models.1 We found several of these techniques provided small boosts in performance, though a simple character-level language model was a strong baseline, and a lower-order LM achieved best performance on Subtask 2. Interestingly, word embeddings provided no consistent benefit, and ensembling struggled to outperform the best component submodel. This suggests the variety of architectures are learning"
W19-4634,W16-4825,1,0.862646,"dels can either be used directly for their “native” task, or produce probabilities that may contain useful signal for a downstream task. Table 2 shows how the native models for each MADAR subtask perform with different values of maximal order N on dev data. N = 4 was best for Subtask 1, and N = 3 was best for Subtask 2. System PPM Language Models Prediction by Partial Matching (PPM) was first introduced as a sequence compression algorithm (Cleary and Witten, 1984) but has been found to be particularly effective as a character language model for classifying short documents (Frank et al., 2000; McNamee, 2016), using the probabilities directly rather than as input to a numeric encoding. PPM is based on a variable-order Markov model that contains a parameter N known as the maximal order. When compressing data files or training a classification model, observations from previously seen data are used to estimate the likelihood of observing a symbol following a given context of up to N characters. Longer contexts are used when available, starting with the maximal order N . However, PPM automatically backs off to use shorter contexts when a symbol has never been observed in a longer context. A context-de"
W19-4634,C18-1113,0,0.0991291,"Missing"
W19-4634,P11-2007,0,0.0162365,"k 2. Interestingly, word embeddings provided no consistent benefit, and ensembling struggled to outperform the best component submodel. This suggests the variety of architectures are learning redundant information, and future work may focus on encouraging decorrelated learning. 1 2 In addition to the data provided by the MADAR subtasks, we used the following data sets to train embeddings or auxiliary language models: Introduction 1. Preexisting collections of the Arabic Dialect Corpus (ADC) of 150k comments from three Arabic-language newspaper sites focused on Saudi Arabia, Jordan, and Egypt (Zaidan and Callison-Burch, 2011) While Modern Standard Arabic (MSA) is used across many countries for formal written communication, regional Arabic dialects vary substantially. Dialect identification has traditionally been performed at the level of broad families of dialects—for instance grouping many dialects across the Arabian Peninsula together. However, even within a single country there is often noticeable variation from one city to another. The MADAR dataset and corresponding shared task aim to perform dialect identification at a finergrained level. Subtask 1 aims to distinguish travel phrases produced between Arabic d"
W19-5366,W18-2705,1,0.847977,"optimizer: 0.001, 0.0003, 0.0006 • Number of attention heads (head): 8, 16 • Number of layers (layer): 2, 4 Dataset sizes are shown in Table 4. JA→EN dataset Train-ALL Train-MTNT Valid-ALL Valid-MTNT Test18-MTNT EN→JA dataset Train-ALL Train-MTNT Valid-ALL Valid-MTNT Test18-MTNT Models segments words • Feed-forward layer size (ffsize): 1024, 2048 3.9m 6506 5416 965 1001 42.7m 155k 88k 23k 13k • Embedding and model size (embedding): 256, 518, 1024 segments words 3.9m 5775 5405 954 1002 42.9m 333k 111k 46k 13k The training process follows a continuedtraining procedure (c.f. Koehn et al. (2018); Khayrallah et al. (2018)): In Stage 1, we train systems from scratch on Train-ALL, and perform early stopping on Valid-ALL. This represents a mixed corpus with both in-domain and out-of-domain bitexts. For all models, we used batch sizes of 4,096 words, checkpointed every 2,000 updates, and stopped training with the bestperplexity checkpoint when validation perplexity on Valid-ALL had failed to improve for 16 consecutive checkpoints. In Stage 2, we fine-tuned the above systems by training on Train-MTNT, and perform early stopping on Valid-MTNT. Effectively, we initialize a new model with Stage 1 model weights, reset"
W19-5366,W18-6417,1,0.852469,"te (LR) for the ADAM optimizer: 0.001, 0.0003, 0.0006 • Number of attention heads (head): 8, 16 • Number of layers (layer): 2, 4 Dataset sizes are shown in Table 4. JA→EN dataset Train-ALL Train-MTNT Valid-ALL Valid-MTNT Test18-MTNT EN→JA dataset Train-ALL Train-MTNT Valid-ALL Valid-MTNT Test18-MTNT Models segments words • Feed-forward layer size (ffsize): 1024, 2048 3.9m 6506 5416 965 1001 42.7m 155k 88k 23k 13k • Embedding and model size (embedding): 256, 518, 1024 segments words 3.9m 5775 5405 954 1002 42.9m 333k 111k 46k 13k The training process follows a continuedtraining procedure (c.f. Koehn et al. (2018); Khayrallah et al. (2018)): In Stage 1, we train systems from scratch on Train-ALL, and perform early stopping on Valid-ALL. This represents a mixed corpus with both in-domain and out-of-domain bitexts. For all models, we used batch sizes of 4,096 words, checkpointed every 2,000 updates, and stopped training with the bestperplexity checkpoint when validation perplexity on Valid-ALL had failed to improve for 16 consecutive checkpoints. In Stage 2, we fine-tuned the above systems by training on Train-MTNT, and perform early stopping on Valid-MTNT. Effectively, we initialize a new model with Sta"
W19-5366,P07-2045,0,0.00872686,"1 We determined this threshold by eyeballing where in the ranked list the garbage started to thin out. • all of Europarl and News Commentary; 552 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 552–558 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics training data sizes more manageable. We therefore did not compare against a model trained on all of the filtered data. We experimented with two preprocessing regimes. In the first, we applied standard preprocessing techniques from the Moses pipeline2 (Koehn et al., 2007), followed by subword splitting with BPE (Sennrich et al., 2016) using 32k merge operations. In the second scenario, we did not use any data preparation, instead applying sentencepiece (Kudo and Richardson, 2018) with subword regularization (Kudo, 2018) directly to the raw text. In this latter setting, we varied the size of the learned subword models, experimenting with 8k, 16k, 24k, and 32k. 2.2 Models 4 layers (BPE) 31.6 27.9 6 layers (BPE) + MTNT + filter + both 32.7 32.6 36.4 37.2 27.9 32.9 33.7 39.9 sp24k + filter sp24k + both 36.5 37.2 34.5 40.0 size WMT15 filter both MTNT18 filter both"
W19-5366,P18-1007,0,0.104464,"58 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics training data sizes more manageable. We therefore did not compare against a model trained on all of the filtered data. We experimented with two preprocessing regimes. In the first, we applied standard preprocessing techniques from the Moses pipeline2 (Koehn et al., 2007), followed by subword splitting with BPE (Sennrich et al., 2016) using 32k merge operations. In the second scenario, we did not use any data preparation, instead applying sentencepiece (Kudo and Richardson, 2018) with subword regularization (Kudo, 2018) directly to the raw text. In this latter setting, we varied the size of the learned subword models, experimenting with 8k, 16k, 24k, and 32k. 2.2 Models 4 layers (BPE) 31.6 27.9 6 layers (BPE) + MTNT + filter + both 32.7 32.6 36.4 37.2 27.9 32.9 33.7 39.9 sp24k + filter sp24k + both 36.5 37.2 34.5 40.0 size WMT15 filter both MTNT18 filter both 8k 16k 24k 36.0 36.2 36.5 33.9 33.9 34.5 36.5 36.9 37.2 38.7 39.7 40.0 Table 3: BLEU scores with the sentencepiece models and no other preprocessing. Observation 1 Improvements are to be had both from more data and from better (in-domain) data. Adding t"
W19-5366,D18-2012,0,0.0241351,"(WMT), Volume 2: Shared Task Papers (Day 1) pages 552–558 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics training data sizes more manageable. We therefore did not compare against a model trained on all of the filtered data. We experimented with two preprocessing regimes. In the first, we applied standard preprocessing techniques from the Moses pipeline2 (Koehn et al., 2007), followed by subword splitting with BPE (Sennrich et al., 2016) using 32k merge operations. In the second scenario, we did not use any data preparation, instead applying sentencepiece (Kudo and Richardson, 2018) with subword regularization (Kudo, 2018) directly to the raw text. In this latter setting, we varied the size of the learned subword models, experimenting with 8k, 16k, 24k, and 32k. 2.2 Models 4 layers (BPE) 31.6 27.9 6 layers (BPE) + MTNT + filter + both 32.7 32.6 36.4 37.2 27.9 32.9 33.7 39.9 sp24k + filter sp24k + both 36.5 37.2 34.5 40.0 size WMT15 filter both MTNT18 filter both 8k 16k 24k 36.0 36.2 36.5 33.9 33.9 34.5 36.5 36.9 37.2 38.7 39.7 40.0 Table 3: BLEU scores with the sentencepiece models and no other preprocessing. Observation 1 Improvements are to be had both from more data a"
W19-5366,D18-1050,0,0.109035,"irst performed word segmentation by Kytea (Neubig et al., 2011)6 , then ran the English Moses preprocessing pipeline to handle potential code-switched English/Japanese in the data. Finally, we induced BPE subword units with 10k, 30k, and 50k merge operations, independently for each side on the bitexts (JA→EN Train-ALL and EN→JA Train-ALL). Unlike the French-English systems, the JapaneseEnglish systems do not use shared BPE and embeddings. Training Data We trained systems using only the bitext data allowed in the shared task constrained setting: • The in-domain Reddit dataset–MTNT version 1.1 (Michel and Neubig, 2018)4 – consists of approximately 6k segments for training (which we label Train-MTNT) and 900 segments for validation (Valid-MTNT) in both JA→EN and EN→JA language directions. Additionally we use the included ”test set” (which we label Test18-MTNT) for internal BLEU benchmarks prior to submitting results for the official 2019 blindtest. We did not use the monolingual part of MTNT. 3.2 We use the Sockeye Transformer models for both JA→EN and EN→JA directions, similar to our French-English systems. The hyperparameter settings are different, however. We performed random search in the following hyper"
W19-5366,P11-2093,0,0.0126734,"model (with no other preprocessing) was just as good as the BPE model (with the Moses preprocessing pipeline). In one situation (adding the filtered data alone), it caused a gain of 0.8 over its BPE counterpart. We further conducted a small experiment varying the sentencepiece model size (Table 3). Larger sentencepiece models were consistently better in this relatively large-data setting. Our score on the official MTNT2019 blind test set was 40.2. 3 Japanese-English Systems 3.1 as the French–English system. For preprocessing on the Japanese side, we first performed word segmentation by Kytea (Neubig et al., 2011)6 , then ran the English Moses preprocessing pipeline to handle potential code-switched English/Japanese in the data. Finally, we induced BPE subword units with 10k, 30k, and 50k merge operations, independently for each side on the bitexts (JA→EN Train-ALL and EN→JA Train-ALL). Unlike the French-English systems, the JapaneseEnglish systems do not use shared BPE and embeddings. Training Data We trained systems using only the bitext data allowed in the shared task constrained setting: • The in-domain Reddit dataset–MTNT version 1.1 (Michel and Neubig, 2018)4 – consists of approximately 6k segmen"
W19-5366,E17-3017,0,0.0322505,"xt) helped on MTNT1 8 (+5) but caused a small drop on WMT15 (-0.1). Scoring At test time, we decoded with beam search using a beam of size 12. We scored with sacreBLEU (Post, 2018), with international tokenization.3 In the spirit of the robustness task, we measure BLEU not just on the reddit dataset, but also on the WMT15 newstest dataset, in order to examine how experimental variables vary in both in- and out-of-domain settings. We believe that testing both in- and outof-domain data is essential to measuring robustness. 2.4 MTNT18 Table 2: French–English translation results. We used Sockeye (Hieber et al., 2017), a sequence to sequence transduction framework written in Python and based on MXNet. Our models were variations of the Transformer architecture (Vaswani et al., 2017), mostly using default settings supplied with Sockeye: an embedding and model size of 512, a feed-forward layer size of 2048, 8 attention heads, and three-way tied embeddings. We used batch sizes of 4,096 words, checkpointed every 5,000 updates, and stopped training with the best-perplexity checkpoint when validation perplexity had failed to improve for 10 consecutive checkpoints. The initial learning rate was set to 0.0002, the"
W19-5366,W18-6319,1,0.893019,"33.7, +5.8) than adding the MTNT training data (27.9 → 32.9, +5), but the gains from both were even greater (+12). Observation 2 In order to ensure that our models did not increase accuracy on the MTNT data at the expense of in-domain data, we report scores on both WMT and MTNT test sets. In only one situation was there a problem: For the 6-layer Transformer, adding the MTNT data alone (without the large amount of filtered bitext) helped on MTNT1 8 (+5) but caused a small drop on WMT15 (-0.1). Scoring At test time, we decoded with beam search using a beam of size 12. We scored with sacreBLEU (Post, 2018), with international tokenization.3 In the spirit of the robustness task, we measure BLEU not just on the reddit dataset, but also on the WMT15 newstest dataset, in order to examine how experimental variables vary in both in- and out-of-domain settings. We believe that testing both in- and outof-domain data is essential to measuring robustness. 2.4 MTNT18 Table 2: French–English translation results. We used Sockeye (Hieber et al., 2017), a sequence to sequence transduction framework written in Python and based on MXNet. Our models were variations of the Transformer architecture (Vaswani et al."
W19-5366,W18-6478,0,0.0302442,"nese. Our goal was to evaluate the performance of reasonable state-of-the-art systems against both the robustness test set as well as more standard “general domain” test sets. We believe this is an important component of evaluating for actual robustness. In this way, we ensure that performance gains on robustness data are not purchased at the expense of this general-domain performance. Our systems used no monolingual data and relatively straightforward state-of-the-art techniques, and produced systems of roughly average performance. To filter the data, we applied dual cross-entropy filtering (Junczys-Dowmunt, 2018). We trained two smaller 4-layer Transformer models, one each for EN–FR and FR–EN, and used them to score the data according to the formula: exp(−(|s1 − s2 |+ 0.5 ∗ (s1 + s2 ))) where s1 is the score (a negative logprob) from the forward FR–EN model and s2 the score from the reverse EN–FR model. We then uniqued this data, sorted by score, and took a random sample of one million lines from the set of all sentence pairs with a score greater than 0.1.1 For all but FR–EN Gigaword, what remained was well less than a million lines. We did this both because prior work has indicated the utility of fil"
W19-5366,P16-1162,0,0.0537571,"ked list the garbage started to thin out. • all of Europarl and News Commentary; 552 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 552–558 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics training data sizes more manageable. We therefore did not compare against a model trained on all of the filtered data. We experimented with two preprocessing regimes. In the first, we applied standard preprocessing techniques from the Moses pipeline2 (Koehn et al., 2007), followed by subword splitting with BPE (Sennrich et al., 2016) using 32k merge operations. In the second scenario, we did not use any data preparation, instead applying sentencepiece (Kudo and Richardson, 2018) with subword regularization (Kudo, 2018) directly to the raw text. In this latter setting, we varied the size of the learned subword models, experimenting with 8k, 16k, 24k, and 32k. 2.2 Models 4 layers (BPE) 31.6 27.9 6 layers (BPE) + MTNT + filter + both 32.7 32.6 36.4 37.2 27.9 32.9 33.7 39.9 sp24k + filter sp24k + both 36.5 37.2 34.5 40.0 size WMT15 filter both MTNT18 filter both 8k 16k 24k 36.0 36.2 36.5 33.9 33.9 34.5 36.5 36.9 37.2 38.7 39."
W19-5366,N19-1209,1,0.830789,"ining: Next, we perform continued training on the top 5 models. The results on Test18-MTNT are shown in Table 6. We observe consistent BLEU gains in these Stage 2 models, close to 2 or 3 points across all systems. This re-affirms the surprising effectiveness of a simple procedure such as continued training; but we should also note that preliminary efforts on English-French did not yield similar gains. Note that we do not measure Valid-ALL in this case since we now expect the models to be optimized specifically for MTNT; it is likely ValidALL scores will degrade due to catastrophic forgetting (Thompson et al., 2019). Results & Discussion The BLEU results for Stage 1 models are shown in Table 5. We performed random search in hyperparameter space, training approximately 40 models in each language-pair. The table is sorted by Test18-MTNT BLEU score and shows the top 5 models in terms of BLEU (id=a,b,c,d,e; id=z,y,x,w,v) as well as another 5 randomly selected model (id=e,f,g,h,i,j; id=u,t,s,r,q). Final Submission: In the final official submission, we performed an 4-ensemble of the Stage 2 Continued Training models of id=a,b,d,e for JA→EN and id=z,y,w,v for EN→JA. Note that the ensemble method in Sockeye curr"
W19-6602,N12-1047,0,0.0245879,"thed 5-gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with count bin features (Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three languages. We observed improvements in BLEU scores under small BPE settings for all three language pairs. We filtered noisy crawled bitext using Zipporah (Xu and Koehn, 2017) and appli"
W19-6602,N09-1025,0,0.130762,"Missing"
W19-6602,P13-2071,1,0.869211,"Missing"
W19-6602,W11-2123,0,0.0624649,"Missing"
W19-6602,P07-1019,0,0.0151049,", BLEU scores for PBMT and NMT systems. length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with count bin features (Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three languages. We observed improvements in BL"
W19-6602,2012.eamt-1.58,0,0.0599057,"Missing"
W19-6602,P07-2045,1,0.0214013,"Missing"
W19-6602,N04-1022,0,0.0982591,"R1) and semi-supervised (ASR2) systems, BLEU scores for PBMT and NMT systems. length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with count bin features (Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three la"
W19-6602,D15-1166,0,0.0210252,"hical lexicalized reordering (Galley and Manning, 2008), a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with count bin features (Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three languages. We observed improvements in BLEU scores under small BPE settings for all three language pairs. We filtered noisy crawled bitext using Zipporah (Xu and Koehn, 2017) and applied the unsupervised morphology induction tool Morfessor (Virpioja et al.,"
W19-6602,P16-1162,0,0.0535767,"mum phrase-length of 5, 200-best translation options, compact phrase table (Junczys-Dowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test. We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). The NMT systems are LSTM sequence-tosequence models (Luong et al., 2015). The layer size is 512, and the number of layers is 4 for Swahili and Tagalog, 2 for Somali. The models were developed using the Fairseq4 toolkit. For NMT, we applied Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split word into subword segments for both source and target languages. The number of BPE operations is 3000 for all three languages. We observed improvements in BLEU scores under small BPE settings for all three language pairs. We filtered noisy crawled bitext using Zipporah (Xu and Koehn, 2017) and applied the unsupervised morphology induction tool Morfessor (Virpioja et al., 2013) to split words up into putative morphemes, with keeping numbers and names unchanged. We noticed that splitting the words to morphemes improves BLEU scores for Somali and Swahili, but does not help for Tagalog."
W19-6602,D17-1319,1,0.933621,"M 808k 5.2M 759k 12.3M test (#sent) 9.5k 11.7k 11.4k Table 2: ASR and MT data statistics. We used parallel corpora (bitext) of around 800k English words to train our MT systems for translating from Somali, Swahili, or Tagalog to English. This data is provided in the BUILD package of the MATERIAL project and contains news, topical, and blog texts with provided source URLs. In addition, we harvested and filtered bitext from Web to augment this baseline bitext. We made this data publicly available2 . It is important to filter web bitext to reduce noise. We filtered the web bitext using Zipporah (Xu and Koehn, 2017) and chose filter thresholds optimized on tune sets. The crawled data improved the MT system by 1 point BLEU or more for these languages. We also added monolingual WMT news and LDC Gigaword data, which include 8.2 billion English tokens in total to train the language models of our MT systems. The IR system indexes and searches ”test” documents that are either speech or text. There are around 20 hours of test speech data and 10k foreign sentences of test text data for each language. We have the reference transcripts and translations of ”test”, hence, we can measure the performance of our ASR an"
W19-6620,W19-5427,0,0.0445494,"Missing"
W19-6620,W16-4117,0,0.0218433,"Missing"
W19-6620,D18-1461,0,0.211124,"w existing literature carefully examines what is the best practice regarding application of subword methods. As hyper-parameter search is expensive, there is a tendency to simply use existing recipes. This is especially true for the number of merge operations when people are using BPE, although this configuration is closely correlated with the granularity of the segmentation on the training corpus, thus having direct influence on the final system performance. Prior to this work, Denkowski and Neubig (2017) recommended 32k BPE merge operation in their work on trustable baselines for NMT, while Cherry et al. (2018) contradicted their study by showing that character-based models outperform 32k BPE. Both of these studies are based on the LSTM-based architectures (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). To the best of our knowledge, there is no work that looks into the same problem for the Transformer architecture extensively.1 In this paper, we aim to provide guidance for this hyper-parameter choice by examining the interaction between MT system performance with the choice of BPE merge operations under the low1 For reference, the original Transformer paper by Vaswani et al. (20"
W19-6620,W17-3203,0,0.149233,"ally by performing a minimum amount of word segmentations in the training set. However, very few existing literature carefully examines what is the best practice regarding application of subword methods. As hyper-parameter search is expensive, there is a tendency to simply use existing recipes. This is especially true for the number of merge operations when people are using BPE, although this configuration is closely correlated with the granularity of the segmentation on the training corpus, thus having direct influence on the final system performance. Prior to this work, Denkowski and Neubig (2017) recommended 32k BPE merge operation in their work on trustable baselines for NMT, while Cherry et al. (2018) contradicted their study by showing that character-based models outperform 32k BPE. Both of these studies are based on the LSTM-based architectures (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). To the best of our knowledge, there is no work that looks into the same problem for the Transformer architecture extensively.1 In this paper, we aim to provide guidance for this hyper-parameter choice by examining the interaction between MT system performance with the choi"
W19-6620,D18-1029,0,0.0525497,"Missing"
W19-6620,W17-4706,0,0.0258213,"lopment set to the extent possible, in order to ensure reasonable comparison. 4.2 Analysis 2: Joint vs Separate BPE Another question that is not extensively explored in the existing literature is whether joint BPE is the definitive better approach to apply BPE. The alternative way, referred to here as separate BPE, is to build separate models for source and target side of the parallel corpus. Sennrich et al. (2016) conducted experiments with both joint and separate BPE, but these experiments were conducted with different BPE size, and not much analysis was conducted on the separate BPE model. Huck et al. (2017) is the only other work we are aware of that used with separate BPE models for their study. It was mentioned that their joint BPE vocabulary of 59500 yielded a German vocabulary twice as large as English, which is an undesirable characteristic for their study. Before comparing the system performance, we would like to systematically understand how the resulting vocabulary is different when jointly and separately applying BPE. Table 4 shows the two Dublin, Aug. 19-23, 2019 |p. 207 deeptransformer shallowtransformer 0 0.5k 1k 2k 4k 8k 16k 32k δ ar-en cs-en de-en fr-en 30.3 24.6 28.1 28.8 30.8 23."
W19-6620,D15-1166,0,0.42579,"is necessary to experiment with a wide range of different BPE operations as there is no typical optimal BPE configuration, whereas for Transformer architectures, smaller BPE size tends to be a typically optimal choice. We urge the community to make prudent choices with subword merge operations, as our experiments indicate that a sub-optimal BPE configuration alone could easily reduce the system performance by 3–4 BLEU points. 1 Introduction While achieving state-of-the-art results, it is a common constraint that Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) systems are only capable of generating a closed set of © 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 symbols. Systems with large vocabulary sizes are too hard to fit onto GPU for training, as the word embedding is generally the most parameter-dense component in the NMT architecture. For that reason, subword methods, such as Byte-Pair Encoding (BPE) (Sennrich et al., 2016), are very widely used for building NMT systems. The general idea of these methods"
W19-6620,N19-4009,0,0.0227419,"Russian (ru), Turkish (tr), Polish (pl) and Hungarian (hu) as our extra languages, paired with English. All the data are tokenized and truecased using the accompanying script from Moses decoder (Koehn et al., 2007) before training and applying BPE models.2 We use subword-nmt3 to train and apply BPE to our data. Unless otherwise specified, all of our BPE models are trained on the concatenation of the source and target training corpus, i.e. the joint BPE scheme in Sennrich et al. (2016). We use SacreBLEU (Post, 2018) to compute BLEU score.4 3.2 Architecture We build our NMT system with fairseq (Ott et al., 2019). We use two pre-configured architectures in fairseq for our study, namely lstm-wiseman-iwslt-de-en (referred to as tiny-lstm) and transformer-iwslt-de-en (referred to as deeptransformer), which are the model architecture tuned for their benchmark system trained on IWSLT 2014 German-English data. However, we find (as can be seen from Table 1) that the number of parameters in lstm-tiny is a magnitude lower than deep-transformer mainly due to the fact that the former has a single-layer uni-directional encoder and a single-layer decoder, while the later has 6 encoder and decoder layers. For a fai"
W19-6620,W18-6319,0,0.0357961,"airs from the TED corpus (Qi et al., 2018). We use Brazilian Portuguese (pt), Hebrew (he), Russian (ru), Turkish (tr), Polish (pl) and Hungarian (hu) as our extra languages, paired with English. All the data are tokenized and truecased using the accompanying script from Moses decoder (Koehn et al., 2007) before training and applying BPE models.2 We use subword-nmt3 to train and apply BPE to our data. Unless otherwise specified, all of our BPE models are trained on the concatenation of the source and target training corpus, i.e. the joint BPE scheme in Sennrich et al. (2016). We use SacreBLEU (Post, 2018) to compute BLEU score.4 3.2 Architecture We build our NMT system with fairseq (Ott et al., 2019). We use two pre-configured architectures in fairseq for our study, namely lstm-wiseman-iwslt-de-en (referred to as tiny-lstm) and transformer-iwslt-de-en (referred to as deeptransformer), which are the model architecture tuned for their benchmark system trained on IWSLT 2014 German-English data. However, we find (as can be seen from Table 1) that the number of parameters in lstm-tiny is a magnitude lower than deep-transformer mainly due to the fact that the former has a single-layer uni-directiona"
W19-6620,N18-2084,0,0.0562604,"Missing"
W19-6620,J82-2005,0,0.725533,"Missing"
W19-6620,P16-1162,0,0.841721,"that Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) systems are only capable of generating a closed set of © 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 symbols. Systems with large vocabulary sizes are too hard to fit onto GPU for training, as the word embedding is generally the most parameter-dense component in the NMT architecture. For that reason, subword methods, such as Byte-Pair Encoding (BPE) (Sennrich et al., 2016), are very widely used for building NMT systems. The general idea of these methods is to exploit the pre-defined vocabulary space optimally by performing a minimum amount of word segmentations in the training set. However, very few existing literature carefully examines what is the best practice regarding application of subword methods. As hyper-parameter search is expensive, there is a tendency to simply use existing recipes. This is especially true for the number of merge operations when people are using BPE, although this configuration is closely correlated with the granularity of the segme"
W19-6620,W17-4739,0,0.1235,"s for a wide range of systems that cover different architectures and both directions of translation for multiple language pairs. While some work has conducted experiments with different BPE settings, they are generally very limProceedings of MT Summit XVII, volume 1 ited in the range of configurations explored. For example, Sennrich et al. (2016), the original paper that proposed the BPE method, compared the system performance when using 60k separate BPE and 90k joint BPE. They found 90k to work better and used that for their subsequent winning WMT 2017 new translation shared task submission (Sennrich et al., 2017). Wu et al. (2016), on the other hand, found 8k–32k merge operations achieving optimal BLEU score performance for the wordpiece method. Denkowski and Neubig (2017) explored several hyperparameter settings, including number of BPE merge operations, to establish strong baseline for NMT on LSTM-based architectures. While Denkowski and Neubig (2017) showed that BPE models are clearly better than word-level models, their experiments on 16k and 32k BPE configuration did not show much difference. They therefore recommended “32K as a generally effective vocabulary size and 16K as a contrastive conditi"
W19-6623,W05-0909,0,0.0307904,"nslate and the other was generated by extracting example sentences from a syntax textbook. The MT-generated English data is most similar to our problem, and the most effective models for that data were the word-based scores from the language model. 6 Figure 4: Percent fluently inadequate at each checkpoint during in-domain training 5 Related Work MT quality metrics are judged based on their correlation with human judgments, and recently that has meant human adequacy judgments (Bojar et al., 2017). This indicates that any of the common MT metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) may also serve as baseline adequacy scores. However, they incorporate elements of fluency while we wish to separate fluency and adequacy. Adequacy is, essentially, semantic equivalence and the goal of SemEval’s Semantic Textual Similarity (STS) task is to measure the degree of semantic equivalence between two sentences (Cer et al., 2017). The cross-lingual version of the task is similar enough to quality estimation that one of the data sets for 2017 actually came from the WMT Proceedings of MT Summit XVII, volume 1 Conclusion We have introduced an approach to automatically detect fluently ina"
W19-6623,D16-1025,0,0.0156047,"n quality. We find that neural models are consistently more prone to this type of error than traditional statistical models. However, improving the overall quality of the MT system such as through domain adaptation reduces these errors. 1 Introduction Recent work has shown that well-trained, indomain neural machine translation (NMT) systems can produce translations that, at the sentence level, are rated on par with human reference translations (Hassan Awadalla et al., 2018). Part of this success comes from the impressive improvements in fluency of NMT output compared to previous MT paradigms (Bentivogli et al., 2016; Toral and Sánchez-Cartagena, 2017; Koehn and Knowles, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 2017). However, NMT has also been shown to sometimes produce output that is low adequacy and even unrelated to the input–particularly when not trained on sufficient in-domain data (Koehn and Knowles, 2017). Because of NMT’s uncanny ability to produce fluent output, these translations may not just be inadequate but fluently inadequate. The fluency of fluently inadequate transl"
W19-6623,Q17-1010,0,0.0194615,"Missing"
W19-6623,W07-0718,0,0.0510771,"rics subset of the data as gold standard human judgments, and we use the reference translations from the news subset in generating synthetic inadequate examples. We use the standardized human direct assessment adequacy scores from WMT16 (Bojar et al., 2016) as gold standard in determining how well each adequacy metric correlates with human judgments. However, for binary questionable/acceptable adequacy judgments, we must be sure that the inadequate examples are clearly inadequate regardless of fluency and other MT quirks. The high correlation between human judgments of fluency and adequacy in Callison-Burch et al (2007) and Graham et al (2017) may indicate that human adequacy judgments are influenced by fluency, lowering the adequacy scores of disfluent translations. To ensure that our inadequate examples are truly inadequate, we rely on synthetic examples. We generate synthetic low adequacy translations by randomly selecting pairs of reference translations from the WMT16 news task Dublin, Aug. 19-23, 2019 |p. 236 Adequacy Metric BLEU Averaged Embeddings BVSS BVSS-Reference BVSS-System CS-EN 0.54275 0.43905 0.61286 0.62178 0.53773 DE-EN 0.41975 0.18998 0.47068 0.47877 0.38887 FI-EN 0.41460 0.31218 0.51856 0."
W19-6623,S17-2001,0,0.0126721,"MT quality metrics are judged based on their correlation with human judgments, and recently that has meant human adequacy judgments (Bojar et al., 2017). This indicates that any of the common MT metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) may also serve as baseline adequacy scores. However, they incorporate elements of fluency while we wish to separate fluency and adequacy. Adequacy is, essentially, semantic equivalence and the goal of SemEval’s Semantic Textual Similarity (STS) task is to measure the degree of semantic equivalence between two sentences (Cer et al., 2017). The cross-lingual version of the task is similar enough to quality estimation that one of the data sets for 2017 actually came from the WMT Proceedings of MT Summit XVII, volume 1 Conclusion We have introduced an approach to automatically detect fluently inadequate translations in machine translation output based on automatic fluency and adequacy metrics. Applying this technique to a diverse set of statistical and neural MT systems, we found that although fluently inadequate translations are rare, NMT does appear to be consistently more prone to this type of error compared to SMT. Improving"
W19-6623,P13-2121,0,0.013361,"resholds for high fluency and dubious adequacy. 3.1 Fluency Experiments Task For WMT16, fluency judgments were collected for Czech-English (CS-EN), GermanEnglish (DE-EN), Finnish-English (FI-EN), Romanian-English (RO-EN), Russian-English (RU-EN), and Turkish-English (TR-EN) in the news shared task. Annotations were collected with the goal of system-level reliability, so many segments only have one judgment. To improve reliability, we use only segments where there are two or more judgments. Model setup Fluency scores are based on a 5gram language model. We built a 5-gram KenLM (Heafield, 2011; Heafield et al., 2013) language model using the monolingual news training data from WMT16. Results For each of the metrics described in section 2.1, we calculated the Pearson correlation with the direct assessment scores for each of the language pair data sets and for all the data combined. Results are shown in Table 1. Although these correlations are lower than we would like, we find that for all language pairs and for the combined data, Word LP mid yields the highest correlation, so we will use this formula for our fluency prediction metric. Dublin, Aug. 19-23, 2019 |p. 235 Fluency Metric MeanLP NormLP Word LPmin"
W19-6623,W11-2123,0,0.0124884,"to determine thresholds for high fluency and dubious adequacy. 3.1 Fluency Experiments Task For WMT16, fluency judgments were collected for Czech-English (CS-EN), GermanEnglish (DE-EN), Finnish-English (FI-EN), Romanian-English (RO-EN), Russian-English (RU-EN), and Turkish-English (TR-EN) in the news shared task. Annotations were collected with the goal of system-level reliability, so many segments only have one judgment. To improve reliability, we use only segments where there are two or more judgments. Model setup Fluency scores are based on a 5gram language model. We built a 5-gram KenLM (Heafield, 2011; Heafield et al., 2013) language model using the monolingual news training data from WMT16. Results For each of the metrics described in section 2.1, we calculated the Pearson correlation with the direct assessment scores for each of the language pair data sets and for all the data combined. Results are shown in Table 1. Although these correlations are lower than we would like, we find that for all language pairs and for the combined data, Word LP mid yields the highest correlation, so we will use this formula for our fluency prediction metric. Dublin, Aug. 19-23, 2019 |p. 235 Fluency Metric"
W19-6623,E17-3017,0,0.0265997,"ded in the Russian and Chinese WMT17 data 5 http://cwiki.apache.org/confluence/display/JOSHUA/ 3 Dublin, Aug. 19-23, 2019 |p. 238 Joshua General Joshua TED Only Joshua Adapted Sockeye General Sockeye TED Only Sockeye Adapted Arabic 23.50 24.49 27.11 29.6 27.42 35.37 German 30.65 28.72 31.35 34.59 32.25 39.9 Farsi 13.41 16.56 17.71 22.22 21.31 27.92 Korean 6.34 9.81 10.24 11.56 14.4 17.22 Russian 24.49 21.85 25.23 28.6 22.9 28.6 Chinese 14.79 13.32 15.70 15.92 16.18 20.37 Table 6: BLEU scores for all systems on TED dev data. 4.1.3 Neural MT Systems The neural systems were built using Sockeye6 (Hieber et al., 2017). The systems used two LSTM layers in both encoder and decoder with hidden size 512 and word embeddings dimension 512. We used a batch size of 4096 and created a checkpoint every 4000 mini-batches. Our systems employed the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.0003. As with the SMT, we built three models for each language: Sockeye General, Sockeye In-Domain, and Sockeye Domain-Adapted. The Sockeye General and In-Domain models were trained with the same data as the corresponding SMT models. The Sockeye Domain-Adapted models were trained using continued training"
W19-6623,D18-1330,0,0.0228862,"Missing"
W19-6623,W17-3204,0,0.284986,"eference translations (Hassan Awadalla et al., 2018). Part of this success comes from the impressive improvements in fluency of NMT output compared to previous MT paradigms (Bentivogli et al., 2016; Toral and Sánchez-Cartagena, 2017; Koehn and Knowles, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 2017). However, NMT has also been shown to sometimes produce output that is low adequacy and even unrelated to the input–particularly when not trained on sufficient in-domain data (Koehn and Knowles, 2017). Because of NMT’s uncanny ability to produce fluent output, these translations may not just be inadequate but fluently inadequate. The fluency of fluently inadequate translations may mislead users into trusting the content based on fluency alone–particularly in the context of other fluent and adequate translations (Martindale and Carpuat, 2018). Mitigating the effects of fluently inadequate translations first requires understanding the scale of the problem and what situations are likely to generate these errors. The general success and high system level quality of NMT suggests that fluently i"
W19-6623,W18-1803,1,0.861989,"ivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 2017). However, NMT has also been shown to sometimes produce output that is low adequacy and even unrelated to the input–particularly when not trained on sufficient in-domain data (Koehn and Knowles, 2017). Because of NMT’s uncanny ability to produce fluent output, these translations may not just be inadequate but fluently inadequate. The fluency of fluently inadequate translations may mislead users into trusting the content based on fluency alone–particularly in the context of other fluent and adequate translations (Martindale and Carpuat, 2018). Mitigating the effects of fluently inadequate translations first requires understanding the scale of the problem and what situations are likely to generate these errors. The general success and high system level quality of NMT suggests that fluently inadequate translations are rare, but we cannot say how rare without a means of automatically identifying potentially fluently inadequate translations in large collections of MT output. In this work, we propose a method to automatically detect fluently inadequate translations based on the underlying characteristics of fluency and adequacy. We vie"
W19-6623,P02-1040,0,0.103547,"pping sentences through Google Translate and the other was generated by extracting example sentences from a syntax textbook. The MT-generated English data is most similar to our problem, and the most effective models for that data were the word-based scores from the language model. 6 Figure 4: Percent fluently inadequate at each checkpoint during in-domain training 5 Related Work MT quality metrics are judged based on their correlation with human judgments, and recently that has meant human adequacy judgments (Bojar et al., 2017). This indicates that any of the common MT metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) may also serve as baseline adequacy scores. However, they incorporate elements of fluency while we wish to separate fluency and adequacy. Adequacy is, essentially, semantic equivalence and the goal of SemEval’s Semantic Textual Similarity (STS) task is to measure the degree of semantic equivalence between two sentences (Cer et al., 2017). The cross-lingual version of the task is similar enough to quality estimation that one of the data sets for 2017 actually came from the WMT Proceedings of MT Summit XVII, volume 1 Conclusion We have introduced an approach"
W19-6623,P16-1162,0,0.0748429,"of neural and phrase-based statistical MT models built from the same general domain data and adapted to translate a more specific domain, namely, transcripts of TED talks. We selected six languages to cover a range of resource availability scenarios and language families: Arabic, Chinese, Farsi, German, Korean and Russian. 4.1.1 Data The number of segments of training and test data for each language is summarized in Table 5. The same tokenization was performed for all systems for a given language, and the tokenized data was split into subwords for NMT training using byte pair encoding (BPE) (Sennrich et al., 2016). The BPE models were trained separately on the source and target language with 30K BPE symbols. All languages used data from the OpenSubtitles1 corpus (Tiedemann, 2009) in the General domain training and dev data sets. The Chinese, German, and Russian models used additional parallel 1 http://www.opensubtitles.org/ Proceedings of MT Summit XVII, volume 1 corpora from WMT172 (Bojar et al., 2017). For the Arabic models, we added data from the Linguistic Data Consortium (LDC)3 and the UN v1 corpus4 (Ziemski et al., 2016). The domain for the In-Domain and DomainAdapted models was TED talks. Traini"
W19-6623,E17-1100,0,0.0184785,"eural models are consistently more prone to this type of error than traditional statistical models. However, improving the overall quality of the MT system such as through domain adaptation reduces these errors. 1 Introduction Recent work has shown that well-trained, indomain neural machine translation (NMT) systems can produce translations that, at the sentence level, are rated on par with human reference translations (Hassan Awadalla et al., 2018). Part of this success comes from the impressive improvements in fluency of NMT output compared to previous MT paradigms (Bentivogli et al., 2016; Toral and Sánchez-Cartagena, 2017; Koehn and Knowles, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 2017). However, NMT has also been shown to sometimes produce output that is low adequacy and even unrelated to the input–particularly when not trained on sufficient in-domain data (Koehn and Knowles, 2017). Because of NMT’s uncanny ability to produce fluent output, these translations may not just be inadequate but fluently inadequate. The fluency of fluently inadequate translations may mislead users into trust"
W19-6623,L16-1561,0,0.0150244,"ta was split into subwords for NMT training using byte pair encoding (BPE) (Sennrich et al., 2016). The BPE models were trained separately on the source and target language with 30K BPE symbols. All languages used data from the OpenSubtitles1 corpus (Tiedemann, 2009) in the General domain training and dev data sets. The Chinese, German, and Russian models used additional parallel 1 http://www.opensubtitles.org/ Proceedings of MT Summit XVII, volume 1 corpora from WMT172 (Bojar et al., 2017). For the Arabic models, we added data from the Linguistic Data Consortium (LDC)3 and the UN v1 corpus4 (Ziemski et al., 2016). The domain for the In-Domain and DomainAdapted models was TED talks. Training, dev, and test sets for the domain were from the Multi-target TED Talks Task (MTTT) corpus (Duh, 2018). All systems, regardless of training setting, were tested on the TED domain test set. Fluency scores for each system were generated based on a language model built on the English side of its primary training data. As noted in Section 2.1, it is important that the language model match the training data, and we expect this to be particularly true when the test set is out-of-domain. We therefore use only the General"
W19-6624,P17-1080,0,0.125148,"ifferent morphologically rich languages. Furthermore, tuning deep character-level models is expensive, even for low-resource settings.1 A middle-ground alternative is character-aware word-level modeling. Here, the NMT system operates over words but uses word embeddings that are sensitive to spellings and thereby has the ability to learn morphological patterns in the language. Such character-aware approaches have been applied successfully in NMT to the source-side word embedding layer (Costa-juss`a and Fonollosa, 2016), but surprisingly, similar gains have not been achieved on the target side (Belinkov et al., 2017). While source-side character-aware models only need to make the source embedding layer character-aware, on the target-side we require both the target embedding layer and the softmax layer 2 1 The dropout rate was found to be critical in Cherry et al. (2018), and each tuning run takes much longer due to longer sequence lengths. 2 Also referred to as generator, final output layer or final linear Dublin, Aug. 19-23, 2019 |p. 244 to be character-aware, which presents additional challenges. We find that the trivial application of methods from Costa-juss`a and Fonollosa (2016) to these target-side"
W19-6624,W16-4117,0,0.012734,"with Noam optimization and 100 warmup steps (Gehring et al., 2017). As Table 3 shows, our CG model with 30k BPE compares favorably to even deep characterlevel models for this low-resource setting. 6 Analysis We are interested in understanding whether our character-aware model is exploiting morphological patterns in the target language. We investigate this by inspecting the relationship between a set of hand-picked features and improvements obtained by our model over the baseline at wordlevel inputs. These features fall into two categories, corpus-dependent and corpus-independent. We following Bentz et al. (2016), and extract features known to correlate with human judgments of morphological complexity. The following corpusdependent features were used: 7 Increasing the recurrent size for deep models resulted in significant drop in BLEU scores. We set the dropout rate to 0.1. Dublin, Aug. 19-23, 2019 |p. 249 (i) Type-Token Ratio (TT): the ratio of the number of word types to the total number of word tokens in the target side. We note that a large corpus tends to have a smaller type-token ratio compared to small corpus. (ii) Word-Alignment Score (A): computed as A = |many-to-one|−|one-to-many| . One-to|a"
W19-6624,D18-1461,0,0.20156,"XVII, volume 1 tice to mitigate the vocabulary size problem with Byte-Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016). BPE iteratively merges consecutive characters into larger chunks based on their frequency, which results in the breaking up of less common words into “subword units.” While BPE addresses the vocabulary size problem, the spellings of the subword units are still ignored. On the other hand, purely character-level NMT translates one character at a time and can implicitly learn about morphological patterns within words as well as generalize to unseen vocabulary. Recently, Cherry et al. (2018) show that very deep character-level models can outperform BPE, however, the smallest data size evaluated was 2 million sentences, so it is unclear if the results hold for low-resource settings and when translating into a range of different morphologically rich languages. Furthermore, tuning deep character-level models is expensive, even for low-resource settings.1 A middle-ground alternative is character-aware word-level modeling. Here, the NMT system operates over words but uses word embeddings that are sensitive to spellings and thereby has the ability to learn morphological patterns in the"
W19-6624,P16-1160,0,0.0161513,"ard word representations, and found no improvements. Our work is also aligned with the characteraware models proposed in (Kim et al., 2016), but projection. Proceedings of MT Summit XVII, volume 1 we additionally employ a gating mechanism between character-aware representations and standard word representations similar to language modeling work by (Miyamoto and Cho, 2016). However, our gating is a learned type-specific vector rather than a fixed hyperparameter. There is additionally a line of work on purely character-level NMT, which generates words one character at a time (Ling et al., 2015; Chung et al., 2016; Passban et al., 2018). While initial results here were not strong, Cherry et al. (2018) revisit this with deeper architectures and sweeping dropout parameters and find that they outperform BPE across settings of the merge hyperparameter. They examine different data sizes and observe improvements in the smaller data size settings— however, the smallest size is about 2 million sentence pairs. In contrast, we look at a smaller order of magnitude data size and present an alternate approach which doesn’t require substantial tuning of parameters across different languages. Finally, Byte-Pair Encod"
W19-6624,P16-2058,0,0.0466523,"Missing"
W19-6624,P07-1033,0,0.352858,"Missing"
W19-6624,N13-1073,0,0.0363299,"ge list of inflected words (in several languages) along with the word’s lemma and a set of morphological tags. For example, the French UniMorph corpus contains the word marchai (walked), which is associated with its lemma, marcher and a set of morphological tags {V,IND,PST,1,SG,PFV}. There are 19 such tags in the French UniMorph corpus. A morphologically richer language like Hungarian, for example, has 36 distinct tags. We used the number of distinct tags (UT) and the number of different tag combinations (UTC) that appear in the UniMorph corpus for each language. Note that 8 We use FastAlign (Dyer et al., 2013) for word alignments with the grow-diag-final-and heuristic from (Och and Ney, 2003) for symmetrization. Proceedings of MT Summit XVII, volume 1 we do not filter out words (and its associated tags) from the UniMorph corpus that are absent in our parallel data. This ensures that the UT and UTC features are completely corpus independent. The Pearson’s correlation between these handpicked features and relative gain observed by our model is shown in Table 4. For this analysis we used the relative gain obtained from the wordlevel experiments. Concretely, the relative gain for Czech was computed as"
W19-6624,P15-1001,0,0.033363,"dings and gated embeddings. We used en-de language pair from the TED multi-target dataset. Std. is our baseline with standard word embeddings, model C is the composition only model and CG combines the characteraware (composed) embedding and standard embedding via a gating function. spellings (character embeddings) of entire target vocabulary, placing a limitation on the target vocabulary size for our model. Which is problematic for word-level modeling (without BPE). To make our character-aware model accommodate large target vocabulary sizes, we incorporate an approximation mechanism based on (Jean et al., 2015). Instead of computing the softmax over the entire vocabulary, we uniformly sample 20k vocabulary types and the vocabulary types that are present in the training batch. During decoding, we compute the forward pass Wo sj in Equation 2 in several splits of the target vocabulary. As no backward pass is required we clear the memory (i.e. delete the computation graph) after each split is computed. 5 Experiments We evaluate our character aware model on 14 different languages in a low-resource setting. Additionally, we sweep over several BPE merge hyperparameter settings from character-level to fully"
W19-6624,P17-4012,0,0.0190775,"krainian to around 174k sentences pairs for Russian (provided in Appendix A), but the validation and test sets are “multi-way parallel”, meaning the English sentences (the source side in our experiments) are the same across all 14 languages, and are about 2k sentences each. We filter out training pairs where the source sentence was longer that 50 tokens (before applying BPE). For word-level results, we used a vocabulary size of 100k (keeping the most frequent types) and replaced rare words by an &lt;UNK&gt; token. Lang uk cs de bg tr pl ru ro pt hu fr fa ar he 5.2 NMT Setup We work with OpenNMT-py (Klein et al., 2017), and modify the target-side embedding layer and softmax layer to use our proposed character-aware composition function. A 2 layer encoder and decoder, with 1000 recurrent units were used in all experiments The embeddings sizes were made to match the RNN recurrent size. We set the character embedding size to 50 and use four CNNs with kernel widths 3, 4, 5 and 6. The four CNN outputs are concatenated into a compositional embeddings and gated with a standard word embedding. The same composition function (with shared parameters) was used for the target embedding layer and the softmax layer. We op"
W19-6624,D15-1166,0,0.36944,"ern present in the language, for example the token politely in our dataset is split into pol+itely, instead of the linguistically plausible split polite+ly.3 Our approach can be applied to word-level sequences and sequences at any BPE merge hyperparameter greater than 0. Increasing the hyperparameter results in more words and longer subwords that can exhibit morphological patterns. Our goal is to exploit these morphological patterns and enrich the word (or subword) representations with characterawareness. 3 Encoder-Decoder NMT An attention-based encoder-decoder network (Bahdanau et al., 2015; Luong et al., 2015) models the probability of a target sentence y of length J given a source sentence x as: p(y |x) = J Y j=1 p(yj |y0:j−1 , x; θ) (1) where θ represents all the parameters of the network. At each time-step the j 0 th output token is 3 We observe this split when merge parameter was 15k. Dublin, Aug. 19-23, 2019 |p. 245 (2) where sj ∈ RD×1 is the decoder hidden state at time j and Wo ∈ R|V|×D is the weight matrix of the softmax layer, which provides a continuous representation for target words. sj is computed using the following recurrence: sj = tanh(Wc [cj ; ˜ sj ]) ˜ sj = f ([sj−1 ; ws yj−1 ;˜ s"
W19-6624,P18-1118,0,0.0415482,"Missing"
W19-6624,D16-1209,0,0.0426477,"Missing"
W19-6624,J03-1002,0,0.0232058,"set of morphological tags. For example, the French UniMorph corpus contains the word marchai (walked), which is associated with its lemma, marcher and a set of morphological tags {V,IND,PST,1,SG,PFV}. There are 19 such tags in the French UniMorph corpus. A morphologically richer language like Hungarian, for example, has 36 distinct tags. We used the number of distinct tags (UT) and the number of different tag combinations (UTC) that appear in the UniMorph corpus for each language. Note that 8 We use FastAlign (Dyer et al., 2013) for word alignments with the grow-diag-final-and heuristic from (Och and Ney, 2003) for symmetrization. Proceedings of MT Summit XVII, volume 1 we do not filter out words (and its associated tags) from the UniMorph corpus that are absent in our parallel data. This ensures that the UT and UTC features are completely corpus independent. The Pearson’s correlation between these handpicked features and relative gain observed by our model is shown in Table 4. For this analysis we used the relative gain obtained from the wordlevel experiments. Concretely, the relative gain for Czech was computed as 21.49−18.44 We see a 18.44 strong correlation between the corpus-independent feature"
W19-6624,N18-1006,0,0.0164014,"ions, and found no improvements. Our work is also aligned with the characteraware models proposed in (Kim et al., 2016), but projection. Proceedings of MT Summit XVII, volume 1 we additionally employ a gating mechanism between character-aware representations and standard word representations similar to language modeling work by (Miyamoto and Cho, 2016). However, our gating is a learned type-specific vector rather than a fixed hyperparameter. There is additionally a line of work on purely character-level NMT, which generates words one character at a time (Ling et al., 2015; Chung et al., 2016; Passban et al., 2018). While initial results here were not strong, Cherry et al. (2018) revisit this with deeper architectures and sweeping dropout parameters and find that they outperform BPE across settings of the merge hyperparameter. They examine different data sizes and observe improvements in the smaller data size settings— however, the smallest size is about 2 million sentence pairs. In contrast, we look at a smaller order of magnitude data size and present an alternate approach which doesn’t require substantial tuning of parameters across different languages. Finally, Byte-Pair Encoding (BPE) (Sennrich et"
W19-6624,P16-1162,0,0.336552,"attention-based encoder-decoder neural machine translation (NMT) models learn wordlevel embeddings, with a continuous representation for each unique word type (Bahdanau et al., 2015). However, this results in a long tail of rare words for which we do not learn good representations. More recently, it has become standard pracc 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. ∗ Equal Contribution Proceedings of MT Summit XVII, volume 1 tice to mitigate the vocabulary size problem with Byte-Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016). BPE iteratively merges consecutive characters into larger chunks based on their frequency, which results in the breaking up of less common words into “subword units.” While BPE addresses the vocabulary size problem, the spellings of the subword units are still ignored. On the other hand, purely character-level NMT translates one character at a time and can implicitly learn about morphological patterns within words as well as generalize to unseen vocabulary. Recently, Cherry et al. (2018) show that very deep character-level models can outperform BPE, however, the smallest data size evaluated"
Y16-2004,W05-0909,0,0.184492,"Missing"
Y16-2004,H05-1025,0,0.0463682,". For evaluation, we compare our rearranged word sequences to conventional n-gram word sequences. Both intrinsic and extrinsic experiments verify that our framework can achieve better performance, proving that our method can be considered as a better alternative for ngram language models. 1 Introduction Probabilistic Language Modeling is a fundamental research direction of Natural Language Processing. It is widely used in various application such as machine translation (Brown et al., 1990), spelling correction (Mays et al., 1990), speech recognition (Rabiner and Juang, 1993), word prediction (Bickel et al., 2005) and so on. Most research about Probabilistic Language Modeling, such as Katz back-off (Katz, 1987), KneserNey (Kneser and Ney, 1995), and modiﬁed KneserNey (Chen and Goodman, 1999), only focus on smoothing methods because they all take the n-gram approach (Shannon, 1948) as a default setting for modeling word sequences in a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot be distinguished accurately even using a high-performance smoothing method such as modiﬁed Kneser-Ney (abbreviated as MKN). An"
Y16-2004,N03-2002,0,0.0867063,"ncy because the connection of ‘as...as’ is still interrupted. On the contrary, the HWS model extracts sequences in a discontinuous way, even ‘soon’ is replaced by another word, the expression ‘as...as’ won’t be affected. This is how the HWS models relieve the data sparseness problem. The HWS model is essentially an n-gram language model based on a different assumption that a word depends upon its nearby high-frequency words instead of its preceding words. Different from other special n-gram language models, such as class-based language model (Brown et al., 1992), factored language model(FLM) (Bilmes and Kirchhoff, 2003), HWS language model doesn’t use any speciﬁc linguistic knowledge or any abstracted categories. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which make it more effective and ﬂexible in relieving the data spar"
Y16-2004,J90-2002,0,0.73567,"dopted to rearrange word sequences in a totally unsupervised fashion, which greatly increases the expandability of HWS models. For evaluation, we compare our rearranged word sequences to conventional n-gram word sequences. Both intrinsic and extrinsic experiments verify that our framework can achieve better performance, proving that our method can be considered as a better alternative for ngram language models. 1 Introduction Probabilistic Language Modeling is a fundamental research direction of Natural Language Processing. It is widely used in various application such as machine translation (Brown et al., 1990), spelling correction (Mays et al., 1990), speech recognition (Rabiner and Juang, 1993), word prediction (Bickel et al., 2005) and so on. Most research about Probabilistic Language Modeling, such as Katz back-off (Katz, 1987), KneserNey (Kneser and Ney, 1995), and modiﬁed KneserNey (Chen and Goodman, 1999), only focus on smoothing methods because they all take the n-gram approach (Shannon, 1948) as a default setting for modeling word sequences in a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot"
Y16-2004,J92-4003,0,0.632591,"of ‘soon’ and its nearby words tend to be lowfrequency because the connection of ‘as...as’ is still interrupted. On the contrary, the HWS model extracts sequences in a discontinuous way, even ‘soon’ is replaced by another word, the expression ‘as...as’ won’t be affected. This is how the HWS models relieve the data sparseness problem. The HWS model is essentially an n-gram language model based on a different assumption that a word depends upon its nearby high-frequency words instead of its preceding words. Different from other special n-gram language models, such as class-based language model (Brown et al., 1992), factored language model(FLM) (Bilmes and Kirchhoff, 2003), HWS language model doesn’t use any speciﬁc linguistic knowledge or any abstracted categories. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which ma"
Y16-2004,P97-1064,0,0.145621,"ries. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which make it more effective and ﬂexible in relieving the data sparseness problem. On this point, it has something in common with structured language model (Chelba, 1997), which ﬁrstly introduced parsing into language modeling. The signiﬁcant difference is, structured language model is based on CFG parsing structures, while HWS model is based on patternoriented structures. 3 Generalized Hierarchical Word Sequence Structure Suppose we are given a sentence s = w1 , w2 , ..., wn   and a permutation function f : s → s , where s =    w1 , w2 , ..., wn is a permutation of s. For each word index i(1 ≤ i ≤ n, wi ∈ s), there is a corresponding    reordered index j(1 ≤ j ≤ n, wj ∈ s , wj = wi ). Then we create an n × n matrix A. For each row j, we ﬁll cell Aj,i"
Y16-2004,P12-1023,0,0.0232078,"...as’ won’t be affected. This is how the HWS models relieve the data sparseness problem. The HWS model is essentially an n-gram language model based on a different assumption that a word depends upon its nearby high-frequency words instead of its preceding words. Different from other special n-gram language models, such as class-based language model (Brown et al., 1992), factored language model(FLM) (Bilmes and Kirchhoff, 2003), HWS language model doesn’t use any speciﬁc linguistic knowledge or any abstracted categories. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which make it more effective and ﬂexible in relieving the data sparseness problem. On this point, it has something in common with structured language model (Chelba, 1997), which ﬁrstly introduced parsing into language modeling. The signiﬁcant difference"
Y16-2004,H93-1016,0,0.46691,"setting for modeling word sequences in a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot be distinguished accurately even using a high-performance smoothing method such as modiﬁed Kneser-Ney (abbreviated as MKN). An alternative solution is to factor the language model probabilities such that the number of unseen sequences are reduced. It is necessary to extract them in another way, instead of only using the information of left-to-right continuous word order. In (Guthrie et al., 2006), skip-gram (Huang et al., 1993)1 is proposed to overcome the data sparseness problem. For each n-gram word sequence, the skip-gram model enumerates all possible word combinations to increase valid sequences. This has truly helped to decrease the unseen sequences, but we should not neglect the fact that it also brings a greatly increase of processing time and redundant contexts. In (Wu and Matsumoto, 2014), a heuristic approach is proposed to convert any raw sentence into a hierarchical word sequence (abbreviated as 1 The k-skip-n-grams for a sentence w1 , ...wm is deﬁned as the set {wi1 , wi2 , ...win |Σn j=1 ij − ij−1 &lt; k}"
Y16-2004,P02-1040,0,0.0967413,"set English as the target language. As for statistical machine translation toolkit, we use Moses system15 to train the translation model and output 50-best translation candidates for each French sentence of the test data. Then we use 139,761 English sentences to train language models. With these models, 50-best translation candidates are reranked. According to these reranking results, the performance of machine translation system is evaluated, which also means, the language models are evaluated indirectly. In this paper, we use the following measures for evaluating reranking results16 . BLEU (Papineni et al., 2002): BLEU score measures how many words overlap in a given candidate translation when compared to a reference translation, which provides some insight into how good the ﬂuency of the output from an engine will be. METEOR (Banerjee and Lavie, 2005): METEOR score computes a one-to-one alignment between matching words in a candidate translation and a reference. TER (Snover et al., 2006): TER score measures the number of edits required to change a system output into one of the references, which gives an indication as to how much post-editing will be required 15 http://www.statmt.org/moses/ We use ope"
Y16-2004,P08-1066,0,0.0312779,", the expression ‘as...as’ won’t be affected. This is how the HWS models relieve the data sparseness problem. The HWS model is essentially an n-gram language model based on a different assumption that a word depends upon its nearby high-frequency words instead of its preceding words. Different from other special n-gram language models, such as class-based language model (Brown et al., 1992), factored language model(FLM) (Bilmes and Kirchhoff, 2003), HWS language model doesn’t use any speciﬁc linguistic knowledge or any abstracted categories. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which make it more effective and ﬂexible in relieving the data sparseness problem. On this point, it has something in common with structured language model (Chelba, 1997), which ﬁrstly introduced parsing into language modeling. The s"
Y16-2004,2006.amta-papers.25,0,0.0450181,"Missing"
Y16-2004,Y14-1056,1,0.769298,"bilities such that the number of unseen sequences are reduced. It is necessary to extract them in another way, instead of only using the information of left-to-right continuous word order. In (Guthrie et al., 2006), skip-gram (Huang et al., 1993)1 is proposed to overcome the data sparseness problem. For each n-gram word sequence, the skip-gram model enumerates all possible word combinations to increase valid sequences. This has truly helped to decrease the unseen sequences, but we should not neglect the fact that it also brings a greatly increase of processing time and redundant contexts. In (Wu and Matsumoto, 2014), a heuristic approach is proposed to convert any raw sentence into a hierarchical word sequence (abbreviated as 1 The k-skip-n-grams for a sentence w1 , ...wm is deﬁned as the set {wi1 , wi2 , ...win |Σn j=1 ij − ij−1 &lt; k}. 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 69 HWS) structure, by which much more valid word sequences can be modeled while remaining the model size as small as that of n-gram. In (Wu and Matsumoto, 2015) (Wu et al., 2015), instead of only using the information of word frequency, the inform"
Y16-2004,Y15-1051,1,0.780001,"that it also brings a greatly increase of processing time and redundant contexts. In (Wu and Matsumoto, 2014), a heuristic approach is proposed to convert any raw sentence into a hierarchical word sequence (abbreviated as 1 The k-skip-n-grams for a sentence w1 , ...wm is deﬁned as the set {wi1 , wi2 , ...win |Σn j=1 ij − ij−1 &lt; k}. 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 69 HWS) structure, by which much more valid word sequences can be modeled while remaining the model size as small as that of n-gram. In (Wu and Matsumoto, 2015) (Wu et al., 2015), instead of only using the information of word frequency, the information of direction and word association are also used to construct higher quality HWS structures. However, they are all speciﬁc methods based on certain heuristic assumptions. For the purpose of further improvements, it is also necessary to generalize those models into one uniﬁed structure. This paper is organized as follows. In Section 2, we review the HWS language model. Then we present a generalized hierarchical word sequence structure (GHWSS) in Section 3. In Section 4, we present two strategies for rear"
