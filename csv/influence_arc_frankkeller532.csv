2020.acl-main.161,P13-1035,0,0.086575,"Missing"
2020.acl-main.161,P14-1035,0,0.0234123,"enser (Cheong and Young, 2014), two planning-based story generators that use the theory of Gerrig and Bernardo (1994) that suspense is created when a protagonist faces obstacles that reduce successful outcomes. Our approach, in contrast, models suspense using general language models fine-tuned on stories, without planning and domain knowledge. The advantage is that the model can be trained on large volumes of available narrative text without requiring expensive annotations, making it more generalisable. Other work emphasises the role of characters and their development in story understanding (Bamman et al., 2014, 2013; Chaturvedi et al., 2017; Iyyer et al., 2016) or summarisation (Gorinski and Lapata, 2018). A further important element of narrative structure is plot, i.e., the sequence of events in which characters interact. Neural models have explicitly modelled events (Martin et al., 2018; Harrison et al., 2017; Rashkin et al., 2018) or the results of actions (Roemmele and Gordon, 2018; Liu et al., 2018a,b). On the other hand, some neural generation models (Fan et al., 2018) just use a hierarchical model on top of a language model; our architecture follows this approach. 3 3.1 Models of Suspense De"
2020.acl-main.161,P17-2097,0,0.0214945,"word enc, sent enc, and story enc. 4 Model 4.1 Architecture Our overall approach leverages contextualised language models, which are a powerful tool in NLP when pretrained on large amounts of text and fine tuned on a specific task (Peters et al., 2018; Devlin et al., 2019). Specifically, we use Generative Pre-Training (GPT, Radford et al., 2018), a model which has proved successful in generation tasks (Radford et al., 2019; See et al., 2019). Hierarchical Model Previous work found that hierarchical models show strong performance in story generation (Fan et al., 2018) and understanding tasks (Cai et al., 2017). The language model and hierarchical encoders we use are unidirectional, which matches the incremental way in which human readers process stories when they experience suspense. Figure 1 depicts the architecture of our hierar1 chical model. It builds a chain of representations that anticipates what will come next in a story, allowing us to infer measures of suspense. For a given sentence, we use GPT as our word encoder (word enc in Figure 1) which turns each word in a sentence into a word embedding wi . Then, we use an RNN (sent enc) to turn the word embeddings of the sentences into a sentence"
2020.acl-main.161,D19-1180,1,0.803364,"inference scheme that uses these representations to compute both surprise and uncertainty reduction. For evaluation, we use the WritingPrompt corpus of short stories (Fan et al., 2018), part of which we annotate with human sentence-by-sentence judgements of suspense. We find that surprise over representations and over probability distributions both predict suspense judgements. However uncertainty reduction over representations is better, resulting in near human-level accuracy. We also show that our models can be used to predict turning points, i.e., major narrative events, in movie synopses (Papalampidi et al., 2019). 2 Related Work In narratology, uncertainty over outcomes is traditionally seen as suspenseful (e.g., O’Neill, 2013; Zillmann, 1996; Abbott, 2008). Other authors claim that suspense can exist without uncertainty (e.g., Smuts, 2008; Hoeken and van Vliet, 2000; Gerrig, 1989) and that readers feel suspense even when they read a story for the second time (Delatorre et al., 2018), which is unexpected if suspense is uncertainty; this is referred to as the paradox of suspense (Prieto-Pablos, 1998; Yanal, 1996). Considering Romeo and Juliet again, in the first view suspense is motivated by primarily"
2020.acl-main.161,D14-1162,0,0.0849602,"Missing"
2020.acl-main.161,N18-1202,0,0.0166677,"will also experiment with uncertainty reduction computed using longer rollouts. 1765 ℓ lm ℓ ⋅ fusion (afﬁne) story_enc (RNN) Concat word and story vectors +1 0 1 0 ( ) = [ ; ( )] +3 3 2 ( ) 3 +2 ( ) ( ) sent_enc (RNN) 0 1 2 3 0 word_enc (GPT) 0 1 Once upon 2 3 a time 1 2 3 Figure 1: Architecture of our hierarchical model. See text for explanation of the components word enc, sent enc, and story enc. 4 Model 4.1 Architecture Our overall approach leverages contextualised language models, which are a powerful tool in NLP when pretrained on large amounts of text and fine tuned on a specific task (Peters et al., 2018; Devlin et al., 2019). Specifically, we use Generative Pre-Training (GPT, Radford et al., 2018), a model which has proved successful in generation tasks (Radford et al., 2019; See et al., 2019). Hierarchical Model Previous work found that hierarchical models show strong performance in story generation (Fan et al., 2018) and understanding tasks (Cai et al., 2017). The language model and hierarchical encoders we use are unidirectional, which matches the incremental way in which human readers process stories when they experience suspense. Figure 1 depicts the architecture of our hierar1 chical m"
2020.acl-main.161,P18-1043,0,0.0272394,"domain knowledge. The advantage is that the model can be trained on large volumes of available narrative text without requiring expensive annotations, making it more generalisable. Other work emphasises the role of characters and their development in story understanding (Bamman et al., 2014, 2013; Chaturvedi et al., 2017; Iyyer et al., 2016) or summarisation (Gorinski and Lapata, 2018). A further important element of narrative structure is plot, i.e., the sequence of events in which characters interact. Neural models have explicitly modelled events (Martin et al., 2018; Harrison et al., 2017; Rashkin et al., 2018) or the results of actions (Roemmele and Gordon, 2018; Liu et al., 2018a,b). On the other hand, some neural generation models (Fan et al., 2018) just use a hierarchical model on top of a language model; our architecture follows this approach. 3 3.1 Models of Suspense Definitions In order to formalise measures of suspense, we assume that a story consists of a sequence of sentences. These sentences are processed one by one, and the sentence at the current timepoint t is represented by an embedding et (see Section 4 for how embeddings are computed). Each embedding is associated with a probability"
2020.acl-main.161,D09-1034,0,0.0318172,"that a story consists of a sequence of sentences. These sentences are processed one by one, and the sentence at the current timepoint t is represented by an embedding et (see Section 4 for how embeddings are computed). Each embedding is associated with a probability P(et ). Continuations of the story are represented by a set of possible next i sentences, whose embeddings are denoted by et+1 . The first measure of suspense we consider is surprise (Hale, 2001), which in the psycholinguistic literature has been successfully used to predict word-based processing effort (Demberg and Keller, 2008; Roark et al., 2009; Van Schijndel and Linzen, 2018a,b). Surprise is a backward-looking predictor: it measures how unexpected the current word is given the words that preceded it (i.e., the left context). Hale formalises surprise as the negative log of the conditional probability of the current word. For stories, we compute surprise over sentences. As our sentence embeddings et include information about the left context e1 , . . . , et−1 , we can write Hale surprise as: Hale St = − log P(et ) (1) An alternative measure for predicting word-byword processing effort used in psycholinguistics is entropy reduction (H"
2020.acl-main.161,W18-1506,0,0.0208222,"can be trained on large volumes of available narrative text without requiring expensive annotations, making it more generalisable. Other work emphasises the role of characters and their development in story understanding (Bamman et al., 2014, 2013; Chaturvedi et al., 2017; Iyyer et al., 2016) or summarisation (Gorinski and Lapata, 2018). A further important element of narrative structure is plot, i.e., the sequence of events in which characters interact. Neural models have explicitly modelled events (Martin et al., 2018; Harrison et al., 2017; Rashkin et al., 2018) or the results of actions (Roemmele and Gordon, 2018; Liu et al., 2018a,b). On the other hand, some neural generation models (Fan et al., 2018) just use a hierarchical model on top of a language model; our architecture follows this approach. 3 3.1 Models of Suspense Definitions In order to formalise measures of suspense, we assume that a story consists of a sequence of sentences. These sentences are processed one by one, and the sentence at the current timepoint t is represented by an embedding et (see Section 4 for how embeddings are computed). Each embedding is associated with a probability P(et ). Continuations of the story are represented b"
2020.acl-main.161,K19-1079,0,0.0322492,"( ) ( ) sent_enc (RNN) 0 1 2 3 0 word_enc (GPT) 0 1 Once upon 2 3 a time 1 2 3 Figure 1: Architecture of our hierarchical model. See text for explanation of the components word enc, sent enc, and story enc. 4 Model 4.1 Architecture Our overall approach leverages contextualised language models, which are a powerful tool in NLP when pretrained on large amounts of text and fine tuned on a specific task (Peters et al., 2018; Devlin et al., 2019). Specifically, we use Generative Pre-Training (GPT, Radford et al., 2018), a model which has proved successful in generation tasks (Radford et al., 2019; See et al., 2019). Hierarchical Model Previous work found that hierarchical models show strong performance in story generation (Fan et al., 2018) and understanding tasks (Cai et al., 2017). The language model and hierarchical encoders we use are unidirectional, which matches the incremental way in which human readers process stories when they experience suspense. Figure 1 depicts the architecture of our hierar1 chical model. It builds a chain of representations that anticipates what will come next in a story, allowing us to infer measures of suspense. For a given sentence, we use GPT as our word encoder (word"
2020.acl-main.161,P18-2119,0,0.0397025,"Missing"
2020.acl-main.161,N01-1021,0,\N,Missing
2020.acl-main.161,N16-1180,0,\N,Missing
2020.acl-main.161,N16-1098,0,\N,Missing
2020.acl-main.161,N18-1160,0,\N,Missing
2020.acl-main.161,P19-1254,0,\N,Missing
2020.acl-main.161,N19-1423,0,\N,Missing
2020.acl-main.174,W14-0907,0,0.0188617,"2 Related Work A large body of previous work has focused on the computational analysis of narratives (Mani, 2012; Richards et al., 2009). Attempts to analyze how stories are written have been based on sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importance criteria (e.g., whether a segment contains protagonist or location information); they create summaries to help readers decide whether they are interested in reading the whole story, without revealing its plot. Mihalcea and Ceylan (2007) summarize books with an unsupervised graph-based approach operating over segments (i.e., topical units)."
2020.acl-main.174,P13-1035,0,0.0570598,"Missing"
2020.acl-main.174,P14-1035,0,0.0202398,"ysis shows that key events identified in the latent space correlate with important summary content. 2 Related Work A large body of previous work has focused on the computational analysis of narratives (Mani, 2012; Richards et al., 2009). Attempts to analyze how stories are written have been based on sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importance criteria (e.g., whether a segment contains protagonist or location information); they create summaries to help readers decide whether they are interested in reading the whole story, without revealing its plot. Mihalcea and Ceylan (2007) summ"
2020.acl-main.174,D18-2029,0,0.0268876,"roduced in Zheng and Lapata (2019) takes directed edges into account, capturing the intuition that the centrality of any two nodes is influenced by their relative position. Also note that the edges of preceding and following scenes are differentially weighted by λ1 and λ2 . Although earlier implementations of T EXT R ANK (Mihalcea and Tarau, 2004) compute node similarity based on symbolic representations such as tf*idf, we adopt a neural approach. Specifically, we obtain sentence representations based on a pretrained encoder. In our experiments, we rely on the Universal Sentence Encoder (USE; Cer et al. 2018), however, other embeddings are possible.1 We represent a scene by the mean of its sentence representations and measure scene similarity ei j using cosine.2 As in the original T EXT R ANK algorithm (Mihalcea and Tarau, 2004), scenes are ranked based on their centrality and the M most central ones are selected to appear in the summary. 3.2 Supervised Screenplay Summarization Most extractive models frame summarization as a classification problem. Following a recent approach (S UMMA RU NN ER; Nallapati et al. 2017), we use a neural network-based encoder to build representations for scenes and app"
2020.acl-main.174,P09-1068,0,0.0461756,"ervised summarization algorithms; (b) we provide a new layer of annotations for the CSI corpus, which can be used for research in long-form summarization; and (c) we demonstrate that narrative structure can facilitate screenplay summarization; our analysis shows that key events identified in the latent space correlate with important summary content. 2 Related Work A large body of previous work has focused on the computational analysis of narratives (Mani, 2012; Richards et al., 2009). Attempts to analyze how stories are written have been based on sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importanc"
2020.acl-main.174,P16-1046,1,0.924798,"rtant aspects of a CSI episode and improve summarization performance over general extractive algorithms, leading to more complete and diverse summaries. 1 Setup Opportunity New Situation Change of Plans Progress Point of no Return Complications Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterward"
2020.acl-main.174,P10-1015,0,0.0685763,"Missing"
2020.acl-main.174,P19-1102,0,0.0428377,"nal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient since important passages can be identified in predictable locations (e.g., by performing a “smart selection” of sentences from the beginning of the document) and the structure itself can be explicitly taken i"
2020.acl-main.174,Q18-1001,1,0.942042,"our method does not involve manually annotating turning points in CSI episodes. Instead, we approximate narrative structure automatically by pretraining on the annotations of the TRIPOD dataset of Papalampidi et al. (2019) and employing a variant of their model. We find that narrative structure representations learned on their dataset (which was created for feature-length films), transfer well across cinematic genres and computational tasks. We propose a framework for end-to-end training in which narrative structure is treated as a latent variable for summarization. We extend the CSI dataset (Frermann et al., 2018) with binary labels indicating whether a scene should be included in the summary and present experiments with both supervised and unsupervised summarization models. An overview of our approach is shown in Figure 2. Our contributions can be summarized as follows: (a) we develop methods for instilling knowledge about narrative structure into generic su1921 pervised and unsupervised summarization algorithms; (b) we provide a new layer of annotations for the CSI corpus, which can be used for research in long-form summarization; and (c) we demonstrate that narrative structure can facilitate screenp"
2020.acl-main.174,N15-1113,1,0.950991,"onding summaries (e.g., by mining IMDb or Wikipedia), the size of such a corpus would be at best in the range of a few hundred examples not hundreds of thousands. Also note that genre differences might render transfer learning (Pan and Yang, 2010) difficult, e.g., a model trained on movie screenplays might not generalize to sitcoms or soap operas. Given the above challenges, we introduce a number of assumptions to make the task feasible. Firstly, our goal is to produce informative summaries, which serve as a surrogate to reading the full script or watching the entire film. Secondly, we follow Gorinski and Lapata (2015) in conceptualizing screenplay summarization as the task of identifying a sequence of informative scenes. Thirdly, we focus on summarizing television programs such as CSI: Crime Scene Investigation (FrIn this work, we adapt general-purpose extractive summarization algorithms (Nallapati et al., 2017; Zheng and Lapata, 2019) to identify informative scenes in screenplays and instill in them knowledge about narrative film structure (Hauge, 2017; Cutting, 2016; Freytag, 1896). Specifically, we adopt a scheme commonly used by screenwriters as a practical guide for producing successful screenplays. A"
2020.acl-main.174,D10-1008,0,0.0770998,"Missing"
2020.acl-main.174,N18-1065,0,0.0230746,"no Return Complications Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient since important passages can be identified in predictable locations (e.g., by performing a “smart selection” of sentences from the beginning of the document) and the"
2020.acl-main.174,J97-1003,0,0.466094,"subsequently project them via distant supervision onto screenplays, thereby creating silver-standard labels. We utilize this silver-standard dataset in order to pretrain a network which performs TP identification. TP Identification Network We first encode screenplay scenes via a BiLSTM equipped with an attention mechanism. We then contextualize them with respect to the whole screenplay via a second BiLSTM. Next, we compute topic-aware scene representations ti via a context interaction layer (CIL) as proposed in Papalampidi et al. (2019). CIL is inspired by traditional segmentation approaches (Hearst, 1997) and measures the semantic similarity of the current scene with a preceding and following context window in the screenplay. Hence, the topic-aware scene representations also encode the degree to which each scene acts as a topic boundary in the screenplay. In the final layer, we employ TP-specific attention mechanisms to compute the probability pi j that scene ti represents the jth TP in the screenplay. Note that we expect the TP-specific attention distributions to be sparse, as there are only a few scenes which are relevant for a TP (recall that TPs are boundary scenes between sections). To en"
2020.acl-main.174,P84-1044,0,0.285232,"Missing"
2020.acl-main.174,J10-1003,0,0.0254828,"sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importance criteria (e.g., whether a segment contains protagonist or location information); they create summaries to help readers decide whether they are interested in reading the whole story, without revealing its plot. Mihalcea and Ceylan (2007) summarize books with an unsupervised graph-based approach operating over segments (i.e., topical units). Their algorithm first generates a summary for each segment and then an overall summary by collecting sentences from the individual segment summaries. Focusing on screenplays, Gorinski and Lapata (2015) generate a"
2020.acl-main.174,P19-1500,1,0.847489,"math Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient since important passages can be identified in predictable locations (e.g., by performing a “smart selection” of sentences from the beginning of the document) and the structure itself can be explicitly taken into account in model de"
2020.acl-main.174,P10-1158,1,0.724258,"rovide a new layer of annotations for the CSI corpus, which can be used for research in long-form summarization; and (c) we demonstrate that narrative structure can facilitate screenplay summarization; our analysis shows that key events identified in the latent space correlate with important summary content. 2 Related Work A large body of previous work has focused on the computational analysis of narratives (Mani, 2012; Richards et al., 2009). Attempts to analyze how stories are written have been based on sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importance criteria (e.g., whether a segment con"
2020.acl-main.174,W04-3252,0,0.0576735,"quence of scenes D = {s1 , s2 , . . . , sn }. Our aim is to select a subset D 0 = {si , . . . , sk } consisting of the most informative scenes (where k < n). Note that this definition produces extractive summaries; we further assume that selected scenes are presented according to their order in the screenplay. We next discuss how summaries can be created using both unsupervised and supervised approaches, and then move on to explain how these are adapted to incorporate narrative structure. 3.1 Unsupervised Screenplay Summarization Our unsupervised model is based on an extension of T EXT R ANK (Mihalcea and Tarau, 2004; Zheng and Lapata, 2019), a well-known algorithm for extractive single-document summarization. In our setting, a screenplay is represented as a graph, in which nodes correspond to scenes and edges between scenes si and s j are weighted by their simi1922 larity ei j . A node’s centrality (importance) is measured by computing its degree: centrality(si ) = λ1 ∑ ei j + λ2 ∑ ei j j<i (1) j&gt;i where λ1 +λ2 = 1. The modification introduced in Zheng and Lapata (2019) takes directed edges into account, capturing the intuition that the centrality of any two nodes is influenced by their relative position"
2020.acl-main.174,K16-1028,0,0.0301533,"pisode and improve summarization performance over general extractive algorithms, leading to more complete and diverse summaries. 1 Setup Opportunity New Situation Change of Plans Progress Point of no Return Complications Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure o"
2020.acl-main.174,D18-1206,1,0.875357,"ns Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient since important passages can be identified in predictable locations (e.g., by performing a “smart selection” of sentences from the beginning of the document) and the structure itself can"
2020.acl-main.174,D19-1180,1,0.888013,"cessful screenplays. According to this scheme, wellstructured stories consist of six basic stages which are defined by five turning points (TPs), i.e., events which change the direction of the narrative, and determine the story’s progression and basic thematic units. In Figure 1, TPs are highlighted for a CSI episode. Although the link between turning points and summarization has not been previously made, earlier work has emphasized the importance of narrative structure for summarizing books (Mihalcea and Ceylan, 2007) and social media content (Kim and Monroy-Hern´andez, 2015). More recently, Papalampidi et al. (2019) have shown how to identify turning points in feature-length screenplays by projecting synopsis-level annotations. Crucially, our method does not involve manually annotating turning points in CSI episodes. Instead, we approximate narrative structure automatically by pretraining on the annotations of the TRIPOD dataset of Papalampidi et al. (2019) and employing a variant of their model. We find that narrative structure representations learned on their dataset (which was created for feature-length films), transfer well across cinematic genres and computational tasks. We propose a framework for e"
2020.acl-main.174,P19-1628,1,0.910543,"on performance over general extractive algorithms, leading to more complete and diverse summaries. 1 Setup Opportunity New Situation Change of Plans Progress Point of no Return Complications Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient si"
2020.acl-main.174,P18-1061,0,0.0155492,"ataset, which also uses character-centered graphs to describe the content of movie video clips. Our work synthesizes various strands of research on narrative structure analysis (Cutting, 2016; Hauge, 2017), screenplay summarization (Gorinski and Lapata, 2015), and neural network modeling (Dong, 2018). We focus on extractive summarization and our goal is to identify an optimal sequence of key events in a narrative. We aim to create summaries which re-tell the plot of a story in a concise manner. Inspired by recent neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018; Zheng and Lapata, 2019), we develop supervised and unsupervised models for our summarization task based on neural representations of scenes and how these relate to the screenplay’s narrative structure. Contrary to most previous work which has focused on characters, we select summary scenes based on events and their importance in the story. Our definition of narrative structure closely follows Papalampidi et al. (2019). However, the model architectures we propose are general and could be adapted to different plot analysis schemes (Field, 2005; Vogler, 2007). To overcome the difficulties in ev"
2021.blackboxnlp-1.27,W19-4828,0,0.147771,"as with pre-trained language models, guage Visual Reasoning for Real (NLVR2) (Suhr it is not clear how and why these models perform et al., 2019). For this task, the model is presented as well as they do, what information they learn and with two images and a sentence and has to predict use in their predictions, or what their limitations whether that sentence is true of the images or not are. While a large body of research has focused on (see Figure 1). Since this task involves the prethe interpretation of pre-trained language models diction of a truth value, it lends itself well to the (e.g., Clark et al. 2019; Tenney et al. 2019; Rogers exploration of negation. We consider two popular et al. 2020; Elazar et al. 2021), such work has been pre-trained vision-and-language models, finetuned more limited for vision-and-language models. This paper focuses on a particular linguistic ca- for the task: LXMERT (Tan and Bansal, 2019) and pability, namely the ability to understand negation. UNITER (Chen et al., 2020). Details on the data Negation is universal across languages (Zeijlstra, and models are in Section 3. 2007) and is very important for interpreting the In order to investigate the performance of the"
2021.blackboxnlp-1.27,P19-1644,0,0.0276927,"es from the analysis of language models to investigate the ability of pretrained vision-and-language models to handle negation. We find that these models severely underperform in the presence of negation. 1 Introduction Figure 1: Examples from the NLVR2 corpus. The inVision-and-language models have made a lot of progress on complex tasks, going beyond recogni- put to the model consists of two images and a sentence. The model’s task is to predict whether the sentence is tion and towards reasoning over the two modalities true of the images or not. The top example shows an (Zellers et al., 2019; Suhr et al., 2019). Following instance where the sentence is true and the bottom one, the success of pre-trained language models such as false. Image from (Suhr et al., 2019). BERT (Devlin et al., 2019) on a range of language tasks, recent advances in vision-and-language have meaning and determining the truth value of a stateinvolved the introduction of pre-trained models ment. Vision-and-language models have applica(e.g., UNITER, Chen et al. 2020, VisualBERT, Li et al. 2019, ViLBERT, Lu et al. 2019, LXMERT, tions in real-world scenarios where it is crucial to Tan and Bansal 2019). These models achieve im- unde"
2021.emnlp-main.65,2020.acl-main.703,0,0.0395414,"details in-between. For example, Abel Magwitch in Great Expectations is in the first two chapters and then reappears explicitly in Chapter 40. Our results show that integrating KB and memory components improves the overall performance of salience detection. Using a vector alternative to infer salience is a slight improvement over the LM. Other measures such as swap salience and knowledge salience perform worse than the main salience measures but still show improvements over our baseline model. 2 Related Work The main architectural innovation is to use an external knowledgebase, based on RAG (Lewis et al., 2020b), and combine this seamlessly with a memory mechanism to improve the model’s predictive performance. The main structure of this model is to use a question and document encoder, both transformers, to learn and look up passages of text from a knowledgebase (based on DPR; Karpukhin et al. 2020) and then fuse this knowledge into a transformer encoder/decoder model such as BART (Lewis et al., 2020a) or T5 (Raffel et al., 2020). Similar models including REALM (Guu et al., 2020), Hard EM (Min et al., 2019a), SpanSeqGen (Min et al., 2020), and Fusion-in-Decoder (Izacard and Grave, 2021) have achieve"
2021.emnlp-main.65,W04-1013,0,0.0382816,"285 .243 .252 .243 .309 .368 .367 .355 .351 .301 .309 .311 .413 .356 .352 .339 .336 .300 .294 .315 .371 .359 .369 .358 .355 .306 .313 .315 .419 .254 .253 .245 .240 .199 .210 .201 .271 .241 .238 .228 .225 .194 .193 .245 .271 .243 .251 .243 .251 .200 .210 .200 .272 Table 3: Shmoop results from the silver label evaluations. Shmoop labels plotted with the salience for a book biggest finding is that salience based on the emchapter. bedding, Emb-Sal is the strongest measure. This As for the ProppLearner data, we report MAP. shows the merit of using the BART model more We also evaluate with ROUGE-L (Lin, 2004), com- flexibly as a general-purpose sentence encoding paring the text by selecting the k most salient sen- model. The Emb-Surp measure is a slight improvement on the baselines, indicating that it is mainly tences according to the measure where k is the the BCF method that causes an improvement in number of salient sentences, and report recall at salience detection, rather than a simple measure k. All measures are calculated by chapter, and we of how much the story changes from sentence to take the mean across the dataset. The results in Table 3 reveal several main themes. sentence. One differ"
2021.emnlp-main.65,D18-1154,0,0.025855,"as novels and plays are prohibitively expensive. This is especially true when multiple annotators are required to ensure high inter-annotator agreement. It would also not be possible with insufficiently trained and lower cost crowdsourced workers. Reading a local passage would not be enough as it is only possible to judge salience over the whole narrative, which can be tens of thousands of words. This requires strong comprehension and thus requires skilled annotators and is a daunting annotation task. Instead, this paper builds on a variant of an approach for event salience in news articles (Liu et al., 2018; Jindal et al., 2020). The method is to align expert-written summaries with the full text, tagging sentences that align with the summary as salient, thus turning the evaluation into a binary ranking problem. The intuition is that the summary will mention only salient events and themes. We use the Shmoop corpus (Chaudhury et al., 2019), which contains classic works of literature, such as Moby Dick, but also plays such as A Midsummer Nights Dream, and short stories including The Mask of the Red Death. The Shmoop corpus has stories split into chapters with aligned summaries. These bullet point s"
2021.emnlp-main.65,D19-1284,0,0.0132487,"ed Work The main architectural innovation is to use an external knowledgebase, based on RAG (Lewis et al., 2020b), and combine this seamlessly with a memory mechanism to improve the model’s predictive performance. The main structure of this model is to use a question and document encoder, both transformers, to learn and look up passages of text from a knowledgebase (based on DPR; Karpukhin et al. 2020) and then fuse this knowledge into a transformer encoder/decoder model such as BART (Lewis et al., 2020a) or T5 (Raffel et al., 2020). Similar models including REALM (Guu et al., 2020), Hard EM (Min et al., 2019a), SpanSeqGen (Min et al., 2020), and Fusion-in-Decoder (Izacard and Grave, 2021) have achieved state-of-theart results in factual domains such as answering natural language questions, trivia or games such as Jeopardy. In these domains, the key insight is that offloading knowledge externally allows models to perform better than much larger transformers We address this limitation by incorporating an that need to encode all knowledge in their weights. episodic knowledge retrieval mechanism (derived These methods that rely on retrieving raw text are from RAG; Lewis et al. 2020b) and fuse this wi"
2021.emnlp-main.65,2020.emnlp-main.466,0,0.0115335,"nnovation is to use an external knowledgebase, based on RAG (Lewis et al., 2020b), and combine this seamlessly with a memory mechanism to improve the model’s predictive performance. The main structure of this model is to use a question and document encoder, both transformers, to learn and look up passages of text from a knowledgebase (based on DPR; Karpukhin et al. 2020) and then fuse this knowledge into a transformer encoder/decoder model such as BART (Lewis et al., 2020a) or T5 (Raffel et al., 2020). Similar models including REALM (Guu et al., 2020), Hard EM (Min et al., 2019a), SpanSeqGen (Min et al., 2020), and Fusion-in-Decoder (Izacard and Grave, 2021) have achieved state-of-theart results in factual domains such as answering natural language questions, trivia or games such as Jeopardy. In these domains, the key insight is that offloading knowledge externally allows models to perform better than much larger transformers We address this limitation by incorporating an that need to encode all knowledge in their weights. episodic knowledge retrieval mechanism (derived These methods that rely on retrieving raw text are from RAG; Lewis et al. 2020b) and fuse this with also competitive with those th"
2021.emnlp-main.65,D19-1410,0,0.0261301,"ning set sizes are BooksCorpus circa 18k works, Gutenberg 27k, and Movie Scripts 1.5k. We split sentences using Blingfire. 3.4 Baselines approach (Miller, 2019). The method uses k-means to cluster BERT sentence vectors according to the number of desired sentences and then selecting the sentences closest to the centroids. Since salience scores are required, we adapt this method to output the cosine distance from the centroid as a salience score. We set the k so that there is one cluster for every 10 sentences. One change from Miller is to use the stsb-roberta-large sentence transformers model (Reimers and Gurevych, 2019), which has sentence embeddings that perform much better on a range of semantic tasks than raw BERT. 3.5 Inference Salience detection is based on the BCF method (Otake et al., 2020). We only use the sentence deletion variant. Let S be the set of all sentences. The salience is σ. For BCF this uses an event removal function r and coherence evaluator c. c is the difference in coherence between when the sentence t is present and removed in (3) for the following n sentences. Note that r can be used more broadly as a structural manipulation function. In this paper r is also used for swap function an"
2021.emnlp-main.65,2020.acl-main.178,0,0.0431237,"periment both with a Wikipedia vant parts of the story, act as an implicit index into KB and Wikiplots, a KB of story plot summaries. these dimensions, and the KB will supplement this The motive for the latter is that these plot fragwith typical plot knowledge. This memory mech- ments or vignettes act as a planning system (or anism is much more suitable than recent work on schema; Schank and Abelson 1977) guiding expecextended transformers for longer sequences, see tations. Riedl and Sugandh (2008) used a similar Tay et al. (2020) and Fournier et al. (2021) for concept in a rule-based system. Sap et al. (2020) thorough reviews. Characters, places, subplots ebb also use a bag-like episodic memory mechanism for and flow in long stories, so the most relevant in- inference in stories without the more sophisticated formation may be hundreds of pages previous with transformer encoders of the RAG model. After the mainly irrelevant information in-between, which experimental work in this paper, a follow-up paper 852 by Shuster et al. (2021) on several RAG variants found that the KB was able to reduce the amount of hallucination in generating dialogue. The KB grounds the text generation in relevant facts ret"
2021.emnlp-main.65,2021.findings-emnlp.320,0,0.0252316,"Missing"
2021.emnlp-main.65,2020.emnlp-main.47,0,0.0166303,"et al. (1977) notion of characters crossing a forbidden semantic border; or suspense as per Zillmann (1996), or Gerrig and Bernardo (1994) concept of the reader as problem solver. There is, therefore, rich potential work in modelling character states, knowledge, intents and contrasting them with the readers’ expectations, and the norms of the narrative world in inferring concepts such as salience, suspense, and surprise. Characters could be implicitly modelled using a per entity memory model extending the current RAG approach. Or take a more structured approach inspired by recent work such as Sims and Bamman (2020) modelling literary character communication, or story generation systems such as CAST (Peng et al., 2021) that model multiple characters goals or C2PO (Ammanabrolu et al., 2021) that more explicitly models causal chain relations. The main overall finding is that the BCF method can infer salience over and above baselines with an improvement on much longer works. We find that augmenting an LM with memory and an external KB can improve the detection of salience and increase the predictive power of the LM on narrative text. We also find that a vector-based version of the concept can perform slight"
2021.emnlp-main.65,2020.acl-main.161,1,0.893672,"formation and are relevant to question edge disparity between the reader and the character answering. to create more suspenseful plots and hence more 851 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 851–865 c November 7–11, 2021. 2021 Association for Computational Linguistics important events. We model knowledge salience as the difference between an expert-informed reader versus a naive one by taking the difference between the average log-likelihood of a base LM and an LM enriched with memory and a KB. We also take inspiration from the model of Wilmot and Keller (2020), who compute suspense and surprise in short stories using vector states from a hierarchical model; this follows from theoretical work by Ely et al. (2015), and cognitive work from Li et al. (2019). We show how a vector salience measure can be computed based on this approach. In addition to exploring new salience measures, our work aims to overcome limitations of existing work on salience modeling. Otake et al. (2020) only evaluate their model on a single type of narrative (Russian fairytales) and on a very small annotated dataset. We address this by using aligned summaries from the Shmoop cor"
2021.emnlp-main.65,P17-1153,0,0.0131005,"al., 2015) provides a large corpus of longer novel-length works and is used for training. However, BooksCorpus consists of free books scraped from Smashwords; these works are highly slanted towards particular genres such as romance and fantasy which are unlike the evaluated task, which is mainly classic works. To supplement BooksCorpus an additional training dataset from Gutenberg using the c-w/gutenberg library filtered to only English language fictional works. Another important area of longer-form storytelling is movies or dramatic works. So to improve diversity, the Movie Scripts datasets (Ramakrishna et al., 2017) is used. Multi-dataset models performed better on the validation set in training than single corpus models, so only these are evaluated. The training set sizes are BooksCorpus circa 18k works, Gutenberg 27k, and Movie Scripts 1.5k. We split sentences using Blingfire. 3.4 Baselines approach (Miller, 2019). The method uses k-means to cluster BERT sentence vectors according to the number of desired sentences and then selecting the sentences closest to the centroids. Since salience scores are required, we adapt this method to output the cosine distance from the centroid as a salience score. We se"
C14-1012,D13-1128,1,0.88814,"ood, such as people having fun at a party, or an action, such as using a computer. The bag-of-terms representation is limited to matching images based on the presence or absence of terms, and not the relation of the terms to each other. Figures 1(a) and (b) highlight the problem with using unstructured representations for image retrieval: there is a person and a computer in both images but only (a) depicts a person actually using the computer. To address this problem with unstructured representations we propose to represent the structure of an image using the Visual Dependency Representation (Elliott and Keller, 2013). The Visual Dependency Representation is a directed labelled graph over the regions of an image that captures the spatial relationships between regions. The representation is inspired by evidence from the psychology literature that people are better at recognising and searching for objects when the spatial relationships between the objects in the image are consistent with our expectations of the world.(Biederman, 1972; Bar and Ullman, 1996). In an automatic image description task, Elliott This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org"
C14-1012,H05-1066,0,0.135657,"Missing"
C14-1012,D13-1072,0,\N,Missing
C14-1012,D11-1041,0,\N,Missing
C16-1126,K15-1038,1,0.798957,"S) correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models. 1 Introduction The eye movements during normal, skilled reading are known to reflect the processing load associated with reading. Recently, eye movement data has been integrated into natural language processing models for weakly supervised part-of-speech (PoS) induction (Barrett et al., 2016), sentence compression (Klerke et al., 2016), supervised PoS tagging (Barrett and Søgaard, 2015a), and supervised parsing (Barrett and Søgaard, 2015b). Barrett et al. (2016) used eye movements from the English portion of a large eye tracking corpus, the Dundee corpus (Kennedy et al., 2003), for weakly supervised PoS induction for English, obtaining significant improvements over a baseline without gaze features. They used a second-order hidden Markov Model, which was type-constrained by Wiktionary dictionaries for their experiments. These results suggest an approach to weakly supervised PoS induction using only a dictionary and eye movement data. Such an approach would be applicable for"
C16-1126,W15-2401,1,0.851881,"S) correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models. 1 Introduction The eye movements during normal, skilled reading are known to reflect the processing load associated with reading. Recently, eye movement data has been integrated into natural language processing models for weakly supervised part-of-speech (PoS) induction (Barrett et al., 2016), sentence compression (Klerke et al., 2016), supervised PoS tagging (Barrett and Søgaard, 2015a), and supervised parsing (Barrett and Søgaard, 2015b). Barrett et al. (2016) used eye movements from the English portion of a large eye tracking corpus, the Dundee corpus (Kennedy et al., 2003), for weakly supervised PoS induction for English, obtaining significant improvements over a baseline without gaze features. They used a second-order hidden Markov Model, which was type-constrained by Wiktionary dictionaries for their experiments. These results suggest an approach to weakly supervised PoS induction using only a dictionary and eye movement data. Such an approach would be applicable for"
C16-1126,P16-2094,1,0.755469,"es have been limited to English, however. This study shows that gaze and part of speech (PoS) correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models. 1 Introduction The eye movements during normal, skilled reading are known to reflect the processing load associated with reading. Recently, eye movement data has been integrated into natural language processing models for weakly supervised part-of-speech (PoS) induction (Barrett et al., 2016), sentence compression (Klerke et al., 2016), supervised PoS tagging (Barrett and Søgaard, 2015a), and supervised parsing (Barrett and Søgaard, 2015b). Barrett et al. (2016) used eye movements from the English portion of a large eye tracking corpus, the Dundee corpus (Kennedy et al., 2003), for weakly supervised PoS induction for English, obtaining significant improvements over a baseline without gaze features. They used a second-order hidden Markov Model, which was type-constrained by Wiktionary dictionaries for their experiments. These results suggest an approach to weakly supervised PoS ind"
C16-1126,N10-1083,0,0.022573,"features as well. 4 Experiment We replicate the experimental setup of Barrett et al. (2016), which used the best model from Li et al. (2012), a second-order hidden Markov model with maximum entropy emissions (SHMM-ME) constrained by Wiktionary tags such that emissions are confined to the allowed PoS tags of the Wiktionary given that the token exists in the Wiktionary. Li et al. (2012) report considerable improvements from the Wiktionary contraint when comparing to unsupervised methods. The second-order model includes transition probabilities from the antecedent state like a first order model (Berg-Kirkpatrick et al., 2010) as well as from the second-order antecedent state. We use the original implementation of Li et al. and we also include a subset of their word-level features, viz., four features detecting hyphens, numerals, punctuation and capitalization. We leave out the three 2 http://www.natcorp.ox.ac.uk http://www.lexique.org 4 http://www.speech.cs.cmu.edu/SLM/toolkit.html 3 1335 suffix features from Li et al.’s basic feature model, as these features do not transfer across languages. These features were also included by Barrett et al. (2016). We use the English Wiktionary dumps made available by Li et al."
C16-1126,W11-2123,0,0.0261518,"Missing"
C16-1126,N16-1179,1,0.774922,"s study shows that gaze and part of speech (PoS) correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models. 1 Introduction The eye movements during normal, skilled reading are known to reflect the processing load associated with reading. Recently, eye movement data has been integrated into natural language processing models for weakly supervised part-of-speech (PoS) induction (Barrett et al., 2016), sentence compression (Klerke et al., 2016), supervised PoS tagging (Barrett and Søgaard, 2015a), and supervised parsing (Barrett and Søgaard, 2015b). Barrett et al. (2016) used eye movements from the English portion of a large eye tracking corpus, the Dundee corpus (Kennedy et al., 2003), for weakly supervised PoS induction for English, obtaining significant improvements over a baseline without gaze features. They used a second-order hidden Markov Model, which was type-constrained by Wiktionary dictionaries for their experiments. These results suggest an approach to weakly supervised PoS induction using only a dictionary and eye movem"
C16-1126,D12-1127,0,0.0361907,"Missing"
C16-1126,D14-1187,0,0.0753916,"Missing"
D07-1016,H91-1060,0,0.0715671,"Missing"
D07-1016,P05-2023,1,0.700122,"Missing"
D07-1016,P05-1039,1,0.896407,"Missing"
D07-1016,forst-kaplan-2006-importance,0,0.01361,"g, pp. 151–160, Prague, June 2007. 2007 Association for Computational Linguistics enable the parser to process inclusions accurately. This paper is organized as follows. We review related work in Section 2, and present the English inclusion classifier in Section 3. Section 4 describes our results on interfacing inclusion detection with parsing, and Section 5 presents an error analysis. Discussion and conclusion follow in Section 6. 2 prisingly low as no differentiation is made between full-word anglicisms and tokens with mixed-lingual morphemes in the gold standard. In the context of parsing, Forst and Kaplan (2006) have observed that the failure to properly deal with foreign inclusions is detrimental to a parser’s accuracy. However, they do not substantiate this claim using numeric results. Related Work 3 Previous work on inclusion detection exists in the TTS literature. Here, the aim is to design a system that recognizes foreign inclusions on the word and sentence level and functions at the front-end to a polyglot TTS synthesizer. Pfister and Romsdorfer (2003) propose morpho-syntactic analysis combined with lexicon lookup to identify foreign words in mixed-lingual text. While they state that their syst"
D07-1016,P03-1054,0,0.00697396,"Missing"
D07-1016,W03-0428,0,0.015994,"ed entity recognition, for which various machine learning (ML) techniques have proved successful. In order to compare the performance of the English inclusion classifier against a trained ML classifier, we pooled the annotated English inclusion evaluation data for all three domains. As the English inclusion classifier does not rely on annotated data, it can be tested and evaluated once for the entire corpus. The ML classifier used for this experiment is a conditional Markov model tagger which is designed for, and proved successful in, named entity recognition in newspaper and biomedical text (Klein et al., 2003; Finkel et al., 2005). It can be trained to perform similar information extraction tasks such as English inclusion detection. To determine the tagger’s performance over the entire set and to investigate the effect of the amount of annotated training data available, a 10-fold crossvalidation test was conducted whereby increasing sub-parts of the training data are provided when testing on each fold. The resulting learning curves in Figure 1 show that the English inclusion classifier has an advantage over the supervised ML approach, despite the fact the latter requires expensive handannotated da"
D11-1028,P08-2002,0,0.0176437,"e a computational model that captures discourse effects on syntax in terms of prediction. The model comprises a co-reference component which explicitly stores discourse mentions of NPs, and a syntactic component which adjust the probabilities of NPs in the syntactic structure based on the mentions tracked by the discourse component. Our model is HMM-based, which makes it possible to efficiently process large amounts of data, allowing an evaluation on eye-tracking corpora, which has recently become the gold-standard in computational psycholinguistics (e.g., Demberg and Keller 2008; Frank 2009; Boston et al. 2008; Mitchell et al. 2010). The paper is structured as follows: In Section 2, we describe the co-reference and the syntactic models and evaluate their performance on standard data sets. Section 3 presents an evaluation of the overall model on the Dundee eye-tracking corpus. The paper closes with a comparison with related work and a general discussion in Sections 4 and 5. 2 Model This model utilises an NP chunker based upon a hidden Markov model (HMM) as an approximation to syntax. Using a simple model such as an HMM facilitates the integration of a co-reference component, and the fact that the mo"
D11-1028,N01-1021,0,0.257718,"Missing"
D11-1028,P10-1021,1,0.860625,"del that captures discourse effects on syntax in terms of prediction. The model comprises a co-reference component which explicitly stores discourse mentions of NPs, and a syntactic component which adjust the probabilities of NPs in the syntactic structure based on the mentions tracked by the discourse component. Our model is HMM-based, which makes it possible to efficiently process large amounts of data, allowing an evaluation on eye-tracking corpora, which has recently become the gold-standard in computational psycholinguistics (e.g., Demberg and Keller 2008; Frank 2009; Boston et al. 2008; Mitchell et al. 2010). The paper is structured as follows: In Section 2, we describe the co-reference and the syntactic models and evaluate their performance on standard data sets. Section 3 presents an evaluation of the overall model on the Dundee eye-tracking corpus. The paper closes with a comparison with related work and a general discussion in Sections 4 and 5. 2 Model This model utilises an NP chunker based upon a hidden Markov model (HMM) as an approximation to syntax. Using a simple model such as an HMM facilitates the integration of a co-reference component, and the fact that the model is generative is a"
D11-1028,P94-1018,0,0.102881,"d is higher. Moreover, the HMM+Ref model provides a significantly better fit than the HMM model, which demonstrates the benefit of co-reference information for modeling reading times. Again, all three measures provide the same result. Table 2 corroborates this result. It list the mixed-model coefficients for the HMM+Ref model and shows that all factors are significant predictors, including both HMM surprisal and residualized HMM+Ref surprisal. 4 Related Work There have been few computational models of human sentence processing that have incorporated a referential or discourse-level component. Niv (1994) proposed a parsing model based on Combinatory Categorial Grammar (Steedman, 2001), in which referential information was used to resolve syntactic ambiguities. The model was able to capture effects of referential information on syntactic garden paths (Altmann and Steedman, 1988). This model differs from that proposed in the present paper, as it is intended to capture psycholinguistic preferences in a qualitative manner, whereas the aim of the present model is to provide a quantitative fit to measures of processing difficulty. Moreover, the model was not based on a large-scale grammar, and was"
D11-1028,J01-4004,0,0.0182731,"ecoding, the decoder therefore also selects a chain of co-referent entities. Generally, for words which have been used in this discourse, the magnitude of probabilities in the ‘seen before’ distribution are much higher than in the ‘discourse new’ distribution. Thus, there is a strong bias to classify NPs which match word-for-word as being co-referent. There remains a possibility that the model primarily captures lexical priming, rather than co-reference. However, we note that string match is a strong indicator of two NPs being coreferFigure 2: Looking up entries from the NP Cache 307 ent (cf. Soon et al. 2001), and, moreover, the matching is done on an NP-by-NP basis, which is more suitable for finding entity coreference, rather than a word-by-word basis, which would be more suitable for lexical priming. An appealing side-effect of using a simple coreference decision rule which is applied incrementally is that it is relatively simple to incrementally compute the transitive closure of co-reference chains, resulting in the entity sets which are then used in evaluation. The co-reference model only has one free parameter, λ, which is estimated from the ACE-2 corpus. The estimate is computed by counting"
D11-1028,W00-0726,0,0.0289315,"Missing"
D11-1028,P10-1120,1,\N,Missing
D13-1004,N10-1083,0,0.0572949,"gory and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syn"
D13-1004,P11-1087,0,0.0581607,"1) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) an"
D13-1004,D10-1056,1,0.945212,"ber 2013. 2013 Association for Computational Linguistics favour of morphology, whereas learners of English favour word order (Slobin, 1982; MacWhinney et al., 1984). These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength (rich morphology versus strict word order), and thus be more language-general, than single-task models. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng,"
D13-1004,D11-1059,1,0.882313,"comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentations and syntactic categories jointly"
D13-1004,E03-1009,0,0.290804,"levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the model to flexibly weigh them according to their utility. Languages differ in the richness of their morphology and strictness of word order. These characteristics appear to be (anti)co"
D13-1004,D07-1023,0,0.0218669,"os et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentation"
D13-1004,N13-1012,0,0.0220972,"earn multiple aspects in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, a"
D13-1004,P12-1020,1,0.802314,"r, children clearly learn multiple aspects in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sou"
D13-1004,W13-2603,0,0.0167602,"on from the stronger cue. The fact that the nature of this improvement varies by language provides evidence that joint learning can effectively accommodate typological diversity. 2 Model The task is to assign word tokens to part of speech categories and simultaneously segment the tokens into morphemes. We assume a relatively simple yet commonly used concatenative morphology which models a word as a stem plus (possibly null) suffix2 . 1 There are languages with much richer morphology than Spanish, but none with a child-directed corpus suitably annotated for evaluation. 2 Fullwood and O’Donnell (2013) recently presented a model of non-concatenative morphology that could be integrated into this model; however, it does not perform well on English (and presumably other mostly concatenative languages). Since this is an unsupervised model, the inferred categories and morphemes lack meaningful labels, but ideally will correspond to gold standard categories and morphemes. 2.1 generated from Dirichlet-multinomials conditioned on the tag t: κ∼ Dir(ακ ) t|κ ∼ Mult(κ) σ∼ Dir(αs ) s|t, σ ∼ Mult(σt ) φ∼ Dir(α f ) f |t, φ ∼ Mult(φt ) Word Order We model a sequence of words as a Hidden Markov Model (HMM)"
D13-1004,P07-1094,1,0.810299,"ferent syntactic categories. Most recent models have included a constraint forcing all tokens of a given type into the same category, which improves performance but often complicates inference. The Bayesian HMM’s performance is therefore not stateof-the-art, but is comparable to other token-based models (Christodoulopoulos et al., 2010) and the model is easy to extend within the Bayesian framework, allowing us to compare multiple versions. This part of the model is parametric, operating over a fixed number of tags T , and is identical to the formulation of tag transitions in the Bayesian HMM (Goldwater and Griffiths, 2007). However, we replace the BHMM’s emission distribution with the morphologically-informed distributions described below. As in the BHMM, the emission distributions are conditioned on the tag, i.e., each tag has its own morphology. 2.2 Morphology The morphology model introduced by Goldwater et al. (2006) generates morphological analyses for a set of tokens. These analyses consist of a tag plus a stem and suffix pair, which are concatenated to form the observed words. Both stem s and suffix f are 32 ∑ P(s|t)P( f |t)P(t) (1) t,s, f s.t. s⊕ f =w where s ⊕ f = w denotes that the concatenation of ste"
D13-1004,P06-1111,0,0.0339909,"odels. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordintern"
D13-1004,J11-2002,0,0.0398389,"Missing"
D13-1004,P08-1046,0,0.0200408,"in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the mod"
D13-1004,E12-1024,1,0.822483,"ther than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the model to flexibly weigh them a"
D13-1004,D07-1043,0,0.0368605,"Missing"
D13-1004,N12-1045,0,0.328378,"Missing"
D13-1004,P05-1044,0,0.0261761,"eral, than single-task models. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories"
D13-1004,P06-1124,0,0.0452368,"nd suffix f are 32 ∑ P(s|t)P( f |t)P(t) (1) t,s, f s.t. s⊕ f =w where s ⊕ f = w denotes that the concatenation of stem and suffix results in the word w. On its own, this distribution over morphological analyses makes independence assumptions that are too strong: most word tokens of a word type have the same analysis, but P0 will re-generate that analysis for every token. To resolve this problem, a Pitman-Yor process (PYP) is placed over the generating distribution above. The Pitman-Yor process has been found to be useful for representing the power-law distributions common in natural language (Teh, 2006; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011). The distribution of draws from a Pitman-Yor process (which, in our case, determines the distribution of word tokens with each morphological analysis) is commonly described using the metaphor of a Chinese restaurant. A series of customers (tokens z = z1 . . . zN ) enter a restaurant with an infinite number of initially empty tables. Upon entering, each customer is seated at a table k with probability p(zi = k|z1 . . . zi−1 , a, b) = ( nk −a if 1 ≤ k ≤ K i−1+b Ka+b if k = K + 1 i−1+b (2) tk sk zi fk lk wi K ti−2 N where nk is the number o"
D13-1004,W11-0301,0,\N,Missing
D13-1004,D10-1083,0,\N,Missing
D13-1128,P12-1038,0,0.784503,"utperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements. 1 Previous approaches to automatic description generation have typically tackled the problem using an object recognition system in conjunction with a natural language generation component based on language models or templates (Kulkarni et al., 2011; Li et al., 2011). Some approaches have utilised the visual attributes of objects (Farhadi et al., 2010), generated descriptions by retrieving the descriptions of similar images (Ordonez et al., 2011; Kuznetsova et al., 2012), relied on an external corpus to predict the relationships between objects (Yang et al., 2011), or combined sentence fragments using a treesubstitution grammar (Mitchell et al., 2012). Introduction Humans are readily able to produce a description of an image that correctly identifies the objects and actions depicted. Automating this process is useful for applications such as image retrieval, where users can go beyond keyword-search to describe their information needs, caption generation for improving the accessibility of existing image collections, story illustration, and in assistive technol"
D13-1128,W11-0326,0,0.328655,"gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements. 1 Previous approaches to automatic description generation have typically tackled the problem using an object recognition system in conjunction with a natural language generation component based on language models or templates (Kulkarni et al., 2011; Li et al., 2011). Some approaches have utilised the visual attributes of objects (Farhadi et al., 2010), generated descriptions by retrieving the descriptions of similar images (Ordonez et al., 2011; Kuznetsova et al., 2012), relied on an external corpus to predict the relationships between objects (Yang et al., 2011), or combined sentence fragments using a treesubstitution grammar (Mitchell et al., 2012). Introduction Humans are readily able to produce a description of an image that correctly identifies the objects and actions depicted. Automating this process is useful for applications such as image retriev"
D13-1128,P05-1012,0,0.0482571,"ike down the road. det aux det nsubj dobj det pobj advmod root (c) Figure 1: (a) Image with regions marked up: BIKE, CAR, MAN , ROAD , TREES ; (b) human-generated image description; (c) visual dependency representation expressing the relationships between MAN, BIKE, and ROAD aligned to the syntactic dependency parse of the first sentence in the human-generated description (b). Finally, we also show that the benefit of the visual dependency representation is maintained when image descriptions are generated from automatically parsed VDRs. We use a modified version of the edge-factored parser of McDonald et al. (2005) to predict VDRs over a set of annotated object regions. This result reaffirms the potential utility of this representation as a means to describe events in images. Note that throughout the paper, we work with goldstandard region annotations; this makes it possible to explore the effect of structured image representations independently of automatic object detection. 2 sentations would be able to correctly infer the action that is taking place, such as the distinction between repairing or riding a bike, which would greatly improve the descriptions it is able to generate. In this paper, we intro"
D13-1128,E12-1076,0,0.809976,"escription generation have typically tackled the problem using an object recognition system in conjunction with a natural language generation component based on language models or templates (Kulkarni et al., 2011; Li et al., 2011). Some approaches have utilised the visual attributes of objects (Farhadi et al., 2010), generated descriptions by retrieving the descriptions of similar images (Ordonez et al., 2011; Kuznetsova et al., 2012), relied on an external corpus to predict the relationships between objects (Yang et al., 2011), or combined sentence fragments using a treesubstitution grammar (Mitchell et al., 2012). Introduction Humans are readily able to produce a description of an image that correctly identifies the objects and actions depicted. Automating this process is useful for applications such as image retrieval, where users can go beyond keyword-search to describe their information needs, caption generation for improving the accessibility of existing image collections, story illustration, and in assistive technology for blind and A common aspect of existing work is that an image is represented as a bag of image regions. Bags of regions encode which objects co-occur in an image, but they are un"
D13-1128,W12-3018,0,0.0128969,"Missing"
D13-1128,D11-1041,0,\N,Missing
D13-1128,P04-1077,0,\N,Missing
D14-1036,W09-1206,0,0.0606412,"Missing"
D14-1036,W05-0620,0,0.405746,"Missing"
D14-1036,P00-1058,0,0.0769682,"modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefix tree. of non-predictive elementary trees. An example of a PLTAG derivation is given in Figure 2. In step 1, a prediction tree is introduced through substitution, which then allows the adjunction of an adverb in step 2. Step 3 involves the verification of the marker introduced by the prediction tree against the elementary tree for open. In order to efficiently parse PLTAG, Demberg et al. (2013) introduce the concept of fringes. Fringes captu"
D14-1036,J13-4008,1,0.901452,"letions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental dependency parser, do not share this advantage, as we will discuss in Section 4.3. 2 semantic role labeling is a novel task. Our model builds on an incremental Tree Adjoining Grammar parser (Demberg et al., 2013) which predicts the syntactic structure of upcoming input. This all"
D14-1036,J05-1004,0,0.125834,"ay hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fri"
D14-1036,W07-2416,0,0.0663346,"Missing"
D14-1036,C92-2066,0,0.6767,"edicts the syntactic structure of upcoming input. This allows us to perform incremental parsing and incremental SRL in tandem, exploiting the predictive component of the parser to assign (potentially incomplete) semantic roles on a word-by-word basis. Similar to work on incremental parsing that evaluates incomplete trees (Sangati and Keller, 2013), we evaluate the incomplete semantic structures produced by our model. 3 Psycholinguistically Motivated TAG Demberg et al. (2013) introduce Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG), a grammar formalism that extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. Elementary trees that contain a foot node are called auxiliary trees; those that do not are called initial trees. Examples for TAG elementary trees are given in Figure 1a–c. To derive a TAG parse for a sentence, we start with the elementary tree of the head of the sentence and integrate the elementary trees of the"
D14-1036,P10-2012,1,0.843954,"mantic garden paths occur because 301 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301–312, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics provide semantically informed completions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental depen"
D14-1036,W13-2607,1,0.850665,"sponding nodes in Tv . For simplicity of presentation, we will use a concrete example, see Figure 5. Figure 5a shows the lexicon entries for the words of the sentence Semantic Role Lexicon Recall that Propbank is used to construct the PLTAG treebank, in order to distinguish between arguments and modifiers, which result in elementary trees with substitution nodes, and auxiliary trees, i.e., trees with a foot node, respectively (see Figure 1). Conveniently, we can use the same information to also enrich the extracted lexicon with the semantic role annotations, following the process described by Sayeed and Demberg (2013).1 For arguments, annotations are retained on the substitution node in the parental tree, while for modifiers, the role annotation is displayed on the foot node of the auxiliary tree. Note that we display role annotation on traces that are leaf nodes, 1 Contrary to Sayeed and Demberg (2013) we put role label annotations for PPs on the preposition rather than their NP child, following of the CoNLL 2005 shared task (Carreras and M`arquez, 2005). 2 Prediction tree T in our algorithm is only used during pr verification, so it set to nil for substitution and adjunction operations. 304 Banks refused"
D14-1036,J14-3006,1,0.841387,"nd argument identification. In this respect it is analogous to unlabeled dependency accuracy reported in the parsing literature. We exSystem Comparison We evaluated three configurations of our system. The first configuration (iSRL) uses all semantic roles for each PLTAG lexicon entry, applies the PLTAG parser, IRPA, and both classifiers to perform identification and disambiguation, as described in Section 4. The second one (MajorityBaseline), solves the problem of argument identification and role disambiguation without the classifiers. For the former we employ a set of heuristics according to Lang and Lapata (2014), that rely on gold syntactic dependency information, sourced from CoNLL input. For the latter, we choose the most frequent role given the gold standard dependency relation label for the particular argument. Note that dependencies have been produced in view of the whole sentence and not incrementally. 308 System iSRL-Oracle iSRL Majority-Baseline Malt-Baseline Prec 91.00 81.48 71.05 60.90 Rec 80.26 75.51 58.10 46.14 F1 85.29 78.38 63.92 52.50 prefixes (up to word 10), presumably as it does not benefit from syntactic prediction, and thus cannot generate incomplete triples early in the sentence,"
D14-1036,D07-1062,0,0.165244,"essing as the path from an argument to the predicate can be very informative but is often quite complicated, and depends on the syntactic formalism used. Many paths through the parse tree are likely to occur infrequently (or not at all), resulting in very sparse information for the classifier to learn from. Moreover, as we will discuss in Section 4.4, such path information is not always available when the input is processed incrementally. There is previous SRL work employing Tree Adjoining Grammar, albeit in a non-incremental setting, as a means to reduce the sparsity of syntaxbased features. Liu and Sarkar (2007) extract a rich feature set from TAG derivations and demonstrate that this improves SRL performance. In contrast to incremental parsing, incremental 302 (a) NP (b) S NNS NP↓ Banks (d) S1 (c) VP VP AP VB RB open rarely VP* A1 A0 NP1 ↓ VP11 Banks refused nsbj a B↓ a B C↓ a B↓ b (a) valid open aux today hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) aux"
D14-1036,W08-2121,0,0.0710313,"Missing"
D14-1036,J93-2004,0,0.0497726,"anks (d) S1 (c) VP VP AP VB RB open rarely VP* A1 A0 NP1 ↓ VP11 Banks refused nsbj a B↓ a B C↓ a B↓ b (a) valid open aux today hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a p"
D14-1036,J08-2001,0,0.0889078,"Missing"
D14-1036,W00-1307,0,0.0413792,"h with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefi"
D14-1036,W09-1201,0,\N,Missing
D14-1036,Q13-1010,1,\N,Missing
D16-1009,P10-1119,0,0.0200184,"Missing"
D16-1009,N01-1021,0,0.404713,"Missing"
D16-1009,W09-1113,0,0.227005,"Missing"
D16-1009,W10-2008,0,0.0419729,"Missing"
D16-1009,petrov-etal-2012-universal,0,0.0266036,"Adding ωi−1 as a predictor results in a significant improvement in model fit (deviance = 4,798, t = 71.3). This shows that NEAT captures the context dependence of fixation sequences to an extend that goes beyond word frequency alone. Parts of Speech Part of speech categories are known to be a predictor of fixation probabilities, with content words being more likely to be fixated than function words (Carpenter and Just, 1983). In Table 4, we give the simulated fixation probabilities and the human fixation probabilities estimated from the Dundee corpus for the tags of the Universal PoS tagset (Petrov et al., 2012), using the PoS annotation of Barrett et al. (2015). We again compare with the probabilities of a threshold predictor derived from word frequency.1 NEAT captures the differences between PoS categories well, as evidenced by the high correlation coefficients. The content word categories ADJ, ADV, NOUN, VERB and X consistently show higher probabilities than the function word categories. While the correlation coefficients for word frequency are very similar, the numerical values of the simulated probabilities are closer to the human ones than those derived from word frequency, which tend towards m"
D16-1009,P82-1020,0,0.869936,"Missing"
D16-1009,D13-1075,0,0.128842,"Missing"
D17-1303,S14-2010,0,0.036251,"S2 GT Pred Black bird standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual se"
D17-1303,S15-2045,0,0.0326146,"d standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Ki"
D17-1303,S12-1051,0,0.0318735,"3 84.6 84.5 91.5 S1 S2 GT Pred Black bird standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et a"
D17-1303,J75-4040,0,0.427596,"Missing"
D17-1303,P11-1020,0,0.0528035,"izing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Kiros et al., 2015), and order embeddings (Vendrov et al., 2016). The shared task baseline is computed based on"
D17-1303,W16-3210,0,0.233939,"Missing"
D17-1303,D16-1026,0,0.275329,"sis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016). Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat et al., 2016) and to learn multilingual multimodal representations (Rajendran et al., 2016; Calixto et al., 2017). Rajendran et al. (2016) propose a 2839 Proceedings of the 2017 Conference on Empirical Metho"
D17-1303,D15-1070,0,0.209515,"We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to han"
D17-1303,P16-1227,0,0.0697283,"evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015"
D17-1303,P16-1168,0,0.2997,"Missing"
D17-1303,N16-1021,0,0.155414,"which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation"
D17-1303,W10-0721,0,0.0735935,"man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Kiros et al., 2015), and order embeddings (Vendrov et al., 2016). The shared task baseline is computed based on word overlap and is high for both the 2014 and the 2015 dataset, indicating that there is substantial"
D17-1303,Q14-1017,0,0.0524692,"the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy Most work on learning a joint space for images and their descriptions is based on Canonical Correlation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016). Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat"
D17-1303,W16-2346,0,0.0614214,"larity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015"
D17-1303,D17-1026,0,0.134783,"odal embeddings, irrespective of the language. Our results also show that the asymmetric scoring function can help learn better embeddings. In Table 3 we present a few examples where P IVOT-A SYM and PARALLEL -A SYM models performed better on both the languages compared to baseline order embedding model even using descriptions of very different lengths as queries. 4.2 Semantic Textual Similarity Results In the semantic textual similarity task (STS), we use the textual embeddings from our model to compute the similarity between a pair of sen2842 Model Shared Task Baseline STS Best System GRAN (Wieting et al., 2017) MLMME (Calixto et al., 2017) VSE (Kiros et al., 2015) OE (Vendrov et al., 2016) P IVOT-S YM PARALLEL -S YM P IVOT-A SYM PARALLEL -A SYM VF − − − VGG19 VGG19 VGG19 VGG19 VGG19 VGG19 VGG19 2012 2014 2015 29.9 51.3 60.4 87.3 83.4 86.4 83.7 84.5 85.0 − 72.7 79.7 80.6 82.7 89.6 82.2 84.1 90.8 80.5 81.8 89.2 82.0 81.4 90.4 83.1 83.8 90.3 84.6 84.5 91.5 S1 S2 GT Pred Black bird standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding pape"
D17-1303,P07-1108,0,0.0352625,"2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat et al., 2016) and to learn multilingual multimodal representations (Rajendran et al., 2016; Calixto et al., 2017). Rajendran et al. (2016) propose a 2839 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2839–2845 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Two men playing soccer on a field Joint space translations of each other, i.e., they are not parallel, although they describe the same image. 3 zwei männer kämpfen um einen fussball Figure 1: Our multilingual multimodal model with image as pi"
D17-1303,P17-2066,0,0.114081,"mage-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015). There has also been a"
D17-1303,Q14-1006,0,0.831264,"create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015). There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy Most work on learning a joint space for images and their descriptions is based on Canonical Correlation Analysis (CCA) or neural variants of CCA over representations of image and its"
D17-1303,W10-0707,0,\N,Missing
D19-1180,W14-0907,0,0.126992,"an overview of the movie’s genre, mood, and artistic style based on screenplay analysis. Gorinski and Lapata (2015) summarize full length screenplays by extracting an optimal chain of scenes via a graph-based approach centered around the characters of the movie. A similar approach has also been adopted by Vicol et al. (2018), who introduce the MovieGraphs dataset consisting of 51 movies and describe video clips with character-centered graphs. Other work creates animated story-boards using the action descriptions of screenplays (Ye and Baldwin, 2008), extracts social networks from screenplays (Agarwal et al., 2014a), or creates xkcd movie narrative charts (Agarwal et al., 2014b). Our work also aims to analyze the narrative structure of movies, but we adopt a high-level approach. We advocate TP identification as a precursor to more fine-grained analysis that unveils character attributes and their relationships. Our approach identifies key narrative events and segments the screenplay accordingly; we argue that this type of preprocessing is useful for applications which might perform question answering and summarization over screenplays. Although our experiments focus solely on the textual modality, turni"
D19-1180,N15-1084,0,0.018318,"s which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epita"
D19-1180,P13-1035,0,0.17446,"re present. Based on the pilot, annotation instructions were devised and an annotation tool was created which allows to label synopses with TPs sentence-bysentence. After piloting the annotation scheme on 30 movies, two new annotators were trained using our instructions and in a second study, they doubly annotated five movies. The remaining movies 1709 § tion. Finally, Koˇcisk`y et al. (2018) recently introduced a dataset consisting of question-answer pairs over 1,572 movie screenplays and books. Previous approaches have focused on finegrained story analysis, such as inducing character types (Bamman et al., 2013, 2014) or understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017). Various approaches have also attempted to analyze the goal and structure of narratives. Black and Wilensky (1979) evaluate the functionality of story grammars in story understanding, Elson and McKeown (2009) develop a platform for representing and reasoning over narratives, and Chambers and Jurafsky (2009) learn fine-grained chains of events. In the context of movie summarization, Gorinski and Lapata (2018) automatically generate an overview of the movie’s genre, mood, and artistic style ba"
D19-1180,P14-1035,0,0.23957,"g to evaluate various theories of storytelling (e.g., by examining a collection of works within a single genre, by an author, or topic) and to develop tools which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how"
D19-1180,D18-2029,0,0.0234737,"and scene sentences. Again, generic and entity-specific representations are combined via concatenation. 4.3 End-to-end TP Identification Our ultimate goal is to identify TPs in screenplays without assuming any goldstandard information about their position in the synopsis. We address this with an end-to-end model which first predicts the sentences that act as TPs in the synopsis (e.g., TAM in Section 4.1) and then feeds these predictions to a model which identifies the corresponding TP scenes (e.g., TAM in Section 4.2). 5 Experimental Setup Training We used the Universal Sentence Encoder (USE; Cer et al. 2018) as a pre-trained sentence encoder for all models and tasks; its performance was superior to BERT (Devlin et al., 2018) and other related pre-trained encoders (for more details, see the Appendix). Since the binary labels in both prediction tasks are imbalanced, we apply class weights to the loss function of our models. We weight each class by its inverse frequency in the training set (for more implementation details, see the Appendix). Inference During inference in our first task (i.e., identification of TPs in synopses), we select one sentence per TP. Specifically, we want to track TA Random"
D19-1180,P09-1068,0,0.0636781,"y introduced a dataset consisting of question-answer pairs over 1,572 movie screenplays and books. Previous approaches have focused on finegrained story analysis, such as inducing character types (Bamman et al., 2013, 2014) or understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017). Various approaches have also attempted to analyze the goal and structure of narratives. Black and Wilensky (1979) evaluate the functionality of story grammars in story understanding, Elson and McKeown (2009) develop a platform for representing and reasoning over narratives, and Chambers and Jurafsky (2009) learn fine-grained chains of events. In the context of movie summarization, Gorinski and Lapata (2018) automatically generate an overview of the movie’s genre, mood, and artistic style based on screenplay analysis. Gorinski and Lapata (2015) summarize full length screenplays by extracting an optimal chain of scenes via a graph-based approach centered around the characters of the movie. A similar approach has also been adopted by Vicol et al. (2018), who introduce the MovieGraphs dataset consisting of 51 movies and describe video clips with character-centered graphs. Other work creates animate"
D19-1180,P17-1171,0,0.0228375,"Missing"
D19-1180,P12-1094,0,0.0238116,"a collection of works within a single genre, by an author, or topic) and to develop tools which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-sh"
D19-1180,N19-1246,0,0.0288818,"Missing"
D19-1180,D18-1134,0,0.0320345,"Missing"
D19-1180,E12-1065,0,0.0835436,"y analysis works at the intersection of natural language processing and literary studies, aiming to evaluate various theories of storytelling (e.g., by examining a collection of works within a single genre, by an author, or topic) and to develop tools which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenpla"
D19-1180,Q18-1001,1,0.929083,"omputational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epitasis), and end (catastrophe) of a story (Pavis, 1998). The German novelist and playwright Gustav Freytag modifie"
D19-1180,N15-1113,1,0.905027,"ationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017). Various approaches have also attempted to analyze the goal and structure of narratives. Black and Wilensky (1979) evaluate the functionality of story grammars in story understanding, Elson and McKeown (2009) develop a platform for representing and reasoning over narratives, and Chambers and Jurafsky (2009) learn fine-grained chains of events. In the context of movie summarization, Gorinski and Lapata (2018) automatically generate an overview of the movie’s genre, mood, and artistic style based on screenplay analysis. Gorinski and Lapata (2015) summarize full length screenplays by extracting an optimal chain of scenes via a graph-based approach centered around the characters of the movie. A similar approach has also been adopted by Vicol et al. (2018), who introduce the MovieGraphs dataset consisting of 51 movies and describe video clips with character-centered graphs. Other work creates animated story-boards using the action descriptions of screenplays (Ye and Baldwin, 2008), extracts social networks from screenplays (Agarwal et al., 2014a), or creates xkcd movie narrative charts (Agarwal et al., 2014b). Our work also aims to analy"
D19-1180,N18-1160,1,0.828518,"d works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epitasis), and end (catastrophe) of a story (Pavis, 1998). The German novelist and playwright Gustav Freytag modified Aristotle’s structure by transforming the triangle"
D19-1180,J97-1003,0,0.921975,"rs → − ← − of the forward hi and backward hi LSTM, respec→ − ← − tively: cpi = hi = [ hi ; hi ] (for a more detailed description, see the Appendix). Representation cpi is the input feature vector for our binary classifier. The model is illustrated in Figure 2a. Topic-Aware Model (TAM) TPs by definition act as boundaries between different thematic units in a movie. Furthermore, long documents are usually comprised of topically coherent text segments, each of which contains a number of text passages such as sentences or paragraphs (Salton et al., 1996). Inspired by text segmentation approaches (Hearst, 1997) which measure the semantic similarity between sequential context windows in order to determine topic boundaries, we enhance our representations with a context interaction layer. The objective of this layer is to measure the similarity of the current sentence with its preceding and following context, thereby encoding whether it functions as a boundary between thematic sections. The enriched model with the context interaction layer is illustrated in Figure 2a. After calculating contextualized sentence representations cpi , we compute the representation of the left lci and right rci contexts of"
D19-1180,N16-1180,0,0.40422,"Missing"
D19-1180,P17-1147,0,0.0161104,"enplays, outperforming strong baselines based on state-of-the-art sentence representations and the expected position of TPs. 2 Related Work Recent years have seen increased interest in the automatic analysis of long and complex narratives. Specifically, Machine Reading Comprehension (MRC) and Question Answering (QA) tasks are transitioning from investigating single short and clean articles or queries (Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2016) to large scale datasets that consist of complex stories (Tapaswi et al., 2016; Frermann et al., 2018; Koˇcisk`y et al., 2018; Joshi et al., 2017) or require reasoning across multiple documents (Welbl et al., 2018; Wang et al., 2018; Dua et al., 2019; Yang et al., 2018). Tapaswi et al. (2016) introduce a multi-modal dataset consisting of questions over 140 movies, while Frermann et al. (2018) attempt to answer a single question, namely who is the perpetrator in 39 episodes of the well-known crime series CSI, again based on multi-modal informa1708 1 https://github.com/ppapalampidi/TRIPOD 3 The TRIPOD Dataset The TRIPOD dataset contains 99 screenplays, accompanied with cast information (according to IMDb), and Wikipedia plot synopses anno"
D19-1180,Q18-1023,0,0.024202,"Missing"
D19-1180,D18-1055,0,0.0219139,"Missing"
D19-1180,P14-5010,0,0.00426099,"Missing"
D19-1180,P13-2085,0,0.0213684,"language processing and literary studies, aiming to evaluate various theories of storytelling (e.g., by examining a collection of works within a single genre, by an author, or topic) and to develop tools which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or"
D19-1180,D16-1264,0,0.015539,"tences and 13,403 screenplay scenes) annotated with TPs; and (c) we present an end-toend neural network model that identifies turning points in plot synopses and projects them onto scenes in screenplays, outperforming strong baselines based on state-of-the-art sentence representations and the expected position of TPs. 2 Related Work Recent years have seen increased interest in the automatic analysis of long and complex narratives. Specifically, Machine Reading Comprehension (MRC) and Question Answering (QA) tasks are transitioning from investigating single short and clean articles or queries (Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2016) to large scale datasets that consist of complex stories (Tapaswi et al., 2016; Frermann et al., 2018; Koˇcisk`y et al., 2018; Joshi et al., 2017) or require reasoning across multiple documents (Welbl et al., 2018; Wang et al., 2018; Dua et al., 2019; Yang et al., 2018). Tapaswi et al. (2016) introduce a multi-modal dataset consisting of questions over 140 movies, while Frermann et al. (2018) attempt to answer a single question, namely who is the perpetrator in 39 episodes of the well-known crime series CSI, again based on multi-modal informa1708 1"
D19-1180,D15-1234,0,0.029536,"ng, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epitasis), and end (catastrophe"
D19-1180,D17-1247,0,0.0434,"izing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epitasis), and end (catastrophe) of a story (Pavis"
D19-1180,P18-1178,0,0.0219843,"Missing"
D19-1180,Q18-1021,0,0.0248921,"ntence representations and the expected position of TPs. 2 Related Work Recent years have seen increased interest in the automatic analysis of long and complex narratives. Specifically, Machine Reading Comprehension (MRC) and Question Answering (QA) tasks are transitioning from investigating single short and clean articles or queries (Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2016) to large scale datasets that consist of complex stories (Tapaswi et al., 2016; Frermann et al., 2018; Koˇcisk`y et al., 2018; Joshi et al., 2017) or require reasoning across multiple documents (Welbl et al., 2018; Wang et al., 2018; Dua et al., 2019; Yang et al., 2018). Tapaswi et al. (2016) introduce a multi-modal dataset consisting of questions over 140 movies, while Frermann et al. (2018) attempt to answer a single question, namely who is the perpetrator in 39 episodes of the well-known crime series CSI, again based on multi-modal informa1708 1 https://github.com/ppapalampidi/TRIPOD 3 The TRIPOD Dataset The TRIPOD dataset contains 99 screenplays, accompanied with cast information (according to IMDb), and Wikipedia plot synopses annotated with turning points. The movies were selected from the Script"
D19-1180,D18-1259,0,0.0495527,"ie “Panic Room”. (Thompson, 1999), and by definition they occur at the junctions of acts. Aside from changing narrative direction, TPs define the movie’s structure, tighten the pace, and prevent the narrative from drifting. The five TPs and their definitions are given in Table 1. We propose the task of turning point identification in movies as a means of analyzing their narrative structure. TP identification provides a sequence of key events in the story and segments the screenplay into thematic units. Common approaches to summarization and QA of long or multiple documents (Chen et al., 2017; Yang et al., 2018; Kratzwald and Feuerriegel, 2018; Elgohary et al., 2018) include a retrieval system as the first step, which selects a subset of relevant passages for further processing. However, Koˇcisk`y et al. (2018) demonstrate that these approaches do not perform equally well for extended narratives, since individual passages are very similar and the same entities are referred to throughout the story. We argue that this challenge can be addressed by TP identification, which finds the most important events and segments the narrative into thematic units. Downstream processing for summarization or question"
E06-1044,W05-0620,0,0.02953,"have proper smoothing methods in place. But since this task has some similarity to role labelling, we can also compare the model to a standard role labeller on both the prediction and role labelling tasks. The questions are: How well do we do labelling, and does a standard role labeller also predict human judgements? Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Márquez, 2005). As our model currently focuses on noun phrase arguments only, we do not adopt these test sets but continue to use ours, defining the correct role label to be the one with the higher probability judgement. We evaluate the model on the McRae test set (recall that the other sets only contain good patients/themes and are therefore susceptible to labeller biases). We formulate frequency baselines for our training data. For PropBank, always assigning Arg1 results in F = 45.7 (43.8 on the full test set). For FrameNet, we assign the most frequent role given the verb, so the baseline is F = 34.4 (26."
E06-1044,J02-3001,0,0.0741546,"e of annotation allows us to induce informative verb classes, whereas the PropBank classes introduce noise at most. 6 Experiment 2: Role Labelling We have shown that our model performs well on its intended task of predicting plausibility judgements, once we have proper smoothing methods in place. But since this task has some similarity to role labelling, we can also compare the model to a standard role labeller on both the prediction and role labelling tasks. The questions are: How well do we do labelling, and does a standard role labeller also predict human judgements? Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Márquez, 2005). As our model currently focuses on noun phrase arguments only, we do not adopt these test sets but continue to use ours, defining the correct role label to be the one with the higher probability judgement. We evaluate the model on the McRae test set (recall that the other sets only contain good patients/themes and are therefore susceptible"
E06-1044,J03-3005,1,0.902965,"Human ratings are on a 7-point scale. In order to further test the coverage of our model, we also include 76 items from Trueswell et al. (1994) with one highly plausible object per verb and a rating each for the subject and object reading of the argument. The data were gathered in the same rating study as the McRae et al. data, so we can assume consistency of the ratings. However, in comparison to the McRae data set, the data is impoverished as it lacks ratings for plausible agents (in terms of the example in Table 1, this means there are no ratings for hunter). Lastly, we use 180 items from Keller and Lapata (2003). In contrast with the previous two studies, the verbs and nouns for these data were not handselected for the plausibility of their combination. Rather, they were extracted from the BNC corpus by frequency criteria: Half the verb-noun combinations are seen in the BNC with high, medium and low frequency, half are unseen combinations of the verb set with nouns from the BNC. The data consists of ratings for 30 verbs and 6 arguments each, interpreted as objects. The human ratings were gathered using the Magnitude Estimation technique (Bard et al., 1996). This data set allows us to test on items th"
E06-1044,W04-0803,0,0.0293733,"ms well on its intended task of predicting plausibility judgements, once we have proper smoothing methods in place. But since this task has some similarity to role labelling, we can also compare the model to a standard role labeller on both the prediction and role labelling tasks. The questions are: How well do we do labelling, and does a standard role labeller also predict human judgements? Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Márquez, 2005). As our model currently focuses on noun phrase arguments only, we do not adopt these test sets but continue to use ours, defining the correct role label to be the one with the higher probability judgement. We evaluate the model on the McRae test set (recall that the other sets only contain good patients/themes and are therefore susceptible to labeller biases). We formulate frequency baselines for our training data. For PropBank, always assigning Arg1 results in F = 45.7 (43.8 on the full test set). For Frame"
E95-1045,C94-1039,0,0.0600366,"Missing"
E99-1005,J90-1003,0,0.0545672,"l association between class c and predicate Pi is given in equations (3) and (4). More specifically, selectional association represents the contribution of a particular semantic class c to the total quantity of information provided by a predicate about the semantic class of its argument, when measured as the relative entropy between the prior distriIFor comparison, the filler items had a mean rating of .998. 2Mutual information, though potentially of interest as a measure of collocational status, was not tested due to its well-known property of overemphasising the significance of rare events (Church and Hanks, 1990). 32 Proceedings of EACL '99 Pattern Example adjective noun adjective specifier noun adjective noun noun educational material usual weekly classes environmental health officers Table 1: Example of noun-adjective patterns Adjective hungry guilty temporary naughty High animal verdict job girl Co-occurrence Frequency Band l Medium I Low 1.79 3.91 4.71 2.94 pleasure secret post dog 1.38 2.56 2.07 1.6 application cat cap lunch 0 0 .69 .69 Table 2: Example stimuli (with log co-occurrence frequencies in the BNC) bution of classes p(c) and the posterior distribution p(c I pi) of the argument classes f"
E99-1005,J93-1003,0,0.0342822,"a table containing the adjective and the head of the noun phrase following it. In the case of compound nouns, we only included sequences of two 3. Conditional probability. Our inclusion of the conditional probability, P(noun I adjective), as a predictor variable also relies on the prediction that plausibility is correlated with corpus frequency. It differs from simple co-occurrence frequency in that it additionally takes the overall adjective frequency into account. 4. Coliocational status. We employ the loglikelihood ratio as a measure of the collocational status of the adjective-noun pair (Dunning, 1993; Daille, 1996). If we assume that plausibility differences between strong tea and powerful tea or guilty verdict and guilty cat reflect differences in collocational status (i.e., appearing together more often than expected by their individual occurrence frequencies), as opposed to being semantic in nature, then the log-likelihood ratio may also predict adjective-noun plausibility. 5. Selectional association. Finally, we evaluate plausibility ratings against Resnik's (1993) measure of selectional association. This measure is attractive because it combines statistical 31 Proceedings of E A C L"
E99-1005,P95-1034,0,0.024587,"n be useful in particular for language generation. Consider a generator which has to make a choice between spotless kitchen and flawless kitchen. An empirical model of plausibility could predict that spotless kitchen is a plausible lexical choice, while flawless kitchen is not. Adjective-noun combinations can be hard to generate given their collocational status. For a generator which selects words solely on semantic grounds without taking into account lexical constraints, the choice between spotless kitchen and flawless kitchen may look equivalent. Current work in natural language generation (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) has shown that corpus-based knowledge can be used to address lexical choice noncompositionally. [The senior senator regretted the decision] had ever been made public. [The senior senator regretted the reporter] had ever seen the report. The majority of research has focussed on investigating the effect of rated plausibility for verb-object combinations in human sentence processing (Garnsey et al., 1997; Pickering and Traxler, 1998). However, plausibility effects have also been observed for adjectivenoun combinations in a head-modifier relationship. 30 Proceedings o"
E99-1005,P98-1116,0,0.0181952,"age generation. Consider a generator which has to make a choice between spotless kitchen and flawless kitchen. An empirical model of plausibility could predict that spotless kitchen is a plausible lexical choice, while flawless kitchen is not. Adjective-noun combinations can be hard to generate given their collocational status. For a generator which selects words solely on semantic grounds without taking into account lexical constraints, the choice between spotless kitchen and flawless kitchen may look equivalent. Current work in natural language generation (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) has shown that corpus-based knowledge can be used to address lexical choice noncompositionally. [The senior senator regretted the decision] had ever been made public. [The senior senator regretted the reporter] had ever seen the report. The majority of research has focussed on investigating the effect of rated plausibility for verb-object combinations in human sentence processing (Garnsey et al., 1997; Pickering and Traxler, 1998). However, plausibility effects have also been observed for adjectivenoun combinations in a head-modifier relationship. 30 Proceedings of EACL '99 In the work report"
E99-1005,E95-1016,0,\N,Missing
E99-1005,C98-1112,0,\N,Missing
H05-1104,C00-1027,0,0.609368,"of comprehension priming has also been noted by the speech recognition community (Kuhn and de Mori, 1990), who use so-called caching language models to improve the performance of speech comprehension software. The concept of caching language models is quite simple: a cache of recently seen words is maintained, and the probability of words in the cache is higher than those outside the cache. While the performance of caching language models is judged by their success in improving speech recognition accuracy, it is also possible to use an abstract measure to diagnose their efficacy more closely. Church (2000) introduces such a diagnostic for lexical priming: adaptation probabilities. Adaptation probabilities provide a method to separate the general problem of priming from a particular implementation (i.e., caching models). They measure the amount of priming that occurs for a given construction, and therefore provide an upper limit for the performance of models such as caching models. Adaptation is based upon three concepts. First is the prior, which serves as a baseline. The prior measures the probability of a word appearing, ignoring the presence or absence of a prime. Second is the positive adap"
J03-3005,W99-0901,0,0.245912,"Missing"
J03-3005,W00-1702,0,0.0574941,"provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, ∗ School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information providers such as Lexis Nexis http://www.lexisnexis.com/ might have databases that are even larger than the Web. Lexis Nexis provides"
J03-3005,A97-1052,0,0.0452224,"Missing"
J03-3005,W98-1505,0,0.0090278,"Missing"
J03-3005,N01-1013,0,0.0131765,"counts and frequencies re-created using all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing"
J03-3005,J02-2003,0,0.457246,"Missing"
J03-3005,P02-1030,0,0.0139535,"Missing"
J03-3005,C94-2119,0,0.0378288,"hing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance o"
J03-3005,J93-1005,0,0.473936,"Missing"
J03-3005,C92-3145,0,0.0511375,"Missing"
J03-3005,W02-1030,1,0.760093,"Missing"
J03-3005,N01-1009,0,0.0323944,"Missing"
J03-3005,J02-3004,0,0.00866337,"Missing"
J03-3005,P01-1046,1,0.796807,"Missing"
J03-3005,E99-1005,1,0.745437,"Missing"
J03-3005,P99-1004,0,0.509734,"heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identified in a heuristic manner and may be noisy. 2.2 Sampling Bigrams from the NANTC We also obtained corpus counts from a second corpus, the North American News Text Corpus (NANTC). This corpus differs in several important respects from the BNC. It is substantially larger, as it contains 350 million words of text. Also, it is not a balanced corpus, as it contains material from only one ge"
J03-3005,C94-1103,0,0.0133857,"Missing"
J03-3005,J98-2002,0,0.15736,"sing all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next secti"
J03-3005,C94-1079,0,0.0838201,"Missing"
J03-3005,H01-1046,0,0.0119651,"Missing"
J03-3005,W03-2606,0,0.0550276,"Missing"
J03-3005,A00-2034,0,0.00346172,"es re-created using all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. Howev"
J03-3005,P99-1020,0,0.00472861,"retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, ∗ School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information providers such as Lexis Nexis http://www.lexisnexis.com/ might have databases that are even larger tha"
J03-3005,P93-1024,0,0.767769,"Missing"
J03-3005,C00-2094,0,0.0143606,"Missing"
J03-3005,P98-2177,0,0.0176536,"Missing"
J03-3005,P99-1068,0,0.258611,"ble for NLP is the Web,1 which currently consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, ∗ School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information p"
J03-3005,P99-1014,0,0.189169,"se; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance of Web counts on a standard task from the literature. 3.4 Pseudodisambiguation In the smoothing literature, re-created frequencies are typically evaluated using pseudodisambiguation (Clark and Weir 2001; Dagan, Lee, and Pereira 1999; Lee 1999; Pereira, Tishby, and Lee 1993; Prescher, Riezler, and Rooth 2000; Rooth et al. 1999). 476 Keller and Lapata Web Frequencies for Unseen Bigrams The aim of the pseudodisambiguation task is to decide whether a given algorithm re-creates frequencies that make it possible to distinguish between seen and unseen bigrams in a given corpus. A set of pseudobigrams is constructed according to a set of criteria (detailed below) that ensure that they are unattested in the training corpus. Then the seen bigrams are removed from the training data, and the smoothing method is used to re-create the frequencies of both the seen bigrams and the pseudobigrams. The smoothing method is then evalua"
J03-3005,1999.tc-1.8,0,\N,Missing
J03-3005,H01-1052,0,\N,Missing
J03-3005,P01-1005,0,\N,Missing
J03-3005,C98-2172,0,\N,Missing
J13-4008,J99-2004,0,0.0217554,"hich creates a great number of new prefix trees: At each prediction step, thousands of prediction trees can potentially be combined with all prefix trees; this is computationally not feasible. Non-incremental parsers, which do not use the unlexicalized prediction trees, have to deal with the much lower level of ambiguity among canonical trees (about 50 trees per word on average if using a lexicon the size of our canonical lexicon). In our parser implementation, we use supertagging to select only the best prediction trees in each step, which reduces the search space considerably. Supertagging (Bangalore and Joshi 1999) is a common approach used in the context of TAG and CCG parsing; the idea is to limit the elementary trees for each word to those that are evaluated highly by some shallow statistical model. We only use supertagging for prediction trees; for canonical trees, we use all (lexicalized) trees that the grammar contains for the word (rare words are replaced by “UNK”). 1041 Computational Linguistics Volume 39, Number 4 Because our parser must run incrementally, the supertagger should not be allowed to have any look-ahead. We found, however, that not having any look-ahead has a detrimental impact on"
J13-4008,A00-1031,0,0.0260399,"he leaf node on the spine slpredict , and the probability of some tree with first fringe fpredict and category of the leaf node on the spine slpredict given a prefix tree with current fringe fp and estimated POS tag of the next word twi+1 . A further simplification is that we represent the current fringes fpredict and fp as an alphabetically ordered set of the categories occurring on it. The reasoning behind this decision is that the order of nodes is less important than the identity of the nodes as possible integration sites. The supertagging model is smoothed with the procedure described by Brants (2000), as it yielded better results than WittenBell smoothing (which suffers from data sparsity in the supertagging task). We use one level of back-off where we estimate P( fpredict , slpredict |fp , ti+1 ) based only on the most likely integration site np instead of the whole fringe fp : max P(fpredict , tpredict |np , twi+1 ) np (12) The reason for backing off to the most probable integration site is that a fringe with more unique categories should not have a lower probability of a particular tree adjoining into it than a fringe containing the same category, but fewer other categories. 5. Treeban"
J13-4008,P00-1058,0,0.0649511,"re 4, the head of the S node is sleeps and the head of the NP node is Peter), but could also be the non-lexical leaf of a prediction tree (the head of the upper VP node in the third prefix tree is the lower VP node). The head of any node on the spine of a canonical elementary tree is always the lexical anchor. 3.6 Probability Model We are now ready to define the probability model for PLTAG. This model allows us to define a probability distribution over the derivations of any given PLTAG grammar. It makes the same independence assumptions as standard models for probabilistic TAG (Resnik 1992b; Chiang 2000): Any two applications of derivation rules are statistically independent events. We deviate from these models, however, with regard to what these events are. Earlier approaches always modeled the probability of substituting or adjoining the lower elementary tree, given the upper elementary tree and the integration site. This is inconsistent with the incremental perspective we take here, which assumes that the prefix tree is given, and we must decide how to integrate an elementary tree for the next word with it. We therefore model the probability of substituting, adjoining, or verifying the ele"
J13-4008,J03-4003,0,0.0323503,"ows us to distinguish high and low attachment. The probability models are now obtained via maximum likelihood estimation from the training data. Many of the substitution and adjunction events are seen rarely or not at all with their full contexts, which indicates the need for smoothing. We use back-off with deleted interpolation, as detailed in Table 1. The weight for each of these contexts is automatically determined by a variant of Witten-Bell smoothing, which calculates a weight for each of the back-off levels for each context (Witten and Bell 1991). We implemented the version described by Collins (2003). For the verification operation, data sparsity for the probability of the tree template τv is less of an issue because the probability of a tree template verifying a prediction tree is conditioned only on the identity of the prediction tree and the trace feature. 6. Evaluation In order to compare the PLTAG parser to other probabilistic parsers, we evaluated parsing accuracy on the Penn Treebank (PTB). We first converted the PTB into a PLTAG 1047 Computational Linguistics Volume 39, Number 4 treebank as described in Section 5. We then trained the parser on Sections 2–21 of the Penn Treebank an"
J13-4008,P04-1015,0,0.358872,"Missing"
J13-4008,W12-4623,1,0.833782,"r to the use of type raising in incremental derivations in CCG (Steedman 2000). For example, the prediction tree in Figure 1d effectively raises the NP in Figure 1a to type (S/(SNP)) so that it can compose with the adverb in Figure 1c. Prediction trees, however, are more powerful in terms of the incremental derivations they support: Some psycholinguistically crucial 1032 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar constructions (such as object relative clauses) are handled easily by PLTAG, but are not incrementally derivable in standard CCG (Demberg 2012). According to Demberg, this problem can be overcome by generalizing the CCG categories involved (in the case of object relative clauses, the category of the relative pronoun needs to be changed). 3.3 Verification Markers are eliminated from a partial derived tree through a new operation called verification. Recall that markers indicate nodes that were predicted during the derivation, without having been introduced by a word that was actually observed so far. The verification operation removes these markers by matching them with the nodes of the canonical elementary tree for a word in the sent"
J13-4008,W08-2304,1,0.394592,"incorrect. Presumably, the human sentence processor uses prediction mechanisms to enable efficient comprehension in real time. The three concepts of incrementality, connectedness, and prediction are fundamentally interrelated: Maintaining connected partial analyses is only nontrivial if the parsing process is incremental, and prediction means that a connected analysis is required also for words the parser has not yet seen. In this article, we exploit the interrelatedness of incrementality, connectedness, and prediction to develop a parsing model for psycholinguistically motivated TAG (PLTAG; Demberg and Keller 2008b). This formalism augments standard tree-adjoining grammar (TAG; Joshi, Levy, and Takahashi 1975) with a predictive lexicon and a verification operation for validating predicted structures. As we show in Section 2, these operations are motivated by psycholinguistic findings. We argue that our PLTAG parser can form the basis for a new model of human sentence processing. We successfully evaluate the predictions of this model against reading time data from an eye-tracking corpus, showing that it provides a better fit with the psycholinguistic data than the standard surprisal model of human sente"
J13-4008,W12-1706,0,0.0167801,"a (Demberg and Keller 2008a). It is important to note, though, that adding verification cost to the baseline LME model increases model fit significantly, which provides some evidence for effectiveness of the verification cost component. 10 The result for Roark structural surprisal differs from that reported by Demberg and Keller (2008a) and Demberg-Winterfors (2010). This can be attributed to the different outlier removal and more conservative treatment of random effects in the present article. 11 Surprisal has subsequently been reported to be a significant predictor of Dundee reading time by Fossum and Levy (2012), who used a context-free grammar induced using the state-split model of Petrov and Klein (2007) in combination with a standard probabilistic Earley parser to compute surprisal estimates. 1056 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar Table 4 Linear mixed effects models of first-pass time for predictors of theoretical interest: Prediction Theory cost, PLTAG surprisal, PLTAG verification cost, Roark lexical surprisal, and Roark structural surprisal, each residualized against low-level predictors (see text for details). Random intercepts of pa"
J13-4008,N01-1021,0,0.605496,"emental parser is to develop a more realistic model of human language processing. A treebank-based evaluation as in the previous section does not directly provide evidence of psycholinguistic validity; however, a parser with good coverage and high parsing accuracy is a prerequisite for an evaluation on eye-tracking corpora, which Keller (2010) argues are the benchmark for models of human sentence processing. In what follows, we report an evaluation study that uses our PLTAG parser to predict human reading times, and compares its performance on this task to a standard model based on surprisal (Hale 2001). Surprisal assumes that processing difficulty is associated with expectations built up by the sentence processor: A word that is unexpected given its preceding context is harder to process. Mathematically, the amount of surprisal at word wi can be formalized as the negative logarithm of the conditional probability of wi given the preceding words in the sentence w1 . . . wi−1 : Surprisalwi = − log P(wi |w1 . . . wi−1 ) (13) P(w1 . . . wi ) P(w1 . . . wi−1 )  = − log P(τpw ...w ) + log = − log τpw 1 i 1 ...wi  τpw P(τpw 1 ...wi−1 ) 1 ...wi−1 Here, P(τpw ...w ) is the probability of the prefix"
J13-4008,J07-3004,0,0.0406482,"Missing"
J13-4008,C92-2066,0,0.739181,"his property is essential for testing psycholinguistic models on realistic data, including eye-tracking corpora. The PLTAG formalism was first proposed by Demberg-Winterfors (2010), who also presents an earlier version of the parsing algorithm, probability model, implementation, and evaluation described in the current article. 3. The PLTAG Formalism We start by introducing the PLTAG formalism, which we will use throughout the article. 3.1 Incremental TAG Parsing Tree Adjoining Grammar (TAG) is a grammar formalism based on combining trees. In what follows we will focus on lexicalized TAG (TAG; Joshi and Schabes 1992), which is the most widely used version of TAG. In this formalism, a TAG lexicon consists of a finite set of elementary trees whose nodes are labeled with nonterminal or terminal symbols. Each elementary tree contains an anchor, a leaf node labeled with a terminal symbol. At most one other leaf—the foot node—may carry a label of the form A∗, where A is a nonterminal symbol. All other leaves are substitution nodes and labeled with symbols of the form A↓. Elementary trees that contain a foot node are called auxiliary trees; those that contain no foot nodes are initial trees. We will generally ca"
J13-4008,kaeshammer-demberg-2012-german,1,0.886542,"Missing"
J13-4008,W04-0302,0,0.0731294,"Missing"
J13-4008,P10-2012,1,0.943603,"psycholinguistically motivated way. We achieve this by exploiting the fact that these three concepts are closely related: In order to guarantee that the syntactic structure of a sentence prefix is fully connected, it may be necessary to build phrases whose lexical anchors (the words that they relate to) have not been encountered yet. In other words, the parser needs to predict upcoming syntactic structure in order to ensure connectedness. This prediction scheme is complemented by an explicit verification mechanism in our approach. Furthermore, unlike most existing psycholinguistic models (see Keller 2010 for an overview), our model achieves broad coverage and acceptable parsing performance on a standard test corpus. This property is essential for testing psycholinguistic models on realistic data, including eye-tracking corpora. The PLTAG formalism was first proposed by Demberg-Winterfors (2010), who also presents an earlier version of the parsing algorithm, probability model, implementation, and evaluation described in the current article. 3. The PLTAG Formalism We start by introducing the PLTAG formalism, which we will use throughout the article. 3.1 Incremental TAG Parsing Tree Adjoining Gr"
J13-4008,P10-1021,1,0.378233,"Missing"
J13-4008,W04-0308,0,0.157879,"Missing"
J13-4008,N07-1051,0,0.0145811,"the baseline LME model increases model fit significantly, which provides some evidence for effectiveness of the verification cost component. 10 The result for Roark structural surprisal differs from that reported by Demberg and Keller (2008a) and Demberg-Winterfors (2010). This can be attributed to the different outlier removal and more conservative treatment of random effects in the present article. 11 Surprisal has subsequently been reported to be a significant predictor of Dundee reading time by Fossum and Levy (2012), who used a context-free grammar induced using the state-split model of Petrov and Klein (2007) in combination with a standard probabilistic Earley parser to compute surprisal estimates. 1056 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar Table 4 Linear mixed effects models of first-pass time for predictors of theoretical interest: Prediction Theory cost, PLTAG surprisal, PLTAG verification cost, Roark lexical surprisal, and Roark structural surprisal, each residualized against low-level predictors (see text for details). Random intercepts of participant and random slopes under participants for the predictors of interest were also included"
J13-4008,C92-1032,0,0.693292,"plicit prediction and verification mechanism (WCDG includes prediction, but not verification), which means that they cannot be used to model psycholinguistic results that involve verification cost.1 A simple form of prediction can be achieved in a chart parser (incomplete edges in the chart can be seen as predictive), but in order to maintain psycholinguistic plausibility, an arc-eager left-corner parsing strategy needs to be used. Other parsing strategies fail to predict human processing difficulty that arises in certain cases, such as for center embedding (Thompson, Dixon, and Lamping 1991; Resnik 1992a). This is an argument against using a top–down parser such as Roark’s for psycholinguistic modeling. Furthermore, it is important to emphasize that a full model of human parsing needs to not only model prediction, but also account for processing difficulty associated with the verification of predictions (we will return to this point in Section 7). None of the existing incremental parsing models includes an explicit verification component. 1 As Demberg and Keller (2009) show, some psycholinguistic results can be accounted for by a model without verification, such as the either . . . or findin"
J13-4008,C92-2065,0,0.795513,"plicit prediction and verification mechanism (WCDG includes prediction, but not verification), which means that they cannot be used to model psycholinguistic results that involve verification cost.1 A simple form of prediction can be achieved in a chart parser (incomplete edges in the chart can be seen as predictive), but in order to maintain psycholinguistic plausibility, an arc-eager left-corner parsing strategy needs to be used. Other parsing strategies fail to predict human processing difficulty that arises in certain cases, such as for center embedding (Thompson, Dixon, and Lamping 1991; Resnik 1992a). This is an argument against using a top–down parser such as Roark’s for psycholinguistic modeling. Furthermore, it is important to emphasize that a full model of human parsing needs to not only model prediction, but also account for processing difficulty associated with the verification of predictions (we will return to this point in Section 7). None of the existing incremental parsing models includes an explicit verification component. 1 As Demberg and Keller (2009) show, some psycholinguistic results can be accounted for by a model without verification, such as the either . . . or findin"
J13-4008,J01-2004,0,0.0519268,"oach, however, allows multiple unconnected subtrees for a sentence prefix and uses a look-ahead of two words, that is, it does not build connected structures. An example of a TAG parser that is both incremental and builds connected structures is the work of Kato, Matsubara, and Inagaki (2004). This comes at the price of strong simplifying assumptions with respect to the TAG formalism, such as not distinguishing modifiers and arguments. (We will return to a discussion of other TAG parsers in Section 6.1.) An example of an incremental parser based on context-free grammars is the one proposed by Roark (2001). That parser uses a top–down algorithm to build fully connected structures; it is also able to compute probabilities for sentence prefixes, which makes it attractive for psycholinguistic modeling, where prefix probabilities are often used to predict human processing difficulty (see Section 7 for details). The Roark parser has been shown to successfully model psycholinguistic data from eye-tracking corpora (Demberg and Keller 2008a; Frank 2009) and other reading time data (Roark et al. 2009). It therefore is a good candidate for a broad-coverage model of human parsing, and 1028 Demberg, Keller"
J13-4008,D09-1034,0,0.0796811,"rs in Section 6.1.) An example of an incremental parser based on context-free grammars is the one proposed by Roark (2001). That parser uses a top–down algorithm to build fully connected structures; it is also able to compute probabilities for sentence prefixes, which makes it attractive for psycholinguistic modeling, where prefix probabilities are often used to predict human processing difficulty (see Section 7 for details). The Roark parser has been shown to successfully model psycholinguistic data from eye-tracking corpora (Demberg and Keller 2008a; Frank 2009) and other reading time data (Roark et al. 2009). It therefore is a good candidate for a broad-coverage model of human parsing, and 1028 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar will serve as a standard of comparison for the model proposed in the current article in Section 7. The Roark parser has been extended with discriminative training (Collins and Roark 2004), resulting in a boost in parsing accuracy. Prefix probabilities cannot be computed straightforwardly in a discriminative framework, however, making this approach less interesting from a psycholinguistic modeling point of view. W"
J13-4008,J10-1001,0,0.0422888,"Missing"
J13-4008,H05-1102,0,0.364427,"ent work has provided evidence for connectedness in a range of other phenomena, including sluicing and ellipsis (Aoshima, Yoshida, and Phillips 2009; Yoshida, Walsh-Dickey, and Sturt 2013). 2.2 Incremental Parsing Models In the previous section, we identified incrementality, connectedness, and prediction as key desiderata for computational models of human parsing. In what follows, we will review work on parsing in computational linguistics in the light of these desiderata. Incremental parsers for a range of grammatical formalisms have been proposed in the literature. An example is the work of Shen and Joshi (2005), who propose an efficient incremental parser for a variant of TAG, spinal TAG. This approach, however, allows multiple unconnected subtrees for a sentence prefix and uses a look-ahead of two words, that is, it does not build connected structures. An example of a TAG parser that is both incremental and builds connected structures is the work of Kato, Matsubara, and Inagaki (2004). This comes at the price of strong simplifying assumptions with respect to the TAG formalism, such as not distinguishing modifiers and arguments. (We will return to a discussion of other TAG parsers in Section 6.1.) A"
J13-4008,P91-1012,0,0.572012,"Missing"
J13-4008,P07-1031,0,0.0242415,"r, we first retrieve the elementary trees for the upcoming lexeme. If the word occurred with more than one POS tag, we choose the POS tag with highest conditional probability given the previous two POS tags. 1042 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar of treebank conversion and lexicon induction here; the reader is referred to DembergWinterfors (2010) for full details. Our PLTAG lexicon (both canonical trees and prediction trees) is derived from the Wall Street Journal section of the Penn Treebank, complemented by noun phrase annotation (Vadas and Curran 2007), and Propbank (Palmer, Gildea, and Kingsbury 2003), as well as a slightly modified version of the head percolation table of Magerman (1994). These additional resources are used to determine the elementary trees for a TAG lexicon, following the procedures proposed by Xia, Palmer, and Joshi (2000). This involves first adding noun phrase annotation to the Penn Treebank, and then determining heads with the head percolation table, augmented with more detailed heuristics for noun phrases.4 As a next step, information from Propbank is used to establish argument and modifier status and to determine w"
J13-4008,C88-2147,0,0.119973,"e as only having an upper half, which again makes a whole node. We assume that lexical leaves only have an upper half, too; this makes no difference, as no substitution or adjunction can be performed on those nodes anyway. The process is illustrated in Figure 3, which shows the recombination of node halves from different elementary trees in the adjunction step of Figure 2: Black node halves come from the elementary tree for sleeps, gray node halves from Peter, and white ones from often. The idea of distinguishing upper and lower node halves that are pushed apart by adjunction comes from FTAG (Vijay-Shanker and Joshi 1988), which equips each node half with a separate feature structure; at the end of the derivation process, the upper and lower feature structures of each node are unified with each other. Node halves will also play a crucial role in PLTAG. 3.2 Prediction Trees We have argued earlier that a psycholinguistic model of sentence processing should be incremental. In the context of TAG and related formalisms, this means that a derivation Figure 3 Fine structure of adjunction. The semicircles represent node halves; all node halves from the same elementary tree are drawn in the same color. 1031 Computation"
J13-4008,P10-1121,0,0.0482399,"Missing"
J13-4008,W00-1307,0,0.27522,"Missing"
J13-4008,N01-1023,0,\N,Missing
J13-4008,P95-1037,0,\N,Missing
J13-4008,J05-1004,0,\N,Missing
N04-1016,P03-1059,0,0.0119155,"rigram model ( f (n1 , p, n2 )); it significantly outperformed the best BNC model. The comparison with the literature in Table 11 showed that the best Altavista model significantly outperformed both the baseline and the best model in the literature (Lauer’s word-based model). The BNC model, on the other hand, Model f (n) f (det, n) f (det, n)/ f (n) Backoff Altavista Count Uncount 87.01 90.13 88.38#6 ∗ 91.22#6 ∗ 83.19 85.38 87.01 89.80 BNC Count Uncount 87.32# 90.39# 51.01 50.23 50.95 50.23 – – Table 12: Performance of Altavista counts and BNC counts for noun countability detection (data from Baldwin and Bond 2003) achieved a performance that is not significantly different from the baseline, and significantly worse than Lauer’s best model. 8 Noun Countability Detection The next analysis task that we consider is the problem of determining the countability of nouns. Countability is the semantic property that determines whether a noun can occur in singular and plural forms, and affects the range of permissible modifiers. In English, nouns are typically either countable (e.g., one dog, two dogs ) or uncountable (e.g., some peace, *one peace, *two peaces ). Baldwin and Bond (2003) propose a method for automa"
N04-1016,H01-1052,0,0.0181579,"f different parts of speech (Keller and Lapata 2003 only tested bigrams involving nouns, verbs, and adjectives). Another important question is whether web-based methods, which are by definition unsupervised, can be competitive alternatives to supervised approaches used for most tasks in the literature. This paper aims to address these questions. We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a,b). Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun countability detection. Table 1 gives an overview of these tasks and their properties. In all cases, we propose a simple, unsupervised n-gram based model whose parameters are estimated using web counts. We compare this model both against a baseline (same model, but parameters estimated on the BNC) and agai"
N04-1016,P01-1005,0,0.0146891,"f different parts of speech (Keller and Lapata 2003 only tested bigrams involving nouns, verbs, and adjectives). Another important question is whether web-based methods, which are by definition unsupervised, can be competitive alternatives to supervised approaches used for most tasks in the literature. This paper aims to address these questions. We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a,b). Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun countability detection. Table 1 gives an overview of these tasks and their properties. In all cases, we propose a simple, unsupervised n-gram based model whose parameters are estimated using web counts. We compare this model both against a baseline (same model, but parameters estimated on the BNC) and agai"
N04-1016,W02-1005,0,0.0126876,"ich word in a confusion set is the correct one in a given context. This choice can be either syntactic (as for {then, than}) or semantic (as for {principal, principle}). A number of machine learning methods have been proposed for context-sensitive spelling correction. These include a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations. Context word features record the presence of a word within a fixed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are usually represented by a small number of words and/or partof-speech tags to the left or right of the target word. The results obtained by a variety of classification methods are given in Table 6. All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (19"
N04-1016,J94-4004,0,0.0259447,"Missing"
N04-1016,W95-0104,0,0.0537882,"alternative surface realizations of a word. This choice is typically modeled by confusion sets such as {principal, principle} or {then, than} under the assumption that each word in the set could be mistakenly typed when another word in the set was intended. The task is to infer which word in a confusion set is the correct one in a given context. This choice can be either syntactic (as for {then, than}) or semantic (as for {principal, principle}). A number of machine learning methods have been proposed for context-sensitive spelling correction. These include a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations. Context word features record the presence of a word within a fixed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are us"
N04-1016,P96-1010,0,0.0150028,"rface realizations of a word. This choice is typically modeled by confusion sets such as {principal, principle} or {then, than} under the assumption that each word in the set could be mistakenly typed when another word in the set was intended. The task is to infer which word in a confusion set is the correct one in a given context. This choice can be either syntactic (as for {then, than}) or semantic (as for {principal, principle}). A number of machine learning methods have been proposed for context-sensitive spelling correction. These include a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations. Context word features record the presence of a word within a fixed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are usually represented by a small"
N04-1016,C94-1042,0,0.0117228,"tion The next analysis task that we consider is the problem of determining the countability of nouns. Countability is the semantic property that determines whether a noun can occur in singular and plural forms, and affects the range of permissible modifiers. In English, nouns are typically either countable (e.g., one dog, two dogs ) or uncountable (e.g., some peace, *one peace, *two peaces ). Baldwin and Bond (2003) propose a method for automatically learning the countability of English nouns from the BNC. They obtain information about noun countability by merging lexical entries from COMLEX (Grishman et al., 1994) and the ALTJ/E Japanese-to-English semantic transfer dictionary (Ikehara et al., 1991). Words are classified into four classes: countable, uncountable, bipartite (e.g., trousers ), and plural only (e.g., goods ). A memory-based classifier is used to learn the four-way distinction on the basis of several linguistically motivated features such as: number of the head noun, number of the modifier, subject-verb agreement, plural determiners. We devised unsupervised models for the countability learning task and evaluated their performance on Baldwin and Bond’s (2003) test data. We concentrated sole"
N04-1016,1991.mtsummit-papers.16,0,0.00821492,"ity of nouns. Countability is the semantic property that determines whether a noun can occur in singular and plural forms, and affects the range of permissible modifiers. In English, nouns are typically either countable (e.g., one dog, two dogs ) or uncountable (e.g., some peace, *one peace, *two peaces ). Baldwin and Bond (2003) propose a method for automatically learning the countability of English nouns from the BNC. They obtain information about noun countability by merging lexical entries from COMLEX (Grishman et al., 1994) and the ALTJ/E Japanese-to-English semantic transfer dictionary (Ikehara et al., 1991). Words are classified into four classes: countable, uncountable, bipartite (e.g., trousers ), and plural only (e.g., goods ). A memory-based classifier is used to learn the four-way distinction on the basis of several linguistically motivated features such as: number of the head noun, number of the modifier, subject-verb agreement, plural determiners. We devised unsupervised models for the countability learning task and evaluated their performance on Baldwin and Bond’s (2003) test data. We concentrated solely on countable and uncountable nouns, as they account for the vast majority of the dat"
N04-1016,A97-1025,0,0.0168859,"on that each word in the set could be mistakenly typed when another word in the set was intended. The task is to infer which word in a confusion set is the correct one in a given context. This choice can be either syntactic (as for {then, than}) or semantic (as for {principal, principle}). A number of machine learning methods have been proposed for context-sensitive spelling correction. These include a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations. Context word features record the presence of a word within a fixed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are usually represented by a small number of words and/or partof-speech tags to the left or right of the target word. The results obtained by a variety of classification methods"
N04-1016,J03-3005,1,0.612398,"s. The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams. For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus. However, in most cases, web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora. We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models. 1 Introduction Keller and Lapata (2003) investigated the validity of web counts for a range of predicate-argument bigrams (verbobject, adjective-noun, and noun-noun bigrams). They presented a simple method for retrieving bigram counts from the web by querying a search engine and demonstrated that web counts (a) correlate with frequencies obtained from a carefully edited, balanced corpus such as the 100M words British National Corpus (BNC), (b) correlate with frequencies recreated using smoothing methods in the case of unseen bigrams, (c) reliably predict human plausibility judgments, and (d) yield state-of-the-art performance on ps"
N04-1016,P95-1007,0,0.740742,"right branching analysis if [n2 n3 ] is more likely than [n1 n2 ]. The dependency model compares [n1 n2 ] against [n1 n3 ] and adopts a right branching analysis if [n1 n3 ] is more likely than [n1 n2 ]. The simplest model of compound noun disambiguation compares the frequencies of the two competing analyses and opts for the most frequent one (Pustejovsky et al., Alta 63.93 77.86 78.68#∗ 68.85 70.49 80.32 68.03 71.31 61.47 65.57 75.40 BNC 63.93 66.39 65.57 65.57 63.11 66.39 63.11 67.21 62.29 57.37 68.03# Table 8: Performance of Altavista counts and BNC counts for compound bracketing (data from Lauer 1995) Model Baseline Best BNC Lauer (1995): adjacency Lauer (1995): dependency Best Altavista Lauer (1995): tuned Upper bound Accuracy 63.93 68.036 †‡ 68.90 77.50 78.68†6 ‡ 80.70 81.50 Table 9: Performance comparison with the literature for compound bracketing 1993). Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. He uses a probability ratio to compare the probability of the leftbranching analysis to that of the right-branching (see (4) for the dependency model and (5) for the adjacency model). ∑ P(t1 → t2"
N04-1016,P00-1012,0,0.015274,"hat the relative order of premodifiers is fixed, and independent of context and the noun being modified. The simplest strategy is what Shaw and Hatzivassiloglou (1999) call direct evidence. Given an adjective pair {a, b}, they count how many times ha, bi and hb, ai appear in the corpus and choose the pair with the highest frequency. Unfortunately the direct evidence method performs poorly when a given order is unseen in the training data. To compensate for this, Shaw and Hatzivassiloglou (1999) propose to compute the transitive closure of the ordering relation: if a ≺ c and c ≺ b, then a ≺ b. Malouf (2000) further proposes a back-off bigram model of adjective pairs for choosing among alternative orders (P(ha, bi|{a, b}) vs. P(hb, ai|{a, b})). He also proposes positional probabilities as a means of estimating how likely it is for a given adjective a to appear first in a sequence by looking at each pair in the training data that contains the adjective a and recording its position. Finally, he uses memory-based learning as a means to encode morphological and semantic similarities among different adjective orders. Each adjective pair ab is encoded as a vector of 16 features (the last eight characte"
N04-1016,J93-2004,0,0.0350948,"Missing"
N04-1016,C00-2094,0,0.0105397,"Missing"
N04-1016,J93-2005,0,0.0204803,"supervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. He uses a probability ratio to compare the probability of the leftbranching analysis to that of the right-branching (see (4) for the dependency model and (5) for the adjacency model). ∑ P(t1 → t2 )P(t2 → t3 ) (4) Rdep = ti ∈cats(wi ) ∑ P(t1 → t3 )P(t2 → t3 ) ∑ P(t1 → t2 ) ∑ P(t2 → t3 ) ti ∈cats(wi ) Bracketing of Compound Nouns The first analysis task we consider is the syntactic disambiguation of compound nouns, which has received a fair amount of attention in the NLP literature (Pustejovsky et al., 1993; Resnik, 1993; Lauer, 1995). The task can be summarized as follows: given a three word compound n1 n3 n3 , determine the correct binary bracketing of the word sequence (see (3) for an example). (3) Model Baseline f (n1 , n2 ) : f (n2 , n3 ) f (n1 , n2 ) : f (n1 , n3 ) f (n1 , n2 )/ f (n1 ) : f (n2 , n3 )/ f (n2 ) f (n1 , n2 )/ f (n2 ) : f (n2 , n3 )/ f (n3 ) f (n1 , n2 )/ f (n2 ) : f (n1 , n3 )/ f (n3 ) f (n1 , n2 ) : f (n2 , n3 ) (NEAR) f (n1 , n2 ) : f (n1 , n3 ) (NEAR) f (n1 , n2 )/ f (n1 ) : f (n2 , n3 )/ f (n2 ) (NEAR) f (n1 , n2 )/ f (n2 ) : f (n2 , n3 )/ f (n3 ) (NEAR) f (n1 , n2 )/ f"
N04-1016,P99-1018,0,0.0148518,"ovement on confusion sets whose words belong to different parts of speech). An advantage of our method is that it can be used for a large number of confusion sets without relying on the availability of training data. 5 Ordering of Prenominal Adjectives The ordering of prenominal modifiers is important for natural language generation systems where the text must be both fluent and grammatical. For example, the sequence big fat Greek wedding is perfectly acceptable, whereas fat Greek big wedding sounds odd. The ordering of prenominal adjectives has sparked a great deal of theoretical debate (see Shaw and Hatzivassiloglou 1999 for an overview) and efforts have concentrated on defining rules based on semantic criteria that account for different orders (e.g., age ≺ color, value ≺ dimension). Data intensive approaches to the ordering problem rely on corpora for gathering evidence for the likelihood of different orders. They rest on the hypothesis that the relative order of premodifiers is fixed, and independent of context and the noun being modified. The simplest strategy is what Shaw and Hatzivassiloglou (1999) call direct evidence. Given an adjective pair {a, b}, they count how many times ha, bi and hb, ai appear in"
N04-1016,1999.tc-1.8,0,\N,Missing
N06-2031,P05-1022,0,0.00889376,"stical analysis in this paper aims to make headway towards such a model. Recently, priming phenomena1 have been exploited to aid automated processing, for instance in automatic speech recognition using cache models, but only recently have attempts been made at using 1 The term priming refers to a process that influences linguistic decision-making. An instance of priming occurs when a syntactic structure or lexical item giving evidence of a linguistic choice (prime) influences the recipient to make the same decision, i.e. re-use the structure, at a later choice-point (target). them in parsing (Charniak and Johnson, 2005). In natural language generation, repetition can be used to increase the alignment of human and computers. A surface-level approach is possible by biasing the n-gram language model used to select the output string from a variety of possible utterances (Brockmann et al., 2005). Priming effects are common and well known. For instance, speakers access lexical items more quickly after a semantically or phonologically similar prime. Recent work demonstrates large effects for particular synonymous alternations (e.g., active vs. passive voice) using traditional laboratory experiments with human subje"
N06-2031,H94-1020,0,0.0199256,"in task-oriented dialogue. A recent psychological perspective models Interactive Alignment between speakers (Pickering and Garrod, 2004), where mutual understanding about task and situation depends on lower-level priming effects. Under the model, we expect priming effects to be stronger when a task requires highlevel alignment of situation models. 2 Method 2.1 Dialogue types We examined two corpora. Switchboard contains 80,000 utterances of spontaneous spoken conversations over the telephone among randomly paired, North American speakers, syntactically annotated with phrase-structure grammar (Marcus et al., 1994). The HCRC Map Task corpus comprises more than 110 dialogues with a total of 20, 400 utterances (Anderson et al., 1991). Like Switchboard, HCRC Map Task is a corpus of spoken, two-person dialogue in English. However, Map Task contains task-oriented dialogue: interlocutors work together to achieve a task as quickly and efficiently as possible. Subjects were asked to give each other directions with the help of a map. The interlocutors are in the same room, but have separate, slightly different maps and are unable to see each other’s maps. 2.2 Syntactic repetitions Both corpora are annotated with"
N07-1044,briscoe-carroll-2002-robust,0,0.0386572,"Missing"
N07-1044,P06-1013,1,0.84589,"ghly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottleneck by inferring the first sense automatically from raw text. Automatically acquired first senses will undoubtedly be noisy when compared to human annotations. Nevertheless, they can be usefully employed in two important tasks: (a) to create preliminary annotations, thus supporting the “annotate automatically, correct manually” methodology used to provide high volume annotation in the Penn Treebank project; and (b) in combination with supervised WSD methods that take context into account; for instance, such methods could default to the dominant sens"
N07-1044,W04-0827,0,0.164611,"Missing"
N07-1044,P04-1036,0,0.0645709,"4 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available. Furthermore, current supervised approaches rarely outperform the simple heuristic of choosing the most common or dominant sense in the training data (henceforth “the first sense heuristic”), despite taking local context into account. One reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottl"
N07-1044,W04-0837,0,0.0710926,"4 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available. Furthermore, current supervised approaches rarely outperform the simple heuristic of choosing the most common or dominant sense in the training data (henceforth “the first sense heuristic”), despite taking local context into account. One reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottl"
N07-1044,E06-1016,0,0.268159,"reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottleneck by inferring the first sense automatically from raw text. Automatically acquired first senses will undoubtedly be noisy when compared to human annotations. Nevertheless, they can be usefully employed in two important tasks: (a) to create preliminary annotations, thus supporting the “annotate automatically, correct manually” methodology used to provide high volume annotation in the Penn Treebank project; and (b) in combination with supervised WSD methods that take context into account; for instance, such methods could default"
N07-1044,S01-1005,0,0.0353219,"ssociations between words and sense descriptions automatically by querying an IR engine whose index terms have been compiled from the corpus of interest. The approach is inexpensive, languageindependent, requires minimal supervision, and uses no additional knowledge other than the word senses proper and morphological query expansions. We 348 Proceedings of NAACL HLT 2007, pages 348–355, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics evaluate our method on two tasks. First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) data sets. Second, we simulate native speakers’ intuitions about the salience of word meanings and examine whether the estimated sense frequencies correlate with sense production data. In all cases our approach outperforms a naive baseline and yields performances comparable to state of the art. In the following section, we provide an overview of existing work on sense ranking. In Section 3, we introduce our IR-based method, and describe several sense ranking models. In Section 4, we present our results. Discussion of our results and future work conclud"
N07-1044,W04-0811,0,0.0145937,"escriptions automatically by querying an IR engine whose index terms have been compiled from the corpus of interest. The approach is inexpensive, languageindependent, requires minimal supervision, and uses no additional knowledge other than the word senses proper and morphological query expansions. We 348 Proceedings of NAACL HLT 2007, pages 348–355, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics evaluate our method on two tasks. First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) data sets. Second, we simulate native speakers’ intuitions about the salience of word meanings and examine whether the estimated sense frequencies correlate with sense production data. In all cases our approach outperforms a naive baseline and yields performances comparable to state of the art. In the following section, we provide an overview of existing work on sense ranking. In Section 3, we introduce our IR-based method, and describe several sense ranking models. In Section 4, we present our results. Discussion of our results and future work conclude the paper (Section 5). 2 Related Work M"
N07-1044,H05-1051,0,0.0211367,"etween a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers’ intuitions. 1 Introduction Word sense disambiguation (WSD), the ability to identify the intended meanings (senses) of words in context, is crucial for accomplishing many NLP tasks that require semantic processing. Examples include paraphrase acquisition, discourse parsing, or metonymy resolution. Applications such as machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005) have also been shown to benefit from WSD. Given the importance of WSD for basic NLP tasks and multilingual applications, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data (see Yarowsky and Florian 2002; Mihalcea and Edmonds 2004 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and th"
N07-1044,H05-1097,0,0.0166808,"al engine to estimate the degree of association between a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers’ intuitions. 1 Introduction Word sense disambiguation (WSD), the ability to identify the intended meanings (senses) of words in context, is crucial for accomplishing many NLP tasks that require semantic processing. Examples include paraphrase acquisition, discourse parsing, or metonymy resolution. Applications such as machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005) have also been shown to benefit from WSD. Given the importance of WSD for basic NLP tasks and multilingual applications, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data (see Yarowsky and Florian 2002; Mihalcea and Edmonds 2004 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words fo"
N16-1022,W03-0601,0,0.338471,"2016 Association for Computational Linguistics Dataset Verbs Acts Images Sen Des PPMI (Yao and Fei-Fei, 2010) 2 24 4800 N N Stanford 40 Actions (Yao et al., 2011) 33 40 9532 N N PASCAL 2012 (Everingham et al., 2015) 9 11 4588 N N 89 Actions (Le et al., 2013) 36 89 2038 N N TUHOI (Le et al., 2014) – 2974 10805 N N COCO-a (Ronchi and Perona, 2015) 140 162 10000 N Y HICO (Chao et al., 2015) 111 600 47774 Y N VerSe (our dataset) 90 163 3518 Y Y Figure 2: Google Image Search trying to disambiguate sit. All clusters pertain to the sit down sense, other senses (baby sit, convene) are not included. (Barnard et al., 2003; Loeff et al., 2006; Saenko and Darrell, 2008; Chen et al., 2015). VSD for nouns is helped by resources such as ImageNet (Deng et al., 2009), a large image database containing 1.4 million images for 21,841 noun synsets and organized according to the WordNet hierarchy. However, we are not aware of any previous work on VSD for verbs, and no ImageNet for verbs exists. Not only image retrieval would benefit from VSD for verbs, but also other multimodal tasks that have recently received a lot of interest, such as automatic image description and visual question answering (Karpathy and Li, 2015; Fan"
N16-1022,C08-1009,1,0.771948,"ction recognition datasets. Acts (actions) are verb-object pairs; Sen indicates whether sense ambiguity is explicitly handled; Des indicates whether image descriptions are included. 2 Related Work There is an extensive literature on word sense disambiguation for nouns, verbs, adjectives and adverbs. Most of these approaches rely on lexical databases or sense inventories such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). Unsupervised WSD approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with B"
N16-1022,N06-2015,0,0.0281592,"f the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with Barnard et al. (2003). Sense discrimination for web images was introduced by Loeff et al. (2006), who used spectral clustering over multimodal features from the images and web text. Saenko and Darrell (2008) used sense definitions in a dictionary to learn a latent LDA space overs senses, which they then used to construct sensespecific classifiers by exploiting the text surrounding an image. 2.1 Related Datasets Most of the datasets relevant for verb sense disambiguation were created by the computer vision community for the task of human action recognition (see Table 1 for an overview). These datasets are annotated with a limited number of actions, where an action is conceptualized as ver"
N16-1022,N15-1070,0,0.00980158,"distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with Barnard et al. (2003). Sense discrimination for web images was introduced by Loeff et al. (2006), who used spectral clustering over multimodal features from the images and web text. Saenko and Darrell (2008) used sense definitions in a dictionary to learn a latent LDA space overs senses, which they then used to construct sensespecific classifiers by exploiting the text surrounding an image. 2.1 Related Datasets Most of the datasets relevant for verb sense disambiguation were creat"
N16-1022,P97-1009,0,0.318417,"omparison of VerSe with existing action recognition datasets. Acts (actions) are verb-object pairs; Sen indicates whether sense ambiguity is explicitly handled; Des indicates whether image descriptions are included. 2 Related Work There is an extensive literature on word sense disambiguation for nouns, verbs, adjectives and adverbs. Most of these approaches rely on lexical databases or sense inventories such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). Unsupervised WSD approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sens"
N16-1022,P06-2071,0,0.0595377,"Missing"
N16-1022,P04-1036,0,0.0604996,"f VerSe with existing action recognition datasets. Acts (actions) are verb-object pairs; Sen indicates whether sense ambiguity is explicitly handled; Des indicates whether image descriptions are included. 2 Related Work There is an extensive literature on word sense disambiguation for nouns, verbs, adjectives and adverbs. Most of these approaches rely on lexical databases or sense inventories such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). Unsupervised WSD approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has us"
N16-1022,D14-1162,0,0.111689,"Missing"
N16-1022,P15-1173,0,0.0198511,"approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with Barnard et al. (2003). Sense discrimination for web images was introduced by Loeff et al. (2006), who used spectral clustering over multimodal features from the images and web text. Saenko and Darrell (2008) used sense definitions in a dictionary to learn a latent LDA space overs senses, which they then used to construct sensespecific classifiers by exploiting the text surrounding an image. 2.1 Related Datasets Most of the datasets relevant for verb sense dis"
N16-1022,P10-4014,0,0.0324809,"disambiguation for nouns, verbs, adjectives and adverbs. Most of these approaches rely on lexical databases or sense inventories such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). Unsupervised WSD approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with Barnard et al. (2003). Sense discrimination for web images was introduced by Loeff et al. (2006), who used spectral clustering over multimodal features from the images and web text. Saenko and Darrell (2008) used sense definitions in a dict"
N18-2119,P16-2094,1,0.844919,"eir model is unsupervised (there is no use of eyetracking data at training time), but achieves a good correlation with eye-tracking data at test time. Furthermore, a number of authors have used eyetracking data for training computer vision models, including zero shot image classification (Karessli et al., 2017), object detection (Papadopoulos et al., 2014), and action classification in still images (Ge et al., 2015; Yun et al., 2015) and videos (Dorr and Vig, 2017). In NLP, some authors have used eye-tracking data collected for text reading to train models that perform part-of-speech tagging (Barrett et al., 2016a,b), grammatical function classification (Barrett and Søgaard, 2015), and sentence compression (Klerke et al., 2016). 3 Fixation Prediction Models Verb Prediction Model (M) In our study, we used the verb prediction model proposed by Gella et al. (2018), which employs a multilabel CNNbased classification approach and is designed to simultaneously predict all verbs associated with an image. This model is trained over a vocabulary that consists of the 250 most common verbs in the TUHOI, Flickr30k, and COCO image description datasets. For each image in these datasets, we obtained a set of verb la"
N18-2119,C16-1126,1,0.826511,"eir model is unsupervised (there is no use of eyetracking data at training time), but achieves a good correlation with eye-tracking data at test time. Furthermore, a number of authors have used eyetracking data for training computer vision models, including zero shot image classification (Karessli et al., 2017), object detection (Papadopoulos et al., 2014), and action classification in still images (Ge et al., 2015; Yun et al., 2015) and videos (Dorr and Vig, 2017). In NLP, some authors have used eye-tracking data collected for text reading to train models that perform part-of-speech tagging (Barrett et al., 2016a,b), grammatical function classification (Barrett and Søgaard, 2015), and sentence compression (Klerke et al., 2016). 3 Fixation Prediction Models Verb Prediction Model (M) In our study, we used the verb prediction model proposed by Gella et al. (2018), which employs a multilabel CNNbased classification approach and is designed to simultaneously predict all verbs associated with an image. This model is trained over a vocabulary that consists of the 250 most common verbs in the TUHOI, Flickr30k, and COCO image description datasets. For each image in these datasets, we obtained a set of verb la"
N18-2119,W15-2401,0,0.0300269,"at training time), but achieves a good correlation with eye-tracking data at test time. Furthermore, a number of authors have used eyetracking data for training computer vision models, including zero shot image classification (Karessli et al., 2017), object detection (Papadopoulos et al., 2014), and action classification in still images (Ge et al., 2015; Yun et al., 2015) and videos (Dorr and Vig, 2017). In NLP, some authors have used eye-tracking data collected for text reading to train models that perform part-of-speech tagging (Barrett et al., 2016a,b), grammatical function classification (Barrett and Søgaard, 2015), and sentence compression (Klerke et al., 2016). 3 Fixation Prediction Models Verb Prediction Model (M) In our study, we used the verb prediction model proposed by Gella et al. (2018), which employs a multilabel CNNbased classification approach and is designed to simultaneously predict all verbs associated with an image. This model is trained over a vocabulary that consists of the 250 most common verbs in the TUHOI, Flickr30k, and COCO image description datasets. For each image in these datasets, we obtained a set of verb labels by extracting all the Center Bias (CB) We compare against a cent"
N18-2119,D16-1092,0,0.176705,"tmap. Model predictions correspond to human intuitions if the two heatmaps correlate. In the present paper, we show that the heatmaps generated by the verb prediction model of Gella et al. (2018) correlate well with heatmaps obtained from human observers performing a verb classification task. We achieve a higher correlation than a range of baselines (center bias, visual salience, and model combinations), indicating that the verb prediction model successfully identifies those image regions that are indicative of the verb depicted in the image. 2 Related Work Most closely related is the work by Das et al. (2016) who tested the hypothesis that the regions attended to by neural visual question answering (VQA) models correlate with the regions attended to by humans performing the same task. Their results were negative: the neural VQA models do not predict human attention better than a baseline visual salience model (see Section 3). It is possible that this result is due to limitations of the study of Das et al. (2016): their evaluation dataset, the VQA-HAT corpus, was collected using mouse-tracking, which is less natural and less sensitive than eye-tracking. Also, their participants did not actually per"
N18-2119,P17-2011,1,0.853571,"CB) baseline (Clarke and Tatler, 2014), and salience map (SM) baseline (Liu and Han, 2016). Results are reported on the validation set of the PASCAL VOC 2012 Actions Fixation data (Mathe and Sminchisescu, 2013). The best score for each class is shown in bold (except upper bound). Model combination are by mean of heatmaps. the eye-tracking setup used, including information on measurement error, please refer to Mathe and Sminchisescu (2015), who used the same setup as Mathe and Sminchisescu (2013). While actions and verbs are distinct concepts (Ronchi and Perona, 2015; Pustejovsky et al., 2016; Gella and Keller, 2017), we can still use the PASCAL Actions Fixation data to evaluate our model. When predicting a verb, the model presumably has to attend to the same regions that humans fixate on when working out which action is depicted – all the actions in the dataset are verb-based, hence recognizing the verb is part of recognizing the action. 5 Results To evaluate the similarity between human fixations and model predictions, we first computed a heatmap based on the human fixations for each image. We used the PyGaze toolkit (Dalmaijer et al., 2014) to generate Gaussian heatmaps weighted by fixation durations."
N18-2119,N16-1022,1,0.8097,"ent research in language and vision has applied fundamental NLP tasks in a multimodal setting. An example is word sense disambiguation (WSD), the task of assigning a word the correct meaning in a given context. WSD traditionally uses textual context, but disambiguation can be performed using an image context instead, relying on the fact that different word senses are often visually distinct. Early work has focused on the disambiguation of nouns (Loeff et al., 2006; Saenko and Darrell, 2008; Chen et al., 2015), but more recent research has proposed visual sense disambiguation models for verbs (Gella et al., 2016). This is a considerably more challenging task, as unlike objects (denoted by nouns), actions (denoted by verbs) are often not clearly localized in an image. Gella et al. (2018) propose a two-stage approach, consisting of a verb prediction model, which labels an image with potential verbs, followed by a visual sense disambiguation model, which uses the image to determine the correct verb senses. While this approach achieves good verb prediction and sense disambiguation accuracy, it is not clear to what extend the model captures human intuitions about visual verbs. Specifically, it is interesti"
N18-2119,D16-1009,1,0.842771,"verb classification model. human correlation of 0.623, which suggests low task validity. Qiao et al. (2017) also use VQA-HAT, but in a supervised fashion: they train the attention component of their VQA model on human attention data. Not surprisingly, this results in a higher correlation with human heatmaps than Das et al.’s (2016) unsupervised approach. However, Qiao et al. (2017) fail to compare to a visual salience model (given their supervised setup, such the salience model would also have to be trained on VQA-HAT for a fair comparison). The work that is perhaps closest to our own work is Hahn and Keller (2016), who use a reinforcement learning model to predict eye-tracking data for text reading (rather than visual processing). Their model is unsupervised (there is no use of eyetracking data at training time), but achieves a good correlation with eye-tracking data at test time. Furthermore, a number of authors have used eyetracking data for training computer vision models, including zero shot image classification (Karessli et al., 2017), object detection (Papadopoulos et al., 2014), and action classification in still images (Ge et al., 2015; Yun et al., 2015) and videos (Dorr and Vig, 2017). In NLP,"
N18-2119,N16-1179,0,0.0316963,"h eye-tracking data at test time. Furthermore, a number of authors have used eyetracking data for training computer vision models, including zero shot image classification (Karessli et al., 2017), object detection (Papadopoulos et al., 2014), and action classification in still images (Ge et al., 2015; Yun et al., 2015) and videos (Dorr and Vig, 2017). In NLP, some authors have used eye-tracking data collected for text reading to train models that perform part-of-speech tagging (Barrett et al., 2016a,b), grammatical function classification (Barrett and Søgaard, 2015), and sentence compression (Klerke et al., 2016). 3 Fixation Prediction Models Verb Prediction Model (M) In our study, we used the verb prediction model proposed by Gella et al. (2018), which employs a multilabel CNNbased classification approach and is designed to simultaneously predict all verbs associated with an image. This model is trained over a vocabulary that consists of the 250 most common verbs in the TUHOI, Flickr30k, and COCO image description datasets. For each image in these datasets, we obtained a set of verb labels by extracting all the Center Bias (CB) We compare against a center bias baseline, which simulates the task-indep"
N18-2119,P06-2071,0,0.0878128,"Missing"
N18-2119,W16-3807,0,0.0348812,"reement (H), center bias (CB) baseline (Clarke and Tatler, 2014), and salience map (SM) baseline (Liu and Han, 2016). Results are reported on the validation set of the PASCAL VOC 2012 Actions Fixation data (Mathe and Sminchisescu, 2013). The best score for each class is shown in bold (except upper bound). Model combination are by mean of heatmaps. the eye-tracking setup used, including information on measurement error, please refer to Mathe and Sminchisescu (2015), who used the same setup as Mathe and Sminchisescu (2013). While actions and verbs are distinct concepts (Ronchi and Perona, 2015; Pustejovsky et al., 2016; Gella and Keller, 2017), we can still use the PASCAL Actions Fixation data to evaluate our model. When predicting a verb, the model presumably has to attend to the same regions that humans fixate on when working out which action is depicted – all the actions in the dataset are verb-based, hence recognizing the verb is part of recognizing the action. 5 Results To evaluate the similarity between human fixations and model predictions, we first computed a heatmap based on the human fixations for each image. We used the PyGaze toolkit (Dalmaijer et al., 2014) to generate Gaussian heatmaps weighte"
N19-1200,W17-4746,0,0.0635256,"Missing"
N19-1200,D07-1007,0,0.0304704,"rrif, 1998). Word sense disambiguation is typically tackled using only textual context; however, in a multimodal setting, visual context is also available and can be used for disambiguation. Most prior work on visual word sense disambiguation has targeted noun senses (Barnard and Johnson, 2005; Loeff et al., 2006; Saenko and Darrell, 2008), but the task has recently been extended to verb senses (Gella et al., 2016, 2019). Resolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visua"
N19-1200,W14-3348,0,0.0113895,"multimodal) affects the accuracy of the verb prediction. We show the top verb predicted by our models for both German and Spanish. The top predicted verb using text-only visual features is incorrect. The unimodal visual features model predicts the correct Spanish verb but the incorrect We also evaluate our verb sense disambiguation model in the challenging downstream task of multimodal machine translation (Specia et al., 2016). We conduct this evaluation on the sentence-level translation subset of MultiSense. We evaluate model performance using BLEU (Papineni et al., 2002) and Meteor scores (Denkowski and Lavie, 2014) between the MultiSense reference description and the translation model output. We also evaluate the verb prediction accuracy of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce sparsity (Sennrich et al., 2016). Our approach"
N19-1200,W17-4718,1,0.872371,"has a clear application in machine translation. Determining the correct sense of a verb is important for high quality translation output, and sometimes text-only translation systems fail when the correct translation would be obvious from visual information (see Figure 1). To show that cross-lingual visual sense disambiguation can improve the performance of translation systems, we annotate a part of our MultiSense dataset with English image descriptions and their German translations. There are two existing multimodal translation evaluation sets with ambiguous words: the Ambiguous COCO dataset (Elliott et al., 2017) contains sentences that are “possibly ambiguous”, and the Multimodal Lexical Translation dataset is restricted to predicting single words instead of full sentences (Lala and Specia, 2018). This type of resource is important for multimodal translation because it is known that humans use visual context to resolve ambiguities for nouns and gender-neutral words (Frank et al., 2018). MultiSense contains sentences that are known to have ambiguities, and it allows for sentence-level and verb prediction evaluation. Here, we use the verbs predicted by our visual sense disambiguation model to constrain"
N19-1200,W16-3210,1,0.896034,"Missing"
N19-1200,N16-1022,1,0.688856,"allenging problems in natural language processing. It is often studied as a word sense disambiguation (WSD) problem, which is the task of assigning the correct sense to a word in a given context (Kilgarrif, 1998). Word sense disambiguation is typically tackled using only textual context; however, in a multimodal setting, visual context is also available and can be used for disambiguation. Most prior work on visual word sense disambiguation has targeted noun senses (Barnard and Johnson, 2005; Loeff et al., 2006; Saenko and Darrell, 2008), but the task has recently been extended to verb senses (Gella et al., 2016, 2019). Resolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et a"
N19-1200,D17-1303,1,0.795124,"just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word similarity tasks, bilingual lexicon induction has so far mainly targeted nouns and these approaches was shown to perform poorly for other word categories such as verbs. Recent work by Gella et al. (2017) and K´ad´ar et al. (2018) has shown using image as pivot between languages can lead to better multilingual multimodal representations and can have successful applications in crosslingual retrieval and 1998 Proceedings of NAACL-HLT 2019, pages 1998–2004 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics multilingual image retrieval. In this paper, we introduce the MultiSense dataset of 9,504 images annotated with English verbs and their translations in German and Spanish. For each image in MultiSense, the English verb is translation-ambiguous, i.e.,"
N19-1200,W17-4749,0,0.0360741,"Missing"
N19-1200,E17-3017,0,0.0224648,"evaluate our verb sense disambiguation model in the challenging downstream task of multimodal machine translation (Specia et al., 2016). We conduct this evaluation on the sentence-level translation subset of MultiSense. We evaluate model performance using BLEU (Papineni et al., 2002) and Meteor scores (Denkowski and Lavie, 2014) between the MultiSense reference description and the translation model output. We also evaluate the verb prediction accuracy of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce sparsity (Sennrich et al., 2016). Our approach uses the German verb predicted by the unimodal visual model (Section 3.1) to constrain the output of the translation decoder (Post and Vilar, 2018). This means that our approach does not directly use visual features, instead it uses the output of the visual verb sense disambiguation model"
N19-1200,P16-1227,0,0.0396333,"a et al., 2016, 2019). Resolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with oth"
N19-1200,K18-1039,1,0.85067,"rmation has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word similarity tasks, bilingual lexicon induction has so far mainly targeted nouns and these approaches was shown to perform poorly for other word categories such as verbs. Recent work by Gella et al. (2017) and K´ad´ar et al. (2018) has shown using image as pivot between languages can lead to better multilingual multimodal representations and can have successful applications in crosslingual retrieval and 1998 Proceedings of NAACL-HLT 2019, pages 1998–2004 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics multilingual image retrieval. In this paper, we introduce the MultiSense dataset of 9,504 images annotated with English verbs and their translations in German and Spanish. For each image in MultiSense, the English verb is translation-ambiguous, i.e., it has more than one poss"
N19-1200,D15-1015,0,0.0325299,"Missing"
N19-1200,L18-1602,0,0.315333,"esolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word sim"
N19-1200,N15-1016,0,0.0156832,"verb. Figure 1 shows an example of an image paired with its English description and German translation. 3 Verb Sense Disambiguation Modeling We propose three models for cross-lingual verb sense disambiguation, based on the visual input, the textual input, or using both inputs. Each model is trained to minimize the negative log probability of predicting the correct verb translation. 1999 Chance Majority Text Image MM German Spanish Spanish mandar German zublasen hinchar aufblasen Unimodal Visual Model Visual features have been shown to be useful for learning semantic representations of words (Lazaridou et al., 2015), bilingual lexicon learning (Kiela et al., 2015), and visual sense disambiguation (Gella et al., 2016), amongst others. We propose a model that learns to predict the verb translation using only visual input. Given an image I, we extract a fixed feature vector from a Convolutional Neural Network, and project it into a hidden layer hv with the learned matrix Wi ∈ Rh×512 (Eqn. 1). The hidden layer is projected into the output vocabulary of v verbs using the learned matrix Wo ∈ Rh×v , and normalized into a probability distribution using a softmax transformation (Eqn. 2). hv = Wi · CNN(I) + bi y ="
N19-1200,S13-2029,0,0.0483847,"these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word similarity tasks, bilingual lexicon induction has so far mainly targeted nouns and these approaches was shown to perform poorl"
N19-1200,P12-3029,0,0.0873912,"Missing"
N19-1200,P06-2071,0,0.205001,"Missing"
N19-1200,P02-1040,0,0.104812,"how varying the input (textual, visual, or multimodal) affects the accuracy of the verb prediction. We show the top verb predicted by our models for both German and Spanish. The top predicted verb using text-only visual features is incorrect. The unimodal visual features model predicts the correct Spanish verb but the incorrect We also evaluate our verb sense disambiguation model in the challenging downstream task of multimodal machine translation (Specia et al., 2016). We conduct this evaluation on the sentence-level translation subset of MultiSense. We evaluate model performance using BLEU (Papineni et al., 2002) and Meteor scores (Denkowski and Lavie, 2014) between the MultiSense reference description and the translation model output. We also evaluate the verb prediction accuracy of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce s"
N19-1200,N18-1119,0,0.013856,"of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce sparsity (Sennrich et al., 2016). Our approach uses the German verb predicted by the unimodal visual model (Section 3.1) to constrain the output of the translation decoder (Post and Vilar, 2018). This means that our approach does not directly use visual features, instead it uses the output of the visual verb sense disambiguation model to guide the translation process. We compare our approach against two state-ofthe-art multimodal translation systems: Caglayan et al. (2017) modulate the target language word embeddings by an element-wise multiplication with a learned transformation of the visual data; Helcl and Libovick´y (2017) use a double attention model that learns to selectively attend to a combination of the source language and the visual data. 5.2 Results Table 4 shows the resul"
N19-1200,W17-4739,0,0.0249021,"Missing"
N19-1200,P16-1162,0,0.0155016,"or scores (Denkowski and Lavie, 2014) between the MultiSense reference description and the translation model output. We also evaluate the verb prediction accuracy of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce sparsity (Sennrich et al., 2016). Our approach uses the German verb predicted by the unimodal visual model (Section 3.1) to constrain the output of the translation decoder (Post and Vilar, 2018). This means that our approach does not directly use visual features, instead it uses the output of the visual verb sense disambiguation model to guide the translation process. We compare our approach against two state-ofthe-art multimodal translation systems: Caglayan et al. (2017) modulate the target language word embeddings by an element-wise multiplication with a learned transformation of the visual data; Helcl and Libovick´y (201"
N19-1200,W16-2363,0,0.0218059,"o verb senses (Gella et al., 2016, 2019). Resolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 201"
N19-1200,W16-2346,1,0.939576,"Missing"
N19-1200,P16-2031,0,0.0163985,"Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word similarity tasks, bilingual lexicon induction has so far mainly targeted nouns and these approaches was shown to perform poorly for other word categories such as verbs. Recent work by Gella et al. (2017) and K´ad´ar et al. (2018) has shown using image as pivot between languages can lead to better multilingual multimodal representations and can have successful applications in crosslingual retrieval and 1998 Proceedings of NAACL-HLT 2019, pages 1998–2004 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics mult"
N19-1200,W09-2413,0,\N,Missing
P01-1046,J92-4003,0,0.0652683,"Missing"
P01-1046,N01-1013,0,0.0681813,"eptual classes (hpersoni, hlife fromi, hentityi, hcausal agenti, hfeaturei, hmerchandisei, hcommodityi, and hobjecti). The words chief and leader have four conceptual classes in common, i.e., hpersoni and hlife formi, hentityi, and hcausal agenti. This means that we will increment the observed co-occurrence count of proud and hpersoni, proud and hlife formi, proud and hentityi, and proud and hcausal agenti by 18 . Since we 2 There are several ways of addressing this problem, e.g., by discounting the contribution of very general classes by finding a suitable class to represent a given concept (Clark and Weir, 2001). do not know the actual class of the noun chief in the corpus, we weight the contribution of each class by taking the average of the constructed frequencies for all seven classes: ∑ ∑ (2) f (a, n) = c∈classes(n) n0 ∈c f (a,n0 ) |classes(n0 )| |classes(n)| Based on (2) the recreated frequency for the pair proud chief in the BNC is 6.12 (see Table 1). 2.2 Distance-Weighted Averaging Distance-weighted averaging induces classes of similar words from word co-occurrences without making reference to a taxonomy. A key feature of this type of smoothing is the function which measures distributional sim"
P01-1046,C94-2119,0,0.443275,"tural language processing applications as a means to address data sparseness, an inherent problem for statistical methods which rely on the relative frequencies of word combinations. The problem arises when the probability of word combinations that do not occur in the training data needs to be estimated. The smoothing methods proposed in the literature (overviews are provided by Dagan et al. (1999) and Lee (1999)) can be generally divided into three types: discounting (Katz, 1987), class-based smoothing (Resnik, 1993; Brown et al., 1992; Pereira et al., 1993), and distance-weighted averaging (Grishman and Sterling, 1994; Dagan et al., 1999). Discounting methods decrease the probability of previously seen events so that the total probability of observed word co-occurrences is less than one, leaving some probability mass to be redistributed among unseen co-occurrences. Class-based smoothing and distance-weighted averaging both rely on an intuitively simple idea: inter-word dependencies are modelled by relying on the corpus evidence available for words that are similar to the words of interest. The two approaches differ in the way they measure word similarity. Distance-weighted averaging estimates word similari"
P01-1046,E99-1005,1,0.274852,"ves and nouns are perceived as more plausible than others. A classical example is strong tea, which is highly plausible, as opposed to powerful tea, which is not. On the other hand, powerful car is highly plausible, whereas strong car is less plausible. It has been argued in the theoretical literature that the plausibility of an adjective-noun pair is largely a collocational (i.e., idiosyncratic) property, in contrast to verb-object or noun-noun plausibility, which is more predictable (Cruse, 1986; Smadja, 1991). The collocational hypothesis has recently been investigated in a corpus study by Lapata et al. (1999). This study investigated potential statistical predictors of adjective-noun plausibility by using correlation analysis to compare judgements elicited from human subjects with five corpus-derived measures: co-occurrence frequency of the adjective-noun pair, noun frequency, conditional probability of the noun given the adjective, the log-likelihood ratio, and Resnik’s (1993) selectional association measure. All predictors but one were positively correlated with plausibility; the highest correlation was obtained with co-occurrence frequency. Resnik’s selectional association measure surprisingly"
P01-1046,P99-1004,0,0.565572,"o establish the validity of smoothing methods independent from a specific natural language processing task. 2 Smoothing Methods Smoothing techniques have been used in a variety of statistical natural language processing applications as a means to address data sparseness, an inherent problem for statistical methods which rely on the relative frequencies of word combinations. The problem arises when the probability of word combinations that do not occur in the training data needs to be estimated. The smoothing methods proposed in the literature (overviews are provided by Dagan et al. (1999) and Lee (1999)) can be generally divided into three types: discounting (Katz, 1987), class-based smoothing (Resnik, 1993; Brown et al., 1992; Pereira et al., 1993), and distance-weighted averaging (Grishman and Sterling, 1994; Dagan et al., 1999). Discounting methods decrease the probability of previously seen events so that the total probability of observed word co-occurrences is less than one, leaving some probability mass to be redistributed among unseen co-occurrences. Class-based smoothing and distance-weighted averaging both rely on an intuitively simple idea: inter-word dependencies are modelled by r"
P01-1046,P93-1024,0,0.301711,"techniques have been used in a variety of statistical natural language processing applications as a means to address data sparseness, an inherent problem for statistical methods which rely on the relative frequencies of word combinations. The problem arises when the probability of word combinations that do not occur in the training data needs to be estimated. The smoothing methods proposed in the literature (overviews are provided by Dagan et al. (1999) and Lee (1999)) can be generally divided into three types: discounting (Katz, 1987), class-based smoothing (Resnik, 1993; Brown et al., 1992; Pereira et al., 1993), and distance-weighted averaging (Grishman and Sterling, 1994; Dagan et al., 1999). Discounting methods decrease the probability of previously seen events so that the total probability of observed word co-occurrences is less than one, leaving some probability mass to be redistributed among unseen co-occurrences. Class-based smoothing and distance-weighted averaging both rely on an intuitively simple idea: inter-word dependencies are modelled by relying on the corpus evidence available for words that are similar to the words of interest. The two approaches differ in the way they measure word s"
P03-1013,C02-1093,0,0.0201149,"and trained on the Penn Treebank (Marcus et al., 1993), which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup. The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German. German has a number of syntactic properties that set it apart from English, and the Negra annotation scheme differs in important respects from the Penn Treebank markup. While Negra has been used to build probabilistic chunkers (Becker and Frank, 2002; Skut and Brants, 1998), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge). Lexicalization can increase parsing performance dramatically for English (Carroll and Rooth, 1998; Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, UK keller@inf.ed.ac.uk Charniak, 1997, 2000; Collins, 1997), and the lexicalized model proposed by Collins (1997) has been successfully applied to Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However,"
P03-1013,P99-1035,0,0.0501244,"Missing"
P03-1013,W00-1201,0,0.0410389,"c chunkers (Becker and Frank, 2002; Skut and Brants, 1998), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge). Lexicalization can increase parsing performance dramatically for English (Carroll and Rooth, 1998; Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, UK keller@inf.ed.ac.uk Charniak, 1997, 2000; Collins, 1997), and the lexicalized model proposed by Collins (1997) has been successfully applied to Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, the resulting performance is significantly lower than the performance of the same model for English (see Table 1). Neither Collins et al. (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. This paper is structured as follows. Section 2 reviews the syntactic properties of German, focusing on its semi-flexible wordorder. Section 3 describes two standard lexicalized models (Carroll and Rooth, 1998; Collins, 1997), as well as an unlexical"
P03-1013,A00-1031,0,0.0338384,"Missing"
P03-1013,W98-1505,0,0.454715,"sing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German. German has a number of syntactic properties that set it apart from English, and the Negra annotation scheme differs in important respects from the Penn Treebank markup. While Negra has been used to build probabilistic chunkers (Becker and Frank, 2002; Skut and Brants, 1998), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge). Lexicalization can increase parsing performance dramatically for English (Carroll and Rooth, 1998; Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, UK keller@inf.ed.ac.uk Charniak, 1997, 2000; Collins, 1997), and the lexicalized model proposed by Collins (1997) has been successfully applied to Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, the resulting performance is significantly lower than the performance of the same model for English (see Table 1). Neither Collins et al. (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexica"
P03-1013,A00-2018,0,0.0276777,"[PP P [NP . . . ]] In the present experiment, we investigated if parsing performance improves if we test and train on a version of Negra on which the transformation in (7) has been applied. In a second series of experiments, we investigated a more general way of dealing with the flatness of Head sister category Head sister head word Head sister head tag Prev. sister category Prev. sister head word Prev. sister head tag C&R Collins Charniak Current X X X X X X X X X X X X X Table 4: Linguistic features in the current model compared to the models of Carroll and Rooth (1998), Collins (1997), and Charniak (2000) Negra, based on Collins’s (1997) model for nonrecursive NPs in the Penn Treebank (which are also flat). For non-recursive NPs, Collins (1997) does not use the probability function in (5), but instead substitutes Pr (and, by analogy, Pl ) by: (8) Pr (Ri ,t(Ri ), l(Ri )|P, Ri−1 ,t(Ri−1 ), l(Ri−1 ), d(i)) Here the head H is substituted by the sister Ri−1 (and Li−1 ). In the literature, the version of Pr in (5) is said to capture head-head relationships. We will refer to the alternative model in (8) as capturing sister-head relationships. Using sister-head relationships is a way of counteracting"
P03-1013,P97-1003,0,0.826819,"9) and Chinese (Bikel and Chiang, 2000). However, the resulting performance is significantly lower than the performance of the same model for English (see Table 1). Neither Collins et al. (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. This paper is structured as follows. Section 2 reviews the syntactic properties of German, focusing on its semi-flexible wordorder. Section 3 describes two standard lexicalized models (Carroll and Rooth, 1998; Collins, 1997), as well as an unlexicalized baseline model. Section 4 presents a series of experiments that compare the parsing performance of these three models (and several variants) on Negra. The results show that both lexicalized models fail to outperform the unlexicalized baseline. This is at odds with what has been reported for English. Learning curves show that the poor performance of the lexicalized models is not due to lack of training data. Section 5 presents an error analysis for Collins’s (1997) lexicalized model, which shows that the head-head dependencies used in this model fail to cope well w"
P03-1013,P99-1065,0,0.573547,"Missing"
P03-1013,W01-0521,0,0.0443988,"Missing"
P03-1013,P02-1043,0,0.0361971,"Missing"
P03-1013,J93-2004,0,0.0499098,"iert . Komponiert hat er gestern Musik. The semi-free wordorder in German means that a context-free grammar model has to contain more rules than for a fixed wordorder language. For transitive verbs, for instance, we need the rules S → V NP NP, S → NP V NP, and S → NP NP V to account for verb initial, verb second, and verb final order (assuming a flat S, see Section 2.2). 2.2 Negra Annotation Scheme The Negra corpus consists of around 350,000 words of German newspaper text (20,602 sentences). The annotation scheme (Skut et al., 1997) is modeled to a certain extent on that of the Penn Treebank (Marcus et al., 1993), with crucial differences. Most importantly, Negra follows the dependency grammar tradition in assuming flat syntactic representations: (a) There is no S → NP VP rule. Rather, the subject, the verb, and its objects are all sisters of each other, dominated by an S node. This is a way of accounting for the semi-free wordorder of German (see Section 2.1): the first NP within an S need not be the subject. (b) There is no SBAR → Comp S rule. Main clauses, subordinate clauses, and relative clauses all share the category S in Negra; complementizers and relative pronouns are simply sisters of the ver"
P03-1013,W98-1117,0,0.0169639,"Treebank (Marcus et al., 1993), which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup. The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German. German has a number of syntactic properties that set it apart from English, and the Negra annotation scheme differs in important respects from the Penn Treebank markup. While Negra has been used to build probabilistic chunkers (Becker and Frank, 2002; Skut and Brants, 1998), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge). Lexicalization can increase parsing performance dramatically for English (Carroll and Rooth, 1998; Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, UK keller@inf.ed.ac.uk Charniak, 1997, 2000; Collins, 1997), and the lexicalized model proposed by Collins (1997) has been successfully applied to Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, the resulting performanc"
P03-1013,A97-1014,0,0.509581,"bank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., Collins 1997; Charniak 2000). However, most of the existing models have been developed for English and trained on the Penn Treebank (Marcus et al., 1993), which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup. The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German. German has a number of syntactic properties that set it apart from English, and the Negra annotation scheme differs in important respects from the Penn Treebank markup. While Negra has been used to build probabilistic chunkers (Becker and Frank, 2002; Skut and Brants, 1998), the research reported in this paper is the first attempt to develop a probabilistic full parsing model for German trained on a treebank (to our knowledge). Lexicalization can increase parsing performance dramatically for English (Carroll and Rooth, 1998; Frank Keller School of"
P05-1038,abeille-etal-2000-building,0,0.210222,"Missing"
P05-1038,W04-3224,0,0.0195941,"ypically manifests itself in a severe reduction in parsing performance compared to the results for English. A second recent strand in parsing research has dealt with the role of lexicalization. The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG). However, this has been challenged by Klein and Manning (2003), who demonstrate that an unlexicalized model can achieve a performance close to the state of the art for lexicalized models. Furthermore, Bikel (2004) provides evidence that lexical information (in the form of bi-lexical dependencies) only makes a small contribution to the performance of parsing models such as Collins’s (1997). The only previous authors that have directly addressed the role of lexicalization in crosslinguistic parsing are Dubey and Keller (2003). They show that standard lexicalized models fail to outperform an unlexicalized baseline (a vanilla PCFG) on Negra, a German treebank (Skut et al., 1997). They attribute this result to two facts: (a) The Negra annotation assumes very flat trees, which means that Collins-style head-l"
P05-1038,W00-1201,0,0.0425255,"Missing"
P05-1038,A00-2018,0,0.576069,"Missing"
P05-1038,P97-1003,0,0.364817,"formance. 1 Introduction This paper brings together two strands of research that have recently emerged in the field of probabilistic parsing: crosslinguistic parsing and lexicalized parsing. Interest in parsing models for languages other than English has been growing, starting with work on Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000; Levy and Manning, 2003). Probabilistic parsing for German has also been explored by a range of authors (Dubey and Keller, 2003; Schiehlen, 2004). In general, these authors have found that existing lexicalized parsing models for English (e.g., Collins 1997) do not straightforwardly generalize to new languages; this typically manifests itself in a severe reduction in parsing performance compared to the results for English. A second recent strand in parsing research has dealt with the role of lexicalization. The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG). However, this has been challenged by Klein and Manning (2003), who demonstrate that an unlexicalized model can achieve a performance close"
P05-1038,P99-1065,0,0.26339,"Missing"
P05-1038,P03-1013,1,0.956802,"robabilistic parsing results for English, but contrary to results for German, where lexicalization has only a limited effect on parsing performance. 1 Introduction This paper brings together two strands of research that have recently emerged in the field of probabilistic parsing: crosslinguistic parsing and lexicalized parsing. Interest in parsing models for languages other than English has been growing, starting with work on Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000; Levy and Manning, 2003). Probabilistic parsing for German has also been explored by a range of authors (Dubey and Keller, 2003; Schiehlen, 2004). In general, these authors have found that existing lexicalized parsing models for English (e.g., Collins 1997) do not straightforwardly generalize to new languages; this typically manifests itself in a severe reduction in parsing performance compared to the results for English. A second recent strand in parsing research has dealt with the role of lexicalization. The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG). However,"
P05-1038,1997.iwpt-1.12,0,0.0307566,"Missing"
P05-1038,P03-1054,0,0.0610904,"al, these authors have found that existing lexicalized parsing models for English (e.g., Collins 1997) do not straightforwardly generalize to new languages; this typically manifests itself in a severe reduction in parsing performance compared to the results for English. A second recent strand in parsing research has dealt with the role of lexicalization. The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG). However, this has been challenged by Klein and Manning (2003), who demonstrate that an unlexicalized model can achieve a performance close to the state of the art for lexicalized models. Furthermore, Bikel (2004) provides evidence that lexical information (in the form of bi-lexical dependencies) only makes a small contribution to the performance of parsing models such as Collins’s (1997). The only previous authors that have directly addressed the role of lexicalization in crosslinguistic parsing are Dubey and Keller (2003). They show that standard lexicalized models fail to outperform an unlexicalized baseline (a vanilla PCFG) on Negra, a German treeban"
P05-1038,P03-1056,0,0.100145,"Missing"
P05-1038,P95-1037,0,0.218718,"Collins et al., 1999) and Chinese (Bikel and Chiang, 2000; Levy and Manning, 2003). Probabilistic parsing for German has also been explored by a range of authors (Dubey and Keller, 2003; Schiehlen, 2004). In general, these authors have found that existing lexicalized parsing models for English (e.g., Collins 1997) do not straightforwardly generalize to new languages; this typically manifests itself in a severe reduction in parsing performance compared to the results for English. A second recent strand in parsing research has dealt with the role of lexicalization. The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG). However, this has been challenged by Klein and Manning (2003), who demonstrate that an unlexicalized model can achieve a performance close to the state of the art for lexicalized models. Furthermore, Bikel (2004) provides evidence that lexical information (in the form of bi-lexical dependencies) only makes a small contribution to the performance of parsing models such as Collins’s (1997). The only previous authors that have directly ad"
P05-1038,J93-2004,0,0.0266677,"n in Figure 2. Finally, the FTB differs from the PTB in that it does not use any empty categories. 2 The French Treebank 2.1 Annotation Scheme The French Treebank (FTB; Abeill´e et al. 2000) consists of 20,648 sentences extracted from the daily newspaper Le Monde, covering a variety of authors and domains (economy, literature, politics, etc.).1 The corpus is formatted in XML and has a rich morphosyntactic tagset that includes part-of-speech tag, ‘subcategorization’ (e.g., possessive or cardinal), inflection (e.g., masculine singular), and lemma information. Compared to the Penn Treebank (PTB; Marcus et al. 1993), the POS tagset of the French Treebank is smaller (13 tags vs. 36 tags): all punctuation marks are represented as the single PONCT tag, there are no separate tags for modal verbs, whwords, and possessives. Also verbs, adverbs and prepositions are more coarsely defined. On the other hand, a separate clitic tag (CL) for weak pronouns is introduced. An example for the word-level annotation in the FTB is given in Figure 1 The phrasal annotation of the FTB differs from that for the Penn Treebank in several aspects. There is no verb phrase: only the verbal nucleus (VN) is annotated. A VN comprises"
P05-1038,C04-1056,0,0.031821,"ults for English, but contrary to results for German, where lexicalization has only a limited effect on parsing performance. 1 Introduction This paper brings together two strands of research that have recently emerged in the field of probabilistic parsing: crosslinguistic parsing and lexicalized parsing. Interest in parsing models for languages other than English has been growing, starting with work on Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000; Levy and Manning, 2003). Probabilistic parsing for German has also been explored by a range of authors (Dubey and Keller, 2003; Schiehlen, 2004). In general, these authors have found that existing lexicalized parsing models for English (e.g., Collins 1997) do not straightforwardly generalize to new languages; this typically manifests itself in a severe reduction in parsing performance compared to the results for English. A second recent strand in parsing research has dealt with the role of lexicalization. The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG). However, this has been chal"
P05-1038,C04-1024,0,0.00527533,"requires in its left or right sister. The subcat requirements are added to the conditioning context. As complements are generated, they are removed from the appropriate subcat multiset. 5 Experiment 1: Unlexicalized Model 5.1 Method This experiment was designed to compare the performance of the unlexicalized baseline model on four different datasets, created by the tree transformations described in Section 3: compounds expanded (Exp), compounds contracted (Cont), compounds expanded with coordination raised (Exp+CR), and compounds contracted with coordination raised (Cont+CR). We used BitPar (Schmid, 2004) for our unlexicalized experiments. BitPar is a parser based on a bit-vector implementation of the CKY algorithm. A grammar and lexicon were read off our training set, along with rule frequencies and frequencies for lexical items, based on which BitPar computes the rule 309 Model Exp Exp+CR Cont Cont+CR LR 59.97 60.75 64.19 66.11 LP 58.64 60.57 64.61 65.55 CBs 1.74 1.57 1.50 1.39 0CB 39.05 40.77 46.74 46.99 ≤2CB 73.23 75.03 76.80 78.95 Tag 91.00 91.08 93.30 93.22 Cov 99.20 99.09 98.48 97.94 Table 1: Results for unlexicalized models (sentences ≤40 words); each model performed its own POS taggin"
P05-1038,A97-1014,0,0.0749729,"ho demonstrate that an unlexicalized model can achieve a performance close to the state of the art for lexicalized models. Furthermore, Bikel (2004) provides evidence that lexical information (in the form of bi-lexical dependencies) only makes a small contribution to the performance of parsing models such as Collins’s (1997). The only previous authors that have directly addressed the role of lexicalization in crosslinguistic parsing are Dubey and Keller (2003). They show that standard lexicalized models fail to outperform an unlexicalized baseline (a vanilla PCFG) on Negra, a German treebank (Skut et al., 1997). They attribute this result to two facts: (a) The Negra annotation assumes very flat trees, which means that Collins-style head-lexicalization fails to pick up the relevant information from non-head nodes. (b) German allows flexible word order, which means that standard parsing models based on context free grammars perform poorly, as they fail to generalize over different positions of the same constituent. As it stands, Dubey and Keller’s (2003) work does not tell us whether treebank flatness or word order flexibility is responsible for their results: for English, the annotation scheme is non"
P06-1053,C00-1027,0,0.0604599,"so interesting from an engineering point of view. To avoid sparse data problems, probabilistic parsing models make strong independence assumptions; in particular, they generally assume that sentences are independent of each other, in spite of corpus evidence for structural repetition between sentences. We therefore expect a parsing model that includes structural repetition to provide a better fit with real corpus data, resulting in better parsing performance. A simple and principled approach to handling structure re-use would be to use adaptation probabilities for probabilistic grammar rules (Church, 2000), analogous to cache probabilities used in caching language models (Kuhn and de Mori, 1990). This is the approach we will pursue in this paper. Dubey et al. (2005) present a corpus study that demonstrates the existence of parallelism in corpus data. This is an important precondition for understanding the parallelism effect; however, they Abstract The psycholinguistic literature provides evidence for syntactic priming, i.e., the tendency to repeat structures. This paper describes a method for incorporating priming into an incremental probabilistic parser. Three models are compared, which involv"
P06-1053,H05-1104,1,0.94043,"ular, they generally assume that sentences are independent of each other, in spite of corpus evidence for structural repetition between sentences. We therefore expect a parsing model that includes structural repetition to provide a better fit with real corpus data, resulting in better parsing performance. A simple and principled approach to handling structure re-use would be to use adaptation probabilities for probabilistic grammar rules (Church, 2000), analogous to cache probabilities used in caching language models (Kuhn and de Mori, 1990). This is the approach we will pursue in this paper. Dubey et al. (2005) present a corpus study that demonstrates the existence of parallelism in corpus data. This is an important precondition for understanding the parallelism effect; however, they Abstract The psycholinguistic literature provides evidence for syntactic priming, i.e., the tendency to repeat structures. This paper describes a method for incorporating priming into an incremental probabilistic parser. Three models are compared, which involve priming of rules between sentences, within sentences, and within coordinate structures. These models simulate the reading time advantage for parallel structures"
P06-1053,N01-1021,0,0.417149,"compared with in (1b), where they are not. There are various approaches to modeling processing difficulty using a probabilistic approach. One possibility is to use an incremental parser with a beam search or an n-best approach. Processing difficulty is predicted at points in the input string where the current best parse is replaced by an alternative derivation (Jurafsky, 1996; Crocker and Brants, 2000). An alternative is to keep track of all derivations, and predict difficulty at points where there is a large change in the shape of the probability distribution across adjacent parsing states (Hale, 2001). A third approach is to calculate the forward probability (Stolcke, 1995) of the sentence using a PCFG. Low probabilities are then predicted to correspond to high processing difficulty. A variant of this third approach is to assume that processing difficulty is correlated with the (log) probability of the best parse (Keller, 2003). This final formulation is the one used for the experiments presented in this paper. 3.1 Method The item set was adapted from that of Frazier et al. (2000). The original two relevant conditions of their experiment (1a,b) differ in terms of length. This results in a"
P06-1053,P03-1054,0,0.00322846,"1995). We have two versions of the parser: one which parses exhaustively, and a second which uses a variable width beam, pruning any edges 1 of the best edge. The merit whose merit is 2000 of an edge is its inside probability times a prior P(LHS) times a lookahead probability (Roark and Johnson, 1999). To speed up parsing time, we right binarize the grammar,3 remove empty nodes, coindexation and grammatical functions. As our goal is to create the simplest possible model which can nonetheless model experimental data, we do not make any tree modification designed to improve accuracy (as, e.g., Klein and Manning 2003). The approach used to implement the Copy model is to have the parser copy the subtree of the first conjunct whenever it comes across a CC tag. Before copying, though, the parser looks ahead to check if the part-of-speech tags after the CC are equivalent to those inside the first conjunct. The copying model is visualized in Figure 1: the top panel depicts a partially completed edge upon seeing a CC tag, and the second panel shows the completed copying operation. It should be clear that 3 We found that using an unbinarized grammar did not alter the results, at least in the exhaustive parsing ca"
P06-1053,P99-1054,0,0.113498,"vior to human reading behavior. We discuss this topic in more detail in Section 3. At this point, it suffices to say that we require a parser which has the prefix property, i.e., which parses incrementally, from left to right. Therefore, we use an Earley-style probabilistic parser, which outputs Viterbi parses (Stolcke, 1995). We have two versions of the parser: one which parses exhaustively, and a second which uses a variable width beam, pruning any edges 1 of the best edge. The merit whose merit is 2000 of an edge is its inside probability times a prior P(LHS) times a lookahead probability (Roark and Johnson, 1999). To speed up parsing time, we right binarize the grammar,3 remove empty nodes, coindexation and grammatical functions. As our goal is to create the simplest possible model which can nonetheless model experimental data, we do not make any tree modification designed to improve accuracy (as, e.g., Klein and Manning 2003). The approach used to implement the Copy model is to have the parser copy the subtree of the first conjunct whenever it comes across a CC tag. Before copying, though, the parser looks ahead to check if the part-of-speech tags after the CC are equivalent to those inside the first"
P06-1053,J95-2002,0,0.316775,"orrelated with processing difficulty, and faster reading times (as is the case with parallel structures) are correlated with processing ease. A probabilistic parser may be considered to be a sentence processing model via a ‘linking hypothesis’, which links the parser’s word-by-word behavior to human reading behavior. We discuss this topic in more detail in Section 3. At this point, it suffices to say that we require a parser which has the prefix property, i.e., which parses incrementally, from left to right. Therefore, we use an Earley-style probabilistic parser, which outputs Viterbi parses (Stolcke, 1995). We have two versions of the parser: one which parses exhaustively, and a second which uses a variable width beam, pruning any edges 1 of the best edge. The merit whose merit is 2000 of an edge is its inside probability times a prior P(LHS) times a lookahead probability (Roark and Johnson, 1999). To speed up parsing time, we right binarize the grammar,3 remove empty nodes, coindexation and grammatical functions. As our goal is to create the simplest possible model which can nonetheless model experimental data, we do not make any tree modification designed to improve accuracy (as, e.g., Klein"
P10-1021,N01-1021,0,0.576616,"ted by Staub and Clifton (2006): following the word either, readers predict or and the complement that follows it, and process it faster compared to a control condition without either. Thus, human language processing takes advantage of the constraints imposed by the preceding semantic and syntactic context to derive expectations about the upcoming input. Much recent work has focused on developing computational measures of these constraints and expectations. Again, the literature is split into syntactic and semantic models. Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficu"
P10-1021,P10-2012,1,0.839713,"to modeling semantic and syntactic costs disjointly using a mixture of probabilistic and nonprobabilistic measures. An interesting question is which aspects of semantics our model is able to capture, i.e., why does the combination of LSA or LDA representations with an incremental parser yield a better fit of the behavioral data. In the psycholinguistic literature, various types of semantic information have been investigated: lexical semantics (word senses, selectional restrictions, thematic roles), sentential semantics (scope, binding), and discourse semantics (coreference and coherence); see Keller (2010) of a detailed discussion. We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). However, discourse coreference effects (such as the ones reported by Altmann and Steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see Dubey 2010 for a model of human sentence processing that can handle coreference). A key"
P10-1021,P10-1120,0,0.0761262,"scope, binding), and discourse semantics (coreference and coherence); see Keller (2010) of a detailed discussion. We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). However, discourse coreference effects (such as the ones reported by Altmann and Steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see Dubey 2010 for a model of human sentence processing that can handle coreference). A key objective for future work will be to investigate models that integrate semantic constraint with syntactic predictions more tightly. For example, we could envisage a parser that uses semantic representations to guide its search, e.g., by pruning syntactic analyses that have a low semantic probability. At the same time, the semantic model should have access to syntactic information, i.e., the composition of word representations should take their syntactic relationships into account, rather than just linear order. Refer"
P10-1021,J01-2004,0,0.26386,"ords). (Rayner 1998) demonstrates that eye-movements are related to the moment-to-moment cognitive activities of readers. They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words. Some existing models of sentence processing integrate semantic information into a probabilistic parser (Narayanan and Jurafsky 2002; Pad´o et al. 2009); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation for a sentence. Furthermore"
P10-1021,P04-1003,0,0.185212,"rsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficulty. Other work (McDonald and Brew 2004) models contextual constraint in information theoretic terms. The assumption is that words carry prior semantic expectations which are updated upon seeing the next word. Expectations are represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. The measures discussed above are typically computed automatically on real-language corpora using data-driven methods and their predictions are verified through analysis of eye-movements that people make while reading. Ample evidence The analysis of reading times can provide insights into the proc"
P10-1021,D09-1034,0,0.451527,"ng times. Mc197 Donald and Shillcock (2003) show that forward and backward transitional probabilities are predictive of first fixation and first pass durations: the higher the transitional probability, the shorter the fixation time. Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk |wk−1 ). Analogously, forward probability is the conditional probability of the current word given the next word, P(wk |wk+1 ). (e.g., left-to-right vs. top-down, PCFGs vs dependency parsing) and different degrees of lexicalization (see Roark et al. 2009 for an overview) . For instance, unlexicalized surprisal can be easily derived by substituting the words in Equation (1) with parts of speech (Demberg and Keller 2008). Surprisal could be also defined using a vanilla language model that does not take any structural or grammatical information into account (Frank 2009). 2.2 2.3 Syntactic Constraint Distributional models of meaning have been commonly used to quantify the semantic relation between a word and its context in computational studies of lexical processing. These models are based on the idea that words with similar meanings will be foun"
P10-1021,P08-1028,1,0.735474,"ation is updated using a Bayesian inference mechanism to reflect the newly arrived information. Like LSA, ICD is based on word co-occurrence vectors, however it does not employ singular value decomposition, and constructs a word-word rather than a word-document co-occurrence matrix. Although this model has been shown to successfully simulate single- and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008). Their aim is not so much to model processing difficulty, but to construct vector-based meaning representations that go beyond individual words. They introduce a h = f (u, v) (2) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus deriv"
P10-1021,D09-1045,1,0.30952,"lated to the moment-to-moment cognitive activities of readers. They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words. Some existing models of sentence processing integrate semantic information into a probabilistic parser (Narayanan and Jurafsky 2002; Pad´o et al. 2009); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation for a sentence. Furthermore, the models of Narayanan and Jurafsky (2002) and Pad´o et al. (2009) do"
P10-1021,J07-2002,1,0.660376,"Missing"
P10-2012,W08-2304,1,0.934166,"n shown to successfully model garden path effects. Being based on probabilistic parsing techniques, ranking-based models generally achieve a broad coverage, but their efficiency and robustness has not been evaluated. Also, they are not designed to capture syntactic prediction or memory effects (other than search with a narrow beam in Brants and Crocker 2000). The ranking-based approach has been generalized by surprisal models (Surp), which predict processing difficulty based on the change in the probability distribution over possible analyses from one word to the next (Hale, 2001; Levy, 2008; Demberg and Keller, 2008a; Ferrara Boston et al., 2008; Roark et al., 2009). These models have been successful in accounting for a range of experimental data, and they achieve broad coverage. They also instantiate a limited form of prediction, viz., they build up expectations about the next word in the input. On the other hand, the efficiency and robustness of these models has largely not been evaluated, and memory costs are not modeled (again except for restrictions in beam size). The prediction model (Pred) explicitly predicts syntactic structure for upcoming words (Demberg and Keller, 2008b, 2009), thus accounting"
P10-2012,N01-1021,0,0.343723,"Both approaches have been shown to successfully model garden path effects. Being based on probabilistic parsing techniques, ranking-based models generally achieve a broad coverage, but their efficiency and robustness has not been evaluated. Also, they are not designed to capture syntactic prediction or memory effects (other than search with a narrow beam in Brants and Crocker 2000). The ranking-based approach has been generalized by surprisal models (Surp), which predict processing difficulty based on the change in the probability distribution over possible analyses from one word to the next (Hale, 2001; Levy, 2008; Demberg and Keller, 2008a; Ferrara Boston et al., 2008; Roark et al., 2009). These models have been successful in accounting for a range of experimental data, and they achieve broad coverage. They also instantiate a limited form of prediction, viz., they build up expectations about the next word in the input. On the other hand, the efficiency and robustness of these models has largely not been evaluated, and memory costs are not modeled (again except for restrictions in beam size). The prediction model (Pred) explicitly predicts syntactic structure for upcoming words (Demberg and"
P10-2012,P10-1120,0,0.043763,"nd Gibson (2005) Stewart et al. (2000); Kehler et al. (2008) the development of language processing models that combine syntactic processing with semantic and discourse processing. So far, this challenge is largely unmet: there are some examples of models that integrate semantic processes such as thematic role assignment into a parsing model (Narayanan and Jurafsky, 2002; Pad´o et al., 2009). However, other semantic factors are not accounted for by these models, and incorporating non-lexical aspects of semantics into models of sentence processing is a challenge for ongoing research. Recently, Dubey (2010) has proposed an approach that combines a probabilistic parser with a model of co-reference and discourse inference based on probabilistic logic. An alternative approach has been taken by Pynte et al. (2008) and Mitchell et al. (2010), who combine a vector-space model of semantics (Landauer and Dumais, 1997) with a syntactic parser and show that this results in predictions of processing difficulty that can be validated against an eye-tracking corpus. Table 2: Semantic factors in human language processing mentality by building fully connected trees. Memory costs are modeled directly as a distan"
P10-2012,P02-1017,0,0.0154723,"human language processor is the product of an acquisition process that is largely unsupervised and has access to only limited training data: children aged 12–36 months are exposed to between 10 and 35 million words of input (Hart and Risley, 1995). The challenge therefore is to develop a model of language acquisition that works with such small training sets, while also giving rise to a language processor that meets the key criteria in Table 1. The CL community is in a good position to rise to this challenge, given the significant progress in unsupervised parsing in recent years (starting from Klein and Manning 2002). However, none of the existing unsupervised models has been evaluated against psycholinguistic data sets, and they are not designed to meet even basic psycholinguistic criteria such as incrementality. A related modeling challenge is the development of processing models for languages other than English. There is a growing body of experimental research investigating human language processing in other languages, but virtually all existing psycholinguistic models only work for English (the only exceptions we are aware of are Dubey et al.’s (2008) and Ferrara Boston et al.’s Beyond Parsing There i"
P10-2012,P10-1021,1,0.916661,"ere are some examples of models that integrate semantic processes such as thematic role assignment into a parsing model (Narayanan and Jurafsky, 2002; Pad´o et al., 2009). However, other semantic factors are not accounted for by these models, and incorporating non-lexical aspects of semantics into models of sentence processing is a challenge for ongoing research. Recently, Dubey (2010) has proposed an approach that combines a probabilistic parser with a model of co-reference and discourse inference based on probabilistic logic. An alternative approach has been taken by Pynte et al. (2008) and Mitchell et al. (2010), who combine a vector-space model of semantics (Landauer and Dumais, 1997) with a syntactic parser and show that this results in predictions of processing difficulty that can be validated against an eye-tracking corpus. Table 2: Semantic factors in human language processing mentality by building fully connected trees. Memory costs are modeled directly as a distance-based penalty that is incurred when a prediction has to be verified later in the sentence. However, the current implementation of the prediction model is neither robust and efficient nor offers broad coverage. Recently, a stack-bas"
P10-2012,D09-1034,0,0.301118,"based on probabilistic parsing techniques, ranking-based models generally achieve a broad coverage, but their efficiency and robustness has not been evaluated. Also, they are not designed to capture syntactic prediction or memory effects (other than search with a narrow beam in Brants and Crocker 2000). The ranking-based approach has been generalized by surprisal models (Surp), which predict processing difficulty based on the change in the probability distribution over possible analyses from one word to the next (Hale, 2001; Levy, 2008; Demberg and Keller, 2008a; Ferrara Boston et al., 2008; Roark et al., 2009). These models have been successful in accounting for a range of experimental data, and they achieve broad coverage. They also instantiate a limited form of prediction, viz., they build up expectations about the next word in the input. On the other hand, the efficiency and robustness of these models has largely not been evaluated, and memory costs are not modeled (again except for restrictions in beam size). The prediction model (Pred) explicitly predicts syntactic structure for upcoming words (Demberg and Keller, 2008b, 2009), thus accounting for experimental results on predictive language pr"
P10-2012,D09-1065,0,0.0487669,"ese problems. As outlined in the previous section, a number of authors have evaluated psycholinguistic models against eye-tracking or reading time corpora. Part of the data and evaluation challenge is to extend this evaluation to neural data as provided by eventrelated potential (ERP) or brain imaging studies (e.g., using functional magnetic resonance imaging, fMRI). Neural data sets are considerably more complex than behavioral ones, and modeling them is an important new task that the community is only beginning to address. Some recent work has evaluated models of word semantics against ERP (Murphy et al., 2009) or fMRI data (Mitchell et al., 2008).4 This is a very promising direction, and the challenge is to extend this approach to the sentence and discourse level (see Bachrach 2008). Again, it will again be necessary to develop standardized test sets of both experimental data and corpus data. 3.3 4 Conclusions In this paper, we discussed the modeling and data/evaluation challenges involved in developing cognitively plausible models of human language processing. Developing computational models is of scientific importance in so far as models are implemented theories: models of language processing all"
P10-2012,J10-1001,0,0.0312582,"an be validated against an eye-tracking corpus. Table 2: Semantic factors in human language processing mentality by building fully connected trees. Memory costs are modeled directly as a distance-based penalty that is incurred when a prediction has to be verified later in the sentence. However, the current implementation of the prediction model is neither robust and efficient nor offers broad coverage. Recently, a stack-based model (Stack) has been proposed that imposes explicit, cognitively motivated memory constraints on the parser, in effect limiting the stack size available to the parser (Schuler et al., 2010). This delivers robustness, efficiency, and broad coverage, but does not model syntactic prediction. Unlike the other models discussed here, no psycholinguistic evaluation has been conducted on the stack-based model, so its cognitive plausibility is preliminary. 2.3 2.4 Acquisition and Crosslinguistics All models of human language processing discussed so far rely on supervised training data. This raises another aspect of the modeling challenge: the human language processor is the product of an acquisition process that is largely unsupervised and has access to only limited training data: childr"
P10-2012,C00-1017,0,\N,Missing
P14-2074,P11-2031,0,0.289656,"pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. 453 pn = c∈cand ∑ c∈cand  BP = N ∑ wn log pn n=1 ∑ ngram∈c ∑ countclip (ngram) ngram∈c 1 e(1−r/c) count(ngram) if c > r if c ≤ r Unigram BLEU without a brevity penalty has been reported by Kulkarni et al. (2011), Li et al. (2011), Ordonez et al. (2011), and Kuznetsova et al. (2012); to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is Elliott and Keller (2013). In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better. ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There is also a variant that measures the cooccurrence of pairs of tokens in both the candidate and reference (a skip-bigram): ROUGE - SU *. The skip-bigram calculation is parameterised with dskip , the maximum number of tokens between the words in the skip-bigram. Setting dskip to 0"
P14-2074,D13-1128,1,0.649925,"language processing have led to an upsurge of research on tasks involving both vision and language. State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image Figure 1: An image from the Flickr8K data set and five human-written descriptions. These descriptions vary in the adjectives or prepositional phrases that describe the woman (1, 3, 4, 5), incorrect or uncertain identification of the cat (1, 3), and include a sentence"
P14-2074,P12-1038,0,0.390137,"Missing"
P14-2074,W11-0326,0,0.512021,"on tasks involving both vision and language. State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image Figure 1: An image from the Flickr8K data set and five human-written descriptions. These descriptions vary in the adjectives or prepositional phrases that describe the woman (1, 3, 4, 5), incorrect or uncertain identification of the cat (1, 3), and include a sentence without a verb (5). (Yang et al., 2011), resul"
P14-2074,E12-1076,0,0.127084,"Missing"
P14-2074,J09-4008,0,0.0343498,"rieved from different images and show differences in how to describe an image. 5 some of which go beyond unigram matchings between references and candidates, whereas they only report unigram BLEU and unigram ROUGE. It is therefore difficult to directly compare the results of our correlation analysis against Hodosh et al.’s agreement analysis, but they also reach the conclusion that unigram BLEU is not an appropriate measure of image description performance. However, we do find stronger correlations with Smoothed BLEU , skip-bigram ROUGE , and Meteor. In contrast to the results presented here, Reiter and Belz (2009) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts. They did, however, find significant correlations of automatic measures against fluency judgements. There are no fluency judgements available for Flickr8K, but Elliott and Keller (2013) report grammaticality judgements for their data, which are comparable to fluency ratings. We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the Elliott and Keller (2013) data. This discrepancy could be"
P14-2074,D11-1041,0,\N,Missing
P14-2074,P02-1040,0,\N,Missing
P14-2074,P04-1077,0,\N,Missing
P14-2074,W11-2107,0,\N,Missing
P15-1115,P05-1022,0,0.381588,"m. These include arbitrarily longrange dependencies contained in a parse tree, and more importantly non-isomorphic representations of the input sentence such as its semantic frame, i.e., the set of all semantic roles tripes that pertain to the same predicate. In order to accommodate these, we decode via beam search over candidate parses. We keep a list of the k-best analyses and prune those whose score scr(x) = Φ(x, y) · w¯ falls below a threshold. 3.2 Incremental k-best Parsing What we described in the previous section could equally apply to k-best re-ranking for full-sentence parsing (e.g., Charniak and Johnson, 2005). For incremental parsing, in addition to outputting yˆ for the full sentence, we need to output prefix trees yˆn for every prefix of length n ∈ {1 . . . N} of sentence x = a1 . . . aN with length N. Let hxn , yˆn , ni, be the state of our model after we have parsed the first n words of sentence x, resulting in analysis yˆn . / 0i, where 0/ is The initial state is defined as hx0 , 0, the empty analysis, and the final state is hx, y, ˆ Ni, which represents a full analysis for the input sentence. We need a function ADV that transitions from a state at word an to a set of states at word 1193 3 4"
P15-1115,W02-1001,0,0.254765,"les are generated by removing either the argument, or the predicate, or the role label, from a complete triple. This provides a way of generalizing between triples that share some information without being completely identical. Predicate/Argument/Role encodes the elements of a complete SRL triple individually (argument, predicate, or role). This allows for further generalization and reduces sparsity. 1195 5 5 Feature Weight Estimation Algorithm 1: Averaged Structured Perceptron 1 We estimate the vector of feature weights w¯ in Equation (2) using the averaged structured perceptron algorithm of Collins (2002); we give the pseudocode in Algorithm 1. The perceptron makes T passes over L training examples. In each iteration, for each sentence prefix/prefix tree pair (xn , yn ), it computes the best scoring prefix tree yˆn among the candidate prefix trees, given the current feature weights w. ¯ In line 7, the algorithm updates w¯ with the difference (if any) between the feature representations of the best scoring prefix tree yˆn and the approximate gold-standard prefix tree y+ n (see Section 3.2). Note that since we use a constant beam during decoding with the PLTAG parser in order to enumerate the se"
P15-1115,J05-1003,0,0.311093,"can be re-constructed by following backpointers in the chart. This is done only for evaluation at the end of the sentence or incrementally on demand. Reranking Features This section describes the features used for reranking the prefix trees generated by the incremental parser. We include three different classes of features, based on local information from PLTAG elementary trees, based on global and structural information from prefix trees, and based on semantic information provided by iSRL triples. In contrast to work on discriminative full-sentence parsing (e.g., Charniak and Johnson, 2005; Collins and Koo, 2005), we can only use features extracted from the prefix trees being constructed incrementally as the sentence is parsed. The right context of the current word cannot be used, as this would violate incrementality. Every feature combination we try also includes the following baseline features: Prefix Tree Probability is the log probability of the prefix tree as scored by the probability model of the baseline parser. The score is normalized by prefix length, to avoid getting larger negative log probability scores for longer prefixes. Elementary Tree Probability is the log probability of the elementa"
P15-1115,P04-1015,0,0.316872,"incomplete triples: the first one is predicateincomplete, with the argument goals assigned an A0, waiting to be attached to a predicate. The second one is argument-incomplete with predicate realized assigned an A1, waiting for an argument to follow. (see Section 1). Note the use of incomplete semantic role triples in Figure 2b. 3 Model We use a discriminative model in order to re-rank the output of the baseline PLTAG parser based on semantic roles assigned by the iSRL system. 3.1 Problem Formulation Our overall approach is closely related to the discriminative incremental parsing framework of Collins and Roark (2004). The goal is to learn a mapping from input sentences x ∈ X to parse trees y ∈ Y . For a given set of training pairs of sentences and gold-standard parse trees (x, y) ∈ X × Y , the output yˆ can be defined as: yˆ = argmax Φ(x, y) · w¯ y∈GEN(x) (1) where GEN(x) is a function that enumerates candidate parse trees for a given input x, Φ is a representation that maps each training example (x, y) to a feature vector Φ(x, y) ∈ Rd , and w¯ ∈ Rd is a vector of feature weights. During training, the task is to estimate w¯ given the training examples. In terms of efficiency, a crucial part of Equation (1"
P15-1115,J13-4008,1,0.636189,"ine an incremental TAG parser with an incremental semantic role labeling (iSRL) system. The iSRL system takes prefix trees and computes their most likely semantic role assignments. We show that these role assignments can be used to re-rank the output of the incremental parser, leading to substantial improvements in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score. 2 Incremental Semantic Role Labeling The current work builds on an existing incremental parser, the Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG) parser of Demberg et al. (2013). The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. Konstas et al. (2014) show that semantic information can be attached to PLTAG structures, making it possible to assign semantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser. 2.1 Psycholinguistically Motivated TAG PLTAG extends standard TAG (Joshi and Schabes, 199"
P15-1115,W09-1205,0,0.0314195,"st of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and previous content to output partial dependency trees; then they output role labels given the full parser output. In contrast to all the joint approaches, we perform both parsing and semantic role labeling stric"
P15-1115,P08-1067,0,0.178743,"here GEN(x) is a function that enumerates candidate parse trees for a given input x, Φ is a representation that maps each training example (x, y) to a feature vector Φ(x, y) ∈ Rd , and w¯ ∈ Rd is a vector of feature weights. During training, the task is to estimate w¯ given the training examples. In terms of efficiency, a crucial part of Equation (1) is the search strategy over parses produced by GEN and, to a smaller degree, the dimensionality of w. ¯ One common decoding technique is to implement a dynamic program, thus avoiding the explicit enumeration of all analyses for a given timestamp (Huang, 2008). However, central to the discriminative approach is the exploration of features that cannot be straightforwardly embedded into the parser using a dynamic program. These include arbitrarily longrange dependencies contained in a parse tree, and more importantly non-isomorphic representations of the input sentence such as its semantic frame, i.e., the set of all semantic roles tripes that pertain to the same predicate. In order to accommodate these, we decode via beam search over candidate parses. We keep a list of the k-best analyses and prune those whose score scr(x) = Φ(x, y) · w¯ falls below"
P15-1115,P10-1110,0,0.0611814,"Missing"
P15-1115,C92-2066,0,0.66769,"Demberg et al. (2013). The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. Konstas et al. (2014) show that semantic information can be attached to PLTAG structures, making it possible to assign semantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser. 2.1 Psycholinguistically Motivated TAG PLTAG extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. To derive a TAG parse for a sentence, we start with the elementary tree of the head of the sentence and integrate the elementary trees of the other lexical items of the sentence using two operations: adjunction at an internal node and substitution at a substitution node (the node at which the operation applies is t"
P15-1115,D14-1036,1,0.669336,"ubstantial improvements in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score. 2 Incremental Semantic Role Labeling The current work builds on an existing incremental parser, the Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG) parser of Demberg et al. (2013). The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. Konstas et al. (2014) show that semantic information can be attached to PLTAG structures, making it possible to assign semantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser. 2.1 Psycholinguistically Motivated TAG PLTAG extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. To d"
P15-1115,P10-1113,0,0.0231158,"ncremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and prev"
P15-1115,J93-2004,0,0.0488098,". T do for i ← 1 . . . L do for n ← 1 . . . N do yˆn = argmaxyn ∈πn Φ(xn , yn ) · w¯ if y+ n 6= yˆn then w¯ ← w¯ + Φ(xn , y+ n ) − Φ(xn , yn ) 1 T 1 L 1 return T ∑t=1 L ∑i=1 ∑N n=1 N wt,i,n 6 6.1 Experiments Setup We use the PLTAG parser of Demberg et al. (2013) to enumerate prefix trees yn and to compute the prefix tree and word probability scores which we use as features. We also use the iSRL system of Konstas et al. (2014) to generate incremental SRL triples. Their system includes a semanticallyenriched lexicon extracted from the Wall Street Journal (WSJ) part of the Penn Treebank corpus (Marcus et al., 1993), converted to PLTAG format. Semantic role annotation is sourced from Propbank. We trained the probability model of the parser and the identification and labeling classifiers of the iSRL system using the intersection of Sections 2–21 of WSJ and the English portion of the CoNLL 2009 Shared Task (Hajiˇc et al., 2009). We learn the weight vector w¯ by training the perceptron algorithm also on Sections 2–21 of WSJ (see Section 5 for details). We use the PoS tags predicted by the parser, rather than gold standard PoS tags. Testing is performed on section 23 of WSJ, for sentences up to 40 words. 6.2"
P15-1115,N07-1050,0,0.0171619,", these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and previous content to output partial dependency trees; then they output role labels given the full parser output. In contrast to all the joint approaches, we perform both parsing and semantic role labeling strictly incrementally, without having access to the whole sentence, outputting prefix trees and iSRL triples for every sentence prefix. Our appro"
P15-1115,J01-2004,0,0.115552,"ints attained by the combination of all features in T REE +P LTAG +S RL. We also report combined SRL F-score computed on the re-ranked syntactic trees (rightmost column of Table 1). We find that compared to the baseline, only a small improvement of 0.55 points is achieved by T REE +P LTAG +S RL, while T REE +P LTAG improves by 0.84 points. The syntax-only variant therefore outperforms the full model, but only by a small margin. 7 Related Work The most similar approach in the literature is Collins and Roark’s (2004) re-ranking model for incremental parsing. They learn the syntactic features of Roark (2001) using the perceptron model of Collins (2002). Similar to us, they use the incremental parser to search over candidate parses. However, they limited themselves to local derivation features (akin to our PLTAG features), and do not explore global syntactic feature (tree features) or SRL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collin"
P15-1115,J10-1001,0,0.0515722,"Missing"
P15-1115,P11-1063,0,0.0505551,"Missing"
P15-1115,W04-0304,0,0.0394096,"nnis Konstas and Frank Keller Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh {ikonstas,keller}@inf.ed.ac.uk Abstract 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an , the parser only has access to the left context (words a1 . . . an−1 ); the right context (words an+1 . . . aN ) is not known yet. This can lead to local ambiguity, i.e., produce additional syntactic analyses that are valid for the sentence prefix, but become invalid as the right context is processed. As an example consider the sentence prefix in (1): Incremental parsing is the task of assigning a syntactic structure to an input sentence as it unfolds word by word"
P15-1115,W05-0636,0,0.0410017,"they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the"
P15-1115,P11-1021,0,0.0502082,"Missing"
P15-1115,J08-2002,0,0.0339219,"RL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce"
P15-1115,P07-1031,0,0.0201983,"s and syntactic features. Finally, our baseline is the PLTAG parser of Demberg et al. (2013), using the original probability model without any re-ranking. A comparison with other incremental parsers would be desirable, but is not trivial to achieve. This is because the PLTAG parser is trained and evaluated on a version of the Penn Treebank that was converted to PLTAG format. This renders our results not directly comparable to parsers that reproduce the Penn Treebank bracketing. For example, the PLTAG parser produces deeper tree structures informed by Propbank and the noun phrase annotation of Vadas and Curran (2007). BASELINE T REE S RL T REE +P LTAG T REE +P LTAG +S RL 0.85 F-score System BASELINE T REE S RL T REE +P LTAG T REE +P LTAG +S RL 10 20 Prefix Length 30 Figure 3: Incremental parsing F-score for increasing sentence prefixes, up to 40 words. 6.3 Results Figure 3 gives the results of evaluating incremental parsing performance. The x-axis shows prefix length, and the y-axis shows incremental F-score computed as suggested by Sangati and Keller (2013). Each point is averaged over all prefixes of a given length in the test set. To quantify the trends shown in this figure, we also compute the area un"
P15-1115,W08-2129,0,0.0238105,"though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce"
P15-1115,P02-1025,0,0.0738327,"Missing"
P15-1115,W09-1201,0,\N,Missing
P15-1115,Q13-1010,1,\N,Missing
P16-2094,K15-1038,1,0.354253,"the near future, including eyetracking by smartphone or webcam (Skovsgaard et al., 2013; Xu et al., 2015). Gaze patterns during reading are strongly influenced by the parts of speech of the words being read. Psycholinguistic experiments show that readers are less likely to fixate on closed-class words that are predictable from context. Readers also fixate longer on rare words, on words that are semantically ambiguous, and on words that are morphologically complex (Rayner, 1998). These findings indicate that eye-tracking data should be useful for classifying words by part of speech, and indeed Barrett and Søgaard (2015) show that word-type-level aggregate statistics collected from eye-tracking corpora can be used as features for supervised PoS tagging, leading to substantial gains in accuracy across domains. This leads us to hypothesize that gaze data should also improve weakly supervised PoS tagging. In this paper, we test this hypothesis by experimenting with a PoS tagging model that uses raw text, dictionary information, and eye-tracking For many of the world’s languages, there are no or very few linguistically annotated resources. On the other hand, raw text, and often also dictionaries, can be harvested"
P16-2094,N10-1083,0,0.0534428,"Missing"
P16-2094,D10-1056,0,0.0110893,"of eye-tracking data and significantly outperforms a baseline that does not have access to eye-tracking data. 1 Introduction According to Ethnologue, there are around 7,000 languages in the world.1 For most of these languages, no or very little linguistically annotated resources are available. This is why over the past decade or so, NLP researchers have focused on developing unsupervised algorithms that learn from raw text, which for many languages is widely available on the web. An example is part-ofspeech (PoS) tagging, in which unsupervised approaches have been increasingly successful (see Christodoulopoulos et al. (2010) for an overview). The performance of unsupervised PoS taggers can be improved further if dictionary information is available, making it possible to constrain the PoS 1 http://www.ethnologue.com/world 579 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 579–584, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 82 full baseline zi-2 xi-2 zi-1 xi-1 zi Dev. tagging accuracy 81 80 79 78 77 76 75 xi 74 Figure 1: Second-order HMM. In addition to the transitional probabilities of the antecedent state zi−1 in first-order"
P16-2094,W11-2123,0,0.0390121,"rden path sentences. 5. C ONTEXT features of the surrounding tokens. This group contains features relating to the fixations of the words in near proximity of the token. The eye can only recognize words a few characters to the left, and seven to eight characters to the right of the fixation (Rayner, 1998). Therefore it is useful to know the fixation pattern around the token. 6. N O G AZE BNC includes word length and word frequency obtained from the British National Corpus, as well as forward and backward transitional probabilities. These were computed using the KenLM language modeling toolkit (Heafield, 2011) with Kneser-Ney smoothing for unseen bigrams. 7. N O G AZE D UN includes the same features as N OGAZE BNC, but computed on the Dundee Corpus. They were extracted using CMUCambridge language modeling toolkit.4 Setup The Dundee Corpus does not include a standard train-development-test split, so we di4 http://www.speech.cs.cmu.edu/SLM/toolkit.html 582 All groups −N O G AZE BNC −N O G AZE D UN −BASIC −E ARLY −L ATE −R EG F ROM −C ONTEXT (Baseline) Accuracy 81.00 80.80 80.28 80.20 79.78 79.53 79.24 79.77 ∆ −0.20 −0.52* −0.08 −0.42* −0.25 −0.29* +0.53* Table 4: Results of an ablation study over fea"
P16-2094,D12-1127,0,0.025679,"Missing"
P17-2011,D15-1021,0,0.0161648,"created a dataset of 3.5k images sampled from the MSCOCO and TUHOI datasets and annotated it with 90 verbs and their OntoNotes senses to distinguish different verb senses using visual context. This is the first dataset that aims to annotate all visual senses Visual Genome: The dataset created by Krishna et al. (2016) has dense annotations of objects, at66 lighted and addressed in recent video action recognition datasets (Caba Heilbron et al., 2015; Sigurdsson et al., 2016), which include generic household activities. An analysis of various image description and question answering datasets by Ferraro et al. (2015) shows the bias in the distribution of word categories. Image description datasets have a higher distribution of nouns compared to other word categories, indicating that the descriptions are object specific, limiting their usefulness for action-based tasks. of a verb. However, the total number of images annotated and number of images for some senses is relatively small, which makes it difficult to use this dataset to train models. The authors further divided their 90 verbs into motion and non-motion verbs according to Levin (1993) verb classes and analyzed visual ambiguity in the task of visua"
P17-2011,P98-1013,0,0.0492615,"recognition tasks, pointing out their strengths and weaknesses. We survey existing literature and provide insights into existing datasets and models for action recognition tasks. 2 2.1 Identifying Visual Verbs and Verb Senses The limitations with early datasets (small scale, domain specificity, and the use of ad-hoc labels that combine verb and object) have been recently addressed in a number of broad-coverage datasets that offer linguistically motivated labels. Often these datasets use existing linguistic resources such as VerbNet (Schuler, 2005), OntoNotes (Hovy et al., 2006) and FrameNet (Baker et al., 1998) to classify verbs and their senses. This allows for a more general, semantically motivated treatment of verbs and verb phrases, and also takes into account that not all verbs are depictable. For example, abstract verbs such as presuming and acquiring are not depictable at all, while other verbs have both depictable and non-depictable senses: play is non-depictable in playing with emotions, but depictable in playing instrument and playing sport. The process of identifying depictable verbs or verb senses is used by Ronchi and Perona (2015), Gella et al. (2016) and Yatskar et al. (2016) to ident"
P17-2011,N16-1022,1,0.609128,"ravel on back of an animal ride-2: sit on and control a vehicle ride-3: be carried in a vehicle verb: riding agent: girl vehicle: horse place: park Figure 2: Categorization of action recognition tasks in images riding horse and riding elephant both instantiate the same verb semantics, i.e., riding animal. Thirdly, existing action labels miss generalizations across verbs, e.g., the fact that fixing bike and repairing bike are semantically equivalent, in spite of the use of different verbs. These observations have led authors to argue that actions should be analyzed at the level of verb senses. Gella et al. (2016) propose the new task of visual verb sense disambiguation (VSD), in which a verb– image pair is annotated with a verb sense taken from an existing lexical database (OntoNotes in this case). While VSD handles distinction between different verb senses, it does not identify or localize the objects that participate in the action denoted by the verb. Recent work (Gupta and Malik, 2015; Yatskar et al., 2016) has filled this gap by proposing the task of visual semantic role labeling (VSRL), in which images are labeled with verb frames, and the objects that fill the semantic roles of the frame are ide"
P17-2011,N06-2015,0,0.0253203,"provide a unified view of action recognition tasks, pointing out their strengths and weaknesses. We survey existing literature and provide insights into existing datasets and models for action recognition tasks. 2 2.1 Identifying Visual Verbs and Verb Senses The limitations with early datasets (small scale, domain specificity, and the use of ad-hoc labels that combine verb and object) have been recently addressed in a number of broad-coverage datasets that offer linguistically motivated labels. Often these datasets use existing linguistic resources such as VerbNet (Schuler, 2005), OntoNotes (Hovy et al., 2006) and FrameNet (Baker et al., 1998) to classify verbs and their senses. This allows for a more general, semantically motivated treatment of verbs and verb phrases, and also takes into account that not all verbs are depictable. For example, abstract verbs such as presuming and acquiring are not depictable at all, while other verbs have both depictable and non-depictable senses: play is non-depictable in playing with emotions, but depictable in playing instrument and playing sport. The process of identifying depictable verbs or verb senses is used by Ronchi and Perona (2015), Gella et al. (2016)"
P17-2011,C14-1115,0,0.0532313,"Missing"
P17-2011,P16-2031,0,0.0445004,"Missing"
P17-2011,N16-1019,0,0.174647,"of an animal ride-2: sit on and control a vehicle ride-3: be carried in a vehicle verb: riding agent: girl vehicle: horse place: park Figure 2: Categorization of action recognition tasks in images riding horse and riding elephant both instantiate the same verb semantics, i.e., riding animal. Thirdly, existing action labels miss generalizations across verbs, e.g., the fact that fixing bike and repairing bike are semantically equivalent, in spite of the use of different verbs. These observations have led authors to argue that actions should be analyzed at the level of verb senses. Gella et al. (2016) propose the new task of visual verb sense disambiguation (VSD), in which a verb– image pair is annotated with a verb sense taken from an existing lexical database (OntoNotes in this case). While VSD handles distinction between different verb senses, it does not identify or localize the objects that participate in the action denoted by the verb. Recent work (Gupta and Malik, 2015; Yatskar et al., 2016) has filled this gap by proposing the task of visual semantic role labeling (VSRL), in which images are labeled with verb frames, and the objects that fill the semantic roles of the frame are ide"
P17-2011,P10-1023,0,0.0576473,"Missing"
P17-2011,D11-1041,0,\N,Missing
P17-2011,C98-1013,0,\N,Missing
P19-1338,D15-1075,0,\N,Missing
P19-1338,P14-1041,0,\N,Missing
P19-1338,D13-1170,0,\N,Missing
P19-1338,W18-5452,0,\N,Missing
P19-1338,D18-1544,0,\N,Missing
P19-1338,N19-1114,0,\N,Missing
P19-1338,N18-1101,0,\N,Missing
P19-1338,N16-1103,0,\N,Missing
Q13-1010,H05-1025,0,0.12505,"g et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists (Bod, 1995; Bod et al., 2003), but none of the existing TSG parsers are incremental. We show how constraints can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG"
Q13-1010,E95-1015,0,0.176636,"ediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists (Bod, 1995; Bod et al., 2003), but none of the existing TSG parsers are incremental. We show how constraints can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers. 111 Transactions of the Association for Computational Linguistics, 1 (2013) 111–124. Action Editor: David Chiang. c Submitted 10/2012; Revised 2/2013; Published 5/2013. 2013 Association for Computational Linguistics. TSG fragments can be arbitrarily large and can contain multi"
Q13-1010,P81-1022,0,0.776709,"fragments taking part in the derivation: P(d) = ∏ P( f ) (4) f ∈d Since the grammar may generate a tree t via multiple derivations D(t) = d1 , d2 , . . . , dm , the probability of the parse tree is the sum of the probabilities of the ITSG derivations in D(t): P(t) = ∑ d∈D(t) 3 P(d) = ∑ ∏ P( f ) (5) d∈D(t) f ∈d Probabilistic ITSG Parser We introduce a probabilistic chart-parsing algorithm to efficiently compute all possible incremental derivations that an ITSG can generate given an input sentence (presented one word at the time). The parsing algorithm is an adaptation of the Earley algorithm (Earley, 1970) and its probabilistic instantiation (Stolcke, 1995). 3.1 Parsing Chart A TSG incremental derivation is represented in the chart as a sequence of chart states, i.e., a path. For a given fringe in an incremental derivation, there will be one or more states in the chart, depending on the length of the fringe’s yield. This is because we need to keep track of the extent to which the yield of each fringe has been consumed within a derivation as the sentence is processed incrementally.5 At the given stage of the derivation, the states offer a compact representation over the partial structures genera"
Q13-1010,W03-2502,0,0.223706,"00; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists (Bod, 1995; Bod et al., 2003), but none of the existing TSG parsers are incremental. We show how constraints can be imposed on the shape of the TSG fragments to enable incremental process"
Q13-1010,P96-1024,0,0.118041,"chart. For training and evaluating the ITSG parser, we employ the Penn WSJ Treebank (Marcus et al., 1993). We use sections 2–21 for training, section 22 and 24 for development and section 23 for testing. 3.6 Minimum Risk Parse (MRP) MPD and MPP aim at obtaining the structure of a sentence which is more likely as a whole under the current probabilistic model. Alternatively, we may want to focus on the single components of a tree structures, e.g., CFG rules covering a certain span of the sentence, and search for the structure which has the highest number of correct constituents, as proposed by Goodman (1996). Such structure is more likely to obtain higher results according to standard parsing evaluations, as the objective being maximized is closely related to the metric used for evaluation (recall/precision on the number of correct labeled constituents). 10 For each scan state in the path, we obtain the fragment in the grammar that maps into the state’s fringe. For ambiguous fringes the most probable fragment that maps into it is selected. 119 Experiments 4.1 Grammar Extraction Following standard practice, we start with some preprocessing of the treebank. After removing traces and functional tags"
Q13-1010,P10-1110,0,0.263391,"spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner,"
Q13-1010,P03-1054,0,0.0314836,"according to standard parsing evaluations, as the objective being maximized is closely related to the metric used for evaluation (recall/precision on the number of correct labeled constituents). 10 For each scan state in the path, we obtain the fragment in the grammar that maps into the state’s fringe. For ambiguous fringes the most probable fragment that maps into it is selected. 119 Experiments 4.1 Grammar Extraction Following standard practice, we start with some preprocessing of the treebank. After removing traces and functional tags, we apply right binarization on the training treebank (Klein and Manning, 2003), with no horizontal and vertical conditioning. This means that when a node X has more than two children, new artificial constituents labeled X@ are created in a right recursive fashion (see Figure 1).13 We then replace words appearing less than five times in the training data by one of 50 unknown word categories based on the presence of lexical features as described in Petrov (2009). Fragment Extraction In order to equip the grammar with a representative set of lexicalized fragments, we use the extraction algorithm of Sangati 11 For an ambiguous fringe, the spanning probability of each fragme"
Q13-1010,J93-2004,0,0.0427386,"t to compute the MPP we need to retrieve all possible derivations of the current sentence, sum up the probabilities of those generating the same tree, and returning the tree with max marginal probability. Unfortunately the number of possible derivations grows exponentially with the length of the sentence, and computing the exact MPP is NP-hard (Sima’an, 1996). In our implementation, we approximate the MPP by performing this marginalization over the Viterbi-best derivations obtained from all stop states in the chart. For training and evaluating the ITSG parser, we employ the Penn WSJ Treebank (Marcus et al., 1993). We use sections 2–21 for training, section 22 and 24 for development and section 23 for testing. 3.6 Minimum Risk Parse (MRP) MPD and MPP aim at obtaining the structure of a sentence which is more likely as a whole under the current probabilistic model. Alternatively, we may want to focus on the single components of a tree structures, e.g., CFG rules covering a certain span of the sentence, and search for the structure which has the highest number of correct constituents, as proposed by Goodman (1996). Such structure is more likely to obtain higher results according to standard parsing evalu"
Q13-1010,N07-1050,0,0.50979,"eech as it is spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst"
Q13-1010,J01-2004,0,0.914677,"95; Altmann and Kamide, 1999). Also language processing systems often deal with speech as it is spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so fa"
Q13-1010,D09-1034,0,0.0203492,"xity; the results obtained were substantially worse than those obtained using Roark’s parsers. 18 Note that neither PRD(m) nor PRS(m) correspond to word error rate (WER). PRD requires the predicted word sequence to be identical to the original sequence, while PRS only requires the predicted words to be present in the original. In contrast, WER measures the minimum number of substitutions, insertions, and deletions needed to transform the predicted sequence into the original sequence. 19 Apart from reporting the results in Roark (2001), we also run the latest version of Roark’s parser, used in Roark et al. (2009), which has higher results compared to the original work. 86 P 79.4 83.7 86.5 87.5 83.5 83.6 85.8 83.5 83.6 85.6 96 F1 79.4 94 83.5 86.5 92 87.6 82.5 90 82.6 84.1 88 83.2 83.4 86 84.8 Roark (last) ITSG Smooth. (MPD)(MPD) ITSG Smooth. Roark (last) Roark et al. (2009) ITSG Smooth. (MPD) 99 97 96 98 95 97 94 93 96 92 95 91 90 94 89 93 88 87 92 F-score F-score Demberg et al. (2014) Schuler et al. (2010) 94 Roark (2001) Roark et al. (2009) 92 ITSG (MPD) ITSG (MPP) 90 ITSG (MRP) ITSG Smoothing (MPD) 88 ITSG Smoothing (MPP) ITSG Smoothing (MRP) R 79.4 83.4 86.6 87.7 81.5 81.6 82.6 83.0 83.2 83.9 F-sc"
Q13-1010,P80-1024,0,0.809222,"for strong equivalence between ITSG and CFG. As an example, the left side of Figure 5 shows a CFG that contains a left-recursive rule. The types of structures this grammar can generate (such as the one given on the right side of the same figure) are characterized by an arbitrarily long chain of rules that can intervene before the second word of the string, “b”, is generated. Given the incrementality constraints, there is no ITSG that can generate the same set of structures that this CFG can generate. However, it may be possible to circumvent this problem by applying the left-corner transform (Rosenkrantz and Lewis, 1970; Aho and Ullman, 1972) to generate an equivalent CFG without left-recursive rules. 2.3 Probabilistic Grammar In the generative process presented above there are a number of choices which are left open, i.e., which fragment is being introduced at a specific stage of a derivation, and when the generative process terminates. A symbolic ITSG can be equipped with 115 P(Y ) + P( finit ) = 1 (1) X P( flex ) = 1 (∀X ∈ N ) (2) finit ∈Finit ∑ Y ∈F Y fsub sub Y P( fsub ) = 1 (∀Y ∈ N ) (3) The probability that an ITSG generates a specific derivation d is obtained by multiplying the probabilities of the"
Q13-1010,sangati-etal-2010-efficiently,1,0.906842,"Missing"
Q13-1010,J10-1001,0,0.428586,"nd Kamide, 1999). Also language processing systems often deal with speech as it is spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models an"
Q13-1010,P11-1063,0,0.24607,"question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, whi"
Q13-1010,C96-2215,0,0.192574,"Missing"
Q13-1010,J95-2002,0,0.600351,"P( f ) (4) f ∈d Since the grammar may generate a tree t via multiple derivations D(t) = d1 , d2 , . . . , dm , the probability of the parse tree is the sum of the probabilities of the ITSG derivations in D(t): P(t) = ∑ d∈D(t) 3 P(d) = ∑ ∏ P( f ) (5) d∈D(t) f ∈d Probabilistic ITSG Parser We introduce a probabilistic chart-parsing algorithm to efficiently compute all possible incremental derivations that an ITSG can generate given an input sentence (presented one word at the time). The parsing algorithm is an adaptation of the Earley algorithm (Earley, 1970) and its probabilistic instantiation (Stolcke, 1995). 3.1 Parsing Chart A TSG incremental derivation is represented in the chart as a sequence of chart states, i.e., a path. For a given fringe in an incremental derivation, there will be one or more states in the chart, depending on the length of the fringe’s yield. This is because we need to keep track of the extent to which the yield of each fringe has been consumed within a derivation as the sentence is processed incrementally.5 At the given stage of the derivation, the states offer a compact representation over the partial structures generated so far. 5A fringe (state) may occur in multiple"
Q13-1010,W04-0304,0,0.306447,"nd for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing"
Q13-1010,P11-1021,0,0.427581,"processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined"
Q13-1010,P02-1025,0,0.738829,"before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a"
Q13-1010,J13-4008,1,\N,Missing
W02-1030,W00-1702,0,0.0497868,"Missing"
W02-1030,H01-1052,0,0.018896,"Missing"
W02-1030,P01-1005,0,0.0242974,"Missing"
W02-1030,E99-1005,1,0.887228,"Missing"
W02-1030,P01-1046,1,0.886591,"Missing"
W02-1030,N01-1009,1,0.837112,"Missing"
W02-1030,P99-1004,0,0.0698162,"Missing"
W02-1030,P99-1020,0,0.10063,"Missing"
W02-1030,P99-1068,0,0.0638918,"Missing"
W02-1030,1999.tc-1.8,0,\N,Missing
W04-2002,C00-1017,0,0.0352985,"Missing"
W04-2002,W98-1505,0,0.0151558,"ntext-free grammar. They are commonly used for broad-coverage grammars, as CFGs large enough to parse unrestricted text are typically highly ambiguous, i.e., a single sentence will receive a large number of parses. The probabilistic component of the grammar can then be used to rank the analyses a sentence might receive, and improbable ones can be eliminated. In the computational linguistics literature, a number of highly successful extensions to the basic PCFG model have been proposed. Of particular interest are lexicalized parsing models such as the ones developed by Collins (1996, 1997) and Carroll and Rooth (1998). In the human parsing literature, a PCFG-based model has been proposed by Jurafsky (1996) and Narayanan and Jurafsky (1998). This model shows how different sources of probabilistic information (such as subcategorization information and rule frequencies) can be combined using Bayesian inference. The model accounts for a range of disambiguation phenomena in linguistic processing. However, the model is only small scale, and it is not clear if it can be extended to provide robustness and coverage of unrestricted text. This problem is addressed by Brants and Crocker (2000) and Crocker and Brants ("
W04-2002,A00-2018,0,0.254301,"tain a realistic estimate of how well these models achieve the aim of providing robust, broad coverage models of human parsing. This can only be assessed by testing the models against realistic samples of unrestricted text or speech obtained from corpora. In this talk, we will present work that aims to perform such an evaluation. We train a series of increasingly sophisticated probabilistic parsing models on an identical training set (the Penn Treebank). These models include a standard unlexicalized PCFG parser, a head-lexicalized parser (Collins, 1997), and a maximum-entropy inspired parser (Charniak, 2000). We test all three models on the Embra corpus, a corpus of newspaper texts annotated with eye-tracking data from 23 subjects (McDonald and Shillcock, 2003). A series of regression analyses are conducted to determine if persentence reading time measures correlate with sentence probabilities predicted by the parsing models. Three baseline models are also included in the evaluation: word frequency, bigram and trigram probability (as predicted by a language model), and part of speech (POS) probability (as predicted by a POS tagger). Models based on n-grams have already been used successfully to m"
W04-2002,P96-1025,0,0.0365077,"analyses produced by a context-free grammar. They are commonly used for broad-coverage grammars, as CFGs large enough to parse unrestricted text are typically highly ambiguous, i.e., a single sentence will receive a large number of parses. The probabilistic component of the grammar can then be used to rank the analyses a sentence might receive, and improbable ones can be eliminated. In the computational linguistics literature, a number of highly successful extensions to the basic PCFG model have been proposed. Of particular interest are lexicalized parsing models such as the ones developed by Collins (1996, 1997) and Carroll and Rooth (1998). In the human parsing literature, a PCFG-based model has been proposed by Jurafsky (1996) and Narayanan and Jurafsky (1998). This model shows how different sources of probabilistic information (such as subcategorization information and rule frequencies) can be combined using Bayesian inference. The model accounts for a range of disambiguation phenomena in linguistic processing. However, the model is only small scale, and it is not clear if it can be extended to provide robustness and coverage of unrestricted text. This problem is addressed by Brants and Cro"
W04-2002,P97-1003,0,0.0254423,"ilable in the literature. This makes it very hard to obtain a realistic estimate of how well these models achieve the aim of providing robust, broad coverage models of human parsing. This can only be assessed by testing the models against realistic samples of unrestricted text or speech obtained from corpora. In this talk, we will present work that aims to perform such an evaluation. We train a series of increasingly sophisticated probabilistic parsing models on an identical training set (the Penn Treebank). These models include a standard unlexicalized PCFG parser, a head-lexicalized parser (Collins, 1997), and a maximum-entropy inspired parser (Charniak, 2000). We test all three models on the Embra corpus, a corpus of newspaper texts annotated with eye-tracking data from 23 subjects (McDonald and Shillcock, 2003). A series of regression analyses are conducted to determine if persentence reading time measures correlate with sentence probabilities predicted by the parsing models. Three baseline models are also included in the evaluation: word frequency, bigram and trigram probability (as predicted by a language model), and part of speech (POS) probability (as predicted by a POS tagger). Models b"
W04-2002,W04-3241,1,0.770714,"from 23 subjects (McDonald and Shillcock, 2003). A series of regression analyses are conducted to determine if persentence reading time measures correlate with sentence probabilities predicted by the parsing models. Three baseline models are also included in the evaluation: word frequency, bigram and trigram probability (as predicted by a language model), and part of speech (POS) probability (as predicted by a POS tagger). Models based on n-grams have already been used successfully to model eye-tracking data, both on a word-by-word basis (McDonald and Shillcock, 2003) and for whole sentences (Keller, 2004). Our results show that for all three parsing models, sentence probability is significantly correlated with reading times measures. However, the models differ as to whether they predict early or late measures: the PCFG and the Collins model significantly predict late reading time measures (total time and gaze duration), but not early measures (first fixation time and skipping rate). The Charniak model is able to significantly predict both early and late measures. An analysis of the baseline models shows that word frequency and POS probability only predict early measures, while bigram and trigr"
W04-2002,J93-2004,0,0.0353505,"Missing"
W04-3241,P02-1026,0,0.403411,"te principle, which predicts that the entropy of a sentence increases with its position in the text. We show that this principle holds for individual sentences (not just for averages), but we also find that the entropy rate effect is partly an artifact of sentence length, which also correlates with sentence position. Secondly, we evaluate a set of predictions that the entropy rate principle makes for human language processing; using a corpus of eye-tracking data, we show that entropy and processing effort are correlated, and that processing effort is constant throughout a text. 1 Introduction Genzel and Charniak (2002, 2003) introduce the entropy rate principle, which states that speakers produce language whose entropy rate is on average constant. The motivation for this comes from information theory: the most efficient way of transmitting information through a noisy channel is at a constant rate. If human communication has evolved to be optimal in this sense, then we would expect humans to produce text and speech with approximately constant entropy. There is some evidence that this is true for speech (Aylett, 1999). For text, the entropy rate principle predicts that the entropy of an individual sentence i"
W04-3241,W03-1009,0,0.743094,"then we would expect humans to produce text and speech with approximately constant entropy. There is some evidence that this is true for speech (Aylett, 1999). For text, the entropy rate principle predicts that the entropy of an individual sentence increases with its position in the text, if entropy is measured out of context. Genzel and Charniak (2002) show that this prediction is true for the Wall Street Journal corpus, for both function words and for content words. They estimate entropy either using a language model or using a probabilistic parser; the effect can be observed in both cases. Genzel and Charniak (2003) extend this results in several ways: they show that the effect holds for different genres (but the effect size varies across genres), and also applies within paragraphs, not only within whole texts. Furthermore, they show that the effect can also be obtained for language other than English (Russian and Spanish). The entropy rate principle also predicts that a language model that takes context into account should yield lower entropy estimates compared to an out of context language model. Genzel and Charniak (2002) show that this prediction holds for caching language models such as the ones pro"
W06-1637,H94-1020,0,0.0505219,"rom the fact that categories merely encode unsatisfied subcategorized arguments. Given that a transitive verb has the same category as the constituent formed by a ditransitive verb and its direct object, we would expect that both categories can prime each other, if they are cognitive units. More generally, we would expect that lexical (terminal) and phrasal (non-terminal) categories of the same syntactic type may prime each other. The interaction of such conditions with the priming effect can be quantified in the statistical model. 3.3 4 Corpus Data 4.1 The Switchboard Corpus The Switchboard (Marcus et al., 1994) corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees. Dialogues were recorded over the telephone among randomly paired North American speakers, who were just given a general topic to talk about. 80,000 utterances of the corpus have been annotated with syntactic structure. This portion, included in the Penn Treebank, has been timealigned (per word) in the Paraphrase project (Carletta et al., 2004). Using the same regression technique as employed here, Reitter et al. (2006b) found a marked structural priming effect for Penn-Treebank style phra"
W06-1637,P94-1018,0,0.0435542,"syntactic structure. This portion, included in the Penn Treebank, has been timealigned (per word) in the Paraphrase project (Carletta et al., 2004). Using the same regression technique as employed here, Reitter et al. (2006b) found a marked structural priming effect for Penn-Treebank style phrase structure rules in Switchboard. Incrementality of Analyses Type-raising and composition allow derivations that are mostly left-branching, or incremental. Adopting a left-to-right processing order for a sentence is important, if the syntactic theory is to make psycholinguistically viable predictions (Niv, 1994; Steedman, 2000). Pickering et al. (2002) present priming experiments that suggest that, in production, structural dominance and linearization do not take place in different stages. Their argument involves verbal phrases with a shifted prepositional object such as showed to the mechanic a torn overall. At a dominance-only level, such phrases are equivalent to non-shifted prepositional constructions (showed a torn overall to the mechanic), but the two variants may be differentiated at a linearization stage. Shifted primes do not prime prepositional objects in their canonical position, thus pri"
W06-1637,carletta-etal-2004-using,0,0.0285793,"of such conditions with the priming effect can be quantified in the statistical model. 3.3 4 Corpus Data 4.1 The Switchboard Corpus The Switchboard (Marcus et al., 1994) corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees. Dialogues were recorded over the telephone among randomly paired North American speakers, who were just given a general topic to talk about. 80,000 utterances of the corpus have been annotated with syntactic structure. This portion, included in the Penn Treebank, has been timealigned (per word) in the Paraphrase project (Carletta et al., 2004). Using the same regression technique as employed here, Reitter et al. (2006b) found a marked structural priming effect for Penn-Treebank style phrase structure rules in Switchboard. Incrementality of Analyses Type-raising and composition allow derivations that are mostly left-branching, or incremental. Adopting a left-to-right processing order for a sentence is important, if the syntactic theory is to make psycholinguistically viable predictions (Niv, 1994; Steedman, 2000). Pickering et al. (2002) present priming experiments that suggest that, in production, structural dominance and lineariza"
W06-1637,P04-1014,0,0.0138117,"gle right-branching normal form derivation (Eisner, 1996) for each possible semantic interpretation. Such normal form derivations only use composition and type-raising where syntactically necessary (eg. in relative clauses). CCG (Steedman, 2000) is a mildly contextsensitive, lexicalized grammar formalism with a transparent syntax-semantics interface and a flexible constituent structure that is of particular interest to psycholinguistics, since it allows the construction of incremental derivations. CCG has also enjoyed the interest of the NLP community, with high-accuracy wide-coverage parsers(Clark and Curran, 2004; Hockenmaier and Steedman, 2002) and generators1 available (White and Baldridge, 2003). Words are associated with lexical categories which specify their subcategorization behaviour, eg. ((S[dcl]NP)/NP)/NP is the lexical category for (tensed) ditransitive verbs in English such as gives or send, which expect two NP objects to their right, and one NP subject to their left. Complex categories X/Y or XY are functors which yield a constituent with category X, if they are applied to a constituent with category Y to their right (/Y) or to their left (Y). Constituents are combined via a small set o"
W06-1637,W03-2316,0,0.0830081,"c interpretation. Such normal form derivations only use composition and type-raising where syntactically necessary (eg. in relative clauses). CCG (Steedman, 2000) is a mildly contextsensitive, lexicalized grammar formalism with a transparent syntax-semantics interface and a flexible constituent structure that is of particular interest to psycholinguistics, since it allows the construction of incremental derivations. CCG has also enjoyed the interest of the NLP community, with high-accuracy wide-coverage parsers(Clark and Curran, 2004; Hockenmaier and Steedman, 2002) and generators1 available (White and Baldridge, 2003). Words are associated with lexical categories which specify their subcategorization behaviour, eg. ((S[dcl]NP)/NP)/NP is the lexical category for (tensed) ditransitive verbs in English such as gives or send, which expect two NP objects to their right, and one NP subject to their left. Complex categories X/Y or XY are functors which yield a constituent with category X, if they are applied to a constituent with category Y to their right (/Y) or to their left (Y). Constituents are combined via a small set of combinatory rule schemata: X/Y Y X X/Z X X/Z T/(TX) X the man SNP Combinatory Cat"
W06-1637,P06-1053,1,0.789682,"Grammar David Reitter Julia Hockenmaier Frank Keller School of Informatics Inst. for Res. in Cognitive Science School of Informatics University of Edinburgh University of Pennsylvania University of Edinburgh 2 Buccleuch Place 3401 Walnut Street 2 Buccleuch Place Edinburgh EH8 9LW, UK Philadelphia PA 19104, USA Edinburgh EH8 9LW, UK dreitter@inf.ed.ac.uk juliahr@cis.upenn.edu keller@inf.ed.ac.uk Abstract recsanyi, 2005), consistent with the experimental literature, but also generalize to syntactic rules across the board, which repeated more often than expected by chance (Reitter et al., 2006b; Dubey et al., 2006). In the present paper, we build on this corpus-based approach to priming, but focus on the role of the underlying syntactic representations. In particular, we use priming to evaluate claims resulting from a particular syntactic theory, which is a way of testing the representational assumptions it makes. Using priming effects to inform syntactic theory is a novel idea; previous corpus-based priming studies have simply worked with uncontroversial classes of constructions (e.g., passive/active). The contribution of this paper is to overcome this limitation by defining a computational model of pr"
W06-1637,P96-1011,0,0.604915,"NP I saw and NPNP you NP (SNP)/NP conj NP &gt;T &gt; heard the man (SNP)/NP &gt;T S/(SNP) S/(SNP) S/NP &gt;B S/NP &gt;B &lt;Φ&gt; S &gt; The combinatory rules of CCG allow multiple, semantically equivalent, syntactic derivations of the same sentence. This spurious ambiguity is the result of CCG’s flexible constituent structure, which can account for long-range dependencies and coordination (as in the above example), and also for interaction with information structure. CCG parsers often limit the use of the combinatory rules (in particular: type-raising) to obtain a single right-branching normal form derivation (Eisner, 1996) for each possible semantic interpretation. Such normal form derivations only use composition and type-raising where syntactically necessary (eg. in relative clauses). CCG (Steedman, 2000) is a mildly contextsensitive, lexicalized grammar formalism with a transparent syntax-semantics interface and a flexible constituent structure that is of particular interest to psycholinguistics, since it allows the construction of incremental derivations. CCG has also enjoyed the interest of the NLP community, with high-accuracy wide-coverage parsers(Clark and Curran, 2004; Hockenmaier and Steedman, 2002) a"
W06-1637,P02-1043,1,0.852294,"al form derivation (Eisner, 1996) for each possible semantic interpretation. Such normal form derivations only use composition and type-raising where syntactically necessary (eg. in relative clauses). CCG (Steedman, 2000) is a mildly contextsensitive, lexicalized grammar formalism with a transparent syntax-semantics interface and a flexible constituent structure that is of particular interest to psycholinguistics, since it allows the construction of incremental derivations. CCG has also enjoyed the interest of the NLP community, with high-accuracy wide-coverage parsers(Clark and Curran, 2004; Hockenmaier and Steedman, 2002) and generators1 available (White and Baldridge, 2003). Words are associated with lexical categories which specify their subcategorization behaviour, eg. ((S[dcl]NP)/NP)/NP is the lexical category for (tensed) ditransitive verbs in English such as gives or send, which expect two NP objects to their right, and one NP subject to their left. Complex categories X/Y or XY are functors which yield a constituent with category X, if they are applied to a constituent with category Y to their right (/Y) or to their left (Y). Constituents are combined via a small set of combinatory rule schemata: X/Y"
W06-1637,N06-2031,1,\N,Missing
W06-1637,P04-1005,0,\N,Missing
W08-2304,W00-1307,0,0.522358,"Missing"
W08-2304,W04-0308,0,\N,Missing
W08-2304,W04-0302,0,\N,Missing
W08-2304,P97-1012,0,\N,Missing
W08-2304,J01-2004,0,\N,Missing
W08-2304,P91-1012,0,\N,Missing
W08-2304,J05-1004,0,\N,Missing
W08-2304,H05-1102,0,\N,Missing
W08-2304,P07-1031,0,\N,Missing
W08-2304,meyers-etal-2004-annotating,0,\N,Missing
W09-0110,N01-1021,0,0.136075,"Missing"
W09-0110,W08-2304,1,\N,Missing
W10-2001,D07-1043,0,0.0112285,"es. In effect this condition is a pseudo-baseline, testing the effects of less- or noninformative sentence features on our proposed models. 4 V I(C, K) = H(C) + H(K) − 2I(C, K) = H(C|K) + H(K|C) A lower score implies closer clusterings, since each clustering has less information not shared with the other: two identical clusterings have a VI of zero. However, VI’s upper bound is dependent on the maximum number of clusters in C or K, making it difficult to compare clustering results with different numbers of clusters. As a third, and, in our view, most informative measure, we use V-measure (VM; Rosenberg and Hirschberg (2007)). Like VI, VM uses the conditional entropy of clusters and categories to evaluate clusterings. However, it also has the useful characteristic of being analogous to the precision and recall measures commonly used in NLP. Homogeneity, the precision analogue, is defined as VH = 1− H(C|K) . H(C) VH is highest when the distribution of categories within each cluster is highly skewed towards a small number of categories, such that the conditional entropy is low. Completeness (recall) is defined symmetrically to VH as: H(K|C) VC = 1 − . H(K) Evaluation Measures Evaluation of fully unsupervised part o"
W10-2001,P07-1094,1,0.843606,"evious work investigating the usefulness of this kind of information for syntactic category acquisition. In other domains, intonation has been used to identify sentence types as a means of improving speech recognition language models. Specifically, (Taylor et al., 1998) found that using intonation to recognize dialogue acts (which to a significant extent correspond to sentence types) and then using a specialized language model for each type of dialogue act led to a significant decrease in word error rate. In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information. We then discuss the distinctions in sentence type that we used and our evaluation measures, and finally our experimental results. We perform experiments on corpora in four different languages: English, Spanish, Cantonese, and Dutch. Our results on Spanish show no difference between the baseline and the models incorporating sentence type, possibly due to the small size of the Spanish corpus. Results on all other corpora show a small improvement in pe"
