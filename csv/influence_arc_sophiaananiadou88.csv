1994.tc-1.2,J93-2001,0,0.166331,"re has been much recent work on collocation, especially in computational lexicography and computational linguistics. Investigations of large collections of general language texts have shown how important a knowledge of collocation is for any language user. Church et al. (6) discuss, for example, the similarities and differences in sense between collocations involving &apos;strong&apos; and those involving &apos;powerful&apos;. These two words are often defined in terms of each other, yet one cannot simply replace one with the other in combination with some other form, for the most part. In the same spirit, Biber (7) looks at collocations involving &apos;certain&apos; and &apos;sure&apos;. Translators are always searching for ""the right way of saying something"" — for the right collocation, we might say, in many instances. It is more than a question of being terminologically accurate, it is also a question of formulating a sentence or phrase such that it sounds as if it belongs to the type of language under study. As in general language, so in special languages, in fact even more so: special languages (sublanguages) have remarkably high incidences of collocation, as is apparent from a brief scan of any special language collec"
1994.tc-1.2,J93-1007,0,0.0967713,"ive device: it carries greater communicative weight than in general language. Up to now, we have avoided defining what we (or others) mean by &apos;collocation&apos;. Unfortunately, definitions of collocation are numerous and varied. Some researchers include multi-element compounds as examples of collocations; some admit only collocations consisting of pairs of words, while others admit collocations consisting of up to, say, five or six words (there may be intervening material); some emphasise syntagmatic aspects, others semantic aspects. The common points regarding collocations appear to be, as Smadja (9) suggests: they are arbitrary, they are domain-dependent, they are recurrent and lastly the occurrence of one word (or more) strongly influences the occurrence of others. It is not a goal of this paper to offer yet another definition of collocation. However, what we can observe is that there is, from a terminologist&apos;s point of view, little advantage to be gained in viewing multi-element compound terms as collocations: they are terms, with all that this implies. The fact that their elements may occur in combination may be useful as one of the guides to recognition of unknown compounds, however"
1994.tc-1.2,J93-2004,0,0.0290182,"C/DTI has supported collaborative industryacademia projects such as the British National Corpus Initiative, ACRONYM (collocation retrieval of thesaurally related items) and DRAFTER (assistant for technical writers to produce drafts in English or in French). The reader is advised to contact Dr Peter Lee, Department of Trade and Industry, 151 Buckingham Palace Road, London, UK, SW1W 9SS for further details of these projects. In the USA, there is a major corpus project at the University of Pennsylvania, which as well as building corpora is developing numerous tools to explore them (Marcus et al. (17)). Almost every corpus project is engaged in building tools to process their texts, there being few suitable tools on the market. 27 The spin-off from all these projects should therefore be significant in terms firstly of tools to explore corpora or text collections and, eventually, grammars, dictionaries, resources and full-blown natural language processing systems and other aids built on the results of all this corpus work. Our discussion will remain general as we wish rather to point out potentially useful techniques. Fortunately, the contribution by Erlandsen in this volume describes one o"
1994.tc-1.2,C94-1084,0,0.0332607,"terms. To a certain extent they do, however often forms that are terms are not picked up by associative techniques, as no evidence can be found to propose a strong association among the elements of e.g. multiword compound terms. Straightforward counting of frequency of occurrence can help (on the hypothesis that frequently occurring forms should represent the most important concepts of specialised texts) but is also misleading: one finds elements being proposed as terms that clearly do not have such status. Recently, there has been an increase in research into this entire area. Daille et al. (20) propose an approach combining statistical and linguistic techniques, applied to aligned texts (original plus translation), to discover compound terms, where the statistical techniques used are sensitive to both frequency and association characteristics of the data. A disadvantage of this work, as with all such 29 work involving aligned texts, is that the quality of the translation is always in doubt, thus results must be interpreted with caution. A good overview of the problems of extracting multiword compound terms is given by Lauriston (21). The particular form that terms take, in various t"
1994.tc-1.2,C94-2167,1,0.750768,"of extracting multiword compound terms is given by Lauriston (21). The particular form that terms take, in various text types and subject areas, is furthermore critical to their recognition as terms. Each domain has its preferred methods of term formation. It is important to have knowledge about term formation possibilities and to know how formations may be affected by a change of register, of text type, of communicative situation and so on. Among the various types of term formation are: derivation, compounding, back formation, borrowing, simile, conversion, compression, and so on. Ananiadou (22) investigates how linguistic knowledge of term formation can be used to drive a term recogniser. Such knowledge is also highly useful in aiding the translator or terminologist in the synthesis of terms and in helping her to decide how to realise a concept in some context. What is clear from research in this area is that certain types of term formation are quite intractable at present, from the point of view of trying to recognise them in running text. Also, it is clear that even successful processing can only hope to propose potential occurrences of terms. Human interpretation must in the fina"
1998.tc-1.15,J85-2001,0,0.097272,"Missing"
2009.jeptalnrecital-demonstration.5,P06-1059,0,0.0192317,"Missing"
2009.jeptalnrecital-demonstration.5,2009.mtsummit-wpt.10,0,0.0604881,"Missing"
2009.jeptalnrecital-demonstration.5,sekine-nobata-2004-definition,0,0.0281166,"Missing"
2020.acl-main.669,P16-1105,0,0.0345685,". We conclude that entity types provide a strong inductive bias for URE.1 1 Introduction Relation extraction (RE) extracts semantic relations between entities from plain text. For instance, “Jon Robin Baitzhead , born in Los Angelestail ...” expresses the relation /people/person/place of birth between the two head-tail entities. Extracted relations are then used for several downstream tasks such as information retrieval (Corcoglioniti et al., 2016) and knowledge base construction (Al-Zaidy and Giles, 2018). RE has been widely studied using fully supervised learning (Nguyen and Grishman, 2015; Miwa and Bansal, 2016; Zhang et al., 2017, 2018) and distantly supervised approaches (Mintz et al., 2009; Riedel et al., 2010; Lin et al., 2016). Unsupervised relation extraction (URE) methods have not been explored as much as fully or distantly supervised learning techniques. URE is promising, since it does not require manually annotated data nor human curated knowledge bases (KBs), which are expensive to produce. Therefore, it can be applied to domains and languages where 1 Source code is available at https://github.com/ ttthy/ure annotated data and KBs are not available. Moreover, URE can discover new relation"
2020.acl-main.669,W15-1506,0,0.0288024,"e important features in URE. We conclude that entity types provide a strong inductive bias for URE.1 1 Introduction Relation extraction (RE) extracts semantic relations between entities from plain text. For instance, “Jon Robin Baitzhead , born in Los Angelestail ...” expresses the relation /people/person/place of birth between the two head-tail entities. Extracted relations are then used for several downstream tasks such as information retrieval (Corcoglioniti et al., 2016) and knowledge base construction (Al-Zaidy and Giles, 2018). RE has been widely studied using fully supervised learning (Nguyen and Grishman, 2015; Miwa and Bansal, 2016; Zhang et al., 2017, 2018) and distantly supervised approaches (Mintz et al., 2009; Riedel et al., 2010; Lin et al., 2016). Unsupervised relation extraction (URE) methods have not been explored as much as fully or distantly supervised learning techniques. URE is promising, since it does not require manually annotated data nor human curated knowledge bases (KBs), which are expensive to produce. Therefore, it can be applied to domains and languages where 1 Source code is available at https://github.com/ ttthy/ure annotated data and KBs are not available. Moreover, URE can"
2020.acl-main.669,D07-1043,0,0.802776,"eration setting in distantly supervised RE, i.e., aligning a large amount of raw text against triplets in a curated KB. A standard metric score is computed by comparing the output relation clusters against the automatically annotated relations. In particular, the NYT-FB dataset (Marcheggiani and Titov, 2016) which is used for evaluation, has been created by mapping relation triplets in Freebase (Bollacker et al., 2008) against plain text articles in the New York Times (NYT) corpus (Sandhaus, 2008). Standard clustering evaluation metrics for URE include B3 (Bagga and Baldwin, 1998), V-measure (Rosenberg and Hirschberg, 2007), and ARI (Hubert and Arabie, 1985). Although the above mentioned experimental setting can be created automatically, there are three challenges to overcome. Firstly, the development and test sets are silver, i.e., they include noisy labelled instances, since they are not human-curated. Secondly, the development and test sentences are part of the training set, i.e., a transductive setting. 7498 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7498–7505 c July 5 - 10, 2020. 2020 Association for Computational Linguistics It is thus unclear how well th"
2020.acl-main.669,D17-1004,0,0.534283,"ty types provide a strong inductive bias for URE.1 1 Introduction Relation extraction (RE) extracts semantic relations between entities from plain text. For instance, “Jon Robin Baitzhead , born in Los Angelestail ...” expresses the relation /people/person/place of birth between the two head-tail entities. Extracted relations are then used for several downstream tasks such as information retrieval (Corcoglioniti et al., 2016) and knowledge base construction (Al-Zaidy and Giles, 2018). RE has been widely studied using fully supervised learning (Nguyen and Grishman, 2015; Miwa and Bansal, 2016; Zhang et al., 2017, 2018) and distantly supervised approaches (Mintz et al., 2009; Riedel et al., 2010; Lin et al., 2016). Unsupervised relation extraction (URE) methods have not been explored as much as fully or distantly supervised learning techniques. URE is promising, since it does not require manually annotated data nor human curated knowledge bases (KBs), which are expensive to produce. Therefore, it can be applied to domains and languages where 1 Source code is available at https://github.com/ ttthy/ure annotated data and KBs are not available. Moreover, URE can discover new relation types, since it is n"
2020.acl-main.669,D11-1135,0,\N,Missing
2020.acl-main.669,P09-1113,0,\N,Missing
2020.acl-main.669,P04-1053,0,\N,Missing
2020.acl-main.669,P16-1200,0,\N,Missing
2020.acl-main.669,D18-1244,0,\N,Missing
2020.acl-main.669,P19-1133,0,\N,Missing
2020.coling-main.507,W99-0201,0,0.34152,"iability and instance complexity can be incorporated into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene,"
2020.coling-main.507,2020.lrec-1.6,0,0.0167226,"label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et a"
2020.coling-main.507,chaimongkol-etal-2014-corpus,0,0.0133183,"hat incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; R"
2020.coling-main.507,L16-1323,0,0.371655,"ference on Computational Linguistics, pages 5760–5773 Barcelona, Spain (Online), December 8-13, 2020 General Label Type Discourse New (DN) Discourse Old (DO) Non-Referring (NR) Property (PR) Description The mention is a new entity in the text The mention refers to an entity which has already been introduced The mention refers to no actual entity (e.g., it in expletive constructions) The mention refers to a property of an entity (e.g., the most durable light is a property of the bulb) Figure 1: An example (adapted from the Phrase Table 1: Four general label types in a coreferDetectives Corpus (Chamberlain et al., 2016)) of ence resolution labelling task (Chamberlain et al., crowd-sourced coreference annotation. 2016). mention, by taking into account the annotation complexity of the mention and annotators’ reliability. The first challenge in proposing the encoder is how to incorporate mention context information. We explore the use of contextual embeddings for this purpose. The second challenge is how to effectively model annotators’ behaviour in terms of their quality of annotation. Modelling annotator reliability is helpful in detecting unreliable annotators and in facilitating appropriate task allocation"
2020.coling-main.507,N19-1423,0,0.024144,"erent mentions that appear in the set of mentions classified as DO(·) or PR(·)3 labels. Each annotator’s label is weighted by the product of the annotator’s category and the overall/per-instance reliability. 4 Experiments Dataset: We evaluate our method on the real-world dataset from (Chamberlain et al., 2016), which includes both crowd labels produced by 280 crowd workers and expert labels for 5,654 mentions (3,277 DNs, 2,192 DOs, 136 PRs and 49 NRs). Mention Contextual Embedding: We compare the use of two pre-trained embeddings from ELMo4 (Peters et al., 2018) and BERT (bert-base-uncased)5 (Devlin et al., 2019). When using BERT, we represent each token by using the BERT model outputs from the last four hidden layers, which is the same setting as used in (Peters et al., 2018). Learning: We use the Adam (Kingma and Ba, 2015) optimiser (α = 0.001, β1 = 0.9, β2 = 0.999). λ1 , λ2 and λ3 are set to 0.0001, 0.005 and 0.5, respectively. We pre-train the encoder for 100 epochs. 2 The details are discussed in Section 6.3. For example, DO(mention1) or PR(mention1) indicates that this annotator labels the current mention as referring to another mention mention1. 4 Original(5.5B): https://allennlp.org/elmo 5 We"
2020.coling-main.507,doddington-etal-2004-automatic,0,0.0186799,"plexity can be incorporated into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008;"
2020.coling-main.507,K15-1020,0,0.0135229,", 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model"
2020.coling-main.507,garcia-gamallo-2014-multilingual,0,0.023439,"ntal results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al.,"
2020.coling-main.507,L16-1021,0,0.0166079,"context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018;"
2020.coling-main.507,N15-1117,0,0.0207658,"d complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al"
2020.coling-main.507,guillou-etal-2014-parcor,0,0.0243891,"ct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt e"
2020.coling-main.507,M98-1029,0,0.0589758,"n about context, annotator reliability and instance complexity can be incorporated into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification"
2020.coling-main.507,N13-1132,0,0.0385466,"; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Late"
2020.coling-main.507,P14-2062,0,0.13373,"al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, th"
2020.coling-main.507,D15-1261,0,0.414246,"rs were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, these models can not be applied to the corefer"
2020.coling-main.507,R11-1036,0,0.0602846,"Missing"
2020.coling-main.507,W17-2314,1,0.821671,"crowd-sourced coreference annotation. 2016). mention, by taking into account the annotation complexity of the mention and annotators’ reliability. The first challenge in proposing the encoder is how to incorporate mention context information. We explore the use of contextual embeddings for this purpose. The second challenge is how to effectively model annotators’ behaviour in terms of their quality of annotation. Modelling annotator reliability is helpful in detecting unreliable annotators and in facilitating appropriate task allocation (Donmez and Carbonell, 2008; Donmez and Carbonell, 2010; Li et al., 2017). Modelling only per-category reliability may not be sufficient to characterise annotators’ behaviour patterns for a given annotation task. The original encoder from (Yin et al., 2017) already estimates the per-category reliability. We additionally model overall and per-instance reliability. In addition, we also model the instance complexity. In the second subtask, i.e. coreference chain inference, based on the predicted general classes in the first subtask, we predict each mention’s target (i.e., its referent entity which is usually another mention in the text). If a mention is classified as"
2020.coling-main.507,N19-1295,1,0.703608,"7; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015)"
2020.coling-main.507,P19-3010,0,0.0652096,"6) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, these models can not be applied to the coreference annotation in a straightforward manner, because coreference labels are m"
2020.coling-main.507,H05-1004,0,0.075939,"and made her pick them up again, and clean and brighten them. Table 4: Instances with lowest and highest complexities as estimated by our model. Mentions are highlighted in bold. We then run the entire autoencoder training by optimising the objective function in Equation (11) until either 300 iterations are reached, or the objective function stops improving. Evaluation: The baselines are: majority voting and the state-of-the-art method, Mention-Pair Annotation model (Paun et al., 2018). Four metrics are used for evaluation, MUC (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and CoNLL Score (Pradhan et al., 2011). 5 Results From Table 3, we observe that our method achieved better performance than baselines.6 We also report the performance using different settings. Without Context and ELMo/BERT denote the models that do not use context, or which use context, respectively. In terms of reliability, each annotator’s PerCategory Reliability is modelled in our model by default. Per-Category + Overall / Per-Instance Reliability indicates that the model supplements per-category reliability with modelling of annotator overall or per-instance reliability. As shown in Tabl"
2020.coling-main.507,P17-1028,0,0.0605923,"e reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, these models can not be applied to the coreference annotation in a"
2020.coling-main.507,P18-1019,0,0.0667627,"l. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or sequence labels (Hovy et al., 2014; Rodrigues et al., 2014; Huang et al., 2015; Nguyen et al., 2017; Nye et al., 2018; Yang et al., 2018; Lin et al., 2019). Note that Raykar et al. (2010) and Felt et al. (2015) also included contextual information. Raykar et al. (2010) incorporated a classifier into their Bayesian model. The classifier took an instance’s representation as its input and then predicted an answer. The unsupervised Latent Dirichlet Allocation topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels. However, these models can not be applied to the coreference annotation in a straightforward ma"
2020.coling-main.507,D18-1218,0,0.400688,"notators have to determine an appropriate referent mention for some mentions. Because the performance of supervised learning models is highly dependent on the quality of training data, the aggregation of these noisy labels, (i.e., the process of determining the label that is most likely to be correct) is important to obtain a high-quality training corpus. Although label aggregation is a well-studied topic, most existing studies of natural language labelling tasks have only focused on aggregating classification or sequence labels. To the best of our knowledge, there is only one previous study (Paun et al., 2018) that has investigated how to aggregate crowd-sourced coreference labels. In this paper, we propose a 2-step framework in which the aggregation task is broken down into two subtasks, i.e., mention classification and coreference chain inference. In the mention classification subtask, our model predicts the general category of a mention as shown in Table 1. Our model is based on the autoencoder proposed in (Yin et al., 2017), but with significant extensions. Our encoder is a classifier which takes as its input the crowd labels for each mention, together with the mention’s context information. Th"
2020.coling-main.507,N18-1202,0,0.0129429,"DO or PR, we only aggregate those crowd-sourced referent mentions that appear in the set of mentions classified as DO(·) or PR(·)3 labels. Each annotator’s label is weighted by the product of the annotator’s category and the overall/per-instance reliability. 4 Experiments Dataset: We evaluate our method on the real-world dataset from (Chamberlain et al., 2016), which includes both crowd labels produced by 280 crowd workers and expert labels for 5,654 mentions (3,277 DNs, 2,192 DOs, 136 PRs and 49 NRs). Mention Contextual Embedding: We compare the use of two pre-trained embeddings from ELMo4 (Peters et al., 2018) and BERT (bert-base-uncased)5 (Devlin et al., 2019). When using BERT, we represent each token by using the BERT model outputs from the last four hidden layers, which is the same setting as used in (Peters et al., 2018). Learning: We use the Adam (Kingma and Ba, 2015) optimiser (α = 0.001, β1 = 0.9, β2 = 0.999). λ1 , λ2 and λ3 are set to 0.0001, 0.005 and 0.5, respectively. We pre-train the encoder for 100 epochs. 2 The details are discussed in Section 6.3. For example, DO(mention1) or PR(mention1) indicates that this annotator labels the current mention as referring to another mention mention"
2020.coling-main.507,W11-1901,0,0.0983738,"Missing"
2020.coling-main.507,W12-4501,0,0.0391908,"ed into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010;"
2020.coling-main.507,D08-1027,0,0.561537,"Missing"
2020.coling-main.507,M95-1002,0,0.378523,"e how information about context, annotator reliability and instance complexity can be incorporated into our encoder network to infer correct labels. c) Experimental results demonstrate that incorporating mention context, annotator reliability and instance complexity can increase the accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused"
2020.coling-main.507,M95-1005,0,0.601386,"eadows. ’Pull off my boots,’ and then he threw them in her face, and made her pick them up again, and clean and brighten them. Table 4: Instances with lowest and highest complexities as estimated by our model. Mentions are highlighted in bold. We then run the entire autoencoder training by optimising the objective function in Equation (11) until either 300 iterations are reached, or the objective function stops improving. Evaluation: The baselines are: majority voting and the state-of-the-art method, Mention-Pair Annotation model (Paun et al., 2018). Four metrics are used for evaluation, MUC (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and CoNLL Score (Pradhan et al., 2011). 5 Results From Table 3, we observe that our method achieved better performance than baselines.6 We also report the performance using different settings. Without Context and ELMo/BERT denote the models that do not use context, or which use context, respectively. In terms of reliability, each annotator’s PerCategory Reliability is modelled in our model by default. Per-Category + Overall / Per-Instance Reliability indicates that the model supplements per-category reliability with modelling of annotator"
2020.coling-main.507,Q18-1042,0,0.0205613,"e accuracy of correct label prediction. Moreover, we conduct a comprehensive analysis that shows the learned complexity and reliability are explainable. 2 Related Work There have been many released coreference corpora in which the annotations were collected from a small number of in-house annotators such as experts (Sundheim, 1995; Hirschman and Chinchor, 1998; Bagga and Baldwin, 1999; Doddington et al., 2004; Pradhan et al., 2012; Singh et al., 2012; Guillou et al., 2014; Garcia and Gamallo, 2014; Chaimongkol et al., 2014; Ghaddar and Langlais, 2016; Cohen et al., 2017; Fonseca et al., 2017; Webster et al., 2018; Bamman et al., 2020; Tsvetkova, 2020). These annotators were assumed to be reliable. Guha et al. (2015) and Chamberlain et al. (2016) attempted to collect coreference annotations from non-expert crowd annotators. Even though crowd aggregation has been studied for many years, most existing studies have focused on aggregating classification labels (Dawid and Skene, 1979; Snow et al., 2008; Raykar et al., 2010; Hovy et al., 2013; Li et al., 2014; Felt et al., 2015; Zheng et al., 2017; Yin et al., 2017; Rodrigues and Pereira, 2018; Guan et al., 2018; Li et al., 2019; Zhang et al., 2019) or seque"
2020.lrec-1.245,C18-1195,0,0.0458436,"Missing"
2020.lrec-1.245,W17-2314,1,0.848799,"e a valuable resource that can be reused in training and evaluating different tools and algorithms. We address some of the problems of manual corpus construction by using APLenty, an easy-to-use, web-based annotation system (Nghiem and Ananiadou, 2018), which aims to maximise both the efficiency and effectiveness of annotation. Firstly, incorporation of active learning (Settles, 2009) means that annotators are only asked to label the most informative and representative examples, rather than the entire corpus, thus reducing the time required. Secondly, the use of proactive learning algorithms (Li et al., 2017) helps to maximise annotation quality by automatically determining which annotator is likely to label an instance most accurately, based on their previous performance. 4. 4.1. Annotation Scheme Design Structured Knowledge Representations Ontologies and classification systems constitute models of domain-specific knowledge. They define fundamental concept types, and specify how these concepts are linked via various types of relations to encode different types of knowledge. Automatically extracting and structuring knowledge from text according to such models can enhance effective sharing and reus"
2020.lrec-1.245,D19-1069,0,0.017195,"attributes. Thus, we will need to consider other ontologies and/or terminological resources, which may include: the Industry Foundation Classes (IFC) (ISO, 2013), which covers building elements at various levels of granularity, and is used in BIM; Uniclass (NBS, 2015), which includes various types of activities, products, tools and locations; and clinical/medical ontologies, such as SNOMED CT (Donnelly, 2006), which covers body parts and injuries. Secondly, we will create a knowledge graph by extracting and structuring knowledge contained in construction safety documents. An end-to-end model (Mesquita et al., 2019) will be implemented to extract NEs and relations between them. We will enrich the domain-specific entities detected with models trained using our corpus with the aid of resources such as ConceptNet (Speer et al., 2017) and DBpedia (Lehmann et al., 2015), which, despite being concerned with general language, may still help to detect entities that are relevant within this domain. Subsequently, we will follow an Open Information Extraction approach to the discovery of relations (Cetto et al., 2018), which can extract a wide range of relation types, without requiring a training corpus in which re"
2020.lrec-1.245,D18-2019,1,0.933056,"odelling of hazards, sources and consequences in PAS 1192-6 (BSI, 2018) Figure 1: Part of the Construction Safety Ontology (Zhang et al., 2015) is vital if the corpus is to be useful for machine learning purposes. Such consistency can usually only be attained by creating detailed annotation guidelines and training annotators. However, once constructed, annotated corpora are a valuable resource that can be reused in training and evaluating different tools and algorithms. We address some of the problems of manual corpus construction by using APLenty, an easy-to-use, web-based annotation system (Nghiem and Ananiadou, 2018), which aims to maximise both the efficiency and effectiveness of annotation. Firstly, incorporation of active learning (Settles, 2009) means that annotators are only asked to label the most informative and representative examples, rather than the entire corpus, thus reducing the time required. Secondly, the use of proactive learning algorithms (Li et al., 2017) helps to maximise annotation quality by automatically determining which annotator is likely to label an instance most accurately, based on their previous performance. 4. 4.1. Annotation Scheme Design Structured Knowledge Representation"
2021.eacl-demos.28,D09-1031,0,0.0492255,"points for annotation instead of annotating all instances of the unlabeled data (Settles, 2009). Some recent research has also utilized proactive learning, in which the system is allowed to assign specific unlabeled instances to specific annotators (Li et al., 2019). The annotators, in these scenarios, only have to annotate a small set of representative and informative data which they can provide reliable labels. It helps reduce the labelling effort and at the same time makes the best use of available annotators. To date, there are many tools available for active learning, such as the TexNLP (Baldridge and Palmer, 2009), the Active-Learning-Scala (Santos and Carvalho, 2014), the JCLAL (Reyes et al., 2016), the LibAct (Yang et al., 2017) libraries, the Vowpal Wabbit1 . These tools, however, focus only on the active learning algorithms and provide no user interface thus making it difficult to use for the end-users. On the other hand, several tools have been made with user-friendly interface such 1 http://hunch.net/˜vw/ as BRAT (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), PubAnnotation (Kim and Wang, 2012), doccano2 . Some of the tools offer active/proactive learning such as APLenty (Nghiem and Anani"
2021.eacl-demos.28,N19-1423,0,0.00757849,"abel for a particular label. The system will update the estimation after every annotation batch. Figure 3: Annotation interface. The displayed sentence was taken from the Sentiment140 dataset. All labels are shown in the blue rectangle box with the shortcut keys next to them. Annotated labels are shown above the sentence. 3 Use cases The typical use cases of Paladin are as following: informative documents, regardless of the class. Paladin currently employs the least confidence uncertainty-based strategy (Culotta and McCallum, 2005) based on the classification outputs from a Transformer model (Devlin et al., 2019). A linear model is added to the embedding output to predict the score for the labels. Previous research has established that active learning can increase the performance of Transformer-based text classifiers (Grießhaber et al., 2020). With the second option, the system uses the same classification outputs but unlabelled instances are taken from each class in equal amounts. The default option in Paladin is the second one. This setting aims to minimise the unbalanced data problems where we have unequal instances for different classes. Paladin uses pool-based sampling scenario, where the data sa"
2021.eacl-demos.28,2020.coling-main.100,0,0.0257257,"angle box with the shortcut keys next to them. Annotated labels are shown above the sentence. 3 Use cases The typical use cases of Paladin are as following: informative documents, regardless of the class. Paladin currently employs the least confidence uncertainty-based strategy (Culotta and McCallum, 2005) based on the classification outputs from a Transformer model (Devlin et al., 2019). A linear model is added to the embedding output to predict the score for the labels. Previous research has established that active learning can increase the performance of Transformer-based text classifiers (Grießhaber et al., 2020). With the second option, the system uses the same classification outputs but unlabelled instances are taken from each class in equal amounts. The default option in Paladin is the second one. This setting aims to minimise the unbalanced data problems where we have unequal instances for different classes. Paladin uses pool-based sampling scenario, where the data samples are chosen for labeling from the unlabeled dataset. The project manager, however, can upload additional unlabeled data to an existing annotation project at anytime. 2.4 1. A user wishes to add more data to an existing dataset to"
2021.eacl-demos.28,W12-2425,0,0.0372578,"ators. To date, there are many tools available for active learning, such as the TexNLP (Baldridge and Palmer, 2009), the Active-Learning-Scala (Santos and Carvalho, 2014), the JCLAL (Reyes et al., 2016), the LibAct (Yang et al., 2017) libraries, the Vowpal Wabbit1 . These tools, however, focus only on the active learning algorithms and provide no user interface thus making it difficult to use for the end-users. On the other hand, several tools have been made with user-friendly interface such 1 http://hunch.net/˜vw/ as BRAT (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), PubAnnotation (Kim and Wang, 2012), doccano2 . Some of the tools offer active/proactive learning such as APLenty (Nghiem and Ananiadou, 2018), DUALIST (Settles and Zhu, 2012), AlpacaTag (Lin et al., 2019), Discrete Active Learning Coref (Li et al., 2020a). Currently, these tools support sequence labelling/coreference resolution tasks but not document classification tasks. To the best of our knowledge, there is no such tool for document classification which supports active/proactive learning. Prodigy3 supports active learning for both sequence labelling and document classification tasks but it is a commercial product. To compen"
2021.eacl-demos.28,2020.acl-main.738,0,0.0284842,"al., 2017) libraries, the Vowpal Wabbit1 . These tools, however, focus only on the active learning algorithms and provide no user interface thus making it difficult to use for the end-users. On the other hand, several tools have been made with user-friendly interface such 1 http://hunch.net/˜vw/ as BRAT (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), PubAnnotation (Kim and Wang, 2012), doccano2 . Some of the tools offer active/proactive learning such as APLenty (Nghiem and Ananiadou, 2018), DUALIST (Settles and Zhu, 2012), AlpacaTag (Lin et al., 2019), Discrete Active Learning Coref (Li et al., 2020a). Currently, these tools support sequence labelling/coreference resolution tasks but not document classification tasks. To the best of our knowledge, there is no such tool for document classification which supports active/proactive learning. Prodigy3 supports active learning for both sequence labelling and document classification tasks but it is a commercial product. To compensate for the lack of available document-level annotation tool, we develop Paladin (Proactive learning annotator for document instances), an open-source web-based system for creating labelled data using active/proactive"
2021.eacl-demos.28,N19-1295,1,0.837303,"r multi-label settings, the system is flexible and can be adapted to other tasks in single-label settings. 1 Introduction Labelled data is essential in many NLP tasks based on Machine Learning. Manually annotating such data is time-consuming, and require a lot of human effort. Active learning has been used to ease this process by choosing the data points for annotation instead of annotating all instances of the unlabeled data (Settles, 2009). Some recent research has also utilized proactive learning, in which the system is allowed to assign specific unlabeled instances to specific annotators (Li et al., 2019). The annotators, in these scenarios, only have to annotate a small set of representative and informative data which they can provide reliable labels. It helps reduce the labelling effort and at the same time makes the best use of available annotators. To date, there are many tools available for active learning, such as the TexNLP (Baldridge and Palmer, 2009), the Active-Learning-Scala (Santos and Carvalho, 2014), the JCLAL (Reyes et al., 2016), the LibAct (Yang et al., 2017) libraries, the Vowpal Wabbit1 . These tools, however, focus only on the active learning algorithms and provide no user"
2021.eacl-demos.28,W17-2314,1,0.583554,"taset: the user can choose “maintain class balance” option in Settings. With this option, the model will try to select more data from the potential minority classes for annotation. Proactive learning In many annotation tasks, we assume that the annotators are experts who always provide correct annotations. But in reality, different annotators have different levels of expertise in different domains. It has been demonstrated that proactive learning is helpful for task allocation in crowdsourcing setting where the level of expertise varies from annotator to annotator (Donmez and Carbonell, 2010; Li et al., 2017, 2019, 2020b). Proactive learning is 4 Experiments and Results 4.1 Simulated Annotators We used the Toxic Comment Classification Challenge dataset5 for this experiment. The dataset contains Wikipedia comments which have been manually labelled for toxic behaviour. There are 240 5 https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge/ data six classes: toxic, severe toxic, obscene, threat, insult, and identity hate. In the experiment, we used 60 comments as the initial training data (seed), 600 comments as test data, and 18,000 for unlabelled data. The instances forming the se"
2021.eacl-demos.28,2020.coling-main.507,1,0.773034,"al., 2017) libraries, the Vowpal Wabbit1 . These tools, however, focus only on the active learning algorithms and provide no user interface thus making it difficult to use for the end-users. On the other hand, several tools have been made with user-friendly interface such 1 http://hunch.net/˜vw/ as BRAT (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), PubAnnotation (Kim and Wang, 2012), doccano2 . Some of the tools offer active/proactive learning such as APLenty (Nghiem and Ananiadou, 2018), DUALIST (Settles and Zhu, 2012), AlpacaTag (Lin et al., 2019), Discrete Active Learning Coref (Li et al., 2020a). Currently, these tools support sequence labelling/coreference resolution tasks but not document classification tasks. To the best of our knowledge, there is no such tool for document classification which supports active/proactive learning. Prodigy3 supports active learning for both sequence labelling and document classification tasks but it is a commercial product. To compensate for the lack of available document-level annotation tool, we develop Paladin (Proactive learning annotator for document instances), an open-source web-based system for creating labelled data using active/proactive"
2021.eacl-demos.28,P19-3010,0,0.0205437,"the JCLAL (Reyes et al., 2016), the LibAct (Yang et al., 2017) libraries, the Vowpal Wabbit1 . These tools, however, focus only on the active learning algorithms and provide no user interface thus making it difficult to use for the end-users. On the other hand, several tools have been made with user-friendly interface such 1 http://hunch.net/˜vw/ as BRAT (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), PubAnnotation (Kim and Wang, 2012), doccano2 . Some of the tools offer active/proactive learning such as APLenty (Nghiem and Ananiadou, 2018), DUALIST (Settles and Zhu, 2012), AlpacaTag (Lin et al., 2019), Discrete Active Learning Coref (Li et al., 2020a). Currently, these tools support sequence labelling/coreference resolution tasks but not document classification tasks. To the best of our knowledge, there is no such tool for document classification which supports active/proactive learning. Prodigy3 supports active learning for both sequence labelling and document classification tasks but it is a commercial product. To compensate for the lack of available document-level annotation tool, we develop Paladin (Proactive learning annotator for document instances), an open-source web-based system f"
2021.eacl-demos.28,D18-2019,1,0.857074,"Missing"
2021.eacl-demos.28,N12-1066,0,0.0327903,"Scala (Santos and Carvalho, 2014), the JCLAL (Reyes et al., 2016), the LibAct (Yang et al., 2017) libraries, the Vowpal Wabbit1 . These tools, however, focus only on the active learning algorithms and provide no user interface thus making it difficult to use for the end-users. On the other hand, several tools have been made with user-friendly interface such 1 http://hunch.net/˜vw/ as BRAT (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), PubAnnotation (Kim and Wang, 2012), doccano2 . Some of the tools offer active/proactive learning such as APLenty (Nghiem and Ananiadou, 2018), DUALIST (Settles and Zhu, 2012), AlpacaTag (Lin et al., 2019), Discrete Active Learning Coref (Li et al., 2020a). Currently, these tools support sequence labelling/coreference resolution tasks but not document classification tasks. To the best of our knowledge, there is no such tool for document classification which supports active/proactive learning. Prodigy3 supports active learning for both sequence labelling and document classification tasks but it is a commercial product. To compensate for the lack of available document-level annotation tool, we develop Paladin (Proactive learning annotator for document instances), an"
2021.eacl-demos.28,E12-2021,1,0.777517,"Missing"
2021.eacl-demos.28,P13-4001,0,0.0433508,"Missing"
2021.eacl-main.135,P17-1067,0,0.024083,"eatures of labelled data to classify inputs into one label (Bostan and Klinger, 2018; Liew et al., 2016; Mohammad et al., 2015; Tang et al., 2013; Wang et al., 2012; Aman and Szpakowicz, 2007). More recently, several neural network models have been developed for this task, obtaining competitive results on different emotion data sets. Some of these models generally focus on a singlelabel emotion classification, in which only a single label is assigned to each input (Islam et al., 2019; Xia and Ding, 2019; Alhuzali et al., 2018b,a; Agrawal et al., 2018; Saravia et al., 2018; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Other models have also been proposed for multilabel emotion classification, in which one or more labels are assigned to each input (see detailed description in section 3.3). Our work is motivated by research focused on learning features corresponding to each emotion as well as incorporating the relations between emotions into a loss function (Fei et al., 2020; He and Xia, 2018). Our work differs from these two works in the following ways: i) our method learns features related to each corresponding emotion without relying on any external resources (e.g. lexicons). ii) We further integrated th"
2021.eacl-main.135,C18-1081,0,0.121135,"ns: GT 1.00 anger disgust fear pessimism sadness joy love optimism anticipation surprise trust 0.75 0.50 0.25 0.00 maF1 anger disgust fear pessimism sadness joy love optimism anticipation surprise trust 0.25 (b) Emotion correlations: Prediction what the emotion annotations have revealed. 3(b), which was learned by SpanEmo, also highlights that negative emotions are positively correlated with each other, and negatively correlated with positive emotions. For example, “anger and disgust” share almost the same patterns, which is consistent with the studies of Mohammad and Bravo-Marquez (2017) and Agrawal et al. (2018), both of which report the same issue with negative emotions of “anger” and “disgust”, as they are easily confused with each other. This is not surprising as their manifestation in language is quite similar in terms of the use of similar words/expression. We also noted this finding when analysing the top-10 key words learned by SpanEmo in section 5.2.1. In short, taking into account emotion correlations is crucial for multi-label emotion classification in addressing the ambiguity characteristic of the task, especially for 0.62 0.60 0.58 0.56 0.54 0.52 0.73 miF1 0.72 0.71 0.70 0.69 0.61 0.60 ja"
2021.eacl-main.135,W18-1104,1,0.793772,"arning task, in which a learner (e.g. linear classifier based methods) is trained on the features of labelled data to classify inputs into one label (Bostan and Klinger, 2018; Liew et al., 2016; Mohammad et al., 2015; Tang et al., 2013; Wang et al., 2012; Aman and Szpakowicz, 2007). More recently, several neural network models have been developed for this task, obtaining competitive results on different emotion data sets. Some of these models generally focus on a singlelabel emotion classification, in which only a single label is assigned to each input (Islam et al., 2019; Xia and Ding, 2019; Alhuzali et al., 2018b,a; Agrawal et al., 2018; Saravia et al., 2018; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Other models have also been proposed for multilabel emotion classification, in which one or more labels are assigned to each input (see detailed description in section 3.3). Our work is motivated by research focused on learning features corresponding to each emotion as well as incorporating the relations between emotions into a loss function (Fei et al., 2020; He and Xia, 2018). Our work differs from these two works in the following ways: i) our method learns features related to each correspondi"
2021.eacl-main.135,W19-5036,1,0.852333,"on modelling multiple coexisting emotions in the input sentence. Experiments performed on the SemEval2018 multilabel emotion data over three language sets (i.e., English, Arabic and Spanish) demonstrate our method’s effectiveness. Finally, we present different analyses that illustrate the benefits of our method in terms of improving the model performance and learning meaningful associations between emotion classes and words in the sentence1 . 1 Introduction Emotion is essential to human communication, thus emotion recognition (ER) models have a host of applications from health and well-being (Alhuzali and Ananiadou, 2019; Arag´on et al., 2019; Chen et al., 2018) to consumer analysis (Alaluf and Illouz, 2019; Herzig et al., 2016) and user profiling (Volkova and Bachrach, 2016; Mohammad and Kiritchenko, 2013), among others. Interest in this area has given rise to new NLP approaches aimed at emotion classification, including single-label and multi-label emotion classification. Most existing approaches for multi-label emotion classi1 Source code is available at https://github.com/ hasanhuz/SpanEmo # Sentence GT S1 well my day started off great the mocha machine wasn’t working @ mcdonalds. anger, disgust, joy, sad"
2021.eacl-main.135,W18-6250,1,0.84947,"arning task, in which a learner (e.g. linear classifier based methods) is trained on the features of labelled data to classify inputs into one label (Bostan and Klinger, 2018; Liew et al., 2016; Mohammad et al., 2015; Tang et al., 2013; Wang et al., 2012; Aman and Szpakowicz, 2007). More recently, several neural network models have been developed for this task, obtaining competitive results on different emotion data sets. Some of these models generally focus on a singlelabel emotion classification, in which only a single label is assigned to each input (Islam et al., 2019; Xia and Ding, 2019; Alhuzali et al., 2018b,a; Agrawal et al., 2018; Saravia et al., 2018; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Other models have also been proposed for multilabel emotion classification, in which one or more labels are assigned to each input (see detailed description in section 3.3). Our work is motivated by research focused on learning features corresponding to each emotion as well as incorporating the relations between emotions into a loss function (Fei et al., 2020; He and Xia, 2018). Our work differs from these two works in the following ways: i) our method learns features related to each correspondi"
2021.eacl-main.135,N19-1151,0,0.0607194,"Missing"
2021.eacl-main.135,S18-1037,0,0.0431946,"Missing"
2021.eacl-main.135,S17-2126,0,0.0257559,"Missing"
2021.eacl-main.135,C18-1179,0,0.0369954,"Missing"
2021.eacl-main.135,N19-1423,0,0.0175081,"tive label set for multi-label emotion classification. We now turn to describing our framework in detail. CLS C1 C2 ... Cm SEP W1 W2 ... Wn Feed Forward Network BERT Encoding CLS C1 C2 ... Cm SEP W1 Classes (C) W2 ... Wn Input (Si) Figure 1: Illustration of our proposed framework (SpanEmo). 2.2 Our Method (SpanEmo) Let {(si , yi )}N i=1 be a set of N examples with the corresponding emotion labels of C classes, where si denotes the input sentence and yi ∈ {0, 1}m represents the label set for si . As shown in Figure 1, both the label set and the input sentence were passed into the encoder BERT (Devlin et al., 2019). The encoder received two segments: the first corresponds to the set of emotion classes, while the second refers to the input sentence. The hidden representations (Hi ∈ RT × D )2 for each input sentence and the label set were obtained as follows: Hi = Encoder([CLS] + |C |+ [SEP] + si ), (1) where {[CLS], [SEP]} are special tokens and |C |denotes the size of emotion classes. Feeding both segments to the encoder has a few advantages. First, the encoder can interpolate between emotion classes and all words in the input sentence. Second, a hidden representation is generated both for words and emo"
2021.eacl-main.135,D17-1169,0,0.0236577,"is trained on the features of labelled data to classify inputs into one label (Bostan and Klinger, 2018; Liew et al., 2016; Mohammad et al., 2015; Tang et al., 2013; Wang et al., 2012; Aman and Szpakowicz, 2007). More recently, several neural network models have been developed for this task, obtaining competitive results on different emotion data sets. Some of these models generally focus on a singlelabel emotion classification, in which only a single label is assigned to each input (Islam et al., 2019; Xia and Ding, 2019; Alhuzali et al., 2018b,a; Agrawal et al., 2018; Saravia et al., 2018; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Other models have also been proposed for multilabel emotion classification, in which one or more labels are assigned to each input (see detailed description in section 3.3). Our work is motivated by research focused on learning features corresponding to each emotion as well as incorporating the relations between emotions into a loss function (Fei et al., 2020; He and Xia, 2018). Our work differs from these two works in the following ways: i) our method learns features related to each corresponding emotion without relying on any external resources (e.g. lexicons"
2021.eacl-main.135,S18-1092,0,0.0496453,"Missing"
2021.eacl-main.135,N19-1137,0,0.0304806,"Missing"
2021.eacl-main.135,N16-2011,0,0.0279635,"Mohammad and Alm, 2015). Earlier studies focused on lexicon-based approaches, which make use of some words and their corresponding labels to identify emotions in text, e.g. NRC4 (Mohammad and Turney, 2013) and 4 Bravo-Marquez et al. (2016) proposes an approach for expanding it for the language used in Twitter. 1580 E mo S entic N et (Poria et al., 2014). Other methods treat the emotion recognition task as a supervised learning task, in which a learner (e.g. linear classifier based methods) is trained on the features of labelled data to classify inputs into one label (Bostan and Klinger, 2018; Liew et al., 2016; Mohammad et al., 2015; Tang et al., 2013; Wang et al., 2012; Aman and Szpakowicz, 2007). More recently, several neural network models have been developed for this task, obtaining competitive results on different emotion data sets. Some of these models generally focus on a singlelabel emotion classification, in which only a single label is assigned to each input (Islam et al., 2019; Xia and Ding, 2019; Alhuzali et al., 2018b,a; Agrawal et al., 2018; Saravia et al., 2018; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Other models have also been proposed for multilabel emotion classificati"
2021.eacl-main.135,S18-1001,0,0.158215,"yˆ. The objective of this loss function is to maximise the distance between positive and negative labels by implicitly retaining the label-dependency information. In other words, the model should be penalised when it predicts a pair of labels that should not co-exist for a given example. 2.4 Experiments Parameter Value Feature dimension Batch size Dropout Early stop patience Number of epochs lr-BERT lr-FFN Optimiser Alpha (α) 768 32 0.1 10 20 2e-5 1e-3 Adam 0.2 Table 2: Hyper-parameter values. lr: refers to the Learning rate. 3.2 Data Set and Task Settings In this work, we chose semEval2018 (Mohammad et al., 2018) for our multi-label emotion classification, which is based on labelled data from tweets in English, Arabic and Spanish. The data was originally partitioned into three sets: training set (Train), validation set (Valid) and test (Test) set. Following the metrics in Mohammad et al. (2018), we run our experiments on micro F1-score, macro F1-score i=1 1575 and Jaccard index score3 . Table 3 presents the summary of all three sets for each language, including the number of instances in the train, valid and test sets. In addition, the number of emotion classes and the percentage of instances with var"
2021.eacl-main.135,D15-2008,0,0.128503,"Missing"
2021.eacl-main.135,S17-1007,0,0.170394,"negative emotions. For example, clue words like “great” are more likely to be associated with “joy”, whereas “wasn’t working” are more likely to be associated with negative emotions. Learning such associations between emotion labels and 1573 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1573–1584 April 19 - 23, 2021. ©2021 Association for Computational Linguistics words in a sentence can help ER models to predict the correct labels. S2 further highlights that certain emotions are more likely to be associated with each other. Mohammad and Bravo-Marquez (2017) also observed that negative emotions are highly associated with each other, while less associated with positive emotions. Based on these observations, we seek to answer the following research questions: i) how to enable ER models to learn emotion-specific associations by taking into account label information and ii) how to benefit from the multiple coexisting emotions in a multi-label emotion data set with the intention of learning label correlations. Our contributions are summarised as follows: I. a novel framework casting the task of multi-label emotion classification as a span-prediction p"
2021.eacl-main.135,S18-1024,0,0.0482276,"Missing"
2021.eacl-main.135,2020.semeval-1.271,0,0.0158361,"through maximising the probability of the correct labels. We experimentally observed that training our approach jointly with those two loss functions produced the best results. The overall training objective was computed as follows: L = (1 − α)LBCE + α M X LLCA , (4) Implementation Details We used PyTorch (Paszke et al., 2017) for implementation and ran all experiments on an Nvidia G e F orce GTX 1080 with 11 GB memory. We also trained BERT BASE utilising the open-source Hugging-Face implementation (Wolf et al., 2019). For experiments related to Arabic, we chose “bertbase-arabic” developed by Safaya et al. (2020), while selecting “bert-base-spanish-uncased” developed by Ca˜nete et al. (2020) for Spanish. All three models were trained on the same hyper-parameters with a fixed initialisation seed, including a feature dimension of 786, a batch size of 32, a dropout rate of 0.1, an early stop patience of 10 and 20 epochs. Adam was selected for optimisation (Kingma and Ba, 2014) with a learning rate of 2e-5 for the BERT encoder, and a learning rate of 1e-3 for the FFN. It should be mentioned that we tuned our method only on the validation set and further report on the analysis of the effect of parameter α"
2021.eacl-main.135,D18-1404,0,0.0165112,"ssifier based methods) is trained on the features of labelled data to classify inputs into one label (Bostan and Klinger, 2018; Liew et al., 2016; Mohammad et al., 2015; Tang et al., 2013; Wang et al., 2012; Aman and Szpakowicz, 2007). More recently, several neural network models have been developed for this task, obtaining competitive results on different emotion data sets. Some of these models generally focus on a singlelabel emotion classification, in which only a single label is assigned to each input (Islam et al., 2019; Xia and Ding, 2019; Alhuzali et al., 2018b,a; Agrawal et al., 2018; Saravia et al., 2018; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Other models have also been proposed for multilabel emotion classification, in which one or more labels are assigned to each input (see detailed description in section 3.3). Our work is motivated by research focused on learning features corresponding to each emotion as well as incorporating the relations between emotions into a loss function (Fei et al., 2020; He and Xia, 2018). Our work differs from these two works in the following ways: i) our method learns features related to each corresponding emotion without relying on any external reso"
2021.eacl-main.135,P16-1148,0,0.0294159,"., English, Arabic and Spanish) demonstrate our method’s effectiveness. Finally, we present different analyses that illustrate the benefits of our method in terms of improving the model performance and learning meaningful associations between emotion classes and words in the sentence1 . 1 Introduction Emotion is essential to human communication, thus emotion recognition (ER) models have a host of applications from health and well-being (Alhuzali and Ananiadou, 2019; Arag´on et al., 2019; Chen et al., 2018) to consumer analysis (Alaluf and Illouz, 2019; Herzig et al., 2016) and user profiling (Volkova and Bachrach, 2016; Mohammad and Kiritchenko, 2013), among others. Interest in this area has given rise to new NLP approaches aimed at emotion classification, including single-label and multi-label emotion classification. Most existing approaches for multi-label emotion classi1 Source code is available at https://github.com/ hasanhuz/SpanEmo # Sentence GT S1 well my day started off great the mocha machine wasn’t working @ mcdonalds. anger, disgust, joy, sadness S2 I’m doing all this to make sure you smiling down on me bro. joy, love, optimism Table 1: Example Tweets from SemEval-18 Task 1. GT represents the gro"
2021.eacl-main.135,P19-1096,0,0.0233697,"k as a supervised learning task, in which a learner (e.g. linear classifier based methods) is trained on the features of labelled data to classify inputs into one label (Bostan and Klinger, 2018; Liew et al., 2016; Mohammad et al., 2015; Tang et al., 2013; Wang et al., 2012; Aman and Szpakowicz, 2007). More recently, several neural network models have been developed for this task, obtaining competitive results on different emotion data sets. Some of these models generally focus on a singlelabel emotion classification, in which only a single label is assigned to each input (Islam et al., 2019; Xia and Ding, 2019; Alhuzali et al., 2018b,a; Agrawal et al., 2018; Saravia et al., 2018; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Other models have also been proposed for multilabel emotion classification, in which one or more labels are assigned to each input (see detailed description in section 3.3). Our work is motivated by research focused on learning features corresponding to each emotion as well as incorporating the relations between emotions into a loss function (Fei et al., 2020; He and Xia, 2018). Our work differs from these two works in the following ways: i) our method learns features rela"
2021.eacl-main.135,C18-1330,0,0.0534325,"Missing"
2021.eacl-main.135,D19-5541,0,0.015717,". We used the tool to tokenise the text, convert words to lower case, normalise user mentions, urls and repeated-characters. 3.3 Multi-label Emotion Classification We compared the performance of SpanEmo to some baseline as well as state-of-the-art models on all three languages. For experiments related to English, we selected seven models, while we chose three models for both Arabic and Spanish. We also include the results of BERT BASE . 3.3.1 English E nglish models include JBNN (He and Xia, 2018), DATN (Yu et al., 2018), NTUA (Baziotis et al., 2018), RERc (Zhou et al., 2018), BERT BASE + DK (Ying et al., 2019), BERT BASE GCN (Xu et al., 2020) and LEM (Fei et al., 2020). JBNN introduces a joint binary neural network, which focuses on learning the relations between emotions based on the theory of Plutchik’s wheel of emotions (Plutchik, 1980), and then performing multi-label emotion classification via integrating 3 jacS is defined as the size of the intersection divided by the size of the union of the true label set and predicted label set. these label relations into the loss function. DATN proposes a dual attention transfer network to improve multi-label emotion classification with the help of sentim"
2021.eacl-main.135,D18-1137,0,0.0156259,"unctionalities, such as tokenisation, normalisation, spelling correction, and segmentation. We used the tool to tokenise the text, convert words to lower case, normalise user mentions, urls and repeated-characters. 3.3 Multi-label Emotion Classification We compared the performance of SpanEmo to some baseline as well as state-of-the-art models on all three languages. For experiments related to English, we selected seven models, while we chose three models for both Arabic and Spanish. We also include the results of BERT BASE . 3.3.1 English E nglish models include JBNN (He and Xia, 2018), DATN (Yu et al., 2018), NTUA (Baziotis et al., 2018), RERc (Zhou et al., 2018), BERT BASE + DK (Ying et al., 2019), BERT BASE GCN (Xu et al., 2020) and LEM (Fei et al., 2020). JBNN introduces a joint binary neural network, which focuses on learning the relations between emotions based on the theory of Plutchik’s wheel of emotions (Plutchik, 1980), and then performing multi-label emotion classification via integrating 3 jacS is defined as the size of the intersection divided by the size of the union of the true label set and predicted label set. these label relations into the loss function. DATN proposes a dual atte"
2021.eacl-main.135,N18-1052,0,0.0603263,"Missing"
2021.findings-acl.77,C18-1039,0,0.0184255,"n TS models performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build TS models that gain a higher SARI score in our evaluation setting. 2 Related Work The exploration of neural networks in TS started with the work of Nisioi et al. (2017), using the largest parallel simplification resource available (Hwang et al., 2015). Neural-based work focused on state-of-the-art deep learning and MT-based methods, such as reinforcement learning (Zhang and Lapata, 2017), adversarial training (Surya et al., 2019), pointer-copy mechanism (Guo et al., 2018), neural semantic encoders (Vu et al., 2018) and transformers supported by paraphrasing rules (Zhao et al., 2018). Other successful approaches include the usage of control tokens to tune the level of simplification expected (Alva-Manchego et al., 2020a; Scarton and Specia, 2018) and the prediction of operations using parallel corpora (Alva-Manchego et al., 2017; Dong et al., 2020). The neural methods are trained mostly on Wikipedia-based sets, varying in size and improvements in the quality of the alignments. Xu et al. (2015) carried out a systematic study on Wikipedia-based simplification res"
2021.findings-acl.77,W10-1001,0,0.0454908,"providing new insights that will motivate the improvement of existing state-ofthe-art TS evaluation methods. Our contributions include the analysis of TS corpora based on existing modifications used for simplification and an empirical study on TS models performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build more robust TS models. 1 Introduction Text Simplification transforms natural language from a complex to a simple format, with the aim to not only reach wider audiences (Rello et al., 2013; De Belder and Moens, 2010; Aluisio et al., 2010; Inui et al., 2003) but also as a preprocessing step in related tasks (Shardlow, 2014; Silveira and Branco, 2012). Simplifications are achieved by using parallel datasets to train sequence-to-sequence text generation algorithms (Nisioi et al., 2017) to make complex sentences easier to understand. They are typically produced by crowdsourcing (Xu et al., 2016; Alva-Manchego et al., 2020a) or by alignment (Cao et al., 2020; Jiang et al., 2020). They are infamously noisy and models trained on these give poor results when evaluated by humans (Cooper and Shardlow, 2020). In this paper we add to the"
2021.findings-acl.77,I17-1030,0,0.0123929,"esource available (Hwang et al., 2015). Neural-based work focused on state-of-the-art deep learning and MT-based methods, such as reinforcement learning (Zhang and Lapata, 2017), adversarial training (Surya et al., 2019), pointer-copy mechanism (Guo et al., 2018), neural semantic encoders (Vu et al., 2018) and transformers supported by paraphrasing rules (Zhao et al., 2018). Other successful approaches include the usage of control tokens to tune the level of simplification expected (Alva-Manchego et al., 2020a; Scarton and Specia, 2018) and the prediction of operations using parallel corpora (Alva-Manchego et al., 2017; Dong et al., 2020). The neural methods are trained mostly on Wikipedia-based sets, varying in size and improvements in the quality of the alignments. Xu et al. (2015) carried out a systematic study on Wikipedia-based simplification resources, claiming Wikipedia is not a quality resource, based on the observed alignments and the type of simplifications. Alva-Manchego et al. (2020a) proposed a new dataset, performing a detailed analysis including edit distance and proportion of words that are deleted, inserted and reordered, and evaluation metrics performance for their proposed corpus. Chasing"
2021.findings-acl.77,2020.acl-main.424,0,0.0801971,"d more robust TS models. 1 Introduction Text Simplification transforms natural language from a complex to a simple format, with the aim to not only reach wider audiences (Rello et al., 2013; De Belder and Moens, 2010; Aluisio et al., 2010; Inui et al., 2003) but also as a preprocessing step in related tasks (Shardlow, 2014; Silveira and Branco, 2012). Simplifications are achieved by using parallel datasets to train sequence-to-sequence text generation algorithms (Nisioi et al., 2017) to make complex sentences easier to understand. They are typically produced by crowdsourcing (Xu et al., 2016; Alva-Manchego et al., 2020a) or by alignment (Cao et al., 2020; Jiang et al., 2020). They are infamously noisy and models trained on these give poor results when evaluated by humans (Cooper and Shardlow, 2020). In this paper we add to the growing narrative around the evaluation of natural language generation (van der Lee et al., 2019; Caglayan et al., 2020; Pang, 2019), focusing on parallel text simplification datasets and how they can be improved. Why do we need to re-evaluate TS resources? In the last decade, TS research has relied on Wikipedia-based datasets (Zhang and Lapata, 2017; Xu et al., 2016; Jiang et al., 20"
2021.findings-acl.77,2020.coling-main.210,0,0.216667,"d Branco, 2012). Simplifications are achieved by using parallel datasets to train sequence-to-sequence text generation algorithms (Nisioi et al., 2017) to make complex sentences easier to understand. They are typically produced by crowdsourcing (Xu et al., 2016; Alva-Manchego et al., 2020a) or by alignment (Cao et al., 2020; Jiang et al., 2020). They are infamously noisy and models trained on these give poor results when evaluated by humans (Cooper and Shardlow, 2020). In this paper we add to the growing narrative around the evaluation of natural language generation (van der Lee et al., 2019; Caglayan et al., 2020; Pang, 2019), focusing on parallel text simplification datasets and how they can be improved. Why do we need to re-evaluate TS resources? In the last decade, TS research has relied on Wikipedia-based datasets (Zhang and Lapata, 2017; Xu et al., 2016; Jiang et al., 2020), despite their known limitations (Xu et al., 2015; Alva-Manchego et al., 2020a) such as questionable sentence pairs alignments, inaccurate simplifications and a limited variety of simplification modifications. Apart from affecting the reliability of models trained on these datasets, their low quality influences the evaluation"
2021.findings-acl.77,2020.acl-main.100,0,0.0939232,"implification transforms natural language from a complex to a simple format, with the aim to not only reach wider audiences (Rello et al., 2013; De Belder and Moens, 2010; Aluisio et al., 2010; Inui et al., 2003) but also as a preprocessing step in related tasks (Shardlow, 2014; Silveira and Branco, 2012). Simplifications are achieved by using parallel datasets to train sequence-to-sequence text generation algorithms (Nisioi et al., 2017) to make complex sentences easier to understand. They are typically produced by crowdsourcing (Xu et al., 2016; Alva-Manchego et al., 2020a) or by alignment (Cao et al., 2020; Jiang et al., 2020). They are infamously noisy and models trained on these give poor results when evaluated by humans (Cooper and Shardlow, 2020). In this paper we add to the growing narrative around the evaluation of natural language generation (van der Lee et al., 2019; Caglayan et al., 2020; Pang, 2019), focusing on parallel text simplification datasets and how they can be improved. Why do we need to re-evaluate TS resources? In the last decade, TS research has relied on Wikipedia-based datasets (Zhang and Lapata, 2017; Xu et al., 2016; Jiang et al., 2020), despite their known limitations"
2021.findings-acl.77,2020.lrec-1.686,1,0.780857,", 2013; De Belder and Moens, 2010; Aluisio et al., 2010; Inui et al., 2003) but also as a preprocessing step in related tasks (Shardlow, 2014; Silveira and Branco, 2012). Simplifications are achieved by using parallel datasets to train sequence-to-sequence text generation algorithms (Nisioi et al., 2017) to make complex sentences easier to understand. They are typically produced by crowdsourcing (Xu et al., 2016; Alva-Manchego et al., 2020a) or by alignment (Cao et al., 2020; Jiang et al., 2020). They are infamously noisy and models trained on these give poor results when evaluated by humans (Cooper and Shardlow, 2020). In this paper we add to the growing narrative around the evaluation of natural language generation (van der Lee et al., 2019; Caglayan et al., 2020; Pang, 2019), focusing on parallel text simplification datasets and how they can be improved. Why do we need to re-evaluate TS resources? In the last decade, TS research has relied on Wikipedia-based datasets (Zhang and Lapata, 2017; Xu et al., 2016; Jiang et al., 2020), despite their known limitations (Xu et al., 2015; Alva-Manchego et al., 2020a) such as questionable sentence pairs alignments, inaccurate simplifications and a limited variety of"
2021.findings-acl.77,P19-1513,0,0.0281509,"ds are trained mostly on Wikipedia-based sets, varying in size and improvements in the quality of the alignments. Xu et al. (2015) carried out a systematic study on Wikipedia-based simplification resources, claiming Wikipedia is not a quality resource, based on the observed alignments and the type of simplifications. Alva-Manchego et al. (2020a) proposed a new dataset, performing a detailed analysis including edit distance and proportion of words that are deleted, inserted and reordered, and evaluation metrics performance for their proposed corpus. Chasing the state-of-the-art is rife in NLP (Hou et al., 2019), and no less so in TS, where a SARI score is too often considered the main quality indicator. However, recent work has shown that these metrics are unreliable (Caglayan et al., 2020) and gains in performance according to them may not deliver improvements in simplification performance when the text is presented to an end user. 3 3.1 Simplification Datasets: Exploration Data and Methods In the initial exploration of TS datasets, we investigated the training, test and validation subsets (when available) of the following: WikiSmall and WikiLarge (Zhang and Lapata, 2017), TurkCorpus (Xu et al., 20"
2021.findings-acl.77,N15-1022,0,0.0145184,"the corpora analysed in this research. Our contributions include 1) the analysis of the most common TS corpora based on quantifying modifications used for simplification, evidencing their limitations and 2) an empirical study on TS models performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build TS models that gain a higher SARI score in our evaluation setting. 2 Related Work The exploration of neural networks in TS started with the work of Nisioi et al. (2017), using the largest parallel simplification resource available (Hwang et al., 2015). Neural-based work focused on state-of-the-art deep learning and MT-based methods, such as reinforcement learning (Zhang and Lapata, 2017), adversarial training (Surya et al., 2019), pointer-copy mechanism (Guo et al., 2018), neural semantic encoders (Vu et al., 2018) and transformers supported by paraphrasing rules (Zhao et al., 2018). Other successful approaches include the usage of control tokens to tune the level of simplification expected (Alva-Manchego et al., 2020a; Scarton and Specia, 2018) and the prediction of operations using parallel corpora (Alva-Manchego et al., 2017; Dong et al"
2021.findings-acl.77,W03-1602,0,0.217627,"s that will motivate the improvement of existing state-ofthe-art TS evaluation methods. Our contributions include the analysis of TS corpora based on existing modifications used for simplification and an empirical study on TS models performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build more robust TS models. 1 Introduction Text Simplification transforms natural language from a complex to a simple format, with the aim to not only reach wider audiences (Rello et al., 2013; De Belder and Moens, 2010; Aluisio et al., 2010; Inui et al., 2003) but also as a preprocessing step in related tasks (Shardlow, 2014; Silveira and Branco, 2012). Simplifications are achieved by using parallel datasets to train sequence-to-sequence text generation algorithms (Nisioi et al., 2017) to make complex sentences easier to understand. They are typically produced by crowdsourcing (Xu et al., 2016; Alva-Manchego et al., 2020a) or by alignment (Cao et al., 2020; Jiang et al., 2020). They are infamously noisy and models trained on these give poor results when evaluated by humans (Cooper and Shardlow, 2020). In this paper we add to the growing narrative a"
2021.findings-acl.77,2020.acl-main.709,0,0.0796387,"sforms natural language from a complex to a simple format, with the aim to not only reach wider audiences (Rello et al., 2013; De Belder and Moens, 2010; Aluisio et al., 2010; Inui et al., 2003) but also as a preprocessing step in related tasks (Shardlow, 2014; Silveira and Branco, 2012). Simplifications are achieved by using parallel datasets to train sequence-to-sequence text generation algorithms (Nisioi et al., 2017) to make complex sentences easier to understand. They are typically produced by crowdsourcing (Xu et al., 2016; Alva-Manchego et al., 2020a) or by alignment (Cao et al., 2020; Jiang et al., 2020). They are infamously noisy and models trained on these give poor results when evaluated by humans (Cooper and Shardlow, 2020). In this paper we add to the growing narrative around the evaluation of natural language generation (van der Lee et al., 2019; Caglayan et al., 2020; Pang, 2019), focusing on parallel text simplification datasets and how they can be improved. Why do we need to re-evaluate TS resources? In the last decade, TS research has relied on Wikipedia-based datasets (Zhang and Lapata, 2017; Xu et al., 2016; Jiang et al., 2020), despite their known limitations (Xu et al., 2015; Al"
2021.findings-acl.77,W19-8643,0,0.0469261,"Missing"
2021.findings-acl.77,P17-2014,0,0.15338,"dels performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build more robust TS models. 1 Introduction Text Simplification transforms natural language from a complex to a simple format, with the aim to not only reach wider audiences (Rello et al., 2013; De Belder and Moens, 2010; Aluisio et al., 2010; Inui et al., 2003) but also as a preprocessing step in related tasks (Shardlow, 2014; Silveira and Branco, 2012). Simplifications are achieved by using parallel datasets to train sequence-to-sequence text generation algorithms (Nisioi et al., 2017) to make complex sentences easier to understand. They are typically produced by crowdsourcing (Xu et al., 2016; Alva-Manchego et al., 2020a) or by alignment (Cao et al., 2020; Jiang et al., 2020). They are infamously noisy and models trained on these give poor results when evaluated by humans (Cooper and Shardlow, 2020). In this paper we add to the growing narrative around the evaluation of natural language generation (van der Lee et al., 2019; Caglayan et al., 2020; Pang, 2019), focusing on parallel text simplification datasets and how they can be improved. Why do we need to re-evaluate TS re"
2021.findings-acl.77,2001.mtsummit-papers.68,0,0.144333,"e-evaluate TS resources? In the last decade, TS research has relied on Wikipedia-based datasets (Zhang and Lapata, 2017; Xu et al., 2016; Jiang et al., 2020), despite their known limitations (Xu et al., 2015; Alva-Manchego et al., 2020a) such as questionable sentence pairs alignments, inaccurate simplifications and a limited variety of simplification modifications. Apart from affecting the reliability of models trained on these datasets, their low quality influences the evaluation relying on automatic metrics that requires goldstandard simplifications, such as SARI (Xu et al., 2016) and BLEU (Papineni et al., 2001). Hence, evaluation data resources must be further explored and improved to achieve reliable evaluation scenarios. There is a growing body of evidence (Xu et al., 2015) (including this work) to show that existing datasets do not contain accurate and well-constructed simplifications, significantly impeding the progress of the TS field. Furthermore, well-known evaluation metrics such as BLEU are not suitable for simplification evaluation. According to previous research (Sulem et al., 2018) BLEU does not significantly correlate with simplicity (Xu et al., 2016), making it inappropriate for TS eva"
2021.findings-acl.77,P18-2113,0,0.0135769,"with the work of Nisioi et al. (2017), using the largest parallel simplification resource available (Hwang et al., 2015). Neural-based work focused on state-of-the-art deep learning and MT-based methods, such as reinforcement learning (Zhang and Lapata, 2017), adversarial training (Surya et al., 2019), pointer-copy mechanism (Guo et al., 2018), neural semantic encoders (Vu et al., 2018) and transformers supported by paraphrasing rules (Zhao et al., 2018). Other successful approaches include the usage of control tokens to tune the level of simplification expected (Alva-Manchego et al., 2020a; Scarton and Specia, 2018) and the prediction of operations using parallel corpora (Alva-Manchego et al., 2017; Dong et al., 2020). The neural methods are trained mostly on Wikipedia-based sets, varying in size and improvements in the quality of the alignments. Xu et al. (2015) carried out a systematic study on Wikipedia-based simplification resources, claiming Wikipedia is not a quality resource, based on the observed alignments and the type of simplifications. Alva-Manchego et al. (2020a) proposed a new dataset, performing a detailed analysis including edit distance and proportion of words that are deleted, inserted"
2021.findings-acl.77,D18-1081,0,0.0439347,"Missing"
2021.findings-acl.77,P19-1198,0,0.0143987,"their limitations and 2) an empirical study on TS models performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build TS models that gain a higher SARI score in our evaluation setting. 2 Related Work The exploration of neural networks in TS started with the work of Nisioi et al. (2017), using the largest parallel simplification resource available (Hwang et al., 2015). Neural-based work focused on state-of-the-art deep learning and MT-based methods, such as reinforcement learning (Zhang and Lapata, 2017), adversarial training (Surya et al., 2019), pointer-copy mechanism (Guo et al., 2018), neural semantic encoders (Vu et al., 2018) and transformers supported by paraphrasing rules (Zhao et al., 2018). Other successful approaches include the usage of control tokens to tune the level of simplification expected (Alva-Manchego et al., 2020a; Scarton and Specia, 2018) and the prediction of operations using parallel corpora (Alva-Manchego et al., 2017; Dong et al., 2020). The neural methods are trained mostly on Wikipedia-based sets, varying in size and improvements in the quality of the alignments. Xu et al. (2015) carried out a systematic"
2021.findings-acl.77,N18-2013,0,0.0197955,"ibuted datasets. We demonstrate that by improving the distribution of TS datasets, we can build TS models that gain a higher SARI score in our evaluation setting. 2 Related Work The exploration of neural networks in TS started with the work of Nisioi et al. (2017), using the largest parallel simplification resource available (Hwang et al., 2015). Neural-based work focused on state-of-the-art deep learning and MT-based methods, such as reinforcement learning (Zhang and Lapata, 2017), adversarial training (Surya et al., 2019), pointer-copy mechanism (Guo et al., 2018), neural semantic encoders (Vu et al., 2018) and transformers supported by paraphrasing rules (Zhao et al., 2018). Other successful approaches include the usage of control tokens to tune the level of simplification expected (Alva-Manchego et al., 2020a; Scarton and Specia, 2018) and the prediction of operations using parallel corpora (Alva-Manchego et al., 2017; Dong et al., 2020). The neural methods are trained mostly on Wikipedia-based sets, varying in size and improvements in the quality of the alignments. Xu et al. (2015) carried out a systematic study on Wikipedia-based simplification resources, claiming Wikipedia is not a quality"
2021.findings-acl.77,D17-1062,0,0.418164,"rowdsourcing (Xu et al., 2016; Alva-Manchego et al., 2020a) or by alignment (Cao et al., 2020; Jiang et al., 2020). They are infamously noisy and models trained on these give poor results when evaluated by humans (Cooper and Shardlow, 2020). In this paper we add to the growing narrative around the evaluation of natural language generation (van der Lee et al., 2019; Caglayan et al., 2020; Pang, 2019), focusing on parallel text simplification datasets and how they can be improved. Why do we need to re-evaluate TS resources? In the last decade, TS research has relied on Wikipedia-based datasets (Zhang and Lapata, 2017; Xu et al., 2016; Jiang et al., 2020), despite their known limitations (Xu et al., 2015; Alva-Manchego et al., 2020a) such as questionable sentence pairs alignments, inaccurate simplifications and a limited variety of simplification modifications. Apart from affecting the reliability of models trained on these datasets, their low quality influences the evaluation relying on automatic metrics that requires goldstandard simplifications, such as SARI (Xu et al., 2016) and BLEU (Papineni et al., 2001). Hence, evaluation data resources must be further explored and improved to achieve reliable eval"
2021.findings-acl.77,D18-1355,0,0.0222214,"of TS datasets, we can build TS models that gain a higher SARI score in our evaluation setting. 2 Related Work The exploration of neural networks in TS started with the work of Nisioi et al. (2017), using the largest parallel simplification resource available (Hwang et al., 2015). Neural-based work focused on state-of-the-art deep learning and MT-based methods, such as reinforcement learning (Zhang and Lapata, 2017), adversarial training (Surya et al., 2019), pointer-copy mechanism (Guo et al., 2018), neural semantic encoders (Vu et al., 2018) and transformers supported by paraphrasing rules (Zhao et al., 2018). Other successful approaches include the usage of control tokens to tune the level of simplification expected (Alva-Manchego et al., 2020a; Scarton and Specia, 2018) and the prediction of operations using parallel corpora (Alva-Manchego et al., 2017; Dong et al., 2020). The neural methods are trained mostly on Wikipedia-based sets, varying in size and improvements in the quality of the alignments. Xu et al. (2015) carried out a systematic study on Wikipedia-based simplification resources, claiming Wikipedia is not a quality resource, based on the observed alignments and the type of simplifica"
2021.findings-emnlp.182,P17-1017,0,0.0472309,"Missing"
2021.findings-emnlp.182,P16-1154,0,0.0738196,"Missing"
2021.findings-emnlp.182,C16-1239,0,0.0171189,"Missing"
2021.findings-emnlp.182,N18-1131,1,0.668723,". Experimental results show the effectiveness of our model in alleviating the incompletion issue and disorder issue whilst copying multi-token entities. 2 2.1 Approach BIO Label Construction In our task, the model input is a sequence of tokens and the output is a set of relation triplets. Following (Nayak and Tou Ng, 2019), we represent the output as a sentence pattern as entity ; entity ; relation |entity ; entity ; relation as presented in Figure 1, where ; is used to separate entities and |is used to separate triplets. Multiple relation tuples with overlapping entities and nested entities (Ju et al., 2018) can be represented in a simple way using these special tokens ; and |. The input and output sentences are then tokenized into subwords by WordPiece (Wu et al., 2016). Then, we adopt the longest common subsequence (LCS) algorithm (Paterson and Dancík, 1994) to generate the BIO labels for each subword. LCS collects the entire longest common subsequences between input and output sentence. For example, as shown in Figure 1, the longest common subsequences are “Evan Z ##ip ##ory ##n”, “Massachusetts Institute of Technology”, “K ##ya ##w K ##ya ##w Na ##ing” and “New York”. Last, we assign a BIO la"
2021.findings-emnlp.182,P18-1047,0,0.0187023,"not, we will retain the original token distribution as BIO = O does. A example is shown in Appendix A.4. During inference, instead of generating tokens in a straightforward manner, at each time step, our model decoder firstly predicts a BIO label . Then, we conduct one of three mask strategies based on the predicted BIO label to narrow down the scope of token distribution, in which it enforces the model 3 Experiments to generate the multi-token entities completely and in a correct order. The detailed mask strategies are Datasets and experiment settings In this as follows: work, we use NYT24 (Zeng et al., 2018) and 2121 NYT29 (Takanobu et al., 2018) as our experimental datasets. Further information can be found in Appendix A.2. We utilize Unilm-base-cased1 as our pretrained model. The model structure of Unilm follows that of BERT-Large (Devlin et al., 2019). The experimental parameters are aligned with those in baselines, full details are listed in Appendix A.3. 3.1 Experimental Results Comparison to previous baselines. Since we extract entity and relation extraction in a seq2seq framework, we compare the performance of GenerativeRE with the state-of-the-art generative models(see Appendix A.1). Tabl"
2021.findings-emnlp.182,P17-1113,0,0.0518008,"Missing"
2021.naacl-main.2,N19-1307,0,0.662694,"tention mechanisms (Lin et al., 2016; Ye and Ling, 2019; Yuan et al., 2019), or hard constraints by explicitly selecting non-noisy instances with reinforcement (Feng et al., 2018; Qin et al., 2018b,a; Wu et al., 2019; Yang et al., 2019) and curriculum learning (Huang and Du, 2019). Noise at the word level was addressed in Liu et al. (2018a) via sub-tree parsing on sentences. Adversarial training has been shown to improve DSRE in Wu et al. (2017), while additional unlabelled examples were exploited to assist classification with Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) in Li et al. (2019). Recent methods use additional information from external resources such as entity types and relations (Vashishth et al., 2018), entity Distantly Supervised RE. Methods developed for DSRE have been around for a long time, building upon the idea of distant supervision (Mintz et al., 2009) with the widely used NYT 10 corpus by Riedel et al. (2010). Methods investigating this problem can be divided into several categories. Initial approaches were mostly graphical models, adopted to perform multi-instance learning (Riedel et al., 2010), sentential evaluation (Hoffmann et al., 2011; Bai and Ritter,"
2021.naacl-main.2,P16-1200,0,0.341587,", the mayor of New Orleans , ... bag 2 Figure 1: Example of the bag-level setting in distantly supervised relation extraction and the main idea of our approach. Sentences are adapted from the NYT 10 dataset (Riedel et al., 2010). The usefulness of distantly supervised relation extraction (DSRE) is reflected in facilitating automatic data annotation, as well as the usage of such data to train models for KB population (Ji and Grishman, 2011). However, DSRE suffers from noisy instances, long-tail relations and unbalanced bag sizes. Typical noise reduction methods have focused on using attention (Lin et al., 2016; Ye and Ling, 2019) or reinforcement learning (Qin et al., 2018b; Wu et al., 2019). For long-tail relations, relation type hierarchies and entity descriptors have been proposed (She et al., 2018; Zhang et al., 2019; Hu et al., 2019), while the limited bag size is usually tackled through incorporation of external data (Beltagy et al., 2019), information from KBs (Vashishth et al., 2018) or pre-trained language models (Alt et al., 2019). Our goal is not to investigate noise reduction, since it has already been widely addressed. Instead, we aim to propose a more general framework that can be eas"
2021.naacl-main.2,D19-1005,0,0.0599611,"Missing"
2021.naacl-main.2,D18-1243,0,0.0643796,"award for best actress . Table 5: Sentence reconstruction examples from the W IKI D ISTANT validation set using different priors. _ corresponds to the UNK word and # indicates a number. 6 Related Work selection of informative instances using either soft constraints, i.e., attention mechanisms (Lin et al., 2016; Ye and Ling, 2019; Yuan et al., 2019), or hard constraints by explicitly selecting non-noisy instances with reinforcement (Feng et al., 2018; Qin et al., 2018b,a; Wu et al., 2019; Yang et al., 2019) and curriculum learning (Huang and Du, 2019). Noise at the word level was addressed in Liu et al. (2018a) via sub-tree parsing on sentences. Adversarial training has been shown to improve DSRE in Wu et al. (2017), while additional unlabelled examples were exploited to assist classification with Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) in Li et al. (2019). Recent methods use additional information from external resources such as entity types and relations (Vashishth et al., 2018), entity Distantly Supervised RE. Methods developed for DSRE have been around for a long time, building upon the idea of distant supervision (Mintz et al., 2009) with the widely used NYT 10 corpus"
2021.naacl-main.2,P18-1046,0,0.548992,"bag-level setting in distantly supervised relation extraction and the main idea of our approach. Sentences are adapted from the NYT 10 dataset (Riedel et al., 2010). The usefulness of distantly supervised relation extraction (DSRE) is reflected in facilitating automatic data annotation, as well as the usage of such data to train models for KB population (Ji and Grishman, 2011). However, DSRE suffers from noisy instances, long-tail relations and unbalanced bag sizes. Typical noise reduction methods have focused on using attention (Lin et al., 2016; Ye and Ling, 2019) or reinforcement learning (Qin et al., 2018b; Wu et al., 2019). For long-tail relations, relation type hierarchies and entity descriptors have been proposed (She et al., 2018; Zhang et al., 2019; Hu et al., 2019), while the limited bag size is usually tackled through incorporation of external data (Beltagy et al., 2019), information from KBs (Vashishth et al., 2018) or pre-trained language models (Alt et al., 2019). Our goal is not to investigate noise reduction, since it has already been widely addressed. Instead, we aim to propose a more general framework that can be easily combined with existing noise reduction methods or pre-traine"
2021.naacl-main.2,P18-1199,0,0.0983512,"bag-level setting in distantly supervised relation extraction and the main idea of our approach. Sentences are adapted from the NYT 10 dataset (Riedel et al., 2010). The usefulness of distantly supervised relation extraction (DSRE) is reflected in facilitating automatic data annotation, as well as the usage of such data to train models for KB population (Ji and Grishman, 2011). However, DSRE suffers from noisy instances, long-tail relations and unbalanced bag sizes. Typical noise reduction methods have focused on using attention (Lin et al., 2016; Ye and Ling, 2019) or reinforcement learning (Qin et al., 2018b; Wu et al., 2019). For long-tail relations, relation type hierarchies and entity descriptors have been proposed (She et al., 2018; Zhang et al., 2019; Hu et al., 2019), while the limited bag size is usually tackled through incorporation of external data (Beltagy et al., 2019), information from KBs (Vashishth et al., 2018) or pre-trained language models (Alt et al., 2019). Our goal is not to investigate noise reduction, since it has already been widely addressed. Instead, we aim to propose a more general framework that can be easily combined with existing noise reduction methods or pre-traine"
2021.naacl-main.2,P19-1023,0,0.105189,"10 and 30 words for W IKI D ISTANT. Each bag in the training set is allowed to contain maximum 500 sentences selected randomly. For prediction on the validation and test sets, all sentences (with full length) are used. W IKI D ISTANT. The WikiDistant dataset is almost double the size of the NYT 10 and contains 454 target relation categories, including the negative relation. It was recently introduced by Han et al. (2020) as a cleaner and more well structured bag-level dataset compared to NYT 10, with fewer negative instances. For the Knowledge Base, we use the version of Wikidata3 provided by Wang et al. (2019b) (in particular the transductive split4 ), containing approximately 5 million entities. Similarly to Freebase, we remove all links between pairs in the test set from the resulting KB, which contains approximately 20 million triples after pruning. 3.2 Bags Table 1: Datasets statistics. ‘NA’ correponds to the ‘no relation’ category. Experimental Settings 3.1 Instances NYT 10 (9) where λ corresponds to a weight in [0, 1]. We weigh the classification loss more than the ELBO to allow the model to better fit the target task. 3 Split 3.4 Baselines In this work we compare with various models applied"
2021.naacl-main.2,N19-1325,0,0.0119584,"t film was ‘ the _ ’ , starring _ and starring _ . _ , who was the first female actress to win the academy award for best actress . Table 5: Sentence reconstruction examples from the W IKI D ISTANT validation set using different priors. _ corresponds to the UNK word and # indicates a number. 6 Related Work selection of informative instances using either soft constraints, i.e., attention mechanisms (Lin et al., 2016; Ye and Ling, 2019; Yuan et al., 2019), or hard constraints by explicitly selecting non-noisy instances with reinforcement (Feng et al., 2018; Qin et al., 2018b,a; Wu et al., 2019; Yang et al., 2019) and curriculum learning (Huang and Du, 2019). Noise at the word level was addressed in Liu et al. (2018a) via sub-tree parsing on sentences. Adversarial training has been shown to improve DSRE in Wu et al. (2017), while additional unlabelled examples were exploited to assist classification with Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) in Li et al. (2019). Recent methods use additional information from external resources such as entity types and relations (Vashishth et al., 2018), entity Distantly Supervised RE. Methods developed for DSRE have been around for a long time"
2021.naacl-main.2,D18-1157,0,0.319431,"Missing"
2021.naacl-main.2,N19-1288,0,0.0493428,"Orleans , ... bag 2 Figure 1: Example of the bag-level setting in distantly supervised relation extraction and the main idea of our approach. Sentences are adapted from the NYT 10 dataset (Riedel et al., 2010). The usefulness of distantly supervised relation extraction (DSRE) is reflected in facilitating automatic data annotation, as well as the usage of such data to train models for KB population (Ji and Grishman, 2011). However, DSRE suffers from noisy instances, long-tail relations and unbalanced bag sizes. Typical noise reduction methods have focused on using attention (Lin et al., 2016; Ye and Ling, 2019) or reinforcement learning (Qin et al., 2018b; Wu et al., 2019). For long-tail relations, relation type hierarchies and entity descriptors have been proposed (She et al., 2018; Zhang et al., 2019; Hu et al., 2019), while the limited bag size is usually tackled through incorporation of external data (Beltagy et al., 2019), information from KBs (Vashishth et al., 2018) or pre-trained language models (Alt et al., 2019). Our goal is not to investigate noise reduction, since it has already been widely addressed. Instead, we aim to propose a more general framework that can be easily combined with ex"
2021.naacl-main.2,D15-1203,0,0.0972093,"RE. Methods developed for DSRE have been around for a long time, building upon the idea of distant supervision (Mintz et al., 2009) with the widely used NYT 10 corpus by Riedel et al. (2010). Methods investigating this problem can be divided into several categories. Initial approaches were mostly graphical models, adopted to perform multi-instance learning (Riedel et al., 2010), sentential evaluation (Hoffmann et al., 2011; Bai and Ritter, 2019) or multi-instance learning and multi-label classification (Surdeanu et al., 2012). Subsequent approaches utilised neural models, with the approach of Zeng et al. (2015) introducing Piecewise Convolutional Neural Networks (PCNN) into the task. Later approaches focused on noise reduction via 18 descriptors (Ji et al., 2017; She et al., 2018; Hu et al., 2019) or Knowledge Bases (Weston et al., 2013; Xu and Barbosa, 2019; Li et al., 2020b). context agnostic knowledge base triples information as latent signals into context aware bag-level entity pairs. Our method is based on a variational autoencoder that is trained jointly with a relation classifier. KB information via a link prediction model is used in the form of prior distributions on the VAE for each pair. T"
2021.naacl-main.2,N19-1306,0,0.0373478,"Missing"
black-etal-2012-data,vasilakopoulos-etal-2004-suite,1,\N,Missing
black-etal-2012-data,N10-1020,0,\N,Missing
black-etal-2012-data,J09-3003,0,\N,Missing
black-etal-2012-data,P09-1026,0,\N,Missing
black-etal-2012-data,rak-etal-2012-collaborative,1,\N,Missing
C00-1077,C92-3150,0,0.19153,"e meaningful way, but terminology has rarely followed suit. We present an approach to term recognition which identi es salient parts of the context and measures their strength of association to relevant candidate terms. The resulting list of ranked terms is shown to improve on that produced by traditional methods, in terms of precision and distribution, while the information acquired in the process can also be used for a variety of other applications, such as disambiguation, lexical tuning and term clustering. 1 Introduction Although statistical approaches to automatic term recognition, e.g. (Bourigault, 1992; Daille et al., 1994; Enguehard and Pantera, 1994; Justeson and Katz, 1995; Lauriston, 1996), have achieved relative success over the years, the addition of suitable linguistic information has the potential to enhance results still further, particularly in the case of small corpora or very specialised domains, where statistical information may not be so accurate. One of the main reasons for the current lack of diversity in approaches to term recognition lies in the diÆculty of extracting suitable semantic information from specialised corpora, particularly in view of the lack of appropriate li"
C00-1077,H92-1022,0,0.0420289,"Missing"
C00-1077,C94-1084,0,0.0172021,"Missing"
C00-1077,W95-0105,0,0.0228592,"e are more non-terms than terms. This therefore demonstrates two things:   more of the terms with the highest semantic weights are valid, and fewer of those with the lowest semantic weights are valid; more valid terms have high semantic weights than non-terms, and more non-terms have lower semantic weights than valid terms. We also tested the similarity measure to see whether adding some statistical information would improve its results, and regulate any discrepancies in the uniformity of the hierarchy. The methods which intuitively seem most plausible are based on information content. e.g.(Resnik, 1995; Smeaton and Quigley, 1996). The information content of a node is related to its probability of occurrence in the corpus. The more frequently it appears, the more likely it is to be important in terms of conveying information, and therefore the higher weighting it should receive. We performed experiments to compare two such methods with our similarity measure. The rst considers the probability of the MSCA of the two terms (the lowest node which is an ancestor of both), whilst the second considers the probability of the nodes of the terms being compared. However, the ndings showed a negligible"
C00-1077,C96-2212,0,0.0918686,"on (in semantic terms) between a candidate term and its context. Evaluation shows improvement over the NC-Value approach, although the percentages are small. This is largely because we have used a very small corpus for testing. The contextual information acquired can also be used for a number of other related tasks, such as disambiguation and clustering. At present, the semantic information is acquired from a pre-existing domain-speci c thesaurus, but there are possibilities for creating such a thesaurus automatically, or enhancing an existing one, using the contextual information we acquire (Ushioda, 1996; Maynard and Ananiadou, 1999b). There is much scope for further extensions of this research. Firstly, it could be extended to other domains and larger corpora, in order to see the true bene t of such an approach. Secondly, the thesaurus could be tailored to the corpus, as we have mentioned. An incremental approach might be possible, whereby the similarity measure is combined with statistical information to tune an existing ontology. Also, the UMLS is not designed as a linguistic resource, but as an information resource. Some kind of integration of the two types of resource would be useful so"
C02-1083,nenadic-etal-2002-automatic,1,0.734504,"nce existing term dictionaries cannot cover the needs of specialists, automatic term extraction tools are important for consistent term discovery. ATRACT (Mima et al., 2001a) is a terminology management workbench that integrates ATR and ATC. Its main aim is to help biologists to gather and manage terminology in the domain. The module retrieves and classifies terms on the fly and sends the results as XML tag information to TIMS. The ATR method is based on the C/NC-value method (Frantzi et al., 2000). The original method has been augmented with acronym acquisition and term variation management (Nenadic et al. 2002), in order to link different terms that denote the same concept. Term variation management is based on term normalisation as an integral part of the ATR process. All orthographic, morphological and syntactic term variations and acronym variants (if any) are conflated prior to the statistical analysis, so that term candidates comprise all variants that appear in a corpus. Besides term recognition, term clustering is an indispensable component in a knowledge management process (see figure 2). Since terminological opacity and polysemy are very common in molecular biology, term clustering is essen"
C02-1083,C96-2212,0,0.117078,"ariants (if any) are conflated prior to the statistical analysis, so that term candidates comprise all variants that appear in a corpus. Besides term recognition, term clustering is an indispensable component in a knowledge management process (see figure 2). Since terminological opacity and polysemy are very common in molecular biology, term clustering is essential for the semantic integration of terms, the construction of domain ontology and for choosing the appropriate semantic information. The ATC method is based on Ushioda’s AMI (Average Mutual Information)-hierarchical clustering method (Ushioda, 1996). Our implementation uses parallel symmetric processing for high speed clustering and is built on the C/NC-value results. As input, we use co-occurrences of automatically recognised terms and their contexts, and the output is a dendrogram of hierarchical term clusters (like a thesaurus). The calculated term cluster information is stored in LiLFeS (see below) and combined with a predefined ontology according to the term classes automatically assigned. 1.3 LiLFeS LiLFeS (Miyao et al., 2000) is a Prolog-like programming language and language processor used for defining definite clause programs wi"
C04-1087,P99-1044,0,0.0214223,"nts (cell in blood, cell from blood), term coordinations (adrenal glands and gonads); Co-affiliation: National Centre for Text Mining, Manchester, UK (v) acronyms and abbreviations: very frequent term variation phenomena in technical sublanguages, especially in biomedicine; sometimes they may be even preferred terms (DNA for deoxyribonucleic acid). Note that variation types (i) – (iii) affect individual constituents, while (iv) and (v) involve variation in structure of the preferred term. In any case, they do not “change” the meaning as they refer to the same concept. Daille et al. (1996) and Jacquemin (1999, 2001) further identified types of variation that modified the meaning of terms. Although many authors mention the problems related to term variation, few have dealt with linking the corresponding term variants. Also, the recognition of variants is typically performed as a separate operation, and not as part of ATR. The simplest technique to handle some types of term variation (e.g. morphological) is based on stemming: if two term forms share a stemmed representation, they are considered as mutual variants (Jacquemin and Tzoukermann, 1999; Ananiadou et al., 2000). However, stemming may result"
C04-1087,nenadic-etal-2002-automatic,1,0.702464,"Missing"
C08-1083,J96-1002,0,0.130705,"t to design effective features for abbreviation recognition and to reuse the knowledge obtained from the training processes. In this paper, we formalize the task of abbreviation recognition as a sequential alignment problem, which finds the optimal alignment (origins of abbreviation letters) between two strings (abbreviation and full form). We design a large amount of features that directly express the events where letters produce or do not produce abbreviations. Preparing an aligned abbreviation corpus, we obtain the optimal combination of the features by using the maximum entropy framework (Berger et al., 1996). We report the remarkable improvements and conclude this paper. 2 2.1 Proposed method Abbreviation alignment model We express a sentence x as a sequence of letters (x1 , ..., xL ), and an abbreviation candidate y in the sentence as a sequence of letters (y1 , ..., yM ). We define a letter mapping a = (i, j) to indicate that the abbreviation letter yj is produced by the letter in the full form xi . A null mapping a = (i, 0) indicates that the letter in the sentence xi is unused to form the abbreviation. Similarly, a null mapping a = (0, j) indicates that the abbreviation letter yj does not ori"
C08-1083,P06-1009,0,0.022573,"2 displays the complete list of generation rules for unigram and bigram features4 , unigram(t) and bigram(s, t). For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai="
C08-1083,P05-1057,0,0.0188745,"rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai= 4 9 13 x: investigate ~ ~ 0 0 #0 0 0 #1 0 0 #2 0 0 #3 0 #4 Shffle 0 0 #5 Shffle 0 0 #6 Shffle 0 0 #7 Shffle 0 0 #8 Shffle 0 . ."
C08-1083,P02-1038,0,0.013388,"For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai= 4 9 13 x: investigate ~ ~ 0 0 #0 0 0 #1 0 0 #2 0 0 #3 0 #4 Shffle 0 0 #5 Shffle 0 0 #6 Shffle 0 0 #7 Shffle"
C08-1083,P02-1021,0,0.515377,"tions including named entity recognition, information retrieval, and question answering. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. The task of abbreviation recognition, in which abbreviations and their expanded forms appearing in actual text are extracted, addresses the term variation problem caused by the increase in the number of abbreviations (Chang and Sch¨utze, 2006). Furthermore, abbreviation recognition is also crucial for disambiguating abbreviations (Pakhomov, 2002; Gaudan et al., 2005; Yu et al., 2006), providing sense inventories (lists of abbreviation definitions), training corpora (context information of full forms), and local definitions of abbreviations. Hence, abbreviation recognition plays a key role in abbreviation management. Numerous researchers have proposed a variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable boundaries, stop words, lengths of abbreviations, and co-occurrence statistics (Park and Byrd, 2001; Wren and Garner, 2002; Liu and Friedman, 2003; Okazaki and Ananiadou"
C08-1083,W01-0516,0,0.670892,"abbreviation recognition is also crucial for disambiguating abbreviations (Pakhomov, 2002; Gaudan et al., 2005; Yu et al., 2006), providing sense inventories (lists of abbreviation definitions), training corpora (context information of full forms), and local definitions of abbreviations. Hence, abbreviation recognition plays a key role in abbreviation management. Numerous researchers have proposed a variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable boundaries, stop words, lengths of abbreviations, and co-occurrence statistics (Park and Byrd, 2001; Wren and Garner, 2002; Liu and Friedman, 2003; Okazaki and Ananiadou, 2006; Zhou et al., 2006; Jain et al., 2007). Schwartz and Hearst (2003) implemented a simple algorithm that finds the shortest expression containing all alphanumerical letters of an abbreviation. Adar (2004) presented four scoring rules to choose the most likely expanded form in multiple candidates. Ao and Takagi (2005) designed more detailed conditions for accepting or discarding candidates of abbreviation definitions. However, these studies have limitations in discovering an optimal combination of heuristic rules from ma"
C08-1083,W02-0312,0,0.115089,"Missing"
C08-1083,D07-1064,0,0.0289905,"list of generation rules for unigram and bigram features4 , unigram(t) and bigram(s, t). For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai= 4 9 13 x: investigate ~"
C08-1096,W97-1002,0,0.0608494,"Missing"
C08-1096,J05-1004,0,0.106505,"Missing"
C08-1096,W06-0602,0,0.399417,"roduce a set of frames that are verb-specific (rather than frames that apply to groups of verbs). Verb-specific frames are able to provide more detailed argument specifications—particularly important in the biomedical field, where phrases that identify information such as location, manner, timing and condition are essential for correct interpretation of events (Tsai et al, 2007). 3 Annotated corpus To aid semantic event frame extraction, we need a corpus annotated with event-level information. Several already exist for biology. Some target extraction of PropBank-style frames (e.g. Chou et al. (2006), Kulick et al. (2004)). The corpus produced by Kim et al. (2008) uses frameindependent roles. However, only a few semantic argument types are annotated. The target of our event frame extraction is a set of semantic frames which specify all potential arguments of gene regulation events. For this purpose, we had to produce our own annotated corpus, using a larger set of event-independent semantic roles than Kim et al. (2008). Our roles had to cover sufficiently wide scope to allow annotation and characterization of all instantiated arguments of relevant events within texts. To our knowledge, th"
C08-1096,W04-3111,0,0.0784792,"Missing"
C08-1096,N03-4009,0,0.0168603,"oka et al, 2005). 763 c. If appropriate, assigned named entity categories to (parts of) the semantic argument span d. If the argument corresponded to a nominalised verb, repeated steps a–c to identify its own arguments. Syntactic chunks were made visible to annotators. In conjunction with annotation guidelines, the chunks were used to help ensure consistency of annotated semantic arguments. For example, the guidelines state that semantic arguments should normally consist of complete (and preferably single) syntactic chunks. The annotation was performed using a customised version of WordFreak (Morton and LaCivita, 2003), a Javabased linguistic annotation tool. 3.4 Corpus statistics The corpus is divided into 2 parts, i.e. 1) 597 abstracts, each annotated by a single annotator, containing a total of 3612 events, 2) 80 pairs of double-annotated documents, allowing checking of inter-annotator agreement and consistency, and containing 1158 distinct events. In the corpus, 277 distinct verbs were annotated as denoting gene regulation events, of which 73 were annotated 10 times or more. In addition, annotation has identified 135 relevant nominalised verbs, of which 22 were annotated 10 times or more. The most commo"
C10-2098,W04-3204,0,0.0257792,"ide variety of semantic categories (e.g., gene, disease, etc.). For example, ER may denote protein estrogen receptor in one context, but cell subunit endoplasmic reticulum in another, One way to entity disambiguation is classifying an entity into pre-defined semantic categories, based on its context (e.g., (Bunescu and Pas¸ca, 2006)). Existing classifiers, such as maximum entropy model, achieved satisfactory results on the “majority” classes with abundant training instances, but failed on the “minority” ones with few or even zero training instances, i.e., the knowledge acquisition bottleneck (Agirre and Martinez, 2004). Furthermore, it is often infeasible to create enough training data for all existing semantic classes. In addition, too many training instances for certain majority classes lead to increased computational complexity for training, and a biased system ignoring the minority ones. These correspond to two previously addressed difficulties in imbalanced learning: “... either (i) you have far more data than your algorithms can deal with, 851 Coling 2010: Poster Volume, pages 851–859, Beijing, August 2010 and you have to select a sample, or (ii) you have no data at all and you have to go through an i"
C10-2098,E06-1002,0,0.0570834,"Missing"
C14-1214,W06-0901,0,0.253654,"g., Riloff (1996). In contrast, supervised learning approaches have constituted a more popular means of approaching the ACE tasks2 . In this paper, we choose to focus on adapting our biomedical-focussed event extraction method to the ACE 2005 task. Our choice is based on the task definition for ACE 2005 having more in common with the BioNLP 2013 GENIA ST definition than the MUC event template task definition. In terms of the characteristics of state-of-the-art event extraction systems designed according to the ACE 2005 model, pipeline-based approaches have been popular (Grishman et al., 2005; Ahn, 2006). Grishman et al. (2005) proposed a method that sequentially identifies textual spans of arguments, role types, and event triggers. This pipeline approach has been further extended in several subsequent studies. For example, Liao et al. (2010) investigated document-level cross-event consistency using co-occurrence of events and event arguments, while Hong et al. (2011) exploited information gathered from the web to ensure cross-entity consistency. 2 Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) for the ACE 2005 task, but th"
C14-1214,W13-2003,0,0.0309991,"Missing"
C14-1214,J92-4003,0,0.309509,"Missing"
C14-1214,M98-1001,0,0.129253,"Missing"
C14-1214,W06-2202,0,0.0313226,"trigger and arguments (see Figures 1 and 2.) A trigger is typically a verb or a nominalised verb that denotes the presence of the event in the text, while the arguments are usually entities. In general, arguments are assigned semantic roles that characterise their contribution towards the event description. Until now, however, there has been little, if any, effort by researchers working on event extraction in different domains to share ideas and techniques, unlike syntactic tasks (e.g., (Miyao and Tsujii, 2008)) and other information extraction tasks, such as named entity recognition (e.g., (Giuliano et al., 2006)) and relation extraction (e.g., (Qian and Zhou, 2012)). This means that the potential to exploit crossdomain features of events to develop more adaptable event extraction systems is an under-studied area. Consequently, although there is a large number of published studies on event extraction, proposing many different methods, no work has previously been reported that aims to adapt an event extraction method developed for one domain to a new domain. In response to the above, we have investigated the feasibility of adapting an event extraction method developed for the biomedical domain to the n"
C14-1214,P13-2082,0,0.0252783,"investigate the adaptation of alternative methods proposed for use in one domain to another domain. Several interesting approaches have been described, such as the utilisation of contextual information beyond the boundaries of individual sentences in the newswire domain (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and joint approaches in the biomedical domain (McClosky et al., 2012), but their adaptability to other domains has not yet been investigated. We also intend to investigate the possibility of discovering and utilising shared information between the two domains (Goldwasser and Roth, 2013). Encouraging greater levels of communication between researchers working on NLP tasks in different domains will help to stimulate such new directions of research, both for event extraction and for other related information extraction tasks, such as relation extraction and coreference resolution. Acknowledgements This work was supported by the Arts and Humanities Research Council (AHRC) [grant number AH/L00982X/1], the Medical Research Council [grant number MR/L01078X/1], the European Community’s Seventh Program (FP7/2007-2013) [grant number 318736 (OSSMETER)], and the JSPS Grant-inAid for You"
C14-1214,W13-2004,0,0.0233546,"Missing"
C14-1214,P11-1113,0,0.412161,"tion than the MUC event template task definition. In terms of the characteristics of state-of-the-art event extraction systems designed according to the ACE 2005 model, pipeline-based approaches have been popular (Grishman et al., 2005; Ahn, 2006). Grishman et al. (2005) proposed a method that sequentially identifies textual spans of arguments, role types, and event triggers. This pipeline approach has been further extended in several subsequent studies. For example, Liao et al. (2010) investigated document-level cross-event consistency using co-occurrence of events and event arguments, while Hong et al. (2011) exploited information gathered from the web to ensure cross-entity consistency. 2 Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) for the ACE 2005 task, but they are not so many and we will focus on the supervised learning approaches in this paper. 2271 Li et al. (2013) recently proposed a joint detection method to detect both triggers and arguments (together with their role types) using a structured perceptron model. The system outperformed the best results reported for the ACE 2005 task in the literature, without the use o"
C14-1214,P08-1030,0,0.49716,"ne-based approaches have been popular (Grishman et al., 2005; Ahn, 2006). Grishman et al. (2005) proposed a method that sequentially identifies textual spans of arguments, role types, and event triggers. This pipeline approach has been further extended in several subsequent studies. For example, Liao et al. (2010) investigated document-level cross-event consistency using co-occurrence of events and event arguments, while Hong et al. (2011) exploited information gathered from the web to ensure cross-entity consistency. 2 Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) for the ACE 2005 task, but they are not so many and we will focus on the supervised learning approaches in this paper. 2271 Li et al. (2013) recently proposed a joint detection method to detect both triggers and arguments (together with their role types) using a structured perceptron model. The system outperformed the best results reported for the ACE 2005 task in the literature, without the use of any external resources. 2.2 Biomedical Event Extraction The task of event extraction has received a large amount of attention from BioNLP researchers in recent years. Interest"
C14-1214,P13-1008,0,0.103204,"nts, role types, and event triggers. This pipeline approach has been further extended in several subsequent studies. For example, Liao et al. (2010) investigated document-level cross-event consistency using co-occurrence of events and event arguments, while Hong et al. (2011) exploited information gathered from the web to ensure cross-entity consistency. 2 Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) for the ACE 2005 task, but they are not so many and we will focus on the supervised learning approaches in this paper. 2271 Li et al. (2013) recently proposed a joint detection method to detect both triggers and arguments (together with their role types) using a structured perceptron model. The system outperformed the best results reported for the ACE 2005 task in the literature, without the use of any external resources. 2.2 Biomedical Event Extraction The task of event extraction has received a large amount of attention from BioNLP researchers in recent years. Interest in this task was largely initiated by the BioNLP 2009 ST, and has been sustained through the organisation of further STs in 2011 and 2013. The STs consist of a nu"
C14-1214,P10-1081,0,0.735232,"mber of ways. Apart from the different textual domain, the tasks adopt varying annotation schemes. The exact kinds of annotations provided at training time are also different, as are the evaluation settings. Several variants of the official task setting for the ACE 2005 corpus have been defined. This is partly due to the demanding nature of the official task definition, which requires the detection of events from scratch, including the recognition of named entities participating in events, together with the resolution of coreferences. Alternative task settings (such as Ji and Grishman (2008); Liao and Grishman (2010))) generally simplify the official task definition, e.g., by omitting the requirement to perform coreference resolution. A further issue is that the test data sets for the official task setting have not been made publicly available. As a result of the multiple existing variations of the ACE 2005 task definition that have been employed by different research efforts, direct comparison of our results with those obtained by other state-of-the art systems is problematic. The solution we have chosen is to adopt the same ACE 2005 event extraction task specification that has been adopted in recent res"
C14-1214,P12-1088,0,0.0204545,"e been popular (Grishman et al., 2005; Ahn, 2006). Grishman et al. (2005) proposed a method that sequentially identifies textual spans of arguments, role types, and event triggers. This pipeline approach has been further extended in several subsequent studies. For example, Liao et al. (2010) investigated document-level cross-event consistency using co-occurrence of events and event arguments, while Hong et al. (2011) exploited information gathered from the web to ensure cross-entity consistency. 2 Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) for the ACE 2005 task, but they are not so many and we will focus on the supervised learning approaches in this paper. 2271 Li et al. (2013) recently proposed a joint detection method to detect both triggers and arguments (together with their role types) using a structured perceptron model. The system outperformed the best results reported for the ACE 2005 task in the literature, without the use of any external resources. 2.2 Biomedical Event Extraction The task of event extraction has received a large amount of attention from BioNLP researchers in recent years. Interest in this task was lar"
C14-1214,J08-1002,0,0.0961272,"domains, the same general features of events normally hold across domains. An event usually consists of a trigger and arguments (see Figures 1 and 2.) A trigger is typically a verb or a nominalised verb that denotes the presence of the event in the text, while the arguments are usually entities. In general, arguments are assigned semantic roles that characterise their contribution towards the event description. Until now, however, there has been little, if any, effort by researchers working on event extraction in different domains to share ideas and techniques, unlike syntactic tasks (e.g., (Miyao and Tsujii, 2008)) and other information extraction tasks, such as named entity recognition (e.g., (Giuliano et al., 2006)) and relation extraction (e.g., (Qian and Zhou, 2012)). This means that the potential to exploit crossdomain features of events to develop more adaptable event extraction systems is an under-studied area. Consequently, although there is a large number of published studies on event extraction, proposing many different methods, no work has previously been reported that aims to adapt an event extraction method developed for one domain to a new domain. In response to the above, we have investi"
C14-1214,D07-1111,0,0.0272836,"VENT TOTAL results obtained using the approximate span & recursive evaluation method, as recommended by the organisers. The method individually evaluates each complete core event, i.e., event triggers with their Theme and/or Cause role arguments, with relaxed span matching, after nested events have been broken down as explained in Section 3.1. Note that the scores do not count the non-named entities, hedges, and links between arguments, since only core events are considered in the official evaluation. We applied both a deep parser, Enju (Miyao and Tsujii, 2008) and a dependency parser, ksdep (Sagae and Tsujii, 2007) to generate features for the ACE 2005 task, and their bio-adapted versions for the GENIA task. We also employed the GENIA sentence splitter (Sætre et al., 2007) for sentence splitting, and the snowball (Porter2) stemmer4 for stemming. We did not make use of any other external resources, such as dictionaries, since this would hinder direct comparison of the two versions of the system. 4.2 Evaluation on GENIA The “Event Detection” column in Table 2 shows evaluation results of BioEE on GENIA. The effects on performance by including entity-related features, i.e., entity base forms and Brown clust"
C14-1214,W13-2002,0,\N,Missing
C94-2167,A92-1014,0,0.0985988,"Missing"
C94-2167,C92-2085,0,0.02174,"Missing"
C96-1009,H92-1022,0,0.00706817,"value greater than the&apos; preset threshold will take part to tile rest of the algorithm. We set this threshold to ;I again for the, same reason as above (the gain we wouhl gel; for precision if we had set a higher threshold would be lost on recall). Table 3 shows the candidate c,ollocations with the higher values, extra(&apos;Le(l with C-value. A lot of eandidate e,otlocations extracted may seem unimportant. This is because t}le algorithm extracts tile word sequences that are fl&apos;equent. Which of these candidate collocations we should keep depends on the apt)lication. Brill&apos;s t)art-of-speeeh tagger, (Brill, 1992), was used to remove the ng r a m s that had an article as their last wor(1. &apos;l&apos;a,I)l(~ 3: Exi,raci,(&apos;d c~m(tida,t(~ (:olloca, i,ion with Cvakae in (l(~,s(:(m(iin&lt;~ or(l(;r. [ C:V 2 _F. I L84 (JandidA te Colloci tion .... 15 i5 ]5 15 ~ V i  L ] - ; S T R , 1 0 E T ,J()UFLNAL StM[ Rel)ort(:r o[ q&apos;tie Wall SI;reei; 19 Journal ( hlil;(xl SI;a,l;es 93 t;[l(; Unilx!d SI;iti;es 44 i;he Unil;ed 59 { l l l l l [ l l ) ( w ) to &lt;~lllOlt(~y) [&apos;FO1H 20 I;O (Ill()ll(;y) [!I&apos;O1H 25 said il; 48 44 (;h(! (;Olnl)~Hly ,VMI ~l;r(?el; J(iurnal 26 &lt;~tillllll)(~l&apos;~ &gt; I;o &lt;~iH()ll(&apos;,y~&gt; [l&apos;Oll[ (i (HIOll(~y) }1 V"
C96-1009,J90-1003,0,0.0475034,"Missing"
C96-1009,C94-1101,0,0.0182249,"Missing"
C96-1009,J93-1007,0,0.65336,"ons are perwtsive in language: ""letters"" are ""deliw:red"", ""tea"" is ""strong"" and not ""powelful"", we ""l&apos;mt progrants"", aitd so Oll. Linguists have long been interested in collocations and the detinitions are nuiaerous and varied. Some researchers include multi-o.leinent eOlnpOuIlds as (;xamples of collocations; some a d m i t only collocations (:onsisl;ing of pairs of words, while others admit only eollo(;ations consisting of a maximum of tive or six words; some emphasize synl,aglnat, ic aspecl;s, others Selnmtl;ic aspects. T h e COlllillOil poini;s regarding collocations a p p e a r to be, as (Smadja, 1993) suggestsl: they are m&apos;bil;rary (it is nol; clear why to ""Bill t h r o u g h "" means to ""fail""), th(&apos;y are d o m a i n - d e p e n d e n t (""interest rate"", ""stock market""), t;hey are recurrenl; and cohesive lo~xical clusters: the presence of one of the. collocates strongly Sltggesl;S /,tie rest of the cellocat, ion (""Ulfited"" could ilnply ""States"" or ""Kingdom""). the classiiics collocations into i)redicative relations, rigid noun phrases and phrasal telnplatcs. It is not the goal of this paper to provide yet another definition of collocation. We adopt as a working definition the one by (Sincla"
D08-1047,H05-1120,0,0.606315,"p yoshimasa.tsuruoka@manchester.ac.uk Sophia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative"
D08-1047,I08-1007,0,0.0973065,"native framework of string similarity. MaCallum et al. (2005) proposed a method to train the costs of edit operations using Conditional Random Fields (CRFs). Bergsma and Kondrak (2007) correct comparative and superlative adjectives, e.g., unpopular → unpopularer → unpopularest and refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary loo"
D08-1047,J96-1002,0,0.0553682,"be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 t∗ = argmax P (t|s). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applicati"
D08-1047,P07-1083,0,0.0802925,"ormance of the L1 regularized logistic regression as a discriminative model, we also built two classifiers based on the Support Vector Machine (SVM). These SVM classifiers were implemented by the SVMperf 7 on a linear kernel8 . An SVM classifier employs the same feature set (substitution rules) as the proposed method so that we can directly compare the L1 regularized logistic regression and the linear-kernel SVM. Another SVM classifier incorporates the five string metrics; this system can be considered as our reproduction of the discriminative string similarity proposed by Bergsma and Kondrak (2007). Table 3 reports the precision (P), recall (R), and F1 score (F1) based on the number of correct decisions for positive instances. The proposed method outperformed the baseline systems, achieving 0.919, 0.888, and 0.984 of F1 scores, respectively. Porter’s stemmer worked on the Inflection set, but not on the Orthography set, which is beyond the scope of the stemming algorithms. CST’s lemmatizer suffered from low recall on the Inflection set because it removed suffixes of base forms, e.g., (cloning, clone) → (clone, clo). Morpha and CST’s lemma6 We used CST’s lemmatiser version 2.13: http://ww"
D08-1047,P00-1037,0,0.103699,"ary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 t∗ = argmax P (t|s). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in whi"
D08-1047,J95-4004,0,0.0169888,"iven s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string (2) Here, the function dist(s, t) denotes the weighted Levenshtein distance (Levenshtein, 1966) between strings s and t. Furthermore, the threshold δ requires the distance between the source string s and a candidate string t to be less"
D08-1047,D07-1019,0,0.710798,"ia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative instances that affect the decision b"
D08-1047,dalianis-jongejan-2006-hand,0,0.022719,"larer → unpopularest and refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination stri"
D08-1047,2005.mtsummit-papers.40,0,0.165856,"a.tsuruoka@manchester.ac.uk Sophia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative"
D08-1047,P06-1129,0,0.339768,"nd refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination stri"
D08-1047,J99-1003,0,0.038867,"f the tasks: classification (Section 3.2) and normalization (Section 3.3). 3.2 Experiment 1: Candidate classification In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels. We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset5 . Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s stemmer (Porter, 1980), Morpha (Minnen et al., 2001), and CST’s lemmatiser (Dalianis and Jonge3 LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.). 4 LRAGR table also provides agreement information even when word forms do not change. For example, the table contains an entry indicating that the first-singular present form of the verb study is st"
D08-1047,P99-1004,0,\N,Missing
D09-1157,W04-3204,0,0.0232618,"ython, users may like to see the results grouped into the following categories: a type of snake, a programming language, or a film (Bunescu and Pas¸ca, 2006). One approach to such lexical disambiguation tasks is supervised classification. However, such techniques suffer from the knowledge acquisition bottleneck, meaning that manually annotating training data is costly and can never satisfy the need by the machine learning algorithms. In addition, supervised techniques may not yield reliable results when the distributions of the semantic classes are different in the training and test datasets (Agirre and Martinez, 2004; Koeling et al., 2005). For example, on the task of word sense disambiguation, a model trained on a dataset where the predominant sense of the word star is “heavenly body”, may not work well on text mainly composed of entertainment news. Such problems are also major concerns when developing a system to disambiguate biomedical named entities (e.g., protein, 1513 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP gene, and disease), for which some researchers rely on hand-crafted rules in addi"
D09-1157,W08-0601,0,0.0270035,"Missing"
D09-1157,P06-4020,0,0.0248306,"OS tags and punctuation labels that were derived from the CLAWS-7 tagset,10 whereas our dataset uses POS labels from the Penn Treebank tagset (Marcus et al., 1994). As RASP does not recognise the Penn tagset, we used its build-in POS tagger. Minipar, on the other hand, does not support input of tokenised or POStagged text, and therefore took split sentences as input. Secondly, the output representations of the parsers are different and we preferred a format that depicts relations between words instead of syntactic constituents. In total, 4 representations were used: grammatical relation (GR) (Briscoe et al., 2006), Stanford typed dependency (SD) (de Marneffe et al., 2006), Minipar’s own representation (Lin, 1998), and ENJU’s predicate-argument structure (PAS). All the above representations define relations of words in triples, where a dependency triple (i.e., GR, SD and Minipar) consists of head, dependent and relation, and a PAS triple contains predicate, argument, and relation. Figure 1 shows a sentence parsed by ENJU in PAS representation. The right-most column in Table 1 lists the output representation of each parser. A syntactic path between an entity and a species word was represented by a sequen"
D09-1157,E06-1002,0,0.0994251,"Missing"
D09-1157,J07-4004,0,0.0353843,"Missing"
D09-1157,de-marneffe-etal-2006-generating,0,0.0109684,"Missing"
D09-1157,D07-1024,0,0.0604273,"Missing"
D09-1157,W07-2202,1,0.896692,"Missing"
D09-1157,E06-1015,0,0.0312211,"niversity of Manchester, UK ‡ National Centre for Text Mining, UK ∗ Department of Computer Science, University of Tokyo, Japan {xinglong.wang,j.tsujii,sophia.ananiadou}@manchester.ac.uk Abstract exploited by similarity measures or machine learning algorithms. For example, Erkan et al. (2007) used the shortest path between two genes according to edit distance in a dependency tree to define a kernel function for extracting gene interactions. Miwa et al. (2008) comparably evaluated a number of kernels for incorporating syntactic features, including the bag-of-word kernel, the subset tree kernel (Moschitti, 2006) and the graph kernel (Airola et al., 2008), and they concluded that combining all kernels achieved better results than using any individual one. Miyao et al. (2008) used syntactic paths as one of the features to train a support vector machines (SVM) model for PPIs and also discussed how different parsers and output representations affected the end results. Named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambiguous identifier in a standard database. One approach to this task is supervised classification. However, the availability of t"
D09-1157,I05-2038,1,0.746845,"ed in Table 1): • Dependency parsers identify one word as the head of a sentence and all other words are either a dependent of that word, or else dependent on some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); Figure 2: A syntactic feature obtained from the ENJU parser. 4.3.2 Syntactic Features Given a sentence, a natural language parser automatically recognises its syntactic structure and outputs a parse tree, in which nodes represent words or"
D09-1157,wang-grover-2008-learning,1,0.829921,"ue words for species are words denoting names of model organisms (e.g., mouse as in 1 http://www.ncbi.nlm.nih.gov/RefSeq http://www.ncbi.nlm.nih.gov/sites/ entrez?db=taxonomy 2 phrase “mouse p53”). Another clue is the presence of the species-indicating prefixes in gene and protein names. For instance, prefix ‘h’ in entity “hSos-1” suggests that it is a human protein. Throughout this paper, we refer to such cue words (e.g., mouse, hSos-1) as “species words”. Note that a species “word” may contain multiple tokens (e.g., E. Coli). We encoded this knowledge in a rule-based species tagging system (Wang and Grover, 2008). The system takes a 2-step approach. First, it marks up species words in the document using a speciesword detection program,3 which searches every word in a dictionary of model organisms and assigns a species ID to the word if a match is found. The dictionary was built using the NCBI taxonomy4 and the UniProt controlled vocabulary of species,5 and in total it contains 420,224 species words for 324,157 species IDs. When species words are identified, we disambiguate an entity mention using one of the following rules: 1. previous species word: If the word preceding an entity is a species word, a"
D09-1157,C00-2137,0,0.0505769,"Missing"
D09-1157,P03-1054,0,0.00368557,"ul to infer biological relations (e.g., Airola et al., 2008; Miwa et al., 2008). We experimented with the following parsers (summarised in Table 1): • Dependency parsers identify one word as the head of a sentence and all other words are either a dependent of that word, or else dependent on some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); Figure 2: A syntactic feature obtained from the ENJU parser. 4.3.2 Syntactic Features Given a sentence, a na"
D09-1157,H05-1053,0,0.0316519,"e the results grouped into the following categories: a type of snake, a programming language, or a film (Bunescu and Pas¸ca, 2006). One approach to such lexical disambiguation tasks is supervised classification. However, such techniques suffer from the knowledge acquisition bottleneck, meaning that manually annotating training data is costly and can never satisfy the need by the machine learning algorithms. In addition, supervised techniques may not yield reliable results when the distributions of the semantic classes are different in the training and test datasets (Agirre and Martinez, 2004; Koeling et al., 2005). For example, on the task of word sense disambiguation, a model trained on a dataset where the predominant sense of the word star is “heavenly body”, may not work well on text mainly composed of entertainment news. Such problems are also major concerns when developing a system to disambiguate biomedical named entities (e.g., protein, 1513 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP gene, and disease), for which some researchers rely on hand-crafted rules in addition to a small amount"
D09-1157,J08-1002,1,0.82825,"Missing"
D09-1157,P08-1006,1,0.821202,"iadou}@manchester.ac.uk Abstract exploited by similarity measures or machine learning algorithms. For example, Erkan et al. (2007) used the shortest path between two genes according to edit distance in a dependency tree to define a kernel function for extracting gene interactions. Miwa et al. (2008) comparably evaluated a number of kernels for incorporating syntactic features, including the bag-of-word kernel, the subset tree kernel (Moschitti, 2006) and the graph kernel (Airola et al., 2008), and they concluded that combining all kernels achieved better results than using any individual one. Miyao et al. (2008) used syntactic paths as one of the features to train a support vector machines (SVM) model for PPIs and also discussed how different parsers and output representations affected the end results. Named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambiguous identifier in a standard database. One approach to this task is supervised classification. However, the availability of training data is often limited, and the available data sets tend to be imbalanced and, in some cases, heterogeneous. We propose a new method that distinguishes a name"
D09-1157,J93-2004,0,\N,Missing
D09-1157,P08-1000,0,\N,Missing
D14-1177,C10-1003,1,0.905281,"Missing"
D14-1177,C12-1046,0,0.380621,"ance of context vectors drastically decreases for lower frequency terms (Kontonatsios et al., 2014; Morin and Daille, 2010). Our work is more closely related to a second class of term alignment methods that exploits the internal structure of terms between a source and a target language. Compositional translation algorithms are based on the principal of compositionality (Keenan and Faltz, 1985), which claims that the translation of the whole is a function of the translation of its parts. Lexical (Morin and Daille, 2010; Daille, 2012; Robitaille et al., 2006; 1702 Tanaka, 2002) and sub-lexical (Delpech et al., 2012) compositional algorithms are knowledgerich approaches that proceed in two steps, namely generation and selection. In the generation step, an input source term is segmented into basic translation units: words (lexical compositional methods) or morphemes (sub-lexical methods). Then a pre-compiled, seed dictionary of words or morphemes is used to translate the components of the source term. Finally, a permutation function generates candidate translations using the list of the translated segments. In the selection step, candidate translations are ranked according to their frequency (Morin and Dai"
D14-1177,J93-1003,0,0.221366,"lemmatisation and Part-of-Speech (PoS) tagging. For English, Spanish and French we used the TreeTagger (Schmid, 1994) while for Greek we used the ILSP toolkit (Papageorgiou et al., 2000). The Japanese corpus was segmented and PoS-tagged using Juman (Kurohashi and Kawahara, 2005). In succession, monolingual context vectors are compiled by considering all lexical units that occur within a window of 3 words before or after a term (a seven-word window). Only lexical units (seeds) that occur in a bilingual dictionary are retained The values in context vectors are LogLikelihood Ratio associations (Dunning, 1993) of the term and a seed lexical unit occurring in it. In a second step, we use the translations in the seed dictionary to map target context vectors into the source vector space. If there are several translations for a term, they are all considered with equal weights. Finally, candidate translations are ranked in descending order of the cosine of the angle between the mapped target vectors and the source 1704 seed term dictionary seed word dictionary Train Project character n-gram model context vectors Annotate the Wikipedia interlingual links to retrieve thematically related articles in each"
D14-1177,P98-1069,0,0.795113,"a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constructed more easily than parallel corpora. Freely available, up-to-date, on-line resources (e.g., Wikipedia) can be employed. In this paper, we exploit two different sources of information to extract bilingual terminology from comparable corpora: the compositional and the contextual clue. The compositional clue is the hypothesis that the representations of a term in any p"
D14-1177,N13-1056,0,0.378841,"s to train the n-gram models and (b) a dictionary of word-to-word correspondences to translate target context vectors. The n-gram and context vector methods are used separately to score term pairs. The n-gram model computes the value of the compositional clue while the context vector estimates the score of the contextual clue. The hybrid model combines both methods by using the corresponding scores as features to train a linear classifier. For this, we used a linear-SVM of the LIBSVM package with default values for all parameters. 4 Data Following previous research (Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013; Klementiev et al., 2012), we construct comparable biomedical corpora using Wikipedia as a freely available resource. Starting with a list of 4K biomedical English terms (query-terms), we collected 4K English Wikipedia articles, by matching query-terms to the topic signatures of articles. Then, we followed Seed dictionaries As shown in Figure 1, the term alignment methods require two seed bilingual dictionaries: a term and a word dictionary. The character n-gram models rely on a bilingual term dictionary to learn associations of n-grams that appear often in technical terms. The dictionary may"
D14-1177,C02-2020,0,0.257592,"s.kontonatsios/ Software/LogReg-TermAlign.tar.gz 2 Related Work Context-based methods (Fung and Yee, 1998; Rapp, 1999) adapt the Distributional Hypothesis (Harris, 1954), i.e., words that occur in similar lexical context tend to have the same meaning, in a multilingual environment. They represent the context of each term t as a context vector, usually following the bag-of-words model. Each dimension of the vector corresponds to a context word occurring within a predefined window, while the corresponding value is computed by a correlation metric, e.g., Log-Likelihood Ratio (Morin et al., 2007; Chiao and Zweigenbaum, 2002) or Point-wise Mutual Information (Andrade et al., 2010). A general bilingual dictionary is then used to translate/project the target context vectors into the source language. As a result, the source and target context vectors become directly comparable. In a final step, candidate translations are being ranked according to a distance metric, e.g., cosine similarity (Tamura et al., 2012) or Jaccard index (Zanzotto et al., 2010; Apidianaki et al., 2012). Whilst context-based methods have become a common practise for bilingual dictionary extraction from comparable corpora, nonetheless, their perf"
D14-1177,A94-1006,0,0.115245,"ervation, we use an existing context-based approach. For evaluation, we investigate the performance of compositional and context-based methods on: (a) similar and unrelated languages, (b) corpora of different degree of comparability and (c) the translation of frequent and rare terms. Finally, we combine the two translation clues, namely string and contextual similarity, in a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constr"
D14-1177,C12-1110,0,0.0296836,"Missing"
D14-1177,E12-1014,0,0.0796051,"(b) a dictionary of word-to-word correspondences to translate target context vectors. The n-gram and context vector methods are used separately to score term pairs. The n-gram model computes the value of the compositional clue while the context vector estimates the score of the contextual clue. The hybrid model combines both methods by using the corresponding scores as features to train a linear classifier. For this, we used a linear-SVM of the LIBSVM package with default values for all parameters. 4 Data Following previous research (Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013; Klementiev et al., 2012), we construct comparable biomedical corpora using Wikipedia as a freely available resource. Starting with a list of 4K biomedical English terms (query-terms), we collected 4K English Wikipedia articles, by matching query-terms to the topic signatures of articles. Then, we followed Seed dictionaries As shown in Figure 1, the term alignment methods require two seed bilingual dictionaries: a term and a word dictionary. The character n-gram models rely on a bilingual term dictionary to learn associations of n-grams that appear often in technical terms. The dictionary may contain both singleword a"
D14-1177,P07-2045,0,0.00455687,"dels rely on a bilingual term dictionary to learn associations of n-grams that appear often in technical terms. The dictionary may contain both singleword and multi-word terms. For English-Spanish and English-French we used UMLS (Bodenreider, 2004) while for English-Japanese we used an electronic dictionary of medical terms (Denshika and Kenkyukai, 1991). An English-Greek biomedical dictionary was not available at the time of conducting these experiments, thus we automatically compiled a dictionary from a parallel corpus. For this, we trained a standard Statistical Machine Translation system (Koehn et al., 2007) on EMEA (Tiedemann, 2009), a biomedical parallel corpus containing sentencealigned documents from the European Medicines Agency. Then, we extracted all English-Greek pairs for which: (a) the English sequence was listed in UMLS and (b) the translation probability was equal or higher to 0.7. The sizes of the seed term dictionaries vary significantly, e.g., 500K entries for English-French but only 20K entries for English-Greek. However, the character n-gram models require a relatively small portion of the corresponding dictionary to converge. In the reported experiments, we used 10K translation"
D14-1177,E14-4022,1,0.816528,"p-to-date, on-line resources (e.g., Wikipedia) can be employed. In this paper, we exploit two different sources of information to extract bilingual terminology from comparable corpora: the compositional and the contextual clue. The compositional clue is the hypothesis that the representations of a term in any pair of languages tend to consist of corresponding lexical or sub-lexical units, e.g., prefixes, suffices and morphemes. In order to capture associations of textual units across languages, we investigate three different character n-gram approaches, namely a Random Forest (RF) classifier (Kontonatsios et al., 2014), Support Vector Machines with an RBF kernel (SVM-RBF) and a Logistic Regression (LogReg) classifier. Whilst the previous approaches take as an input monolingual features and then try to find cross-lingual mappings, our proposed method (LogReg classifier) considers multilingual features, i.e., tuples of cooccurring n-grams. The contextual clue is the hypothesis that mutual translations of a term tend to occur in similar lexical context. Context-based approaches are unsupervised methods that compare the context distributions of a source and a target term. A bilingual seed dictionary is used to"
D14-1177,C10-1073,0,0.0655114,"M-RBF) and a Logistic Regression (LogReg) classifier. Whilst the previous approaches take as an input monolingual features and then try to find cross-lingual mappings, our proposed method (LogReg classifier) considers multilingual features, i.e., tuples of cooccurring n-grams. The contextual clue is the hypothesis that mutual translations of a term tend to occur in similar lexical context. Context-based approaches are unsupervised methods that compare the context distributions of a source and a target term. A bilingual seed dictionary is used to map context vector dimensions of two languages. Li and Gaussier (2010) suggested that the seed dictionary can be used to estimate the degree of comparability of a bilingual corpus. Given a seed dictionary, the corpus comparability is the expectation of finding for each word of the source corpus, its translation in the target part of the corpus. The performance of context-based methods has been shown to depend on the frequency of terms to be translated and the 1701 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1701–1712, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics corpu"
D14-1177,papageorgiou-etal-2000-unified,0,0.00884113,"idate translations by classification margin. 3.2 Context vectors We follow a standard approach to calculate context similarity of source and target terms (Rapp, 1999; Morin and Daille, 2010; Morin and Prochasson, 2011a; Delpech et al., 2012). Context vectors of candidate terms in the source and target language are populated after normalising each bilingual corpus, separately. Normalisation consists of stop-word filtering, tokenisation, lemmatisation and Part-of-Speech (PoS) tagging. For English, Spanish and French we used the TreeTagger (Schmid, 1994) while for Greek we used the ILSP toolkit (Papageorgiou et al., 2000). The Japanese corpus was segmented and PoS-tagged using Juman (Kurohashi and Kawahara, 2005). In succession, monolingual context vectors are compiled by considering all lexical units that occur within a window of 3 words before or after a term (a seven-word window). Only lexical units (seeds) that occur in a bilingual dictionary are retained The values in context vectors are LogLikelihood Ratio associations (Dunning, 1993) of the term and a seed lexical unit occurring in it. In a second step, we use the translations in the seed dictionary to map target context vectors into the source vector s"
D14-1177,P11-1133,0,0.0678086,"ry of term translation pairs to train the n-gram models and (b) a dictionary of word-to-word correspondences to translate target context vectors. The n-gram and context vector methods are used separately to score term pairs. The n-gram model computes the value of the compositional clue while the context vector estimates the score of the contextual clue. The hybrid model combines both methods by using the corresponding scores as features to train a linear classifier. For this, we used a linear-SVM of the LIBSVM package with default values for all parameters. 4 Data Following previous research (Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013; Klementiev et al., 2012), we construct comparable biomedical corpora using Wikipedia as a freely available resource. Starting with a list of 4K biomedical English terms (query-terms), we collected 4K English Wikipedia articles, by matching query-terms to the topic signatures of articles. Then, we followed Seed dictionaries As shown in Figure 1, the term alignment methods require two seed bilingual dictionaries: a term and a word dictionary. The character n-gram models rely on a bilingual term dictionary to learn associations of n-grams that appear often in te"
D14-1177,P99-1067,0,0.682704,"milarity, in a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constructed more easily than parallel corpora. Freely available, up-to-date, on-line resources (e.g., Wikipedia) can be employed. In this paper, we exploit two different sources of information to extract bilingual terminology from comparable corpora: the compositional and the contextual clue. The compositional clue is the hypothesis that the representatio"
D14-1177,W11-1205,0,0.0718035,"ale, linear classifications problems. LIBLINEAR implements two linear classification algorithms: LogReg and linear-SVM. Both models solve the same optimisation problem, i.e., determine the optimal separating plane, but they adopt different loss functions. Since LIBLINEAR does not support decision value estimations for the linear-SVM, we only experimented with LogReg. Similarly to SVM-RBF, LogReg ranks candidate translations by classification margin. 3.2 Context vectors We follow a standard approach to calculate context similarity of source and target terms (Rapp, 1999; Morin and Daille, 2010; Morin and Prochasson, 2011a; Delpech et al., 2012). Context vectors of candidate terms in the source and target language are populated after normalising each bilingual corpus, separately. Normalisation consists of stop-word filtering, tokenisation, lemmatisation and Part-of-Speech (PoS) tagging. For English, Spanish and French we used the TreeTagger (Schmid, 1994) while for Greek we used the ILSP toolkit (Papageorgiou et al., 2000). The Japanese corpus was segmented and PoS-tagged using Juman (Kurohashi and Kawahara, 2005). In succession, monolingual context vectors are compiled by considering all lexical units that oc"
D14-1177,W02-2026,0,0.0328349,"for rare terms. Finally, we hypothesised that the n-gram and context-based methods provide complimentary information. To test this hypothesis, we developed a hybrid method that combines compositional and contextual similarity scores as features in a linear classifier. The hybrid model achieved significantly better top-20 translation accuracy than the two methods separately but minor improvements were observed in terms of top-1 accuracy. As future work, we plan to improve the quality of the extracted dictionary further by exploiting additional translation signals. For example, previous works (Schafer and Yarowsky, 2002; Klementiev et al., 2012) have reported that the temporal and topic similarity are clues that indicate translation equivalence. It would be interesting to investigate the contribution of different clues for various 1709 experimental parameters, e.g., domain, distance of languages, types of comparable corpora. Acknowledgements The authors would like to thank Dr. Danushka Bollegala for providing feedback on this paper and the three anonymous reviewers for their useful comments and suggestions. This work was funded by the European Community’s Seventh Framework Program (FP7/2007-2013) [grant numb"
D14-1177,P07-1084,0,0.154948,".uk/postgrad/georgios.kontonatsios/ Software/LogReg-TermAlign.tar.gz 2 Related Work Context-based methods (Fung and Yee, 1998; Rapp, 1999) adapt the Distributional Hypothesis (Harris, 1954), i.e., words that occur in similar lexical context tend to have the same meaning, in a multilingual environment. They represent the context of each term t as a context vector, usually following the bag-of-words model. Each dimension of the vector corresponds to a context word occurring within a predefined window, while the corresponding value is computed by a correlation metric, e.g., Log-Likelihood Ratio (Morin et al., 2007; Chiao and Zweigenbaum, 2002) or Point-wise Mutual Information (Andrade et al., 2010). A general bilingual dictionary is then used to translate/project the target context vectors into the source language. As a result, the source and target context vectors become directly comparable. In a final step, candidate translations are being ranked according to a distance metric, e.g., cosine similarity (Tamura et al., 2012) or Jaccard index (Zanzotto et al., 2010; Apidianaki et al., 2012). Whilst context-based methods have become a common practise for bilingual dictionary extraction from comparable co"
D14-1177,J03-1002,0,0.0270204,"e the performance of compositional and context-based methods on: (a) similar and unrelated languages, (b) corpora of different degree of comparability and (c) the translation of frequent and rare terms. Finally, we combine the two translation clues, namely string and contextual similarity, in a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constructed more easily than parallel corpora. Freely available, up-to-date, on-line"
D14-1177,J96-1001,0,0.129838,"mbine the two translation clues, namely string and contextual similarity, in a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constructed more easily than parallel corpora. Freely available, up-to-date, on-line resources (e.g., Wikipedia) can be employed. In this paper, we exploit two different sources of information to extract bilingual terminology from comparable corpora: the compositional and the contextual clue. The comp"
D14-1177,D12-1003,0,0.341756,"nsion of the vector corresponds to a context word occurring within a predefined window, while the corresponding value is computed by a correlation metric, e.g., Log-Likelihood Ratio (Morin et al., 2007; Chiao and Zweigenbaum, 2002) or Point-wise Mutual Information (Andrade et al., 2010). A general bilingual dictionary is then used to translate/project the target context vectors into the source language. As a result, the source and target context vectors become directly comparable. In a final step, candidate translations are being ranked according to a distance metric, e.g., cosine similarity (Tamura et al., 2012) or Jaccard index (Zanzotto et al., 2010; Apidianaki et al., 2012). Whilst context-based methods have become a common practise for bilingual dictionary extraction from comparable corpora, nonetheless, their performance is subject to various factors, one of which is the quality of the comparable corpus. Li and Gaussier (2010) introduced the corpus comparability metric and showed that it is related to the performance of context vectors. The higher the corpus comparability is, the higher the performance of context vectors is. Furthermore, context vector approaches are sensitive to the frequency o"
D14-1177,C02-1065,0,0.109739,"Missing"
D14-1177,E93-1015,0,0.519059,"Missing"
D14-1177,C10-1142,1,0.883552,"Missing"
D14-1177,C98-1066,0,\N,Missing
D14-1177,E06-1029,0,\N,Missing
D18-2019,W17-2314,1,0.550244,"Missing"
D18-2019,N07-2028,0,0.0285272,"del to label the instances in the unlabeled set. Every instance now has a score indicating how informative or representative it is. Finally, the system aggregates these scores to get the score of the whole sentence. The system sends the most informative sentences to the annotators, based on the sentences’ scores. When the system receives the annotations, a new iteration round starts. Active learning for sequence labeling can use different query strategies. Most common query strategies are Least Confidence (Culotta and McCallum, 2005), Margin Sampling (Scheffer et al., 2001), Entropy Sampling (Mann and McCallum, 2007), Vote Entropy (Dagan and Engelson, 1995), Kullback Leibler Divergence (Settles and Craven, 2008), Expected Gradient Length (Settles et al., 2008), Information Density (Settles and Craven, 2008) strategies. Among which, no query strategy is completely outperformed other strategies (Settles and Craven, 2008). APLenty curhttps://uima.apache.org/ 110 DELETE SKIP NEXT (ORG) The NBH (ORG) trade data is based on cash flow , MIT data on customs statistics . Figure 2: Annotation interface 4 rently employs the least confidence uncertaintybased strategy for sequence models based on the sequence outputs"
D18-2019,D11-1136,0,0.0370788,"nnotation and uses a vector graphics-based visualization component for rendering. BRAT can, at the same time, display many annotation layers. WebAnno (Yimam et al., 2013) improves the annotation interface of BRAT by letting the annotators choose the annotation layer(s) for rendering. WebAnno offers a purely web-based generic annotation tool and supports distributed annotation. PubAnnotation (Kim and Wang, 2012) also offers a web-based annotation interface but its main focus is to improve the reusability of corpora and annotations. These tools do not support active/proactive learning. DUALIST (Settles, 2011; Settles and Zhu, 2012) and Prodigy2 are most closely related to APLenty. DUALIST is an active learning annotation paradigm that offers annotation interface for semi-supervised active learning. Prodigy is a commercial product which provides an annotation tool powered by active learning. Unfortunately, both DUALIST and Prodigy do not support proactive learning. 3 Result http://hunch.net/˜vw/ https://prodi.gy/ 109 the unlabelled data, on which the annotators will work. Training and testing data is not required, but unlabelled data is mandatory. When there is no training data, the active learnin"
D18-2019,D08-1112,0,0.0678472,"nformative or representative it is. Finally, the system aggregates these scores to get the score of the whole sentence. The system sends the most informative sentences to the annotators, based on the sentences’ scores. When the system receives the annotations, a new iteration round starts. Active learning for sequence labeling can use different query strategies. Most common query strategies are Least Confidence (Culotta and McCallum, 2005), Margin Sampling (Scheffer et al., 2001), Entropy Sampling (Mann and McCallum, 2007), Vote Entropy (Dagan and Engelson, 1995), Kullback Leibler Divergence (Settles and Craven, 2008), Expected Gradient Length (Settles et al., 2008), Information Density (Settles and Craven, 2008) strategies. Among which, no query strategy is completely outperformed other strategies (Settles and Craven, 2008). APLenty curhttps://uima.apache.org/ 110 DELETE SKIP NEXT (ORG) The NBH (ORG) trade data is based on cash flow , MIT data on customs statistics . Figure 2: Annotation interface 4 rently employs the least confidence uncertaintybased strategy for sequence models based on the sequence outputs from a Conditional Random Fields model (Okazaki, 2007). 3.4 Case study One use case that describe"
D18-2019,N12-1066,0,0.353072,"ses a vector graphics-based visualization component for rendering. BRAT can, at the same time, display many annotation layers. WebAnno (Yimam et al., 2013) improves the annotation interface of BRAT by letting the annotators choose the annotation layer(s) for rendering. WebAnno offers a purely web-based generic annotation tool and supports distributed annotation. PubAnnotation (Kim and Wang, 2012) also offers a web-based annotation interface but its main focus is to improve the reusability of corpora and annotations. These tools do not support active/proactive learning. DUALIST (Settles, 2011; Settles and Zhu, 2012) and Prodigy2 are most closely related to APLenty. DUALIST is an active learning annotation paradigm that offers annotation interface for semi-supervised active learning. Prodigy is a commercial product which provides an annotation tool powered by active learning. Unfortunately, both DUALIST and Prodigy do not support proactive learning. 3 Result http://hunch.net/˜vw/ https://prodi.gy/ 109 the unlabelled data, on which the annotators will work. Training and testing data is not required, but unlabelled data is mandatory. When there is no training data, the active learning algorithm will choose"
D18-2019,E12-2021,1,0.918472,"Missing"
D18-2019,W03-0419,0,0.525358,"Missing"
D18-2019,P09-1117,0,0.0758115,"Missing"
D18-2019,P13-4001,0,0.411786,"Missing"
D19-1381,P16-1231,0,0.026668,"tions, our model detects nested events by searching a sequence of actions that construct event structures incrementally in a bottom-up manner. We focus on event detection since the existing methods do not consider nested and overlapping structures as a whole during learning. Treating them simultaneously helps the model avoid inferring wrong causality relations between entities. Our model detects overlapping events by maintaining multiple beams and detecting events from all the beams, in contrast to existing transitionbased methods (Nivre, 2003, 2006; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Vlachos and Craven, 2012). We define an LSTM-based neural network model that can represent nested event structure to choose actions. linear layer score Model We describe our search-based neural network (SBNN) model that constitutes events from a relation graph by structured prediction. SBNN resembles an incremental transition-based parser (Nivre, 2006), but the search order, actions and representations are defined for DAG structures. We first discuss how we generate the relation graph in §2.1, then describe the structured prediction algorithm in §2.2 and the neural network in §2.3 and lastly"
D19-1381,W18-2311,0,0.0443169,"Missing"
D19-1381,D14-1082,0,0.0207605,"n a relation graph of trigger-argument relations, our model detects nested events by searching a sequence of actions that construct event structures incrementally in a bottom-up manner. We focus on event detection since the existing methods do not consider nested and overlapping structures as a whole during learning. Treating them simultaneously helps the model avoid inferring wrong causality relations between entities. Our model detects overlapping events by maintaining multiple beams and detecting events from all the beams, in contrast to existing transitionbased methods (Nivre, 2003, 2006; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Vlachos and Craven, 2012). We define an LSTM-based neural network model that can represent nested event structure to choose actions. linear layer score Model We describe our search-based neural network (SBNN) model that constitutes events from a relation graph by structured prediction. SBNN resembles an incremental transition-based parser (Nivre, 2006), but the search order, actions and representations are defined for DAG structures. We first discuss how we generate the relation graph in §2.1, then describe the structured prediction algorithm in §2.2 an"
D19-1381,P15-1017,0,0.0277166,"3–7, 2019. 2019 Association for Computational Linguistics relations into complete event structures. Joint approaches have also been explored (Rao et al., 2017; Riedel and McCallum, 2011; Vlachos and Craven, 2012; Venugopal et al., 2014), but they focus on finding relation graphs and detect events with rules. McClosky et al. (2011) treats events as dependency structures by constraining event structures to map to trees, thus their method cannot represent overlapping event structures. Other neural models in event extraction are in the general domain (Feng et al., 2016; Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016), but they used the ACE2005 corpus which does not have nested events (Miwa et al., 2014). Furthermore, there are some efforts on applying transition-based methods on DAG structures in dependency parsing, e.g., (Sagae and Tsujii, 2008; Wang et al., 2018), however, they do not consider overlapping and nested structures. E2 We show that our event detection model achieves performance comparable to the event detection module of the state-of-the-art TEES system (Bj¨orne and Salakoski, 2018) on the BioNLP CG Shared Task 2013 (Pyysalo et al., 2013) without the use of any syntacti"
D19-1381,W16-2922,0,0.0137891,"sted events, to compute the precision, we compare the predicted nested events with all gold events and to compute recall, we compare gold nested events with all predicted events. The evaluation script detects nested events by comparing the whole tree structure down to its sub-events until it reaches the flat events. Hence, the performance scores of the nested events inevitably include the performance on flat events. 3.2 Training Details and Model Parameters We implemented our model using the Chainer library (Tokui et al., 2015). We initialised the word embeddings using pre-trained embeddings (Chiu et al., 2016) while other embeddings are initialised using the normal distribution. All the embeddings and weight parameters were updated with mini-batch using the AMSGrad optimiser (Reddi et al., 2018). We also incorporated early stopping to choose the number of training epochs and tuned hyper-parameters (dropout, learning rate and weight decay rate) using grid search. The model parameters can be found in appendix A. 4 Results and Analyses Table 1 shows the event detection performance of the models on the test set. Our model achieves performance comparable to the state-of-the-art TEES event detection modu"
D19-1381,P04-1015,0,0.16143,"word dimensions so that it can be used as argument representation in nested events as shown in Figure 2. Then, we passed the event embedding into a linear hidden layer and output zt . Finally, the scoring function σ is calculated as σ(at |St−1 , Bt−1 ) = sigmoid(zt ). 2.4 Training From the relation graph generated in §2.1, we calculate gold action sequences that construct the gold event structures on the graph. The loss is summed over all actions and for all the events during the beam search and thus the objective function is to minimise their negative log-likelihood. We employ early updates (Collins and Roark, 2004): if the gold falls out of the beam, we stop searching and update the model immediately. 3 Experimental Settings We applied our model to the BioNLP CG shared task 2013 (Pyysalo et al., 2015). We used the original data partition and employed the official evaluation metrics. We focussed on the CG task dataset over other BioNLP datasets because of its complexity and size (N´edellec et al., 2013; Bj¨orne and Salakoski, 2018; Pyysalo et al., 2013). The CG dataset has the most number of entity types and event types and thus is the most complex among the available (and accessible) BioNLP datasets. Fu"
D19-1381,P15-1033,0,0.0204009,"igger-argument relations, our model detects nested events by searching a sequence of actions that construct event structures incrementally in a bottom-up manner. We focus on event detection since the existing methods do not consider nested and overlapping structures as a whole during learning. Treating them simultaneously helps the model avoid inferring wrong causality relations between entities. Our model detects overlapping events by maintaining multiple beams and detecting events from all the beams, in contrast to existing transitionbased methods (Nivre, 2003, 2006; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Vlachos and Craven, 2012). We define an LSTM-based neural network model that can represent nested event structure to choose actions. linear layer score Model We describe our search-based neural network (SBNN) model that constitutes events from a relation graph by structured prediction. SBNN resembles an incremental transition-based parser (Nivre, 2006), but the search order, actions and representations are defined for DAG structures. We first discuss how we generate the relation graph in §2.1, then describe the structured prediction algorithm in §2.2 and the neural networ"
D19-1381,P16-2011,0,0.0224431,"pages 3679–3686, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics relations into complete event structures. Joint approaches have also been explored (Rao et al., 2017; Riedel and McCallum, 2011; Vlachos and Craven, 2012; Venugopal et al., 2014), but they focus on finding relation graphs and detect events with rules. McClosky et al. (2011) treats events as dependency structures by constraining event structures to map to trees, thus their method cannot represent overlapping event structures. Other neural models in event extraction are in the general domain (Feng et al., 2016; Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016), but they used the ACE2005 corpus which does not have nested events (Miwa et al., 2014). Furthermore, there are some efforts on applying transition-based methods on DAG structures in dependency parsing, e.g., (Sagae and Tsujii, 2008; Wang et al., 2018), however, they do not consider overlapping and nested structures. E2 We show that our event detection model achieves performance comparable to the event detection module of the state-of-the-art TEES system (Bj¨orne and Salakoski, 2018) on the BioNLP CG Shared Task 2013 (Pyysalo"
D19-1381,N18-1131,1,0.85565,"Missing"
D19-1381,P11-1163,0,0.535477,"red argument event, a flat event (E1). Introduction Nested and overlapping event structures, which occur widely in text, are important because they can capture relations between events such as causality, e.g., a “production” event is a consequence of a “discovery” event, which in turn is a result of an “exploration” event. Event extraction involves the identification of a trigger and a set of its arguments in a given text. Figure 1 shows an example of a nested and overlapping event structure in the biomedical domain. The relation graph (topmost) forms a directed acyclic graph (DAG) structure (McClosky et al., 2011) and it encapsulates 15 event structures. It contains nested event structures such as E2,E3 because one of their arguments, in this case E1, is an event. Specifically, E1 is a flat event since its argument is an entity. Moreover, E2 and E3 are also overlapping events (explicitly shown in the relation graph having two induction triggers) because they share a common argument, E1. State-of-the-art approaches to event extraction in the biomedical domain are pipeline systems (Bj¨orne and Salakoski, 2018; Miwa et al., 2013) that decompose event extraction into simpler tasks such as: i) trigger/entit"
D19-1381,C14-1214,1,0.823874,"roaches have also been explored (Rao et al., 2017; Riedel and McCallum, 2011; Vlachos and Craven, 2012; Venugopal et al., 2014), but they focus on finding relation graphs and detect events with rules. McClosky et al. (2011) treats events as dependency structures by constraining event structures to map to trees, thus their method cannot represent overlapping event structures. Other neural models in event extraction are in the general domain (Feng et al., 2016; Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016), but they used the ACE2005 corpus which does not have nested events (Miwa et al., 2014). Furthermore, there are some efforts on applying transition-based methods on DAG structures in dependency parsing, e.g., (Sagae and Tsujii, 2008; Wang et al., 2018), however, they do not consider overlapping and nested structures. E2 We show that our event detection model achieves performance comparable to the event detection module of the state-of-the-art TEES system (Bj¨orne and Salakoski, 2018) on the BioNLP CG Shared Task 2013 (Pyysalo et al., 2013) without the use of any syntactic and hand-engineered features. Furthermore, analyses on the development set show our model performs fewer num"
D19-1381,D16-1008,0,0.069396,"Missing"
D19-1381,W13-2001,0,0.0826722,"Missing"
D19-1381,N16-1034,0,0.0161426,"ociation for Computational Linguistics relations into complete event structures. Joint approaches have also been explored (Rao et al., 2017; Riedel and McCallum, 2011; Vlachos and Craven, 2012; Venugopal et al., 2014), but they focus on finding relation graphs and detect events with rules. McClosky et al. (2011) treats events as dependency structures by constraining event structures to map to trees, thus their method cannot represent overlapping event structures. Other neural models in event extraction are in the general domain (Feng et al., 2016; Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016), but they used the ACE2005 corpus which does not have nested events (Miwa et al., 2014). Furthermore, there are some efforts on applying transition-based methods on DAG structures in dependency parsing, e.g., (Sagae and Tsujii, 2008; Wang et al., 2018), however, they do not consider overlapping and nested structures. E2 We show that our event detection model achieves performance comparable to the event detection module of the state-of-the-art TEES system (Bj¨orne and Salakoski, 2018) on the BioNLP CG Shared Task 2013 (Pyysalo et al., 2013) without the use of any syntactic and hand-engineered"
D19-1381,P15-2060,0,0.0238132,"Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics relations into complete event structures. Joint approaches have also been explored (Rao et al., 2017; Riedel and McCallum, 2011; Vlachos and Craven, 2012; Venugopal et al., 2014), but they focus on finding relation graphs and detect events with rules. McClosky et al. (2011) treats events as dependency structures by constraining event structures to map to trees, thus their method cannot represent overlapping event structures. Other neural models in event extraction are in the general domain (Feng et al., 2016; Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016), but they used the ACE2005 corpus which does not have nested events (Miwa et al., 2014). Furthermore, there are some efforts on applying transition-based methods on DAG structures in dependency parsing, e.g., (Sagae and Tsujii, 2008; Wang et al., 2018), however, they do not consider overlapping and nested structures. E2 We show that our event detection model achieves performance comparable to the event detection module of the state-of-the-art TEES system (Bj¨orne and Salakoski, 2018) on the BioNLP CG Shared Task 2013 (Pyysalo et al., 2013) without the"
D19-1381,W03-3017,0,0.168499,"AG structures. Given a relation graph of trigger-argument relations, our model detects nested events by searching a sequence of actions that construct event structures incrementally in a bottom-up manner. We focus on event detection since the existing methods do not consider nested and overlapping structures as a whole during learning. Treating them simultaneously helps the model avoid inferring wrong causality relations between entities. Our model detects overlapping events by maintaining multiple beams and detecting events from all the beams, in contrast to existing transitionbased methods (Nivre, 2003, 2006; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Vlachos and Craven, 2012). We define an LSTM-based neural network model that can represent nested event structure to choose actions. linear layer score Model We describe our search-based neural network (SBNN) model that constitutes events from a relation graph by structured prediction. SBNN resembles an incremental transition-based parser (Nivre, 2006), but the search order, actions and representations are defined for DAG structures. We first discuss how we generate the relation graph in §2.1, then describe the structured p"
D19-1381,W13-2008,1,0.827655,"., 2016; Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016), but they used the ACE2005 corpus which does not have nested events (Miwa et al., 2014). Furthermore, there are some efforts on applying transition-based methods on DAG structures in dependency parsing, e.g., (Sagae and Tsujii, 2008; Wang et al., 2018), however, they do not consider overlapping and nested structures. E2 We show that our event detection model achieves performance comparable to the event detection module of the state-of-the-art TEES system (Bj¨orne and Salakoski, 2018) on the BioNLP CG Shared Task 2013 (Pyysalo et al., 2013) without the use of any syntactic and hand-engineered features. Furthermore, analyses on the development set show our model performs fewer number of classifications in less time. action scoring function (shared) embedding S B E1 score BiLSTM layer event emb layer S B structure and buffer emb layer relation embedding layer Bcl-2 / VEGF induction of BiLSTM layer word emb layer tumor angiogenesis Figure 2: An illustration of the proposed neural model detecting event structures in a bottom-up manner, where E1 event representation becomes an argument to E2 event structure on the example sentence us"
D19-1381,C00-2137,0,0.13641,"Missing"
D19-1381,W17-2315,0,0.137684,"Missing"
D19-1381,W11-1807,0,0.0327068,"which words and phrases in a sentence potentially constitute as participants of an event, ii) relation detection, which finds pairwise relations between triggers and arguments, and iii) event detection, which combines pairwise 3679 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3679–3686, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics relations into complete event structures. Joint approaches have also been explored (Rao et al., 2017; Riedel and McCallum, 2011; Vlachos and Craven, 2012; Venugopal et al., 2014), but they focus on finding relation graphs and detect events with rules. McClosky et al. (2011) treats events as dependency structures by constraining event structures to map to trees, thus their method cannot represent overlapping event structures. Other neural models in event extraction are in the general domain (Feng et al., 2016; Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016), but they used the ACE2005 corpus which does not have nested events (Miwa et al., 2014). Furthermore, there are some efforts on applying transiti"
D19-1381,C08-1095,0,0.0198556,"us on finding relation graphs and detect events with rules. McClosky et al. (2011) treats events as dependency structures by constraining event structures to map to trees, thus their method cannot represent overlapping event structures. Other neural models in event extraction are in the general domain (Feng et al., 2016; Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016), but they used the ACE2005 corpus which does not have nested events (Miwa et al., 2014). Furthermore, there are some efforts on applying transition-based methods on DAG structures in dependency parsing, e.g., (Sagae and Tsujii, 2008; Wang et al., 2018), however, they do not consider overlapping and nested structures. E2 We show that our event detection model achieves performance comparable to the event detection module of the state-of-the-art TEES system (Bj¨orne and Salakoski, 2018) on the BioNLP CG Shared Task 2013 (Pyysalo et al., 2013) without the use of any syntactic and hand-engineered features. Furthermore, analyses on the development set show our model performs fewer number of classifications in less time. action scoring function (shared) embedding S B E1 score BiLSTM layer event emb layer S B structure and buffe"
D19-1381,D14-1090,0,0.0236886,"stitute as participants of an event, ii) relation detection, which finds pairwise relations between triggers and arguments, and iii) event detection, which combines pairwise 3679 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3679–3686, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics relations into complete event structures. Joint approaches have also been explored (Rao et al., 2017; Riedel and McCallum, 2011; Vlachos and Craven, 2012; Venugopal et al., 2014), but they focus on finding relation graphs and detect events with rules. McClosky et al. (2011) treats events as dependency structures by constraining event structures to map to trees, thus their method cannot represent overlapping event structures. Other neural models in event extraction are in the general domain (Feng et al., 2016; Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016), but they used the ACE2005 corpus which does not have nested events (Miwa et al., 2014). Furthermore, there are some efforts on applying transition-based methods on DAG structures in dependency pa"
D19-1498,N19-1370,0,0.110406,"nodes are words and edges represent intra- and inter-sentential relations between the words. They connected words with different dependency edges and trained a binary logistic regression classifier. They evaluated their model on distantly supervised full-text articles from PubMed for Gene-Drug associations, restricting pairs within a window of consecutive sentences. Following this work, other approaches incorporated graphical models for document-level RE such as graph LSTM (Peng et al., 2017), graph CNN (Song et al., 2018) or RNNs on dependency tree structures (Gupta et al., 2019). Recently, Jia et al. (2019) improved n-ary RE using information from multiple sentences and paragraphs in a document. Similar to our approach, they choose to directly classify concept-level pairs rather than multiple mention-level pairs. Although they consider sub-relations to model related tuples, they ignore interactions with other entities outside of the target tuple in the discourse units. Non-graph-based approaches utilise different intra- and inter-sentence models and merge the resulted predictions (Gu et al., 2016, 2017). Other approaches extract document-level representations for each candidate entity pair (Zhen"
D19-1498,C16-1139,0,0.034617,"in a sentence (Zeng et al., 2014; Nguyen and Grishman, 2015) as well as incorporating external syntactic tools (Miwa and Bansal, 4932 2016; Zhang et al., 2018). Christopoulou et al. (2018) considered intra-sentence entity interactions without domain dependencies by modelling long dependencies between the entities of a sentence. Other approaches deal with distantlysupervised datasets but are also limited to intra-sentential relations. They utilise Piecewise Convolutional Neural Networks (PCNN) (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Zhou et al., 2018), entity descriptors (Jiang et al., 2016) and graph CNNs (Vashishth et al., 2018) to perform MIL on bags-of-sentences that contain multiple mentions of an entity pair. Recently, Zeng et al. (2017) proposed a method for extracting paths between entities using the target entities’ mentions in several different sentences (in possibly different documents) as intermediate connectors. They allow mention-mention edges only if these mentions belong to the same entity and consider that a single mention pair exists in a sentence. On the contrary, we not only allow interactions between all mentions in the same sentence, but also consider multip"
D19-1498,P16-1200,0,0.0524628,"vised RE, utilising CNN or RNN, ignoring multiple entities in a sentence (Zeng et al., 2014; Nguyen and Grishman, 2015) as well as incorporating external syntactic tools (Miwa and Bansal, 4932 2016; Zhang et al., 2018). Christopoulou et al. (2018) considered intra-sentence entity interactions without domain dependencies by modelling long dependencies between the entities of a sentence. Other approaches deal with distantlysupervised datasets but are also limited to intra-sentential relations. They utilise Piecewise Convolutional Neural Networks (PCNN) (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Zhou et al., 2018), entity descriptors (Jiang et al., 2016) and graph CNNs (Vashishth et al., 2018) to perform MIL on bags-of-sentences that contain multiple mentions of an entity pair. Recently, Zeng et al. (2017) proposed a method for extracting paths between entities using the target entities’ mentions in several different sentences (in possibly different documents) as intermediate connectors. They allow mention-mention edges only if these mentions belong to the same entity and consider that a single mention pair exists in a sentence. On the contrary, we not only allow interactions betwee"
D19-1498,N19-1308,0,0.0282338,"unusual central bitemporal hemianopic scotoma was found . Figure 1: Example of document-level, inter-sentence relations adapted from the CDR dataset (Li et al., 2016a). The solid and dotted lines represent intra- and inter-sentence relations, respectively. Introduction The extraction of relations between named entities in text, known as Relation Extraction (RE), is an important task of Natural Language Processing (NLP). Lately, RE has attracted a lot of attention from the field, in an effort to improve the inference capability of current methods (Zeng et al., 2017; Christopoulou et al., 2018; Luan et al., 2019). In real-world scenarios, a large amount of relations are expressed across sentences. The task of identifying these relations is named inter-sentence RE. Typically, inter-sentence relations occur in 1 Source code available at https://github.com/ fenchri/edge-oriented-graph textual snippets with several sentences, such as documents. In these snippets, each entity is usually repeated with the same phrases or aliases, the occurrences of which are often named entity mentions and regarded as instances of the entity. The multiple mentions of the target entities in different sentences can be useful"
D19-1498,P09-1113,0,0.0421887,"can help us infer that the entity ethambutol has a relation with the entity scotoma. The most common technique that is currently 4925 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4925–4936, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics used to deal with multiple mentions of named entities is Multi-Instance Learning (MIL). Initially, MIL was introduced by Riedel et al. (2010) in order to reduce noise in distantly supervised corpora (Mintz et al., 2009). In DS, training instances are created from large, raw corpora using Knowledge Base (KB) entity linking and automatic annotation with heuristic rules. MIL in this setting considers multiple sentences (bags) that contain a pair of entities serving as the multiple instances of this pair. Verga et al. (2018) introduced another MIL setting for relation extraction between named entities in a document. In this setting, entities mapped to the same KB ID are considered as mentions of an entity concept and pairs of mentions correspond to the pair’s multiple instances. However, document-level RE is not"
D19-1498,P16-1105,1,0.775874,"Missing"
D19-1498,W18-2314,0,0.421678,"d to the same KB ID are considered as mentions of an entity concept and pairs of mentions correspond to the pair’s multiple instances. However, document-level RE is not common in the general domain, as the entity types of interest can often be found in the same sentence (Banko et al., 2007). On the contrary, in the biomedical domain, document-level relations are particularly important given the numerous aliases that biomedical entities can have (Quirk and Poon, 2017). To deal with document-level RE, recent approaches assume that only two mentions of the target entities reside in the document (Nguyen and Verspoor, 2018; Verga et al., 2018) or utilise different models for intra- and inter-sentence RE (Gu et al., 2016; Li et al., 2016b; Gu et al., 2017). In contrast with approaches that employ sequential models (Nguyen and Verspoor, 2018; Gu et al., 2017; Zhou et al., 2016), graph-based neural approaches have proven useful in encoding longdistance, inter-sentential information (Peng et al., 2017; Quirk and Poon, 2017; Gupta et al., 2019). These models interpret words as nodes and connections between them as edges. They typically perform on the nodes by updating the representations during training. However, a"
D19-1498,W15-1506,0,0.0373678,"s potentially simulate such links, by encoding the co-referring entities into the sentence representation. Finally, incomplete entity linking results into additional model errors. For instance, in the third example, hemorrhage and intracranial bleeding are synonymous terms. However, they are assigned different KB IDs, hence treated as different entities. The model can find the intra-sentential relation but not the inter-sentential one. 6 Related Work Traditional approaches focus on intra-sentence supervised RE, utilising CNN or RNN, ignoring multiple entities in a sentence (Zeng et al., 2014; Nguyen and Grishman, 2015) as well as incorporating external syntactic tools (Miwa and Bansal, 4932 2016; Zhang et al., 2018). Christopoulou et al. (2018) considered intra-sentence entity interactions without domain dependencies by modelling long dependencies between the entities of a sentence. Other approaches deal with distantlysupervised datasets but are also limited to intra-sentential relations. They utilise Piecewise Convolutional Neural Networks (PCNN) (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Zhou et al., 2018), entity descriptors (Jiang et al., 2016) and graph CNNs (Vashishth et al., 2018) t"
D19-1498,Q17-1008,0,0.404919,"ven the numerous aliases that biomedical entities can have (Quirk and Poon, 2017). To deal with document-level RE, recent approaches assume that only two mentions of the target entities reside in the document (Nguyen and Verspoor, 2018; Verga et al., 2018) or utilise different models for intra- and inter-sentence RE (Gu et al., 2016; Li et al., 2016b; Gu et al., 2017). In contrast with approaches that employ sequential models (Nguyen and Verspoor, 2018; Gu et al., 2017; Zhou et al., 2016), graph-based neural approaches have proven useful in encoding longdistance, inter-sentential information (Peng et al., 2017; Quirk and Poon, 2017; Gupta et al., 2019). These models interpret words as nodes and connections between them as edges. They typically perform on the nodes by updating the representations during training. However, a relation between two entities depends on different contexts. It could thus be better expressed with an edge connection that is unique for the pair. A straightforward way to address this is to create graph-based models that rely on edge representations rather focusing on node representations, which are shared between multiple entity pairs. In this work, we tackle document-level, i"
D19-1498,D14-1162,0,0.09477,"(a) Overall 40 20 EoG (SS) EoG (SS direct) 0 2 4 8 16 32 Number of inference steps (c) Inter-sentential Figure 3: Performance as a function of the number of inference steps when using direct (SSdirect ) or direct and indirect (SS) sentence-to-sentence edges, on the CDR development set. 5 Overall 55.14 63.57 62.77 61.57 62.92 61.41 59.70 57.41 60.46 56.86 Analysis & Discussion We first analyse the performance of our main model (EoG) using different pre-trained word embeddings. Table 3 shows the performance difference between domain-specific (PubMed) (Chiu et al., 2016), general-domain (GloVe) (Pennington et al., 2014) and randomly initialized (random) word embeddings. As observed, our proposed model performs consistently with both in-domain and out-of-domain pre-trained word embeddings. The low performance of random embeddings is due to the small size of the dataset, which results in lower quality embeddings. For further analysis, we choose the CDR dataset as it is manually annotated. To better analyse the behaviour of our model, we conduct analysis on the effect of direct and indirect sentence-tosentence edges as a function of the inference steps. Figures 3a, 3b and 3c illustrate the performance of both g"
D19-1498,E17-1110,0,0.199859,"pair. Verga et al. (2018) introduced another MIL setting for relation extraction between named entities in a document. In this setting, entities mapped to the same KB ID are considered as mentions of an entity concept and pairs of mentions correspond to the pair’s multiple instances. However, document-level RE is not common in the general domain, as the entity types of interest can often be found in the same sentence (Banko et al., 2007). On the contrary, in the biomedical domain, document-level relations are particularly important given the numerous aliases that biomedical entities can have (Quirk and Poon, 2017). To deal with document-level RE, recent approaches assume that only two mentions of the target entities reside in the document (Nguyen and Verspoor, 2018; Verga et al., 2018) or utilise different models for intra- and inter-sentence RE (Gu et al., 2016; Li et al., 2016b; Gu et al., 2017). In contrast with approaches that employ sequential models (Nguyen and Verspoor, 2018; Gu et al., 2017; Zhou et al., 2016), graph-based neural approaches have proven useful in encoding longdistance, inter-sentential information (Peng et al., 2017; Quirk and Poon, 2017; Gupta et al., 2019). These models interp"
D19-1498,N19-1147,0,0.0439499,"s. Non-graph-based approaches utilise different intra- and inter-sentence models and merge the resulted predictions (Gu et al., 2016, 2017). Other approaches extract document-level representations for each candidate entity pair (Zheng et al., 2018; Li et al., 2018; Wu et al., 2019), or use syntactic dependency structures (Zhou et al., 2016; Peng et al., 2016). Verga et al. (2018) proposed a Transformer-based model for documentlevel relation extraction with multi-instance learning, merging multiple mention pairs. Nguyen and Verspoor (2018) used a CNN with additional character-level embeddings. Singh and Bhatia (2019) also utilised Transformer and connected two target entities by combining them directly and via a contextual token. However, they consider a single target entity pair per document. 7 Conclusion We presented a novel edge-oriented graph neural model for document-level relation extraction using multi-instance learning. The proposed model constructs a document-level graph with heterogeneous types of nodes and edges, modelling intraand inter-sentence pairs simultaneously with an iterative algorithm over the graph edges. To the best of our knowledge, this is the first approach to utilise an edge-ori"
D19-1498,D18-1246,0,0.136044,"ostly graph-based. Quirk and Poon (2017) introduced the notion of a document graph, where nodes are words and edges represent intra- and inter-sentential relations between the words. They connected words with different dependency edges and trained a binary logistic regression classifier. They evaluated their model on distantly supervised full-text articles from PubMed for Gene-Drug associations, restricting pairs within a window of consecutive sentences. Following this work, other approaches incorporated graphical models for document-level RE such as graph LSTM (Peng et al., 2017), graph CNN (Song et al., 2018) or RNNs on dependency tree structures (Gupta et al., 2019). Recently, Jia et al. (2019) improved n-ary RE using information from multiple sentences and paragraphs in a document. Similar to our approach, they choose to directly classify concept-level pairs rather than multiple mention-level pairs. Although they consider sub-relations to model related tuples, they ignore interactions with other entities outside of the target tuple in the discourse units. Non-graph-based approaches utilise different intra- and inter-sentence models and merge the resulted predictions (Gu et al., 2016, 2017). Othe"
D19-1498,D18-1157,0,0.0353429,"Missing"
D19-1498,N18-1080,0,0.480983,"4936, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics used to deal with multiple mentions of named entities is Multi-Instance Learning (MIL). Initially, MIL was introduced by Riedel et al. (2010) in order to reduce noise in distantly supervised corpora (Mintz et al., 2009). In DS, training instances are created from large, raw corpora using Knowledge Base (KB) entity linking and automatic annotation with heuristic rules. MIL in this setting considers multiple sentences (bags) that contain a pair of entities serving as the multiple instances of this pair. Verga et al. (2018) introduced another MIL setting for relation extraction between named entities in a document. In this setting, entities mapped to the same KB ID are considered as mentions of an entity concept and pairs of mentions correspond to the pair’s multiple instances. However, document-level RE is not common in the general domain, as the entity types of interest can often be found in the same sentence (Banko et al., 2007). On the contrary, in the biomedical domain, document-level relations are particularly important given the numerous aliases that biomedical entities can have (Quirk and Poon, 2017). To"
D19-1498,P16-1123,0,0.279624,"Missing"
D19-1498,D15-1206,0,0.0398492,"s the length of the edge and eik corresponds to the representation of the edge between nodes i and k. During the second step, we aggregate the original (short) edge representation and the new (longer) edge representation resulted from Equation (3) with linear interpolation as follows: (2l) (l) eij = β eij + (1 − β) X   (l) (l) f eik , ekj , (4) k6=i,j where β ∈ [0, 1] is a scalar that controls the contribution of the shorter edge presentation. In general β is larger for shorter edges as we expect that the relation between two nodes is better expressed through the shortest path between them (Xu et al., 2015; Borgwardt and Kriegel, 2005). The two steps are repeated a finite number of times N . The number of iterations is correlated with the final length of the edge representations. With initial edge length l equal to 1, the first iteration results in edges of length up-to 2. The second iteration results in edges of length up-to 4. Similarly, after N iterations, the length of edges will be up-to 2N . 2.5 Classification Layer To classify the concept-level entity pairs of interest, we incorporate a softmax classifier, using the entity-to-entity edges (EE) of the document graph that correspond to the"
D19-1498,D15-1203,0,0.138169,"l approaches focus on intra-sentence supervised RE, utilising CNN or RNN, ignoring multiple entities in a sentence (Zeng et al., 2014; Nguyen and Grishman, 2015) as well as incorporating external syntactic tools (Miwa and Bansal, 4932 2016; Zhang et al., 2018). Christopoulou et al. (2018) considered intra-sentence entity interactions without domain dependencies by modelling long dependencies between the entities of a sentence. Other approaches deal with distantlysupervised datasets but are also limited to intra-sentential relations. They utilise Piecewise Convolutional Neural Networks (PCNN) (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Zhou et al., 2018), entity descriptors (Jiang et al., 2016) and graph CNNs (Vashishth et al., 2018) to perform MIL on bags-of-sentences that contain multiple mentions of an entity pair. Recently, Zeng et al. (2017) proposed a method for extracting paths between entities using the target entities’ mentions in several different sentences (in possibly different documents) as intermediate connectors. They allow mention-mention edges only if these mentions belong to the same entity and consider that a single mention pair exists in a sentence. On the contrar"
D19-1498,C14-1220,0,0.272301,"these edges, S nodes potentially simulate such links, by encoding the co-referring entities into the sentence representation. Finally, incomplete entity linking results into additional model errors. For instance, in the third example, hemorrhage and intracranial bleeding are synonymous terms. However, they are assigned different KB IDs, hence treated as different entities. The model can find the intra-sentential relation but not the inter-sentential one. 6 Related Work Traditional approaches focus on intra-sentence supervised RE, utilising CNN or RNN, ignoring multiple entities in a sentence (Zeng et al., 2014; Nguyen and Grishman, 2015) as well as incorporating external syntactic tools (Miwa and Bansal, 4932 2016; Zhang et al., 2018). Christopoulou et al. (2018) considered intra-sentence entity interactions without domain dependencies by modelling long dependencies between the entities of a sentence. Other approaches deal with distantlysupervised datasets but are also limited to intra-sentential relations. They utilise Piecewise Convolutional Neural Networks (PCNN) (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Zhou et al., 2018), entity descriptors (Jiang et al., 2016) and graph CNN"
D19-1498,D17-1186,0,0.127266,"d . A bilateral retrobulbar neuropathy with an unusual central bitemporal hemianopic scotoma was found . Figure 1: Example of document-level, inter-sentence relations adapted from the CDR dataset (Li et al., 2016a). The solid and dotted lines represent intra- and inter-sentence relations, respectively. Introduction The extraction of relations between named entities in text, known as Relation Extraction (RE), is an important task of Natural Language Processing (NLP). Lately, RE has attracted a lot of attention from the field, in an effort to improve the inference capability of current methods (Zeng et al., 2017; Christopoulou et al., 2018; Luan et al., 2019). In real-world scenarios, a large amount of relations are expressed across sentences. The task of identifying these relations is named inter-sentence RE. Typically, inter-sentence relations occur in 1 Source code available at https://github.com/ fenchri/edge-oriented-graph textual snippets with several sentences, such as documents. In these snippets, each entity is usually repeated with the same phrases or aliases, the occurrences of which are often named entity mentions and regarded as instances of the entity. The multiple mentions of the targe"
D19-1498,D18-1244,0,0.127672,"Finally, incomplete entity linking results into additional model errors. For instance, in the third example, hemorrhage and intracranial bleeding are synonymous terms. However, they are assigned different KB IDs, hence treated as different entities. The model can find the intra-sentential relation but not the inter-sentential one. 6 Related Work Traditional approaches focus on intra-sentence supervised RE, utilising CNN or RNN, ignoring multiple entities in a sentence (Zeng et al., 2014; Nguyen and Grishman, 2015) as well as incorporating external syntactic tools (Miwa and Bansal, 4932 2016; Zhang et al., 2018). Christopoulou et al. (2018) considered intra-sentence entity interactions without domain dependencies by modelling long dependencies between the entities of a sentence. Other approaches deal with distantlysupervised datasets but are also limited to intra-sentential relations. They utilise Piecewise Convolutional Neural Networks (PCNN) (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Zhou et al., 2018), entity descriptors (Jiang et al., 2016) and graph CNNs (Vashishth et al., 2018) to perform MIL on bags-of-sentences that contain multiple mentions of an entity pair. Recently, Zeng"
D19-5727,N19-1423,0,0.156661,"many cases, a mention and its antecedent are far away, e.g., a mention can occur in the result section of a paper while its antecedent is in the abstract section. To address these problems, we enhance the baseline system in two ways; we propose to filter noisy spans by using syntactic information and increase the number of antecedent candidates to capture such long-distance coreferent pairs. We further boost the system by replacing the underlying Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) layer with the Bidirectional Encoder Representations from Transformer (BERT) model (Devlin et al., 2019)—a contextualized language model that can efficiently capture context in a wide range of NLP tasks. We have evaluated our system on six common metrics for coreference resolution including B3 , BLANC, CEAFE, CEAFM, LEA, and MUC using the official evaluation script provided by the shared task organizers. By increasing the numThis paper describes our system developed for the coreference resolution task of the CRAFT Shared Tasks 2019. The CRAFT corpus is more challenging than other existing corpora because it contains full text articles. We have employed an existing span-based state-of-theart neur"
D19-5727,D17-1018,0,0.107226,"n2 , Makoto Miwa1,3 , Hiroya Takamura1 , Sophia Ananiadou2 1 Artificial Intelligence Research Center (AIRC), National Institute of Advanced Industrial Science and Technology (AIST), Japan 2 National Centre for Text Mining, University of Manchester, United Kingdom 3 Toyota Technological Institute, Japan {long.trieu, khoa.duong, takamura.hiroya}@aist.go.jp, makoto-miwa@toyota-ti.ac.jp, {nhung.nguyen, Sophia.Ananiadou}@manchester.ac.uk Abstract present our approach to address the coreference resolution task in this challenging corpus. We employ the state-of-the-art end-to-end coreference system (Lee et al., 2017) as our baseline. The system generates all continuous sequences of words (or spans) in each sentence as mention candidates, which means the number of candidates increases linearly to the number of sentences. Such candidates may contain a large number of noisy spans, which are spans in a sentence that do not fit any noun phrases according to the corresponding parse tree. Such noisy spans are often wasteful when being included in the list of candidates for the coreference resolution step. Especially for the CRAFT corpus, of which the average number of sentences is more than 300, the number of no"
D19-5727,W11-1811,0,0.0703575,"Missing"
D19-5727,W18-2324,1,0.892144,"Missing"
E03-1077,nenadic-etal-2002-automatic,1,0.905473,"ed tags, including POS, syntactic and domains-specific (i.e. semantic, e.g. protein, DNA, etc.) tags. Usually, a tagging scheme includes additional structural complexities such as nesting and possible combinations of syntactic and semantic structures (e.g. a noun phrase which contains a DNA name), which may cause difficulties during document processing. Multi-layered and interlaced annotations have been addressed by several systems, usually by following the TIPSTER architecture (Grishman, 1995), i.e. by manipulating tags via an external relational database (RDB). For example, the TIMS system (Nenadic et al., 2002) addresses terminology-driven literature mining via a RDB, which stores XML-tag information separately from the original documents. The main reasons behind this choice are easy import and integration of different tags for the same document and efficient manipulation of these tags. However, in this paper we will discuss possible advantages of using an XML-native database (DB) to facilitate corpus-mining. The main reasons for this are portability and self-description of XML documents and natural association between them and XML-native databases (see Section 6 for comparison between XML-native DB"
E03-1077,X96-1043,0,\N,Missing
E09-1090,W06-2920,0,0.0543335,"Missing"
E09-1090,P05-1022,0,0.749547,"obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 790–798, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 790 NP VBN VP QP NN VBD DT JJ CD CD NNS . NP VBD NP . ounces . Estimated volume was a light 2.4 million ounces . volume was Figure 1: Chunking, the first (base) level. Figure 3: Chunking, the 3rd level. NP S NP VBD DT JJ volume was a light QP million NNS . ounces . Figure 2: Chunking, the 2nd level. NP VP . volume was . Figure 4: Chunking, the 4th level. chain CRF model to perform chunking. Although our pa"
E09-1090,A00-2018,0,0.0635557,"Missing"
E09-1090,C04-1041,0,0.0107461,"of history-based approaches, it is one step closer to the whole-sentence approaches because the parser uses a whole-sequence model (i.e. CRFs) for individual chunking tasks. In other words, our parser could be located somewhere between traditional history-based approaches and whole-sentence approaches. One of our motivations for this work was that our parsing model may achieve a better balance between accuracy and speed than existing parsers. It is also worth mentioning that our approach is similar in spirit to supertagging for parsing with lexicalized grammar formalisms such as CCG and HPSG (Clark and Curran, 2004; Ninomiya et al., 2006), in which significant speed-ups for parsing time are achieved. In this paper, we show that our approach is indeed appealing in that the parser runs very fast and gives competitive accuracy. We evaluate our parser on the standard data set for parsing experiments (i.e. the Penn Treebank) and compare it with existing approaches to full parsing. This paper is organized as follows. Section 2 presents the overall chunk parsing strategy. Section 3 describes the CRF model used to perform individual chunking steps. Section 4 describes the depth-first algorithm for finding the b"
E09-1090,P07-1104,0,0.0328599,"a sentence. We examined the distribution of the heights of the trees in sections 2-21 of the Wall Street Journal (WSJ) corpus. The result is shown in Figure 5. Most of the sentences have less than 20 levels. The average was 10.0, which means we need to perform, on average, 10 chunking tasks to obtain a full parse tree for a sentence if the parsing is performed in a deterministic manner. 3 T X K X where R(λ) is introduced for the purpose of regularization which prevents the model from overfitting the training data. The L1 or L2 norm is commonly used in statistical natural language processing (Gao et al., 2007). We used L1-regularization, which is defined as R(λ) = Chunking with CRFs K 1 X |λk |, C k=1 where C is the meta-parameter that controls the degree of regularization. We used the OWL-QN algorithm (Andrew and Gao, 2007) to obtain the parameters that maximize the L1-regularized conditional log-likelihood. The accuracy of chunk parsing is highly dependent on the accuracy of each level of chunking. This section describes our approach to the chunking task. A common approach to the chunking problem is to convert the problem into a sequence tagging task by using the “BIO” (B for beginning, I for ins"
E09-1090,P08-1067,0,0.269083,"ms. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Petrov and Klein (2008) introduced latent variables in tree CRFs and proposed a caching mechanism to speed up the computation. In general, the latter whole-sentence approaches give better accuracy than history-based approaches because they can better trade off decisions made in different parts in a parse tree. However, the whole-sentence approaches tend to require a large computational cost both in training and parsi"
E09-1090,P95-1037,0,0.0328373,"recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1 The head word is identified by using the headpercolation table (Magerman, 1995). 791 # sentences 5000 3.1 Linear Chain CRFs 4000 A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: 3000 K T X X 1 λk fk (t, yt , yt−1 , x), exp p(y|x) = Z(x) t=1 k=1 2000 1000 where fk (t, yt , yt−1 , x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: 0 0 5 10 15 Height 20 25 30 Figure 5: Distribution of tree height in WSJ sections 2-21. Z(x) = X exp y a maximum entropy classifier and achieved an fscore of 85.9. However"
E09-1090,N06-1020,0,0.0327579,"porate some useful restrictions in producing chunking hypotheses. For example, we could naturally incorporate the restriction that every chunk has to contain at least one symbol that has just been created in the previous level3 . It is hard for the normal CRF model to incorporate such restrictions. Introducing latent variables into the CRF model may be another promising approach. This is the main idea of Petrov and Klein (2008), which significantly improved parsing accuracy. A totally different approach to improving the accuracy of our parser is to use the idea of “selftraining” described in (McClosky et al., 2006). The basic idea is to create a larger set of training data by applying an accurate parser (e.g. reranking parser) to a large amount of raw text. We can then use the automatically created treebank as the additional training data for our parser. This approach suggests that accurate (but slow) parsers and fast (but not-so-accurate) parsers can actually help each other. Also, since it is not difficult to extend our parser to produce N-best parsing hypotheses, one could build a fast reranking parser by using the parser as the base (hypotheses generating) parser. from the history-based model’s inab"
E09-1090,P08-1006,1,0.693944,"computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their s"
E09-1090,W06-1619,1,0.941698,"University of Manchester, UK ‡ National Centre for Text Mining (NaCTeM), UK ∗ Department of Computer Science, University of Tokyo, Japan {yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk Abstract and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Pet"
E09-1090,W97-0301,0,0.205987,"Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae & Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka & Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 8 Conclusion 7 Discussion Although the idea of treating full parsing as a series of chunking problems has a long history, there has not been a competitive parser based on this parsing framework. In this paper, we hav"
E09-1090,P06-2089,0,0.308636,"mputer Science, University of Manchester, UK ‡ National Centre for Text Mining (NaCTeM), UK ∗ Department of Computer Science, University of Tokyo, Japan {yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk Abstract and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Pet"
E09-1090,N03-1028,0,0.504054,"is highly dependent on the accuracy of each level of chunking. This section describes our approach to the chunking task. A common approach to the chunking problem is to convert the problem into a sequence tagging task by using the “BIO” (B for beginning, I for inside, and O for outside) representation. For example, the chunking process given in Figure 1 is expressed as the following BIO sequences. 3.2 Features Table 1 shows the features used in chunking for the base level. Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). We use unigrams, bigrams, and trigrams of part-of-speech (POS) tags and words. The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states. We B-NP I-NP O O O B-QP I-QP O O This representation enables us to use the linearchain CRF model to perform chunking, since the task is simply assigning appropriate labels to a sequence. 792 Symbol Unigrams Symbol Bigrams Symbol Trigrams Word Unigrams Word Bigrams Word Trigrams s−2 , s−1 , s0 , s+1 , s+2 s−2 s−1 , s−1 s0 , s0 s+1 , s"
E09-1090,W05-1514,1,0.908906,", and converts it into NP. This process is repeated until the whole sentence is chunked at the fourth level. The full parse tree is recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1 The head word is identified by using the headpercolation table (Magerman, 1995). 791 # sentences 5000 3.1 Linear Chain CRFs 4000 A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: 3000 K T X X 1 λk fk (t, yt , yt−1 , x), exp p(y|x) = Z(x) t=1 k=1 2000 1000 where fk (t, yt , yt−1 , x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: 0 0 5 10 15 Height 20 25 30 Figure 5: Distribution"
E09-1090,P06-1006,0,0.0111322,"king. The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discrimina"
E09-1090,E99-1016,0,\N,Missing
E09-1090,J03-4003,0,\N,Missing
E09-1090,P08-1109,0,\N,Missing
E09-2016,francopoulo-etal-2006-lexical,0,0.0814867,"Missing"
E09-2016,W04-3230,0,0.0179538,"multi-word expressions are used as phrase queries. Passages are ranked with Okapi BM25 (Robertson et al., 1995). Table 1 shows the preliminary Mean Average Precision (MAP) scores of applying the BLTagger to the TREC data set. By adding biology multi-word expressions identified by the BLTagger to query terms (row (a)), we were able to obtain a slightly better Passage2 score. As the BLTagger outputs semantic IDs which are defined in the BioLexicon, we tried to use these semantic IDs for query expansion (rows (b) and (d)). However, the MAP scores degraded. is selected as the best path. Following Kudo et al. (2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab2, to our POS tagging task. The features used were: • • • • • • POS BIOMED POS-BIOMED bigram of adjacent POS bigram of adjacent BIOMED bigram of adjacent POS-BIOMED During the construction of the trellis, white space is considered as the delimiter unless otherwise stated within dictionary entries. This means that unknown tokens are character sequences without spaces. As the BioLexicon associates biomedical semantic IDs with terms, the BLTagger attaches semantic IDs to the tokenizing/tagging results. 4. Application 2: En"
E09-2016,quochi-etal-2008-lexicon,0,0.0376864,"Missing"
E12-2021,W05-0620,0,0.0614673,"Missing"
E12-2021,doddington-etal-2004-automatic,0,0.0628567,"an be set up to support most text annotation tasks. The most basic annotation primitive identifies a text span and assigns it a type (or tag or label), marking for e.g. POS-tagged tokens, chunks or entity mentions (Figure 1 top). These base annotations can be connected by binary relations – either directed or undirected – which can be configured for e.g. simple relation extraction, or verb frame annotation BRAT (Figure 1 middle and bottom). n-ary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). F"
E12-2021,J02-3001,0,0.0249721,", allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). Further, both the BRAT client and server implement full support for the Unicode standard, which allow the tool to support the annotation of text using e.g. Chinese or Devan¯agar¯ı characters. BRAT is distributed with examples from over 20 corpora for a variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the CoNLL shared tasks on language-independent named entity recognition (Tjong Kim Sang and De Meulder, 2003) and mult"
E12-2021,W11-1801,1,0.354037,"Missing"
E12-2021,W03-3017,0,0.0113928,"dheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). Further, both the BRAT client and server implement full support for the Unicode standard, which allow the tool to support the annotation of text using e.g. Chinese or Devan¯agar¯ı characters. BRAT is distributed with examples from over 20 corpora for a variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the CoNLL shared tasks on language-independent named entity recognition (Tjong Kim Sang and De Meulder, 2003) and multilingual dependency parsing (Buchholz and Marsi, 2006). BRAT also imple"
E12-2021,N06-4006,0,0.00738235,"the semantic class disambiguation component (Stenetorp et al., 2011a). Although further research is needed to establish the benefits of this approach in various annotation tasks, we view the results of this initial experiment as promising regarding the potential of our approach to using machine learning to support annotation efforts. 5 informed by experience from several annotation tasks and research efforts spanning more than a decade. A variety of previously introduced annotation tools and approaches also served to guide our design decisions, including the fast annotation mode of Knowtator (Ogren, 2006), the search capabilities of the XConc tool (Kim et al., 2008), and the design of web-based systems such as MyMiner (Salgado et al., 2010), and GATE Teamware (Cunningham et al., 2011). Using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation (Tsuruoka et al., 2008) and translation (Mart´ınezG´omez et al., 2011), efforts which served as inspiration for our own approach. BRAT , along with conversion tools and extensive documentation, is freely available under the open-source MIT license from its homepa"
E12-2021,W11-1816,1,0.207231,"Missing"
E12-2021,X96-1048,0,0.137156,"lly configurable and can be set up to support most text annotation tasks. The most basic annotation primitive identifies a text span and assigns it a type (or tag or label), marking for e.g. POS-tagged tokens, chunks or entity mentions (Figure 1 top). These base annotations can be connected by binary relations – either directed or undirected – which can be configured for e.g. simple relation extraction, or verb frame annotation BRAT (Figure 1 middle and bottom). n-ary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003)"
E12-2021,W08-0605,1,0.44524,"experience from several annotation tasks and research efforts spanning more than a decade. A variety of previously introduced annotation tools and approaches also served to guide our design decisions, including the fast annotation mode of Knowtator (Ogren, 2006), the search capabilities of the XConc tool (Kim et al., 2008), and the design of web-based systems such as MyMiner (Salgado et al., 2010), and GATE Teamware (Cunningham et al., 2011). Using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation (Tsuruoka et al., 2008) and translation (Mart´ınezG´omez et al., 2011), efforts which served as inspiration for our own approach. BRAT , along with conversion tools and extensive documentation, is freely available under the open-source MIT license from its homepage at http://brat.nlplab.org Acknowledgements The authors would like to thank early adopters of BRAT who have provided us with extensive feedback and feature suggestions. This work was supported by Grant-in-Aid for Specially Promoted Research (MEXT, Japan), the UK Biotechnology and Biological Sciences Research Council (BBSRC) under project Automated Biologic"
E12-2021,W06-2920,0,\N,Missing
E12-2021,W03-0419,0,\N,Missing
E12-2021,P05-1013,0,\N,Missing
E14-4022,C10-1003,1,0.850791,"extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word window) and (b) are listed in the seed bili"
E14-4022,P07-1084,0,0.555352,"w that dictionaries extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word window) and (b) are l"
E14-4022,J03-1002,0,0.00526689,"ora. We evaluate the RF classifier against a popular term alignment method, namely context vectors, and we report an improvement of the translation accuracy. As an application, we use the automatically extracted dictionary in combination with a trained Statistical Machine Translation (SMT) system to more accurately translate unknown terms. The dictionary extraction method described in this paper is freely available 1 . 1 Background Bilingual dictionaries of technical terms are important resources for many Natural Language Processing (NLP) tasks including Statistical Machine Translation (SMT) (Och and Ney, 2003) and Cross-Language Information Retrieval (Ballesteros and Croft, 1997). However, manually creating and updating such resources is an expensive process. In addition to this, new terms are constantly emerging. Especially in the biomedical domain, which is the focus of this work, there is a vast number of neologisms, i.e., newly coined terms, (Pustejovsky et al., 2001). Early work on bilingual lexicon extraction focused on clean, parallel corpora providing satisfactory results (Melamed, 1997; Kay and R¨oscheisen, 1993). However, parallel corpora are expensive to construct and for some domains an"
E14-4022,P02-1040,0,0.0886851,"serve in Table 3, a strong language model can more accurately select the correct translation among top-k candidates. The dictionary extracted by the RF improved the translation performance by 2.5 BLEU points for the top-10 candidates and context vectors by 0.45 for the top-20 candidates. Results We evaluated the translation performance of the SMT that uses the dictionary extracted by the RF against the following baselines: (i) Moses using only the training parallel data (Moses), (ii) Moses using the dictionary extracted by context vectors (Moses+context vector). The evaluation metric is BLEU (Papineni et al., 2002). Table 2 shows the BLEU score achieved by the SMT systems when we append the top-k translations to the phrase table. Moses Moses+ RF Moses+ Context Vectors BLEU on top-k translations 1 10 24.22 24.22 20 24.22 25.32 24.626 24.42 23.88 23.69 23.74 Moses Moses+ RF Moses+ Context Vectors BLEU on top-k translations 1 10 28.85 28.85 20 28.85 30.98 31.35 31.2 28.18 29.17 29.3 Table 3: Translation performance when adding top-k translations to the phrase table. SMT systems use a language model trained on training and test Spanish sentences of the parallel corpus. Table 2: Translation performance when"
E14-4022,C02-2020,0,0.65547,"cancer”. Furthermore, we show that dictionaries extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word w"
E14-4022,P98-1069,0,0.750459,"arable corpus of Wikipedia articles that are related to the medical sub-domain of “breast cancer”. Furthermore, we show that dictionaries extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i"
E14-4022,P99-1067,0,0.331174,"ipedia articles that are related to the medical sub-domain of “breast cancer”. Furthermore, we show that dictionaries extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect"
E14-4022,P07-2045,0,0.00791894,"nglish terms for which we are extracting translations and ranki is the position of the first correct translation from returned list of candidates 5 nlm.nih.gov/research/umls 6 http://en.wikipedia.org/wiki/Help:Searching 4 7 each frequency range contains 100 randomly sampled terms 113 ing the SMT and 1K sentences for evaluation. The test sentences contain 1, 200 terms that do not appear in the training parallel corpus. These terms occur in the Wikipedia comparable corpus. Hence, the previously extracted dictionaries list a possible translation. Using the PubMed parallel corpus, we train Moses (Koehn et al., 2007), a phrase-based SMT system. 4.2 To further investigate the effect of the language model on the translation performance of the augmented SMT systems, we conducted an oracle experiment. In this ideal setting, we assume a strong language model, that is trained on both training and test Spanish sentences of the parallel corpus, in order to assign a higher probability to a correct translation if it exists in the deployed dictionary. As we observe in Table 3, a strong language model can more accurately select the correct translation among top-k candidates. The dictionary extracted by the RF improve"
E14-4022,W13-2512,1,0.78176,"Missing"
E14-4022,D12-1003,0,0.125027,"omly select only one. Negative instances 3 Baseline method Experiments In this section, we evaluate the two dictionary extraction methods, namely context vectors and RF, on a comparable corpus of Wikipedia articles. For the evaluation metric, we use the top-k translation accuracy 3 and the mean reciprocal 2 http://members.unine.ch/jacques.savoy/clef/index.html the percentage of English terms whose top k candidates contain a correct translation 3 112 rank (MRR) 4 as in previous approaches (Chiao and Zweigenbaum, 2002; Chiao and Zweigenbaum, 2002; Morin and Prochasson, 2011; Morin et al., 2007; Tamura et al., 2012). As a reference list, we use the UMLS metathesaurus5 . In addition to this, considering that in several cases the dictionary extraction methods retrieved synonymous translations that do not appear in the reference list, we manually inspected the answers. Finally, unlike previous approaches (Chiao and Zweigenbaum, 2002), we do not restrict the test list only to those English terms whose Spanish translations are known to occur in the target corpus. In such cases, the performance of dictionary extraction methods have been shown to achieve a lower performance (Tamura et al., 2012). 3.1 curacy and"
E14-4022,P97-1039,0,0.0524334,"es for many Natural Language Processing (NLP) tasks including Statistical Machine Translation (SMT) (Och and Ney, 2003) and Cross-Language Information Retrieval (Ballesteros and Croft, 1997). However, manually creating and updating such resources is an expensive process. In addition to this, new terms are constantly emerging. Especially in the biomedical domain, which is the focus of this work, there is a vast number of neologisms, i.e., newly coined terms, (Pustejovsky et al., 2001). Early work on bilingual lexicon extraction focused on clean, parallel corpora providing satisfactory results (Melamed, 1997; Kay and R¨oscheisen, 1993). However, parallel corpora are expensive to construct and for some domains and language pairs are scarce resources. For these reasons, the focus has shifted to comparable corpora 1 http://personalpages.manchester. ac.uk/postgrad/georgios.kontonatsios/ Software/RF-TermAlign.tar.gz 111 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 111–116, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics English-Spanish comparable corpus of Wikipedia articles that are related to"
E14-4022,C08-1125,0,0.0607731,"Missing"
E14-4022,W11-1205,0,\N,Missing
E14-4022,C98-1066,0,\N,Missing
E14-4022,J93-1006,0,\N,Missing
E17-1093,I08-2085,0,0.0249469,"er of the Association for Computational Linguistics: Volume 1, Long Papers, pages 991–1001, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics followed the description-comes-first (DCF) paradigm (Osi´nski et al., 2004; Weiss, 2006; Zhang, 2009). DCF-based methods work by firstly identifying a set of cluster labels, and subsequently forming document clusters by measuring the relevance of each document to a potential cluster label. DCF-based approaches have several shortcomings, which include poor clustering performance and low readability of cluster descriptors (Lee et al., 2008; Carpineto et al., 2009). More recent developments in descriptive clustering have proposed alternative techniques, which approach the problems of improving clustering performance and descriptive label quality from various different angles. For instance, Scaiella et al. (2012) identifies Wikipedia concepts in documents and then computes relatedness between documents according to the linked structure of Wikipedia. Navigli and Crisafulli (2010) propose a method that takes into account synonymy and polysemy. Their method utilises the Google Web1T corpus to identify word senses based on word co-oc"
E17-1093,D10-1012,0,0.0273314,"nt to a potential cluster label. DCF-based approaches have several shortcomings, which include poor clustering performance and low readability of cluster descriptors (Lee et al., 2008; Carpineto et al., 2009). More recent developments in descriptive clustering have proposed alternative techniques, which approach the problems of improving clustering performance and descriptive label quality from various different angles. For instance, Scaiella et al. (2012) identifies Wikipedia concepts in documents and then computes relatedness between documents according to the linked structure of Wikipedia. Navigli and Crisafulli (2010) propose a method that takes into account synonymy and polysemy. Their method utilises the Google Web1T corpus to identify word senses based on word co-occurrences and computes the similarity between documents using the extracted sense information. More recently, Mu et al. (2016) presented their co-embedding based descriptive clustering approach that learns a common co-embedding vector space of documents and candidate descriptive phrases. The co-embedded space simplifies the clustering and cluster labelling task into a more straightforward process of computing similarity between pairs of docum"
E17-1093,Y06-1066,0,0.0464941,"in order to predict word occurrences given a local context (Mnih and Hinton, 2009; Mikolov et al., 2013b; Mikolov et al., 2013a; Pennington et al., 2014). Subsequently, the PV model was proposed to learn representations of both words and documents (Le and Mikolov, 2014; Dai et al., 2015). The PV model has been Related Work Descriptive Clustering Descriptive clustering methods typically use an unsupervised approach to firstly group documents into flat or hierarchical clusters (Steinbach et al., 2000). Document clusters are then characterised using a set of informative and discriminative words (Zhu et al., 2006), phrases (Mu et al., 2016; Li et al., 2008) or sentences (Kim et al., 2015). Early approaches to descriptive clustering 992 shown to be capable of learning a semantically richer representation of documents compared to unstructured BoW models. To our knowledge, our work constitutes the first attempt to use distributed representation models to co-embed documents and phrases for unsupervised descriptive clustering. local context: X 1 X log p(pt |dt ) + log p(pt |c) (1) |Ct | t∈TP c∈Ct X 1 X log p(ws |c) + log p(ws |ds ) + |Cs | 3 where TP is the set of training phrase instances; pt ∈ P is the t-"
E17-1093,D14-1162,0,0.0791339,"ority of our methods in both clustering performance and labelling quality. 2 2.1 2.2 Distributed Representation Distributed representation techniques are becoming increasingly important in a number of supervised learning tasks, e.g., sentiment analysis (Dai et al., 2015), text classification (Dai et al., 2015; Ma et al., 2015) and named entity recognition (Turian et al., 2010). A number of models have been proposed to learn distributed word or phrase representations in order to predict word occurrences given a local context (Mnih and Hinton, 2009; Mikolov et al., 2013b; Mikolov et al., 2013a; Pennington et al., 2014). Subsequently, the PV model was proposed to learn representations of both words and documents (Le and Mikolov, 2014; Dai et al., 2015). The PV model has been Related Work Descriptive Clustering Descriptive clustering methods typically use an unsupervised approach to firstly group documents into flat or hierarchical clusters (Steinbach et al., 2000). Document clusters are then characterised using a set of informative and discriminative words (Zhu et al., 2006), phrases (Mu et al., 2016; Li et al., 2008) or sentences (Kim et al., 2015). Early approaches to descriptive clustering 992 shown to be"
E17-1093,P10-1040,0,0.0237372,"bels to them), and the previously introduced CEDL method (Mu et al., 2016), which carries out both clustering and labelling. Experimental results based on publicly available benchmark text collections demonstrate the effectiveness and superiority of our methods in both clustering performance and labelling quality. 2 2.1 2.2 Distributed Representation Distributed representation techniques are becoming increasingly important in a number of supervised learning tasks, e.g., sentiment analysis (Dai et al., 2015), text classification (Dai et al., 2015; Ma et al., 2015) and named entity recognition (Turian et al., 2010). A number of models have been proposed to learn distributed word or phrase representations in order to predict word occurrences given a local context (Mnih and Hinton, 2009; Mikolov et al., 2013b; Mikolov et al., 2013a; Pennington et al., 2014). Subsequently, the PV model was proposed to learn representations of both words and documents (Le and Mikolov, 2014; Dai et al., 2015). The PV model has been Related Work Descriptive Clustering Descriptive clustering methods typically use an unsupervised approach to firstly group documents into flat or hierarchical clusters (Steinbach et al., 2000). Do"
E17-1093,D15-1094,0,\N,Missing
I08-1050,W06-3309,0,0.400917,"Missing"
I08-1050,W95-0107,0,0.0126842,". Table 3 shows typical moves of sections in Medline abstracts. The majority of sequences in this table consists of four sections compatible with the ANSI standard, purpose, methods, results, and conclusions. Moreover, the most frequent sequence is “OBJECTIVE → METHOD(S) → RESULTS → CONCLUSION(S),” supposing that AIM and PURPOSE are equivalent to OBJECTIVE. Hence, this study assumes four sections, OBJECTIVE, METHOD, RESULTS, and CONCLUSIONS. Meanwhile, it is common for NP chunking tasks to represent a chunk (e.g., NP) with two labels, the begin (e.g., B-NP) and inside (e.g., I-NP) of a chunk (Ramshaw and Marcus, 1995). Although none of the previous studies employed this representation, attaching B- and I- prefixes to section labels may improve a classifier by associating clue phrases (e.g., “to determine”) with the starts of sections (e.g., B-OBJECTIVE). We will compare classification performances on two sets of label representations: namely, we will compare four section labels and eight labels with BI prefixes attached to section names. 4 4.1 Evaluation Experiment We constructed two sets of corpora (‘pure’ and ‘expanded’), each of which contains 51,000 abstracts sampled from the abstracts with structured"
I08-1050,N03-1028,0,0.0192388,"esult sentences appearing before method sentences were described in an abstract. Moreover, we would like to model the structure of abstract sentences rather than modeling just the section label for each sentence. Thus, the task is more suitably formalized as a sequence labeling problem: given an abstract with sentences x = (x1 , ..., xn ), determine the optimal sequence of section names y = (y1 , ..., yn ) of all possible sequences. Conditional Random Fields (CRFs) have been successfully applied to various NLP tasks including part-of-speech tagging (Lafferty et al., 2001) and shallow parsing (Sha and Pereira, 2003). CRFs define a conditional probability distribution p(y|x) for output and input sequences, y and x, p(y|x) = 1 exp {λ · F (y, x)} . Zλ (x) (1) Therein: function F (y, x) denotes a global feature X f (y, x, i), (2) i i ranges over the input sequence, function f (y, x, i) is a feature vector for input sequence x and output sequence y at position i (based on state features and transition features), λ is a vector where an element λk represents the weight of feature Fk (y, x), and Zλ (x) is a normalization factor, Zλ (x) = X exp {λ · F (y, x)} . (3) y The optimal output sequence y ˆ for an input s"
I08-1050,J02-4002,0,0.870395,"Missing"
I08-1050,H05-1059,0,0.0588171,"Missing"
I08-1050,P06-4011,0,0.214503,"Missing"
I08-2122,I05-1018,1,0.788734,"se. In order to observe such differences, we need to integrate available combinations of tools into a workflow and to compare the combinatorial results. Although generic frameworks like UIMA (Unstructured Information Management Architecture) provide interoperability to solve this problem, the solution they provide is only partial. In order for truly interoperable toolkits to become a reality, we also need 1 Introduction Recently, an increasing number of TM/NLP tools such as part-of-speech (POS) taggers (Tsuruoka et al., 2005), named entity recognizers (NERs) (Settles, 2005) syntactic parsers (Hara et al., 2005) and relation or event extractors (ERs) have been developed. Nevertheless, it is still very difficult to integrate independently developed tools into an aggregated application that achieves a specific task. The difficulties are caused not only by differences in programming platforms and different input/output data formats, but also by the lack of higher level interoperability among modules developed by different groups. 859 uima.jcas.cas.TOP tcas.uima.Annotation -begin: int -end: int SyntacticAnnotation POS SemanticAnnotation UnknownPOS PennPOS -posType: String Token Sentence Phrase NamedEntit"
I08-2122,W04-1213,0,0.0372378,"Missing"
I08-2122,J93-2004,0,0.0293416,"type systems have to be related through a sharable type system, which our platform defines. Such a shared type system can bridge modules with different type systems, though the bridging module may lose some information during the translation process. Whether such a sharable type system can be defined or not is dependent on the nature of each problem. For example, a sharable type system for POS tags in English can be defined rather easily, since most of POS-related modules (such as POS taggers, shallow parsers, etc.) more or less follow the well established types defined by the Penn Treebank (Marcus et al., 1993) tag set. Figure 1 shows a part of our sharable type system. We deliberately define a highly organized type hierarchy as described above. Secondly we should consider that the type system may be used to compare a similar sort of tools. Types should be defined in a distinct and 861 hierarchical manner. For example, both tokenizers and POS taggers output an object of type Token, but their roles are different when we assume a cascaded pipeline. We defined Token as a supertvpe, POSToken as subtypes of Token. Each tool should have an individual type to make clear which tool generated which instance,"
I08-2122,E06-1015,0,0.0352696,"Missing"
I08-2122,J96-1002,0,0.0129233,"Missing"
kano-etal-2010-u,W09-1401,1,\N,Missing
kano-etal-2010-u,W04-1213,0,\N,Missing
kano-etal-2010-u,W09-1504,1,\N,Missing
korkontzelos-ananiadou-2014-locating,N10-1097,0,\N,Missing
korkontzelos-ananiadou-2014-locating,D10-1121,0,\N,Missing
korkontzelos-ananiadou-2014-locating,W10-2923,0,\N,Missing
korkontzelos-ananiadou-2014-locating,D11-1002,0,\N,Missing
korkontzelos-ananiadou-2014-locating,P08-1081,0,\N,Missing
korkontzelos-ananiadou-2014-locating,W10-0508,0,\N,Missing
korkontzelos-ananiadou-2014-locating,U10-1006,0,\N,Missing
L16-1290,P08-1095,0,0.0132231,"sts and responses, dialogue acts have been employed extensively to identify the structure of student discussion threads (Kim et al., 2006) and forum threads (Lin et al., 2009; Kim et al., 2010; Wang et al., 2011). In contrast to these works, the present work does not pursue the dialogue structure of threads, but instead proposes a detailed set of types and assigns them to messages. Conversation disentanglement is another aspect of discussion analysis. Based on the idea that multiple conversations may occur simultaneously, the task is to identify to which conversation each message corresponds (Elsner and Charniak, 2008; Wang et al., 2011). In the present work, discussions are already threaded and we hypothesise that there is a single discussion in each thread. Conversation focus detection is the task of identifying the message in a thread that contains the most important information (Feng et al., 2006). This task is also similar to our work, since it is also based on analysing the content of threaded messages. The most similar work to the present one focuses on classifying messages according to their purpose in a discussion thread (Bhatia et al., 2012). A small set of message types are used: asking a questi"
L16-1290,N06-1027,0,0.0314622,"e of threads, but instead proposes a detailed set of types and assigns them to messages. Conversation disentanglement is another aspect of discussion analysis. Based on the idea that multiple conversations may occur simultaneously, the task is to identify to which conversation each message corresponds (Elsner and Charniak, 2008; Wang et al., 2011). In the present work, discussions are already threaded and we hypothesise that there is a single discussion in each thread. Conversation focus detection is the task of identifying the message in a thread that contains the most important information (Feng et al., 2006). This task is also similar to our work, since it is also based on analysing the content of threaded messages. The most similar work to the present one focuses on classifying messages according to their purpose in a discussion thread (Bhatia et al., 2012). A small set of message types are used: asking a question, repeating a question, asking for clarification of a request, providing more details about a request, suggesting a solution and providing positive or negative solution feedback. This set of message types was originally proposed in the second task of the Mailing Lists and Forums (MLAF)"
L16-1290,W10-2923,0,0.0175524,"sing the quality of OSS projects. 2. Related work The idea of identifying requests and responses within online communication has been applied to email (Shrestha and McKeown, 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), online forums (Ding et al., 2008; Cong et al., 2008) and online forums and issue trackers related to OSS (Korkontzelos and Ananiadou, 2014). More detailed than the coarse-grained classes of requests and responses, dialogue acts have been employed extensively to identify the structure of student discussion threads (Kim et al., 2006) and forum threads (Lin et al., 2009; Kim et al., 2010; Wang et al., 2011). In contrast to these works, the present work does not pursue the dialogue structure of threads, but instead proposes a detailed set of types and assigns them to messages. Conversation disentanglement is another aspect of discussion analysis. Based on the idea that multiple conversations may occur simultaneously, the task is to identify to which conversation each message corresponds (Elsner and Charniak, 2008; Wang et al., 2011). In the present work, discussions are already threaded and we hypothesise that there is a single discussion in each thread. Conversation focus det"
L16-1290,korkontzelos-ananiadou-2014-locating,1,0.904646,"trics. However, complimentary information about OSS quality can be extracted by analysing messages posted to communication channels (newsgroups, forums, mailing lists), and issue trackers supporting OSS projects. For example, analysing online communication related to an OSS project can provide information about the speed at which the community responds to user requests, the rate that bugs are fixed or the satisfaction of users about the responses they receive to their requests. In our previous work, we investigated the task of identifying request and responses among online messages about OSS (Korkontzelos and Ananiadou, 2014). This classification allowed us to compute a preliminary level of metrics that indicate the quality of support offered online. For example, we computed metrics such as the number of unanswered threads or issues in a communication channel or the average time taken to respond to a request. In this paper, we classify messages according to a more fine-grained set of content types. A more informative set of content types will allow the design of more fine-grained quality-indicating metrics. For example, the broad class of responses (Korkontzelos and Ananiadou, 2014) can be split into more in- mess"
L16-1290,C04-1128,0,0.110894,"Missing"
L16-1290,D11-1002,0,0.0216865,"f OSS projects. 2. Related work The idea of identifying requests and responses within online communication has been applied to email (Shrestha and McKeown, 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), online forums (Ding et al., 2008; Cong et al., 2008) and online forums and issue trackers related to OSS (Korkontzelos and Ananiadou, 2014). More detailed than the coarse-grained classes of requests and responses, dialogue acts have been employed extensively to identify the structure of student discussion threads (Kim et al., 2006) and forum threads (Lin et al., 2009; Kim et al., 2010; Wang et al., 2011). In contrast to these works, the present work does not pursue the dialogue structure of threads, but instead proposes a detailed set of types and assigns them to messages. Conversation disentanglement is another aspect of discussion analysis. Based on the idea that multiple conversations may occur simultaneously, the task is to identify to which conversation each message corresponds (Elsner and Charniak, 2008; Wang et al., 2011). In the present work, discussions are already threaded and we hypothesise that there is a single discussion in each thread. Conversation focus detection is the task o"
L16-1290,P07-2032,0,0.0603623,"Missing"
L18-1042,W15-4319,0,0.0603429,"Missing"
L18-1042,D10-1098,0,0.0327746,"Named entity recognition (Nadeau and Sekine, 2007) is a common text mining task in which an algorithm is used to identify parts of an unstructured text that can be categorised according to a given schema. Named entity recognition has been applied across diverse corpora including but not limited to Twitter (Derczynski et al., 2015; Baldwin et al., 2015), biomedical papers (Munkhdalai et al., 2015; Wang et al., 2015) and other areas. Approaches for named entity recognition range from using dictionaries (Cohen and Sarawagi, 2004) to using regular expressions and other rules (Kluegl et al., 2016; Chiticariu et al., 2010), using machine learning approaches such as the conditional random field (Lu et al., 2015), or more recently leveraging advances in the field of deep learning (Chiu and Nichols, 2015; Lample et al., 2016). Relation extraction (Aggarwal and Zhai, 2012) is a task in information extraction that requires an algorithm to link together two named entities according to a given schema which defines the meaning of the link. Relation extraction requires named entity recognition to be performed as a prerequisite. Relation extraction may be performed using supervised (Nguyen and Grishman, 2015; Pons et al."
L18-1042,N16-1030,0,0.0139094,"ed entity recognition has been applied across diverse corpora including but not limited to Twitter (Derczynski et al., 2015; Baldwin et al., 2015), biomedical papers (Munkhdalai et al., 2015; Wang et al., 2015) and other areas. Approaches for named entity recognition range from using dictionaries (Cohen and Sarawagi, 2004) to using regular expressions and other rules (Kluegl et al., 2016; Chiticariu et al., 2010), using machine learning approaches such as the conditional random field (Lu et al., 2015), or more recently leveraging advances in the field of deep learning (Chiu and Nichols, 2015; Lample et al., 2016). Relation extraction (Aggarwal and Zhai, 2012) is a task in information extraction that requires an algorithm to link together two named entities according to a given schema which defines the meaning of the link. Relation extraction requires named entity recognition to be performed as a prerequisite. Relation extraction may be performed using supervised (Nguyen and Grishman, 2015; Pons et al., 2015) or unsupervised methods (Quan et al., 2014). Simple heuristics such as two entities in close proximity may serve as a baseline, but are less powerful than measures which take structural and contex"
L18-1042,W15-1506,0,0.0126707,"et al., 2016; Chiticariu et al., 2010), using machine learning approaches such as the conditional random field (Lu et al., 2015), or more recently leveraging advances in the field of deep learning (Chiu and Nichols, 2015; Lample et al., 2016). Relation extraction (Aggarwal and Zhai, 2012) is a task in information extraction that requires an algorithm to link together two named entities according to a given schema which defines the meaning of the link. Relation extraction requires named entity recognition to be performed as a prerequisite. Relation extraction may be performed using supervised (Nguyen and Grishman, 2015; Pons et al., 2015) or unsupervised methods (Quan et al., 2014). Simple heuristics such as two entities in close proximity may serve as a baseline, but are less powerful than measures which take structural and context features of the named entities into account. 2. Corpus Construction We annotated a set of 200 abstracts and 100 full papers with the entities that were of interest to us, as well as relations between these entities. Although we could have leveraged existing corpora for some of the entity types that we were interested in, we found that there were very few corpora dealing with met"
maynard-ananiadou-2000-creating,C94-1024,0,\N,Missing
maynard-ananiadou-2000-creating,C92-2070,0,\N,Missing
maynard-ananiadou-2000-creating,C96-2212,0,\N,Missing
mihaila-ananiadou-2014-meta,nawaz-etal-2012-identification,1,\N,Missing
mihaila-ananiadou-2014-meta,miltsakaki-etal-2004-penn,0,\N,Missing
mihaila-ananiadou-2014-meta,liakata-etal-2010-corpora,0,\N,Missing
mihaila-ananiadou-2014-meta,J08-1002,0,\N,Missing
mihaila-ananiadou-2014-meta,W08-0606,0,\N,Missing
mihaila-ananiadou-2014-meta,W03-1210,0,\N,Missing
mihaila-ananiadou-2014-meta,W08-0607,0,\N,Missing
mihaila-ananiadou-2014-meta,prasad-etal-2008-penn,0,\N,Missing
mihaila-ananiadou-2014-meta,blanco-etal-2008-causal,0,\N,Missing
mihaila-ananiadou-2014-meta,W12-4302,0,\N,Missing
N19-1295,D15-1075,0,0.0456967,"bedding (Avg.) and the output at the last step of an LSTM layer (Embed.→LSTM). For the embedding we use word2vec embeddings pre-trained on Google News (Mikolov et al., 2013) for the question classification and RTE tasks, and a pre-trained embedding (Pyysalo et al., 2013) trained on a combination of English Wikipedia, PubMed and PMC texts for the sentence classification task. For the RTE task, we implemented two classifiers. For the first one, each instance (i.e. a sentence pair) was represented as a concatenation of the average word embedding for each sentence (Cat. Avg.). We also implemented Bowman et al. (2015), which runs each sentence through an LSTM, concatenates the outputs, and then feeds the concatenated output to an FNN with tanh activations. Reliability Estimator: We model the reliability estimator as an FNN. Its structure is the same as the classifier, albeit with different sizes of the two hidden layers. For the experiments listed in Table 2, the number of units of each hidden layer in the FNN are 5, 100, 25, 25, 50, and 100 respectively. The input to the estimator is the concatenation of the instance xi (i.e. its original feature vector or the output of the last hidden layer of the classi"
N19-1295,N13-1132,0,0.0762286,"t-effective annotators (Donmez and Carbonell, 2008; Li et al., 2017). Although reliability estimation has been studied for a long time, only a limited number of studies have examined how to model the reliability of each annotator on a perinstance basis. Additionally, these in turn have only considered binary labels (Yan et al., 2010, 2014; Wang and Bi, 2017), and cannot be extended to multi-class classification in a straightforward manner. In order to handle both binary and multi-class labels, our approach extends one of the most popular probabilistic models for label aggregation, proposed by Hovy et al. (2013). One challenge of extending the model is the definition of the label and reliability probability distributions on a perinstance basis. Our approach introduces a classifier which predicts the correct label of an instance, and a reliability estimator, providing the probability that an annotator will label a given instance correctly. The approach allows us to simultaneously estimate the per-instance reliability of the annotators and the correct labels, allowing the two processes to inform each other. Another challenge is to select appropriate training methods to learn a model with high and stabl"
N19-1295,W17-2314,1,0.563358,"ator with expertise in a single domain to be unreliable for the model, even though the annotations are reliable within the annotator’s domain of expertise. Estimating per-instance reliability is also helpful for unreliable annotator detection and task allocation in crowdsourcing, where the cost of labelling data is reduced using proactive learn2873 Proceedings of NAACL-HLT 2019, pages 2873–2883 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ing strategies for pairing instances with the most cost-effective annotators (Donmez and Carbonell, 2008; Li et al., 2017). Although reliability estimation has been studied for a long time, only a limited number of studies have examined how to model the reliability of each annotator on a perinstance basis. Additionally, these in turn have only considered binary labels (Yan et al., 2010, 2014; Wang and Bi, 2017), and cannot be extended to multi-class classification in a straightforward manner. In order to handle both binary and multi-class labels, our approach extends one of the most popular probabilistic models for label aggregation, proposed by Hovy et al. (2013). One challenge of extending the model is the defi"
N19-1295,C02-1150,0,0.0936005,"m 1. The algorithm is run until either a maximum number of iterations is reached, or the objective function stops improving. 2876 Dataset moon circle 3-class Question Classification Figure 3: Three 2-dimensional datasets. 4 4.1 Sentence Classification Evaluation Settings RTE Data Simulated Annotators 2-Dimensional Datasets: In order to see whether our method can work well on simple cases, we create three 2-dimensional synthetic datasets, which we refer to as moon, circle and 3-class as shown in Figure 3. Text Classification: For text classification we use the datasets Question Classification (Li and Roth, 2002), which contains short questions along with the type of answer expected, and Sentence Classification (Chambers, 2013), which consists of sentences selected from medical publications. Examples of instance/class pairs for the text classification datasets include ”Where is the Orinoco?” (class: ”location”) for the Question Classification dataset, and ”New types of potent force clamps are discovered.” (class: ”author’s own work”) for the Sentence Classification dataset. For these datasets that do not include crowd annotations, we synthesise annotations by simulating different annotators as follows"
N19-1295,P17-1028,0,0.118863,"igh agreement between the predicted label and the ground truth for some NLP tasks (Snow et al., 2008). Dawid and Skene (1979), Whitehill et al. (2009), Raykar et al. (2010), Welinder et al. (2010), Liu et al. (2012), Zhou et al. (2012), Kim and Ghahramani (2012), Hovy et al. (2013), Yan et al. (2010; 2014), Li et al. (2014b) and Zhang et al. (2014) investigated binary or multi-class label prediction using probabilistic graphical models. Karger et al. (2011), Wang and Bi (2017), and Bonald and Combes (2017) formalised the label prediction as an optimisation problem. Rodrigues et al. (2014) and Nguyen et al. (2017) investigated how to aggregate sequence labels using probabilistic graphical models. 2874 Classifier: The classifier ft (xi ) provides the predicted probabilities of an instance belonging to each category, p(ti |xi ). ti is the underlying label for instance xi , the ith instance, and takes a value in the set of categories T . Note that there is no restriction on what classifier is used, other than that it can be trained using expectation maximisation. The inclusion of a classifier directly in the model means that it can be trained while taking into account the uncertainty of the data and predi"
N19-1295,D08-1027,0,0.547262,"Missing"
nawaz-etal-2010-meta,W04-0207,0,\N,Missing
nawaz-etal-2010-meta,W08-0606,0,\N,Missing
nawaz-etal-2010-meta,E99-1015,0,\N,Missing
nawaz-etal-2010-meta,N07-2036,0,\N,Missing
nawaz-etal-2010-meta,P06-1128,0,\N,Missing
nawaz-etal-2010-meta,D09-1155,0,\N,Missing
nawaz-etal-2010-meta,P07-1125,0,\N,Missing
nawaz-etal-2010-meta,W04-3103,0,\N,Missing
nawaz-etal-2010-meta,W04-1205,0,\N,Missing
nawaz-etal-2012-identification,J93-2004,0,\N,Missing
nawaz-etal-2012-identification,J08-1002,0,\N,Missing
nawaz-etal-2012-identification,H05-1044,0,\N,Missing
nawaz-etal-2012-identification,P06-1128,0,\N,Missing
nawaz-etal-2012-identification,J05-1004,0,\N,Missing
nawaz-etal-2012-identification,W10-3100,0,\N,Missing
nawaz-etal-2012-identification,W10-3111,0,\N,Missing
nawaz-etal-2012-identification,W10-3112,1,\N,Missing
nenadic-etal-2002-automatic,spasic-etal-2002-tuning,1,\N,Missing
nenadic-etal-2002-automatic,W02-1408,1,\N,Missing
nenadic-etal-2002-automatic,C94-2167,1,\N,Missing
nenadic-etal-2002-automatic,C92-3150,0,\N,Missing
nenadic-etal-2006-towards,C04-1087,1,\N,Missing
nenadic-etal-2006-towards,W04-3110,0,\N,Missing
nenadic-etal-2006-towards,nenadic-etal-2002-automatic,1,\N,Missing
nenadic-etal-2006-towards,okazaki-ananiadou-2006-clustering,1,\N,Missing
okazaki-ananiadou-2006-clustering,C04-1087,1,\N,Missing
okazaki-ananiadou-2006-clustering,W05-1304,1,\N,Missing
P06-2083,P02-1021,0,0.219363,"Missing"
P06-2083,W01-0516,0,0.235171,"Missing"
P09-1054,P04-1014,0,0.00603949,"tal results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 1 Introduction Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005). Loglinear models have a major advantage over other 477 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477–485, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning f"
P09-1054,W05-0622,0,0.00845476,"i If the structure is a sequence, the model is called a linear-chain CRF model, and the marginal probabilities of the features and the partition function can be efficiently computed by using the forwardbackward algorithm. The model is used for a variety of sequence labeling tasks such as POS tagging, chunking, and named entity recognition. If the structure is a tree, the model is called a tree CRF model, and the marginal probabilities can be computed by using the inside-outside algorithm. The model can be used for tasks like syntactic parsing (Finkel et al., 2008) and semantic role labeling (Cohn and Blunsom, 2005). In this paper, we present a simple method for solving these two problems in SGD learning. The main idea is to keep track of the total penalty and the penalty that has been applied to each weight, so that the L1 penalty is applied based on the difference between those cumulative values. That way, the application of L1 penalty is needed only for the features that are used in the current sample, and also the effect of noisy gradient is smoothed away. 2.1 Training The weights of the features in a log-linear model are optimized in such a way that they maximize the regularized conditional log-like"
P09-1054,W02-1001,0,0.468961,"Missing"
P09-1054,P06-1059,1,0.746285,",10 which provided POS tags and chunk tags. We did not use any information on the named entity tags output by the GENIA tagger. For the features, we used unigrams of neighboring chunk tags, substrings (shorter than 10 characters) of the current word, and the shape of the word (e.g. “IL-2” is converted into “AA-#”), on top of the features used in the text chunking experiments. The results are shown in Figure 5 and Table 2. The trend in the results is the same as that of the text chunking task: our SGD algorithms show much faster convergence than the OWL-QN algorithm and produce compact models. Okanohara et al. (2006) report an f-score of 71.48 on the same data, using semi-Markov CRFs. -2.6 -2.8 -3 -3.2 -3.4 OWL-QN SGD-L1 (Clipping) SGD-L1 (Cumulative) SGD-L1 (Cumulative + ED) -3.6 -3.8 0 10 20 30 40 50 Passes Figure 5: NLPBA 2004 named entity recognition task: Objective. -1.8 -1.9 Objective function -2 -2.1 -2.2 -2.3 -2.4 -2.5 OWL-QN SGD-L1 (Clipping) SGD-L1 (Cumulative) SGD-L1 (Cumulative + ED) -2.6 -2.7 -2.8 0 10 20 30 40 50 Passes Figure 6: POS tagging task: Objective. 4.3 Part-Of-Speech Tagging ing because it was “prohibitive” (7-8 days for sections 0-18 of the WSJ corpus). For the features, we used u"
P09-1054,W96-0213,0,0.530357,"evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging. Experimental results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 1 Introduction Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005). Loglinear models have a major advantage over other 477 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477–485, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and u"
P09-1054,P07-1096,0,0.0378243,"Missing"
P09-1054,D08-1016,0,0.0112981,"log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning framework is attracting attention because it often requires much less training time in practice than batch training algorithms, especially when the training data is large and redundant. SGD was recently used for NLP tasks including machine translation (Tillmann and Zhang, 2006) and syntactic parsing (Smith and Eisner, 2008; Finkel et al., 2008). Also, SGD is very easy to implement because it does not need to use the Hessian information on the objective function. The implementation could be as simple as the perceptron algorithm. pact and accurate models much more quickly than the OWL-QN algorithm. This paper is organized as follows. Section 2 provides a general description of log-linear models used in NLP. Section 3 describes our stochastic gradient descent method for L1-regularized loglinear models. Experimental results are presented in Section 4. Some related work is discussed in Section 5. Section 6 gives som"
P09-1054,P08-1109,0,0.0549519,"Missing"
P09-1054,P07-1104,0,0.0274516,"ion, which aims to obtain the weights of the features that maximize the conditional likelihood of the training data. In maximum likelihood training, regularization is normally needed to prevent the model from overfitting the training data, The two most common regularization methods are called L1 and L2 regularization. L1 regularization penalizes the weight vector for its L1-norm (i.e. the sum of the absolute values of the weights), whereas L2 regularization uses its L2-norm. There is usually not a considerable difference between the two methods in terms of the accuracy of the resulting model (Gao et al., 2007), but L1 regularization has a significant advantage in practice. Because many of the weights of the features become zero as a result of L1-regularized training, the size of the model can be much smaller than that produced by L2-regularization. Compact models require less space on memory and storage, and enable the application to start up quickly. These merits can be of vital importance when the application is deployed in resource-tight environments such as cell-phones. A common way to train a large-scale L1regularized model is to use a quasi-Newton method. Kazama and Tsujii (2003) describe a m"
P09-1054,P06-1091,0,0.00589602,"L and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning framework is attracting attention because it often requires much less training time in practice than batch training algorithms, especially when the training data is large and redundant. SGD was recently used for NLP tasks including machine translation (Tillmann and Zhang, 2006) and syntactic parsing (Smith and Eisner, 2008; Finkel et al., 2008). Also, SGD is very easy to implement because it does not need to use the Hessian information on the objective function. The implementation could be as simple as the perceptron algorithm. pact and accurate models much more quickly than the OWL-QN algorithm. This paper is organized as follows. Section 2 provides a general description of log-linear models used in NLP. Section 3 describes our stochastic gradient descent method for L1-regularized loglinear models. Experimental results are presented in Section 4. Some related work"
P09-1054,P05-1073,0,0.00918396,"compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 1 Introduction Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005). Loglinear models have a major advantage over other 477 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477–485, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning framework is attracting attention because it often re"
P09-1054,W03-1018,1,0.434131,"he resulting model (Gao et al., 2007), but L1 regularization has a significant advantage in practice. Because many of the weights of the features become zero as a result of L1-regularized training, the size of the model can be much smaller than that produced by L2-regularization. Compact models require less space on memory and storage, and enable the application to start up quickly. These merits can be of vital importance when the application is deployed in resource-tight environments such as cell-phones. A common way to train a large-scale L1regularized model is to use a quasi-Newton method. Kazama and Tsujii (2003) describe a method for training a L1-regularized log-linear model with a bound constrained version of the BFGS algorithm (Nocedal, 1980). Andrew and Gao (2007) present an algorithm called OrthantWise Limited-memory Quasi-Newton (OWLQN), which can work on the BFGS algorithm without bound constraints and achieve faster convergence. Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion. This learning framework is attractive because it often requires much less training time in practice than batch tra"
P09-1054,wellner-vilain-2006-leveraging,0,0.0282722,"Missing"
P09-1054,W04-1213,0,0.0388038,"eature set as ours.8 Their library uses the OWL-QN algorithm for optimization. Although direct comparison of training times is not impor6 http://crfpp.sourceforge.net/ http://www.chokkan.org/software/crfsuite/benchmark.html 8 ditto 7 482 tant due to the differences in implementation and hardware platforms, these results demonstrate that our algorithm can actually result in a very fast implementation of a CRF trainer. -2.2 Objective function -2.4 4.2 Named Entity Recognition The second set of experiments used the named entity recognition data set provided for the BioNLP/NLPBA 2004 shared task (Kim et al., 2004).9 The training data consist of 18,546 sentences in which each token is annotated with the “IOB” tags representing biomedical named entities such as the names of proteins and RNAs. The training and test data were preprocessed by the GENIA tagger,10 which provided POS tags and chunk tags. We did not use any information on the named entity tags output by the GENIA tagger. For the features, we used unigrams of neighboring chunk tags, substrings (shorter than 10 characters) of the current word, and the shape of the word (e.g. “IL-2” is converted into “AA-#”), on top of the features used in the tex"
P09-1054,J93-2004,0,\N,Missing
P12-3021,kano-etal-2010-u,1,0.849535,"., 2011; Corbett and MurrayRust, 2006). However, such tools inherently impose inconveniences on users, such as a lack of GUI, often arduous manual installation procedures, proficiency in programming or familiarity with the details of machine learning algorithms. These limitations are overcome by GUI-equipped, workflow-supporting platforms that often directly use the solutions provided by the former tools. The notable examples of such platforms designed specifically for NLP and text mining tasks are GATE (Cunningham et al., 2002), a suite of text processing and annotation tools, and U-Compare (Kano et al., 2010), a standalone application supporting the UIMA framework that formed the inspiration for Argo. Although the GUI platforms provide machine learning solutions, these are usually limited to using pre-trained models and providing a rich set of features for training requires resorting to programming. Argo, on the other hand, allows the users to train their own models with either a generic set of features or customisable features without having to write a single line of code. This capability is provided in Argo entirely through its GUI. Figure 1: Screen capture of Argo’s web-based interface. 3 Argo"
P12-3021,W04-1213,0,0.0255414,"ets, the two datasets, the two labelling formats and the two estimation algorithms. 5.1 Table 2: Comparison of setups with basic and extended features for the chunking and NER tasks. clude surface shape, length, prefixes, suffixes, and the presence of various combinations of letters, digits and symbols. The context n-grams include unigrams for all feature definitions and bigrams for selected ones. Figure 3b shows a sample of the actual extended set. We use two datasets, one prepared for the CoNLL 2000 shared task (Tjong et al., 2000) and another prepared for the BioNLP/NLPBA 2004 shared task (Kim et al., 2004). They represent two different tagging tasks, chunking and named entity recognition, respectively. The CoNLL 2000 chunking dataset involves 10 labels and comes pre-tokenised with 211,727 tokens in the training set and 47,377 tokens in the test set. The dataset also provides partof-speech tags for each token. The BioNLP/NLPBA 2004 named entity recognition dataset involves five biology-related labels and consists of 472,006 and 96,780 tokens in the training and testing sets, respectively. Contrary to the former dataset, there is 125 Results Table 1 shows the precision, recall and f-scores of our"
P12-3021,N01-1025,0,0.0298644,"ion supporting the tokens in the BioNLP/NLPBA dataset. To compensate for it we automatically generated part of speech and chunk labels for each token. The chosen datasets/tasks are by no means an exhaustive set of representative comparative-setup datasets available. Our goal is not to claim the superiority of our approach over the solutions reported in the respective shared tasks. Instead, we aim to show that our generic setup is comparable to those task-tuned solutions. We further explore the options of both Feature Generator and CRF++ Trainer by manipulating labelling formats (IOB vs IOBES (Kudo and Matsumoto, 2001)) for the former and parameter estimation algorithms (L2 - vs L1 -norm regularisation) for the latter. Ultimately, there are 32 setups as the result of the combinations of the two feature sets, the two datasets, the two labelling formats and the two estimation algorithms. 5.1 Table 2: Comparison of setups with basic and extended features for the chunking and NER tasks. clude surface shape, length, prefixes, suffixes, and the presence of various combinations of letters, digits and symbols. The context n-grams include unigrams for all feature definitions and bigrams for selected ones. Figure 3b"
P12-3021,W00-0726,0,0.0336075,"tely, there are 32 setups as the result of the combinations of the two feature sets, the two datasets, the two labelling formats and the two estimation algorithms. 5.1 Table 2: Comparison of setups with basic and extended features for the chunking and NER tasks. clude surface shape, length, prefixes, suffixes, and the presence of various combinations of letters, digits and symbols. The context n-grams include unigrams for all feature definitions and bigrams for selected ones. Figure 3b shows a sample of the actual extended set. We use two datasets, one prepared for the CoNLL 2000 shared task (Tjong et al., 2000) and another prepared for the BioNLP/NLPBA 2004 shared task (Kim et al., 2004). They represent two different tagging tasks, chunking and named entity recognition, respectively. The CoNLL 2000 chunking dataset involves 10 labels and comes pre-tokenised with 211,727 tokens in the training set and 47,377 tokens in the test set. The dataset also provides partof-speech tags for each token. The BioNLP/NLPBA 2004 named entity recognition dataset involves five biology-related labels and consists of 472,006 and 96,780 tokens in the training and testing sets, respectively. Contrary to the former dataset"
P13-3006,J08-1002,0,0.0338249,"Missing"
P13-3006,P06-1128,0,0.0530611,"Missing"
P13-3006,P09-2004,0,0.025539,"BioCause contains 381 unique explicit triggers in the corpus, each being used, on average, only 2.10 times. The number decreases to 347 unique triggers when they are lemmatised, corresponding to an average usage of 2.30 times per trigger. Both count settings show the diversity of causality-triggering phrases that are used in the biomedical domain. 2 Related Work A large amount of work related to discourse parsing and discourse relation identification exists in the general domain, where researchers have not only identified discourse connectives, but also developed end-to-end discourse parsers (Pitler and Nenkova, 2009; Lin et al., 2012). Most work is based on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), a corpus of lexically-grounded annotations of discourse relations. Until now, comparatively little work has been carried out on causal discourse relations in the biomedical domain, although causal associations between biological entities, events and processes are central to most claims of interest (Kleinberg and Hripcsak, 2011). The equivalent of the PDTB for the biomedical domain is the BioDRB corpus (Prasad et al., 2011), containing 16 types of discourse relations, e.g., temporal, causal and"
P13-3006,prasad-etal-2008-penn,0,0.0366111,"mes. The number decreases to 347 unique triggers when they are lemmatised, corresponding to an average usage of 2.30 times per trigger. Both count settings show the diversity of causality-triggering phrases that are used in the biomedical domain. 2 Related Work A large amount of work related to discourse parsing and discourse relation identification exists in the general domain, where researchers have not only identified discourse connectives, but also developed end-to-end discourse parsers (Pitler and Nenkova, 2009; Lin et al., 2012). Most work is based on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), a corpus of lexically-grounded annotations of discourse relations. Until now, comparatively little work has been carried out on causal discourse relations in the biomedical domain, although causal associations between biological entities, events and processes are central to most claims of interest (Kleinberg and Hripcsak, 2011). The equivalent of the PDTB for the biomedical domain is the BioDRB corpus (Prasad et al., 2011), containing 16 types of discourse relations, e.g., temporal, causal and conditional. The number of purely causal relations annotated in this corpus is 542. There are anoth"
P13-3006,W09-1301,0,0.0733946,"Missing"
P13-3006,W11-1804,1,0.896736,"Missing"
P13-3006,W11-0210,1,0.898906,"Missing"
P13-4008,2005.eamt-1.12,0,0.0699137,"Missing"
P13-4008,W06-2714,0,0.0791357,"Missing"
P13-4008,P02-1022,0,0.0559359,"the input data, such as a source text and its translation. The data stored in different Sofas is not restricted to textual information; it can also correspond to other modalities, such as audio data. This makes the Sofa mechanism equally suitable for storing the output of text-to-speech workflows. Our extensions to U-Compare are thus implemented by reading and displaying the contents of different types of Sofas. Related work Over the past few years, an increasing numbers of researchers have begun to create and distribute their own workflow construction architectures (Ferrucci and Lally, 2004; Cunningham et al., 2002; Grishman et al., 1997; Sch¨afer, 2006) or platforms (Kano et al., 2011; Rak et al., 2012; Ogrodniczuk and Karagiozov, 2011; Savova et al., 2010) that allow the rapid development of NLP applications. GATE (Cunningham et al., 2002) is a workflow construction framework that has been used to develop several types of NLP applications, including summarisation systems. It facilitates the development of a wide range of NLP applications by providing a collection of components that can process The Sofa mechanism has previously been 44 erate on one or more specific languages other than English and 4 ar"
P13-4020,kano-etal-2010-u,1,0.848313,"Missing"
P13-4020,E12-2021,1,0.853897,"Missing"
P18-2014,P15-1061,0,0.273121,"Missing"
P18-2014,D15-1062,0,0.0818755,"dge graphs (KG) for knowledge graph completion (Jiang et al., 2017) and the creation of knowledge graph embeddings (Wang et al., 2017; Shi and Weninger, 2017). These models rely on paths between existing relations in order to infer new associations between entities in KGs. However, for relation extraction from a sentence, related pairs are not predefined and consequently all entity pairs need to be considered to extract relations. In addition, state-of-the-art RE models sometimes depend on external syntactic tools to build the shortest dependency path (SDP) between two entities in a sentence (Xu et al., 2015; Miwa and Bansal, 2016). This dependence on external tools leads to domain dependent models. Introduction Relation extraction (RE) is a task of identifying typed relations between known entity mentions in a sentence. Most existing RE models treat each relation in a sentence individually (Miwa and Bansal, 2016; Nguyen and Grishman, 2015). However, a sentence typically contains multiple relations between entity mentions. RE models need to consider these pairs simultaneously to model the dependencies among them. The relation between a pair of interest (namely “target” pair) can be influenced by"
P18-2014,C16-1138,0,0.029902,"/tticoin/LSTM-ER 4 84 The authors kindly provided us with the data split. # Entities l=1 l=2 l=4 l=8 2 3 [4, 6) [6, 12) [12, 23) 71.2 70.1 56.5 59.2 54.7 69.8 67.5 59.7 64.2∗ 59.3 72.9 67.8 59.3 62.2 62.3∗ 71.0 63.5∗ 59.9 60.4 55.0 able to encode linguistic and syntactic properties of long word sequences, making them preferable for sequence-related tasks, e.g. natural language generation (Goyal et al., 2016), machine translation (Sutskever et al., 2014). State-of-the-art systems have proved to achieve good performance on relation extraction using RNNs (Cai et al., 2016; Miwa and Bansal, 2016; Xu et al., 2016; Liu et al., 2015). Nevertheless, most approaches do not take into consideration the dependencies between relations in a single sentence (dos Santos et al., 2015; Nguyen and Grishman, 2015) and treat each pair separately. Current graph-based models are applied on knowledge graphs for distantly supervised relation extraction (Zeng et al., 2017). Graphs are defined on semantic types in their method, whereas we built entity-based graphs in sentences. Other approaches also treat multiple relations in a sentence (Gupta et al., 2016; Miwa and Sasaki, 2014; Li and Ji, 2014), but they fail to model l"
P18-2014,D17-1186,0,0.0762816,"Missing"
P18-2014,P16-2034,0,0.126561,"ntic type representation tz and two relative position representations: to target entity ei , pzi and to target entity ej , pzj . The final representation for a context word wz of a target pair is, vijz = [ez ; tz ; pzi ; pzj ]. For a sentence, the context representations for all entity pairs can be expressed as a three-dimensional matrix C, where rows and columns correspond to entities and the depth corresponds to the context words. The context words representations of each target pair are then compiled into a single representation with an attention mechanism. Following the method proposed in Zhou et al. (2016), we calculate weights for the context words of the targetpair and compute their weighted average, 2.4 Our main aim is to support the relation between an entity pair by using chains of intermediate relations between the pair entities. Thus, the goal of this layer is to generate a single representation for a finite number of different lengths walks between two target entities. To achieve this, we represent a sentence as a directed graph, where the entities constitute the graph nodes and edges correspond to the representation of the relation between the two nodes. The representation of one-lengt"
P19-1423,D14-1181,0,0.00942432,"stracts annotated with several biomedical named entities from PubMed. We selected chemical compounds from the annotated entities and aligned them with the graph database Biochem4j (Swainston et al., 2017). Biochem4j is a freely available database that integrates several resources such as 4311 2 http://www.nactem.ac.uk/Thalia/ Data CDR CHR Count # Articles # Positive pairs # Negative pairs # Articles # Positive pairs # Negative pairs Train 500 1,038 4,198 7,298 19,643 69,843 Dev. 500 1,012 4,069 1,182 3,185 11,466 Test 500 1,066 4,119 3,614 9,578 33,339 models: CNN-RE, a re-implementation from Kim (2014) and Zhou et al. (2016a) and RNN-RE, a reimplementation from Sahu and Anand (2018). In all models we use bi-affine pairwise scoring to detect relations. 3.4 Table 1: Statistics of the CDR and CHR datasets. UniProt, KEGG and NCBI Taxonomy3 . If two chemical entities have a relation in Biochem4j, we consider them as positive instances in the dataset, otherwise as negative. 3.2 Data Pre-processing Table 1 shows the statistics for CDR and CHR datasets. For both datasets, the annotated entities can have more than one associated Knowledge Base (KB) ID. If there is at least one common KB ID between m"
P19-1423,P16-1200,0,0.0589536,"ees are often used to extract local dependencies of semantic relations (Culotta and Sorensen, 2004; Liu et al., 2015) in intra-sentence ∗ Corresponding author. relation extraction (RE). However, such dependencies are not adequate for inter-sentence RE, since different sentences have different dependency trees. Figure 1 illustrates such a case between Oxytocin and hypotension. To capture their relation, it is essential to connect the co-referring entities Oxytocin and Oxt. RNNs and CNNs, which are often used for intra-sentence RE (Zeng et al., 2014; dos Santos et al., 2015; Zhou et al., 2016b; Lin et al., 2016), are not effective on longer sequences (Sahu and Anand, 2018) thus failing to capture such non-local dependencies. We propose a novel inter-sentence RE model that builds a labelled edge Graph CNN (GCNN) model (Marcheggiani and Titov, 2017) on a document-level graph. The graph nodes correspond to words and edges represent local and nonlocal dependencies among them. The documentlevel graph is formed by connecting words with local dependencies from syntactic parsing and sequential information, as well as non-local dependencies from coreference resolution and other semantic dependencies (Peng et"
P19-1423,P15-2047,0,0.0321442,"ties often span across multiple sentences. In order to extract inter-sentence relations, most approaches utilise distant supervision to automatically generate document-level corpora (Peng et al., 2017; Song et al., 2018). Recently, Verga et al. (2018) introduced multi-instance learning (MIL) (Riedel et al., 2010; Surdeanu et al., 2012) to treat multiple mentions of target entities in a document. Inter-sentential relations depend not only on local but also on non-local dependencies. Dependency trees are often used to extract local dependencies of semantic relations (Culotta and Sorensen, 2004; Liu et al., 2015) in intra-sentence ∗ Corresponding author. relation extraction (RE). However, such dependencies are not adequate for inter-sentence RE, since different sentences have different dependency trees. Figure 1 illustrates such a case between Oxytocin and hypotension. To capture their relation, it is essential to connect the co-referring entities Oxytocin and Oxt. RNNs and CNNs, which are often used for intra-sentence RE (Zeng et al., 2014; dos Santos et al., 2015; Zhou et al., 2016b; Lin et al., 2016), are not effective on longer sequences (Sahu and Anand, 2018) thus failing to capture such non-loca"
P19-1423,N16-1116,0,0.0654007,"Missing"
P19-1423,P14-5010,0,0.00430317,"Gu et al. (2017) and Verga et al. (2018). In the CHR dataset, both directions were generated for each candidate chemical pair as chemicals can be either a reactant (first argument) or a product (second argument) in an interaction. We processed the datasets using the GENIA Sentence Splitter4 and GENIA tagger (Tsuruoka et al., 2005) for sentence splitting and word tokenisation, respectively. Syntactic dependencies were obtained using the Enju syntactic parser (Miyao and Tsujii, 2008) with predicate-argument structures. Coreference type edges were constructed using the Stanford CoreNLP software (Manning et al., 2014). 3.3 Baseline Models For the CDR dataset, we compare with five stateof-the-art models: SVM (Xu et al., 2016b), ensemble of feature-based and neural-based models (Zhou et al., 2016a), CNN and Maximum Entropy (Gu et al., 2017), Piece-wise CNN (Li et al., 2018) and Transformer (Verga et al., 2018). We additionally prepare and evaluate the following 3 http://biochem4j.org http://www.nactem.ac.uk/y-matsu/ geniass/ 4 Model Training We used 100-dimentional word embeddings trained on PubMed with GloVe (Pennington et al., 2014; TH et al., 2015). Unlike Verga et al. (2018), we used the pre-trained word"
P19-1423,D17-1159,0,0.541903,"e for inter-sentence RE, since different sentences have different dependency trees. Figure 1 illustrates such a case between Oxytocin and hypotension. To capture their relation, it is essential to connect the co-referring entities Oxytocin and Oxt. RNNs and CNNs, which are often used for intra-sentence RE (Zeng et al., 2014; dos Santos et al., 2015; Zhou et al., 2016b; Lin et al., 2016), are not effective on longer sequences (Sahu and Anand, 2018) thus failing to capture such non-local dependencies. We propose a novel inter-sentence RE model that builds a labelled edge Graph CNN (GCNN) model (Marcheggiani and Titov, 2017) on a document-level graph. The graph nodes correspond to words and edges represent local and nonlocal dependencies among them. The documentlevel graph is formed by connecting words with local dependencies from syntactic parsing and sequential information, as well as non-local dependencies from coreference resolution and other semantic dependencies (Peng et al., 2017). We infer relations between entities using MIL-based bi-affine pairwise scoring function (Verga et al., 2018) on the entity node representations. Our contribution is threefold. Firstly, we pro4309 Proceedings of the 57th Annual M"
P19-1423,P16-1105,1,0.795081,"i , d2i , respectively. As entities can have more than one mention, we calculate the relative position of a word from the closest target entity mention. For each word i, we concatenate the word and position representations into an input representation, xi = [wi ; d1i ; d2i ]. 2.2 Graph Construction In order to build a document-level graph for an entire abstract, we use the following categories of inter- and intra-sentence dependency edges, as shown with different colours in Figure 2. Syntactic dependency edge: The syntactic structure of a sentence reveals helpful clues for intrasentential RE (Miwa and Bansal, 2016). We thus use labelled syntactic dependency edges between the words of each sentence, by treating each syntactic dependency label as a different edge type. Coreference edge: As coreference is an important indicator of local and non-local dependencies (Ma et al., 2016), we connect co-referring phrases in a document using coreference type edges. Adjacent sentence edge: We connect the syntactic root of a sentence with the roots of the previous and next sentences with adjacent sentence type edges (Peng et al., 2017) for non-local dependencies between neighbouring sentences. Adjacent word edge: In"
P19-1423,J08-1002,0,0.0497906,"wn KB ID and removed relations between the same entity (self-relations). For the CDR dataset, we performed hypernym filtering similar to Gu et al. (2017) and Verga et al. (2018). In the CHR dataset, both directions were generated for each candidate chemical pair as chemicals can be either a reactant (first argument) or a product (second argument) in an interaction. We processed the datasets using the GENIA Sentence Splitter4 and GENIA tagger (Tsuruoka et al., 2005) for sentence splitting and word tokenisation, respectively. Syntactic dependencies were obtained using the Enju syntactic parser (Miyao and Tsujii, 2008) with predicate-argument structures. Coreference type edges were constructed using the Stanford CoreNLP software (Manning et al., 2014). 3.3 Baseline Models For the CDR dataset, we compare with five stateof-the-art models: SVM (Xu et al., 2016b), ensemble of feature-based and neural-based models (Zhou et al., 2016a), CNN and Maximum Entropy (Gu et al., 2017), Piece-wise CNN (Li et al., 2018) and Transformer (Verga et al., 2018). We additionally prepare and evaluate the following 3 http://biochem4j.org http://www.nactem.ac.uk/y-matsu/ geniass/ 4 Model Training We used 100-dimentional word embed"
P19-1423,Q17-1008,0,0.355248,"Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction. 1 Figure 1: Sentences with non-local dependencies between named entities. The red arrow represents a relation between co-referred entities and yellow arrows represent semantically dependent relations. Example adapted from the CDR dataset (Wei et al., 2015). Introduction Semantic relationships between named entities often span across multiple sentences. In order to extract inter-sentence relations, most approaches utilise distant supervision to automatically generate document-level corpora (Peng et al., 2017; Song et al., 2018). Recently, Verga et al. (2018) introduced multi-instance learning (MIL) (Riedel et al., 2010; Surdeanu et al., 2012) to treat multiple mentions of target entities in a document. Inter-sentential relations depend not only on local but also on non-local dependencies. Dependency trees are often used to extract local dependencies of semantic relations (Culotta and Sorensen, 2004; Liu et al., 2015) in intra-sentence ∗ Corresponding author. relation extraction (RE). However, such dependencies are not adequate for inter-sentence RE, since different sentences have different depend"
P19-1423,D14-1162,0,0.0912602,"Coreference type edges were constructed using the Stanford CoreNLP software (Manning et al., 2014). 3.3 Baseline Models For the CDR dataset, we compare with five stateof-the-art models: SVM (Xu et al., 2016b), ensemble of feature-based and neural-based models (Zhou et al., 2016a), CNN and Maximum Entropy (Gu et al., 2017), Piece-wise CNN (Li et al., 2018) and Transformer (Verga et al., 2018). We additionally prepare and evaluate the following 3 http://biochem4j.org http://www.nactem.ac.uk/y-matsu/ geniass/ 4 Model Training We used 100-dimentional word embeddings trained on PubMed with GloVe (Pennington et al., 2014; TH et al., 2015). Unlike Verga et al. (2018), we used the pre-trained word embeddings in place of sub-word embeddings to align with our word graphs. Due to the size of the CDR dataset, we merged the training and development sets to train the models, similarly to Xu et al. (2016a) and Gu et al. (2017). We report the performance as the average of five runs with different parameter initialisation seeds in terms of precision (P), recall (R) and F1-score. We used the frequencies of the edge types in the training set to choose the top-N edges in Section 2.3. We refer to the supplementary materials"
P19-1423,P15-1061,0,0.0383207,"on non-local dependencies. Dependency trees are often used to extract local dependencies of semantic relations (Culotta and Sorensen, 2004; Liu et al., 2015) in intra-sentence ∗ Corresponding author. relation extraction (RE). However, such dependencies are not adequate for inter-sentence RE, since different sentences have different dependency trees. Figure 1 illustrates such a case between Oxytocin and hypotension. To capture their relation, it is essential to connect the co-referring entities Oxytocin and Oxt. RNNs and CNNs, which are often used for intra-sentence RE (Zeng et al., 2014; dos Santos et al., 2015; Zhou et al., 2016b; Lin et al., 2016), are not effective on longer sequences (Sahu and Anand, 2018) thus failing to capture such non-local dependencies. We propose a novel inter-sentence RE model that builds a labelled edge Graph CNN (GCNN) model (Marcheggiani and Titov, 2017) on a document-level graph. The graph nodes correspond to words and edges represent local and nonlocal dependencies among them. The documentlevel graph is formed by connecting words with local dependencies from syntactic parsing and sequential information, as well as non-local dependencies from coreference resolution an"
P19-1423,D18-1246,0,0.108412,"that all the types in the graph are effective for inter-sentence relation extraction. 1 Figure 1: Sentences with non-local dependencies between named entities. The red arrow represents a relation between co-referred entities and yellow arrows represent semantically dependent relations. Example adapted from the CDR dataset (Wei et al., 2015). Introduction Semantic relationships between named entities often span across multiple sentences. In order to extract inter-sentence relations, most approaches utilise distant supervision to automatically generate document-level corpora (Peng et al., 2017; Song et al., 2018). Recently, Verga et al. (2018) introduced multi-instance learning (MIL) (Riedel et al., 2010; Surdeanu et al., 2012) to treat multiple mentions of target entities in a document. Inter-sentential relations depend not only on local but also on non-local dependencies. Dependency trees are often used to extract local dependencies of semantic relations (Culotta and Sorensen, 2004; Liu et al., 2015) in intra-sentence ∗ Corresponding author. relation extraction (RE). However, such dependencies are not adequate for inter-sentence RE, since different sentences have different dependency trees. Figure 1"
P19-1423,P16-1220,0,0.0434194,"Missing"
P19-1423,D12-1042,0,0.041194,"non-local dependencies between named entities. The red arrow represents a relation between co-referred entities and yellow arrows represent semantically dependent relations. Example adapted from the CDR dataset (Wei et al., 2015). Introduction Semantic relationships between named entities often span across multiple sentences. In order to extract inter-sentence relations, most approaches utilise distant supervision to automatically generate document-level corpora (Peng et al., 2017; Song et al., 2018). Recently, Verga et al. (2018) introduced multi-instance learning (MIL) (Riedel et al., 2010; Surdeanu et al., 2012) to treat multiple mentions of target entities in a document. Inter-sentential relations depend not only on local but also on non-local dependencies. Dependency trees are often used to extract local dependencies of semantic relations (Culotta and Sorensen, 2004; Liu et al., 2015) in intra-sentence ∗ Corresponding author. relation extraction (RE). However, such dependencies are not adequate for inter-sentence RE, since different sentences have different dependency trees. Figure 1 illustrates such a case between Oxytocin and hypotension. To capture their relation, it is essential to connect the"
P19-1423,K17-1045,0,0.0152641,"in comparison with the state-of-the-art. tions. They restricted the relation candidates in up to two-span sentences. Verga et al. (2018) considered multi-instance learning for document-level RE. Our work is different from Verga et al. (2018) in that we replace Transformer with a GCNN model for full-abstract encoding using non-local dependencies such as entity coreference. GCNN was firstly proposed by Kipf and Welling (2017) and applied on citation networks and knowledge graph datasets. It was later used for semantic role labelling (Marcheggiani and Titov, 2017), multi-document summarization (Yasunaga et al., 2017) and temporal relation extraction (Vashishth et al., 2018). Zhang et al. (2018) used a GCNN on a dependency tree for intrasentence RE. Unlike previous work, we introduced a GCNN on a document-level graph, with both intra- and inter-sentence dependencies for intersentence RE. 6 Conclusion Figure 3: Performance of GCNN model on the CDR development set when using the top-N most frequent edge types and consider the rest as a single “rare” type. Model GCNN (best) − Adjacent word − Syntactic dependency − Coreference − Self-node − Adjacent sentence Overall 57.19 55.75 56.12 56.44 56.85 57.00 Intra 63"
P19-1423,C14-1220,0,0.293244,"only on local but also on non-local dependencies. Dependency trees are often used to extract local dependencies of semantic relations (Culotta and Sorensen, 2004; Liu et al., 2015) in intra-sentence ∗ Corresponding author. relation extraction (RE). However, such dependencies are not adequate for inter-sentence RE, since different sentences have different dependency trees. Figure 1 illustrates such a case between Oxytocin and hypotension. To capture their relation, it is essential to connect the co-referring entities Oxytocin and Oxt. RNNs and CNNs, which are often used for intra-sentence RE (Zeng et al., 2014; dos Santos et al., 2015; Zhou et al., 2016b; Lin et al., 2016), are not effective on longer sequences (Sahu and Anand, 2018) thus failing to capture such non-local dependencies. We propose a novel inter-sentence RE model that builds a labelled edge Graph CNN (GCNN) model (Marcheggiani and Titov, 2017) on a document-level graph. The graph nodes correspond to words and edges represent local and nonlocal dependencies among them. The documentlevel graph is formed by connecting words with local dependencies from syntactic parsing and sequential information, as well as non-local dependencies from"
P19-1423,W15-3820,1,0.847899,"ere constructed using the Stanford CoreNLP software (Manning et al., 2014). 3.3 Baseline Models For the CDR dataset, we compare with five stateof-the-art models: SVM (Xu et al., 2016b), ensemble of feature-based and neural-based models (Zhou et al., 2016a), CNN and Maximum Entropy (Gu et al., 2017), Piece-wise CNN (Li et al., 2018) and Transformer (Verga et al., 2018). We additionally prepare and evaluate the following 3 http://biochem4j.org http://www.nactem.ac.uk/y-matsu/ geniass/ 4 Model Training We used 100-dimentional word embeddings trained on PubMed with GloVe (Pennington et al., 2014; TH et al., 2015). Unlike Verga et al. (2018), we used the pre-trained word embeddings in place of sub-word embeddings to align with our word graphs. Due to the size of the CDR dataset, we merged the training and development sets to train the models, similarly to Xu et al. (2016a) and Gu et al. (2017). We report the performance as the average of five runs with different parameter initialisation seeds in terms of precision (P), recall (R) and F1-score. We used the frequencies of the edge types in the training set to choose the top-N edges in Section 2.3. We refer to the supplementary materials for the details o"
P19-1423,D18-1244,0,0.16069,"idates in up to two-span sentences. Verga et al. (2018) considered multi-instance learning for document-level RE. Our work is different from Verga et al. (2018) in that we replace Transformer with a GCNN model for full-abstract encoding using non-local dependencies such as entity coreference. GCNN was firstly proposed by Kipf and Welling (2017) and applied on citation networks and knowledge graph datasets. It was later used for semantic role labelling (Marcheggiani and Titov, 2017), multi-document summarization (Yasunaga et al., 2017) and temporal relation extraction (Vashishth et al., 2018). Zhang et al. (2018) used a GCNN on a dependency tree for intrasentence RE. Unlike previous work, we introduced a GCNN on a document-level graph, with both intra- and inter-sentence dependencies for intersentence RE. 6 Conclusion Figure 3: Performance of GCNN model on the CDR development set when using the top-N most frequent edge types and consider the rest as a single “rare” type. Model GCNN (best) − Adjacent word − Syntactic dependency − Coreference − Self-node − Adjacent sentence Overall 57.19 55.75 56.12 56.44 56.85 57.00 Intra 63.43 62.53 62.89 63.27 63.84 63.99 Inter 36.90 35.61 34.75 35.65 33.20 35.20 Tab"
P19-1423,P18-1149,0,0.0734011,"ge types. Hence, to include the node information itself into the representation, we form selfnode type edges on all the nodes of the graph. 2.3 GCNN Layer We compute the representation of each input word i by applying GCNN (Kipf and Welling, 2017; Defferrard et al., 2016) on the constructed document graph. GCNN is an advanced version of CNN for graph encoding that learns semantic representations for the graph nodes, while preserving its structural information. In order to learn edge type-specific representations, we use a labelled edge GCNN, which keeps separate parameters for each edge type (Vashishth et al., 2018). The GCNN iteratively updates the representation of each input word i as follows:    X  k xk+1 =f Wl(i,u) xku + bkl(i,u)  , i u∈ν(i) where xk+1 is the i-th word representation rei sulted from the k-th GCNN block, ν(i) is a set k of neighbouring nodes to i, Wl(i,u) and bkl(i,u) are the parameters of the k-th block for edge type l between nodes i and u. We stack K GCNN blocks to accumulate information from distant neighbouring nodes and use edge-wise gating to control information from neighbouring nodes. Similar to Marcheggiani and Titov (2017), we maintain separate parameters for each ed"
P19-1423,P16-2034,0,0.641295,"ncies. Dependency trees are often used to extract local dependencies of semantic relations (Culotta and Sorensen, 2004; Liu et al., 2015) in intra-sentence ∗ Corresponding author. relation extraction (RE). However, such dependencies are not adequate for inter-sentence RE, since different sentences have different dependency trees. Figure 1 illustrates such a case between Oxytocin and hypotension. To capture their relation, it is essential to connect the co-referring entities Oxytocin and Oxt. RNNs and CNNs, which are often used for intra-sentence RE (Zeng et al., 2014; dos Santos et al., 2015; Zhou et al., 2016b; Lin et al., 2016), are not effective on longer sequences (Sahu and Anand, 2018) thus failing to capture such non-local dependencies. We propose a novel inter-sentence RE model that builds a labelled edge Graph CNN (GCNN) model (Marcheggiani and Titov, 2017) on a document-level graph. The graph nodes correspond to words and edges represent local and nonlocal dependencies among them. The documentlevel graph is formed by connecting words with local dependencies from syntactic parsing and sequential information, as well as non-local dependencies from coreference resolution and other semantic de"
P19-1423,N18-1080,0,0.297141,"h are effective for inter-sentence relation extraction. 1 Figure 1: Sentences with non-local dependencies between named entities. The red arrow represents a relation between co-referred entities and yellow arrows represent semantically dependent relations. Example adapted from the CDR dataset (Wei et al., 2015). Introduction Semantic relationships between named entities often span across multiple sentences. In order to extract inter-sentence relations, most approaches utilise distant supervision to automatically generate document-level corpora (Peng et al., 2017; Song et al., 2018). Recently, Verga et al. (2018) introduced multi-instance learning (MIL) (Riedel et al., 2010; Surdeanu et al., 2012) to treat multiple mentions of target entities in a document. Inter-sentential relations depend not only on local but also on non-local dependencies. Dependency trees are often used to extract local dependencies of semantic relations (Culotta and Sorensen, 2004; Liu et al., 2015) in intra-sentence ∗ Corresponding author. relation extraction (RE). However, such dependencies are not adequate for inter-sentence RE, since different sentences have different dependency trees. Figure 1 illustrates such a case betwee"
P19-1423,P04-1054,0,\N,Missing
piao-etal-2008-clustering,W04-1807,0,\N,Missing
rak-etal-2012-collaborative,kano-etal-2010-u,1,\N,Missing
rak-etal-2012-collaborative,W04-1213,0,\N,Missing
rak-etal-2014-interoperability,kano-etal-2010-u,1,\N,Missing
rak-etal-2014-interoperability,hernandez-2012-tackling,0,\N,Missing
rak-etal-2014-interoperability,pazienza-etal-2012-pearl,0,\N,Missing
rak-etal-2014-interoperability,bank-schierle-2012-survey,0,\N,Missing
rak-etal-2014-interoperability,W13-2311,1,\N,Missing
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
S16-1093,S12-1059,0,0.0710363,"Missing"
S16-1093,P14-1023,0,0.0524117,"external 614 Proceedings of SemEval-2016, pages 614–620, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics knowledge resources (e.g., WordNet). As an example, Liu et al. (2015) used the shortest path that links two words in the WordNet taxonomy as a similarity measure between words. To calculate a similarity score between sentences, the authors used the sum of the similarity scores of the constituent words. Content-based features are based upon the distributional similarity of words and sentences. Distributional semantics methods (Mikolov et al., 2013; Baroni et al., 2014) encode the lexical context of words into a vector representation. A vector representation of a sentence may be estimated as a function of the vectors of the constituent words (Mitchell and Lapata, 2010). The semantic relatedness between sentences is measured using the cosine of the angle of the composed sentence vectors. 3 Feature Sources We have collected a variety of resources to help us create semantic similarity features. Some of these (Subsections 3.2 and 3.4) give features at the level of the sentence itself. Other resources (Subsections 3.1, 3.3, 3.5. 3.6 and 3.7) give features at the"
S16-1093,J06-1003,0,0.125348,"varying the number of topics; starting from a small number of 5 topics which resulted in a coarse-grained topic-based representation to a larger number of 800 topics which produced a more finegrained representation. In our experiments, we used the freely available MALLET toolkit (McCallum, 2002). Additionally, we performed hyper-parameter optimisation for every 10 Gibbs sampling iterations and set the total number of iterations to 2, 000. 3.5 WordNet For WordNet-based similarity between a pair of words we have chosen Jiang-Conrath (Jiang and Conrath, 1997) similarity based on an evaluation by Budanitsky and Hirst (2006). To compute the score, we lemmatise the words using Stanford CoreNLP (Manning et al., 2014), find corresponding synsets in Princeton WordNet (Fellbaum, 1998) and obtain the Jiang-Conrath value using the WS4J library2 . 2 615 Lexical paraphrase scores https://code.google.com/archive/p/ws4j/ 3.6 Character string Sometimes semantically related words are very similar as sequences of characters, e.g. cooperate and co-operate or recover and recovery. To handle such cases we compute Levenshtein distance (Levenshtein, 1966) between words. To keep the similarity score xl in the [0, 1] range, we adjust"
S16-1093,W14-3348,0,0.106732,"ndow is 5 for both of the models. The numbers of dimensions in the resulting vectors are 150,000 and 300 for the count-based and the CBOW models respectively. 3.2 Machine translation A pair of input sentences can be considered as the input and output of a machine translation system. Therefore, we can apply machine translation (MT) 1 http://www.statmt.org/wmt14/ translation-task.html#download metrics to estimate the semantic relatedness of the input pair. Specifically, we used three popular MT metrics: BLEU (Papineni et al., 2002), Translation Edit Rate (TER) (Snover et al., 2006), and METEOR (Denkowski and Lavie, 2014). 3.3 Another promising resource for similarity estimation is the lexical paraphrase database (PPDB) by Ganitkevitch et al. (2013). Each of the word pairs in the PPDB has a set of 31 different features (Ganitkevitch and Callison-Burch, 2014). In this work, we calculate the similarity of a word pair by using the formula that Ganitkevitch and Callison-Burch (2014) recommended to measure paraphrases’ quality. However, for word pairs that have been seen very rarely, i.e., their rarity penalty score in PPDB is higher than 0.1, we simply set the similarity score at 0 instead of applying the formula."
S16-1093,ganitkevitch-callison-burch-2014-multilingual,0,0.0213651,"the input and output of a machine translation system. Therefore, we can apply machine translation (MT) 1 http://www.statmt.org/wmt14/ translation-task.html#download metrics to estimate the semantic relatedness of the input pair. Specifically, we used three popular MT metrics: BLEU (Papineni et al., 2002), Translation Edit Rate (TER) (Snover et al., 2006), and METEOR (Denkowski and Lavie, 2014). 3.3 Another promising resource for similarity estimation is the lexical paraphrase database (PPDB) by Ganitkevitch et al. (2013). Each of the word pairs in the PPDB has a set of 31 different features (Ganitkevitch and Callison-Burch, 2014). In this work, we calculate the similarity of a word pair by using the formula that Ganitkevitch and Callison-Burch (2014) recommended to measure paraphrases’ quality. However, for word pairs that have been seen very rarely, i.e., their rarity penalty score in PPDB is higher than 0.1, we simply set the similarity score at 0 instead of applying the formula. 3.4 Topic modelling We induce a topic-based vector representation of sentences by applying the Latent Dirichlet Allocation (LDA) method (Blei et al., 2003). We hypothesise that a varying granularity of vectorrepresentations provide compleme"
S16-1093,N13-1092,0,0.0622369,"Missing"
S16-1093,O97-1002,0,0.307859,"we extract 26 different topic-based vector representations by varying the number of topics; starting from a small number of 5 topics which resulted in a coarse-grained topic-based representation to a larger number of 800 topics which produced a more finegrained representation. In our experiments, we used the freely available MALLET toolkit (McCallum, 2002). Additionally, we performed hyper-parameter optimisation for every 10 Gibbs sampling iterations and set the total number of iterations to 2, 000. 3.5 WordNet For WordNet-based similarity between a pair of words we have chosen Jiang-Conrath (Jiang and Conrath, 1997) similarity based on an evaluation by Budanitsky and Hirst (2006). To compute the score, we lemmatise the words using Stanford CoreNLP (Manning et al., 2014), find corresponding synsets in Princeton WordNet (Fellbaum, 1998) and obtain the Jiang-Conrath value using the WS4J library2 . 2 615 Lexical paraphrase scores https://code.google.com/archive/p/ws4j/ 3.6 Character string Sometimes semantically related words are very similar as sequences of characters, e.g. cooperate and co-operate or recover and recovery. To handle such cases we compute Levenshtein distance (Levenshtein, 1966) between word"
S16-1093,S12-1061,0,0.0423498,"the heart of the semantic similarity task. In our solution, we have incorporated several categories of features from a variety of sources. Our approach covers both low-level visual features such Related Work The presented system has been submitted to the Semantic Textual Similarity (STS) shared task at SemEval 2016, which has been organised since 2012 (Agirre et al., 2016). Existing approaches to the problem adopt a plethora of similarity measures including string-based, content-based and knowledgebased methods. String-based methods (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007; Jimenez et al., 2012) exploit surface features (e.g., character n-grams, lemmas) to compute a semantic similarity score between two sentences. B¨ar et al. (2012) showed that string-based features improve performance when using machine learning. Knowledge-based features (Mihalcea et al., 2006; Gabrilovich and Markovitch, 2007) estimate the semantic similarity of textual units using external 614 Proceedings of SemEval-2016, pages 614–620, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics knowledge resources (e.g., WordNet). As an example, Liu et al. (2015) used the shortest pa"
S16-1093,S15-2014,0,0.0307234,"tsopoulos, 2007; Jimenez et al., 2012) exploit surface features (e.g., character n-grams, lemmas) to compute a semantic similarity score between two sentences. B¨ar et al. (2012) showed that string-based features improve performance when using machine learning. Knowledge-based features (Mihalcea et al., 2006; Gabrilovich and Markovitch, 2007) estimate the semantic similarity of textual units using external 614 Proceedings of SemEval-2016, pages 614–620, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics knowledge resources (e.g., WordNet). As an example, Liu et al. (2015) used the shortest path that links two words in the WordNet taxonomy as a similarity measure between words. To calculate a similarity score between sentences, the authors used the sum of the similarity scores of the constituent words. Content-based features are based upon the distributional similarity of words and sentences. Distributional semantics methods (Mikolov et al., 2013; Baroni et al., 2014) encode the lexical context of words into a vector representation. A vector representation of a sentence may be estimated as a function of the vectors of the constituent words (Mitchell and Lapata,"
S16-1093,W07-1407,0,0.0299073,"larity-scoring systems? The answer is at the heart of the semantic similarity task. In our solution, we have incorporated several categories of features from a variety of sources. Our approach covers both low-level visual features such Related Work The presented system has been submitted to the Semantic Textual Similarity (STS) shared task at SemEval 2016, which has been organised since 2012 (Agirre et al., 2016). Existing approaches to the problem adopt a plethora of similarity measures including string-based, content-based and knowledgebased methods. String-based methods (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007; Jimenez et al., 2012) exploit surface features (e.g., character n-grams, lemmas) to compute a semantic similarity score between two sentences. B¨ar et al. (2012) showed that string-based features improve performance when using machine learning. Knowledge-based features (Mihalcea et al., 2006; Gabrilovich and Markovitch, 2007) estimate the semantic similarity of textual units using external 614 Proceedings of SemEval-2016, pages 614–620, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics knowledge resources (e.g., WordNet). As an example, Liu et al. (201"
S16-1093,P14-5010,0,0.0144893,"grained topic-based representation to a larger number of 800 topics which produced a more finegrained representation. In our experiments, we used the freely available MALLET toolkit (McCallum, 2002). Additionally, we performed hyper-parameter optimisation for every 10 Gibbs sampling iterations and set the total number of iterations to 2, 000. 3.5 WordNet For WordNet-based similarity between a pair of words we have chosen Jiang-Conrath (Jiang and Conrath, 1997) similarity based on an evaluation by Budanitsky and Hirst (2006). To compute the score, we lemmatise the words using Stanford CoreNLP (Manning et al., 2014), find corresponding synsets in Princeton WordNet (Fellbaum, 1998) and obtain the Jiang-Conrath value using the WS4J library2 . 2 615 Lexical paraphrase scores https://code.google.com/archive/p/ws4j/ 3.6 Character string Sometimes semantically related words are very similar as sequences of characters, e.g. cooperate and co-operate or recover and recovery. To handle such cases we compute Levenshtein distance (Levenshtein, 1966) between words. To keep the similarity score xl in the [0, 1] range, we adjust the obtained distance d by computing xl = (l − d)/l, where l denotes the length of the long"
S16-1093,P02-1040,0,0.09552,"we tokenised and transferred the corpus into lowercase text. The size of the context window is 5 for both of the models. The numbers of dimensions in the resulting vectors are 150,000 and 300 for the count-based and the CBOW models respectively. 3.2 Machine translation A pair of input sentences can be considered as the input and output of a machine translation system. Therefore, we can apply machine translation (MT) 1 http://www.statmt.org/wmt14/ translation-task.html#download metrics to estimate the semantic relatedness of the input pair. Specifically, we used three popular MT metrics: BLEU (Papineni et al., 2002), Translation Edit Rate (TER) (Snover et al., 2006), and METEOR (Denkowski and Lavie, 2014). 3.3 Another promising resource for similarity estimation is the lexical paraphrase database (PPDB) by Ganitkevitch et al. (2013). Each of the word pairs in the PPDB has a set of 31 different features (Ganitkevitch and Callison-Burch, 2014). In this work, we calculate the similarity of a word pair by using the formula that Ganitkevitch and Callison-Burch (2014) recommended to measure paraphrases’ quality. However, for word pairs that have been seen very rarely, i.e., their rarity penalty score in PPDB i"
S16-1093,2006.amta-papers.25,0,0.0131658,"e text. The size of the context window is 5 for both of the models. The numbers of dimensions in the resulting vectors are 150,000 and 300 for the count-based and the CBOW models respectively. 3.2 Machine translation A pair of input sentences can be considered as the input and output of a machine translation system. Therefore, we can apply machine translation (MT) 1 http://www.statmt.org/wmt14/ translation-task.html#download metrics to estimate the semantic relatedness of the input pair. Specifically, we used three popular MT metrics: BLEU (Papineni et al., 2002), Translation Edit Rate (TER) (Snover et al., 2006), and METEOR (Denkowski and Lavie, 2014). 3.3 Another promising resource for similarity estimation is the lexical paraphrase database (PPDB) by Ganitkevitch et al. (2013). Each of the word pairs in the PPDB has a set of 31 different features (Ganitkevitch and Callison-Burch, 2014). In this work, we calculate the similarity of a word pair by using the formula that Ganitkevitch and Callison-Burch (2014) recommended to measure paraphrases’ quality. However, for word pairs that have been seen very rarely, i.e., their rarity penalty score in PPDB is higher than 0.1, we simply set the similarity sco"
saetre-etal-2008-connecting,N03-2020,1,\N,Missing
saetre-etal-2008-connecting,P06-1128,1,\N,Missing
saetre-etal-2008-connecting,P05-1011,1,\N,Missing
saetre-etal-2008-connecting,nenadic-etal-2006-towards,1,\N,Missing
spasic-etal-2002-tuning,W02-1408,1,\N,Missing
spasic-etal-2002-tuning,C00-1077,1,\N,Missing
spasic-etal-2002-tuning,C92-2082,0,\N,Missing
thompson-etal-2008-building,N03-4009,0,\N,Missing
thompson-etal-2008-building,W06-0602,0,\N,Missing
thompson-etal-2008-building,W04-3111,0,\N,Missing
thompson-etal-2008-building,J05-1004,0,\N,Missing
W02-1408,C94-2167,1,0.398869,"since technical terms semantically characterise documents and therefore represent starting place for knowledge acquisition tasks. For example, Mima et al. (2002) introduce TIMS, a terminology-based knowledge acquisition system, which integrates automatic term recognition, term variation management, contextbased automatic term clustering, ontology-based inference, and intelligent tag information retrieval. The system’s aim is to provide efficient access and integration of heterogeneous biological textual data and databases. There are numerous approaches to ATR. Some methods (Bourigault (1992), Ananiadou (1994)) rely purely on linguistic information, namely morpho-syntactic features of term candidates. Recently, hybrid approaches combining linguistic and statistical knowledge are becoming increasingly used (Frantzi et al. (2000), Nakagawa et al. (1998)). There is a range of clustering and classification approaches that are based on statistical measures of word co-occurrences (e.g. Ushioda (1996)), or syntactic information derived from corpora (e.g. Grefenstette (1994)). However, few of them deal with term clustering: Maynard and Ananiadou (2000) present a method that uses manually defined semantic f"
W02-1408,C92-3150,0,0.138543,"terminology-based, since technical terms semantically characterise documents and therefore represent starting place for knowledge acquisition tasks. For example, Mima et al. (2002) introduce TIMS, a terminology-based knowledge acquisition system, which integrates automatic term recognition, term variation management, contextbased automatic term clustering, ontology-based inference, and intelligent tag information retrieval. The system’s aim is to provide efficient access and integration of heterogeneous biological textual data and databases. There are numerous approaches to ATR. Some methods (Bourigault (1992), Ananiadou (1994)) rely purely on linguistic information, namely morpho-syntactic features of term candidates. Recently, hybrid approaches combining linguistic and statistical knowledge are becoming increasingly used (Frantzi et al. (2000), Nakagawa et al. (1998)). There is a range of clustering and classification approaches that are based on statistical measures of word co-occurrences (e.g. Ushioda (1996)), or syntactic information derived from corpora (e.g. Grefenstette (1994)). However, few of them deal with term clustering: Maynard and Ananiadou (2000) present a method that uses manually"
W02-1408,C00-1077,1,0.830035,"nd functional similarities between terms to define a combined similarity measure. The approach has been tested and evaluated in the domain of molecular biology, and preliminary results are presented. Introduction In a knowledge intensive discipline such as molecular biology, the vast and constantly increasing amount of information demands innovative techniques to gather and systematically structure knowledge, usually available only from text/document resources. In order to discover new knowledge, one has to identify main concepts, which are linguistically represented by domain specific terms (Maynard and Ananiadou (2000)). There is an increased amount of new terms that represent newly created concepts. Since existing term dictionaries usually do not meet the needs of specialists, automatic term extraction tools are indispensable for efficient term discovery and dynamic update of term dictionaries. However, automatic term recognition (ATR) is not the ultimate aim: terms recognised should be related to existing knowledge and/or to each other. This entails the fact that terms should be classified or clustered so that semantically similar terms are grouped together. Classification and/or clustering of terms are i"
W02-1408,C02-1083,1,0.801574,"Missing"
W02-1408,C92-2082,0,\N,Missing
W02-1408,nenadic-etal-2002-automatic,1,\N,Missing
W02-1408,C96-2212,0,\N,Missing
W03-1303,C00-1077,1,0.876808,"Missing"
W03-1303,W02-1408,1,0.875831,"Missing"
W03-1303,W03-1316,1,0.862619,"e term is more independent if the set of its host terms is more versatile. Term distribution in top-ranked candidate terms is further improved by taking into account their context. The relevant context words, including nouns, verbs and adjectives, are extracted and assigned weights based on how frequently they cooccur with top-ranked term candidates. Subsequently, context factors are assigned to candidate terms according to their co-occurrence with topranked context words. Finally, new termhood estimations (NC-values) are calculated as a linear combination of the C-values and context factors. Nenadic et al. (2003a) modified the C/NC-value to recognise acronyms as a special type of singleword terms, and, thus, enhanced the recall of the method. On the other hand, the modified version incorporates the unification of term variants into the linguistic part of the method, which also improved the precision, since the statistical analysis is more reliable when performed over classes of equivalent term variants instead of separate terms. 3.2 Domain-Specific Verb Recognition Verbs are extracted from the corpus and ranked based on the frequency of occurrence and the frequency of their co-occurrence with terms."
W03-1303,spasic-etal-2002-tuning,1,0.758002,"ach of these verbs is associated with a set of classes and terms it co-occurs with. Let Ci = {ci,1, ci,2, ... , ci,mi} denote a set of classes assigned automatically to the verb vi (1 ≤ i ≤ n) by a learning algorithm based on the information found in the corpus and the training ontology. As indicated earlier, we define such set to be a verb complementation pattern for the given verb. 4.1 Statistical Analysis Figure 1. Learning the complementation pattern for the verb bind 4 As we planned to use verb complementation patterns for term classification, we modified the original learning algorithm (Spasic et al., 2002) by attaching the frequency information to terms and their classes. When substituting a less general class by its more general counterpart, 2 the frequency information is updated by summing the two respective frequencies of occurrence. In the final verb complementation pattern, each class ci,j has the frequency feature fi,j, which aggregates the frequency of co-occurrence with vi (1 ≤ i ≤ n; 1 ≤ j ≤ mi) for the given class and its subclasses. The frequency information is used to estimate the class probabilities given a verb, P(ci,j |vi): Term Classification Method pi , j = The verb complementa"
W03-1303,nenadic-etal-2002-automatic,1,\N,Missing
W03-1316,nenadic-etal-2002-automatic,1,0.911198,"variability. In the first case, we used the EngCG POS tagger (Voutilainen and Heikkila, 1993) to generate lemmas, so that lemmatised words were used as features, while, in the second case, we generated stems by the Porter’s algorithm (Porter, 1980). Analogously to words, the same idf-based measure was used for weights, and experiments were also performed with all features and with the features appearing in no less than two documents. 3.2 Terms as features Many literature-mining techniques rely heavily on the identification of main concepts, linguistically represented by domain specific terms (Nenadic et al., 2002b). Terms represent the most important concepts in a domain and have been used to characterise documents semantically (Maynard and Ananiadou, 2002). Since terms are semantic indicators used in scientific discourse, we hypothesised that they might be useful classification features. The high neology rate for terms makes existing glossaries incomplete for active and time-limited research, and thus automatic term extraction tools are needed for efficient terminological processing. In order to automatically generate term as features, we have used an enhanced version of the C-value method (Frantzi e"
W03-1316,W02-0301,0,0.118968,"ypically rely on supervised machine learning techniques that examine the wider context in which terms are used. For example, Raychaudhuri et al. (2002) used document-based word counts and naive Bayesian classification, maximum entropy modelling and nearest-neighbour classification to assign the GO ontology codes to a set of genes. Recently, support-vector machines (SVMs, (Vapnik, 1995)) have been widely used as fast, effective and reliable means for text-based classification, both for document classification (Joachims, 1998) and classification of specific named entities (Stapley et al., 2002; Kazama et al., 2002). Regardless of the learning approach and target entities (documents or terms), different types of text features have been employed for the classification task. For example, a bag-of-words approach was used by Stapley et al. (2002) to classify proteins, while Collier et al. (2001) used orthographic features to classify different biological entities. On the other hand, Hatzivassiloglou et al. (2001) experimented with morphological, distributional and shallow-syntactic information to discriminate between proteins, genes and RNAs. In this paper we analyse the impact of different types of features"
W03-1316,C00-1077,1,0.936372,"Missing"
W03-1316,C96-2212,0,0.0797542,"Missing"
W03-2911,C94-2167,1,0.713069,"types of concepts (e.g. gene and protein names in biomedicine), these are only guidelines and as such do not impose restrictions to domain experts, who frequently introduce ad-hoc terms. Thus, the lack of clear naming conventions makes the automatic term recognition (ATR) task difficult even for languages that are not morphologically and derivationally rich. ATR tools have been developed for English (Frantzi et al., 2000), French (Jacquemin, 2001), Japanese (Nakagawa and Mori, 2000), etc. Some methods rely purely on linguistic information, namely morpho-syntactic features of term candidates (Ananiadou, 1994). Hybrid approaches combining linguistic patterns and statistical measures (e.g. (Frantzi et al., 2000)) and machine-learning techniques (e.g. (Hatzivassiloglou et al., 2001)) have been also used. However, few studies have been done for morphologically rich Slavic languages. For example, Vintar (2000) presented two methods for extraction of terminological collocations in order to assist the translation process in Slovene. The statistical approach was based on the mutual expectation and LocalMax measures, and involved collocation extraction from raw text. The extracted collocations were filtere"
W04-1812,C96-2212,0,0.0186125,"in humans → human cancer human’s cancer human carcinoma 3.3. Term clustering } Beside term recognition, term clustering is an indispensable component of the literature mining process. Since terminological opacity and polysemy are very common in molecular biology and biomedicine, term clustering is essential for the semantic integration of terms, the construction of domain ontologies and semantic tagging. ATC in our system is performed using a hierarchical clustering method in which clusters are merged based on average mutual information measuring how strongly terms are related to one another [7]. Terms automatically recognized by the NC-value method and their co-occurrences are used as input, and a dendrogram of terms is produced as output. Parallel symmetric processing is used for high-speed clustering. The calculated term cluster information is encoded and used for calculating semantic similarities in SCE component. More precisely, the similarity between two individual terms is determined according to their position in a dendrogram. Also a commonality measure is defined as the number of shared ancestors between two terms in the dendrogram, and a positional 85 86 CompuTerm 2004 Post"
W05-1304,W03-1018,1,\N,Missing
W05-1304,J96-1002,0,\N,Missing
W05-1304,nenadic-etal-2002-automatic,1,\N,Missing
W07-1505,brants-hansen-2002-developments,0,0.0214672,"nal) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al., 1993), or semantic annotations, such as the one underlying ACE (Doddington et al., 2004), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in terms of morpho-syntactic, syntactic and semantic annotations (named entities and events). In the meantime, the Annotation"
W07-1505,declerck-2006-synaf,0,0.0662362,"4), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in terms of morpho-syntactic, syntactic and semantic annotations (named entities and events). In the meantime, the Annotation Compatibility Working Group (Meyers, 2006) began to concentrate its activities on the mutual compatibility of annotation schemata for, e.g., POS tagging, treebanking, role labeling, time"
W07-1505,C96-1079,0,0.0606984,"(cf. Figure 16) links annotated (named) entities to the ontologies and databases through appropriate attributes, viz. ontologyEntry and sdbEntry. The attribute specificType specifies the analyzed entity in a more detailed way (e.g., Organism can be specified through the species values ‘human’, ‘mouse’, ‘rat’, etc.) The subtypes are currently being developed in the bio-medical domain and cover, e.g., genes, pro39 teins, organisms, diseases, variations. This hierarchy can easily be extended or supplemented with entities from other domains. For illustration purposes, we extended it here by MUC (Grishman and Sundheim, 1996) entity types such as Person, Organization, etc. This scheme is still under construction and will soon also incorporate the representation of relationships between entities and domain-specific events. The general type Relation will then be extended with specific conceptual relations such as location, part-of, etc. The representation of events will be covered by a type which aggregates pre-defined relations between entities and the event mention. An event type such as InhibitionEvent would link the text spans in the sentence ‘protein A inhibits protein B’ in attributes agent (‘protein A’), pati"
W07-1505,W02-1706,0,0.0720681,"Missing"
W07-1505,ide-etal-2000-xces,0,0.0297293,"nres. The Dublin Core Metadata Initiative3 established a de facto standard for the Semantic Web.4 For (computational) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al., 1993), or semantic annotations, such as the one underlying ACE (Doddington et al., 2004), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in ter"
W07-1505,W03-0804,0,0.0538625,"those single modules which serve, by and large, the same functionality? Second, how can we build NLP systems by composing them, at the abstract level of functional specification, from these already existing component building blocks disregarding concrete implementation matters? Yet another burning issue relates to the increasing availability of multiple metadata annotations both in corpora and language processors. If alternative annotation tag sets are chosen for the same functional task a ‘data conversion’ problem is created which should be solved at the abstract specification level as well (Ide et al., 2003). Software engineering methodology points out that these requirements are best met by properly identifying input/output capabilities of constituent components and by specifying a general data model (e.g., based on UML (Rumbaugh et al., 1999)) in order to get rid of the low-level implementation (i.e., coding) layer. A particularly promising proposal along this line of thought is the Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004) originating from IBM research activities.1 UIMA is but the latest attempt in a series of proposals concerned with more generic NLP e"
W07-1505,J93-2004,0,0.0430432,"gn of annotation schemata for language resources and their standardization have a long-standing tradition in the NLP community. In the very beginning, this work often focused exclusively on subdomains of text analysis such as document structure meta-information, syntactic or semantic analysis. The Text Encoding Initiative (TEI)2 provided schemata for the exchange of documents of various genres. The Dublin Core Metadata Initiative3 established a de facto standard for the Semantic Web.4 For (computational) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al., 1993), or semantic annotations, such as the one underlying ACE (Doddington et al., 2004), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations."
W07-1505,W06-0606,0,0.0289483,"n of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in terms of morpho-syntactic, syntactic and semantic annotations (named entities and events). In the meantime, the Annotation Compatibility Working Group (Meyers, 2006) began to concentrate its activities on the mutual compatibility of annotation schemata for, e.g., POS tagging, treebanking, role labeling, time annotation, etc. The goal of these initiatives, however, has never been to design an annotation scheme for a complete 2 http://www.tei-c.org http://dublincore.org 4 http://www.w3.org/2001/sw 5 http://www.ilc.cnr.it/EAGLES96/ 6 http://bioie.ldc.upenn.edu 3 34 NLP pipeline as needed, e.g., for information extraction or text mining tasks (Hahn and Wermter, 2006). This lack is mainly due to missing standards for specifying comprehensive NLP software archi"
W07-1505,P05-1011,0,0.0136284,"ides the attribute parent, Constituent holds the attributes cat which stores the complex syntactic category of the current constituent (e.g., NP, VP), and head which links to the head word of the constituent. In order to account for multiple annotations in the constituent-based approach, we introduced corresponding constituent types which specialize Constituent. This parallels our approach which we advocate for alternatives in POS tagging and the management of alternative chunking results. Currently, the scheme supports three different constituent types, viz. PTBConstituent, GENIAConstituent (Miyao and Tsujii, 2005) and PennBIoIEConstituent. The attributes of the type PTBConstituent cover the complete repertoire of annotation items contained in the Penn Treebank, such as functional tags for form/function dicrepancies (formFuncDisc), grammatical role (gramRole), adverbials (adv) and miscellaneous tags (misc). The representation of null elements, topicalized elements and gaps with corresponding references to the lexicalized elements in a tree is reflected in attributes nullElement, tpc, map and ref, respectively. GENIAConstituent and PennBIoIEConstituent inherit from PTBConstituent all listed attributes an"
W07-1505,W06-2713,0,0.0542362,"Missing"
W07-1505,W06-2714,0,0.130035,"Missing"
W07-1505,doddington-etal-2004-automatic,0,\N,Missing
W07-1505,laprun-etal-2002-pratical,0,\N,Missing
W08-0605,P96-1042,0,0.0596406,"he scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies h"
W08-0605,W04-1213,0,0.0305898,"Missing"
W08-0605,W04-3111,0,0.0196017,"naniadou1,3 1 School of Computer Science, The University of Manchester, UK 2 Department of Computer Science, The University of Tokyo, Japan 3 National Centre for Text Mining (NaCTeM), Manchester, UK yoshimasa.tsuruoka@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp sophia.ananiadou@manchester.ac.uk Abstract 1 Introduction However, the lack of annotated corpora, which are indispensable for training machine learning models, makes it difficult to broaden the scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a mac"
W08-0605,P06-1059,1,0.831588,"been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of the given corpus. This sampling bias is not a serious problem if one is to use the annotated corpus only for their own machine learning purpose and with the same machine learning algorithm. However, the existence of bias is not desirable if one also wants the corpus to be used by other applications or researchers. For the same reason, acThis paper presents an active learning"
W08-0605,W04-1221,0,0.0116076,"e learning has been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of the given corpus. This sampling bias is not a serious problem if one is to use the annotated corpus only for their own machine learning purpose and with the same machine learning algorithm. However, the existence of bias is not desirable if one also wants the corpus to be used by other applications or researchers. For the same reason, acThis paper pr"
W08-0605,P04-1075,0,0.0346889,"edical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recog"
W08-0605,W03-0419,0,0.0384343,"Missing"
W08-0605,D07-1051,0,0.0159407,"l dictionaries or using more sophisticated probabilistic models such as semi-Markov CRFs (Sarawagi and Cohen, 2004). These enhancements should further improve the coverage, keeping 36 the same degree of cost reduction. The idea of improving the efficiency of annotation work by using automatic taggers is certainly not new. Tanabe et al. (2005) applied a gene/protein name tagger to the target sentences and modified the results manually. Culotta and McCallum (2005) proposed to have the human annotator select the correct annotation from multiple choices produced by a CRF tagger for each sentence. Tomanek et al. (2007) discuss the reusability of named entityannotated corpora created by an active learning approach and show that it is possible to build a corpus that is useful to different machine learning algorithms to a certain degree. The limitation of our framework is that it is useful only when the target named entities are sparse because the upper bound of cost saving is limited by the proportion of the relevant sentences in the corpus. Our framework may therefore not be suitable for a situation where one wants to make annotations for named entities of many categories simultaneously (e.g. creating a corp"
W08-0605,W06-3328,0,0.0242091,"Missing"
W08-0609,C00-1030,0,0.0752154,"Missing"
W08-0609,W04-1217,0,0.0158855,"Missing"
W08-0609,W02-0301,0,0.0688202,"Missing"
W08-0609,W04-1213,0,0.271624,"Missing"
W08-0609,W04-3230,0,0.0122048,"ph (i.e., trellis) according to the word order. Estimate the score of every path using the weights of node and edges estimated by training using Conditional Random Fields. Select the best path. Figure 3 shows an example of our dictionarybased approach. Suppose that the input is “IL2-mediated activation”. A trellis is created based on the lexical entries in a dictionary. The selection criteria for the best path are determined by the CRF tagging model trained on the Genia corpus. In this example, IL-2/NN-PROTEIN -/- mediated/VVN activation/NN is selected as the best path. Following Kudo et al. (Kudo et al., 2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab1 , to our POS/PROTEIN tagging task. MeCab’s dictionary databases employ double arrays (Aoe, 1989) which enable efficient lexical look-ups. The features used were: 2.2.2 Dictionary construction A dictionary-based approach requires the dictionary to cover not only a wide variety of biomedical terms but also entries with: • all possible capitalization • all possible linguistic inflections We constructed a freely available, wide-coverage English word dictionary that satisfies these conditions. We did consider the MedPost p"
W08-0609,P06-1059,1,0.86642,"Missing"
W08-0609,W04-1218,0,0.0519101,"Missing"
W08-0609,W04-1221,0,0.0201854,"Missing"
W08-0609,W04-1220,0,0.0327216,"Missing"
W08-0609,W03-1309,0,0.0372735,"Missing"
W08-0609,W04-1219,0,0.013421,"Missing"
W08-0609,W04-1214,0,\N,Missing
W08-0609,E99-1023,0,\N,Missing
W08-0609,W03-1305,0,\N,Missing
W09-1504,W05-0304,0,0.0208375,"ork, a component which generates CASes is called a Collection Reader. We have developed several collection readers which read annotated corpora and generates annotations using the U-Compare type system. Because our primary target domain was biomedical field, there are corpus readers for the biomedical corpora; Aimed corpus (Bunescu et al., 2006) reader and BioNLP ’09 shared task format reader generate event annotations like protein-protein interaction annotations; Readers for BIO/IOB format, Bio1 corpus (Tateisi et al., 2000), BioCreative (Hirschman et al., 2004) task 1a format, BioIE corpus (Bies et al., 2005), NLPBA shared task dataset (Kim et al., 2004), Texas Corpus (Bunescu et al., 2005), Yapex Corpus (Kristofer Franzen et al., 2002), generate biomedical named entities, and Genia Treebank corpus (Tateisi et al., 2005) reader generates Penn Treebank (Marcus et al., 1993) style bracketing and part-of-speech annotations. Format readers require users to prepare annotated data, while others include corpora themselves, automatically downloaded as an archive on users’ demand. In addition, there is File System Collection Reader from Apache UIMA which reads files as plain text. We have developed an onli"
W09-1504,de-marneffe-etal-2006-generating,0,0.00947855,"Missing"
W09-1504,W04-1213,0,0.0318457,"d a Collection Reader. We have developed several collection readers which read annotated corpora and generates annotations using the U-Compare type system. Because our primary target domain was biomedical field, there are corpus readers for the biomedical corpora; Aimed corpus (Bunescu et al., 2006) reader and BioNLP ’09 shared task format reader generate event annotations like protein-protein interaction annotations; Readers for BIO/IOB format, Bio1 corpus (Tateisi et al., 2000), BioCreative (Hirschman et al., 2004) task 1a format, BioIE corpus (Bies et al., 2005), NLPBA shared task dataset (Kim et al., 2004), Texas Corpus (Bunescu et al., 2005), Yapex Corpus (Kristofer Franzen et al., 2002), generate biomedical named entities, and Genia Treebank corpus (Tateisi et al., 2005) reader generates Penn Treebank (Marcus et al., 1993) style bracketing and part-of-speech annotations. Format readers require users to prepare annotated data, while others include corpora themselves, automatically downloaded as an archive on users’ demand. In addition, there is File System Collection Reader from Apache UIMA which reads files as plain text. We have developed an online interactive text reader, named Input Text R"
W09-1504,J93-2004,0,0.0317693,"e corpus readers for the biomedical corpora; Aimed corpus (Bunescu et al., 2006) reader and BioNLP ’09 shared task format reader generate event annotations like protein-protein interaction annotations; Readers for BIO/IOB format, Bio1 corpus (Tateisi et al., 2000), BioCreative (Hirschman et al., 2004) task 1a format, BioIE corpus (Bies et al., 2005), NLPBA shared task dataset (Kim et al., 2004), Texas Corpus (Bunescu et al., 2005), Yapex Corpus (Kristofer Franzen et al., 2002), generate biomedical named entities, and Genia Treebank corpus (Tateisi et al., 2005) reader generates Penn Treebank (Marcus et al., 1993) style bracketing and part-of-speech annotations. Format readers require users to prepare annotated data, while others include corpora themselves, automatically downloaded as an archive on users’ demand. In addition, there is File System Collection Reader from Apache UIMA which reads files as plain text. We have developed an online interactive text reader, named Input Text Reader. 27 Analysis Engine Components There are many tools covering from basic syntactic annotations to the biomedical annotations. Some of the tools are running as web services, but users can freely mix local services and w"
W09-1504,I08-2122,1,0.908976,"ry of type system compatible components. These all implement the UCompare type system described in section 3. 3 2 Component and Capability In the UIMA framework, Annotation is a base type which has begin and end offset values. In this paper we call any objects (any subtype of TOP) as annotations. http://www.oasis-open.org/committees/uima/ 23 2.2.1 Related Works There also exist several other public UIMA component repositories: CMU UIMA component repository, BioNLP UIMA repository (Baumgartner et al., 2008), JCoRe (Hahn et al., 2008), Tsujii Lab Component Repository at the University of Tokyo (Kano et al., 2008a), etc. Each group uses their own type system, and so components provided by each group are incompatible. Unlike U-Compare these repositories are basically only collections of UIMA components, UCompare goes further by providing a fully integrated set of UIMA tools and utilities. 2.2.2 Integrated Platform U-Compare provides a variety of features as part of an integrated platform. The system can be launched with a single click in a web browser; all required libraries are downloaded and updated automatically in background. The Workflow Manager GUI helps users to create workflows in an easy drag-"
W09-1504,I05-2038,1,0.765119,"r primary target domain was biomedical field, there are corpus readers for the biomedical corpora; Aimed corpus (Bunescu et al., 2006) reader and BioNLP ’09 shared task format reader generate event annotations like protein-protein interaction annotations; Readers for BIO/IOB format, Bio1 corpus (Tateisi et al., 2000), BioCreative (Hirschman et al., 2004) task 1a format, BioIE corpus (Bies et al., 2005), NLPBA shared task dataset (Kim et al., 2004), Texas Corpus (Bunescu et al., 2005), Yapex Corpus (Kristofer Franzen et al., 2002), generate biomedical named entities, and Genia Treebank corpus (Tateisi et al., 2005) reader generates Penn Treebank (Marcus et al., 1993) style bracketing and part-of-speech annotations. Format readers require users to prepare annotated data, while others include corpora themselves, automatically downloaded as an archive on users’ demand. In addition, there is File System Collection Reader from Apache UIMA which reads files as plain text. We have developed an online interactive text reader, named Input Text Reader. 27 Analysis Engine Components There are many tools covering from basic syntactic annotations to the biomedical annotations. Some of the tools are running as web se"
W09-1504,J08-1002,1,\N,Missing
W10-1919,P08-2026,0,0.0668102,"Missing"
W10-1919,W09-1402,0,0.0253425,"Missing"
W10-1919,W09-1313,1,0.872817,"Missing"
W10-1919,de-marneffe-etal-2006-generating,0,0.0410254,"Missing"
W10-1919,W09-1401,1,\N,Missing
W10-3112,P07-1125,0,0.369085,"knowledge annotation. These corpora vary in both the richness of the annotation added, and the type/size of the units at which the meta-knowledge annotation has been performed. Taking the unit of annotation into account, we can distinguish between annotations that apply to continuous text-spans, and annotations that have been performed at the event level. Text-Span Annotation: Such annotations have mostly been carried out at the sentence level. They normally concentrate on a single aspect (or dimension) of meta-knowledge, normally either speculation/certainty level, (e.g., Light et al., 2004; Medlock & Briscoe, 2007; Vincze et al., 2008) or general information content/rhetorical intent, e.g., background, methods, results, insights. This latter type of annotation has been attempted both on abstracts, (e.g., McKnight & Srinivasan, 2003; Ruch et al., 2007) and full papers, (e.g. Teufel et al., 1999; Langer et al., 2004; Mizuta & Collier, 2004), with the number of distinct annotation categories varying between 4 and 14. Despite the availability of these corpora, annotation at the sentence level can often be too granular. In terms of information content, a sentence may describe, for example, both an experimen"
W10-3112,P06-1128,0,0.029482,"be assigned. The event has two slots, i.e. theme and cause whose labels help to characterise the contribution that the slot filler makes towards the meaning of the event. In this case, the slots are filled by the subject and object of the verb activates, both of which correspond to different types of bio-entities (i.e. operon and protein). IE systems trained to extract bio-events from texts allow users to formulate semantic queries over the extracted events. Such queries can specify semantic restrictions on the events in terms of event types, semantic role labels and named entity types etc. (Miyao et al., 2006), in addition to particular keywords. For example, it would be possible to search only for those texts containing bio-events of type negative_regulation where the cause is an entity of type protein. Such queries provide a great deal more descriptive power than traditional keyword searches over unstructured documents. Biomedical corpora that have been manually annotated with event level information (e.g., Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009) facilitate the training of systems such as those described above. Whilst event-based querying has advantages for efficient search"
W10-3112,W04-1205,0,0.134181,"formed at the event level. Text-Span Annotation: Such annotations have mostly been carried out at the sentence level. They normally concentrate on a single aspect (or dimension) of meta-knowledge, normally either speculation/certainty level, (e.g., Light et al., 2004; Medlock & Briscoe, 2007; Vincze et al., 2008) or general information content/rhetorical intent, e.g., background, methods, results, insights. This latter type of annotation has been attempted both on abstracts, (e.g., McKnight & Srinivasan, 2003; Ruch et al., 2007) and full papers, (e.g. Teufel et al., 1999; Langer et al., 2004; Mizuta & Collier, 2004), with the number of distinct annotation categories varying between 4 and 14. Despite the availability of these corpora, annotation at the sentence level can often be too granular. In terms of information content, a sentence may describe, for example, both an experimental method and its results. The situation becomes more complicated if a sentence contains an expression of speculation. If this is only marked at the sentence level, there may be confusion about which part(s) of the sentence are affected by the speculative expression. Certain corpora and associated systems have attempted to addre"
W10-3112,W09-1105,0,0.0373936,"tation at the sentence level can often be too granular. In terms of information content, a sentence may describe, for example, both an experimental method and its results. The situation becomes more complicated if a sentence contains an expression of speculation. If this is only marked at the sentence level, there may be confusion about which part(s) of the sentence are affected by the speculative expression. Certain corpora and associated systems have attempted to address these issues. The BioScope corpus (Vincze et al., 2008) annotates the scopes of negative and speculative keywords, whilst Morante & Daelemans (2009) have trained a system to undertake this task. The scheme described by Wilbur et al. (2006) applies annotation to fragments of sentences, which are created on the basis of changes in the meta-knowledge expressed. The scheme consists of multiple annotation dimensions which capture aspects of both certainty and rhetorical/pragmatic intent, amongst other things. Training a system to automatically annotate these dimensions is shown to be highly feasible (Shatkay et al., 2008). Event-Level Annotation: Explicit annotation of meta-knowledge at the event-level is currently rather minimal within biomed"
W10-3112,nawaz-etal-2010-meta,1,0.100188,"Missing"
W10-3112,E99-1015,0,0.044001,"-spans, and annotations that have been performed at the event level. Text-Span Annotation: Such annotations have mostly been carried out at the sentence level. They normally concentrate on a single aspect (or dimension) of meta-knowledge, normally either speculation/certainty level, (e.g., Light et al., 2004; Medlock & Briscoe, 2007; Vincze et al., 2008) or general information content/rhetorical intent, e.g., background, methods, results, insights. This latter type of annotation has been attempted both on abstracts, (e.g., McKnight & Srinivasan, 2003; Ruch et al., 2007) and full papers, (e.g. Teufel et al., 1999; Langer et al., 2004; Mizuta & Collier, 2004), with the number of distinct annotation categories varying between 4 and 14. Despite the availability of these corpora, annotation at the sentence level can often be too granular. In terms of information content, a sentence may describe, for example, both an experimental method and its results. The situation becomes more complicated if a sentence contains an expression of speculation. If this is only marked at the sentence level, there may be confusion about which part(s) of the sentence are affected by the speculative expression. Certain corpora"
W10-3112,D09-1155,0,0.0310651,"Missing"
W10-3112,W08-0606,0,0.433457,"se corpora vary in both the richness of the annotation added, and the type/size of the units at which the meta-knowledge annotation has been performed. Taking the unit of annotation into account, we can distinguish between annotations that apply to continuous text-spans, and annotations that have been performed at the event level. Text-Span Annotation: Such annotations have mostly been carried out at the sentence level. They normally concentrate on a single aspect (or dimension) of meta-knowledge, normally either speculation/certainty level, (e.g., Light et al., 2004; Medlock & Briscoe, 2007; Vincze et al., 2008) or general information content/rhetorical intent, e.g., background, methods, results, insights. This latter type of annotation has been attempted both on abstracts, (e.g., McKnight & Srinivasan, 2003; Ruch et al., 2007) and full papers, (e.g. Teufel et al., 1999; Langer et al., 2004; Mizuta & Collier, 2004), with the number of distinct annotation categories varying between 4 and 14. Despite the availability of these corpora, annotation at the sentence level can often be too granular. In terms of information content, a sentence may describe, for example, both an experimental method and its res"
W10-3112,W04-0207,0,\N,Missing
W10-3112,N07-2036,0,\N,Missing
W10-3112,W04-3103,0,\N,Missing
W11-0210,passonneau-2004-computing,0,0.080482,"Missing"
W11-0210,J00-4006,0,\N,Missing
W11-0210,J08-4004,0,\N,Missing
W11-0210,M95-1001,0,\N,Missing
W11-1507,J93-1003,0,0.0611157,"gical (Heid, 1998) rule patterns, often in combination with terminological or other lexical resources (Gaizauskas et al., 2000) and are typically language and domain specific. Statistical approaches typically combine linguistic information with statistical measures. These measures can be coarsely classified into two categories: unithood-based and termhood-based. Unithood-based approaches measure the attachment strength among the constituents of a candidate term. For example, some unithood-based measures are frequency of co-occurrence, hypothesis testing statistics, log-likelihood ratios test (Dunning, 1993) and pointwise mutual information (Church and Hanks, 1990). Termhood-based approaches attempt to measure the degree up to which a candidate expression is a valid term, i.e. refers to a specialised concept. They attempt to measure this degree by considering nestedness information, namely the frequencies of candidate terms and their subsequences. Examples of such approaches are C-Value and NCValue (Frantzi et al., 2000) and the statistical barrier method (Nakagawa, 2000). It has been experimentally shown that termhoodbased approaches to automatic term extraction outperform unithood-based ones an"
W11-1507,2005.mtsummit-papers.11,0,0.00421473,"respective category (orlanguage Dutch German English French snippets 50,363 41,334 19,767 6,182 language Spanish Danish Italian Swedish snippets 3,430 2,478 1,100 699 Table 1: Number of snippets per identified language. ganisations, persons, geographic locations and subject terms) and are compared to our term results list. 4 Figure 2: Length of snippets per identified language. Experimental Setting For training the language identification component, we used the European Parliament Proceedings Parallel Corpus (Europarl) which covers the proceedings of the European Parliament from 1996 to 2006 (Koehn, 2005). The corpus size is 40 million words per language and is translated in Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish. In our experiments, we take as input for subsequent term recognition only the snippets identified as English text. In the experiments reported in this work, we accept as term candidates morpho-syntactic pattern sequences which consist of adjectives and nouns, and end with a noun. The C-Value algorithm (cf. Section 3.3) was implemented under two different settings: i. one only considering as term candidates adjective and noun s"
W11-1507,J90-1003,0,\N,Missing
W11-1804,W11-1828,0,0.0564394,"Missing"
W11-1804,P05-1022,0,0.0320164,"ced to only core arguments, event modifications are removed, and resulting duplicate events removed. We term this the core task. In terms of the subtask division applied in the BioNLP’09 Shared Task and the GE task of 2011, the core task is analogous to subtask 1 and the full task analogous to the combination of subtasks 1–3. 5 5.1 Results Participation Final results to the task were successfully submitted by seven participants. Table 5 summarizes the information provided by the participating teams. We note that full parsing is applied in all systems, with the specific choice of the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) and conversion into the Stanford Dependency representation (de Marneffe et al., 2006) being adopted by five participants. Further, five of the seven systems are predominantly machine learning-based. These can be seen as extensions of trends that were noted in analysis of the BioNLP Rank Team Org 1 FAUST 3NLP 2 UMass 1NLP 3 Stanford 3NLP Word CoreNLP, SnowBall CoreNLP, SnowBall CoreNLP 4 ConcordU 2NLP - 5 UTurku 6 PNNL 7 PredX 1BI Porter 1CS, 1NLP, Porter 2BI 1CS, 1NLP LGP NLP Parse Trig. McCCJ + SD Arg. Events Group. Other resources Modif. C"
W11-1804,de-marneffe-etal-2006-generating,0,0.13301,"Missing"
W11-1804,W11-1827,0,0.406561,"ons are largely compatible with ID ones (see detailed results below). This is encouraging for future applications of the event extraction approach: as manual annotation requires considerable effort and time, the ability to use existing annotations is important for the feasibility of adaptation of the approach to new domains. While several participants made use of supporting syntactic analyses provided by the organizers (Stenetorp et al., 2011), none applied the analyses for supporting tasks, such as coreference or entity relation extraction results – at least in cases due to time constraints (Kilicoglu and Bergler, 2011). 5.2 Evaluation results Table 6 presents the primary results by event type, and Table 7 summarizes these results. The full task requires the extraction of additional arguments and event modifications and involves multiple novel challenges from previously addressed domain tasks including a new subdomain, full-text documents, several new entity types and a new event category. 31 Team FAUST UMass Stanford ConcordU UTurku PNNL PredX recall 48.03 46.92 46.30 49.00 37.85 27.75 22.56 prec. 65.97 62.02 55.86 40.27 48.62 52.36 35.18 F-score 55.59 53.42 50.63 44.21 42.57 36.27 27.49 Table 7: Primary ev"
W11-1804,W11-1801,1,0.755037,"Missing"
W11-1804,W11-1802,0,0.446286,"ected by participants addressing the full task. 2.2 Relations The ID task involves one relation, E QUIV, defining entities (of any of the core types) to be equivalent. This relation is used to annotate abbreviations and local aliases and it is not a target of extraction, but provided for reference and applied in evaluation, where references to any of a set of equivalent entities are treated identically. 2.3 Events The primary extraction targets of the ID task are the event types summarized in Table 1. These are a superset of those targeted in the BioNLP ST’09 and its repeat, the 2011 GE task (Kim et al., 2011b). This design makes it possible to study aspects of domain adaptation by having the same extraction targets in two subdomains of biomedicine, that of transcription factors in human blood cells (GE) and infectious diseases. The events in the ID task extend on those of GE in the inclusion of additional entity types as participants in previously considered event types and the introduction of a new type, P ROCESS. We next briefly discuss the semantics of these events, defined (as in GE) with reference to the communitystandard Gene Ontology (Ashburner et al., 2000). We refer to (Kim et al., 2008;"
W11-1804,W11-1806,0,0.20456,"Missing"
W11-1804,W11-1818,0,0.0187268,"Missing"
W11-1804,W09-1313,1,0.0825789,"es is not part of the ID task. As named entity recognition (NER) is considered in other prominent domain evaluations (Krallinger et al., 2008), we have chosen to isolate aspects of extraction performance relating to NER from the main task of interest, event extraction, by providing participants with human-created gold annotations for core entities. These annotations are briefly presented in the following. Mentions of names of genes and their products (RNA and proteins) are annotated with a single type, without differentiating between subtypes, following the guidelines of the GENIA GGP corpus (Ohta et al., 2009). This type is named P RO TEIN to maintain consistency with related tasks (e.g. BioNLP ST’09), despite slight inaccuracy for cases specifically referencing RNA or DNA forms. Two-component systems, consisting of two proteins, frequently have names derived from the names of the proteins involved (e.g. PhoP-PhoR or SsrA/SsrB). Mentions of TCSs are annotated as T WO - COMPONENT- SYSTEM, nesting P ROTEIN annotations if present. Regulons and operons are collections of genes whose expression is jointly regulated. Like the names of TCSs, their names may derive from the names of the involved genes and"
W11-1804,W11-1803,1,0.370525,"Missing"
W11-1804,W10-1919,1,0.809456,"2010 2008–2010 2008–2010 2007–2010 2008–2010 2008–2009 2007–2008 Entity type P ROTEIN C HEMICAL O RGANISM T WO - COMPONENT- SYSTEM prec. 54.64 32.24 90.38 87.69 rec. 39.64 19.05 47.70 47.24 F 45.95 23.95 62.44 61.40 Table 3: Automatic core entity tagging performance. Table 2: Corpus composition. Journals in which selected articles were published with number of articles (#) and publication years. following tools and settings were adopted, with parameters tuned on initial annotation for two documents: design was guided by previous studies on NER and event extraction in a closely related domain (Pyysalo et al., 2010; Ananiadou et al., 2011). P ROTEIN: NeMine (Sasaki et al., 2008) trained on the JNLPBA data (Kim et al., 2004) with threshold 0.05, filtered to only GENE and PROTEIN types. 3.1 O RGANISM: Linnaeus (Gerner et al., 2010) with “variant matching” for species names variants. Document selection The training and test data were drawn from the primary text content of recent full-text PMC open access documents selected by infectious diseases domain experts (Virginia Tech team) as representative publications on two-component regulatory systems. Table 2 presents some characteristics of the corpus composi"
W11-1804,W11-1807,0,0.0968254,"Missing"
W11-1804,W11-1808,0,0.49423,"Missing"
W11-1804,W08-0609,1,0.0820893,"Entity type P ROTEIN C HEMICAL O RGANISM T WO - COMPONENT- SYSTEM prec. 54.64 32.24 90.38 87.69 rec. 39.64 19.05 47.70 47.24 F 45.95 23.95 62.44 61.40 Table 3: Automatic core entity tagging performance. Table 2: Corpus composition. Journals in which selected articles were published with number of articles (#) and publication years. following tools and settings were adopted, with parameters tuned on initial annotation for two documents: design was guided by previous studies on NER and event extraction in a closely related domain (Pyysalo et al., 2010; Ananiadou et al., 2011). P ROTEIN: NeMine (Sasaki et al., 2008) trained on the JNLPBA data (Kim et al., 2004) with threshold 0.05, filtered to only GENE and PROTEIN types. 3.1 O RGANISM: Linnaeus (Gerner et al., 2010) with “variant matching” for species names variants. Document selection The training and test data were drawn from the primary text content of recent full-text PMC open access documents selected by infectious diseases domain experts (Virginia Tech team) as representative publications on two-component regulatory systems. Table 2 presents some characteristics of the corpus composition. To focus efforts on natural language text likely to express"
W11-1804,W11-1816,1,0.465395,"Missing"
W11-1804,W09-1401,1,\N,Missing
W11-1804,W04-1213,0,\N,Missing
W11-1804,C10-1088,1,\N,Missing
W11-3307,2010.eamt-1.42,0,0.103822,"tools and related web services for a large number of European languages. LRs will be documented with highquality metadata and aggregated in central inventories, allowing for uniform search and access to resources. A further aim of METASHARE is to promote the use of widely acceptable standards for LR building, in order to ensure the greatest possible interoperability of LRs. META-SHARE shares some goals with related initiatives, such as the Open Language Archives Community (OLAC) (Hughes & Kamat, 2005), which is developing a virtual library of LRs augmented with metadata; the PANACEA project (Bel, 2010), which is creating a library of interoperable web services that automate the stages involved in the production and maintenance of LRs required by MT systems; and the Common Language Resources and Technology Infrastructure META-NET is a Network of Excellence aiming to improve significantly on the number of language technologies that can assist European citizens, by enabling enhanced communication and cooperation across languages. A major outcome will be METASHARE, a searchable network of repositories that collect resources such as language data, tools and related web services, covering a large"
W11-3307,varadi-etal-2008-clarin,0,0.0217485,"tion of a pilot version of METASHARE, in which standard functionality is enhanced through the integration of U-Compare. As an initial step, UIMA-compliant LRs are currently being created for a subset of European languages, based on the resources that will be made available by the METANET4U partners. This will allow us to demonstrate that METASHARE has the potential to serve not only as a useful tool to locate resources for a range of languages, but also to act as an integrated environment that allows for rapid prototyping and testing of applications that make use of these resources. (CLARIN) (Váradi et al., 2008), which is establishing an integrated and interoperable research infrastructure of LRs and technology. A memorandum of understanding between METANET and CLARIN recognizes that they are complementary initiatives with harmonisable goals. Whilst CLARIN is largely oriented towards the social sciences and humanities research community, META-NET aims at supporting Human Language Technology (HLT) development, and thus will target HLT researchers and developers, language professionals (translators, interpreters, etc.), as well as industrial players, with a particular emphasis on cross-lingual technolo"
W11-3307,W10-3001,0,0.0160697,"in this case, this does not matter, as long as the required information is also present in the CAS. 4 U-Compare and META-SHARE The utility of U-Compare has already been amply demonstrated through its use in many tasks by both NLP experts and non-expert users, from the individual level to worldwide challenges. These include the BioNLP’09 shared task (Kim et al., 2009) for the extraction of biomolecular events (bio-events) that appear in biomedical literature, in which U-Compare served as an official support system; the CoNLL2010 shared task on the detection of speculation in biomedical texts (Farkas et al., 2010); the BioCreative II.5 challenge (Sætre et al., 2009) of text-mining and information-extraction systems applied to the biological domain; and linking with Taverna (Kano et al., 2010), a generic workflow management system. Mostly, these usages have been limited to the processing of biomedical texts in the English language. Integration within META-SHARE will additionally allow the utility of U-Compare to be demonstrated in a multilingual scenario, where it will help to facilitate the rapid expansion of NLP applications covering a range of European languages. In order to ensure the success of thi"
W11-3307,W09-1401,1,0.761364,"following a tool whose output is either POSToken or RichToken, since both of these tool types will output token annotations with part-ofspeech information. Even though tools outputting RichToken information would contain some redundant information in this case, this does not matter, as long as the required information is also present in the CAS. 4 U-Compare and META-SHARE The utility of U-Compare has already been amply demonstrated through its use in many tasks by both NLP experts and non-expert users, from the individual level to worldwide challenges. These include the BioNLP’09 shared task (Kim et al., 2009) for the extraction of biomolecular events (bio-events) that appear in biomedical literature, in which U-Compare served as an official support system; the CoNLL2010 shared task on the detection of speculation in biomedical texts (Farkas et al., 2010); the BioCreative II.5 challenge (Sætre et al., 2009) of text-mining and information-extraction systems applied to the biological domain; and linking with Taverna (Kano et al., 2010), a generic workflow management system. Mostly, these usages have been limited to the processing of biomedical texts in the English language. Integration within META-SH"
W11-3307,laprun-etal-2002-pratical,0,\N,Missing
W12-2410,W11-1828,1,0.799541,"Missing"
W12-2410,W09-1402,1,0.89609,"Missing"
W12-2410,W10-1904,1,0.854996,"Missing"
W12-2410,P05-1022,0,0.0136407,"et VBZ is agent> VBN mediated IN by . . NN CKI EPI detection TEES 2.3 <Theme <Site Entity Serine Theme> Cause> Phosphorylation phosphorylation Protein of T-bet Protein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and foll"
W12-2410,W03-1018,0,0.0217666,"ed by CKI . McCJ-parser + Stanford Conversion <nsubjpass <nn NN Serine prep_of> NN phosphorylation D IN of REL detection <auxpass NN T-bet VBZ is agent> VBN mediated IN by . . NN CKI EPI detection TEES 2.3 <Theme <Site Entity Serine Theme> Cause> Phosphorylation phosphorylation Protein of T-bet Protein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Ea"
W12-2410,W09-1401,1,0.800261,"by existing PubMed-scale event extraction efforts. The methods and data introduced in this study are freely available from bionlp.utu.fi. 1 Introduction Biomedical domain information extraction has in recent years seen a shift from focus on the extraction of simple pairwise relations (Pyysalo et al., 2008; Tikk et al., 2010) towards the extraction of events, represented as structured associations of arbitrary numbers of participants in specific roles (Ananiadou et al., 2010). Domain event extraction has been popularized in particular by the BioNLP Shared Task (ST) challenges in 2009 and 2011 (Kim et al., 2009; Kim et al., 2011). While the BioNLP ST’09 emphasized protein interactions and regulatory relationships, the expressive event formalism can also be applied to the extraction of statements regarding the properties of individual proteins. Accordingly, the EPI (Epigenetics and Post-Translational Modifications) subchallenge of the BioNLP ST’11 provided corpora and competitive evaluations for the detection of epigenetics and post-translational modification (PTM) events, while the REL (Entity Relations) subchallenge covers structural and complex membership relations of proteins (Ohta et al., 2011b;"
W12-2410,W11-1801,1,0.780089,"ein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion to the BioNLP ST format are imported into a database (E). (Adapted from Bj¨orne and Salakoski (2011)). The extraction of events"
W12-2410,de-marneffe-etal-2006-generating,0,0.0781296,"Missing"
W12-2410,N10-1004,0,0.0126786,"d IN by . . NN CKI EPI detection TEES 2.3 <Theme <Site Entity Serine Theme> Cause> Phosphorylation phosphorylation Protein of T-bet Protein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion"
W12-2410,W10-1903,1,0.868585,"Missing"
W12-2410,W11-1803,1,0.805324,"ein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion to the BioNLP ST format are imported into a database (E). (Adapted from Bj¨orne and Salakoski (2011)). The extraction of events"
W12-2410,W09-1301,1,0.907084,"Missing"
W12-2410,W11-1812,1,0.693859,"ein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion to the BioNLP ST format are imported into a database (E). (Adapted from Bj¨orne and Salakoski (2011)). The extraction of events"
W12-2410,W11-1816,1,0.886081,"Missing"
W12-2410,W10-1921,1,0.898429,"Missing"
W12-2412,W11-1828,0,0.0929443,"Missing"
W12-2412,W11-1820,0,0.0684248,"Missing"
W12-2412,P11-1098,0,0.0165262,"c span of text supporting extracted information,5 the requirement of the BioNLP ST setting that the output of event extraction systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise fol"
W12-2412,doddington-etal-2004-automatic,0,0.282956,"IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise follows the primary BioNLP ST evaluation criteria, but incorporates the following two exceptions: 1. remove the requirement to match trigger spans 2. only require entity texts, not spans, to match The first alternative criterion has also been previously consider"
W12-2412,W11-1824,0,0.0264993,"Missing"
W12-2412,P10-1160,0,0.0661515,"Missing"
W12-2412,W11-1827,0,0.568696,"Missing"
W12-2412,W11-1801,1,0.941981,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,W11-1822,0,0.0679042,"Missing"
W12-2412,W11-1826,0,0.0323279,"Missing"
W12-2412,P11-1163,0,0.0329128,"l only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating to evaluation and the applicability of methods to the task. In the following section, we discuss some of these issues and propose alternatives to representation and evaluation addressing them. 4.1 Evaluation The evaluation of the BioNLP ST is instance-based and text-bound: each event in gold annotation and each event extracted by a system is considered in"
W12-2412,P09-1113,0,0.0205423,"tly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and event annotation is associated with a specific span of text. Contrasted to the alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating"
W12-2412,W11-1803,1,0.935394,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,D07-1075,0,0.0343243,"IE systems to identify a specific span of text supporting extracted information,5 the requirement of the BioNLP ST setting that the output of event extraction systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode"
W12-2412,W11-1804,1,0.922475,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,W11-1812,1,0.828716,"ferring to the real-world entities in text, the overall task is “text-bound” in the sense of requiring not only the extraction of targeted statements from text, but also the identification of specific regions of text expressing each piece of extracted information. Events can further be marked with modifiers identifying additional features such as being explicitly negated or stated in a speculative context. Figure 1 shows an illustration of event annotations. This BioNLP ST 2009 formulation of the event extraction task was followed also in three 2011 main tasks: the GE (Kim et al., 2011c), ID (Pyysalo et al., 2011a) and EPI (Ohta et al., 2011) tasks. A variant of this representation that omits event triggers was applied in the BioNLP ST 2011 bacteria track (Bossy et al., 2011), and simpler, binary relationtype representations were applied in three supporting tasks (Nguyen et al., 2011; Pyysalo et al., 2011b; Jourde et al., 2011). Due to the challenges of consistent evaluation and processing for tasks involvIn this section, we present the new collection of automatically created event analyses and demonstrate one use of the data through an evaluation of events that no system could successfully extract. 3"
W12-2412,W11-1825,0,0.02855,"P ST 2011 bacteria track (Bossy et al., 2011), and simpler, binary relationtype representations were applied in three supporting tasks (Nguyen et al., 2011; Pyysalo et al., 2011b; Jourde et al., 2011). Due to the challenges of consistent evaluation and processing for tasks involvIn this section, we present the new collection of automatically created event analyses and demonstrate one use of the data through an evaluation of events that no system could successfully extract. 3.1 Following the BioNLP ST 2011, the MSR-NLP group called for the release of outputs from various participating systems (Quirk et al., 2011) and made analyses of their system available.2 Despite the obvious benefits of the availability of these resources, we are not aware of other groups following this example prior to the time of this publication. To create the combined resource, we approached each group that participated in the three targeted BioNLP ST 2011 main tasks to ask for their support to the creation of a dataset including analyses from their event extraction systems. This suggestion met with the support of all but a few groups that were approached.3 The groups providing analyses from their systems into this merged resou"
W12-2412,D11-1001,0,0.0233845,"alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating to evaluation and the applicability of methods to the task. In the following section, we discuss some of these issues and propose alternatives to representation and evaluation addressing them. 4.1 Evaluation The evaluation of the BioNLP ST is instance-based and text-bound: each event in gold annotation"
W12-2412,W11-1808,0,0.31655,"Missing"
W12-2412,W11-1816,1,0.901463,"Missing"
W12-2412,H91-1059,0,0.261268,"on systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise follows the primary BioNLP ST evaluation criteria, but incorporates the following two exceptions: 1. remove the re"
W12-2412,W10-1921,1,0.866603,"Missing"
W12-2412,W11-1821,0,0.0439833,"Missing"
W12-2412,W11-0204,0,0.0629482,"Missing"
W12-2412,W11-1805,0,0.0284448,"Missing"
W12-2412,W11-1802,1,\N,Missing
W12-2412,W11-1809,0,\N,Missing
W12-2412,W11-1811,1,\N,Missing
W12-2412,W11-1810,0,\N,Missing
W12-3806,W11-1828,0,0.0482957,"Missing"
W12-3806,W10-3001,0,0.0645895,"Missing"
W12-3806,W10-3010,0,0.161775,"sk, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine how cue-and-scope recognition systems can be used to produce a state-of-the-art negation/speculation detection system for the EE task. 2 Resources Several existing resources can support the inv"
W12-3806,W11-1827,0,0.0114523,"us used to train the CLiPSNESP system, the GE test set does not, and thus test set results are not expected to be overfit. We noted when performing development set experiments that training machine learning-based methods on the negation/speculation annotations of the event-annotated corpora was problematic due to the sparseness of these flags in the annotation. To address this issue, we merge the training data of the three corpora in all experiments with machine learning methods. 5.2 Baseline methods We use the event analyses created by the UTurku (Bj¨orne and Salakoski, 2011) and UConcordia (Kilicoglu and Bergler, 2011) systems for the BioNLP 2011, the only systems that included negation and speculation analyses. To investigate the impact on a system that did not include a negation/speculation component, we further consider analyses created Negation (R/P/F) EPI GE ID H HR 29.23/31.67/30.40 27.69/32.73/30.00 53.92/52.84/53.38 53.24/71.89/61.18 44.00/31.88/36.97 44.00/37.93/40.74 M ME MC MCE 47.69/20.00/28.18 60.00/66.10/62.90 40.00/74.29/52.00 58.46/73.08/64.96 43.00/25.25/31.82 58.36/70.08/63.69 58.36/76.34/66.15 61.77/83.03/70.84 46.00/26.74/33.82 54.00/69.23/60.67 52.00/61.90/56.52 58.00/70.73/63.74 Table"
W12-3806,W09-1401,1,0.94131,"ion/Speculation Annotations: A Bridge Not Too Far Pontus Stenetorp1 Sampo Pyysalo2,3 Tomoko Ohta2,3 Sophia Ananiadou2,3 and Jun’ichi Tsujii2,3,4 1 Department of Computer Science, University of Tokyo, Tokyo, Japan 2 School of Computer Science, University of Manchester, Manchester, United Kingdom 3 National Centre for Text Mining, University of Manchester, Manchester, United Kingdom 4 Microsoft Research Asia, Beijing, People’s Republic of China {pontus,smp,okap}@is.s.u-tokyo.ac.jp sophia.ananiadou@manchester.ac.uk jtsujii@microsoft.com Abstract some marking of certainty and polarity (LDC, 2005; Kim et al., 2009; Saur and Pustejovsky, 2009; Kim et al., 2011a; Thompson et al., 2011). We study two approaches to the marking of extra-propositional aspects of statements in text: the task-independent cue-and-scope representation considered in the CoNLL-2010 Shared Task, and the tagged-event representation applied in several recent event extraction tasks. Building on shared task resources and the analyses from state-of-the-art systems representing the two broad lines of research, we identify specific points of mismatch between the two perspectives and propose ways of addressing them. We demonstrate the feas"
W12-3806,W11-1801,1,0.855017,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1802,0,0.0691394,"Missing"
W12-3806,W11-1806,0,0.142886,"2.43 48.08/51.02/49.50 25.65/10.84/15.24 22.08/42.24/29.00 27.27/50.30/35.37 31.82/53.85/40.00 45.83/10.58/17.19 29.17/28.00/28.57 37.50/31.03/33.96 33.33/42.11/37.21 Table 5: Results for Speculation for our two heuristics and the four combinations of ML features. by the FAUST system, which achieved the highest performance at two of the three tasks considered (Riedel et al., 2011). The UTurku system is a pipeline ML-based EE system, while the UConcordia system is strictly rule-based. FAUST is an ML-based model combination system incorporating information from the parser-based Stanford system (McClosky et al., 2011) and the jointly-modelled UMass system (Riedel and McCallum, 2011). We also performed preliminary experiments for the other released submissions to the BioNLP 2011 Shared Task, but due to space limitations focus only on the three above-mentioned systems. results tables we abbreviate the feature set names as done in Table 3 and use H for the heuristic method and R for its root extension. As our machine learning component we use LIBLINEAR (Fan et al., 2008) with a L2-regularised L2-loss SVM model. We optimise the SVM regularisation parameter C using 10-fold cross-validation on the training data."
W12-3806,W09-1304,0,0.0270192,"icallyoriented and task-oriented perspectives on negation/speculation detection. In this study, we make use of the following resources. First, we study the three BioNLP 2011 Shared Task corpora that include annotation for negation and speculation: the GE, EPI and ID main task corpora (Table 1). Second, we make use of supporting analyses provided for these corpora in response to a call sent by the BioNLP Shared Task organisers to the developers of third-party systems (Stenetorp et al., 2011). Specifically, we use the output of the BiographTA NeSp Scope Labeler (here referred to as CLiPS-NESP) (Morante and Daelemans, 2009; Morante et al., 2010) provided by the University of Antwerp CLiPS center. This system provides cue-and-scope analyses for negation and speculation and was demonstrated to have state-of-the-art performance at the relevant CoNLL-2010 Shared Task. Finally, we make use of the event analyses created by systems that participated in the BioNLP Shared Task, made available to the research community for the majority of the shared task submissions (Pyysalo et al., 2012). These analyses represent the stateof-the-art in event extraction and their capability to detect event structures as well as marking t"
W12-3806,W10-3006,0,0.158592,"Missing"
W12-3806,W11-1803,1,0.674875,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1804,1,0.674411,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W12-2412,1,0.850965,"(Stenetorp et al., 2011). Specifically, we use the output of the BiographTA NeSp Scope Labeler (here referred to as CLiPS-NESP) (Morante and Daelemans, 2009; Morante et al., 2010) provided by the University of Antwerp CLiPS center. This system provides cue-and-scope analyses for negation and speculation and was demonstrated to have state-of-the-art performance at the relevant CoNLL-2010 Shared Task. Finally, we make use of the event analyses created by systems that participated in the BioNLP Shared Task, made available to the research community for the majority of the shared task submissions (Pyysalo et al., 2012). These analyses represent the stateof-the-art in event extraction and their capability to detect event structures as well as marking them for negation and speculation. The above three resources present us with many opportunities to relate scope-based annotations to three highly relevant event-based corpora containing negation/speculation annotations. 3 Manual Analysis To gain deeper insight into the data and the challenges in combining the cue-and-scope and eventoriented perspectives, we performed a manual analysis of the corpus annotations using the manually Name Negated Events Speculated Ev"
W12-3806,W11-1807,0,0.0123498,"27/50.30/35.37 31.82/53.85/40.00 45.83/10.58/17.19 29.17/28.00/28.57 37.50/31.03/33.96 33.33/42.11/37.21 Table 5: Results for Speculation for our two heuristics and the four combinations of ML features. by the FAUST system, which achieved the highest performance at two of the three tasks considered (Riedel et al., 2011). The UTurku system is a pipeline ML-based EE system, while the UConcordia system is strictly rule-based. FAUST is an ML-based model combination system incorporating information from the parser-based Stanford system (McClosky et al., 2011) and the jointly-modelled UMass system (Riedel and McCallum, 2011). We also performed preliminary experiments for the other released submissions to the BioNLP 2011 Shared Task, but due to space limitations focus only on the three above-mentioned systems. results tables we abbreviate the feature set names as done in Table 3 and use H for the heuristic method and R for its root extension. As our machine learning component we use LIBLINEAR (Fan et al., 2008) with a L2-regularised L2-loss SVM model. We optimise the SVM regularisation parameter C using 10-fold cross-validation on the training data. We use the training, development and test set partition provided"
W12-3806,W11-1808,0,0.183789,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1816,1,0.908546,"Missing"
W12-3806,W08-0606,0,0.206797,"ons of text statements have explicitly included Although extra-propositional aspects are recognised as important, there is no clear consensus on how to address their annotation and extraction from text. Some comparatively early efforts focused on the detection of negation cue phrases associated with specific (previously detected) terms through regular expression-based rules (Chapman et al., 2001). A number of later efforts identified the scope of negation cues with phrases in constituency analyses in sentence structure (Huang and Lowe, 2007). Drawing in part on this work, the BioScope corpus (Vincze et al., 2008) applied a representation where both cues and their associated scopes are marked as contiguous spans of text (Figure 1 bottom). This approach was also applied in the CoNLL-2010 Shared Task (Farkas et al., 2010), in which 13 participating groups proposed approaches for Task 2, which required the identification of uncertainty cues and their associated scopes in text. In the following, we will term this task-independent, linguisticallymotivated approach as the cue-and-scope representation (please see Vincze et al. (2008) for details regarding the representation). For IE efforts, more task-oriente"
W12-4304,H92-1045,0,0.108814,"g. For PubMed, we simply selected a random set of citations and extracted their abstract and title texts. For PMC, we initially extracted all non-overlapping section texts (PMC XML <sec> elements) as well as caption texts (<caption> elements), and then selected a random set of extracts. This selection seeks to maximize the diversity of the texts in the full-text section of the corpus, and the selection of extracts larger than isolated sentences aims to allow the corpus to be used to study methods making use of broader context, e.g. by incorporating constraints such as one sense per discourse (Gale et al., 1992). 2 3 We selected a total of 500 documents using this protocol, half from PubMed and half from PMC document extracts. (Descriptive statistics of the abstracts and full-text extracts subcorpora are given later in Table 3.) 2.6 Annotation Process Primary annotation was created by a PhD biologist with extensive experience in domain information extraction and text annotation (TO). The use of any relevant resources, such as the full article being annotated or species-specific anatomy ontologies in the OBO foundry, was encouraged for resolving unclear or ambiguous cases during annotation. Initial an"
W12-4304,W10-1909,0,0.127256,"rehensive analysis must include entities at multiple levels of biological organization, from the molecular to the organism level. The detection of references to anatomical entities such as “kidney” and “blood” is thus required for the automatic structured analysis of biomedical scientific text. Although a wealth of lexical and ontological resources covering anatomical entities are available (Rosse and Mejino, 2003; Smith et al., 2007; Bodenreider, 2004; Haendel et al., 2009), such resources do not alone confer the ability to reliably detect mentions of anatomical entities in natural language (Gerner et al., 2010a; Travillian et al., 2011; Pyysalo et al., 2012b). To support the development and evaluation of reliable anatomical entity mention detection methods, corpus resources annotated specifically for the task are necessary. In this study, we aim to create a reference standard for evaluating methods for anatomical entity mention detection and for training machine learningbased methods for the task. We seek to select a set of texts that are representative of the relevant scientific literature, i.e. open-domain in the sense of avoiding bias toward, for example, specific species, levels of biological o"
W12-4304,W04-1213,0,0.115942,"ue to occupying different levels at different stages of development, we adopt a separate D EVELOPING ANATOMICAL STRUCTURE category, as done also in e.g. Uberon (Haendel et al., 2009). 1 http://obofoundry.org/ 28 Annotation Scope We diverge from the scope of anatomy ontologies in two important aspects in our annotation. First, ontologies of anatomy commonly incorporate everything from molecules to whole organisms within their scope. However, in entity mention detection, many molecular level anatomical entities fall within the scope of the established gene/protein mention detection tasks (e.g. (Kim et al., 2004; Tanabe et al., 2005)), and whole organism mentions similarly largely within what is covered by existing methods and resources for organism mention detection (Gerner et al., 2010b; Naderi et al., 2011). To avoid overlap with established tasks and to focus on the novel aspects of anatomical entity mention detection, we exclude biological macromolecules and mentions of organism names from the scope of our annotation, as argued in (Pyysalo et al., 2012b). Second, these ontologies typically represent canonical anatomy, an idealized state that is rarely (if ever) encountered in reality (Bada and H"
W12-4304,W11-1901,0,0.0257235,"mical entities (e.g. “muscle tissue”). Both names and nominal mentions are annotated similarly, without distinction. We exclude pronouns (it, that) from annotation even when they un29 cytoplasm Tissue Part-of Cell of phagocytic microglia Frag Tissue thyroid and eye muscle membranes Figure 2: Part-of relation marking entity mention spanning a prepositional phrase (above) and Frag relation marking coordination with ellipsis (below). ambiguously refer to an anatomical entity; we consider the identification and resolution of such mentions part of the distinct coreference resolution task (see e.g. Pradhan et al. (2011)). In addition to names and nominal mentions, we mark adjectives that have an unambiguous sense of relating to a specific anatomical entity. Thus, for example, both “kidney” and “renal” (relating to the kidneys) are annotated as O RGAN in expressions such as “kidney failure” and “renal failure”. The choice to annotate adjectival references is motivated by the expected needs of applications making use of automatically detected anatomical entity mentions. For example, for semantic search targeting documents relating to organ failure, a document discussing “renal failure” is obviously relevant an"
W12-4304,E12-2021,1,0.874312,"Missing"
W12-4304,W03-0419,0,0.0245807,"Missing"
W12-4305,W09-3742,1,0.893764,"Missing"
W12-4305,W10-3001,0,0.0345169,"Missing"
W12-4305,W10-1913,1,0.871066,"kas et al (2010), Morante & Sporleder (2012)). Finally, Shatkay et al (2008) define a multidimensional scheme, which combines several of the above-mentioned aspects. Recent work has compared schemes to discover mappings and relative merits. Liakata et al. (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al. (2010) compared three schemes for the identification of discourse structure in scientific abstracts from cancer research assessment articles. The work showed a subsumption relation between the scheme of Hirohata et al. (2008), a cut-down version of the 5 http://www.nactem.ac.uk/medie/ 44 scheme proposed by Teufel et al. (2009) and CoreSC (1st layer), from general to specific. 8 Conclusion We have compared three different schemes, each taking a different perspective to the annotation of scientific discourse. The comparison shows that the three schemes are complementary, with different strengths and p"
W12-4305,I08-1050,1,0.952295,"0 Result 51 0 14 0 20 0 0 0 6 18 0 0 0 0 1 103 7 OtherResult 11 0 1 0 0 1 0 1 0 10 0 0 0 0 0 12 47 OtherFact 4 0 1 0 0 2 5 3 0 7 0 0 0 0 0 2 54 RegResult 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Intertextual 13 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 Intratextual 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 280 4 35 0 28 3 34 4 29 127 0 24 2 0 2 178 136 Table 5: Segments vs Event Meta-Knowledge 43 7 Related Work A number of schemes for annotating scientific discourse elements at the sentence level have been proposed. Certain schemes have been aimed at abstracts, e.g., (McKnight & Srinivasan, 2003; Ruch et al., 2007; Hirohata et al., 2008; Björne et al., 2009). The work of Hirohata et al. (2009) has been integrated with the MEDIE service5 (Miyao et al., 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al., 1999; Teufel & Moens, 2002; Teufel et al., 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. AZ was modified for the"
W12-4305,W08-0607,0,0.0612837,"Missing"
W12-4305,liakata-etal-2010-corpora,1,0.879485,") extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. For a recent overview see de Waard and Pander Maat (2012). Modality and negation in text have also been the focus of recent workshops (Farkas et al (2010), Morante & Sporleder (2012)). Finally, Shatkay et al (2008) define a multidimensional scheme, which combines several of the above-mentioned aspects. Recent work has compared schemes to discover mappings and relative merits. Liakata et al. (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al. (2010) compared three schemes for the identification of discourse structure in scientific abstracts from cancer research assessment articles. The work showed a subsumption relation between the scheme of Hirohata et al. (2008), a cut-down version of th"
W12-4305,W04-3103,0,0.0971683,"Missing"
W12-4305,P06-1128,0,0.0451617,"Missing"
W12-4305,J12-2001,0,0.0160302,"nd summarisation tasks with Computational Linguistics papers. AZ was modified for the annotation of biology papers by Mizuta et al. (2005) in order to facilitate information extraction, and more recently Teufel et al. (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. For a recent overview see de Waard and Pander Maat (2012). Modality and negation in text have also been the focus of recent workshops (Farkas et al (2010), Morante & Sporleder (2012)). Finally, Shatkay et al (2008) define a multidimensional scheme, which combines several of the above-mentioned aspects. Recent work has compared schemes to discover mappings and relative merits. Liakata et al. (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al. (2010) compared three schemes for"
W12-4305,nawaz-etal-2010-meta,1,0.821695,"rd event annotated corpora exist; examples include the GENIA Event Corpus (Kim et al., 2008), GREC (Thompson et al., 2009) and BioInfer (Pyysalo et al., 2007), in addition to the corpora produced for the shared tasks. Figure 1. Bio-Event Representation 3.2 Meta-knowledge Annotation Until recently, the only attempts to recognise information relating to the correct interpretation of events were restricted to sparse details regarding negation and speculation (Kim et al., 2011). In order to address this problem, a multidimensional annotation scheme especially tailored to bio-events was developed (Nawaz et al., 2010; Thompson et al., 2011). The scheme identifies and categorises several different types of contextual details regarding events (termed meta-knowledge), including discourse information. Different types of meta-knowledge are encoded through five distinct dimensions (Figure 2). The advantage of using multiple dimensions is that the interplay between the assigned values in each dimension can reveal both subtle and substantial differences in the types of meta-knowledge expressed. In the majority of cases, meta-knowledge is expressed through the presence of particular “clue” words or phrases, althou"
W12-4305,J02-4002,0,0.330601,"24 2 0 2 178 136 Table 5: Segments vs Event Meta-Knowledge 43 7 Related Work A number of schemes for annotating scientific discourse elements at the sentence level have been proposed. Certain schemes have been aimed at abstracts, e.g., (McKnight & Srinivasan, 2003; Ruch et al., 2007; Hirohata et al., 2008; Björne et al., 2009). The work of Hirohata et al. (2009) has been integrated with the MEDIE service5 (Miyao et al., 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al., 1999; Teufel & Moens, 2002; Teufel et al., 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. AZ was modified for the annotation of biology papers by Mizuta et al. (2005) in order to facilitate information extraction, and more recently Teufel et al. (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of"
W12-4305,D09-1155,0,0.509488,"types that seem to be exclusive and useful (de Waard & Pander Maat, 2009). Three classes of segment types are defined: − Basic segment types: segments referring directly to the topic of study – see Table 2. − ‘Other’-segment types: segments referring to conceptual or experimental work in other research papers than the current one − Regulatory segment types: ‘regulatory’ clauses that control and introduce other segments. A list of segment types is presented in Table 2; further details, including a list of all segment types and correlations with verb tense can be found in de Waard & Pander Maat (2009). The focus of this work is to identify linguistic features that characterise these discourse segment types, according to three aspects: − Verb tense, aspect, mood and voice − Semantic verb class − Epistemic modality markers So far, 6 full-text papers (comprising about 2300 segments) have been manually annotated with segment types and correlated with the above features. A first automated validation was promising (de Waard, Buitelaar and Eigener, 2009). The need for parsing at a clause level is especially prominent in biological text, since specific semantic roles are played by particular claus"
W12-4305,W08-0606,0,0.0653538,"Missing"
W12-4305,W12-4306,1,\N,Missing
W12-4305,nawaz-etal-2012-identification,1,\N,Missing
W12-4305,W11-1825,0,\N,Missing
W12-4305,E99-1015,0,\N,Missing
W12-4305,W09-1402,0,\N,Missing
W12-4305,L12-1000,0,\N,Missing
W13-2008,W11-1828,0,0.0112182,"Missing"
W13-2008,W13-2003,0,0.0164694,"Missing"
W13-2008,W11-1826,0,0.0290686,"Missing"
W13-2008,P05-1022,0,0.140045,"Missing"
W13-2008,W13-2010,0,0.0333957,"Missing"
W13-2008,W11-1806,0,0.0334856,"Missing"
W13-2008,W11-1816,1,0.890054,"Missing"
W13-2008,W13-2012,1,0.200549,"Missing"
W13-2008,J08-1002,0,0.0344355,"Missing"
W13-2008,W12-4304,1,0.922588,"ns. For other categories of annotation, correct (gold standard) annotations are provided also for test data. 2.1 Table 1: Entity types. Indentation corresponds to is-a structure. Labels in gray identify groupings defined for organization only, not annotated types. Development Theme Cancer Equiv Cancer progression of chronic myeloid leukemia (CML) Figure 2: Example Equiv relation. drawn primarily from the Common Anatomy Reference Ontology (Haendel et al., 2008), a small, species-independent upper-level ontology based on the Foundational Model of Anatomy (Rosse and Mejino Jr, 2003). We refer to Ohta et al. (2012) for more detailed discussion of the anatomical entity type definitions. Entities The entity types defined in the CG task are shown in Table 1. The molecular level entity types largely match the scope of types such as P ROTEIN and C HEMICAL included in previous ST tasks (Kim et al., 2012; Pyysalo et al., 2012b). However, the CG types are more fine grained, and the types P RO TEIN DOMAIN OR REGION and DNA DOMAIN OR REGION are used in favor of the non-specific type E NTITY, applied in a number of previous tasks for additional event arguments (see Section 2.3). The definitions of the anatomical e"
W13-2008,W13-2011,0,0.0422743,"Missing"
W13-2008,D07-1111,0,0.0149447,"Missing"
W13-2008,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2008,W11-1801,1,\N,Missing
W13-2008,E12-2021,1,\N,Missing
W13-2009,P05-1022,0,0.013255,"could provide further insight into the relative strengths and weaknesses of these two systems. Table 6: Primary evaluation results Event Extraction System8 (Bj¨orne et al., 2011) (TEES). The two systems share the same overall architecture, a one-best pipeline with SVMbased stages for event trigger detection, triggerargument relation detection, argument grouping into event structures, and modification prediction. The feature representations of both systems draw on substructures of dependency-like representations of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMi"
W13-2009,W08-0608,0,0.0128679,"Missing"
W13-2009,W11-1828,0,0.0171244,"Missing"
W13-2009,W11-0214,1,0.596369,"in interactions (Krallinger et al., 2007; Pyysalo et al., 2008; Tikk et al., 2010). However, most such efforts have employed simple representations, such as entity pairs, that are not sufficient for capturing molecular reactions to the level of detail required to support the curation of pathway models. Additionally, previous efforts have not directly involved the semantics (e.g. reaction type definitions) of such models. Perhaps in part due to these reasons, natural language processing and information extraction methods have not been widely embraced by biomedical pathway curation communities (Ohta et al., 2011c; Ohta et al., 2011a). We believe that the extraction of structured event representations (Figure 1) pursued in the BioNLP Shared Tasks offers many opportunities to make significant contributions to support the development, evaluation and maintenance of biomolecular pathways. The Pathway Curation (PC) task, a main task of the BioNLP Shared Task 2013, is proposed as a step toward realizing these opportunities. The PC task aims to evaluate the applicability of event extraction systems to pathway curation and to encourage the further development of methods for related tasks. The design of the ta"
W13-2009,W12-4304,1,0.900591,"Missing"
W13-2009,W11-0215,1,0.854069,"iyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011), Ohta et al. (2011b) and Ohta et al. (2011c). 4.2 5 Although participation in this initial run of the PC task was somewhat limited, the two participating systems have been applied to a large variety of event extraction tasks over the last years and have shown consistently competitive performance with the state of the art (Bj¨orne and Salakoski, 2011; Miwa et al., 2012). It is thus reasonable to assume that the higher performance achieved by the Evaluation results Table 6 summarizes the primary evaluation results. The two systems demonstrate broadly similar performance in terms of F-scores, wi"
W13-2009,D07-1111,1,0.639577,"prediction. The feature representations of both systems draw on substructures of dependency-like representations of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011), Ohta et al. (2011b) and Ohta et al. (2011c). 4.2 5 Although participation in this initial run of"
W13-2009,I05-2038,1,0.757182,"s of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011), Ohta et al. (2011b) and Ohta et al. (2011c). 4.2 5 Although participation in this initial run of the PC task was somewhat limited, the two participating systems have been applied to a large variety of event"
W13-2009,J08-1002,1,0.790519,"ion, triggerargument relation detection, argument grouping into event structures, and modification prediction. The feature representations of both systems draw on substructures of dependency-like representations of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011),"
W13-2009,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2009,W09-1401,1,\N,Missing
W13-2009,E12-2021,1,\N,Missing
W13-2012,W13-2009,1,0.627053,"biomedical natural language processing (BioNLP), automatic extraction of biomedical events from texts becomes practical and the extracted events have been successfully employed in several applications, such as EVEX (Bj¨orne et al., 2012; Van Landeghem et al., 2013) and PathText (Miwa et al., 2013a). The practical applications reveal a problem in that both event types and structures need to be covered more widely. The BioNLP Shared Task 2013 (BioNLP-ST 2013) offers several tasks addressing the problem, and especially in the Cancer Genetics (CG) (Pyysalo et al., 2013) and Pathway Curation (PC) (Ohta et al., 2013) tasks, new entity/event types and biomedical problems are focused. Among dozens of extraction systems proposed during and after the two previous BioNLP shared tasks (Kim et al., 2011; Kim et al., 2012; Pyysalo et al., 2012b), EventMine (Miwa et al., 2012)1 has been applied to several biomedical event extraction corpora, and it achieved the state-of-theart performance in several corpora (Miwa et al., 2013b). In these tasks, an event associates with 1 2 EventMine for CG and PC Tasks This section briefly introduces EventMine and the PC and CG tasks, and then explains its task specific configurat"
W13-2012,W12-2410,1,0.884482,"Missing"
W13-2012,W11-0215,1,0.845,"xternal corpora in the stacked models for the PC task. We train models for the CG task using the configuration described above. For PC, in addition to the configuration, we incorporated a stacking 3 4 Recall 42.87 43.37 43.59 Precision 47.72 46.42 48.77 F-score 45.16 44.84 46.04 Table 1: Effect of the type generalisations for expanding possible instances (+Exp.) and stacking method (+Stack.) on the PC development data set. method (Wolpert, 1992) using the models with the same configuration for seven other available corpora: GENIA, EPI, ID, DNA methylation (Ohta et al., 2011a), Exhaustive PTM (Pyysalo et al., 2011), mTOR (Ohta et al., 2011b) and CG. The prediction scores of all the models are used as additional features in the detectors. Although some corpora may not directly relate to the PC task and models trained on such corpora can produce noisy features, we use all the corpora without selection since the stacking often improve the performance, e.g., (Pyysalo et al., 2012a; Miwa et al., 2013b). 3 Evaluation We first evaluate the type generalisations for expanding possible event structures and the stacking method in Table 1. The scores were calculated using the evaluation script provided by the organ"
W13-2012,W13-2008,1,0.26313,"PC task. 1 Introduction With recent progress in biomedical natural language processing (BioNLP), automatic extraction of biomedical events from texts becomes practical and the extracted events have been successfully employed in several applications, such as EVEX (Bj¨orne et al., 2012; Van Landeghem et al., 2013) and PathText (Miwa et al., 2013a). The practical applications reveal a problem in that both event types and structures need to be covered more widely. The BioNLP Shared Task 2013 (BioNLP-ST 2013) offers several tasks addressing the problem, and especially in the Cancer Genetics (CG) (Pyysalo et al., 2013) and Pathway Curation (PC) (Ohta et al., 2013) tasks, new entity/event types and biomedical problems are focused. Among dozens of extraction systems proposed during and after the two previous BioNLP shared tasks (Kim et al., 2011; Kim et al., 2012; Pyysalo et al., 2012b), EventMine (Miwa et al., 2012)1 has been applied to several biomedical event extraction corpora, and it achieved the state-of-theart performance in several corpora (Miwa et al., 2013b). In these tasks, an event associates with 1 2 EventMine for CG and PC Tasks This section briefly introduces EventMine and the PC and CG tasks,"
W13-2012,D07-1111,0,0.0732521,"nces in the training data, we allow the creation of the latter instances by generalising the triggers, i.e., REGULATION:Theme-Gene expression, and we used all the created instances for classification. The type generalisations may incorporate noisy instances but they pose the possibility to find unannotated event structures. To avoid introducing unexpected event structures, we apply the generalisations only to the regulation trigger types. We basically follow the setting for EPI in Miwa et al. (2012). We employ a deep syntactic parser Enju (Miyao and Tsujii, 2008) and a dependency parser GDep (Sagae and Tsujii, 2007). We utilise liblinear-java (Fan et al., 2008)3 with the L2-regularised L2-loss linear SVM setting for the SVM implementation, and Snowball4 for the stemmer. We, however, use no external resources (e.g., dictionaries) or tools (e.g., a coreference resolver) except for the external corpora in the stacked models for the PC task. We train models for the CG task using the configuration described above. For PC, in addition to the configuration, we incorporated a stacking 3 4 Recall 42.87 43.37 43.59 Precision 47.72 46.42 48.77 F-score 45.16 44.84 46.04 Table 1: Effect of the type generalisations fo"
W13-2012,J08-1002,0,0.0920307,"are no Positive regulation:Theme-Gene expression instances in the training data, we allow the creation of the latter instances by generalising the triggers, i.e., REGULATION:Theme-Gene expression, and we used all the created instances for classification. The type generalisations may incorporate noisy instances but they pose the possibility to find unannotated event structures. To avoid introducing unexpected event structures, we apply the generalisations only to the regulation trigger types. We basically follow the setting for EPI in Miwa et al. (2012). We employ a deep syntactic parser Enju (Miyao and Tsujii, 2008) and a dependency parser GDep (Sagae and Tsujii, 2007). We utilise liblinear-java (Fan et al., 2008)3 with the L2-regularised L2-loss linear SVM setting for the SVM implementation, and Snowball4 for the stemmer. We, however, use no external resources (e.g., dictionaries) or tools (e.g., a coreference resolver) except for the external corpora in the stacked models for the PC task. We train models for the CG task using the configuration described above. For PC, in addition to the configuration, we incorporated a stacking 3 4 Recall 42.87 43.37 43.59 Precision 47.72 46.42 48.77 F-score 45.16 44.8"
W13-2012,W11-0214,0,0.0195847,"oreference resolver) except for the external corpora in the stacked models for the PC task. We train models for the CG task using the configuration described above. For PC, in addition to the configuration, we incorporated a stacking 3 4 Recall 42.87 43.37 43.59 Precision 47.72 46.42 48.77 F-score 45.16 44.84 46.04 Table 1: Effect of the type generalisations for expanding possible instances (+Exp.) and stacking method (+Stack.) on the PC development data set. method (Wolpert, 1992) using the models with the same configuration for seven other available corpora: GENIA, EPI, ID, DNA methylation (Ohta et al., 2011a), Exhaustive PTM (Pyysalo et al., 2011), mTOR (Ohta et al., 2011b) and CG. The prediction scores of all the models are used as additional features in the detectors. Although some corpora may not directly relate to the PC task and models trained on such corpora can produce noisy features, we use all the corpora without selection since the stacking often improve the performance, e.g., (Pyysalo et al., 2012a; Miwa et al., 2013b). 3 Evaluation We first evaluate the type generalisations for expanding possible event structures and the stacking method in Table 1. The scores were calculated using th"
W13-2012,U13-1019,0,\N,Missing
W13-2310,liakata-etal-2010-corpora,0,0.0332013,"012a; Rak et al., 2012b) is a web-based platform that allows multiple branching and merging of UIMA pipelines. It incorporates several U-Compare components and consequently, supports the U-Compare type system. decomposes pipelines into parallel sub-workflows, each linked to a different annotation scheme. The resulting annotations produced by each subworkflow can be either merged within a single document or visualised in parallel views. 2 Related work Previous studies have shown the advantages of comparing and integrating different annotation schemes on a corpus of documents (Guo et al., 2010; Liakata et al., 2010; Liakata et al., 2012b). Guo et al. (2010) compared three different discourse annotation schemes applied to a corpus of biomedical abstracts on cancer risk assessment and concluded that two of the schemes provide more fine-grained information than the other scheme. They also revealed a subsumption relation between two schemes. Such outcomes from comparing schemes are meaningful for users who wish to select the most appropriate scheme for annotating their data. Liakata et al. (2012) underline that different discourse annotation schemes capture different dimensions of discourse. Hence, there mi"
W13-2310,W01-1605,0,0.0889504,"usually treat the text as a sequence of coherent textual zones (e.g., clauses and sentences). One line of research has been to identify which zones are logically connected to each other, and to characterise these links through the assignment of discourse relations. There are variations in the complexity of the schemes used to annotate these discourse relations. For example, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) defines 23 types of discourse relations that are used to structure the text into complex discourse trees. Whilst this scheme was used to enrich the Penn TreeBank (Carlson et al., 2001), the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) used another scheme to identify discourse relations that hold between pairs of text spans. It categorises the relations into types such as “causal”, “temporal” and “conditional”, which can be either explicit or implicit, depending on whether or ∗ The authors have contributed equally to the development of this work and production of the manuscript. 79 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 79–88, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics ti"
W13-2310,P02-1022,0,0.351528,"the different schemes could complement each other in order to lay the foundations for a possible future harmonisation of the schemes. The results of this analysis provide evidence that it would be useful to carry out further such analyses involving other such schemes, including an investigaIn this paper, we describe our extensions to UCompare, supporting the integration and visualisation of resources annotated according to multiple discourse annotation schemes. Our method 80 entity recognition and several of them allow the representation and analysis of discourse phenomena (Kano et al., 2011; Cunningham et al., 2002; Savova et al., 2010; Gurevych et al., 2007). However, none of them has demonstrated the integration of resources annotated according to multiple annotation schemes within a single NLP pipeline. GATE (Cunningham et al., 2002) is an open source NLP infrastructure that has been used for the development of various language processing tasks. It is packaged with an exhaustive number of NLP components, including discourse analysis modules, e.g., coreference resolution. Furthermore, GATE offers a GUI environment and wrappers for UIMA-compliant components. However, GATE implements a limited workflow"
W13-2310,W12-4305,1,0.899124,"Missing"
W13-2310,X96-1043,0,0.0287249,"n of the GENIA-MK scheme can be used to assign confidence values to the Conclusion, Result, Implication and Hypothesis categories of CoreSC and DiscSeg. In contrast to previous studies, our proposed approach automatically integrates multiple annotation schemes. The proposed mechanism allows users to easily compare, integrate and visualise multiple discourse annotation schemes in an interoperable NLP infrastructure, i.e., UCompare. There are currently a number of freely-available NLP workflow infrastructures (Ferrucci and Lally, 2004; Cunningham et al., 2002; Sch¨afer, 2006; Kano et al., 2011; Grishman, 1996; Baumgartner et al., 2008; Hahn et al., 2008; Savova et al., 2010; Gurevych et al., 2007; Rak et al., 2012b). Most of the available infrastructures support the development of standard NLP applications, e.g., partof-speech tagging, deep parsing, chunking, named 3 A UIMA architecture for processing multiple annotation schemes In UIMA, a document, together with its associated annotations, is represented as a standardised data structure, namely the Common Analysis Structure (CAS). Each CAS can contain any number of nested sub-CASes, i.e., Subjects of Analysis (Sofas), each of which can associate"
W13-2310,W10-1913,0,0.0142527,"rgo (Rak et al., 2012a; Rak et al., 2012b) is a web-based platform that allows multiple branching and merging of UIMA pipelines. It incorporates several U-Compare components and consequently, supports the U-Compare type system. decomposes pipelines into parallel sub-workflows, each linked to a different annotation scheme. The resulting annotations produced by each subworkflow can be either merged within a single document or visualised in parallel views. 2 Related work Previous studies have shown the advantages of comparing and integrating different annotation schemes on a corpus of documents (Guo et al., 2010; Liakata et al., 2010; Liakata et al., 2012b). Guo et al. (2010) compared three different discourse annotation schemes applied to a corpus of biomedical abstracts on cancer risk assessment and concluded that two of the schemes provide more fine-grained information than the other scheme. They also revealed a subsumption relation between two schemes. Such outcomes from comparing schemes are meaningful for users who wish to select the most appropriate scheme for annotating their data. Liakata et al. (2012) underline that different discourse annotation schemes capture different dimensions of disc"
W13-2310,prasad-etal-2008-penn,0,0.0266643,"nes (e.g., clauses and sentences). One line of research has been to identify which zones are logically connected to each other, and to characterise these links through the assignment of discourse relations. There are variations in the complexity of the schemes used to annotate these discourse relations. For example, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) defines 23 types of discourse relations that are used to structure the text into complex discourse trees. Whilst this scheme was used to enrich the Penn TreeBank (Carlson et al., 2001), the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) used another scheme to identify discourse relations that hold between pairs of text spans. It categorises the relations into types such as “causal”, “temporal” and “conditional”, which can be either explicit or implicit, depending on whether or ∗ The authors have contributed equally to the development of this work and production of the manuscript. 79 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 79–88, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics tion of how discourse relations and functional discourse ann"
W13-2310,E12-2021,1,0.854939,"Missing"
W13-2310,rak-etal-2012-collaborative,1,0.909732,"l., 2011) have been annotated with various types of functional discourse information. A solution to the challenges introduced above is offered by the Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004), which defines a common workflow metadata format facilitating the straightforward combination of NLP resources into a workflow. Based on the interoperability of the UIMA framework, numerous researchers distribute their own tools as UIMA-compliant components (Kano et al., 2011; Baumgartner et al., 2008; Hahn et al., 2008; Savova et al., 2010; Gurevych et al., 2007; Rak et al., 2012b). However, UIMA is only intended to provide an abstract framework for the interoperability of language resources, leaving the actual implementation to third-party developers. Hence, UIMA does not explicitly address interoperability issues of tools and corpora. U-Compare (Kano et al., 2011) is a UIMAbased workflow construction platform that provides a graphical user interface (GUI) via which users can rapidly create NLP pipelines using a drag-and-drop mechanism. Conforming to UIMA standards, U-Compare components and pipelines are compatible with any UIMA application via a common and sharable"
W13-2310,E99-1015,0,0.0881028,"g to their specific function in the discourse. Examples of functional discourse annotations include whether a particular zone asserts new information into the discourse or represents a speculation or hypothesis. In scientific texts, knowing the type of information that a zone represents (e.g., background knowledge, hypothesis, experimental observation, conclusion, etc.) allows for automatic isolation of new knowledge claims (S´andor and de Waard, 2012). Several annotation schemes have been developed to classify textual zones according to their rhetorical status or general information content (Teufel et al., 1999; Mizuta et al., 2006; Wilbur et al., 2006; de Waard and Pander Maat, 2009; Liakata et al., 2012a). Related to these studies are efforts to capture information relating to discourse function at the level of events, i.e., structured representations of pieces of knowledge which, when identified, facilitate sophisticated semantic searching (Ananiadou et al., 2010). Since there can be multiple events in a sentence or clause, the identification of discourse information at the event level can allow for a more detailed analysis of discourse elements than is possible when considering larger units of t"
W13-2310,W12-4302,0,0.0257497,"Missing"
W13-2310,W06-2714,0,\N,Missing
W13-2311,buyko-etal-2008-ontology,0,0.0296346,"our approach in that it incorporates certain RDF Turtle, SPARQL-like semantics. Contrary to the aforementioned solutions, we do not define any new language or syntax. Instead, we rely completely on an existing data query and manipulation language, SPARQL. By doing so, we shift the problem of conversion from the definition of a new language to representing UIMA structures in an existing language, such that they can be conveniently manipulated in that language. A separate line of research pertains to the formalisation of textual annotations with knowledge representations such as RDF and OWL4 . Buyko et al. (2008) link UIMA annotations to the reference ontology OLiA (Chiarcos, 2012) that contains a broad vocabulary of linguistic terminology. The authors claim that two conceptually similar type systems can be aligned with the reference ontology. The linking involves the use of OLiA’s associated annotation and linking ontology model pairs that have been created for a number of annotation schemata. Furthermore, a UIMA type system has to define additional features for each linked type that tie a given type to an annotation model. In effect, in order to convert a type from an arbitrary type system to anothe"
W13-2311,W11-3307,1,0.843407,"ing UIMA Truly Interoperable with SPARQL Rafal Rak and Sophia Ananiadou National Centre for Text Mining School of Computer Science, University of Manchester {rafal.rak,sophia.ananiadou}@manchester.ac.uk Abstract work that supports the interoperability of mediaprocessing software components by defining common data structures and interfaces the components exchange and implement. The architecture has been gaining interest from academia and industry alike for the past decade, which resulted in a multitude of UIMA-supporting repositories of analytics. Notable examples include METANET4U components (Thompson et al., 2011) featured in U-Compare1 , DKPro (Gurevych et al., 2007), cTAKES (Savova et al., 2010), BioNLP-UIMA Component Repository (Baumgartner et al., 2008), and JULIE Lab’s UIMA Component Repository (JCoRe) (Hahn et al., 2008). However, despite conforming to the UIMA standard, each repository of analytics usually comes with its own set of type systems, i.e., representations of data models that are meant to be shared between analytics and thus ensuring their interoperability. At present, UIMA does not facilitate the alignment of (all or selected) types between type systems, which makes it impossible to"
W13-2311,chiarcos-2012-ontologies,0,0.0144568,"ntics. Contrary to the aforementioned solutions, we do not define any new language or syntax. Instead, we rely completely on an existing data query and manipulation language, SPARQL. By doing so, we shift the problem of conversion from the definition of a new language to representing UIMA structures in an existing language, such that they can be conveniently manipulated in that language. A separate line of research pertains to the formalisation of textual annotations with knowledge representations such as RDF and OWL4 . Buyko et al. (2008) link UIMA annotations to the reference ontology OLiA (Chiarcos, 2012) that contains a broad vocabulary of linguistic terminology. The authors claim that two conceptually similar type systems can be aligned with the reference ontology. The linking involves the use of OLiA’s associated annotation and linking ontology model pairs that have been created for a number of annotation schemata. Furthermore, a UIMA type system has to define additional features for each linked type that tie a given type to an annotation model. In effect, in order to convert a type from an arbitrary type system to another similar type system, both systems must be modified and an annotation"
W13-2311,pazienza-etal-2012-pearl,0,0.110942,"IMA annotations. The rules are encoded in XML, and– contrary to the previous language that relies solely on its own syntax—include XPath expressions for patterns, constraints, and assigning values to new Related Work In practice, type alignment or conversion is the creation of new UIMA feature structures based on the existing ones. Current efforts in this area mostly involve solutions that are essentially 2 http://www.w3.org/TR/2013/REC-sparql11-overview20130321 3 http://www.w3.org/RDF/ 91 feature structures. This implies that the input of the conversion process must be encoded in XML. PEARL (Pazienza et al., 2012) is a language for projecting UIMA annotations onto RDF repositories. Similarly to the previous approaches, the language defines a set of rules triggered upon encountering UIMA annotations. The language is designed primarily to work in CODA, a platform that facilitates population of ontologies with the output of NLP analytics. Although it does not directly facilitate the production or conversion of UIMA types, the PEARL language shares similarities to our approach in that it incorporates certain RDF Turtle, SPARQL-like semantics. Contrary to the aforementioned solutions, we do not define any n"
W13-2311,hernandez-2012-tackling,0,\N,Missing
W13-2512,P02-1051,0,0.118069,"Missing"
W13-2512,W03-0428,0,0.0102285,"capture. Secondly, RF is considered one of the most accurate classifier available (D´ıaz-Uriarte and De Andres, 2006; Jiang et al., 2007). Finally, RF is reported to cope well with datasets where the number of features is larger than the number of observations (D´ıaz-Uriarte and De Andres, 2006). In our dataset, the number of features is almost four times more than that of the observations. We represent pairs of terms using character gram features (i.e., first order features). Such shallow features have been proven effective in a number of NLP applications including: Named Entity Recognition (Klein et al., 2003), Multilingual Named Entity Transliteration (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) and 2 Related Work In this section, we review previous approaches that exploit the internal structure of sequences to align terms or named entities across languages. (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) use character gram features, similar to the feature space that we propose in this paper, to train discriminative, supervised models. Klementiev and Roth (2006) introduce a supervised Perceptron model for English and Russian named entities. They construct a character gram feature"
W13-2512,J93-2003,0,0.0365567,"nnot directly associate the source with the target character grams. 2 3 98 www2.chkd.cnki.net/kns50/ nlm.nih.gov/research/umls Figure 1: Example of a term construction rule as a branch in a decision tree. Input pair of English-French terms : (e1 , e2 , e3 , f1 , f2 , f3 ) English first order French first order Second order φ1 (e1 , e2 ) φ1 (f1 , f2 ) φ1 (e1 e2 , f1 f2 ), φ1 (e1 e2 , f2 f3 ) φ1 (e2 , e3 ) φ1 (f2 , f3 ) φ1 (e2 e3 , f1 f2 ), φ1 (e2 e3 , f2 f3 ) Table 2: Example of first and second order features using a predefined n-gram size of 2. open source implementation of the 5 IBM-models (Brown et al., 1993). GIZA++ is traditionally trained on a bilingual, parallel corpus of aligned sentences and estimates the probability P (s|t) of a source translation unit (typically a word), s, given a target unit t. To apply GIZA++ on our dataset, we consider the list of terms as parallel sentences. GIZA++, trained on a list of terms, estimates the alignment probability of English-Chinese and English-French textual units, i.e. character ngrams. Each entry i, j in the translation table is the probability P (si |tj ), where si and tj are the source and target character n-grams in row i and column j, respectivel"
W13-2512,P06-1103,0,0.0299983,"ssifier available (D´ıaz-Uriarte and De Andres, 2006; Jiang et al., 2007). Finally, RF is reported to cope well with datasets where the number of features is larger than the number of observations (D´ıaz-Uriarte and De Andres, 2006). In our dataset, the number of features is almost four times more than that of the observations. We represent pairs of terms using character gram features (i.e., first order features). Such shallow features have been proven effective in a number of NLP applications including: Named Entity Recognition (Klein et al., 2003), Multilingual Named Entity Transliteration (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) and 2 Related Work In this section, we review previous approaches that exploit the internal structure of sequences to align terms or named entities across languages. (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) use character gram features, similar to the feature space that we propose in this paper, to train discriminative, supervised models. Klementiev and Roth (2006) introduce a supervised Perceptron model for English and Russian named entities. They construct a character gram feature space as follows: firstly, they extract all distinct character grams f"
W13-2512,W02-0902,0,0.0326977,"erms from comparable corpora would be to directly classify all possible pairs of terms into translations or non-translations. However, in comparable corpora, the size of the search space is quadratic to the input data. Therefore, the classification task is much more challenging since the distribution of positive and negative instances is highly skewed. To cope with the vast search space of comparable corpora, we plan to incorporate context-based approaches with the RF classification method. Context-based approaches, such as distributional vector similarity (Fung and McKeown, 1997; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008), can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity. Finally, the proposed method can be also used to online augment the phrase table of Statistical Machine Translation (SMT) in order to better handle the Out-of-Vocabulary problem i.e. inability to translate textual units that consist of one or more words and do not occur in the training data (Habash, 2008). creased. Table 3 summarises the highest performance achieved by the RF, SVMs, GIZA++ and Levenshtein distance all trained and tested on the same"
W13-2512,2005.iwslt-1.1,0,0.0371976,"ure space that we propose in this paper, to train discriminative, supervised models. Klementiev and Roth (2006) introduce a supervised Perceptron model for English and Russian named entities. They construct a character gram feature space as follows: firstly, they extract all distinct character grams from both source and target named entity. Then, they pair character grams of the source named entity with character grams of the corresponding target named entity into features. In or96 Lepage and Denoual (2005) presented an analogical learning machine translation system as part of the IWSLT task (Eck and Hori, 2005) that requires no training process and it is able to achieve state-of-the art performance. The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?] (Lepage, 1998). Analogical learning has been proven effective in translating unseen words (Langlais and Patry, 2007). Furthermore, analogical learning is reported to achieve a better precision but a lower recall than a p"
W13-2512,P07-2045,0,0.00387293,"hat are transliterated, i.e. present phonetic similarities, across languages. SMT-based approaches built on top of existing SMT frameworks to identify translation pairs of terms (Tsunakawa et al., 2008; Wu et al., 2008). Tsunakawa et al. (2008), align terms between a source language Ls and a target language Lt using a pivot language Lp . They assume that two bilingual dictionaries exist: from Ls to Lp and from Lp to Lt . Then, they train GIZA++ (Och and Ney, 2003) on both directions and they merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by translitera"
W13-2512,W04-3248,0,0.0319651,"phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target language). The resulting hybrid systems were reported to perform at least as well as existing SMT systems (Feng et al., 2004). 3 Methodology Let em = (e1 , · · · , em ) be an English term consisting of m translation units and f n = (f1 , · · · , f"
W13-2512,D07-1025,0,0.381372,"iarte and De Andres, 2006; Jiang et al., 2007). Finally, RF is reported to cope well with datasets where the number of features is larger than the number of observations (D´ıaz-Uriarte and De Andres, 2006). In our dataset, the number of features is almost four times more than that of the observations. We represent pairs of terms using character gram features (i.e., first order features). Such shallow features have been proven effective in a number of NLP applications including: Named Entity Recognition (Klein et al., 2003), Multilingual Named Entity Transliteration (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) and 2 Related Work In this section, we review previous approaches that exploit the internal structure of sequences to align terms or named entities across languages. (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) use character gram features, similar to the feature space that we propose in this paper, to train discriminative, supervised models. Klementiev and Roth (2006) introduce a supervised Perceptron model for English and Russian named entities. They construct a character gram feature space as follows: firstly, they extract all distinct character grams from both source and target n"
W13-2512,D07-1092,0,0.0172441,"age and Denoual (2005) presented an analogical learning machine translation system as part of the IWSLT task (Eck and Hori, 2005) that requires no training process and it is able to achieve state-of-the art performance. The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?] (Lepage, 1998). Analogical learning has been proven effective in translating unseen words (Langlais and Patry, 2007). Furthermore, analogical learning is reported to achieve a better precision but a lower recall than a phrasebased machine translation system when translating medical terms (Langlais et al., 2009). der to reduce the number of features, they link only those character grams whose position offsets in the source and target sequence differs by -1, 0 or 1. Freitag and Khadivi (2007) employ the same character gram feature space but they do not constraint the included character-grams to their relative position offsets in the source and target sequence. The boolean features are defined for every distin"
W13-2512,E09-1056,0,0.0190269,"the art performance. The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?] (Lepage, 1998). Analogical learning has been proven effective in translating unseen words (Langlais and Patry, 2007). Furthermore, analogical learning is reported to achieve a better precision but a lower recall than a phrasebased machine translation system when translating medical terms (Langlais et al., 2009). der to reduce the number of features, they link only those character grams whose position offsets in the source and target sequence differs by -1, 0 or 1. Freitag and Khadivi (2007) employ the same character gram feature space but they do not constraint the included character-grams to their relative position offsets in the source and target sequence. The boolean features are defined for every distinct character-grams observed in the data of length k or shorter. Using this feature space they train an Averaged Perceptron model, able to incorporate an arbitrary number of features in the input v"
W13-2512,P08-2015,0,0.0219904,"th the RF classification method. Context-based approaches, such as distributional vector similarity (Fung and McKeown, 1997; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008), can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity. Finally, the proposed method can be also used to online augment the phrase table of Statistical Machine Translation (SMT) in order to better handle the Out-of-Vocabulary problem i.e. inability to translate textual units that consist of one or more words and do not occur in the training data (Habash, 2008). creased. Table 3 summarises the highest performance achieved by the RF, SVMs, GIZA++ and Levenshtein distance all trained and tested on the same dataset. The resulting performance of the RF compared with GIZA++ is statistically significant (p &lt; 0.0001) in all experiments. Comparing the RF with the SVMs, we note that in the EnglishFrench dataset, the performance of the SVM-RBF is approximately the same with the performance of our proposed method. However, this comes with a cost. Firstly, SVMs can possibly achieve a comparable performance to the RF when using multilingual, second order feature"
W13-2512,P98-1120,0,0.0165174,"y with character grams of the corresponding target named entity into features. In or96 Lepage and Denoual (2005) presented an analogical learning machine translation system as part of the IWSLT task (Eck and Hori, 2005) that requires no training process and it is able to achieve state-of-the art performance. The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?] (Lepage, 1998). Analogical learning has been proven effective in translating unseen words (Langlais and Patry, 2007). Furthermore, analogical learning is reported to achieve a better precision but a lower recall than a phrasebased machine translation system when translating medical terms (Langlais et al., 2009). der to reduce the number of features, they link only those character grams whose position offsets in the source and target sequence differs by -1, 0 or 1. Freitag and Khadivi (2007) employ the same character gram feature space but they do not constraint the included character-grams to their relative"
W13-2512,P08-1088,0,0.0277,"pora would be to directly classify all possible pairs of terms into translations or non-translations. However, in comparable corpora, the size of the search space is quadratic to the input data. Therefore, the classification task is much more challenging since the distribution of positive and negative instances is highly skewed. To cope with the vast search space of comparable corpora, we plan to incorporate context-based approaches with the RF classification method. Context-based approaches, such as distributional vector similarity (Fung and McKeown, 1997; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008), can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity. Finally, the proposed method can be also used to online augment the phrase table of Statistical Machine Translation (SMT) in order to better handle the Out-of-Vocabulary problem i.e. inability to translate textual units that consist of one or more words and do not occur in the training data (Habash, 2008). creased. Table 3 summarises the highest performance achieved by the RF, SVMs, GIZA++ and Levenshtein distance all trained and tested on the same dataset. The resulting"
W13-2512,Y06-1018,0,0.0190735,"one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target language). The resulting hybrid systems were reported to perform at least as well as existing SMT systems (Feng et al., 2004). 3 Methodology Let em = (e1 , · · · , em ) be an English term consisting of m translation units and f n = (f1 , · · · , fn ) a French or Chin"
W13-2512,2007.mtsummit-papers.65,0,0.0368839,"Missing"
W13-2512,J03-1002,0,0.00851808,"ove character gram based methods mainly focused on aligning named entities of the general domain, i.e. person names, locations, organizations, etc., that are transliterated, i.e. present phonetic similarities, across languages. SMT-based approaches built on top of existing SMT frameworks to identify translation pairs of terms (Tsunakawa et al., 2008; Wu et al., 2008). Tsunakawa et al. (2008), align terms between a source language Ls and a target language Lt using a pivot language Lp . They assume that two bilingual dictionaries exist: from Ls to Lp and from Lp to Lt . Then, they train GIZA++ (Och and Ney, 2003) on both directions and they merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in va"
W13-2512,2008.amta-papers.19,1,0.858003,"Missing"
W13-2512,W07-0704,0,0.0362204,"Missing"
W13-2512,popovic-ney-2004-towards,0,0.0255953,"They assume that two bilingual dictionaries exist: from Ls to Lp and from Lp to Lt . Then, they train GIZA++ (Och and Ney, 2003) on both directions and they merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target language). The resulting hybrid systems were reported to perform at least"
W13-2512,P95-1050,0,0.180709,"onaries of terms from comparable corpora would be to directly classify all possible pairs of terms into translations or non-translations. However, in comparable corpora, the size of the search space is quadratic to the input data. Therefore, the classification task is much more challenging since the distribution of positive and negative instances is highly skewed. To cope with the vast search space of comparable corpora, we plan to incorporate context-based approaches with the RF classification method. Context-based approaches, such as distributional vector similarity (Fung and McKeown, 1997; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008), can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity. Finally, the proposed method can be also used to online augment the phrase table of Statistical Machine Translation (SMT) in order to better handle the Out-of-Vocabulary problem i.e. inability to translate textual units that consist of one or more words and do not occur in the training data (Habash, 2008). creased. Table 3 summarises the highest performance achieved by the RF, SVMs, GIZA++ and Levenshtein distance all traine"
W13-2512,C04-1089,0,0.0282872,"merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target language). The resulting hybrid systems were reported to perform at least as well as existing SMT systems (Feng et al., 2004). 3 Methodology Let em = (e1 , · · · , em ) be an English term consisting of m translation units and f"
W13-2512,W03-1719,0,0.0259876,"of the 6th Workshop on Building and Using Comparable Corpora, pages 95–104, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics English Morpheme: -ache head-ache back-ache ear-ache Chinese Morpheme: 痛 头-痛 痛 腰-痛 痛 耳朵-痛 痛 French Morpheme: -mal mal de tˆete mal au dos mal d’oreille Table 1: An example of English, Chinese and French terms consisting of the same morphemes predicting authorship (Stamatatos, 2006). In addition, by selecting character n-grams instead of word n-grams, one avoids to segment words in Chinese which has been proven to be a challenging topic (Sproat and Emerson, 2003). We evaluate our proposed method on two datasets of biomedical terms (English-French and English-Chinese) that contain equal numbers of positive and negative instances. RF achieves higher classification performance than baseline methods. To boost SVM’s performance further, we used a second order feature space to represent the data. It consists of pairs of character grams that co-occur in translation pairs. In the second order feature space, the performance of SVMs improved significantly. The rest of the paper is structured as follows. In Section 2, we present previous approaches in identifyin"
W13-2512,tsunakawa-etal-2008-building-bilingual,1,0.923421,"fined for every distinct character-grams observed in the data of length k or shorter. Using this feature space they train an Averaged Perceptron model, able to incorporate an arbitrary number of features in the input vectors, for English and Arabic named entities. The above character gram based methods mainly focused on aligning named entities of the general domain, i.e. person names, locations, organizations, etc., that are transliterated, i.e. present phonetic similarities, across languages. SMT-based approaches built on top of existing SMT frameworks to identify translation pairs of terms (Tsunakawa et al., 2008; Wu et al., 2008). Tsunakawa et al. (2008), align terms between a source language Ls and a target language Lt using a pivot language Lp . They assume that two bilingual dictionaries exist: from Ls to Lp and from Lp to Lt . Then, they train GIZA++ (Och and Ney, 2003) on both directions and they merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical frag"
W13-2512,W04-3211,0,\N,Missing
W13-2512,W04-3242,0,\N,Missing
W13-2512,C98-1116,0,\N,Missing
W13-2512,W03-1726,0,\N,Missing
W14-1110,E12-2021,1,0.75402,"arge summaries and scientific articles. In discharge summaries, phenotype information is explicitly described in the patient’s medical history, diagnoses and test results. On the other hand, scientific articles summarise results and research findings. This means that certain types of information that occur frequently in discharge summaries are extremely rare in scientific articles, such that their occurrences are too sparse to be useful in training TM systems, and hence they were not annotated. The annotation was carried out by two medical doctors, using the Brat Rapid Annotation Tool (brat) (Stenetorp et al., 2012), a highlyconfigurable and flexible web-based tool for textual annotation. Annotations in the corpus should reflect the instructions provided in the guidelines as closely as possible, in order to ensure that the annotations are of ahigh quality. A standard means of providing evidence regarding the reliability of annotations in a corpus is to calculate a statistic known as the inter-annotator agreement (IAA). IAA provides assurance that different annotators can produce the same annotations when working independently and separately. There are several different methods of calculating IAA, which c"
W14-1110,W04-3111,0,0.0298856,"esign was guided by a domain expert, includes both entities and relations pertinent to CHF. Two further domain experts performed the annotation, resulting in high quality annotation, with agreement rates up to 0.92 F-Score. 1 2 Related work There are many well-known publicly available corpora of scientific biomedical literature, which are annotated for biological entities and/or their interactions (often referred to as events) (Roberts et al., 2009; Xia & Yetisgen-Yildiz, 2012). Examples include GENIA (Kim et al., 2008), BioInfer (Pyysalo et al., 2007) GREC (Thompson et al., 2009), PennBioIE (Kulick et al., 2004), GENETAG (Tanabe et al., 2005) and LLL’05 (Hakenberg et al., 2005). However, none of these corpora is annotated with the types of entities and relationships that are relevant to the study of phenotype information. On the other hand, corpora of clinical text drawn from EHRs are rare, due to privacy and confidentiality concerns, but also because of the time-consuming, expensive and tedious nature of producing high quality annotations, which are reliant on the expertise of domain experts (Uzuner et al., 2011). A small number of corpora, however, have been made available, mainly in the context of"
W14-1110,ogren-etal-2008-constructing,0,0.0263295,"ased a corpus of de-identified clinical records annotated to support a number of IE challenges with multiple levels of annotation, i.e., entities and relations (Uzuner et al., 2008; Uzuner, 2009). The 2010 challenge included the release of a corpus of discharge summaries and patient reports in which named entities and relations concerning medical problems, tests and treatments were annotated (Uzuner et al., 2011). A corpus of EHRs from Mayo Clinic has been annotated with both linguistic information (partof–speech tags and shallow parsing results) and named entities corresponding to disorders (Ogren et al., 2008; Savova et al., 2010). 3 shared tasks. The schema is based on a set of requirements developed by a cardiologist. Taking into account our chosen focus of annotating phenotype information relating to the CHF disease, the cardiologist was asked firstly to determine a set of relevant entity types that relate to CHF phenotype information and the role of the decline in kidney function in the cycle of CHF (exemplified in Table 1), secondly to locate words that modify the entity (such as polarity clues) and thirdly to identify the types of relationships that exist between these entity types in the de"
W14-1110,W07-1013,0,0.0405756,"On the other hand, corpora of clinical text drawn from EHRs are rare, due to privacy and confidentiality concerns, but also because of the time-consuming, expensive and tedious nature of producing high quality annotations, which are reliant on the expertise of domain experts (Uzuner et al., 2011). A small number of corpora, however, have been made available, mainly in the context of shared task challenges, which aim to encourage the development of information extraction (IE) systems. These corpora vary in terms of the text type and annotation granularity. For example, the corpus presented in (Pestian et al., 2007) concerns only structured data from radiology reports, while the corpus presented in (Meystre & Haug, 2006) contains unstructured parts of EHRs, but annotated with medical problem only at the document level. Other corpora are more similar to ours, in that that they include text-bound annotations Introduction An ever-increasing number of scientific articles is published every year. For example, in 2012, more than 500,000 articles were published in MEDLINE (U.S. National Library of Medicine , 2013). A researcher would thus need to review at least 20 articles per day in order to keep up to date w"
W14-1110,U06-1012,0,\N,Missing
W15-3804,P07-1034,0,0.0318926,"Missing"
W15-3804,W09-1418,0,0.034238,"e classifier. The main feature classes of the final feature set are listed below: BRAF is not active and is not required for MEK/ERK activation Figure 5: Dependency link representation example for a Biomedical sentence as analysed by Enju • Dependency type: Enju provides the dependency type (prepositional, coordination, noun modifier, etc) for each dependency link/edge. This type can be exploited either to assign different weights to each dependency edge of the argument-trigger path, or to consider different path patterns. Such manipulation has been employed in rule-based event extraction in (Kilicoglu and Bergler, 2009), achieving good accuracy but low recall. Also, extracting specific path patterns renders the approach dependent on a particular parser, thus limiting the independence and adaptability of its application. Since the focus of this study was on recall and adaptability, the dependency type information was ignored, except for the case that follows. • • • • • • Shortest dependency path (numeric) Entity Type (nominal) Participation in other events (binary) PoS (Part of Speech) (nominal) Context PoS (surrounding tokens) (nominal) Relative position to the event trigger (before/after) (nominal) • Depend"
W15-3804,W11-1828,0,0.0423606,"Missing"
W15-3804,W09-1401,0,0.308065,"of the event arguments are missing. Although such errors may not be of vital importance in all domains, they can be extremely detrimental when attempting to link an event to a biomedical model, since they can lead to erroneous or useless assertions. In text mining, events refer to units representing dynamic, n-ary relations between named entities. In the biomedical domain, this definition can be narrowed to units representing molecular interactions stated within textual documents (Bj¨orne and Salakoski, 2011). The typical structure of events (as defined and used in BioNLP shared tasks, e.g., (Kim et al., 2009), (N´edellec et al., 2013)) includes an obligatory predicate/trigger, i.e., a word sequence in text that characterises the event type. Potentially, an event may also have one or more arguments, i.e., entities in text that are semantically linked to the trigger. Considering the trigger and the arguments as nodes, the links between them can be considered as directed edges (from the trigger to the argument), which represent the role that the argument plays with respect to the trigger. As events are dynamic elements, the same entity can participate in different events, and may assume different rol"
W15-3804,W13-2003,0,0.0300707,"Missing"
W15-3804,W11-1801,0,0.0646967,"Missing"
W15-3804,W09-1403,0,0.052093,"Missing"
W15-3804,W13-2012,1,0.845901,"espect to the trigger. As events are dynamic elements, the same entity can participate in different events, and may assume different roles in each event. Also, since events are solid information units, they can themselves act as arguments to other events, leading to the extraction of complex/nested events (Bj¨orne et al., 2010). These characteristics can be observed in the example of Fig 1 presented in Section 1. The work described here focusses on resolving the problem by applying a generic constraint relaxation post-processing strategy to the output of an event extraction system (EventMine (Miwa and Ananiadou, 2013)), with the aim of reducing the number of recognised events that have missing arguments. Motivated by an analysis of the Big Mechanism testing corpus described in Section 3, we relax the annotation constraints related to argument roles and subsequently reconsider all the entities within a sentence that are valid argument candidates, by exploiting syntactical dependencies. We employ the confidence values obtained from an Adaboost (Freund and Schapire, 1997) classifier to rank candidate arguments for each event trigger, and to determine which of them constitute valid additions to the event. Usin"
W15-3804,P08-1006,0,0.028059,"Missing"
W15-3804,W13-2001,0,0.0284258,"Missing"
W15-3804,W11-0215,0,0.0177843,"one of the new corpus with various methods such as kernel based ones (Daum´e III, 2009; Kulis et al., 2011) or transfer component analysis (Pan et al., 2011). Finally, (Miwa et al., 2013) suggests the use of a filtering model, which consid3 Corpora and Annotation Considerations 3.1 Training Corpora For training the wide coverage method was applied on the combination of the training sets of the following corpora, treated as described in (Miwa et al., 2013) : Genia09 of BioNLP ’09 (Kim et al., 2009), Genia11, EPI & ID of BioNLP ’11 (Kim et al., 2011), DNA-methylation (Ohta et al., 2011), ePTM (Pyysalo et al., 2011), mTOR (Caron et al., 2010), and MLEE (Pyysalo et al., 2012). 3.2 Testing Corpora The corpus that provided the motivation for this work, henceforth referred to as BM, is a small annotated set of six passages extracted from fulltext biomedical research papers in PubMed 2 . It concerns cancer pathway curation and was manually annotated with biomedical named entities and events by expert biologists participating in the Big Mechanism project (Cohen, 2014). In total it consists of 155 event and 247 named entity annotations. The range of the entities and events annotated render it a valid candidate"
W15-3804,D09-1101,0,0.0539929,"Missing"
W15-3804,D11-1001,0,0.023888,"ul insights into factors that could further improve performance. 2 Event Structure 2.2 Event Mining Architecture In order to extract structures of the complexity illustrated in Figure 1, current state-of-the-art systems break event extraction down into multiple classification tasks that have to be solved in order to produce the final structured event representation. The learning process to carry out these tasks can be undertaken either sequentially in a pipelined manner, as in EventMine (Miwa and Ananiadou, 2013) and TEES (Bj¨orne and Salakoski, 2013), or as joint learning task, as for FAUST (Riedel and McCallum, 2011). EventMine, the system employed in this work, utilises the pipelined approach, and consists of the following modules: Background: The Event Extraction Task in Biomedicine • Event trigger classifier: Identifies spans of text that act as triggers and annotates them with the corresponding type (event label). • Argument detector: Links each trigger with at most one argument and annotates the edge (link) with the corresponding argument role type. • Multiple argument detector: Adds additional arguments to the pairs of the previous step, finalIn this section we provide an overview of the event extra"
W15-3804,E12-2021,1,0.789136,"Missing"
W15-3804,D08-1067,0,\N,Missing
W15-3804,P07-1033,0,\N,Missing
W16-3921,W15-4319,0,0.0381493,"Missing"
W16-3921,N15-1075,0,0.0176698,"dings of the 2nd Workshop on Noisy User-generated Text, pages 153–163, Osaka, Japan, December 11 2016. of many researchers. Several efforts (Chieu and Ng, 2002; Florian et al., 2003; Ratinov and Roth, 2009), tackle this problem by hand-crafting linguistic features and presenting them to a machine learning algorithm, which will then discriminate between various semantic types based on those features. This particular approach, however, is usually done ad hoc making it very tedious to adapt features to other domains. Recently, extensions to this approach have been introduced (Godin et al., 2015; Cherry and Guo, 2015; Toh et al., 2015) by considering word embeddings as sources of features for classification. Other studies (Luo et al., 2015; Yamada123 et al., 2015) combined this machine learning-based approach with entity linking methods which exploit knowledge bases (e.g., Wikipedia) to detect named mentions. With the NLP research community’s continuously surging interest in deep learning, approaches based on deep neural networks have also been applied to NER. While they take away the burden that comes with handcrafting linguistic features, they require huge amounts of data. Nevertheless, they have been p"
W16-3921,C02-1025,0,0.414077,"ging scheme and applied dropout as a regularisation technique in order to reduce overfitting. Even without handcrafting linguistic features nor leveraging any of the W-NUT-provided gazetteers, we obtained robust performance with our approach, which ranked third amongst all shared task participants according to the official evaluation on a gold standard named entity-annotated corpus of 3,856 tweets. 1 Introduction Named entity recognition (NER) is one of the most fundamental natural language processing (NLP) tasks that is central to understanding unstructured, textual data. Popular approaches (Chieu and Ng, 2002; Florian et al., 2003; Ratinov and Roth, 2009; Luo et al., 2015) mainly rely on hand-crafted features and gazetteers which require knowledge about the domain. Recently, there has been a surge in terms of interest in applying deep learning techniques to NLP tasks. These methods, together with substantial amount of annotated data, can learn features automatically and have been reported to outperform traditional methods on certain NLP tasks (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016). However, in spite of the perceived success of deep learning methods particularly in NER in"
W16-3921,Q16-1026,0,0.275796,"sing (NLP) tasks that is central to understanding unstructured, textual data. Popular approaches (Chieu and Ng, 2002; Florian et al., 2003; Ratinov and Roth, 2009; Luo et al., 2015) mainly rely on hand-crafted features and gazetteers which require knowledge about the domain. Recently, there has been a surge in terms of interest in applying deep learning techniques to NLP tasks. These methods, together with substantial amount of annotated data, can learn features automatically and have been reported to outperform traditional methods on certain NLP tasks (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016). However, in spite of the perceived success of deep learning methods particularly in NER in newswire (Ma and Hovy, 2016), performance on social media content, particularly that from Twitter, has been lagging behind (Baldwin et al., 2015). There are two state-of-the-art deep learning methods for newswire NER, namely, those proposed by Chiu and Nichols (2016) and Ma and Hovy (2016). Considering that the former requires hand-crafted features (e.g., word matches against gazetteers) and yet obtains only a small boost in performance over the latter which does not rely on any such features, we took"
W16-3921,D10-1057,0,0.0322011,"2016; Ma and Hovy, 2016; Lample et al., 2016). For instance, state-of-the-art NER systems for newswire were built upon deep learning-based approaches (Ma and Hovy, 2016; Chiu and Nichols, 2016). It has been shown, however, that even such state-of-the-art methods tend to underperform when applied to other domains, particularly on social media content, e.g., tweets (Baldwin et al., 2015). Challenges in this domain include noise inherent in the data, the many new terms—neologisms—introduced in social media over time, and changes in linguistic conventions in general, also known as language drift (Dredze et al., 2010; Ritter et al., 2011; Eisenstein, 2013; Fromreide et al., 2014). In this paper, we describe our methods addressing the above-mentioned challenges in the context of the 2016 Named Entity Recognition in Twitter Task (Strauss et al., 2016) organised as part of the Second Workshop on Noisy User-generated Text (W-NUT). For our contribution to the said shared task, we combined deep learning-based approaches with distant supervision methods for generating weakly labelled data, and further optimised performance by exploring the use of different tagging schemes and word embeddings. 3 Methodology We ca"
W16-3921,P15-1033,0,0.0260276,"(e.g., from the first up to the current layer), the product becomes much smaller or bigger depending on the gradient value. This is called the vanishing/exploding gradient problem common in deep backpropagation investigated more deeply in Bengio et al. (1994) and Pascanu et al. (2013). To remedy this problem, we use a variant of RNNs known as the Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTM addresses the shortcomings of vanilla RNNs by allowing the network to capture information for an extended number of time steps. Furthermore, we incorporate the findings of Dyer et al. (2015) that bi-directional LSTM (BLSTM) provides more benefits since it allows access to both the past and future contexts, i.e., m words to each of the left and right sides of a given word, where m is the window size. 3.1 Neural Network Architecture The input to the network is a concatenation of character representations of words extracted from a convolutional neural network and word embeddings, which will be described in Section 3.3. The inputs are fed into a bi-directional LSTM (BLSTM) and the outputs are concatenated and fed into the CRF layer to jointly decode the best label sequence. For a sch"
W16-3921,N13-1037,0,0.0128772,"16). For instance, state-of-the-art NER systems for newswire were built upon deep learning-based approaches (Ma and Hovy, 2016; Chiu and Nichols, 2016). It has been shown, however, that even such state-of-the-art methods tend to underperform when applied to other domains, particularly on social media content, e.g., tweets (Baldwin et al., 2015). Challenges in this domain include noise inherent in the data, the many new terms—neologisms—introduced in social media over time, and changes in linguistic conventions in general, also known as language drift (Dredze et al., 2010; Ritter et al., 2011; Eisenstein, 2013; Fromreide et al., 2014). In this paper, we describe our methods addressing the above-mentioned challenges in the context of the 2016 Named Entity Recognition in Twitter Task (Strauss et al., 2016) organised as part of the Second Workshop on Noisy User-generated Text (W-NUT). For our contribution to the said shared task, we combined deep learning-based approaches with distant supervision methods for generating weakly labelled data, and further optimised performance by exploring the use of different tagging schemes and word embeddings. 3 Methodology We cast the NER task as a sequence labelling"
W16-3921,N09-1037,0,0.0272891,"er predictions (Ratinov and Roth, 2009; Dai et al., 2015). According to the popular begininside-outside (BIO) scheme, each token is tagged as any of ‘B’, ‘I’ or ‘O’ depending on whether it is at the beginning, inside or outside a named entity, respectively. The more fine-grained BIOES scheme, however, additionally makes use of ‘E’ and ‘S’ to also distinguish tokens at the end and those comprising single-token entities. Approaches to sequence labelling based on the conditional random fields (CRF) algorithm are known to demonstrate strong performance (McCallum and Li, 2003; Leaman et al., 2008; Finkel and Manning, 2009). CRFs find the most probable label sequence given a sequence of tokens encoded using features, e.g., surface forms, lemma, part-of-speech tags, surrounding tokens, morphology (Sang and Veenstra, 1999). Meanwhile, convolutional neural networks (CNNs) are sparse feed-forward neural networks which have been shown to effectively extract morphological features such as word prefixes and suffixes (Chiu and Nichols, 2016). One can thus explore the combination of the ability of CRFs to model relations between labels of tokens in a tweet, and the effectiveness of CNNs to extract morphological features"
W16-3921,W03-0425,0,0.376101,"ied dropout as a regularisation technique in order to reduce overfitting. Even without handcrafting linguistic features nor leveraging any of the W-NUT-provided gazetteers, we obtained robust performance with our approach, which ranked third amongst all shared task participants according to the official evaluation on a gold standard named entity-annotated corpus of 3,856 tweets. 1 Introduction Named entity recognition (NER) is one of the most fundamental natural language processing (NLP) tasks that is central to understanding unstructured, textual data. Popular approaches (Chieu and Ng, 2002; Florian et al., 2003; Ratinov and Roth, 2009; Luo et al., 2015) mainly rely on hand-crafted features and gazetteers which require knowledge about the domain. Recently, there has been a surge in terms of interest in applying deep learning techniques to NLP tasks. These methods, together with substantial amount of annotated data, can learn features automatically and have been reported to outperform traditional methods on certain NLP tasks (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016). However, in spite of the perceived success of deep learning methods particularly in NER in newswire (Ma and Hovy,"
W16-3921,fromreide-etal-2014-crowdsourcing,0,0.020705,"state-of-the-art NER systems for newswire were built upon deep learning-based approaches (Ma and Hovy, 2016; Chiu and Nichols, 2016). It has been shown, however, that even such state-of-the-art methods tend to underperform when applied to other domains, particularly on social media content, e.g., tweets (Baldwin et al., 2015). Challenges in this domain include noise inherent in the data, the many new terms—neologisms—introduced in social media over time, and changes in linguistic conventions in general, also known as language drift (Dredze et al., 2010; Ritter et al., 2011; Eisenstein, 2013; Fromreide et al., 2014). In this paper, we describe our methods addressing the above-mentioned challenges in the context of the 2016 Named Entity Recognition in Twitter Task (Strauss et al., 2016) organised as part of the Second Workshop on Noisy User-generated Text (W-NUT). For our contribution to the said shared task, we combined deep learning-based approaches with distant supervision methods for generating weakly labelled data, and further optimised performance by exploring the use of different tagging schemes and word embeddings. 3 Methodology We cast the NER task as a sequence labelling problem: every tweet is"
W16-3921,W15-4322,0,0.038962,"Missing"
W16-3921,N16-1030,0,0.201943,"tural language processing (NLP) tasks that is central to understanding unstructured, textual data. Popular approaches (Chieu and Ng, 2002; Florian et al., 2003; Ratinov and Roth, 2009; Luo et al., 2015) mainly rely on hand-crafted features and gazetteers which require knowledge about the domain. Recently, there has been a surge in terms of interest in applying deep learning techniques to NLP tasks. These methods, together with substantial amount of annotated data, can learn features automatically and have been reported to outperform traditional methods on certain NLP tasks (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016). However, in spite of the perceived success of deep learning methods particularly in NER in newswire (Ma and Hovy, 2016), performance on social media content, particularly that from Twitter, has been lagging behind (Baldwin et al., 2015). There are two state-of-the-art deep learning methods for newswire NER, namely, those proposed by Chiu and Nichols (2016) and Ma and Hovy (2016). Considering that the former requires hand-crafted features (e.g., word matches against gazetteers) and yet obtains only a small boost in performance over the latter which does not rely on an"
W16-3921,D15-1104,0,0.113938,"rder to reduce overfitting. Even without handcrafting linguistic features nor leveraging any of the W-NUT-provided gazetteers, we obtained robust performance with our approach, which ranked third amongst all shared task participants according to the official evaluation on a gold standard named entity-annotated corpus of 3,856 tweets. 1 Introduction Named entity recognition (NER) is one of the most fundamental natural language processing (NLP) tasks that is central to understanding unstructured, textual data. Popular approaches (Chieu and Ng, 2002; Florian et al., 2003; Ratinov and Roth, 2009; Luo et al., 2015) mainly rely on hand-crafted features and gazetteers which require knowledge about the domain. Recently, there has been a surge in terms of interest in applying deep learning techniques to NLP tasks. These methods, together with substantial amount of annotated data, can learn features automatically and have been reported to outperform traditional methods on certain NLP tasks (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016). However, in spite of the perceived success of deep learning methods particularly in NER in newswire (Ma and Hovy, 2016), performance on social media content"
W16-3921,P16-1101,0,0.230031,"most fundamental natural language processing (NLP) tasks that is central to understanding unstructured, textual data. Popular approaches (Chieu and Ng, 2002; Florian et al., 2003; Ratinov and Roth, 2009; Luo et al., 2015) mainly rely on hand-crafted features and gazetteers which require knowledge about the domain. Recently, there has been a surge in terms of interest in applying deep learning techniques to NLP tasks. These methods, together with substantial amount of annotated data, can learn features automatically and have been reported to outperform traditional methods on certain NLP tasks (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016). However, in spite of the perceived success of deep learning methods particularly in NER in newswire (Ma and Hovy, 2016), performance on social media content, particularly that from Twitter, has been lagging behind (Baldwin et al., 2015). There are two state-of-the-art deep learning methods for newswire NER, namely, those proposed by Chiu and Nichols (2016) and Ma and Hovy (2016). Considering that the former requires hand-crafted features (e.g., word matches against gazetteers) and yet obtains only a small boost in performance over the latter whic"
W16-3921,W03-0430,0,0.152734,"sophisticated tagging schemes leads to better predictions (Ratinov and Roth, 2009; Dai et al., 2015). According to the popular begininside-outside (BIO) scheme, each token is tagged as any of ‘B’, ‘I’ or ‘O’ depending on whether it is at the beginning, inside or outside a named entity, respectively. The more fine-grained BIOES scheme, however, additionally makes use of ‘E’ and ‘S’ to also distinguish tokens at the end and those comprising single-token entities. Approaches to sequence labelling based on the conditional random fields (CRF) algorithm are known to demonstrate strong performance (McCallum and Li, 2003; Leaman et al., 2008; Finkel and Manning, 2009). CRFs find the most probable label sequence given a sequence of tokens encoded using features, e.g., surface forms, lemma, part-of-speech tags, surrounding tokens, morphology (Sang and Veenstra, 1999). Meanwhile, convolutional neural networks (CNNs) are sparse feed-forward neural networks which have been shown to effectively extract morphological features such as word prefixes and suffixes (Chiu and Nichols, 2016). One can thus explore the combination of the ability of CRFs to model relations between labels of tokens in a tweet, and the effectiv"
W16-3921,D14-1162,0,0.0795084,"Missing"
W16-3921,W09-1119,0,0.738117,"arisation technique in order to reduce overfitting. Even without handcrafting linguistic features nor leveraging any of the W-NUT-provided gazetteers, we obtained robust performance with our approach, which ranked third amongst all shared task participants according to the official evaluation on a gold standard named entity-annotated corpus of 3,856 tweets. 1 Introduction Named entity recognition (NER) is one of the most fundamental natural language processing (NLP) tasks that is central to understanding unstructured, textual data. Popular approaches (Chieu and Ng, 2002; Florian et al., 2003; Ratinov and Roth, 2009; Luo et al., 2015) mainly rely on hand-crafted features and gazetteers which require knowledge about the domain. Recently, there has been a surge in terms of interest in applying deep learning techniques to NLP tasks. These methods, together with substantial amount of annotated data, can learn features automatically and have been reported to outperform traditional methods on certain NLP tasks (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016). However, in spite of the perceived success of deep learning methods particularly in NER in newswire (Ma and Hovy, 2016), performance on s"
W16-3921,D11-1141,0,0.0465259,"16; Lample et al., 2016). For instance, state-of-the-art NER systems for newswire were built upon deep learning-based approaches (Ma and Hovy, 2016; Chiu and Nichols, 2016). It has been shown, however, that even such state-of-the-art methods tend to underperform when applied to other domains, particularly on social media content, e.g., tweets (Baldwin et al., 2015). Challenges in this domain include noise inherent in the data, the many new terms—neologisms—introduced in social media over time, and changes in linguistic conventions in general, also known as language drift (Dredze et al., 2010; Ritter et al., 2011; Eisenstein, 2013; Fromreide et al., 2014). In this paper, we describe our methods addressing the above-mentioned challenges in the context of the 2016 Named Entity Recognition in Twitter Task (Strauss et al., 2016) organised as part of the Second Workshop on Noisy User-generated Text (W-NUT). For our contribution to the said shared task, we combined deep learning-based approaches with distant supervision methods for generating weakly labelled data, and further optimised performance by exploring the use of different tagging schemes and word embeddings. 3 Methodology We cast the NER task as a"
W16-3921,E99-1023,0,0.19819,"inning, inside or outside a named entity, respectively. The more fine-grained BIOES scheme, however, additionally makes use of ‘E’ and ‘S’ to also distinguish tokens at the end and those comprising single-token entities. Approaches to sequence labelling based on the conditional random fields (CRF) algorithm are known to demonstrate strong performance (McCallum and Li, 2003; Leaman et al., 2008; Finkel and Manning, 2009). CRFs find the most probable label sequence given a sequence of tokens encoded using features, e.g., surface forms, lemma, part-of-speech tags, surrounding tokens, morphology (Sang and Veenstra, 1999). Meanwhile, convolutional neural networks (CNNs) are sparse feed-forward neural networks which have been shown to effectively extract morphological features such as word prefixes and suffixes (Chiu and Nichols, 2016). One can thus explore the combination of the ability of CRFs to model relations between labels of tokens in a tweet, and the effectiveness of CNNs to extract morphological features of words. CNNs, however, suffer from one drawback, i.e., failure to capture the contextual information surrounding a word. To alleviate this issue, we instead employed recurrent neural networks (RNNs)"
W16-3921,W16-3919,0,0.0395148,"Missing"
W16-3921,W15-4321,0,0.0161783,"hop on Noisy User-generated Text, pages 153–163, Osaka, Japan, December 11 2016. of many researchers. Several efforts (Chieu and Ng, 2002; Florian et al., 2003; Ratinov and Roth, 2009), tackle this problem by hand-crafting linguistic features and presenting them to a machine learning algorithm, which will then discriminate between various semantic types based on those features. This particular approach, however, is usually done ad hoc making it very tedious to adapt features to other domains. Recently, extensions to this approach have been introduced (Godin et al., 2015; Cherry and Guo, 2015; Toh et al., 2015) by considering word embeddings as sources of features for classification. Other studies (Luo et al., 2015; Yamada123 et al., 2015) combined this machine learning-based approach with entity linking methods which exploit knowledge bases (e.g., Wikipedia) to detect named mentions. With the NLP research community’s continuously surging interest in deep learning, approaches based on deep neural networks have also been applied to NER. While they take away the burden that comes with handcrafting linguistic features, they require huge amounts of data. Nevertheless, they have been proven to be effecti"
W16-3921,W15-4320,0,0.0390481,"Missing"
W17-2314,W04-3202,0,0.0524327,"m is specialised for each class. Kapoor et al. (2007) proposed a decision-theoretic method for the task of voice mail classification. They defined a criterion named “expected value-of-information” that combines the misclassification risk with the labelling cost. Cost-sensitive active learning was also applied to part-of-speech (POS) tagging (Haertel et al., 2008). In this work, an hourly cost measurement was determined and a linear regression model was trained to predict the annotation cost. Hwa (2000) aimed to reduce the manual effort for a parsing task by using tree entropy cost. Meanwhile, Baldridge and Osborne (2004) measured the total annotation cost to create a treebank by using unit cost and discriminant cost. 5 A potential limitation of our approach is that the initial step is reliant on the availability of a gold standard corpus to estimate the experts’ performance. However, for some domains, it may be difficult to obtain such a dataset. Therefore, as future work, we will explore how we can assess experts’ performance without the need for goldstandard labelled data. As a further extension to our work, we will explore the deployment of our method on crowd sourcing platforms, such as CrowdFlower5 and A"
W17-2314,P08-2017,0,0.0341603,". They also proposed a model that assigns different costs to unlabelled instances according to their annotation difficulty. For the multi-class classification task, Moon and Carbonell (2014) used the same approach but they had multiple experts, each of whom is specialised for each class. Kapoor et al. (2007) proposed a decision-theoretic method for the task of voice mail classification. They defined a criterion named “expected value-of-information” that combines the misclassification risk with the labelling cost. Cost-sensitive active learning was also applied to part-of-speech (POS) tagging (Haertel et al., 2008). In this work, an hourly cost measurement was determined and a linear regression model was trained to predict the annotation cost. Hwa (2000) aimed to reduce the manual effort for a parsing task by using tree entropy cost. Meanwhile, Baldridge and Osborne (2004) measured the total annotation cost to create a treebank by using unit cost and discriminant cost. 5 A potential limitation of our approach is that the initial step is reliant on the availability of a gold standard corpus to estimate the experts’ performance. However, for some domains, it may be difficult to obtain such a dataset. Ther"
W17-2314,W00-1306,0,0.15041,"ation task, Moon and Carbonell (2014) used the same approach but they had multiple experts, each of whom is specialised for each class. Kapoor et al. (2007) proposed a decision-theoretic method for the task of voice mail classification. They defined a criterion named “expected value-of-information” that combines the misclassification risk with the labelling cost. Cost-sensitive active learning was also applied to part-of-speech (POS) tagging (Haertel et al., 2008). In this work, an hourly cost measurement was determined and a linear regression model was trained to predict the annotation cost. Hwa (2000) aimed to reduce the manual effort for a parsing task by using tree entropy cost. Meanwhile, Baldridge and Osborne (2004) measured the total annotation cost to create a treebank by using unit cost and discriminant cost. 5 A potential limitation of our approach is that the initial step is reliant on the availability of a gold standard corpus to estimate the experts’ performance. However, for some domains, it may be difficult to obtain such a dataset. Therefore, as future work, we will explore how we can assess experts’ performance without the need for goldstandard labelled data. As a further ex"
W17-2314,D08-1112,0,0.355376,"Pro-active learning results on the three corpora when using different BatchSize Figure 2: The best pro-active learning results on the three corpora in comparison to the baselines is the most useful instance for learning an NER model. There are several ways to implement this, such as least confidence (Culotta and McCallum, 2005)–the lower the probability of a sequence of labels, the less confidence the model, and entropy (Kim et al., 2006) that can measure the uncertainty of a probability distribution. Some other criteria are a diversity measurement (Kim et al., 2006) and a density criterion (Settles and Craven, 2008). Reliable corresponds to the number of times that the reliable expert was selected in Reliable baseline experiment. The figure illustrates that the number of times that the fallible expert is selected grows continually as the number of iterations increases. This shows that our method can effectively distribute appropriate unlabelled sentences to the fallible expert, in order to save on annotation costs. 4 4.1 Related work 4.2 Active learning for NER Cost-sensitive active learning Cost-sensitive active learning is a type of active learning method that considers the annotation cost, e.g., budge"
W17-2314,D08-1027,0,0.415833,"Missing"
W17-2314,N06-2018,0,0.169245,"Various active learning criteria were investigated using the three corpora. We firstly estimated the performance (F1 score) of a supervised NER model by using CRF++ and the above-mentioned features. We then compared the performance of each active learning criterion with that of the supervised model. If the performance of one criterion approximates that of the supervised with the least number of iterations, we consider the criterion as the best one for proactive learning experiments. We experimented with the following criteria: least confidence (Culotta and McCallum, 2005), normalized entropy (Kim et al., 2006), MMR (Maximal Marginal Relevance) (Kim et al., 2006), density (Settles and Craven, 2008) when using feature vectors and word embeddings, and the combination of least confidence and density criterion. Equation 8 describes the combination criterion used in our experiments. In this equation, U L is the current unlabelled dataset, xu is the uth unlabelled sentence in U L, the parameter λ = 0.8, and the similarity score (Settles and Craven, 2008) were calculated by using feature vectors. LSTM-CRF Pre. Re. F1 75.69 74.11 74.89 77.18 74.77 75.96 75.41 73.91 74.66 Table 2: Performance of CRF and LSTM"
W17-2314,N16-1030,0,0.0256481,"a higher expertise level in the target domain, and has a very low error rate. In order to determine an appropriate annotator for each sentence, we calculate the probability that an annotator will assign the correct sequence of labels in a selected unlabelled sentence. Furthermore, at each iteration, we use a batch sampling mechanism to select several sentences for annotators to label (instead of selecting only a single sentence), which optimises both cost and performance. For evaluation purposes, we simulate the two annotators by using two machine-learning based NER methods, namely LSTM-CRF (Lample et al., 2016) as the reliable expert, and CRF (Lafferty et al., 2001) as the fallible expert. We then apply our method to three corpora from different domains: ACE2005 (Walker et al., 2006) for general language entities, COPIOUS—an in-house corpus of biodiversity entities1 , and GENIA (Kim et al., 2003)—a corpus of biomedical entities. Our ex1 perimental results demonstrate that by using the proposed method, we can obtain a high-quality labelled corpus at a lower cost than current baseline methods. The contributions of our work are as follows. Firstly, we have modified the conventional proactive learning m"
W18-1302,W09-1401,0,0.0564574,"nition of uncertainty that covers phenomena of citation distortion, contradictions and claim inconsistencies, and also presented a method based on word embeddings for expanding a small seed list of cues to generate rich resources for uncertainty identification. The aforementioned concepts have also been annotated in corpora at different levels of granularity. The BioScope corpus (Vincze et al., 2008), as well the biomedical part of the CoNLL 2010 task (Farkas et al., 2010) contain annotations of speculation and negation cues and their scope within the sentence. The BioNLP Shared Task corpora (Kim et al., 2009, 2011; N´edellec et al., 2013) also contain speculation and negation annotations, marked-up as attributes of events. The GENIAMK corpus (Thompson et al., 2011) also contains event-level attribute annotations, but covering more meta-knowledge aspects, including certainty level, polarity and knowledge type (see SecRelated Work In this section, we provide an overview of related work on uncertainty in both the scientific and 7 of news articles, as part of the fake-news challenge (FNC-I) that focusses on detection of stance. In comparison to the scientific domain, there have been relatively fewer"
W18-1302,W12-4306,0,0.0436204,"Missing"
W18-1302,W11-1801,0,0.0832752,"Missing"
W18-1302,W04-3103,0,0.0971734,"Missing"
W18-2324,W16-2922,0,0.0171056,"formance of both mention detection and mention linking on the BioNLP dataset, but the integration could not enhance the performance on the CRAFT corpus. 2 We incorporate the following domain-specific features to enhance the baseline system. In-domain word embeddings: The input word embeddings play an important role in deep learning. Instead of using embeddings trained on general domains, e.g., word embeddings provided with the word2vec tool (Mikolov et al., 2013), we use 200-dimensional embeddings trained on the whole PubMed and PubMed Central Open Access subset (PMC) with a window size of 2 (Chiu et al., 2016). Grammatical numbers: We check mentions’ grammatical numbers, i.e., whether each mention is singular or plural. A mention is singular if its part-of-speech tag is N N or if it is one of the five singular pronouns: it, its, itself, this, and that. A mention is plural if its part-of-speech tag is N N S or if it is one of the seven plural pronouns: they, their, theirs, them, themselves, these, and those. MetaMap entity tags: We employ MetaMapLite2 to identify all possible entities according to the UMLS semantic types.3 In cases that MetaMapLite assigns multiple semantic types for each entity, we"
W18-2324,P10-1040,0,0.0752527,"dataset, we also employed the scorer provided by the shared task organisers to make fair comparisons with previous work. We reported the performance on two sub-tasks: (1) mention detection, i.e., to identify coreferent mentions, such as named entities, prepositions or noun phrases, and (2) mention linking, i.e., to link these mentions if they refer to the same thing. The result of the first task affects that of the second one. Settings We first directly applied the Lee2017 system to the corpora. Lee2017 used two pretrained embeddings in general domains provided by Pennington et al. (2014) and Turian et al. (2010), and all default features such as speaker, genre, and distance. To train the Lee2017 system, we employed the same hyper-parameters as reported in Lee et al. (2017) except for a threshold ratio. Although Lee2017 used the ratio λ = 0.4 to reduce the number of mentions from the list of candidates, we tuned it on the BioNLP development set and used λ = 0.7. We then investigate the impact of each feature on the biomedical texts by preparing the following four systems: • Lee2017: general embeddings, speaker, genre, and distance features 3.3 Results Results on the development sets of the two corpora"
W18-2324,N16-1114,0,0.068981,"Missing"
W18-2324,D17-1018,0,0.330097,"al., 2011).1 Given the fact that deep learning methods can produce the state-of-the-art performance on general texts, we are motivated to apply such methods to biomedical texts. We therefore raise three research questions in this paper: • How does a general domain neural system with no parser information perform on biomedical domain? • How we can incorporate domain-specific information into the neural system? • Which performance range the system is in comparison with existing systems? In order to address these questions, we directly apply the end-to-end neural coreference resolution system by Lee et al. (2017) (Lee2017) to biomedical texts. We then investigate domain specific features such as domain-specific word embeddings, grammatical number agreements between mentions, i.e., mentions are singular or plural, and agreements of MetaMap (Aronson and Lang, 2010) entity tags of mentions. These features do not rely on any syntactic parsers. Moreover, these features are also general for any biomedical corpora and not restricted to the corpora we use. Existing biomedical coreference resolution systems depend on features and/or rules based on syntactic parsers. In this paper, we investigate the utility of"
W19-5036,N19-1213,0,0.0310946,"Missing"
W19-5036,D17-1169,0,0.421029,"studies, it is possible to leverage sentiment analysis features automatically, without relying on any hand-crafted features. One common approach is to pre-train a classifier on a corpus annotated with sentiment information and then to adapt this pretrained classifier to the detection of ADRs. The advantage of this approach is that the target system only needs access to the pre-trained model, but not the original sentiment corpus, which can be important for storage and data regulation issues. This method has been investigated by various researchers (Devlin et al., 2018; Howard and Ruder, 2018; Felbo et al., 2017). Felbo et al. (2017) learned a rich representation for detecting sentiment, sarcasm, and emotion using millions of emojis’ dataset, acquired from Twitter. They demonstrated that this approach performs well and can achieve results that are competitive with state of the art systems. Recently, Devlin et al. (2018) built a deep bidirectional representation from transformers, which can be fine-tuned to different target tasks with an additional output layer. The model, which is called “Bert”, showed significant improvements for a wide array of tasks, such as text classification, textual entailment"
W19-5036,W18-6250,1,0.492029,"Missing"
W19-5036,P18-1031,0,0.061863,"s proposed in the above studies, it is possible to leverage sentiment analysis features automatically, without relying on any hand-crafted features. One common approach is to pre-train a classifier on a corpus annotated with sentiment information and then to adapt this pretrained classifier to the detection of ADRs. The advantage of this approach is that the target system only needs access to the pre-trained model, but not the original sentiment corpus, which can be important for storage and data regulation issues. This method has been investigated by various researchers (Devlin et al., 2018; Howard and Ruder, 2018; Felbo et al., 2017). Felbo et al. (2017) learned a rich representation for detecting sentiment, sarcasm, and emotion using millions of emojis’ dataset, acquired from Twitter. They demonstrated that this approach performs well and can achieve results that are competitive with state of the art systems. Recently, Devlin et al. (2018) built a deep bidirectional representation from transformers, which can be fine-tuned to different target tasks with an additional output layer. The model, which is called “Bert”, showed significant improvements for a wide array of tasks, such as text classification"
W19-5036,S17-2126,0,0.0632838,"Missing"
W19-5036,C16-1084,0,0.0737769,"Missing"
W19-5036,W18-5909,0,0.435921,"cenario. The rest of the paper is organised as follows: Section 2 provides a review of related work. Section 3 presents the two datasets used to create our model. Section 4 describes our method and model. Section 5 reports on the analysis of results while Section 6 provides some conclusions. 2 Related Work There is a growing body of literature concerned with the detection and classification of ADRs in social media texts (Wang et al., 2018; Huynh et al., 2016; Ebrahimi et al., 2016; Liu and Chen, 2015). Recent work has employed sentiment analysis features to improve the classification of ADRs (Wu et al., 2018; Kiritchenko et al., 2017; Alimova and Tutubalina, 2017; Korkontzelos et al., 2016; Sarker and Gonzalez, 2015). Nikfarjam et al. (2015) exploited a set of features, including context features, ADR lexicon, part of speech (POS) and negation, to enhance the performance of ADR extraction. The authors chose Conditional Random Field as their classifier (CRF). Korkontzelos et al. (2016) followed the same research hypothesis, but focused on the evaluation of sentiment analysis features as an aid to extracting ADRs, based on the correlation between negative sentiments and ADRs. Alimova and Tutubalina"
W19-5036,W19-4302,0,0.0196219,"rom over-fitting to the training set (Hinton 2 DailyStrength is a specialised social networking website for health. 3 https://github.com/alexandra-chron/ ntua-slp-semeval2018 3.2 Preprocessing 341 , where Wh , bh are the attention’s weights. Classification layer: The vector r is an encoded representation of the whole input text (i.e. a tweet or post), which is eventually passed to a fully-connected layer for classification. A binary classification decision is made according to whether or not the input text mentions ADRs. Transfer Learning: There are two common approaches to transfer learning (Peters et al., 2019). One approach is to use the last layer of a pretrained model when fine-tuning to the target task. In this scenario, the network is used as a feature extractor. An alternative approach is to use the network for initialization, i.e., the full network is unfrozen and then fine-tuned to the target task. In this work, After training the sentiment classification model, we exclude its output layer and replace it by an ADR output layer. Finally, the network is fine-tuned to detect the ADRs adopting the same architecture and hyper-parameters as the original model. We analyse the fine-tuning methods in"
W19-5036,S17-2088,0,0.0184897,"(Hochreiter and Schmidhuber, 1997), a self-attention mechanism (Bahdanau et al., 2014) and a classification layer. Figure 1 depicts the network architecture of our model. Table 1: Data statistics (DailyS. = DailyStrength) 3.1 Sentiment Analysis corpus We firstly train a sentiment analysis model on Twitter data from the SemEval17-task4A, which focuses on classifying the sentiment polarity of tweets on the subject of current affairs into predefined categories, e.g. positive, negative, and neutral. The dataset is partitioned into a training set of 50, 000 tweets and a test set of 12, 000 tweets (Rosenthal et al., 2017). A description of the sentiment analysis model is provided in section 4. Figure 1: A description of the framework for our system. Since Twitter data possesses specific characteristics, including informal language, misspellings, and abbreviations, we pre-process the data before In our different experiments, we use both an LSTM and a bi-directional LSTM (BiLSTM). Both are able to capture sequential dependencies especially in time series data, of which language can be seen as an example. The model’s weights are initialized from the word2vec embedding with 300 dimensional size3 . Additionally, th"
wang-etal-2012-biomedical,W06-0127,0,\N,Missing
wang-etal-2012-biomedical,W00-1313,0,\N,Missing
