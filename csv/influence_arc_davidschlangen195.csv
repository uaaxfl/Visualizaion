2007.sigdial-1.9,font-llitjos-black-2002-evaluation,0,0.0274263,"ctive setting and in one with restricted interactivity (push-to-talk). The completely non-interactive material collected here gives us a good further comparison. We were especially interested in the use subjects made of the player tool to recreate some semblance of ‘interactivity’ through stopping, skipping and repeating audio material. The analysis of this is still going on. 4 Related Work As mentioned in the introduction, conducting experiments over the Internet is common practice in Psychology these days (Birnbaum, 2001; Reips, 2002),7 However, these experiments rarely involve audio. (Font Llitjos and Black, 2002; Black and Tokuda, 2005) present experiments on collecting evaluations of speech over the Internet; SpeechRecorder (Draxler, 2006) offers recording over the Internet much like our system, but with no provisions for recording other behavioural measures like reaction times. The combination of experiment / collection with instant user-based quality assessment that our approach offers is, to our knowledge, novel. 5 Conclusions and Future Work We have presented an implemented methodology for distributed collection of speech data. The implemented tool is flexible in the kind of stimuli that can be"
2020.emnlp-main.26,P15-1020,0,0.0718879,"fixes to what remains a non-incremental algorithm, providing some “housekeeping” to manage the potentially non-monotonic results. To alleviate the effect of the partiality of the input, we test the use of anticipated continuations, inspired by the mechanism of predictive processing discussed in cognitive science (Christiansen and Chater, 2016) and the idea of interactive utterance completion introduced by DeVault et al. (2011). Related strategies to predict upcoming content and to wait for more right context are also applied in recent work on simultaneous translation (Grissom II et al., 2014; Oda et al., 2015; Ma et al., 2019). The use of truncated inputs during training, discussed below, aims at making intermediate structures available during learning, an issue discussed in K¨ohn (2018). This is a variation of chunked training used in Dalvi et al. (2018). 3 Evaluation of incremental processors The hierarchical nature of language makes it likely that incremental processing leads to non-monotonic output due to re-analysis, as in the well-known “garden path” sentences. Incremental systems may edit the output by adding, revoking, and substituting its parts (Baumann et al., 2011). We expect an increme"
2020.emnlp-main.26,E12-1052,1,0.787069,"language model. In a combination of bidirectional word representations with the Transformer architecture, we observe the establishment of BERT (Devlin et al., 2019) as a current state-ofthe-art model, on top of which an output layer can be added to solve classification and tagging tasks. 2.2 Incremental processing The motivation to build incremental processors, as defined by Kempen and Hoenkamp (1982) and Levelt (1989), is twofold: they are more cognitively plausible and, from the viewpoint of engineering, real-time applications such as parsing (Nivre, 2004), SRL (Konstas et al., 2014), NLU (Peldszus et al., 2012), dialog state tracking (Trinh et al., 2018), NLG and speech synthesis (Buschmeier et al., 2012) and ASR (Selfridge et al., 2011) require that the input be continually evaluated based on incoming prefixes while the output is being produced and updated. Another advantage is a better use of computational resources, as a module does not have to wait for the completion of another one to start processing (Skantze and Schlangen, 2009). In robots, linguistic processing must also be intertwined with its perceptions and actions, happening simultaneously (Brick and Scheutz, 2007). Research on processing"
2020.emnlp-main.26,D14-1162,0,0.0847844,"Missing"
2020.emnlp-main.26,N18-1202,0,0.0300335,"ent works have confirmed that bidirectionality can afford an increase in performance (Graves and Schmidhuber, 2005; Huang et al., 2015; Zhai et al., 2017). More recently, Vaswani et al. (2017) has consolidated the application of attention mechanisms on NLP tasks with Transformers, which are not constrained by only two directions, as BiLSTMs. Instead, complete sentences are accessed at once. The need for NLP neural networks to be grounded on robust language models and reliable word representations has become clear. The full right and 358 left context of words started to play a major role as in Peters et al. (2018), which resorts to bidirectionality to train a language model. In a combination of bidirectional word representations with the Transformer architecture, we observe the establishment of BERT (Devlin et al., 2019) as a current state-ofthe-art model, on top of which an output layer can be added to solve classification and tagging tasks. 2.2 Incremental processing The motivation to build incremental processors, as defined by Kempen and Hoenkamp (1982) and Levelt (1989), is twofold: they are more cognitively plausible and, from the viewpoint of engineering, real-time applications such as parsing (N"
2020.emnlp-main.26,P16-2067,0,0.0237021,"processing. Since then, the field has witnessed the emergence of a miscellany of neural architectures that take the temporal structure of language into account. In particular, LSTMs (Hochreiter and Schmidhuber, 1997) have been vastly used for sequence-tosequence or sequence classification tasks, which are ubiquitous in NLP. Bidirectional LSTMs (Schuster and Paliwal, 1997; Baldi et al., 1999) are an extension to LSTMs that exploit bidirectionality and whose basic processing units are full sentences. They achieved remarkable results in many tasks, e.g. part-ofspeech tagging (Ling et al., 2015; Plank et al., 2016), chunking (Zhai et al., 2017), named entity recognition (Chiu and Nichols, 2016), semantic role labeling (He et al., 2017), slot filling and intent detection (E et al., 2019) and opinion mining (˙Irsoy and Cardie, 2014). Subsequent works have confirmed that bidirectionality can afford an increase in performance (Graves and Schmidhuber, 2005; Huang et al., 2015; Zhai et al., 2017). More recently, Vaswani et al. (2017) has consolidated the application of attention mechanisms on NLP tasks with Transformers, which are not constrained by only two directions, as BiLSTMs. Instead, complete sentences"
2020.inlg-1.38,W05-0909,0,0.216605,"s are guaranteed to express correct information, at the cost of naturalness, and so can serve as an upper bound on semantic metrics. 4.3 Evaluation Metrics In the test set, we have available both the symbolic representation of the states (and hence objectively know what the required change is) and the set of reference instructions E. We define metrics making use of either. We use common metrics from caption generation: BLEU-4, measuring token overlap up to 4grams (Papineni et al., 2002); CIDEr, measuring overlap based on the consensus of reference instructions (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), measuring unigram overlap with advanced normalization like stemming and synonym comparison, and ROUGE-L (Lin, 2004) which measures similarity based on longest common subsequences. We apply each individual metric by comparing the generated instruction against all available reference instructions for the respective image pair using the pycocoevalcap library.4 To better analyse task performance, where it matters that the blocks are correctly referred to, we also parse the generated instructions using our instruction parser, to extract what was mentioned as target block and as landmark. We can t"
2020.inlg-1.38,P18-1182,0,0.0350021,"Missing"
2020.inlg-1.38,gargett-etal-2010-give,0,0.0225821,"asanmi et al., 2019; Park et al., 2019), the task is to verbalise what is different between two otherwise very similar images. Our task contains this, but goes beyond it in that it also has to be verbalised how that difference can be effected. In that work, specialised architectures are presented that can more easily extract differences. Here, we wanted to start by exploring more standard captioning approaches as a baseline in order to fully understand the requirements of our task, leaving further architectural adaptations to future work. After early work in the context of the GIVE challenge (Gargett et al. (2010), Byron et al. (2007)), there is some renewed interest in instruction giving. K¨ohn et al. (2020) presented an instruction giving platform called MC-Saar-Instruct where players can interact with a bot in the Minecraft world which instructs them to build something. This is very related to our interest in this project; for now, however, that work still assumes a symbolic representation as input. In the field of natural language generation, the production of referring expressions is a wellestablished task (Krahmer and van Deemter, 2012), increasingly also tackled with neural methods (Zarrieß and"
2020.inlg-1.38,W19-1608,0,0.130953,"hmer and van Deemter, 2012), increasingly also tackled with neural methods (Zarrieß and Schlangen, 2018; Castro Ferreira et al., 2018). IG-BA includes this; but as will become clear in the next section, the data that we use here allows us to factor it out, as references to objects can simply be done via unique names. The complexity in our task comes from the spatial language required to denote locations, which is something not found to that degree neither in image captioning nor referring expression generation. Generation of spatial expressions has seen some attention in recent years, e.g. by Ghanimifard and Dobnik (2019a), who investigate the spatial language that neural language models can learn and express. 3 Data: BLOCKS and BLOCKSgen We will now describe how we can make use of data collected for instruction following for our task of instruction giving. 3.1 BLOCKS : Instruction Following Bisk et al. (2016) collected the BLOCKS dataset in order to study instruction following in a simple visual environment. The environment consists of up to 20 blocks of the same size, which are placed on a board. The blocks are uniquely labelled either with a number between 1 and 20, or with the logo of a major company; thi"
2020.inlg-1.38,W19-8668,0,0.130424,"hmer and van Deemter, 2012), increasingly also tackled with neural methods (Zarrieß and Schlangen, 2018; Castro Ferreira et al., 2018). IG-BA includes this; but as will become clear in the next section, the data that we use here allows us to factor it out, as references to objects can simply be done via unique names. The complexity in our task comes from the spatial language required to denote locations, which is something not found to that degree neither in image captioning nor referring expression generation. Generation of spatial expressions has seen some attention in recent years, e.g. by Ghanimifard and Dobnik (2019a), who investigate the spatial language that neural language models can learn and express. 3 Data: BLOCKS and BLOCKSgen We will now describe how we can make use of data collected for instruction following for our task of instruction giving. 3.1 BLOCKS : Instruction Following Bisk et al. (2016) collected the BLOCKS dataset in order to study instruction following in a simple visual environment. The environment consists of up to 20 blocks of the same size, which are placed on a board. The blocks are uniquely labelled either with a number between 1 and 20, or with the logo of a major company; thi"
2020.inlg-1.38,2020.sigdial-1.7,0,0.0933723,"Missing"
2020.inlg-1.38,J12-1006,0,0.0241617,"Missing"
2020.inlg-1.38,W04-1013,0,0.023979,"4.3 Evaluation Metrics In the test set, we have available both the symbolic representation of the states (and hence objectively know what the required change is) and the set of reference instructions E. We define metrics making use of either. We use common metrics from caption generation: BLEU-4, measuring token overlap up to 4grams (Papineni et al., 2002); CIDEr, measuring overlap based on the consensus of reference instructions (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), measuring unigram overlap with advanced normalization like stemming and synonym comparison, and ROUGE-L (Lin, 2004) which measures similarity based on longest common subsequences. We apply each individual metric by comparing the generated instruction against all available reference instructions for the respective image pair using the pycocoevalcap library.4 To better analyse task performance, where it matters that the blocks are correctly referred to, we also parse the generated instructions using our instruction parser, to extract what was mentioned as target block and as landmark. We can then compare these 4 https://github.com/salaniz/pycocoevalcap We train all of our models on the BLOCKS dataset using t"
2020.inlg-1.38,D18-1287,0,0.0542746,"Missing"
2020.inlg-1.38,P02-1040,0,0.106509,"states, we can also use the template generator described above to generate instructions for the test set. The generated instructions are guaranteed to express correct information, at the cost of naturalness, and so can serve as an upper bound on semantic metrics. 4.3 Evaluation Metrics In the test set, we have available both the symbolic representation of the states (and hence objectively know what the required change is) and the set of reference instructions E. We define metrics making use of either. We use common metrics from caption generation: BLEU-4, measuring token overlap up to 4grams (Papineni et al., 2002); CIDEr, measuring overlap based on the consensus of reference instructions (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), measuring unigram overlap with advanced normalization like stemming and synonym comparison, and ROUGE-L (Lin, 2004) which measures similarity based on longest common subsequences. We apply each individual metric by comparing the generated instruction against all available reference instructions for the respective image pair using the pycocoevalcap library.4 To better analyse task performance, where it matters that the blocks are correctly referred to, we also"
2020.inlg-1.38,W18-6563,1,0.853284,"t al. (2010), Byron et al. (2007)), there is some renewed interest in instruction giving. K¨ohn et al. (2020) presented an instruction giving platform called MC-Saar-Instruct where players can interact with a bot in the Minecraft world which instructs them to build something. This is very related to our interest in this project; for now, however, that work still assumes a symbolic representation as input. In the field of natural language generation, the production of referring expressions is a wellestablished task (Krahmer and van Deemter, 2012), increasingly also tackled with neural methods (Zarrieß and Schlangen, 2018; Castro Ferreira et al., 2018). IG-BA includes this; but as will become clear in the next section, the data that we use here allows us to factor it out, as references to objects can simply be done via unique names. The complexity in our task comes from the spatial language required to denote locations, which is something not found to that degree neither in image captioning nor referring expression generation. Generation of spatial expressions has seen some attention in recent years, e.g. by Ghanimifard and Dobnik (2019a), who investigate the spatial language that neural language models can le"
2020.lrec-1.71,D16-1127,0,0.0327066,"dialogue modelling used to be mostly rule-based with the dialogue being driven by pre-specified knowledge representations (e.g., (Bobrow et al., 1977), (Traum and Larsson, 2003), (Stede and Schlangen, 2004)), recent years have seen efforts of basing this task on models directly learned from data. A particular strand of this research has modelled the task of producing a dialogue contribution in analogy to the translation task as one of going from one sequence (the user utterance) to another sequence (the system utterance). The first such models solely based on data driven end-toend approaches (Li et al., 2016; Serban et al., 2017) tended to generate universal and inconsistent utterances regarding content and personality. We illustrate this problem with the example in Figure 1, distinguishing the two consistency dimensions knowledge (a speaker should not “forget” previously known facts) and opinion (a speaker should not change their opinion, at least not without any overt trigger in the conversation). In this example, each system response is locally coherent (a good reply to its immediate precursor), but globally inconsistent. While this particular example is constructed, it is not very far from wh"
2020.lrec-1.71,D16-1230,0,0.0342682,"found, we combined the generated score with the logits from the classification layer of our model to choose the final sequence. As the classifier loss has learned to distinguish between a correct and two wrong utterances, this gives an additional source for choosing a final beam. 5. Evaluation In section 3.2. we validated our human/human dataset regarding correct usage of the given profiles. Now we want to evaluate the general dialogue quality for both our dataset and the output of the baseline model. As automated metrics are not very meaningful when used to evaluate the quality of dialogues (Liu et al., 2016), we have performed a human evaluation. The results are shown in table 4. First we explain the used metrics and then evaluate the results regarding our dataset and baseline model. 5.1. Human Evaluation Metrics For the human evaluation we used Amazon Mechanical Turk again. To evaluate our dataset, we presented pairs of dialogue and one profile to crowd workers to rate. For our baseline model, we asked crowd-workers to chat about a given movie, but did not mention that their chat partner is a bot. We asked the Turker to rate some statements according to their agreement on a Likert scale with fiv"
2020.lrec-1.71,P14-5010,0,0.00300292,"riginal dialogues. However, for training we used byte pair encoding. 3.2. Dataset Validation After collecting the dialogues we post-processed and validated the dataset. As it is not possible to supervise the crowd-worker automatically while chatting, we have to be sure that a) they really talked about the profile entities and b) adhere to the opinions specified there. 3.2.1. Named Entity Resolution As a first step we extracted all named entities from each dialogue. Even though with the existence of powerful natural language processing tools like Spacy (Honnibal and Montani, 2017) and CoreNLP (Manning et al., 2014), which can detect mentions of names, organizations or countries with high precision (named entitiy recognition, NER), detecting movie titles still remains a challenging problem (Ashwini and Choi, 2014), especially with grammatical errors and spelling mistakes. However, for each dialogue, we knew which movie they were (supposed to be) chatting about, which reduces the complexity of named entity recognition in our domain. We used three different metrics to find an entity: First, exact string match on the lowercased strings, which has high precision but very low recall. Second, we compared every"
2020.lrec-1.71,N10-1020,0,0.0573961,"he two consistency dimensions knowledge (a speaker should not “forget” previously known facts) and opinion (a speaker should not change their opinion, at least not without any overt trigger in the conversation). In this example, each system response is locally coherent (a good reply to its immediate precursor), but globally inconsistent. While this particular example is constructed, it is not very far from what these early models would have been liable to produce. One reason for this is that these models were optimised only for local coherence, and trained from datasets such as TwitterCorpus (Ritter et al., 2010) and OpenSubtitles corpus (Tiedemann, 2012). These datasets contain dialogues from many people, without any information about the speakers and their opinions or knowledge state. To tackle issues like these, several augmented dialogue datasets have been introduced in recent years. Zhou et al. (2018) created a dataset with conversations based on Wikipedia articles about popular movies. Another more general dataset (Dinan et al., 2018) explicitly tasked one person in each conversation to link the used knowledge to each written utterance. Models trained on these augU1: “Do you know Pulp Fiction?”"
2020.lrec-1.71,tiedemann-2012-parallel,0,0.0114535,"ker should not “forget” previously known facts) and opinion (a speaker should not change their opinion, at least not without any overt trigger in the conversation). In this example, each system response is locally coherent (a good reply to its immediate precursor), but globally inconsistent. While this particular example is constructed, it is not very far from what these early models would have been liable to produce. One reason for this is that these models were optimised only for local coherence, and trained from datasets such as TwitterCorpus (Ritter et al., 2010) and OpenSubtitles corpus (Tiedemann, 2012). These datasets contain dialogues from many people, without any information about the speakers and their opinions or knowledge state. To tackle issues like these, several augmented dialogue datasets have been introduced in recent years. Zhou et al. (2018) created a dataset with conversations based on Wikipedia articles about popular movies. Another more general dataset (Dinan et al., 2018) explicitly tasked one person in each conversation to link the used knowledge to each written utterance. Models trained on these augU1: “Do you know Pulp Fiction?” S1: “Yes, I love Pulp Fiction.”u U2: “Yeah,"
2020.lrec-1.71,P18-1205,0,0.160638,"used knowledge to each written utterance. Models trained on these augU1: “Do you know Pulp Fiction?” S1: “Yes, I love Pulp Fiction.”u U2: “Yeah, Pulp Fiction is great.” S2: “I hate that movie.”u U3: “You hate Pulp Fiction?” S3: “I don‘t know it, is it good?”u Figure 1: A constructed example of a dialogue that is locally coherent, but globally incoherent along the dimensions knowledge (S3 to S1, S2) and opinion (S2 to S1). mented datasets produced to more engaging and more natural dialogues, as shown in that paper. As opposed to additional general knowledge, the PERSONA - CHAT dialogue corpus (Zhang et al., 2018) is based on personality profiles. Crowd workers were matched together in a role-playing chat and asked to get to know each other, considering profile information which was individually provided for every participant. Different types of neural networks were trained on that dataset, which were shown to also produce more engaging and consistent dialogues compared to models trained on other datasets. We contribute to this research a corpus that combines these strands, as it consists of dialogues that were collected in a setting where we controlled both the knowledge available to the dialogue part"
2020.lrec-1.71,D18-1076,0,0.0451894,"Missing"
2021.acl-long.546,D16-1147,0,0.02419,"fixed vocabulary. The architectures are based on recurrent neural networks such as LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) cells or self-attention layers (Vaswani et al., 2017) in sequence-to-sequence structures. To integrate knowledge in addition to the dialogue history these models can be augmented by additional recurrent cells to encode the knowledge into a fixedsized vector representation (Young et al., 2018; Parthasarathi and Pineau, 2020; Ghazvininejad et al., 2018). This can be traced back to first endto-end approaches reading documents for questionanswering (Miller et al., 2016) or more general sequential data (Sukhbaatar et al., 2015). He et al. (2017) embedded knowledge graphs (stored as triples) with LSTM cells and message-passing, and then used a decoder LSTM to generate a suitable answer. Long et al. (2017) used a CNN architecture to encode external knowledge instead. The recent success of unsupervised pre-trained language generation models such as GPT-2 yielded a variety of conversational models using selfattention based on the idea of fine-tuning the models with specific knowledge-grounded dialogue datasets (which we will discuss in Section 3). These models co"
2021.acl-long.546,P19-1081,0,0.115981,"e a concise encoding of knowledge graphs into a Transformer-based decoder architecture for knowledge-grounded dialogue generation. Transformers for natural language generation can be viewed as graph neural networks which use selfattention (Veliˇckovi´c et al., 2018) for neighborhood aggregation on fully-connected word graphs (Xu et al., 2019). We utilize this relationship and restrict the self-attention weights to match the underlying graph structure. Our comprehensive human evaluation with models trained with the publicly available datasets KOMODIS (Galetzka et al., 2020) and O PEN D IAL KG (Moon et al., 2019), both providing dialogues enriched with structured knowledge, shows that we can reduce the space requirement for context without negative effects on the precision of reproduction of knowledge and perceived consistency. Moreover, our models generate dialogues that are judged to be more detailed and interesting. For reproducibility, we publish all necessary source code and data (https://github.com/fabiangal/ space-efficient-context-encoding-acl21). 2 Knowledge-Augmented Neural Conversational Models Neural conversational models can be categorized into retrieval-based approaches (Lowe et al., 201"
2021.acl-long.546,2001.mtsummit-papers.68,0,0.015068,"models. In our experiments, perplexity does not correlate with naturalness (estimated by the human evaluators), indicating that this metric is insufficient to evaluate the overall dialogue quality, in particular across models trained on different datasets. In information retrieval, precision/recall and F1 score are typically chosen as automated metrics to evaluate the retrieval capabilities of a system. In our case, we are interested in the ability of reproducing entities and relations from the knowledge graph. Commonly used word-overlap metrics, such as ROUGE-N/ROUGE-L (Lin, 2004), or BLEU (Papineni et al., 2001), aren’t capable of measuring these. Evaluating precision and recall requires precise co-reference resolution, parse tree annotation and question labelling (e.g. entities, relations, intent). Assume following background facts and generated utterance: • h‘Pulp Fiction’, ‘has genre’, ‘crime’i • h‘comedy’, ‘bot opinion’, ‘I like’i • “It is a crime movie, but I am more interested in comedy films.” Without knowing the intent (asking for the genre of ’Pulp Fiction’ vs. asking for the preferred genre) of the previous utterance, we cannot determine if the occurrences of crime and comedy are true or fa"
2021.acl-long.546,N10-1020,0,0.0368425,"1994 trivia Actor Movie Actor type trivia Bruce Willis actor actor type type type genre Goodfellas year genre Movie depth 0 actor Samuel L. Jackson Crime opinion 1990 depth 1 Favourite depth 2 Figure 1: Illustration of the underlying subgraph data model for the external knowledge of a KOMODIS dialogue for different graphs depths: Nodes (green) with their fact-based attributes (blue) and opinions (orange). Subgraphs for depth 1 and depth 2 are incomplete. logue datasets, with Open-Subtitles (Vinyals and Le, 2015) and Twitter-Corpus (Sordoni et al., 2015) being some popular examples (see also (Ritter et al., 2010; Duplessis et al., 2016)). Some recently published datasets emphasize knowledgeable dialogues by integrating external information sources. The objective is to create models that generate consistent dialogues with a high knowledge retrieval accuracy (utilizing information from user profiles or knowledge graphs). Dinan et al. (2019) released the Wizard of Wikipedia dataset with over 22k open-domain dialogues. In each dialogue, one participant is playing the “wizard”, i.e. an expert who is presented with potentially interesting and relevant Wikipedia article excerpts, while the chat partner is t"
2021.acl-long.546,2021.eacl-main.24,0,0.0255743,"tion of knowledge and perceived consistency. Moreover, our models generate dialogues that are judged to be more detailed and interesting. For reproducibility, we publish all necessary source code and data (https://github.com/fabiangal/ space-efficient-context-encoding-acl21). 2 Knowledge-Augmented Neural Conversational Models Neural conversational models can be categorized into retrieval-based approaches (Lowe et al., 2015; Wu et al., 2017) that choose a next utterance from a set of suitable candidates, and generative approaches (Serban et al., 2016; Wolf et al., 2019; Chaudhuri et al., 2019; Roller et al., 2021) which decode the next utterance token by token out of a fixed vocabulary. The architectures are based on recurrent neural networks such as LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) cells or self-attention layers (Vaswani et al., 2017) in sequence-to-sequence structures. To integrate knowledge in addition to the dialogue history these models can be augmented by additional recurrent cells to encode the knowledge into a fixedsized vector representation (Young et al., 2018; Parthasarathi and Pineau, 2020; Ghazvininejad et al., 2018). This can be traced back to first endto-"
2021.acl-long.546,N15-1020,0,0.0325092,"age certificate Pulp Fiction shot location year United States 1994 trivia Actor Movie Actor type trivia Bruce Willis actor actor type type type genre Goodfellas year genre Movie depth 0 actor Samuel L. Jackson Crime opinion 1990 depth 1 Favourite depth 2 Figure 1: Illustration of the underlying subgraph data model for the external knowledge of a KOMODIS dialogue for different graphs depths: Nodes (green) with their fact-based attributes (blue) and opinions (orange). Subgraphs for depth 1 and depth 2 are incomplete. logue datasets, with Open-Subtitles (Vinyals and Le, 2015) and Twitter-Corpus (Sordoni et al., 2015) being some popular examples (see also (Ritter et al., 2010; Duplessis et al., 2016)). Some recently published datasets emphasize knowledgeable dialogues by integrating external information sources. The objective is to create models that generate consistent dialogues with a high knowledge retrieval accuracy (utilizing information from user profiles or knowledge graphs). Dinan et al. (2019) released the Wizard of Wikipedia dataset with over 22k open-domain dialogues. In each dialogue, one participant is playing the “wizard”, i.e. an expert who is presented with potentially interesting and relev"
2021.acl-long.546,P17-1046,0,0.0324695,"oth providing dialogues enriched with structured knowledge, shows that we can reduce the space requirement for context without negative effects on the precision of reproduction of knowledge and perceived consistency. Moreover, our models generate dialogues that are judged to be more detailed and interesting. For reproducibility, we publish all necessary source code and data (https://github.com/fabiangal/ space-efficient-context-encoding-acl21). 2 Knowledge-Augmented Neural Conversational Models Neural conversational models can be categorized into retrieval-based approaches (Lowe et al., 2015; Wu et al., 2017) that choose a next utterance from a set of suitable candidates, and generative approaches (Serban et al., 2016; Wolf et al., 2019; Chaudhuri et al., 2019; Roller et al., 2021) which decode the next utterance token by token out of a fixed vocabulary. The architectures are based on recurrent neural networks such as LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) cells or self-attention layers (Vaswani et al., 2017) in sequence-to-sequence structures. To integrate knowledge in addition to the dialogue history these models can be augmented by additional recurrent cells to encode"
2021.acl-long.546,D19-3014,0,0.011561,"he results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting. 1 Introduction Building on the idea of attention-based seq2seq models (Vaswani et al., 2017), recent language models such as BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019) enable neural conversational models to generate responses that appear human-like and engaging (Yu et al., 2019). A closer look, however, reveals that the lack of long-term memory to represent consistent (world) knowledge and personality over multiple speaker turns can lead to incoherent content being generated (Li et al., 2016; Serban et al., 2017). Initiated by the Conversational Intelligence Challenge (Burtsev et al., 2018; Dinan et al., 2020), the research focus therefore shifted towards knowledge-grounded dialogue ∗ † The first two authors contributed equally to this paper. Corresponding author generation, resulting in first promising approaches using Transformer-based architectures (Dinan et al.,"
2021.acl-long.546,P18-1205,0,0.0828933,"ded knowledge graphs (stored as triples) with LSTM cells and message-passing, and then used a decoder LSTM to generate a suitable answer. Long et al. (2017) used a CNN architecture to encode external knowledge instead. The recent success of unsupervised pre-trained language generation models such as GPT-2 yielded a variety of conversational models using selfattention based on the idea of fine-tuning the models with specific knowledge-grounded dialogue datasets (which we will discuss in Section 3). These models concatenate the additional context information as plain text to the input sequence (Zhang et al., 2018; Dinan et al., 2019; Galetzka et al., 2020). To differentiate context from dialogue, additional tokens are learned during fine-tuning and added to the word tokens. For bigger knowledge graphs, the limitation of the input sequence length of these models makes an information retrieval system necessary to estimate a small subset of relevant information that can be fed into the model. 3 Knowledge-Grounded Dialogue Datasets The increasing availability of conversational content on social media platforms such as Twitter or Reddit led to the construction of many dia7029 I like 16 “Worked on the film"
2021.acl-short.85,J08-4004,0,0.206349,"ld normally also be described by sentence B”). 3 3.1 How Can It be Evaluated? Relation Task / Dataset Given a task and a dataset, the first question to ask is how well the latter exemplifies the former. Investigating this is relatively straightforward. First, the dataset should be verified, which is to check whether the provided input/output pairs can indeed be judged correct relative to the task (in its intensional description). If the examples are collected specifically for the purpose of exemplifying the task, this is the process of controlling annotation, and standard methodologies exist (Artstein and Poesio, 2008; Pustejovsky and Stubbs, 2013). Care needs to be taken that the task is actually welldefined enough to pose an unambiguous challenge to capable language users.3 Validating a dataset is a less formalised process. It comprises arguing that the dataset indeed exemplifies the task intension well. For example, pairs only of images of giraffes and sentences describing them would arguably not exemplify the general task of image description very well (even if the descriptions are accurate), while perhaps exemplifying the task of giraffe image description. Another way to evaluate a dataset is by tryin"
2021.acl-short.85,2020.acl-main.463,0,0.0155788,"the methodology of using language tasks to drive research on models of language competence. I have argued that the success of this approach hinges on how well progress on one task can be translated into progress on other tasks. While some steps have been taken in this direction, current work still appears to mostly focus on isolated tasks (or groups of tasks). Overcoming this, in my opinion, will require more explicit considerations about how tasks and capabilities are connected, and how the set of capabilities is structured—to ensure movement is not only uphill, but rather up the right hill (Bender and Koller, 2020), and it indeed is a single hill. For this, a (re-)connection with the fields that study the composition of language competence—linguistics and cognitive and developemental psychology— seems advisable (if only to disagree explicitly). As a positive proposal, I suggest that a focus should be put on assembling a curriculum of tasks, organised in a complexity and inclusion hierarchy, and that the benchmarking target should be the developmental trajectory on this. Working this out in detail I must leave for future work. 673 11 In the neighbouring field of Computer Vision, there recently have been"
2021.acl-short.85,Q19-1043,0,0.0200969,"ly motivated and the output a linguistically motivated object (which may be consumed in a pipeline that itself is motivated as a product task); finally, a benchmark task – which is the type that concerns us here –is one which gets its value from how well it tests a particular ability (and nothing else) and how well it discriminates learners based on this ability.5 For a language benchmark task, the argument roughly goes as follows (even if typically only made implicitly): To be good at task T , an agent been collecting and providing datasets for more than 20 years now (Dua and Graff, 2019). 3 Pavlick and Kwiatkowski (2019), for example, show that the task of annotation textual entailments can lead to faultless 671 annotator disagreements. 4 The task of visual question answering provides an interesting example case of such a development. After Antol et al. (2015) introduced the first large scale dataset for this task, it quickly became clear that this dataset could be handled competitively by models that were deprived of visual input (“language bias”, as noted e.g. by Jabri et al., 2016). This problem was then addressed by Goyal et al. (2017) with the construction of a less biased (and hence more valid) corpus f"
2021.acl-short.85,N18-1202,0,0.00739276,"antly, we’d also want the model M learned from D to perform well on D0 . Similarly, if we have another task T 0 of which we think that it involves similar capabilities, we should expect it to be amenable to being modelled with a learning algorithm of similar type to M . What do we learn from a model M 0 (introducing architectural innovation κ over M ) performing better on T (via D)? We can take this as indication that κ may be what is responsible for increasing performance, and hence what is leading to a more adequate model of CT . 4.2 Multi-Task Models With the advent of pre-training in NLP (Peters et al., 2018; Devlin et al., 2018), where a model is trained on (a typically large amount of) data under a specific task-regime (typically language modelling, i.e. the task of predicting the next word in a running text) and then becomes part of the model for a target task, it has become common to test on a collection of tasks (Wang et al., 2019b,a). What do we learn from such a setup? In our Figure 1, if we find a task on which we can pre-train a model MP that becomes a part of models M and M 0 , and which makes them more powerful than models that do not have access to the pre-trained model, then we can i"
2021.acl-short.85,N18-1101,0,0.0230459,"volves. For a given c ∈ CT , is “c as required by T ” fully separable from any 6 To give some examples of informal versions of this argument, and choosing papers more or less randomly, here are some quotes: From the paper that introduced the visual question answering task (Antol et al., 2015): “What makes for a compelling AI-complete task? [. . . ] Open-ended questions require a potentially vast set of AI capabilities to answer – fine-grained recognition [. . . ], object detection [. . . ], activity recognition [. . . ], knowledge based reasoning [. . . ], and commonsense reasoning [. . . ].” Williams et al. (2018), on computing entailments: “The task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU. [. . . ] In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, and lexical and syntactic ambiguity.” 7 Such an attack challenges the claim of there being a necessary connection between handling T and possessing capability c. It might still very well be that humans can only perform this task if they possess capability c (and all the knowledge involved in it), because they wouldn’t be a"
2021.alvr-1.7,J95-2003,0,0.696465,"l and non-repetitive way, following principles of communicative discourse as for example in the recent PhotoBook dataset (Takmaz et al., 2020). Our work is complementary to such undertakings as it focuses on the interpretative rather than generative part. 3 Understanding reference in situated dialogue The notion of coreference chain–the sequence of mentions pointing to a same entity in a text–is central in coreference resolution. Built on top of the document as a unit, this notion relies on and in turn informs theories about accessibility hierarchy and salience of entities (Ariel, 1988, 2004; Grosz et al., 1995). In dialogue, however, references crisscross between the speakers and, one step further, in situated dialogue references crisscross between the speakers and the objects in the image. In this section we revise the annotation challenges in the annotation of anaphoric phenomena in data of this genre. 3.1 1. This is a picture of a bathtub. 2. The tub is white. 3. The wall and base of the tub are brown. 4. The door appears to be glass. 5. There is a handrail on the side wall. (2) 3.2 P1: closest to me, from left to right red, blue, white, red P2: ok, on your side I only see red, blue, white Speake"
2021.alvr-1.7,W19-8621,1,0.849301,"the challenges along two axes: • Dialogue: built by two speakers who each have their own mental state and cognitive process but who are communicating through referring expressions which are projected in the same conversation. • Shared physical context: simultaneous access to an image or other perceptual context which enables non-linear references to it. Instead, the reference is guided by visual attention. We present a linguistic perspective on these challenges by analysing a pilot annotation of two situated dialogue corpora: the Cups corpus (Dobnik et al., 2020) and the Tell-me-more corpus (Ilinykh et al., 2019), shown below in Figure 1 and example (1) respectively. Starting from the annotation scheme for several textual coreference datasets (Artstein and Poesio, 2006; Pradhan et al., 2007; Uryupina et al., 2019), this exercise proved useful to pinpoint in what ways the purely textual docIn recent years, a large number of corpora have been developed for vision and language tasks. We argue that there is still significant room for corpora that increase the complexity of both visual and linguistic domains and which capture different varieties of perceptual and conversational contexts. Working with two c"
2021.alvr-1.7,C69-7001,0,0.360956,"Perspective of participant 1. (b) Perspective of participant 2. Related Work (c) Top-down perspective of the Cups corpus scene with ground truth object IDs. Pointing to the inability of NLP tools to handle the textual part in situated dialogue, early works had described the need to ground the dialogue in the image in a manner informed by linguistics (Byron, 2003). As content develops in a text, entities are introduced and re-mentioned, establishing discourse referents. The context is provided by the document and no extra-linguistic reference is needed for resolving the reference to an entity (Karttunen, 1969). In situated dialogue, on the other hand, the visual modality brings the extra-linguistic context as a source of referents. Here, resolving references to entities can be thus achieved by either looking at the picture or by reading the discourse. Recording both strategies separately is crucial if we want to understand and model them soundly, in keeping with theories of cognitive processing (cf. (Kelleher et al., 2005)). Extending the coreference annotation paradigm is thus the best bet although not a lot Figure 1: Participant 1 cannot see the cups circled in blue, whereas participant 2 cannot"
2021.alvr-1.7,J18-3007,0,0.0133011,"lue, whereas participant 2 cannot see the cups circled in red. Person 3 is visible to both participants as a reference point. of work exists in this area. Textual coreference Annotated data for the coreference resolution task has mainly focused on news texts and concrete nouns, excluding reference to events and other coreferential relations such as bridging, deixis, and ambiguous items well documented in the linguistic literature but deemed infrequent or too difficult to process (Poesio, 2016). In contrast, there is a growing body of literature interested in phenomena beyond the nominal case (Kolhatkar et al., 2018; Nedoluzhko and Lapshinova-Koltunski, 2016), resulting in new, 40 although still small in size, annotated corpora (Lapshinova-Koltunski et al., 2018; Zeldes, 2017; Uryupina et al., 2020). concrete references can be grounded to the image easily, there are also some difficult cases. References can be found to portions of the image without a bounding box, such as base of the tub in example (1). Visual coreference Coreference work based on the popular VisDial dataset (Das et al., 2017) targets only a limited set of referential expressions, partly because it relies on automatic tools (Kottur et al"
2021.alvr-1.7,2020.emnlp-main.353,0,0.0314277,"ot clear. Last, as the image determines the scope of the referentiality, typical semantic properties are frequently used to refer back to the objects in the image: colour, shapes, sizes. These can be genuinely referential (a form of ellipsis) or used in an attributive manner. Compare for example white in the second sentence of (1), with (2) below. Referring expressions generation The goal in this area is to generate expressions over several turns of conversation in a natural and non-repetitive way, following principles of communicative discourse as for example in the recent PhotoBook dataset (Takmaz et al., 2020). Our work is complementary to such undertakings as it focuses on the interpretative rather than generative part. 3 Understanding reference in situated dialogue The notion of coreference chain–the sequence of mentions pointing to a same entity in a text–is central in coreference resolution. Built on top of the document as a unit, this notion relies on and in turn informs theories about accessibility hierarchy and salience of entities (Ariel, 1988, 2004; Grosz et al., 1995). In dialogue, however, references crisscross between the speakers and, one step further, in situated dialogue references c"
2021.alvr-1.7,L18-1065,0,0.0264339,"ts in this area. Textual coreference Annotated data for the coreference resolution task has mainly focused on news texts and concrete nouns, excluding reference to events and other coreferential relations such as bridging, deixis, and ambiguous items well documented in the linguistic literature but deemed infrequent or too difficult to process (Poesio, 2016). In contrast, there is a growing body of literature interested in phenomena beyond the nominal case (Kolhatkar et al., 2018; Nedoluzhko and Lapshinova-Koltunski, 2016), resulting in new, 40 although still small in size, annotated corpora (Lapshinova-Koltunski et al., 2018; Zeldes, 2017; Uryupina et al., 2020). concrete references can be grounded to the image easily, there are also some difficult cases. References can be found to portions of the image without a bounding box, such as base of the tub in example (1). Visual coreference Coreference work based on the popular VisDial dataset (Das et al., 2017) targets only a limited set of referential expressions, partly because it relies on automatic tools (Kottur et al., 2018; Yu et al., 2019), which are known to be problematic with this genre. With a focus in grounded human interaction, there are corpora whose tex"
2021.alvr-1.7,Q14-1006,0,0.057396,"resources combining language and vision while preserving continuity with the existing best practices in the area of coreference annotation. 1 Introduction With the ease of combining representations from different modalities provided by neural networks, text and vision are coming together. There is a growing body of resources addressing a setting in which the visual context can be exploited to support a textual task, for example visual coreference resolution. Several corpora have been developed in the domain of vision and language (V&L), for example corpora of image captions (Lin et al., 2014; Young et al., 2014; Krishna et al., 2017), images and paragraph descriptions (Krause et al., 2017), visual question answering (Antol et al., 2015), visual dialogue (Das et al., 2017) and embodied question answering (Das et al., 2018). Through these the V&L research has progressively moved from sentence descriptions to descriptions involving utterances and conversations, therefore adding complexity to their semantic representations. In parallel to the corpora, V&L systems have been developed but of course these are limited by the complexity of the task for which the dataset has been collected. The end goal of th"
2021.alvr-1.7,W16-0707,0,0.0247443,"Missing"
2021.alvr-1.7,D19-1516,0,0.0255101,"hko and Lapshinova-Koltunski, 2016), resulting in new, 40 although still small in size, annotated corpora (Lapshinova-Koltunski et al., 2018; Zeldes, 2017; Uryupina et al., 2020). concrete references can be grounded to the image easily, there are also some difficult cases. References can be found to portions of the image without a bounding box, such as base of the tub in example (1). Visual coreference Coreference work based on the popular VisDial dataset (Das et al., 2017) targets only a limited set of referential expressions, partly because it relies on automatic tools (Kottur et al., 2018; Yu et al., 2019), which are known to be problematic with this genre. With a focus in grounded human interaction, there are corpora whose textual part comprises question answer pairs (Antol et al., 2015; Goyal et al., 2017). Those, however, are short in nature, with few opportunities for re-mention of the different objects in the image and hence coreference. Last, corpora designed towards navigation and location (Stoia et al., 2008; Thomason et al., 2019) focusing on different kind of task and descriptions might be good candidates that could be explored and extended in a similar fashion as our corpora. (1) In"
2021.alvr-1.7,stoia-etal-2008-scare,0,0.0762251,"Missing"
2021.emnlp-main.90,W11-4605,0,0.677242,"improvements on mentally via restart-incrementality by repeatseveral NLP tasks, processes the input sequence edly feeding, to an unchanged model, increasas a whole, thus prioritising parallelisation to the ingly longer input prefixes to produce partial detriment of the notion of linear order. outputs. However, this approach is computationally costly and does not scale efficiently One way to employ non-incremental models in for long sequences. In parallel, we witness incremental settings is resorting to an incremental efforts to make Transformers more efficient, interface, like in Beuck et al. (2011), where a come.g. the Linear Transformer (LT) with a replete recomputation of the available partial input currence mechanism. In this work, we examhappens at each time step to deliver partial output. ine the feasibility of LT for incremental NLU Madureira and Schlangen (2020) examined the outin English. Our results show that the recurput stability of non-incremental encoders in this rent LT model has better incremental perforrestart-incremental fashion. While qualitatively mance and faster inference speed compared to the standard Transformer and LT with restartfeasible, this procedure is compu"
2021.emnlp-main.90,P93-1008,0,0.200541,"Missing"
2021.emnlp-main.90,H94-1010,0,0.195258,"e t. For LT+ models, we use incremental averaging to avoid recomputation. By doing this, sequence classification is performed similarly for all models. Models Restart Causal Incremental Recurrence Masking Delay Baseline LT LT+R LT+R+CM LT+R+CM+D X X - X X X X X -* -* X Table 1: Overview of the Transformer models. * means we perform further comparisons with a delayed variant. 4 4.1 Experimental Setup Datasets We evaluate our models on 9 datasets in English, which were also used in Madureira and Schlangen (2020). The tasks consist of sequence tagging: slot filling (ATIS, Hemphill et al. (1990); Dahl et al. (1994) and SNIPS, Coucke et al. (2018)), chunking (CoNLL-2000, Tjong Kim Sang and Buchholz (2000)), NER and PoS tagging (OntoNotes 5.0, WSJ section, Weischedel et al. (2013)); and sequence classification: intent detection (ATIS and SNIPS) and sentiment classification (positive/negative, Kotzias et al. (2015) and pros/cons, Ganapathibhotla and Liu (2008)). More details are available in the Appendix. tal and non-incremental evaluation, we follow the approach by Baumann et al. (2011) and Madureira and Schlangen (2020), evaluating incremental outputs with respect to the final output produced by the mode"
2021.emnlp-main.90,H90-1021,0,0.698418,"Missing"
2021.emnlp-main.90,2021.emnlp-main.830,0,0.0400819,"y, a procedure with high computational cost. The computational cost of a restart-incremental Transformer can be reduced with more efficient models or even avoided if an inherently incremental Transformer architecture existed. Recent works have proposed modifications that could help achieve that. For instance, by approximating the softmax attention with a recurrent state (Katharopoulos et al., 2020; Choromanski et al., 2021; Peng et al., 2021). The Linear Transformer model (Katharopoulos et al., 2020, LT henceforth) can be viewed as an RNN when the attention is causal (see also, very recently, Kasai et al., 2021). 3 3.1 Methods Overview of the Linear Transformer In LTs, the similarity score between a query and a key for the i-th position is computed using a kernel function. The causal attention can be written as: φ(Qi )T Si φ(Qi )T Zi i i X X Si = φ(Kj )VjT ; Zi = φ(Kj ) Atti (Q, K, V ) = (1) Si = Si−1 + φ(Ki )ViT (3) Zi = Zi−1 + φ(Ki ) (4) with S0 = Z0 = 0. As an RNN, the run-time complexity is linear with respect to the sequence length and constant for each added token, which promises faster inference compared to the restartincremental approach. 3.2 Models We examine the behaviour of Transformer mod"
2021.emnlp-main.90,P10-2012,0,0.0195266,"y In parallel, there is ongoing research on ways training the model to wait for right context beto make Transformers more efficient, e.g. the Linfore committing to an output and that training with input prefixes is beneficial for delivering ear Transformer (LT) introduced by Katharopoulos correct partial outputs. et al. (2020). Besides being more efficient, LTs can be employed with a recurrence mechanism based 1 Introduction on causal masking that turns them into models simOne fundamental property of human language pro- ilar to RNNs. In this work, we examine the suitcessing is incrementality (Keller, 2010). Humans ability of using LTs in incremental processing for process language on a word-by-word basis by main- sequence tagging and classification in English. We taining a partial representation of the sentence also inspect the use of the delay strategy (Baumann meaning at a fast pace and with great accuracy et al., 2011; Oda et al., 2015; Ma et al., 2019) to ex(Marslen-Wilson, 1973). The garden path effect, amine the effect of right context availability on the for example, shows that language comprehension is model’s incremental performance. Our hypothesis approximated incrementally before com"
2021.emnlp-main.90,P14-2130,0,0.0286708,"via restart-incrementality, trained with access to full sequences. 2. LT: the LT encoder incrementalised via restart-incrementality, trained with access to full sequences. 3. LT+R: the LT encoder trained as in (2) but during test time we use its recurrent state vector to predict the label at each time step, as in an RNN. 4. LT+R+CM: the LT encoder trained with causal masking to ensure each token representation can only attend to previous tokens. During inference, we convert the model to an RNN as in (3). Training with input prefixes aims at encouraging the learning of intermediate structures (Köhn and Menzel, 2014) and the anticipation of future output (Ma et al., 2019). 5. LT+R+CM+D: similar to (4), but, during training, the output for the input token xt is obtained at time t + d, where d ∈ {1, 2} is the delay, following the approach in Turek et al. (2020). There is evidence that additional right context improve the models’ incremental performance (Baumann et al., 2011; Ma et al., 2019; Madureira and Schlangen, 2020), which results in a trade-off between providing timely output or waiting for more context to deliver more stable output. (2) We also delay the output by 1 and 2 time steps for the baseline"
2021.emnlp-main.90,2020.emnlp-main.26,1,0.925138,"detriment of the notion of linear order. outputs. However, this approach is computationally costly and does not scale efficiently One way to employ non-incremental models in for long sequences. In parallel, we witness incremental settings is resorting to an incremental efforts to make Transformers more efficient, interface, like in Beuck et al. (2011), where a come.g. the Linear Transformer (LT) with a replete recomputation of the available partial input currence mechanism. In this work, we examhappens at each time step to deliver partial output. ine the feasibility of LT for incremental NLU Madureira and Schlangen (2020) examined the outin English. Our results show that the recurput stability of non-incremental encoders in this rent LT model has better incremental perforrestart-incremental fashion. While qualitatively mance and faster inference speed compared to the standard Transformer and LT with restartfeasible, this procedure is computationally costly, incrementality, at the cost of part of the nonespecially for long sequences, since it requires as incremental (full sequence) quality. We show many forward passes as the number of input tokens. that the performance drop can be mitigated by In parallel, ther"
2021.emnlp-main.90,P15-1020,0,0.0154121,"s being more efficient, LTs can be employed with a recurrence mechanism based 1 Introduction on causal masking that turns them into models simOne fundamental property of human language pro- ilar to RNNs. In this work, we examine the suitcessing is incrementality (Keller, 2010). Humans ability of using LTs in incremental processing for process language on a word-by-word basis by main- sequence tagging and classification in English. We taining a partial representation of the sentence also inspect the use of the delay strategy (Baumann meaning at a fast pace and with great accuracy et al., 2011; Oda et al., 2015; Ma et al., 2019) to ex(Marslen-Wilson, 1973). The garden path effect, amine the effect of right context availability on the for example, shows that language comprehension is model’s incremental performance. Our hypothesis approximated incrementally before committing to is that recurrence will allow LTs to be better in ina careful syntactic analysis (Frazier and Rayner, cremental processing as it captures sequence order. 1982; Altmann and Steedman, 1988; Trueswell As LTs use an approximation of softmax attention, et al., 1994, inter alia). we also expect a performance drop compared to The not"
2021.emnlp-main.90,D14-1162,0,0.089784,"h by Baumann et al. (2011) and Madureira and Schlangen (2020), evaluating incremental outputs with respect to the final output produced by the models. While the final output may differ from the gold standard, it serves as the target for the incremental output, as the non-incremental performance is an upper bound for incremental processing (Baumann et al., 2011). 4.3 Implementation We re-implement the Transformer and use the original implementation of the LT.1 All models are trained to minimise cross-entropy with the AdamW optimiser (Loshchilov and Hutter, 2019). We use 300-D GloVe embeddings (Pennington et al., 2014) which are passed through a linear projection layer with size dmodel . All experiments were performed on a GPU GeForce GTX 1080 Ti. Details on the implementation, hyperparameters and reproducibility are available in the Appendix. Our implementation is publicly available.2 5 Results and Discussion Tasks Baseline LT LT+R LT+R+ LT+R+ LT+R +CM CM+D1 CM+D2 ATIS-Slot SNIPS-Slot Chunk NER 94.51 90.13 91.27 89.55 93.67 87.98 88.42 86.13 86.84 63.16 67.54 52.04 93.78 81.88 86.63 69.09 94.38 85.72 89.42 81.39 93.54 86.91 89.33 85.55 PoS Tagging ATIS-Intent SNIPS-Intent Pos/Neg Pros/Cons 96.88 97.20 97.1"
2021.emnlp-main.90,W13-3516,0,0.0789841,"Missing"
2021.emnlp-main.90,W00-0726,0,0.32362,"Missing"
2021.emnlp-main.90,E09-1085,1,0.69724,"Missing"
2021.naacl-main.328,2020.acl-main.158,0,0.0949095,"e and what neural models trained on a language modelling objective do encode. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence. 1 Introduction Statistical models trained on large amounts of data using the language modelling objective (predicting words in context) have shown to pick up an intriguing amount of implicit knowledge about other tasks, for example syntactic knowledge (Warstadt et al., 2020; Hu et al., 2020) or world knowledge (Trinh and Le, 2019; Tamborrino et al., 2020). They have also been shown to exhibit, within these tasks, interesting divergences from expectation and sensitivity to confounding factors (e.g. McCoy et al. (2019)). Inspired by the recently released SyntaxGym (Gauthier et al., 2020), which enables specific and standardised evaluation of syntactic knowledge encoded in such models, we explore whether similar methods can be applied to the study of discourse knowledge or coherence, i.e., constraints acting across sentence boundaries, as illustrated in (1) (where ""#"" marks the less"
2021.naacl-main.328,2005.mtsummit-papers.11,0,0.127502,"us. evaluated here fail to pick up. 4.5 Explicit Connectives Test Suite Hypothesis: Meaning is constructed by building a representation for each new sentence based on the content of the previous sentences, and a first level of the coherence between two segments is embodied by explicit connectives. Hence, an inappropriate connective between two segments will yield a content gap. Sensitivity to content-meaning implies then sensitivity to a change in explicit connectives. For this exercise, we work with Disco-Annotation (Popescu-Belis et al., 2012), a corpus of segments from the Europarl corpus (Koehn, 2005) annotated with discourse connective senses.6 Eight discourse connectives are annotated in the corpus (as, although, though, while, since, yet, however, meanwhile), with one of five possible senses (contrast, concession, causal, temporal, comparison). We excluded all examples where the connective is in a segment initial position, since the previous segment is not provided, a setting incompatible with our constraints. This removed all examples of meanwhile. A minimal pair is created from each segment (7), where all the tokens up to the connective are used as context, followed by the original co"
2021.naacl-main.328,W18-5023,0,0.0212763,"daries, as illustrated in (1) (where ""#"" marks the less acceptable variant). a. #The lone ranger rode off into the sunset. Then he jumped on his horse. b. The lone ranger jumped on his horse. Then he rode into the sunset. A common approach to coherence evaluation consists in shuffling the sentence order of a text, thereby creating incoherent text samples that need to be discriminated from the original (Barzilay and Lapata, 2008). While this approach to creating incoherent test data is intuitive enough, recent studies suggest that it paints only a partial picture of what constitutes coherence (Lai and Tetreault, 2018; Mohammadi et al., 2020; Pishdad et al., 2020). It does not pinpoint the qualities that make the shuffled text incoherent, it does not tell us which linguistic devices are at fault, emphasising the need to move beyond this technique. This paper aims to add to the growing body of research stressing the need for more qualitative evaluations of text coherence (See et al., 2019; Mohammadi et al., 2020; Pishdad et al., 2020). We design different test suites created semiautomatically from existing corpora. This eases the burden of creating them from scratch and ensures the inclusion of multiple gen"
2021.naacl-main.328,J08-1001,0,0.561592,"n of syntactic knowledge encoded in such models, we explore whether similar methods can be applied to the study of discourse knowledge or coherence, i.e., constraints acting across sentence boundaries, as illustrated in (1) (where ""#"" marks the less acceptable variant). a. #The lone ranger rode off into the sunset. Then he jumped on his horse. b. The lone ranger jumped on his horse. Then he rode into the sunset. A common approach to coherence evaluation consists in shuffling the sentence order of a text, thereby creating incoherent text samples that need to be discriminated from the original (Barzilay and Lapata, 2008). While this approach to creating incoherent test data is intuitive enough, recent studies suggest that it paints only a partial picture of what constitutes coherence (Lai and Tetreault, 2018; Mohammadi et al., 2020; Pishdad et al., 2020). It does not pinpoint the qualities that make the shuffled text incoherent, it does not tell us which linguistic devices are at fault, emphasising the need to move beyond this technique. This paper aims to add to the growing body of research stressing the need for more qualitative evaluations of text coherence (See et al., 2019; Mohammadi et al., 2020; Pishda"
2021.naacl-main.328,D19-1060,0,0.0520853,"Missing"
2021.naacl-main.328,2020.acl-demos.10,0,0.180584,"e to the notion of coherence. 1 Introduction Statistical models trained on large amounts of data using the language modelling objective (predicting words in context) have shown to pick up an intriguing amount of implicit knowledge about other tasks, for example syntactic knowledge (Warstadt et al., 2020; Hu et al., 2020) or world knowledge (Trinh and Le, 2019; Tamborrino et al., 2020). They have also been shown to exhibit, within these tasks, interesting divergences from expectation and sensitivity to confounding factors (e.g. McCoy et al. (2019)). Inspired by the recently released SyntaxGym (Gauthier et al., 2020), which enables specific and standardised evaluation of syntactic knowledge encoded in such models, we explore whether similar methods can be applied to the study of discourse knowledge or coherence, i.e., constraints acting across sentence boundaries, as illustrated in (1) (where ""#"" marks the less acceptable variant). a. #The lone ranger rode off into the sunset. Then he jumped on his horse. b. The lone ranger jumped on his horse. Then he rode into the sunset. A common approach to coherence evaluation consists in shuffling the sentence order of a text, thereby creating incoherent text sample"
2021.naacl-main.328,J95-2003,0,0.909764,"sing than region 2 in condition match). Coherence. While the notion of syntactic acceptability is well studied from a linguistic point of view and in terms of neural language model representations (Marvin and Linzen, 2018; Warstadt et al., 2019, 2020; Hu et al., 2020, inter alia), it remains less clear what neural models are capable of capturing when modelling language across sentence boundaries. There exists a large body of work in linguistics regarding different notions of coherence, such as the influence of coreference (Hobbs, 1979; Barzilay and Lapata, 2008, inter alia), Centering theory (Grosz et al., 1995), discourse structure (Mann and Thompson, 1987; Webber et al., 2003), and phenomena that connect utterances in dialogue, such as conversational maxims (Grice, 1975) or speaker interaction (Lascarides and Asher, 2009). Many of these are also mentioned by coherence evaluation studies, nonetheless they mostly revert to the use of some form of sentence-order variations (Chen et al., 2019; Moon et al., 2019; Xu et al., 2019; Mesgar et al., 2020). While some progress has been made towards incorporating more linguistically motivated test sets (Chen et al., 2019; Mohammadi et al., 2020; Pishdad et al."
2021.naacl-main.328,D18-1151,0,0.156956,"on, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices beyond sentence order perturbations, allowing for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective do encode. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence. 1 Introduction Statistical models trained on large amounts of data using the language modelling objective (predicting words in context) have shown to pick up an intriguing amount of implicit knowledge about other tasks, for example syntactic knowledge (Warstadt et al., 2020; Hu et al., 2020) or world knowledge (Trinh and Le, 2019; Tamborrino et al., 2020). They have also been shown to exhibit, within these tasks, interesting divergences from exp"
2021.naacl-main.328,P19-1334,0,0.0197999,"m is equally suited to evaluate linguistic qualities that contribute to the notion of coherence. 1 Introduction Statistical models trained on large amounts of data using the language modelling objective (predicting words in context) have shown to pick up an intriguing amount of implicit knowledge about other tasks, for example syntactic knowledge (Warstadt et al., 2020; Hu et al., 2020) or world knowledge (Trinh and Le, 2019; Tamborrino et al., 2020). They have also been shown to exhibit, within these tasks, interesting divergences from expectation and sensitivity to confounding factors (e.g. McCoy et al. (2019)). Inspired by the recently released SyntaxGym (Gauthier et al., 2020), which enables specific and standardised evaluation of syntactic knowledge encoded in such models, we explore whether similar methods can be applied to the study of discourse knowledge or coherence, i.e., constraints acting across sentence boundaries, as illustrated in (1) (where ""#"" marks the less acceptable variant). a. #The lone ranger rode off into the sunset. Then he jumped on his horse. b. The lone ranger jumped on his horse. Then he rode into the sunset. A common approach to coherence evaluation consists in shuffling"
2021.naacl-main.328,2020.sigdial-1.28,0,0.020298,"PT-2 (Radford et al., 2019) has been shown to perform very well on many downstream language tasks. See et al. (2019) quantitatively evaluate GPT-2 as a language generator and find that it generally performs on par with a state-of-the-art neural story generation model. However, they also note that their automatic measures focus mostly on text diversity and stress the need for more qualitative evaluation methods for notions like text coherence. GPT-2 is also the basis of the recently proposed dialogue model D IALO GPT (Zhang et al., 2020), which is fine-tuned on conversational data from Reddit. Mehri and Eskenazi (2020) argue that D I 4165 ALO GPT encodes several notions of dialogue quality, including coherence. They manually create several positive and negative follow-up utterances for certain dialog qualities (e.g. “Wow, that’s interesting!"" or “I’m confused.""). The likelihood of D IALO GPT outputting either of them is then used to give an overall score per quality. The notion of dialogue coherence, although shown to be among the most important for predicting overall dialogue quality, is found to be one of the hardest to predict using this method. The authors attribute this to the fact that coherence (or t"
2021.naacl-main.328,2020.acl-main.133,0,0.22665,"stics regarding different notions of coherence, such as the influence of coreference (Hobbs, 1979; Barzilay and Lapata, 2008, inter alia), Centering theory (Grosz et al., 1995), discourse structure (Mann and Thompson, 1987; Webber et al., 2003), and phenomena that connect utterances in dialogue, such as conversational maxims (Grice, 1975) or speaker interaction (Lascarides and Asher, 2009). Many of these are also mentioned by coherence evaluation studies, nonetheless they mostly revert to the use of some form of sentence-order variations (Chen et al., 2019; Moon et al., 2019; Xu et al., 2019; Mesgar et al., 2020). While some progress has been made towards incorporating more linguistically motivated test sets (Chen et al., 2019; Mohammadi et al., 2020; Pishdad et al., 2020), most evaluation studies focus on models trained specifically on coherence classification and prediction tasks. Language models. The recently proposed transformer language model GPT-2 (Radford et al., 2019) has been shown to perform very well on many downstream language tasks. See et al. (2019) quantitatively evaluate GPT-2 as a language generator and find that it generally performs on par with a state-of-the-art neural story genera"
2021.naacl-main.328,2020.lrec-1.134,0,0.18363,"(1) (where ""#"" marks the less acceptable variant). a. #The lone ranger rode off into the sunset. Then he jumped on his horse. b. The lone ranger jumped on his horse. Then he rode into the sunset. A common approach to coherence evaluation consists in shuffling the sentence order of a text, thereby creating incoherent text samples that need to be discriminated from the original (Barzilay and Lapata, 2008). While this approach to creating incoherent test data is intuitive enough, recent studies suggest that it paints only a partial picture of what constitutes coherence (Lai and Tetreault, 2018; Mohammadi et al., 2020; Pishdad et al., 2020). It does not pinpoint the qualities that make the shuffled text incoherent, it does not tell us which linguistic devices are at fault, emphasising the need to move beyond this technique. This paper aims to add to the growing body of research stressing the need for more qualitative evaluations of text coherence (See et al., 2019; Mohammadi et al., 2020; Pishdad et al., 2020). We design different test suites created semiautomatically from existing corpora. This eases the burden of creating them from scratch and ensures the inclusion of multiple genres, crucially including"
2021.naacl-main.328,D19-1231,0,0.0739461,"xists a large body of work in linguistics regarding different notions of coherence, such as the influence of coreference (Hobbs, 1979; Barzilay and Lapata, 2008, inter alia), Centering theory (Grosz et al., 1995), discourse structure (Mann and Thompson, 1987; Webber et al., 2003), and phenomena that connect utterances in dialogue, such as conversational maxims (Grice, 1975) or speaker interaction (Lascarides and Asher, 2009). Many of these are also mentioned by coherence evaluation studies, nonetheless they mostly revert to the use of some form of sentence-order variations (Chen et al., 2019; Moon et al., 2019; Xu et al., 2019; Mesgar et al., 2020). While some progress has been made towards incorporating more linguistically motivated test sets (Chen et al., 2019; Mohammadi et al., 2020; Pishdad et al., 2020), most evaluation studies focus on models trained specifically on coherence classification and prediction tasks. Language models. The recently proposed transformer language model GPT-2 (Radford et al., 2019) has been shown to perform very well on many downstream language tasks. See et al. (2019) quantitatively evaluate GPT-2 as a language generator and find that it generally performs on par with"
2021.naacl-main.328,N16-1098,0,0.191852,"composed of an ordered set of sentences in a logical sequence; shuffling the sentences breaks the logical order and hence coherence. Since sequentiality is central to the language modelling task, models successfully distinguish between both versions. This shuffling technique has been widely applied in the evaluation of coherence models (Barzilay and Lapata, 2008; Chen et al., 2019; Moon et al., 2019; Xu et al., 2019; Mesgar et al., 2020). We include it as baseline for our method, in order to contrast how more fine-grained notions of coherence compare to this broad approach. We use ROCStories (Mostafazadeh et al., 2016) and the P ERSONA -C HAT corpus (Zhang et al., 2018) to evaluate sentence order for narration as well as dialogue data. The ROCStories corpus consists of coherent five-sentence stories which were gathered by employing crowdworkers and contain several temporal and causal relations between the sentences. To create the P ERSONA -C HAT corpus (Zhang et al., 2018), crowd sourced dialogue participants were assigned a persona in the form of descriptive natural language sentences and were asked to talk to each other impersonating their assigned persona. The dialogues contain at least 6 turns and we ex"
2021.naacl-main.328,2020.coling-main.539,0,0.135666,"e less acceptable variant). a. #The lone ranger rode off into the sunset. Then he jumped on his horse. b. The lone ranger jumped on his horse. Then he rode into the sunset. A common approach to coherence evaluation consists in shuffling the sentence order of a text, thereby creating incoherent text samples that need to be discriminated from the original (Barzilay and Lapata, 2008). While this approach to creating incoherent test data is intuitive enough, recent studies suggest that it paints only a partial picture of what constitutes coherence (Lai and Tetreault, 2018; Mohammadi et al., 2020; Pishdad et al., 2020). It does not pinpoint the qualities that make the shuffled text incoherent, it does not tell us which linguistic devices are at fault, emphasising the need to move beyond this technique. This paper aims to add to the growing body of research stressing the need for more qualitative evaluations of text coherence (See et al., 2019; Mohammadi et al., 2020; Pishdad et al., 2020). We design different test suites created semiautomatically from existing corpora. This eases the burden of creating them from scratch and ensures the inclusion of multiple genres, crucially including dialogue data. Each te"
2021.naacl-main.328,popescu-belis-etal-2012-discourse,0,0.0199918,"-mention test suite. WSJ and VPC refer to the News portion of the ARRAU corpus. evaluated here fail to pick up. 4.5 Explicit Connectives Test Suite Hypothesis: Meaning is constructed by building a representation for each new sentence based on the content of the previous sentences, and a first level of the coherence between two segments is embodied by explicit connectives. Hence, an inappropriate connective between two segments will yield a content gap. Sensitivity to content-meaning implies then sensitivity to a change in explicit connectives. For this exercise, we work with Disco-Annotation (Popescu-Belis et al., 2012), a corpus of segments from the Europarl corpus (Koehn, 2005) annotated with discourse connective senses.6 Eight discourse connectives are annotated in the corpus (as, although, though, while, since, yet, however, meanwhile), with one of five possible senses (contrast, concession, causal, temporal, comparison). We excluded all examples where the connective is in a segment initial position, since the previous segment is not provided, a setting incompatible with our constraints. This removed all examples of meanwhile. A minimal pair is created from each segment (7), where all the tokens up to th"
2021.naacl-main.328,K19-1079,0,0.128244,"ed from the original (Barzilay and Lapata, 2008). While this approach to creating incoherent test data is intuitive enough, recent studies suggest that it paints only a partial picture of what constitutes coherence (Lai and Tetreault, 2018; Mohammadi et al., 2020; Pishdad et al., 2020). It does not pinpoint the qualities that make the shuffled text incoherent, it does not tell us which linguistic devices are at fault, emphasising the need to move beyond this technique. This paper aims to add to the growing body of research stressing the need for more qualitative evaluations of text coherence (See et al., 2019; Mohammadi et al., 2020; Pishdad et al., 2020). We design different test suites created semiautomatically from existing corpora. This eases the burden of creating them from scratch and ensures the inclusion of multiple genres, crucially including dialogue data. Each test suite addresses a hypothesis about an underlying linguistic device contributing to a text’s coherence, i.e., choice of referring expressions, discourse connectives, and intention (speaker commitment). Our contributions are the following: We • extend SyntaxGym to handle phenomena acting across sentence boundaries, but keep the"
2021.naacl-main.328,2020.acl-main.357,0,0.0143891,"objective do encode. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence. 1 Introduction Statistical models trained on large amounts of data using the language modelling objective (predicting words in context) have shown to pick up an intriguing amount of implicit knowledge about other tasks, for example syntactic knowledge (Warstadt et al., 2020; Hu et al., 2020) or world knowledge (Trinh and Le, 2019; Tamborrino et al., 2020). They have also been shown to exhibit, within these tasks, interesting divergences from expectation and sensitivity to confounding factors (e.g. McCoy et al. (2019)). Inspired by the recently released SyntaxGym (Gauthier et al., 2020), which enables specific and standardised evaluation of syntactic knowledge encoded in such models, we explore whether similar methods can be applied to the study of discourse knowledge or coherence, i.e., constraints acting across sentence boundaries, as illustrated in (1) (where ""#"" marks the less acceptable variant). a. #The lone ranger rode off into the sunse"
2021.naacl-main.328,Q19-1040,0,0.0240523,"h region 1: The woman region 2: play region 3: the guitar https://cpllab.github.io/lm-zoo/ Each test suite also contains a prediction of the expected difference between conditions. Splitting the input into different regions makes it possible to measure the difference in model predictions at the token or phrase level. (e.g. region 2 in condition mismatch should be more surprising than region 2 in condition match). Coherence. While the notion of syntactic acceptability is well studied from a linguistic point of view and in terms of neural language model representations (Marvin and Linzen, 2018; Warstadt et al., 2019, 2020; Hu et al., 2020, inter alia), it remains less clear what neural models are capable of capturing when modelling language across sentence boundaries. There exists a large body of work in linguistics regarding different notions of coherence, such as the influence of coreference (Hobbs, 1979; Barzilay and Lapata, 2008, inter alia), Centering theory (Grosz et al., 1995), discourse structure (Mann and Thompson, 1987; Webber et al., 2003), and phenomena that connect utterances in dialogue, such as conversational maxims (Grice, 1975) or speaker interaction (Lascarides and Asher, 2009). Many of"
2021.naacl-main.328,J03-4002,0,0.273832,"of syntactic acceptability is well studied from a linguistic point of view and in terms of neural language model representations (Marvin and Linzen, 2018; Warstadt et al., 2019, 2020; Hu et al., 2020, inter alia), it remains less clear what neural models are capable of capturing when modelling language across sentence boundaries. There exists a large body of work in linguistics regarding different notions of coherence, such as the influence of coreference (Hobbs, 1979; Barzilay and Lapata, 2008, inter alia), Centering theory (Grosz et al., 1995), discourse structure (Mann and Thompson, 1987; Webber et al., 2003), and phenomena that connect utterances in dialogue, such as conversational maxims (Grice, 1975) or speaker interaction (Lascarides and Asher, 2009). Many of these are also mentioned by coherence evaluation studies, nonetheless they mostly revert to the use of some form of sentence-order variations (Chen et al., 2019; Moon et al., 2019; Xu et al., 2019; Mesgar et al., 2020). While some progress has been made towards incorporating more linguistically motivated test sets (Chen et al., 2019; Mohammadi et al., 2020; Pishdad et al., 2020), most evaluation studies focus on models trained specificall"
2021.naacl-main.328,P19-1363,0,0.0134033,"ndy when i volunteer."" ""i am a kindergarten teacher."" ""i love art and want to be a famous artist."" ""i am a kindergarten teacher."" Hypothesis: While it is possible for different speakers to have different opinions, speakers should not This highlights the importance of quality over quancontradict themselves. This test suite targets the tity. In future work, we will inspect this phenotion of speaker commitment in dialogue models. nomenon more closely and combine the selection The test suite is created automatically based on the of items with human evaluation, to gain a better DialogueNLI corpus (Welleck et al., 2019), which understanding of how the notion of speaker comcontains pairs of utterances annotated as contradic- mitment is and can be encoded in neural dialogue tion, entailment or neutral. The sentence pairs are models. 4170 GPT-2 Connective used in manipulation DIALOGPT Connective used in manipulation although – – as 0.92 1.00 however 0.92 0.86 since 0.857 0.86 though 0.84 0.43 while 0.86 1.00 yet 0.90 0.86 although – – as 0.88 1.00 however 0.83 0.86 since 0.82 1.00 though 0.76 0.93 while 0.76 0.79 yet 0.89 1.00 as_causal as_comparison as_concession as_PREPOSITION as_temporal 0.44 0.96 0.33 0.99"
2021.naacl-main.328,P19-1067,0,0.103259,"of work in linguistics regarding different notions of coherence, such as the influence of coreference (Hobbs, 1979; Barzilay and Lapata, 2008, inter alia), Centering theory (Grosz et al., 1995), discourse structure (Mann and Thompson, 1987; Webber et al., 2003), and phenomena that connect utterances in dialogue, such as conversational maxims (Grice, 1975) or speaker interaction (Lascarides and Asher, 2009). Many of these are also mentioned by coherence evaluation studies, nonetheless they mostly revert to the use of some form of sentence-order variations (Chen et al., 2019; Moon et al., 2019; Xu et al., 2019; Mesgar et al., 2020). While some progress has been made towards incorporating more linguistically motivated test sets (Chen et al., 2019; Mohammadi et al., 2020; Pishdad et al., 2020), most evaluation studies focus on models trained specifically on coherence classification and prediction tasks. Language models. The recently proposed transformer language model GPT-2 (Radford et al., 2019) has been shown to perform very well on many downstream language tasks. See et al. (2019) quantitatively evaluate GPT-2 as a language generator and find that it generally performs on par with a state-of-the-a"
2021.naacl-main.328,P18-1205,0,0.0224496,"ce; shuffling the sentences breaks the logical order and hence coherence. Since sequentiality is central to the language modelling task, models successfully distinguish between both versions. This shuffling technique has been widely applied in the evaluation of coherence models (Barzilay and Lapata, 2008; Chen et al., 2019; Moon et al., 2019; Xu et al., 2019; Mesgar et al., 2020). We include it as baseline for our method, in order to contrast how more fine-grained notions of coherence compare to this broad approach. We use ROCStories (Mostafazadeh et al., 2016) and the P ERSONA -C HAT corpus (Zhang et al., 2018) to evaluate sentence order for narration as well as dialogue data. The ROCStories corpus consists of coherent five-sentence stories which were gathered by employing crowdworkers and contain several temporal and causal relations between the sentences. To create the P ERSONA -C HAT corpus (Zhang et al., 2018), crowd sourced dialogue participants were assigned a persona in the form of descriptive natural language sentences and were asked to talk to each other impersonating their assigned persona. The dialogues contain at least 6 turns and we extract only the utterances and ignore the persona des"
2021.naacl-main.328,2020.acl-demos.30,0,0.24398,"on tasks. Language models. The recently proposed transformer language model GPT-2 (Radford et al., 2019) has been shown to perform very well on many downstream language tasks. See et al. (2019) quantitatively evaluate GPT-2 as a language generator and find that it generally performs on par with a state-of-the-art neural story generation model. However, they also note that their automatic measures focus mostly on text diversity and stress the need for more qualitative evaluation methods for notions like text coherence. GPT-2 is also the basis of the recently proposed dialogue model D IALO GPT (Zhang et al., 2020), which is fine-tuned on conversational data from Reddit. Mehri and Eskenazi (2020) argue that D I 4165 ALO GPT encodes several notions of dialogue quality, including coherence. They manually create several positive and negative follow-up utterances for certain dialog qualities (e.g. “Wow, that’s interesting!"" or “I’m confused.""). The likelihood of D IALO GPT outputting either of them is then used to give an overall score per quality. The notion of dialogue coherence, although shown to be among the most important for predicting overall dialogue quality, is found to be one of the hardest to pre"
C08-2003,N07-2012,0,0.0501159,"Missing"
C14-1170,N09-1043,1,0.827037,"rk. That will be followed by a description of the task and the model. In Section 4 we will show how our model performs in two experiments, the first uses speech and a visual scene, the second incorporates visual cues. 2 2.1 Background and Related Work Background: Incremental Dialogue Processing Dialogue systems that process incrementally produce behavior that is perceived by human users to be more natural than systems that use a turn-based approach (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). Incremental dialogue has seen improvements in speech recognition (Baumann et al., 2009), speech synthesis (Buschmeier et al., 2012), and dialogue management (Buß et al., 2010; Selfridge et al., 2012). Futhermore, architectures for incremental dialogue systems have been proposed (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011) and incremental toolkits are also available (Baumann and Schlangen, 2012). In this paper, we approach natural language understanding (NLU), which aims to map an utterance to an intention, as a component in the incremental model of dialogue processing as described in (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009), where incremental syst"
C14-1170,W12-1641,1,0.853923,"n of the task and the model. In Section 4 we will show how our model performs in two experiments, the first uses speech and a visual scene, the second incorporates visual cues. 2 2.1 Background and Related Work Background: Incremental Dialogue Processing Dialogue systems that process incrementally produce behavior that is perceived by human users to be more natural than systems that use a turn-based approach (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). Incremental dialogue has seen improvements in speech recognition (Baumann et al., 2009), speech synthesis (Buschmeier et al., 2012), and dialogue management (Buß et al., 2010; Selfridge et al., 2012). Futhermore, architectures for incremental dialogue systems have been proposed (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011) and incremental toolkits are also available (Baumann and Schlangen, 2012). In this paper, we approach natural language understanding (NLU), which aims to map an utterance to an intention, as a component in the incremental model of dialogue processing as described in (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009), where incremental systems consist of a network of processing modul"
C14-1170,W10-4342,1,0.904349,"Missing"
C14-1170,W07-1210,0,0.202906,"te P (U |R) using Bayes’ rule, which cancels P (U ) and introduces P (R) into the summation, but P (R) can be dropped since (in this work) it can be approximated with a uniform distribution, yielding: P (I|U ) = P (I) X P (R = r|U )P (R = r|I) (2) r∈R There are, however, three important differences between the realisation of our model and the one presented in Kennington et al., (2013), all of which are a direct result of replacing, as we do here, the ngram model represented by P (U |R) with output from a parser that produces a Robust Minimal Recursion Semantics (RMRS) semantic representation (Copestake, 2007). Such a representation provides our model with a structured way to abstract over the surface forms. We will first give a brief explanation of the RMRS framework, then describe each of the three differences between our model and that of Kennington et al., (2013), namely (1) how the language grounds with the world, (2) how the frame is built, and (3) when to consider evidence for the slots in the frame. RMRS RMRS is a framework for representing semantics that factors a logical form into elementary predicates (EP). For example in Table 1, the first row represents the first word of an utterance,"
C14-1170,W09-3902,0,0.0249608,"ly does not work incrementally; semantic parsing / statistical natural language understanding via logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (Meurs et al., 2008; Meza-Ruiz et al., 2008), and Dynamic Bayesian Networks (Meurs et al., 2009); see also overviews of NLU in (De Mori et al., 2008; Tur and De Mori, 2011), but typically neither provide situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situational grounding; as well as integration of gaze into language understanding (Prasov and Chai, 2010). We move beyond this work in that we present a model that is incremental, uses a form of grounded semantics, can easily incorporate multi-modal information sources, and which inference can be performed quickly, satisfying the demands of real-time dialogue. 3 3.1 Task and Model Task The task for our model is as follows: to compute at any moment a distribution over possible in"
C14-1170,2007.sigdial-1.25,1,0.875987,"Missing"
C14-1170,W10-4302,1,0.91827,"take). (2) Building the Frame In this paper, intentions are represented as frames. However, unlike Kennington et al., (2013), we don’t assume beforehand that we know the slots of the frame. To determine the slots, we turn again to RMRS and build a slot for each entity that is produced (more on this below). This kind of frame, coupled with the RMRS representation, shows not just a meaning representation, but also interpretation of the representation in the current model (the real situation / visual domain of discourse), outputted incrementally making our model fully incremental in the sense of Heintze et al., (2010). The final, bolded NLU frame in Figure 1 shows the addressee (in this case, the dialogue system) as the recipient of the request, the request itself is a take request, where the object to be taken is obj5, as indexed by the real world, and that object happens to be red (i.e., e12 represents the notion of redness). (3) Driven by Sematics Another important difference is when to consider the semantic evidence and when to ignore it, in terms of when to apply the model for interpretation of the slots. In Kennington et al., (2013), each slot in the frame was processed at each increment in the entir"
C14-1170,W12-1643,1,0.927304,"curacy (i.e., they do not represent ground truth); we use the top-predicted output of the RMRS parser explained in Peldszus et al (2012). 4.1 Pento Puzzle with Speech  ACTION  OBJECT RESULT Figure 2: Example Pentomino Board  rotate  obj4  clockwise Figure 3: Pento gold frame example  X8 E 2  X 14 E 21  addr  rotate   obj4 clockwise Figure 4: Pento frame example from our model Data and Task The Pentomino domain (Fern´andez et al., 2007) contains task-oriented conversational data which has been used in several situated dialogue studies (Heintze et al., 2010; Peldszus et al., 2012; Kennington and Schlangen, 2012; Kennington et al., 2013). This corpus was collected in a Wizard-of-Oz study, where the user goal was to instruct the computer to pick up, delete, rotate or mirror puzzle tiles on a rectangular board (as in Figure 2), and place them onto another board. For each utterance, the corpus records the state of the game board before the utterance, the immediately preceding system action, and the intended interpretation of the utterance (as understood by the Wizard) in the form of a semantic frame specifying action-type and arguments, where those arguments are objects occurring in the description of t"
C14-1170,W13-4030,1,0.543024,"world in which the utterance is happening. The slots of these frames are to be filled with semantic constants, that is, they are uniquely resolved, if appropriate, to objects in the shared environment. This is illustrated in Figure 1 where the words of the utterance give 1804 rise to the part-of-speech tags, the incrementally growing syntax, semantic representation, and, finally, the intention. Note how x14 in the bolded NLU frame resolves to an object identifier for a real object in the shared scene (red cross in the bottom-left of the game board shown on the right in the figure). 3.2 Model Kennington et al., (2013) presented a simple, incremental model of NLU, which is an update model (i.e., increments build on previous ones) and which can potentially work in real time and in situated environments. The goal of the model is to recover I, the intention of the speaker behind the utterance, word by word. We observe U , the current word (or in this paper, a semantic meaning representation, see below) and an unobserved mediating variable R which represents visual or abstract properties of the object of the intention. Formally, we are interested in P (I|U ), the probability of a certain intention I underlying"
C14-1170,W13-4048,1,0.429032,"ad Table 2: Incremental Results for Pento slots with varying sentence lengths, Kennington et al.,(2013), Edit overhead represents all lengths of utterances. 4.2 1-6 12.03 37.84 1.57 1-6 30.64 32.27 3.1 1-6 59.72 62.80 7.71 7-8 7.8 26.02 9-14 12.59 24.11 7-8 17.66 19.20 9-14 14.46 15.79 7-8 54.50 64.13 9-14 48.94 60.72 Table 3: Incremental Results for Pento slots with varying sentence lengths, current work. Edit overhead represents all lengths of utterances. Pento Puzzle with Speech, Gaze, and Deixis Data and Task The second experiment uses data also from the Pentomino domain, as described in (Kousidis et al., 2013; Kennington et al., 2013), also a Wizard-of-Oz study consisting of 7 participants, example in Figure 1. The user was to select a puzzle tile (out of a possible 15) on a game board shown on a large monitor, and then describe this piece to the “system” (wizard). Speech, eye gaze (tracked by Seeingmachines FaceLab) and pointing gestures (tracked by Microsoft Kinect) were recorded. After the participant uttered a confirmation, the wizard began a new episode, generating a new random board and 1808 the process repeated. The task for the NLU in this experiment was reference resolution. The informati"
C14-1170,P11-1060,0,0.0168979,"and each node is an IU in the IU network. The focus of this paper is the top layer (module), but how it is produced depends on the layers below it. 2.2 Related Work The work presented in this paper connects and extends recent work in grounded semantics (Roy, 2005; Hsiao et al., 2008; Liu et al., 2012; Chai et al., 2014), which aims to connect language with the world, but typically does not work incrementally; semantic parsing / statistical natural language understanding via logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (Meurs et al., 2008; Meza-Ruiz et al., 2008), and Dynamic Bayesian Networks (Meurs et al., 2009); see also overviews of NLU in (De Mori et al., 2008; Tur and De Mori, 2011), but typically neither provide situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situational grounding; as well as integration of gaze into language understanding (Prasov and Chai,"
C14-1170,W12-1621,0,0.0229038,"es are also interconnected via so-called same level links (SLL) and groundedin links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that sequence to convey what IUs directly affect them. See Figure 1 for an example; each layer represents a module in the IU-module network and each node is an IU in the IU network. The focus of this paper is the top layer (module), but how it is produced depends on the layers below it. 2.2 Related Work The work presented in this paper connects and extends recent work in grounded semantics (Roy, 2005; Hsiao et al., 2008; Liu et al., 2012; Chai et al., 2014), which aims to connect language with the world, but typically does not work incrementally; semantic parsing / statistical natural language understanding via logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (Meurs et al., 2008; Meza-Ruiz et al., 2008), and Dynamic Bayesian Networks (Meurs et al., 2009); see also overviews of NLU in (De Mori et al., 2008; Tur and De Mori, 2011), but typically neither provide situated interpre"
C14-1170,W12-4705,1,0.743439,"tive as long as x14 is referenced as an entity in the RMRS increment. With these important extensions, our model of NLU is highly driven by the semantic meaning representation that is being built incrementally for the utterance. We will now show through two experiments how our approach improves upon previous work. 4 Experiments Similar to Kennington et al., (2013), we use the model represented formally in Equation 2, where P (R|U ) is realised using a maximum entropy classifier (ME) that predicts properties from RMRS evidence.1 We use the German RMRS parser described in Peldszus et al (2012), Peldszus and Schlangen (2012) which is a top-down PCFG parser that builds RMRS structure incrementally with the parse. We train an individual model for each RMRS entity type (e.g., e and x), where the features are the entity type, relations, and predicates of an RMRS increment and the class label are the visual properties. 1 http://opennlp.apache.org/ 1806 The RMRS representations are not checked for accuracy (i.e., they do not represent ground truth); we use the top-predicted output of the RMRS parser explained in Peldszus et al (2012). 4.1 Pento Puzzle with Speech  ACTION  OBJECT RESULT Figure 2: Example Pentomino Bo"
C14-1170,E12-1052,1,0.886474,"14 is under way, and active as long as x14 is referenced as an entity in the RMRS increment. With these important extensions, our model of NLU is highly driven by the semantic meaning representation that is being built incrementally for the utterance. We will now show through two experiments how our approach improves upon previous work. 4 Experiments Similar to Kennington et al., (2013), we use the model represented formally in Equation 2, where P (R|U ) is realised using a maximum entropy classifier (ME) that predicts properties from RMRS evidence.1 We use the German RMRS parser described in Peldszus et al (2012), Peldszus and Schlangen (2012) which is a top-down PCFG parser that builds RMRS structure incrementally with the parse. We train an individual model for each RMRS entity type (e.g., e and x), where the features are the entity type, relations, and predicates of an RMRS increment and the class label are the visual properties. 1 http://opennlp.apache.org/ 1806 The RMRS representations are not checked for accuracy (i.e., they do not represent ground truth); we use the top-predicted output of the RMRS parser explained in Peldszus et al (2012). 4.1 Pento Puzzle with Speech  ACTION  OBJECT RESULT"
C14-1170,D10-1046,0,0.185627,"ang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (Meurs et al., 2008; Meza-Ruiz et al., 2008), and Dynamic Bayesian Networks (Meurs et al., 2009); see also overviews of NLU in (De Mori et al., 2008; Tur and De Mori, 2011), but typically neither provide situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situational grounding; as well as integration of gaze into language understanding (Prasov and Chai, 2010). We move beyond this work in that we present a model that is incremental, uses a form of grounded semantics, can easily incorporate multi-modal information sources, and which inference can be performed quickly, satisfying the demands of real-time dialogue. 3 3.1 Task and Model Task The task for our model is as follows: to compute at any moment a distribution over possible intentions which the speaker wanted to convey in the utterance, expressed as semantic frames, given the unfolding utterance and information about the state of the world in which the utterance is happening. The slots of these"
C14-1170,E09-1081,1,0.955484,"es. 2 2.1 Background and Related Work Background: Incremental Dialogue Processing Dialogue systems that process incrementally produce behavior that is perceived by human users to be more natural than systems that use a turn-based approach (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). Incremental dialogue has seen improvements in speech recognition (Baumann et al., 2009), speech synthesis (Buschmeier et al., 2012), and dialogue management (Buß et al., 2010; Selfridge et al., 2012). Futhermore, architectures for incremental dialogue systems have been proposed (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011) and incremental toolkits are also available (Baumann and Schlangen, 2012). In this paper, we approach natural language understanding (NLU), which aims to map an utterance to an intention, as a component in the incremental model of dialogue processing as described in (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009), where incremental systems consist of a network of processing modules. Each module has a left buffer and a right buffer, where a typical module takes input This work is licensed under a Creative Commons Attribution 4.0 International Licence. P"
C14-1170,W12-1638,0,0.012733,"performs in two experiments, the first uses speech and a visual scene, the second incorporates visual cues. 2 2.1 Background and Related Work Background: Incremental Dialogue Processing Dialogue systems that process incrementally produce behavior that is perceived by human users to be more natural than systems that use a turn-based approach (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). Incremental dialogue has seen improvements in speech recognition (Baumann et al., 2009), speech synthesis (Buschmeier et al., 2012), and dialogue management (Buß et al., 2010; Selfridge et al., 2012). Futhermore, architectures for incremental dialogue systems have been proposed (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011) and incremental toolkits are also available (Baumann and Schlangen, 2012). In this paper, we approach natural language understanding (NLU), which aims to map an utterance to an intention, as a component in the incremental model of dialogue processing as described in (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009), where incremental systems consist of a network of processing modules. Each module has a left buffer and a right buffer, where a typica"
C14-1170,W10-4301,0,0.021815,"as an autonomous robot. In the following section we will provide background and present related work. That will be followed by a description of the task and the model. In Section 4 we will show how our model performs in two experiments, the first uses speech and a visual scene, the second incorporates visual cues. 2 2.1 Background and Related Work Background: Incremental Dialogue Processing Dialogue systems that process incrementally produce behavior that is perceived by human users to be more natural than systems that use a turn-based approach (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). Incremental dialogue has seen improvements in speech recognition (Baumann et al., 2009), speech synthesis (Buschmeier et al., 2012), and dialogue management (Buß et al., 2010; Selfridge et al., 2012). Futhermore, architectures for incremental dialogue systems have been proposed (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011) and incremental toolkits are also available (Baumann and Schlangen, 2012). In this paper, we approach natural language understanding (NLU), which aims to map an utterance to an intention, as a component in the incremental model of dialogue processing as descri"
C14-1170,E09-1085,1,0.884864,"tuated dialogue system, such as an autonomous robot. In the following section we will provide background and present related work. That will be followed by a description of the task and the model. In Section 4 we will show how our model performs in two experiments, the first uses speech and a visual scene, the second incorporates visual cues. 2 2.1 Background and Related Work Background: Incremental Dialogue Processing Dialogue systems that process incrementally produce behavior that is perceived by human users to be more natural than systems that use a turn-based approach (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). Incremental dialogue has seen improvements in speech recognition (Baumann et al., 2009), speech synthesis (Buschmeier et al., 2012), and dialogue management (Buß et al., 2010; Selfridge et al., 2012). Futhermore, architectures for incremental dialogue systems have been proposed (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011) and incremental toolkits are also available (Baumann and Schlangen, 2012). In this paper, we approach natural language understanding (NLU), which aims to map an utterance to an intention, as a component in the incremental model"
C14-1170,D07-1071,0,0.0146782,"irectly affect them. See Figure 1 for an example; each layer represents a module in the IU-module network and each node is an IU in the IU network. The focus of this paper is the top layer (module), but how it is produced depends on the layers below it. 2.2 Related Work The work presented in this paper connects and extends recent work in grounded semantics (Roy, 2005; Hsiao et al., 2008; Liu et al., 2012; Chai et al., 2014), which aims to connect language with the world, but typically does not work incrementally; semantic parsing / statistical natural language understanding via logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (Meurs et al., 2008; Meza-Ruiz et al., 2008), and Dynamic Bayesian Networks (Meurs et al., 2009); see also overviews of NLU in (De Mori et al., 2008; Tur and De Mori, 2011), but typically neither provide situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situa"
C14-1170,P09-1110,0,0.0145037,"1 for an example; each layer represents a module in the IU-module network and each node is an IU in the IU network. The focus of this paper is the top layer (module), but how it is produced depends on the layers below it. 2.2 Related Work The work presented in this paper connects and extends recent work in grounded semantics (Roy, 2005; Hsiao et al., 2008; Liu et al., 2012; Chai et al., 2014), which aims to connect language with the world, but typically does not work incrementally; semantic parsing / statistical natural language understanding via logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (Meurs et al., 2008; Meza-Ruiz et al., 2008), and Dynamic Bayesian Networks (Meurs et al., 2009); see also overviews of NLU in (De Mori et al., 2008; Tur and De Mori, 2011), but typically neither provide situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situational grounding; as well as int"
C14-1170,W12-1814,1,\N,Missing
D17-1100,D14-1005,0,0.10202,"patially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode information about the region relative to the image,"
D17-1100,P14-1023,0,0.052968,"s as girl and lady in Figure 1 do. Similar to our training procedure for situational embeddings, we now learn 300-dimensional word embeddings that predict occurrences of a word based on co-referential contexts, pairing each word with all words from referring expressions describing the same object. Textual Context (TXT) We learn standard distributional word embeddings from our corpus, ignoring extra-linguistic context. We train a skip-gram model (Mikolov et al., 2013) with negative sampling with window width 5, 300 dimensions. For comparison, we also use the textual word embeddings provided by Baroni et al. (2014), trained on a much larger web corpus (5word context window, 10 negative samples, 400 dimensions). We distinguish the two textual embeddings using the subscripts TXTref , TXTweb . 2.2 Situational Grounding (SIT) Visual Grounding (VIS) Given a set of referring expressions containing the word w and their corresponding referent (oj , rj ), w ∈ rj , we can derive a visual context for the word w by averaging over the visual representations of its referents visj , as proposed for instance by Kiela and Bottou (2014). The visual context of a word can be seen as a ‘visual prototype’. We derive represen"
D17-1100,D15-1242,0,0.0515475,"Missing"
D17-1100,W11-2501,0,0.0929366,"Missing"
D17-1100,P15-2020,0,0.0449432,"Missing"
D17-1100,P12-1015,0,0.110083,"ts denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode information about the regio"
D17-1100,E17-2016,0,0.0453696,"Missing"
D17-1100,Q13-1016,0,0.0318762,"e provides a learner not only with an example of a referent for the word lady, it also provides the information that lady can co-refer with girl, and that its denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1"
D17-1100,N15-1097,0,0.122025,"ous representations for words; in the following, we evaluate them for how well they predict semantic relations. Similarity We evaluate on some similarity data sets, reporting Spearman ρ correlations between human ratings and cosine similarities for word vectors. We use the MEN (Bruni et al., 2012) and 960 Silberer and Lapata (2014)’s data with semantic (SemSim) and visual similarity (VisSim) ratings. Model MEN SemSim VisSim Compat. Hyp.Dir. # pairs 989 2041 2041 4843 334 Compatibility As generic semantic similarity judgements are known to be “fuzzy” (Faruqui et al., 2016), we also evaluate on Kruszewski and Baroni (2015)’s benchmark on semantic compatibility. They define two words as being semantically compatible “if they can potentially refer to the same thing”. We expect our denotational and visual embeddings to be highly useful for this task. We report unsupervised results obtained from cosine similarities between word embeddings. VIS TXT ref DEN SIT DEN k TXT ref 0.404 0.550 0.646 0.470 0.654 0.469 0.584 0.583 0.468 0.632 0.427 0.484 0.491 0.371 0.531 0.241 0.230 0.163 0.134 0.207 78.14 55.69 81.14 59.58 79.94 TXT web 0.799 0.708 0.578 0.262 90.42 Table 1: Word similarity and relatedness evaluation Hypern"
D17-1100,N16-1043,0,0.0407168,"Missing"
D17-1100,W16-2506,0,0.146012,"atedness We now have four different continuous representations for words; in the following, we evaluate them for how well they predict semantic relations. Similarity We evaluate on some similarity data sets, reporting Spearman ρ correlations between human ratings and cosine similarities for word vectors. We use the MEN (Bruni et al., 2012) and 960 Silberer and Lapata (2014)’s data with semantic (SemSim) and visual similarity (VisSim) ratings. Model MEN SemSim VisSim Compat. Hyp.Dir. # pairs 989 2041 2041 4843 334 Compatibility As generic semantic similarity judgements are known to be “fuzzy” (Faruqui et al., 2016), we also evaluate on Kruszewski and Baroni (2015)’s benchmark on semantic compatibility. They define two words as being semantically compatible “if they can potentially refer to the same thing”. We expect our denotational and visual embeddings to be highly useful for this task. We report unsupervised results obtained from cosine similarities between word embeddings. VIS TXT ref DEN SIT DEN k TXT ref 0.404 0.550 0.646 0.470 0.654 0.469 0.584 0.583 0.468 0.632 0.427 0.484 0.491 0.371 0.531 0.241 0.230 0.163 0.134 0.207 78.14 55.69 81.14 59.58 79.94 TXT web 0.799 0.708 0.578 0.262 90.42 Table 1:"
D17-1100,N10-1011,0,0.0347955,"r with girl, and that its denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode informa"
D17-1100,N15-1016,0,0.071782,"co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode information about the region relative to the image, the full representation"
D17-1100,P14-2050,0,0.269625,"expressions provide richly structured contexts that go beyond just linking individual expressions with their denotations. As an example consider the scene in Figure 1 depicting several referents and corresponding referring expressions produced by different speakers. This scene provides a learner not only with an example of a referent for the word lady, it also provides the information that lady can co-refer with girl, and that its denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of image"
D17-1100,E17-1016,0,0.0460185,"Missing"
D17-1100,Q14-1006,0,0.649018,"able and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode information about the region relative to the image, the full representation hence is a vector of"
D17-1100,P16-2074,0,0.0298582,"71.64 VIS TXT ref DEN 70.14 68.49 73.67 71.63 71.57 74.32 TXT web 69.16 71.89 Table 4: Accuracies for co-referential expression detection top txtref den vis red text den vis small txtref den vis upper, bototm, bottom, bottem upper, topmost, tippy, above upper, above, of, corner yellow, purple, maroon, blue maroon, redman, reddish, allmiddle and, purple, yellow, pink large, smaller, big, tiny smaller, smallest, little, littiest directly, of, between, slightly Table 5: Top nearest neighbours for some example adjectives embeddings on this task (see previous findings on e.g. predicting antonyms (Nguyen et al., 2016)), the clear advange of denotational over visual embeddings is noteworthy. Whereas visual grounding is relatively effective for modeling compatibility between nouns (see Table 1), it does not seem to capture attribute meaning accurately as illustrated in Table 5. Here, the average of all visual objects referred to as e.g. small seems to be rather noisy and lead to high similarity with rather random words (directly) whereas denotational embeddings model accurate compatibility relations between e.g. small-smaller. Training From R EFER I T, we extract 161K training and 18K test pairs, dividing in"
D17-1100,E17-2012,0,0.0567279,"Missing"
D17-1100,P16-1115,1,0.842473,"a referent for the word lady, it also provides the information that lady can co-refer with girl, and that its denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the regi"
D17-1100,P14-1068,0,0.0687445,"Missing"
D17-1100,D14-1086,0,\N,Missing
E09-1081,W04-0308,0,0.00617063,"ing cycle and commits and revokes the various types of dysfluencies described for example by Levelt (1989) can be modelled. 4 It depends on the goals behind building the model whether this is considered a downside or desired behaviour. 715 modules have to be used later in the processing chain that can only handle complete input (that is, are non-incremental); we may call such a system prefix-based predictive, semi-incremental. (that is grounded in the LB-IU). In such a setup, the consuming module lags behind the sending module only for exactly the time it needs to process the input. Following Nivre (2004), we can call this strict incrementality. f:in≥out describes modules that potentially collect a certain amount of LB-IUs before producing an RB-IU based on them. This situation has been depicted in Figure 3 above. f:in≤out characterises modules that update RB more often than their LB is updated. This could happen in modules that produce endogenic information like clock signals, or that produce continuously improving hypotheses over the same input (see below), or modules that ‘expand’ their input, like a TTS that produces audio frames. With these categories in hand, we can make further distinct"
E09-1081,E09-1085,1,0.274642,"re • A list of modules that are part of the system. • For each of those a description in terms of which operations from Section 3.4.1 the module implements, and a characterisation of its behaviour in the terms of Section 3.4.2. • A set of axioms describing the connections between module buffers (and hence the network topology), as explained in Section 3.2. • Specifications of the format of the IUs that are produced by each module, in terms of the definition of slots in Section 3.3. 4 Example Specification We have built a fully incremental dialogue system, called N UMBERS (for more details see Skantze and Schlangen (2009)), that can engage in dialogues in a simple domain, number dictation. The system can not only be described in the terms explained here, but it also directly instantiates some of the data types described here. 6 The notion of connectedness is adapted from Sturt and Lombardo (2005), who provide evidence that the human parser strives for connectedness. 716 have a f:in≥out; c:in≥out characteristic. The system achieves a very high degree of responsiveness—by using incremental ASR and prosodic analysis for turn-taking decisions, it can react in around 200ms when suitable places for backchannels are"
E09-1081,W10-4301,1,\N,Missing
E09-1081,W10-4308,1,\N,Missing
E09-1081,W10-4342,1,\N,Missing
E09-1081,W09-3905,1,\N,Missing
E09-1081,W09-3902,0,\N,Missing
E09-1081,N09-1043,1,\N,Missing
E09-1081,J09-3001,0,\N,Missing
E09-1085,W08-0101,0,0.352261,"eciding when the user’s turn or utterance is finished, and typically has to rely only on silence detection and a time-out. Silence, however, is not a good indicator: sometimes there is silence but no turn-change is intended (e.g., hesitations), sometimes there isn’t silence, but the turn changes (Sacks et al., 1974). Speakers appear to use other knowledge sources, such as prosody, syntax and semantics to detect or even project the end of the utterance. Attempts have been made to incorporate such knowledge sources for turn-taking decisions in spoken dialogue systems (e.g., Ferrer et al., 2002; Raux & Eskenazi, 2008). To do so, incremental dialogue processing is clearly needed. Incremental processing can also lead to better use of resources, since later modules can start to work on partial results and do not have to wait until earlier modules have completed processing the whole utterance. For example, while the speech recogniser starts to identify words, the parser can already add these to the chart. Later modules can also assist in the processing and for example resolve ambiguities as they come up. Stoness et al. (2004) shows how a reference resolution module can help an incremental parser with NP suitab"
E09-1085,E09-1081,1,0.27148,"s the survey above shows, a number of studies have been done on incrementality in different areas of language processing. There are, however, to our knowledge no studies on how the various components could or should be integrated into a complete, fully incremental dialogue system, and how such a system might be perceived by naïve users, compared to a nonincremental system. This we provide here. 2.2 A general, abstract model The NUMBERS system presented in this paper can be seen as a specific instance (with some simplifying assumptions) of a more general, abstract model that we have developed (Schlangen & Skantze, 2009). We will here only briefly describe the parts of the general model that are relevant for the exposition of our system. We model the dialogue processing system as a collection of connected processing modules. The smallest unit of information that is communicated along the connections is called the incremental unit (IU), the unit of the “minimal amount of characteristic input”. Depending on what the module does, IUs may be audio frames, words, syntactic phrases, communicative acts, 746 etc. The processing module itself is modelled as consisting of a Left Buffer (LB), the Processor proper, and a"
E09-1085,W04-0304,0,0.112754,"ces for turn-taking decisions in spoken dialogue systems (e.g., Ferrer et al., 2002; Raux & Eskenazi, 2008). To do so, incremental dialogue processing is clearly needed. Incremental processing can also lead to better use of resources, since later modules can start to work on partial results and do not have to wait until earlier modules have completed processing the whole utterance. For example, while the speech recogniser starts to identify words, the parser can already add these to the chart. Later modules can also assist in the processing and for example resolve ambiguities as they come up. Stoness et al. (2004) shows how a reference resolution module can help an incremental parser with NP suitability judgements. Similarly, Aist et al. (2006) shows how a VP advisor could help an incremental parser. On the output side, an incremental dialogue system could monitor what is actually happening to the utterance it produces. As discussed by Raux & Eskenazi (2007), most dialogue managers operate asynchronously from the output components, which may lead to problems if the dialogue manager produces several actions and the user responds to one of them. If the input components do not have any information about t"
E09-1085,J81-4005,0,0.785795,"Missing"
E12-1052,N09-1043,1,0.8234,"Missing"
E12-1052,W11-4605,0,0.0337284,"Missing"
E12-1052,W07-1210,0,0.328236,"way, predicates can be introduced without fixed arity and arguments can be introduced without knowing which predicates they are arguments of. We will make use of this second form of underspecification and enrich lexical predicates with arguments incrementally. Combining two RMRS structures involves at least joining their list of EPs and ARGRELs and of scope constraints. Additionally, equations between the variables can connect two structures, which is an essential requirement for semantic construction. A semantic algebra for the combination of RMRSs in a non-lexicalist setting is defined in (Copestake, 2007). Unsaturated semantic increments have open slots that need to be filled by what is called the hook of another structure. Hook and slot are triples [`:a:x] consisting of a label, an anchor and an index variable. Every variable of the hook is equated with the corresponding one in the slot. This way the semantic representation can grow monotonically at each combinatory step by simply adding predicates, constraints and equations. Our approach differs from (Copestake, 2007) only in the organisation of the slots: In an incremental setting, a proper semantic representation is desired for every singl"
E12-1052,2007.sigdial-1.25,1,0.393009,"Missing"
E12-1052,W11-0144,0,0.230166,"Missing"
E12-1052,E09-1081,1,0.860825,"ested but actually non-existent token; repairs adjust unknown tokens to the requested token. These robust operations have strong penalties on the probability to make sure they will survive in the derivation only in critical situations. Additionally, only a single one of them is allowed to occur between the recognition of two adjacent input tokens. Figure 1 illustrates this process for the first few words of the example sentence “nimm den winkel in der dritten reihe” (take the bracket in the third row), using the incremental unit (IU) model to represent increments and how they are linked; see (Schlangen and Skantze, 2009).2 Here, syntactic 2 Very briefly: rounded boxes in the Figures represent IUs, and dashed arrows link an IU to its predecessor on the same level, where the levels correspond to processing stages. The Figure shows the levels of input words, POS-tags, syntactic derivations and logical forms. Multiple IUs sharing derivations (“CandidateAnalysisIUs”) are represented by three features: a list of the last parser actions of the derivation (LD), with rule expansions or (robust) lexical matches; the derivation probability (P); and the remaining stack (S), where S* is the grammar’s start symbol and S! a"
E12-1052,W09-3905,1,0.865664,"he derivations. If pragmatic feedback is enabled, the probability of every reprentation that does not resolve in the current context is degraded by a constant factor (we used 0.001 in our experiments described below, determined by experimentation). The degradation thus changes the derivation order in the parsing queue for the next input item and increases the chances of degraded derivations to be pruned in the following parsing step. 4 4.1 Experiments and Results Data We use data from the Pentomino puzzle piece domain (which has been used before for example by (Fern´andez and Schlangen, 2007; Schlangen et al., 2009)), collected in a Wizard-of-Oz study. In this specific setting, users gave instructions to the system (the wizard) in order to manipulate (select, rotate, mirror, delete) puzzle pieces on an upper board and to put them onto a lower board, reaching a pre-specified goal state. Figure 2 shows an example configuration. Each participant took part in several rounds in which the distinguishing characteristics for puzzle pieces (color, shape, proposed name, position on the board) varied widely. In total, 20 participants played 284 games. We extracted the semantics of an utterance from the wizard’s res"
E12-1052,W10-4308,1,0.852256,"h can express more fine-grainedly the temporal development of hypotheses. be used. But as we are building this module for an interactive system, ultimately, accuracy in recovering meaning is what we are interested in, and so we see this not just as a proxy, but actually as a more valuable metric. Moreover, this metric can be applied at each incremental step, which is not clear how to do with more traditional metrics. 4.4 Experiments Our parser, semantic construction and reference resolution modules are implemented within the InproTK toolkit for incremental spoken dialogue systems development (Schlangen et al., 2010). In this toolkit, incremental hypotheses are modified as more information becomes available over time. Our modules support all such modifications (i. e. also allow to revert their states and output if word input is revoked). As explained in Section 4.1, we used offline recognition results in our evaluation. However, the results would be identical if we were to use the incremental speech recognition output of InproTK directly. The system performs several times faster than real-time on a standard workstation computer. We thus consider it ready to improve practical end-toend incremental systems"
E12-1052,J09-3001,0,0.071876,"erence information in guiding a bottom-up chart-parser, which is evaluated on a single dialogue transcript. In contrast, our model uses a probabilistic top-down parser with beam search (following Roark (2001)) and is evaluated on a large number of real-world utterances as processed by an automatic speech recogniser. Similarly, DeVault and Stone (2003) describe a system that implements interaction between a parser and higher-level modules (in this case, even more principled, trying to prove presuppositions), which however is also only tested on a small, constructed data-set. Schuler (2003) and Schuler et al. (2009) present a model where information about reference is used directly within the speech recogniser, and hence informs not only syntactic processing but also word recognition. To this end, the processing is folded into the decoding step of the ASR, and is realised as a hierarchical HMM. While technically interesting, this approach is by design nonmodular and restricted in its syntactic expressivity. The work presented here also has connections to work in psycholinguistics. Pad´o et al. (2009) present a model that combines syntactic and semantic models into one plausibility judgement that is compu"
E12-1052,P03-1067,0,0.0293431,"odule that uses reference information in guiding a bottom-up chart-parser, which is evaluated on a single dialogue transcript. In contrast, our model uses a probabilistic top-down parser with beam search (following Roark (2001)) and is evaluated on a large number of real-world utterances as processed by an automatic speech recogniser. Similarly, DeVault and Stone (2003) describe a system that implements interaction between a parser and higher-level modules (in this case, even more principled, trying to prove presuppositions), which however is also only tested on a small, constructed data-set. Schuler (2003) and Schuler et al. (2009) present a model where information about reference is used directly within the speech recogniser, and hence informs not only syntactic processing but also word recognition. To this end, the processing is folded into the decoding step of the ASR, and is realised as a hierarchical HMM. While technically interesting, this approach is by design nonmodular and restricted in its syntactic expressivity. The work presented here also has connections to work in psycholinguistics. Pad´o et al. (2009) present a model that combines syntactic and semantic models into one plausibili"
E12-1052,W10-4301,0,0.460696,"Missing"
E12-1052,E09-1085,1,0.925149,"Missing"
E12-1052,W04-0304,0,0.0320882,"the output of automatic speech recognition. The remainder of this paper is structured as follows: We discuss related work in the next section, and then describe in general terms our model and its components. In Section 4 we then describe the data resources we used for the experiments and the actual implementation of the model, the baselines for comparison, and the results of our experiments. We close with a discussion and an outlook on future work. 2 Related Work The idea of using real-world reference to inform syntactic structure building has been previously explored by a number of authors. Stoness et al. (2004, 2005) describe a proof-of-concept imple514 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 514–523, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics mentation of a “continuous understanding” module that uses reference information in guiding a bottom-up chart-parser, which is evaluated on a single dialogue transcript. In contrast, our model uses a probabilistic top-down parser with beam search (following Roark (2001)) and is evaluated on a large number of real-world utterances as processed"
E17-2014,P14-1132,0,0.0888803,"dictors capturing referential meaning is promising to account for the fact that the negative instances used for training word predictors vary in their degree of semantic similarity to the positive instances of a word. We explored two different ways of integrating this information—by undersampling and by directly predicting similarity—and found the prediction approach to work better, especially for low- and medium-frequent words that have a range of lexically similar neighbors in the model’s vocabulary. In a similar vein, zero-shot learning approaches to object recognition (Frome et al., 2013; Lazaridou et al., 2014; Norouzi et al., 2013) have transferred visual knowledge from known object classes to unknown classes via distributional similarity. Here, we show that visual knowledge can be Where similarities do not help In Table 4, we can see results for words where similarity-based training does not help. For words with more than 50 training instances, distributional similarities degrade performance most for adjectives and words expressing visual attributes (color, shape, location). In these cases, distributional similarities group attributes from the same scale (color or location), but do not account fo"
E17-2014,P14-1023,0,0.0310392,"neural network, “GoogLeNet” (Szegedy et al., 2015), that was trained on data from the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We add 7 features that encode information about the region relative to the image: the (relative) coordinates of two corners, its (relative) area, distance to the center, and orientation of the image. The full representation hence is a vector of 1031 features. As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with 5-word context window, 10 negative samples, 400 dimensions). 4 Individual Words As shown in Table 1, the similarity-based training has a strong positive effect for entry-level nouns, whereas the effect on the overall vocabulary is rather small. This further suggests that distributional similarities improve certain word predictors substantially, whereas others might be affected even negatively. Therefore, in the following, we report average precision for individual words, namely for those cases where similarity-based regression has the strongest positive or negative effect as com"
E17-2014,P16-2074,0,0.155292,"tween object names. We find that the latter, similarity-based training method leads to substantial improvements for particular words such as entry-level nouns or hypernyms, whereas predictors for other words such as adjectives do not benefit from distributional knowledge. These results suggest that, in principle, semantic relatedness might be promising knowledge source for training more accurate visual models of referential word use, but it also supports recent findings showing that distributional models do not capture all aspects of semantic relatedness equally well (Rubinstein et al., 2015; Nguyen et al., 2016). Someone who knows the meaning of the word child will most probably know a) how to distinguish children from other entities in the real world and b) that child is related to other words, such as girl, boy, mother, etc. Traditionally, these two aspects of lexical meaning—which, following (Marconi, 1997), we may call referential and inferential, respectively—have been modeled in quite distinct settings. Semantic similarity has been a primary concern for distributional models of word meaning that treat words as vectors which are aggregated over their contexts, cf. (Turney and Pantel, 2010; Erk,"
E17-2014,D14-1086,0,0.212512,"ve been referred to by very similar words. (E.g., undersampling boy instances as negative instances for the child classifier.) This should allow the word classifier to focus on visual distinctions between objects that are semantically more important. When compiling the training set of a WAP - NOSIM classifier for word w, we look at its 10 most similar words in the vocabulary according to a distributional model (trained with word2vec, see below) and remove their instances from the set of negative instances ¬w. Data As training data, we use the training split of the REFERIT corpus collected by (Kazemzadeh et al., 2014), which is based on the medium-sized SAIAPR image collection (Grubinger et al., 2006) (99.5k image regions). For testing, we use the training section of REFCOCO corpus collected by (Yu et al., 2016), which is based on the MSCOCO collection (Lin et al., 2014) containing over 300k images with object segmentations. This gives us a large enough test set to make stable predictions about the quality of individual word predictors, which often only have a few positive instances in the test set of the REFERIT corpus. We follow (Schlangen et al., 2016) and select words with a minimum frequency of 40 in"
E17-2014,P15-2119,0,0.0199925,"ibutional similarities between object names. We find that the latter, similarity-based training method leads to substantial improvements for particular words such as entry-level nouns or hypernyms, whereas predictors for other words such as adjectives do not benefit from distributional knowledge. These results suggest that, in principle, semantic relatedness might be promising knowledge source for training more accurate visual models of referential word use, but it also supports recent findings showing that distributional models do not capture all aspects of semantic relatedness equally well (Rubinstein et al., 2015; Nguyen et al., 2016). Someone who knows the meaning of the word child will most probably know a) how to distinguish children from other entities in the real world and b) that child is related to other words, such as girl, boy, mother, etc. Traditionally, these two aspects of lexical meaning—which, following (Marconi, 1997), we may call referential and inferential, respectively—have been modeled in quite distinct settings. Semantic similarity has been a primary concern for distributional models of word meaning that treat words as vectors which are aggregated over their contexts, cf. (Turney a"
E17-2014,P16-1115,1,0.721325,"hat identify objects in images, by exploiting distributional similarity information during training. We show that for certain words (such as entry-level nouns or hypernyms), we can indeed learn better referential word meanings by taking into account their semantic similarity to other words. For other words, there is no or even a detrimental effect, compared to a learning setup that presents even semantically related objects as negative instances. 1 This paper extends upon recent work on learning models of referential word use on large-scale corpora of images paired with referring expressions (Schlangen et al., 2016). As in previous approaches in HRI, that work treats words during training and application as independent predictors, with no relations between them. Our starting assumption here is that this misses potentially useful information: e.g., that the costs for confusing referents of child vs. boy should be much lower than for confusing referents of child vs. car. We thus investigate whether knowledge about semantic similarities between words can be exploited to learn more accurate visual word predictors, accounting for this intuition that certain visual object distinctions are semantically more imp"
E17-2014,P16-1058,1,0.922997,"sifiers are trained with logistic regression (using `1 penalty). (This is the (Schlangen et al., 2016) model.) 3 Experimental Set-up We focus on assessing to what extent similaritybased visual word predictors capture the referential meaning of a word in a more accurate way, and distinguish its potential referents from other random objects. To factor out effects of compositionality and context that arise in reference generation or resolution, we measure how well a predictor for a word w is able to retrieve from a sampled test set objects that have been referred to by w (Schlangen et al., 2016; Zarrieß and Schlangen, 2016a) evaluate on full referring expressions). Undersampling similar objects (WAP - NOSIM) As discussed above, it is intuitive to assume that a visual classifier that distinguishes referents of a word from other objects in an image should be less penalized for making errors on objects that are categorically related. For instance, the classifier for child should be less penalized for giving high probabilities to referents of boy than to referents of car. A straightforward way to introduce these differences during training is by undersampling negative instances that have been referred to by very si"
E17-2014,W16-6642,1,0.928942,"sifiers are trained with logistic regression (using `1 penalty). (This is the (Schlangen et al., 2016) model.) 3 Experimental Set-up We focus on assessing to what extent similaritybased visual word predictors capture the referential meaning of a word in a more accurate way, and distinguish its potential referents from other random objects. To factor out effects of compositionality and context that arise in reference generation or resolution, we measure how well a predictor for a word w is able to retrieve from a sampled test set objects that have been referred to by w (Schlangen et al., 2016; Zarrieß and Schlangen, 2016a) evaluate on full referring expressions). Undersampling similar objects (WAP - NOSIM) As discussed above, it is intuitive to assume that a visual classifier that distinguishes referents of a word from other objects in an image should be less penalized for making errors on objects that are categorically related. For instance, the classifier for child should be less penalized for giving high probabilities to referents of boy than to referents of car. A straightforward way to introduce these differences during training is by undersampling negative instances that have been referred to by very si"
E17-2079,D15-1138,0,0.0168467,"mong which only one is correct. B¨orschinger et al. (2011) introduced an approach to ground language learning based on unsupervised PCFG induction. Kim and Mooney (2012) presents an enhancement of the PCFG approach that scales to such problems with highlyambiguous supervision. Berant et al. (2013) and Dong and Lapata (2016) map natural language to machine interpretable logical forms with 494 Related work (DFG). The first author would like to acknowledge the support from the China Scholarship Council. question-answer pairs. Tellex et al. (2012), Salvi et al. (2012), Matuszek et al. (2013), and Andreas and Klein (2015) proposed approaches to learn grounded semantics from natural language and action associations. These approaches paired ambiguous robot actions with natural language descriptions from humans. While these approaches achieve good learning performance, the ambiguous logical forms paired with the sentences were manually annotated. We attempted to align utterances and potential logical forms by continuously observing the instruction following actions. Our approach not only needs no human annotation or prior pairing of natural language and logical forms for the learning task, but also acquires less"
E17-2079,Q13-1005,0,0.0728297,"Missing"
E17-2079,D13-1160,0,0.0231929,"t not Experiment 1. The model performs better on the utterance level, which suggests that the hypothesis updating process can sucThere has been some recent work on grounded semantics with ambiguous supervision. For example, Kate and Mooney (2007) and Kim and Mooney (2010) paired sentences with multiple representations, among which only one is correct. B¨orschinger et al. (2011) introduced an approach to ground language learning based on unsupervised PCFG induction. Kim and Mooney (2012) presents an enhancement of the PCFG approach that scales to such problems with highlyambiguous supervision. Berant et al. (2013) and Dong and Lapata (2016) map natural language to machine interpretable logical forms with 494 Related work (DFG). The first author would like to acknowledge the support from the China Scholarship Council. question-answer pairs. Tellex et al. (2012), Salvi et al. (2012), Matuszek et al. (2013), and Andreas and Klein (2015) proposed approaches to learn grounded semantics from natural language and action associations. These approaches paired ambiguous robot actions with natural language descriptions from humans. While these approaches achieve good learning performance, the ambiguous logical fo"
E17-2079,D11-1131,0,0.0545883,"Missing"
E17-2079,P09-1010,0,0.0955888,"Missing"
E17-2079,C10-2062,0,0.0256676,"iment 1, the meaning representation is assembled incrementally as described above, but evaluated utterance-final. In Experiment 2, the model is evaluated incrementally, after each word of the utterance. Hence, late predictions (where a part of the utterance meaning is predicted later than would have been possible) are penalised in Experiment 2, but not Experiment 1. The model performs better on the utterance level, which suggests that the hypothesis updating process can sucThere has been some recent work on grounded semantics with ambiguous supervision. For example, Kate and Mooney (2007) and Kim and Mooney (2010) paired sentences with multiple representations, among which only one is correct. B¨orschinger et al. (2011) introduced an approach to ground language learning based on unsupervised PCFG induction. Kim and Mooney (2012) presents an enhancement of the PCFG approach that scales to such problems with highlyambiguous supervision. Berant et al. (2013) and Dong and Lapata (2016) map natural language to machine interpretable logical forms with 494 Related work (DFG). The first author would like to acknowledge the support from the China Scholarship Council. question-answer pairs. Tellex et al. (2012),"
E17-2079,D12-1040,0,0.015264,"ictions (where a part of the utterance meaning is predicted later than would have been possible) are penalised in Experiment 2, but not Experiment 1. The model performs better on the utterance level, which suggests that the hypothesis updating process can sucThere has been some recent work on grounded semantics with ambiguous supervision. For example, Kate and Mooney (2007) and Kim and Mooney (2010) paired sentences with multiple representations, among which only one is correct. B¨orschinger et al. (2011) introduced an approach to ground language learning based on unsupervised PCFG induction. Kim and Mooney (2012) presents an enhancement of the PCFG approach that scales to such problems with highlyambiguous supervision. Berant et al. (2013) and Dong and Lapata (2016) map natural language to machine interpretable logical forms with 494 Related work (DFG). The first author would like to acknowledge the support from the China Scholarship Council. question-answer pairs. Tellex et al. (2012), Salvi et al. (2012), Matuszek et al. (2013), and Andreas and Klein (2015) proposed approaches to learn grounded semantics from natural language and action associations. These approaches paired ambiguous robot actions w"
E17-2079,W12-2802,0,0.0209662,"Kim and Mooney (2010) paired sentences with multiple representations, among which only one is correct. B¨orschinger et al. (2011) introduced an approach to ground language learning based on unsupervised PCFG induction. Kim and Mooney (2012) presents an enhancement of the PCFG approach that scales to such problems with highlyambiguous supervision. Berant et al. (2013) and Dong and Lapata (2016) map natural language to machine interpretable logical forms with 494 Related work (DFG). The first author would like to acknowledge the support from the China Scholarship Council. question-answer pairs. Tellex et al. (2012), Salvi et al. (2012), Matuszek et al. (2013), and Andreas and Klein (2015) proposed approaches to learn grounded semantics from natural language and action associations. These approaches paired ambiguous robot actions with natural language descriptions from humans. While these approaches achieve good learning performance, the ambiguous logical forms paired with the sentences were manually annotated. We attempted to align utterances and potential logical forms by continuously observing the instruction following actions. Our approach not only needs no human annotation or prior pairing of natura"
E17-2079,P16-1224,0,0.0132046,"d the words that trigger them, and show that the temporal alignment leads to a better model than just recording the utterance-final action sequence. Introduction Situated instruction giving and following is a good setting for language learning, as it allows for the association of language with externalised meaning. For example, the reaction of drawing a circle on the top left of a canvas provides a visible signal of the comprehension of “top left, a circle”. That such signals are also useful for machine learning of meaning has been shown by some recent work (inter alia (Chen and Mooney, 2011; Wang et al., 2016)). While in that work instructions were presented as text and the comprehension signals (goal configurations or successful navigations) were aligned with full instructions, we explore signals that are aligned more fine-grainedly, possibly to sub-utterance chunks of material. This, we claim, is a setting that is more representative of situated interaction, where typically no strict turn 2 The learning task We now describe the learning task formally. We aim to enable a computer to learn word and utterance meanings by observing human reactions in a scene drawing task. At the beginning, the comput"
E17-2079,D10-1040,0,\N,Missing
E17-2079,P16-1004,0,\N,Missing
I17-2061,P16-1115,1,0.926562,"escription of it.1 In current work, these descriptions are typically ‘monomodal’, either purely verbal descriptions (Schuster et al., 2015; Hu et al., 2016),2 or via hand-drawn sketches (Sangkloy et al., 2016; Qian et al., 2016; Yu et al., 2016). In this work, we were interested in combining these modalities for image retrieval. We collected verbal descriptions of images (as shown in Figure 1), where the images were taken from an existing collection that provides for each image a matching sketch (Sangkloy et al., 2016). We trained “words-as-classifiers” models (Kennington and Schlangen, 2015; Schlangen et al., 2016) on the verbal descriptions to match these with images, and used the “triplet network” introduced by Sangkloy et al. (2016) to extract embeddings for the sketches. These models provide comparison scores for descriptions and candidate images, and can be combined into a joint score for a multimodal description (Section 3). We experiment with reduced sketches containing only a certain amount of the strokes from the full sketch, and Introduction In natural interactions, descriptions are typically multimodal: Someone explaining a route might point at visible landmarks while talking, or gesture them"
I17-2061,W15-2812,0,0.0327131,"image description improves recall. Verbal descriptions paired with fully detailed sketches still perform better than these sketches alone. We see these results as supporting the assumption that natural user interfaces should respond to multimodal input, where possible, rather than just language alone. 1 Figure 1: A photograph; a verbal description of its content; and a sketch. the task of image retrieval, that is, the task of retrieving one out of many photographs, based on a description of it.1 In current work, these descriptions are typically ‘monomodal’, either purely verbal descriptions (Schuster et al., 2015; Hu et al., 2016),2 or via hand-drawn sketches (Sangkloy et al., 2016; Qian et al., 2016; Yu et al., 2016). In this work, we were interested in combining these modalities for image retrieval. We collected verbal descriptions of images (as shown in Figure 1), where the images were taken from an existing collection that provides for each image a matching sketch (Sangkloy et al., 2016). We trained “words-as-classifiers” models (Kennington and Schlangen, 2015; Schlangen et al., 2016) on the verbal descriptions to match these with images, and used the “triplet network” introduced by Sangkloy et al"
I17-2061,P15-1029,1,0.930544,"f many photographs, based on a description of it.1 In current work, these descriptions are typically ‘monomodal’, either purely verbal descriptions (Schuster et al., 2015; Hu et al., 2016),2 or via hand-drawn sketches (Sangkloy et al., 2016; Qian et al., 2016; Yu et al., 2016). In this work, we were interested in combining these modalities for image retrieval. We collected verbal descriptions of images (as shown in Figure 1), where the images were taken from an existing collection that provides for each image a matching sketch (Sangkloy et al., 2016). We trained “words-as-classifiers” models (Kennington and Schlangen, 2015; Schlangen et al., 2016) on the verbal descriptions to match these with images, and used the “triplet network” introduced by Sangkloy et al. (2016) to extract embeddings for the sketches. These models provide comparison scores for descriptions and candidate images, and can be combined into a joint score for a multimodal description (Section 3). We experiment with reduced sketches containing only a certain amount of the strokes from the full sketch, and Introduction In natural interactions, descriptions are typically multimodal: Someone explaining a route might point at visible landmarks while"
L16-1019,C14-1189,0,0.0257379,"ional database that can be easily processed and queried across the different experimental settings in PentoRef. 2. Related Work Compared to other resources used in dialogue research, PentoRef follows a tradition perhaps best exemplified by the HCRC Map Task Corpus (Anderson et al., 1991; MacMahon et al., 2006) in that it combines the naturalness of unscripted conversation with the advantages of taskoriented dialogue, such as careful control over aspects of the linguistic and extralinguistic context. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, conseq"
L16-1019,D15-1224,0,0.0148599,"xt. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, consequently, the distractor objects (objects present in the scene which are not the target of a referring expression) cannot be directly controlled. 125 Although attempts have been made to systematically assess the effects of the different domains on the reference task (Gkatzia et al., 2015), the comparability of existing reference corpora is limited as they are based on very different types of visual stimuli. PentoRef provides an unusually wide spectrum of experimental settings that have been invest"
L16-1019,W10-4302,1,0.760804,"erring expression generation (REG). The corpus is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the sub-corpora could not"
L16-1019,D14-1086,0,0.0700623,"and extralinguistic context. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, consequently, the distractor objects (objects present in the scene which are not the target of a referring expression) cannot be directly controlled. 125 Although attempts have been made to systematically assess the effects of the different domains on the reference task (Gkatzia et al., 2015), the comparability of existing reference corpora is limited as they are based on very different types of visual stimuli. PentoRef provides an unusually wide spectrum of experimental setting"
L16-1019,P15-1029,1,0.854429,"is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the sub-corpora could not be directly exploited for systematic studies of referring"
L16-1019,W13-4030,1,0.855465,"ration (REG). The corpus is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the sub-corpora could not be directly exploited for"
L16-1019,koolen-krahmer-2010-tuna,0,0.0962069,"aps best exemplified by the HCRC Map Task Corpus (Anderson et al., 1991; MacMahon et al., 2006) in that it combines the naturalness of unscripted conversation with the advantages of taskoriented dialogue, such as careful control over aspects of the linguistic and extralinguistic context. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, consequently, the distractor objects (objects present in the scene which are not the target of a referring expression) cannot be directly controlled. 125 Although attempts have been made to systematically assess the effects"
L16-1019,W10-4210,0,0.0784762,"Missing"
L16-1019,W09-3905,1,0.796519,"resolution (RR) and referring expression generation (REG). The corpus is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the"
L16-1019,tokunaga-etal-2012-rex,0,0.141485,"Missing"
L16-1281,D14-1009,1,0.898212,"Missing"
L16-1281,lacheret-etal-2014-rhapsodie,0,0.0608519,"Missing"
L16-1281,C10-1154,0,0.0352277,"Missing"
L16-1549,W12-1814,1,0.0640968,"Missing"
L16-1549,H93-1016,0,0.149821,"Missing"
L16-1549,W10-4341,1,0.0408093,"Missing"
L16-1549,wittenburg-etal-2006-elan,0,0.0345379,"Missing"
L18-1333,L16-1551,0,0.0300159,"le and here is another description gray triangle and gestures (left). The arrow indicates the movement of the green ball. 3-D gesture features from these videos. Motion tracking sensors that recently have become readily available as well (e.g., Kinect1 and Leap sensor2 ) make it possible to record large scale 3-D gesture datasets, such as (Tompson et al., 2014; Marin et al., 2014; Liu and Shao, 2013; Sadeghipour and Morency, 2011) and datasets mentioned in (Cheng et al., 2016); however, most of these existing datasets are collected for gesture classification tasks without accompanied speech. (Fotinea et al., 2016) presented a dataset of multimodal commands, where gestures and accompanied are both recorded. However, the gestures are with defined meanings that are independent of speech. In addition to previous datasets, we present a corpus composed of natural multimodal communications with high-resolution hand motion data, in which the meaning of gestures depends on accompanied speech. 3. The Scene Description Experiment In this experiment, we aimed to collect intuitive scene descriptions. Participants were shown simple scenes (as shown in Figure 1) briefly and asked to describe the scenes from memory. T"
L18-1333,schiel-etal-2002-smartkom,0,0.234323,"Missing"
N15-1031,N12-1058,0,0.0590805,"Missing"
N15-1031,W12-1633,1,0.512713,"e of RR, ignoring pronouns or deixis. In this paper, we opted to use the model presented in Kennington et al. (2013), the simple incremental update model (SIUM). It has been tested extensively against data from a puzzle-playing human/computer interaction domain (the PENTO data, (Kousidis et al., 2013)); it can incorporate multi-modal information, works in real-time, and can resolve definite, exophoric, and deictic references in a single framework, all of which makes it a potential candidate for working in an interactive, multi-modal dialogue system. The model is similar to the one proposed in Funakoshi et al. (2012), which could resolve descriptions, anaphora, and deixis in a unified manner, but that model does not work incrementally.1 The main contributions of this paper are the more thorough exposition of the model (in Section 3) and its application and evaluation on much less constrained, more interactive (and hence realistic) data than what it has previously been tested on (Section 4). Moreover, the data set used here is also from a typologically very different language (Japanese) than what the model has been previously tested on (German), and so the robustness of the model against these differences"
N15-1031,J95-3003,0,0.412762,"Missing"
N15-1031,P10-1128,1,0.618145,"rence resolution, which has been tested in a simpler setup, on more natural data coming from a corpus of human/human interactions. The model is incremental in that it does not wait until the end of an utterance to process, rather it updates its interpretation at each word increment. The model can also incorporate other modalities, such as gaze or pointing cues (deixis) incrementally. We also model the saliency of the context, and show that the model can easily take such contextual information into account. The model improves over previous work on reference resolution applied to the same data (Iida et al., 2010; Iida et al., 2011). The paper is structured as follows: in the following section we discuss related work on incremental resolution of referring expressions. We explain the model that we use in Section 3 and the data we apply it to in Section 4. We then describe the experiments and the results and provide a discussion. 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015."
N15-1031,I11-1010,1,0.539537,"hich has been tested in a simpler setup, on more natural data coming from a corpus of human/human interactions. The model is incremental in that it does not wait until the end of an utterance to process, rather it updates its interpretation at each word increment. The model can also incorporate other modalities, such as gaze or pointing cues (deixis) incrementally. We also model the saliency of the context, and show that the model can easily take such contextual information into account. The model improves over previous work on reference resolution applied to the same data (Iida et al., 2010; Iida et al., 2011). The paper is structured as follows: in the following section we discuss related work on incremental resolution of referring expressions. We explain the model that we use in Section 3 and the data we apply it to in Section 4. We then describe the experiments and the results and provide a discussion. 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for"
N15-1031,W13-4030,1,0.881205,"(Schlangen et al., 2009), a model that used Markov Logic Networks to resolve objects on a screen (Kennington and Schlangen, 2013), a model of RR and incremental feedback (Traum et al., 2012), and an approach that used a semantic representation to refer to objects (Peldszus et al., 2012; 273 Kennington et al., 2014). However, the approaches reported there did not incorporate multi-modal information, were too slow to work in real-time, were evaluated on constrained data, or only focused on a specific type of RR, ignoring pronouns or deixis. In this paper, we opted to use the model presented in Kennington et al. (2013), the simple incremental update model (SIUM). It has been tested extensively against data from a puzzle-playing human/computer interaction domain (the PENTO data, (Kousidis et al., 2013)); it can incorporate multi-modal information, works in real-time, and can resolve definite, exophoric, and deictic references in a single framework, all of which makes it a potential candidate for working in an interactive, multi-modal dialogue system. The model is similar to the one proposed in Funakoshi et al. (2012), which could resolve descriptions, anaphora, and deixis in a unified manner, but that model"
N15-1031,C14-1170,1,0.724457,"s deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014). Incorporating pointing (deictic) gestures is also potentially useful in situated RR; as for example Matuszek et al. (2014) have shown in work on resolving objects processed by computer vision techniques. Chen and Eugenio (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures. However, these approaches were applied in settings in which communication between the two interlocutors was constrained, or the developed systems did not process incrementally. Kehler (2000) presented approach that focused more on interaction in a map task, though the model was not incremental, nor did grounding occur between language and world, as we do here. Incremental RR has also"
N15-1031,W14-4314,0,0.0217786,"f the corpus we used. 3 The Simple Incremental Update Model Following Kennington et al. (2013) and Kennington et al. (2014), we model the task at hand as one of recovering I, the intention of the speaker making the RE, where I ranges over the possible alternatives (the objects in the domain). This recovery proceeds incrementally (word by word), for RE of arbitrary length. That is, if U denotes the current word, we are interested in P (I|U ), the current hypothesis about 1 It can be argued that any non-incremental model could be made into an incremental one by applying that model at each word (Khouzaimi et al., 2014), but we would argue that more modeling effort is required in order for the model to work in an interactive dialogue system, see (Schlangen and Skantze, 2009; Aist et al., 2007; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991). the intended referent, given the observed word. We assume the presence of an unobserved, latent variable R, which models properties of the candidate objects such as colour or shape; explained further below), and so the computation formally is: P (I|U ) = X P (I, U, R) r∈R P (U ) (1) Which, after making some independence assumptions, can be factored into: P (I"
N15-1031,W13-4048,1,0.882779,"definite descriptions makes up a large part of human communication (Poesio and Vieira, 1997). In task-oriented situations, these references are often to entities that are visible in the shared environment. This kind of reference has attracted attention in recent computational research, but the kinds of interactions studied are often fairly restricted in controlled lab situations (Tanenhaus and Spivey-Knowlton, 1995) or simulated human/computer interactions, (Schlangen David Schlangen Bielefeld University Universit¨atsstraße 25 Bielefeld Germany david.schlangen@ uni-bielefeld.de et al., 2009; Kousidis et al., 2013; Chai et al., 2014). In such task-oriented, co-located settings, interlocutors can make use of extra-linguistic cues such as gaze or pointing gestures. Furthermore, listeners resolve references as they unfold, often identifying the referred entity before the end of the reference (Tanenhaus and Spivey-Knowlton, 1995; Spivey et al., 2002), however research in reference resolution has mostly focused on full, completed referring expressions. In this paper we make a first move towards addressing somewhat more complex domains. We apply a model of reference resolution, which has been tested in a sim"
N15-1031,W13-4010,0,0.019311,"intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014). Incorporating pointing (deictic) gestures is"
N15-1031,W14-4304,0,0.0126023,"ulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014). Incorporating pointing (deictic) gestures is also potentially useful in situated RR; as for example Matuszek et al. (2014) have shown in work on resolving objects processed by computer vision techniques. Chen and Eugenio (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures. However, these approaches were applied in settings in which communication between the two interlocutors was constrained, or the developed systems did not process incrementally. Kehler (2000) presented approach that focused more on interaction in a map task, thou"
N15-1031,E12-1052,1,0.853341,"n in a map task, though the model was not incremental, nor did grounding occur between language and world, as we do here. Incremental RR has also been studied in a number of papers, including a framework for fast incremental interpretation (Schuler et al., 2009), a Bayesian filtering model approach that was sensitive to disfluencies (Schlangen et al., 2009), a model that used Markov Logic Networks to resolve objects on a screen (Kennington and Schlangen, 2013), a model of RR and incremental feedback (Traum et al., 2012), and an approach that used a semantic representation to refer to objects (Peldszus et al., 2012; 273 Kennington et al., 2014). However, the approaches reported there did not incorporate multi-modal information, were too slow to work in real-time, were evaluated on constrained data, or only focused on a specific type of RR, ignoring pronouns or deixis. In this paper, we opted to use the model presented in Kennington et al. (2013), the simple incremental update model (SIUM). It has been tested extensively against data from a puzzle-playing human/computer interaction domain (the PENTO data, (Kousidis et al., 2013)); it can incorporate multi-modal information, works in real-time, and can re"
N15-1031,D10-1046,0,0.029976,"vide a discussion. 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this"
N15-1031,E09-1081,1,0.837559,"one of recovering I, the intention of the speaker making the RE, where I ranges over the possible alternatives (the objects in the domain). This recovery proceeds incrementally (word by word), for RE of arbitrary length. That is, if U denotes the current word, we are interested in P (I|U ), the current hypothesis about 1 It can be argued that any non-incremental model could be made into an incremental one by applying that model at each word (Khouzaimi et al., 2014), but we would argue that more modeling effort is required in order for the model to work in an interactive dialogue system, see (Schlangen and Skantze, 2009; Aist et al., 2007; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991). the intended referent, given the observed word. We assume the presence of an unobserved, latent variable R, which models properties of the candidate objects such as colour or shape; explained further below), and so the computation formally is: P (I|U ) = X P (I, U, R) r∈R P (U ) (1) Which, after making some independence assumptions, can be factored into: P (I|U ) = X 1 P (I) P (U |R)P (R|I) P (U ) (2) r∈R This is an update model in the usual sense that the posterior P (I|U ) at one step becomes the prior P (I) at"
N15-1031,W09-3905,1,0.960556,"on (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen"
N15-1031,J09-3001,0,0.0280009,"io (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures. However, these approaches were applied in settings in which communication between the two interlocutors was constrained, or the developed systems did not process incrementally. Kehler (2000) presented approach that focused more on interaction in a map task, though the model was not incremental, nor did grounding occur between language and world, as we do here. Incremental RR has also been studied in a number of papers, including a framework for fast incremental interpretation (Schuler et al., 2009), a Bayesian filtering model approach that was sensitive to disfluencies (Schlangen et al., 2009), a model that used Markov Logic Networks to resolve objects on a screen (Kennington and Schlangen, 2013), a model of RR and incremental feedback (Traum et al., 2012), and an approach that used a semantic representation to refer to objects (Peldszus et al., 2012; 273 Kennington et al., 2014). However, the approaches reported there did not incorporate multi-modal information, were too slow to work in real-time, were evaluated on constrained data, or only focused on a specific type of RR, ignoring pr"
N15-1031,W08-0113,1,0.80871,"lated Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static sc"
N15-1031,E09-1085,1,0.886272,"er making the RE, where I ranges over the possible alternatives (the objects in the domain). This recovery proceeds incrementally (word by word), for RE of arbitrary length. That is, if U denotes the current word, we are interested in P (I|U ), the current hypothesis about 1 It can be argued that any non-incremental model could be made into an incremental one by applying that model at each word (Khouzaimi et al., 2014), but we would argue that more modeling effort is required in order for the model to work in an interactive dialogue system, see (Schlangen and Skantze, 2009; Aist et al., 2007; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991). the intended referent, given the observed word. We assume the presence of an unobserved, latent variable R, which models properties of the candidate objects such as colour or shape; explained further below), and so the computation formally is: P (I|U ) = X P (I, U, R) r∈R P (U ) (1) Which, after making some independence assumptions, can be factored into: P (I|U ) = X 1 P (I) P (U |R)P (R|I) P (U ) (2) r∈R This is an update model in the usual sense that the posterior P (I|U ) at one step becomes the prior P (I) at the next. P (R|I) provides the link between the"
N15-1031,tokunaga-etal-2012-rex,1,0.782204,"ter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car"
N15-1031,J98-2001,0,\N,Missing
N15-1031,J00-2002,0,\N,Missing
P05-1031,J96-1002,0,0.00311287,"d” features, e.g. strings; we have run this learner both with only the features listed above and with the utterances (and POS-tags) as an additional feature. • T I MBL (Tilburg Memory-Based Learner), (Daelemans et al., 2003), which implements a memory-based learning algorithm (IB 1) which predicts the class of a test data point by looking at its distance to all examples from the training data, using some distance metric. In our experiments, we have used the weighted-overlap method, which assigns weights to all features. • M AX E NT, Zhang Le’s C++ implementation8 of maximum entropy modelling (Berger et al., 1996). In our experiments, we used L-BFGS parameter estimation. We also implemented a na¨ıve bayes classifier and ran it on the fragment-task, with a data-set consisting only of the strings and POS-tags. To determine the contribution of all features, we used an iterative process similar to the one described in (Kohavi and John, 1997; Strube and M¨uller, 2003): we start with training a model using a baseline set of features, and then add each remaining feature individually, recording the gain (w.r.t. the fmeasure (f (0.5), to be precise)), and choosing the best-performing feature, incrementally unti"
P05-1031,J96-2004,0,0.0427573,"our corpus, speech disfluencies.6 We now describe the creation of the data we used for training. We first describe the data-sets for the different tasks, and then the features used to represent the events that are to be classified. 3.2 Data Sets Data creation for the fragment-identification task (henceforth simply fragment-task) was straightforspeakers are discussed below. 4 We have used the MMAX tool (M¨uller and Strube, 2001)) for the annotation. 5 To test the reliability of the annotation scheme, we had a subset of the data annotated by two annotators and found a satisfactory κ-agreement (Carletta, 1996) of κ = 0.81. 6 The tagger is available free for academic research from http://www.ims.uni-stuttgart.de/projekte/ corplex/TreeTagger/DecisionTreeTagger.html. 249 ward: for each utterance, a number of features was derived automatically (see next section) and the correct class (fragment / other) was added. (Note that none of the manually annotated attributes were used.) This resulted in a file with 5,999 data points for classification. Given that there were 307 fragments, this means that in this data-set there is a ratio positives (fragments) vs. negatives (non-fragments) for the classifier of 1"
P05-1031,W02-0203,0,0.266929,"Missing"
P05-1031,C04-1035,0,0.349234,"Missing"
P05-1031,garofolo-etal-2004-nist,0,0.0137073,"they also allow the fragment classification decision to come from another source—a language-model used in an automatic speech recognition system, for example— and to use only the antecedent-classifier. The other approach is to do both at the same time, i.e. to classify pairs of utterances into those that combine a fragment and its antecedent and those that don’t. We report the results of our experiments with these tasks below, after describing the data we used. 3 Corpus, Features, and Data Creation 3.1 Corpus As material we have used six transcripts from the “NIST Meeting Room Pilot Corpus” (Garofolo et al., 2004), a corpus of recordings and transcriptions of multi-party meetings.3 Those six transcripts con2 The boundaries are fuzzy here, however, as backchannels can also be fragmental repetitions of previous material, and sometimes it is not clear how to classify a given utterance. A similar problem of classifying fragments is discussed in (Schlangen, 2003) and we will not go further into this here. 3 We have chosen a multi-party setting because we are ultimately interested in automatic summarisation of meetings. In this paper here, however, we view our task as a “stand-alone task”. Some of the proble"
P05-1031,P04-1019,0,0.236924,"s(x, y) hits(x, y) 1 + )∗ hits(x) hits(y) 2 We now describe the experiments we performed and their results. 4 Experiments and Results 4.1 Experimental Setup For the learning experiments, we used three classifiers on all data-sets for the the three tasks: • SLIPPER (Simple Learner with Iterative Pruning to Produce Error Reduction), (Cohen and Singer, 1999), which is a rule learner which combines the separate-and-conquer approach with confidencerated boosting. It is unique among the classifiers that 7 The name is short for google distance, which indicates its relatedness to the feature used by (Poesio et al., 2004); it is however a measure of similarity, not distance, as described above. we have used in that it can make use of “set-valued” features, e.g. strings; we have run this learner both with only the features listed above and with the utterances (and POS-tags) as an additional feature. • T I MBL (Tilburg Memory-Based Learner), (Daelemans et al., 2003), which implements a memory-based learning algorithm (IB 1) which predicts the class of a test data point by looking at its distance to all examples from the training data, using some distance metric. In our experiments, we have used the weighted-over"
P05-1031,W03-2106,1,0.846404,"ld be analysed as being the result of ellipsis: backchannels for example (like the “Right” in utterance 4 in (2) above) seem to directly fulfil their discourse function without any need for 248 reconstruction.2 To keep matters simple, we concentrate in this paper on NSUs of a certain kind, namely those that a) do not predominantly have a discourse-management function (like for example backchannels), but rather convey messages (i.e., propositions, questions or requests)—this is what distinguishes fragments from other NSUs—and b) have individual utterances as antecedents. In the terminology of (Schlangen and Lascarides, 2003), fragments of the latter type are resolution-via-identity-fragments, where the elided information can be identified in the context and need not be inferred (as opposed to resolution-viainference-fragments). Choosing only this special kind of NSUs poses the question whether this subgroup is distinguished from the general group of fragments by criteria that can be learnt; we will return to this below when we analyse the errors made by the classifier. We have defined two approaches to this task. One is to split the task into two sub-tasks: identifying fragments in a corpus, and identifying antec"
P05-1031,P03-1022,0,0.0747453,"Missing"
P12-3018,W11-2015,1,0.870434,"als, which in turn can prompt different utterance continuations, or starting an utterance before all information required in the utterance is available (“so, uhm, there are flights to Seoul on uh . . . ”), signaling that the turn is being held. Another, less conventional type of speech-based system that could profit from iSS is “babelfish-like” simultaneous speech-to-speech translation. Research on architectures, higher-level processing modules and lower-level processing modules that would enable such behaviour is currently underway (Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010; Baumann and Schlangen, 2011), but a synthesis component that would unlock the full potential of such strategies is so far missing. In this paper, we present such a component, which is capable of (a) starting to speak before utterance processing has finished; (b) handling edits made to (as-yet unspoken) parts of the utterance even while a prefix is already being spoken; (c) enabling adaptations of delivery parameters such as speaking rate or pitch; (d) autonomously making appropriate deliveryrelated decisions; (e) providing information about progress in delivery; and, last but not least, (f) running in real time. Our iSS"
P12-3018,W12-1814,1,0.927611,"k before utterance processing has finished; (b) handling edits made to (as-yet unspoken) parts of the utterance even while a prefix is already being spoken; (c) enabling adaptations of delivery parameters such as speaking rate or pitch; (d) autonomously making appropriate deliveryrelated decisions; (e) providing information about progress in delivery; and, last but not least, (f) running in real time. Our iSS component is built on top of an existing non-incremental synthesis component, MaryTTS (Schröder and Trouvain, 2003), and on an existing architecture for incremental processing, I NPROTK (Baumann and Schlangen, 2012). 103 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 103–108, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics After a discussion of related work (Section 2), we describe the basic elements of our iSS component (Section 3) and some demonstrator applications that we created which showcase certain abilities.1 2 Related Work Typically, in current SDSs utterances are generated (either by lookup/template-based generation, or, less commonly, by concept-to-utterance natural language generation (NLG)) and then syn"
P12-3018,E09-1081,1,0.611896,"Us marks whether the IU’s production is upcoming, ongoing, or completed. Listeners may subscribe to be notified about such progress changes using an update interface on IUs. The applications in Figures 2 and 4 make use of this interface to mark the words of the utterance in bold for completed, and in italic for ongoing words (incidentally, the screenshot in Figure 4 was taken exactly at the boundary between “delete” and “the”). 4.3 Low-Latency Switching of Alternatives A major goal of iSS is to change what is being said while the utterance is ongoing. Forward-pointing same-level links (SLLs, (Schlangen and Skantze, 2009; Baumann and Schlangen, 2012)) as shown in Figure 3 allow to construct alternative utterance paths beforehand. Deciding on the actual utterance continuation is a simple re-ranking of the forward 4.5 Figure 3: Incremental units chained together via forwardpointing same-level links to form an utterance tree. In a multi-threaded, real-time system, the crawling vocoder may reach the end of synthesis before the NLG component (in its own thread) has been able to add a continuation to the ongoing utterance. To avoid this case, special hesitation words can be inserted at the end of a yet unfinished u"
P12-3018,W10-4308,1,0.884927,"espond rather closely to Levelt’s (1989) model of human speech production. Levelt distinguishes several, partially independent processing modules (conceptualization, formulation, articulation, see Figure 1) that function incrementally and “in a highly automatic, reflex-like way” (Levelt, 1989, p. 2). 105 Technical Overview of Our System As a basis, we use MaryTTS (Schröder and Trouvain, 2003), but we replace Mary’s internal data structures with structures that support incremental specifications; these we take from an extant incremental spoken dialogue system architecture and toolkit, INPROTK (Schlangen et al., 2010; Baumann and Schlangen, 2012). In this architecture, incremental processing as the processing of incremental units (IUs), which are the smallest ‘chunks’ of information at a specific level (such as words, or phonemes, as can be seen in Figure 1). IUs are interconnected to form a network (e. g. words keep links to their associated phonemes, and vice-versa) which stores the system’s complete information state. The iSS component takes an IU sequence of chunks of words as input (from an NLG component). Crucially, this sequence can then still be modified, through: (a) continuations, which simply l"
P12-3018,W10-4301,0,0.363583,"hat prompt for backchannel signals, which in turn can prompt different utterance continuations, or starting an utterance before all information required in the utterance is available (“so, uhm, there are flights to Seoul on uh . . . ”), signaling that the turn is being held. Another, less conventional type of speech-based system that could profit from iSS is “babelfish-like” simultaneous speech-to-speech translation. Research on architectures, higher-level processing modules and lower-level processing modules that would enable such behaviour is currently underway (Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010; Baumann and Schlangen, 2011), but a synthesis component that would unlock the full potential of such strategies is so far missing. In this paper, we present such a component, which is capable of (a) starting to speak before utterance processing has finished; (b) handling edits made to (as-yet unspoken) parts of the utterance even while a prefix is already being spoken; (c) enabling adaptations of delivery parameters such as speaking rate or pitch; (d) autonomously making appropriate deliveryrelated decisions; (e) providing information about progress in delivery; and, last but not least, (f)"
P12-3018,E09-1085,1,0.528957,"installments (Clark, 1996) that prompt for backchannel signals, which in turn can prompt different utterance continuations, or starting an utterance before all information required in the utterance is available (“so, uhm, there are flights to Seoul on uh . . . ”), signaling that the turn is being held. Another, less conventional type of speech-based system that could profit from iSS is “babelfish-like” simultaneous speech-to-speech translation. Research on architectures, higher-level processing modules and lower-level processing modules that would enable such behaviour is currently underway (Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010; Baumann and Schlangen, 2011), but a synthesis component that would unlock the full potential of such strategies is so far missing. In this paper, we present such a component, which is capable of (a) starting to speak before utterance processing has finished; (b) handling edits made to (as-yet unspoken) parts of the utterance even while a prefix is already being spoken; (c) enabling adaptations of delivery parameters such as speaking rate or pitch; (d) autonomously making appropriate deliveryrelated decisions; (e) providing information about progress in delivery"
P15-1029,P12-1015,0,0.0146767,", target is row 3. formal semantics for similarly descriptive terms, where parts of the semantics are modelled by a perceptual classifier. These approaches had limited lexicons (where we attempt to model all words in our corpus), and do not process incrementally, which we do here. Recent efforts in multimodal distributional semantics have also looked at modelling word meaning based on visual context. Originally, vector space distributional semantics focused words in the context of other words (Turney and Pantel, 2010); recent multimodal approaches also consider low-level features from images. Bruni et al. (2012) and Bruni et al. (2014) for example model word meaning by word and visual context; each modality is represented by a vector, fused by concatenation. Socher et al. (2014) and Kiros et al. (2014) present approaches where words/phrases and images are mapped into the same high-dimensional space. While these approaches similarly provide a link between words and images, they are typically tailored towards a different setting (the words being descriptions of the whole image, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similaritie"
P15-1029,D13-1134,0,0.0343432,"world, returns I ∗ , the identifier of one the objects in the world that is the referent of the RE. A number of recent papers have used stochastic models for frr where, given W and U , a distribution over a specified set of candidate entities in W is obtained and the probability assigned to each entity represents the strength of belief that it is the referent. The referent is then the argmax: I ∗ = argmax P (I|U, W ) I (1) Recently, generative approaches, including our own, have been presented (Funakoshi et al., 2012; Kennington et al., 2013; Kennington et al., 2014; Kennington et al., 2015b; Engonopoulos et al., 2013) which model U as words or ngrams and the world W as a set of objects in a virtual game board, represented as a set properties or concepts (in some cases, extra-linguistic or discourse aspects were also modelled in W , such as deixis). In Matuszek et al. (2014), W was represented as a distribution over properties of tangible objects and U was a Combinatory Categorical Grammar parse. In all of these approaches, the objects are distinct and represented via symbolically specified properties, such as colour and shape. The set of properties is either read directly from the world if it is virtual, o"
P15-1029,W13-0109,0,0.0155202,"being descriptions of the whole image, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similarities and differences to future work and only note for now that our approach, relying on much simpler classifiers (log-linear, basically), works with much smaller data sets and additionally seem to provide an easier interface to more traditional ways of composition (see Section 3 above). The issue of semantic compositionality is also actively discussed in the distributional semantics literature (see, e.g., (Mitchell and Lapata, 2010; Erk, 2013; Lewis and Steedman, 2013; Paperno et al., 2014)), investigating how to combine vectors. This could be seen as composition on the level of intensions (if one sees distributional representations as intensions, as is variously hinted at, e.g. Erk (2013)). In our approach, composition is done on the extensional level (by interpolating distributions over candidate objects). We do not see our approach as being in opposition to these attempts. Rather, we envision a system of semantics that combines traditional symbolic expressions (on which inferences can be modelled via syntactic calculi) with dis"
P15-1029,W12-1633,0,0.391139,", July 26-31, 2015. 2015 Association for Computational Linguistics of the (relevant aspects of the) world, returns I ∗ , the identifier of one the objects in the world that is the referent of the RE. A number of recent papers have used stochastic models for frr where, given W and U , a distribution over a specified set of candidate entities in W is obtained and the probability assigned to each entity represents the strength of belief that it is the referent. The referent is then the argmax: I ∗ = argmax P (I|U, W ) I (1) Recently, generative approaches, including our own, have been presented (Funakoshi et al., 2012; Kennington et al., 2013; Kennington et al., 2014; Kennington et al., 2015b; Engonopoulos et al., 2013) which model U as words or ngrams and the world W as a set of objects in a virtual game board, represented as a set properties or concepts (in some cases, extra-linguistic or discourse aspects were also modelled in W , such as deixis). In Matuszek et al. (2014), W was represented as a distribution over properties of tangible objects and U was a Combinatory Categorical Grammar parse. In all of these approaches, the objects are distinct and represented via symbolically specified properties, su"
P15-1029,W13-4030,1,0.465421,"Association for Computational Linguistics of the (relevant aspects of the) world, returns I ∗ , the identifier of one the objects in the world that is the referent of the RE. A number of recent papers have used stochastic models for frr where, given W and U , a distribution over a specified set of candidate entities in W is obtained and the probability assigned to each entity represents the strength of belief that it is the referent. The referent is then the argmax: I ∗ = argmax P (I|U, W ) I (1) Recently, generative approaches, including our own, have been presented (Funakoshi et al., 2012; Kennington et al., 2013; Kennington et al., 2014; Kennington et al., 2015b; Engonopoulos et al., 2013) which model U as words or ngrams and the world W as a set of objects in a virtual game board, represented as a set properties or concepts (in some cases, extra-linguistic or discourse aspects were also modelled in W , such as deixis). In Matuszek et al. (2014), W was represented as a distribution over properties of tangible objects and U was a Combinatory Categorical Grammar parse. In all of these approaches, the objects are distinct and represented via symbolically specified properties, such as colour and shape. T"
P15-1029,C14-1170,1,0.83417,"ional Linguistics of the (relevant aspects of the) world, returns I ∗ , the identifier of one the objects in the world that is the referent of the RE. A number of recent papers have used stochastic models for frr where, given W and U , a distribution over a specified set of candidate entities in W is obtained and the probability assigned to each entity represents the strength of belief that it is the referent. The referent is then the argmax: I ∗ = argmax P (I|U, W ) I (1) Recently, generative approaches, including our own, have been presented (Funakoshi et al., 2012; Kennington et al., 2013; Kennington et al., 2014; Kennington et al., 2015b; Engonopoulos et al., 2013) which model U as words or ngrams and the world W as a set of objects in a virtual game board, represented as a set properties or concepts (in some cases, extra-linguistic or discourse aspects were also modelled in W , such as deixis). In Matuszek et al. (2014), W was represented as a distribution over properties of tangible objects and U was a Combinatory Categorical Grammar parse. In all of these approaches, the objects are distinct and represented via symbolically specified properties, such as colour and shape. The set of properties is e"
P15-1029,W15-0124,1,0.924061,"gical semantics (Montague, 1973; Gamut, 1991; Partee et al., 1993) has little to say about this process – its focus is on the construction of syntactically manipulable objects that model inferential relations; here, e.g. the inference that there are (at least) two objects. Vector space approaches to distributional semantics (Turney and Pantel, 2010) similarly focuses on something else, namely Background: Reference Resolution Reference resolution (RR) is the task of resolving referring expressions (REs; as in Example (1)) to a referent, the entity to which they are intended to refer. Following Kennington et al. (2015a), this can be formalised as a function frr that, given a representation U of the RE and a representation W 1 But see discussion below of recent extensions of these approaches taking this into account. 292 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 292–301, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics of the (relevant aspects of the) world, returns I ∗ , the identifier of one the objects in the world that is the referent of the RE."
P15-1029,N15-1031,1,0.928124,"gical semantics (Montague, 1973; Gamut, 1991; Partee et al., 1993) has little to say about this process – its focus is on the construction of syntactically manipulable objects that model inferential relations; here, e.g. the inference that there are (at least) two objects. Vector space approaches to distributional semantics (Turney and Pantel, 2010) similarly focuses on something else, namely Background: Reference Resolution Reference resolution (RR) is the task of resolving referring expressions (REs; as in Example (1)) to a referent, the entity to which they are intended to refer. Following Kennington et al. (2015a), this can be formalised as a function frr that, given a representation U of the RE and a representation W 1 But see discussion below of recent extensions of these approaches taking this into account. 292 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 292–301, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics of the (relevant aspects of the) world, returns I ∗ , the identifier of one the objects in the world that is the referent of the RE."
P15-1029,W02-0109,0,0.0248714,"for evaluations labelled RD the training data always includes all of DD plus 9 folds of RD , testing on RD . The sets address the following questions: • how well does the sr model work on its own with just words? – DD . WO • how well does the sr model work when it knows about REs? – DD . ST • how well does the sr model work when it knows about REs, but not about relations? – RD . ST (sr) • how well does the model learn relation words after it has learned about sr? RD . ST (r) • how well does the rr model work (together with the sr)? RD . ST with DD . ST (rr) Words were stemmed using the NLTK (Loper and Bird, 2002) Snowball Stemmer, reducing the 296 vocabulary size to 1306. Due to sparsity, for relation words with a token count of less than 4 (found by ranging over values in a held-out set) relational features were piped into an UNK relation, which was used for unseen relations during evaluation (we assume the UNK relation would learn a general notion of ‘nearness’). For the individual word classifiers, we always paired one negative example with one positive example. For this evaluation, word classifiers for sr were given the following features: RGB values, HSV values, x and y coordinates of the centroi"
P15-1029,P14-1009,0,0.0309006,"e, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similarities and differences to future work and only note for now that our approach, relying on much simpler classifiers (log-linear, basically), works with much smaller data sets and additionally seem to provide an easier interface to more traditional ways of composition (see Section 3 above). The issue of semantic compositionality is also actively discussed in the distributional semantics literature (see, e.g., (Mitchell and Lapata, 2010; Erk, 2013; Lewis and Steedman, 2013; Paperno et al., 2014)), investigating how to combine vectors. This could be seen as composition on the level of intensions (if one sees distributional representations as intensions, as is variously hinted at, e.g. Erk (2013)). In our approach, composition is done on the extensional level (by interpolating distributions over candidate objects). We do not see our approach as being in opposition to these attempts. Rather, we envision a system of semantics that combines traditional symbolic expressions (on which inferences can be modelled via syntactic calculi) with distributed representations (which model conceptual"
P15-1029,Q13-1015,0,\N,Missing
P15-1029,P06-4018,0,\N,Missing
P15-1029,Q14-1017,0,\N,Missing
P16-1058,J95-3003,0,0.0639703,"., 2012). The connection between reference in installments on the one and the status of distractors and distinguishing expressions on the other hand is relatively unexplored, though it seems natural to combine the two perspectives (DeVault et al., 2005). Figure 1 shows an example for very a simple but highly effective expression - it mentions color as a salient and distinguishing property while avoiding a potentially unclear object name. Task-oriented REG has looked at reference as a collaborative process where a speaker and a listener try to reach a common goal (Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1995; DeVault et al., 2005). Given the real-time constraints of situated interaction, a speaker often has to start uttering before she has found the optimal expression, but at the same time, she can tailor, extend, adapt, revise or correct her referring expressions in case the listener signals that he did not understand. Thus, human speakers can flexAs our second contribution, we extend our probabilistic word selection model to work in a simple interactive installment component that tries to avoid semantically inadequate words as much as possible and only expands the expression in case of misunder"
P16-1058,D14-1086,0,0.0919433,"semantically appropriate expressions. In a human evaluation, we observe that users are sensitive to inadequate object names - which unfortunately are not unlikely to be generated from low-level visual input. We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words. We enhance a word-based REG with contextaware, referential installments and find that they substantially improve the referential success of the system. 1 girl in front anywhere brown Figure 1: Example images and REs from the ReferIt corpus (Kazemzadeh et al., 2014) other objects in the scene but does not overload the listener with unnecessary information. Figure 1 illustrates this with two examples from a corpus of REs collected from human subjects for objects in images (Kazemzadeh et al., 2014). Research on referring expression generation (REG) has mostly focussed on (ii), modeling pragmatic adequacy in attribute selection tasks, using as input a fully specified, symbolic representation of the visual attributes of an object and its distractors in a scene (Dale and Reiter, 1995; Krahmer and Van Deemter, 2012). In this paper, we follow a more recent tren"
P16-1058,P06-1131,0,0.0389732,"as much as possible and only expands the expression in case of misunderstanding. We present an algorithm that generates these installments depending on the context, based on ideas from traditional REG algo611 2.3 ibly split and adapt their REs over several utterances during an interaction, a phenomenon called “reference in installments”. In a corpus analysis of the S-GIVE domain, (Striegnitz et al., 2012) showed that installments are pervasive in humanhuman interaction in a task-oriented environment. However, while there has been research on goaloriented and situated REG (Stoia et al., 2006; Kelleher and Kruijff, 2006; Striegnitz et al., 2011; Garoufi and Koller, 2013), installments have been rarely implemented and empirically tested in interactive systems. A noticeable exception is the work by Fang et al. (2014) who use reinforcement learning to induce an installment strategy that is targeted at robots that have uncertain knowledge about the objects in their environment. Using relatively simple computer-generated scenes and a standard representations of objects as sets of attributes, they learn a strategy that first guides the user to objects that the system can recognize with high confidence. Our work is"
P16-1058,P15-1029,1,0.785423,"rties of the target referent, and (ii) is pragmatically and contextually appropriate, i.e. distinguishes the target from 610 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 610–620, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics rithms like (Dale and Reiter, 1995). We find that a context-aware installment strategy greatly improves referential success as it helps to avoid and repair misunderstandings and offers a combined treatment of semantic and pragmatic adequacy. the low-level visual features of an object (Kennington and Schlangen, 2015). As our first contribution, we train this model on the ReferIt corpus (Kazemzadeh et al., 2014) and define decoding mechanisms tailored to REG. Large-scale recognition of objects and their attributes in images is still a non-trivial task. Consequently, REG systems now face the challenge of dealing with semantically inadequate expressions. For instance, in Figure 1, the system might not precisely distinguish between man or woman and generate an inadequate, confusing RE like man in the middle. Therefore, we focus on evaluating our system in an object identification task with users, in contrast"
P16-1058,P05-3001,0,0.0997705,"partners try to minimize their joint effort and often prefer to present simple expressions that can be expanded on or repaired, if necessary (Clark and Wilkes-Gibbs, 1986). This strategy, called “referring in installments” is very effective for achieving common ground in taskoriented interaction (Fang et al., 2014) and is attested in dialogue data (Striegnitz et al., 2012). The connection between reference in installments on the one and the status of distractors and distinguishing expressions on the other hand is relatively unexplored, though it seems natural to combine the two perspectives (DeVault et al., 2005). Figure 1 shows an example for very a simple but highly effective expression - it mentions color as a salient and distinguishing property while avoiding a potentially unclear object name. Task-oriented REG has looked at reference as a collaborative process where a speaker and a listener try to reach a common goal (Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1995; DeVault et al., 2005). Given the real-time constraints of situated interaction, a speaker often has to start uttering before she has found the optimal expression, but at the same time, she can tailor, extend, adapt, revise or cor"
P16-1058,P15-2017,0,0.0537773,"Van Deemter, 2012). In this paper, we follow a more recent trend (Kazemzadeh et al., 2014; Gkatzia et al., 2015) and investigate REG on real-world images. In this setting, a low-level visual representation of an image (a scene) segmented into regions (objects), including the region of the target referent, constitutes the input. This task is closely related to the recently very active field of image-to-text generation, where deep learning approaches have been used to directly map low-level visual input to natural language sentences, e.g. (Vinyals et al., 2015; Chen and Lawrence Zitnick, 2015; Devlin et al., 2015). Similarly, we propose to cast REG on images as a word selection task. Thus, we base this work on a model of perceptually grounded word meaning, which associates words with classifiers that predict their semantic appropriateness given Introduction A speaker who wants to refer to an object in a visual scene will try to produce a referring expression (RE) that (i) is semantically adequate, i.e. accurately describes the visual properties of the target referent, and (ii) is pragmatically and contextually appropriate, i.e. distinguishes the target from 610 Proceedings of the 54th Annual Meeting of"
P16-1058,koolen-krahmer-2010-tuna,0,0.0245709,"e object types and, consequently, about potential distractors of the target. This does not apply to REG on real-world images which, as we will show in this paper, triggers some new challenges and research questions for this field. Subsequent work has shown that human speakers do not necessarily produce minimally distinguishing expressions (van Deemter et al., 2006; Viethen and Dale, 2008; Koolen et al., 2011), and has tried to account for the wide range of factors - such as different speakers, modalities, object categories - that are related to attribute selection, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013; Tarenskeen et al., 2015). Research on reference in human interaction has noticed that conversation partners try to minimize their joint effort and often prefer to present simple expressions that can be expanded on or repaired, if necessary (Clark and Wilkes-Gibbs, 1986). This strategy, called “referring in installments” is very effective for achieving common ground in taskoriented interaction (Fang et al., 2014) and is attested in dialogue data (Striegnitz et al., 2012). The connection between reference in installments on the one and the status of distractors and disting"
P16-1058,J12-1006,0,0.180256,"Missing"
P16-1058,W12-1621,0,0.0167311,"favourably to the non-dynamic version of our system (see Section 3). This question is, however, closely linked to another, more intricate question: what is the best strategy to realize installments that, on the one hand, provide enough information so that a user can eventually identify the referent and, on the other hand, avoid misleading words? To date, even highly interactive systems do not generally treat installments, or if they do, only realise them via templates, e.g. (Stoia et al., 2006; Staudte et al., 2012; Garoufi and Koller, 2013; Dethlefs and Cuay´ahuitl, 2015). As pointed out by Liu et al. (2012), data-driven approaches are not straightforward to set-up, due to the “mismatched perceptual basis” between a human listener and an REG system. Based on the insights of our error analysis in Section 3.4, we will rely on a general installment strategy that is mostly targeted at avoiding semantically inadequate object names, and emphasizing the fact that location words generated by the system convey more reliable information. We have implemented two versions of this general strategy: (i) pattern-based installments that always avoid object names in their initial expression and dynamically extend"
P16-1058,W10-4210,0,0.498036,"Missing"
P16-1058,P16-1115,1,0.792542,"egative instances are randomly samples from the complementary set of utterances (e.g. not containing red). We used this relatively simple model in our work, because first of all we wanted to test wether it scales from a controlled domain of typical reference game scenes (Kennington and Schlangen, 2015) to real-world images. Second, as compared to standard object recognisers that predict abstract image labels annotated in e.g. ImageNet (Deng et al., 2009), this model directly captures the relation between actual words used in REs and visual properties of the corresponding referents. Following (Schlangen et al., 2016), we can easily base our classifiers on such a high-performance convolutional neural network (Szegedy et al., 2015), by applying it on our images and extracting the final fully-connected layer before the classification layer (see Section 3.1). The ReferIt corpus We train and evaluate our system on the ReferIt data set collected by Kazemzadeh et al. (2014). The basis of the corpus is a collection of “20,000 still natural images taken from locations around the world” (Grubinger et al., 2006), which was augmented by Escalante et al. (2010) with segmentation masks identifying objects in the images"
P16-1058,W06-1412,0,0.230702,"lly inadequate words as much as possible and only expands the expression in case of misunderstanding. We present an algorithm that generates these installments depending on the context, based on ideas from traditional REG algo611 2.3 ibly split and adapt their REs over several utterances during an interaction, a phenomenon called “reference in installments”. In a corpus analysis of the S-GIVE domain, (Striegnitz et al., 2012) showed that installments are pervasive in humanhuman interaction in a task-oriented environment. However, while there has been research on goaloriented and situated REG (Stoia et al., 2006; Kelleher and Kruijff, 2006; Striegnitz et al., 2011; Garoufi and Koller, 2013), installments have been rarely implemented and empirically tested in interactive systems. A noticeable exception is the work by Fang et al. (2014) who use reinforcement learning to induce an installment strategy that is targeted at robots that have uncertain knowledge about the objects in their environment. Using relatively simple computer-generated scenes and a standard representations of objects as sets of attributes, they learn a strategy that first guides the user to objects that the system can recognize with"
P16-1058,W11-2845,0,0.0924581,"y expands the expression in case of misunderstanding. We present an algorithm that generates these installments depending on the context, based on ideas from traditional REG algo611 2.3 ibly split and adapt their REs over several utterances during an interaction, a phenomenon called “reference in installments”. In a corpus analysis of the S-GIVE domain, (Striegnitz et al., 2012) showed that installments are pervasive in humanhuman interaction in a task-oriented environment. However, while there has been research on goaloriented and situated REG (Stoia et al., 2006; Kelleher and Kruijff, 2006; Striegnitz et al., 2011; Garoufi and Koller, 2013), installments have been rarely implemented and empirically tested in interactive systems. A noticeable exception is the work by Fang et al. (2014) who use reinforcement learning to induce an installment strategy that is targeted at robots that have uncertain knowledge about the objects in their environment. Using relatively simple computer-generated scenes and a standard representations of objects as sets of attributes, they learn a strategy that first guides the user to objects that the system can recognize with high confidence. Our work is targeted at more complex"
P16-1058,W12-1504,0,0.786605,", modalities, object categories - that are related to attribute selection, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013; Tarenskeen et al., 2015). Research on reference in human interaction has noticed that conversation partners try to minimize their joint effort and often prefer to present simple expressions that can be expanded on or repaired, if necessary (Clark and Wilkes-Gibbs, 1986). This strategy, called “referring in installments” is very effective for achieving common ground in taskoriented interaction (Fang et al., 2014) and is attested in dialogue data (Striegnitz et al., 2012). The connection between reference in installments on the one and the status of distractors and distinguishing expressions on the other hand is relatively unexplored, though it seems natural to combine the two perspectives (DeVault et al., 2005). Figure 1 shows an example for very a simple but highly effective expression - it mentions color as a salient and distinguishing property while avoiding a potentially unclear object name. Task-oriented REG has looked at reference as a collaborative process where a speaker and a listener try to reach a common goal (Clark and Wilkes-Gibbs, 1986; Heeman a"
P16-1058,W06-1420,0,0.0544983,"Missing"
P16-1058,W08-1109,0,0.0281354,"r object from D, if the target and distractor have different values. This is mostly based on the assumption that we have objects of particular types (e.g. people, furniture, etc.) and that the system has perfect knowledge about these object types and, consequently, about potential distractors of the target. This does not apply to REG on real-world images which, as we will show in this paper, triggers some new challenges and research questions for this field. Subsequent work has shown that human speakers do not necessarily produce minimally distinguishing expressions (van Deemter et al., 2006; Viethen and Dale, 2008; Koolen et al., 2011), and has tried to account for the wide range of factors - such as different speakers, modalities, object categories - that are related to attribute selection, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013; Tarenskeen et al., 2015). Research on reference in human interaction has noticed that conversation partners try to minimize their joint effort and often prefer to present simple expressions that can be expanded on or repaired, if necessary (Clark and Wilkes-Gibbs, 1986). This strategy, called “referring in installments” is very effective for"
P16-1058,W09-0629,0,\N,Missing
P16-1058,D15-1224,0,\N,Missing
P16-1115,H89-2010,0,0.072733,"Missing"
P16-1115,D14-1086,0,0.146161,"r. For uniquely referring expressions (“the red cross”), what is required is to pick the most likely candidate from the distribution: [[the]] = λx. arg max x (5) … The “Words-As-Classifiers” Model … 3 Figure 2: Image 27437 from IAPR TC -12 (left), with region masks from SAIAPR TC -12 (middle); “brown shirt guy on right” is a referring expression in R EFER I T G AME for the region singled out on the right done manually and provide close maskings of the objects. This extended dataset is also known as “SAIAPR TC -12” (for “segmented and annotated IAPR TC -12”). The third component is provided by Kazemzadeh et al. (2014), who collected a large number of expressions referring to (presegmented) objects from these images, using a crowd-sourcing approach where two players were paired and a director needed to refer to a predetermined object to a matcher, who then selected it. (An example is given in Figure 2 (right).) This corpus contains 120k referring expressions, covering nearly all of the 99.5k regions from SAIAPR TC -12.3 The average length of a referring expression from this corpus is 3.4 tokens. The 500k token realise 10,340 types, with 5785 hapax legomena. The most frequent tokens (other than articles and"
P16-1115,P15-1029,1,0.878484,"lassifiers Model David Schlangen Sina Zarrieß Casey Kennington Dialogue Systems Group // CITEC // Faculty of Linguistics and Literary Studies Bielefeld University, Germany first.last@uni-bielefeld.de Abstract HRI work (&gt; 300, see below). More formally, the task is to retrieve, given a referring expression e and an image I, the region bb∗ of the image that is most likely to contain the referent of the expression. As candidate regions, we use both manually annotated regions as well as automatically computed ones. As our starting point, we use the “words-asclassifiers” model recently proposed by Kennington and Schlangen (2015). It has before only been tested in a small domain and with specially designed features; here, we apply it to real-world photographs and use learned representations from a convolutional neural network (Szegedy et al., 2015). We learn models for between 400 and 1,200 words, depending on the training data set. As we show, the model performs competitive with the state of the art (Hu et al., 2016; Mao et al., 2016) on the same data sets. Our background interest in situated interaction makes it important for us that the approach we use is ‘dialogue ready’; and it is, in the sense that it supports i"
P16-1115,Q13-1016,0,0.0616644,"nting these with positional information, we show that the model achieves performance competitive with the state of the art in a reference resolution task (given expression, find bounding box of its referent), while, as we argue, being conceptually simpler and more flexible. 1 Introduction A common use of language is to refer to objects in the shared environment of speaker and addressee. Being able to simulate this is of particular importance for verbal human/robot interfaces (HRI), and the task has consequently received some attention in this field (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Here, we study a somewhat simpler precursor task, namely that of resolution of reference to objects in static images (photographs), but use a larger set of object types than is usually done in 2 Related Work The idea of connecting words to what they denote in the real world via perceptual features goes back at least to Harnad (1990), who coined “The Symbol Grounding Problem”: “[H]ow can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads?” The pro1 The code for reproducing the results reported in this"
P16-1115,Q14-1017,0,0.0439729,"uently taken approach is to use a convolutional neural network (CNN) to map the image to a dense vector (which we do as well, as we will describe below), and then condition a neural language model (typically, an LSTM) on this to produce an output string (Vinyals et al., 2015; Devlin et al., 2015). Fang et al. (2015) modify this approach somewhat, by using what they call “word detectors” first to specifically propose words for image regions, out of which the caption is then generated. This has some similarity to our word models as described below, but again is tailored more towards generation. Socher et al. (2014) present a more compositional variant of this type of approach where sentence representations are composed along the dependency parse of the sentence. The representation of the root node is then mapped into a multimodal space in which distance between sentence and image representation can be used to guide image retrieval, which is the task in that paper. Our approach, in contrast, composes on the level of denotations and not that of representation. Two very recent papers carry this type of approach over to the problem of resolving references to objects in images. Both (Hu et al., 2015) and (Ma"
P16-1115,P16-1058,1,0.79994,"pler domain; for our domain, new and more richly annotated data such as VISUALgenome looks promising for learning a wide variety of relations.9 The use of denotations / extensions might make possible transfer of methods from extensional semantics, e.g. for the addition of operators such as negation or generalised quantifiers. The design of the model, as mentioned in the introduction, makes it amenable for use in interactive systems that learn; we are currently exploring this avenue. Lastly, the word/object classifiers also show promise in the reverse task, generation of referring expressions (Zarrieß and Schlangen, 2016). All this is future work. In its current state— besides, we believe, strongly motivating this future work—, we hope that the model can also serve as a strong baseline to other future approaches to reference resolution, as it is conceptually simple and easy to implement. Acknowledgments We thank Hu et al. (2016), Mao et al. (2016) and Tamara Berg for giving us access to their data. Thanks are also due to the anonymous reviewers for their very insightful comments. We acknowledge support by the Cluster of Excellence “Cognitive Interaction Technology” (CITEC; EXC 277) at Bielefeld University, whi"
P16-1115,P15-2017,0,\N,Missing
P17-1023,P14-1023,0,0.0467697,"age and Word Embeddings Following Schlangen et al. (2016), we derive representations of our visual inputs with a convolutional neural network, ‘GoogleNet’ (Szegedy et al., 2015), which was trained on the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We add 7 features that encode information about the region relative to the image, thus representing each object as a vector of 1031 features. As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions). 4 4.1 Lexical Mapping Through Individual Word Classifiers Three Models of Interfacing Visual and Distributional Information Direct Cross-Modal Mapping Following Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space. Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individ"
P17-1023,D15-1003,0,0.0379694,"uman users under natural, interactive conditions (Kazemzadeh et al., 2014), and train and test on the corresponding head nouns in these REs. This is similar to picture naming setups used in psycholinguistic research (cf. Levelt et al. (1991)) and based on the simplifying assumption that the name used for referring to an object can be determined successfully without looking at other objects in the image. We now summarise the details of our setup: Cross-modal transfer Rather than fusing different modalities into a single, joint space, other work has looked at cross-modal mapping between spaces. Herbelot and Vecchi (2015) present a model that learns to map vectors in a distributional space to vectors in a set-theoretic space, showing that there is a functional relationship between distributional information and conceptual knowledge representing quantifiers and predicates. More related to our work are cross-modal mapping models,that learn to transfer from a representation of an object or image in the visual space to a vector in a distributional space (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014). Here, the motivation is to exploit the rich lexical knowledge encoded in a"
P17-1023,D14-1086,0,0.599207,"et al. (2002) use computer vision techniques to process a video feed, and to compute colour, positional and spatial features. These features are then associated in a learning process with certain words, resulting in an association of colour features with colour words, spatial features with prepositions, etc., and based on this, these words can be interpreted with reference to the scene currently presented to the video feed. Whereas Roy’s work still looked at relatively simple scenes with graphical objects, research on REG has recently started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015). Importantly, the lowlevel visual features that can be extracted from these scenes correspond less directly to particular word classes. Moreover, the visual scenes contain many different types of objects, which poses new challenges for REG. For instance, Zarrieß and Schlangen (2016) find that semantic errors related to mismatches between nouns (e.g. the system generates tree vs. man) are particularly disturbing for users. Whereas Zarrieß and Schlangen (2016) propose a strategy to avoid object names when the systems confiden"
P17-1023,D14-1005,0,0.0199454,"o actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multimodal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. 3 Task and Data We define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.) To get at this, we develop our approach using a corpus of referring expression"
P17-1023,N10-1011,0,0.0874076,"and lexical aspects of referential word meaning. to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multimodal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. 3 Task and Data We define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.) To get at this, we develop"
P17-1023,J12-1006,0,0.0539971,"Missing"
P17-1023,P14-1132,0,0.465117,"oint space, other work has looked at cross-modal mapping between spaces. Herbelot and Vecchi (2015) present a model that learns to map vectors in a distributional space to vectors in a set-theoretic space, showing that there is a functional relationship between distributional information and conceptual knowledge representing quantifiers and predicates. More related to our work are cross-modal mapping models,that learn to transfer from a representation of an object or image in the visual space to a vector in a distributional space (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014). Here, the motivation is to exploit the rich lexical knowledge encoded in a distributional space for learning visual classifications. In practice, these models are mostly used for zeroshot learning where the test set contains object categories not observed during training. When tested on standard object recognition tasks, transfer, however, comes at a price. Frome et al. (2013) and Norouzi et al. (2013) both find that it slightly degrades performance as compared to a plain object classification using standard accuracy metrics (called flat “hit @k metric” in their paper). Interestingly though,"
P17-1023,P15-1027,0,0.242826,"predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Zarrieß and Schlangen, 2017) that treats words as individual 244 rors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning. to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-know"
P17-1023,P16-1115,1,0.622953,"milarity. Indeed, we have found in a recent study that the contribution of distributional information to learning referential word meanings is restricted to certain types of words and does not generalize across the vocabulary (Zarrieß and Schlangen, 2017). The goal of this work is to learn a model of referential word meaning that makes accurate object naming predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Zarrieß and Schlangen, 2017) that treats words as individual 244 rors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning. to actual"
P17-1023,N15-1016,0,0.354013,"predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Zarrieß and Schlangen, 2017) that treats words as individual 244 rors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning. to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-know"
P17-1023,P14-1068,0,0.0447118,"referential word meaning. to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multimodal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. 3 Task and Data We define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.) To get at this, we develop our approach using a corpus"
P17-1023,P16-1058,1,0.915408,"process a video feed, and to compute colour, positional and spatial features. These features are then associated in a learning process with certain words, resulting in an association of colour features with colour words, spatial features with prepositions, etc., and based on this, these words can be interpreted with reference to the scene currently presented to the video feed. Whereas Roy’s work still looked at relatively simple scenes with graphical objects, research on REG has recently started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015). Importantly, the lowlevel visual features that can be extracted from these scenes correspond less directly to particular word classes. Moreover, the visual scenes contain many different types of objects, which poses new challenges for REG. For instance, Zarrieß and Schlangen (2016) find that semantic errors related to mismatches between nouns (e.g. the system generates tree vs. man) are particularly disturbing for users. Whereas Zarrieß and Schlangen (2016) propose a strategy to avoid object names when the systems confidence is low, we focus on improving the generation of"
P17-1023,E17-2014,1,0.686247,"can be mapped to. However, distributional representations of word meaning are known to capture a rather fuzzy notion of lexical similarity, e.g. car is similar to van and to street. A cross-modal transfer model is “forced” to learn to map objects into the same area in the semantic space if their names are distributionally similar, but regardless of their actual visual similarity. Indeed, we have found in a recent study that the contribution of distributional information to learning referential word meanings is restricted to certain types of words and does not generalize across the vocabulary (Zarrieß and Schlangen, 2017). The goal of this work is to learn a model of referential word meaning that makes accurate object naming predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Zarrieß and Schlangen, 2017) that treats words as individual 244 rors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it see"
P17-1023,D15-1224,0,\N,Missing
P19-1063,J12-1006,0,0.060858,"Missing"
P19-1063,D17-1098,0,0.0266983,"re, we show that this strategy often improves reference resolution accuracies of an automatic listener. 2 build a pragmatic speaker that produces more discriminative captions, applying equation 2 at each step of the inference process. They evaluate their model in a reference game where an automatic listener (trained on a different portion of the image data) is used to test whether the generated caption singles out the target image among a range of distractor images. A range of related articles have extended neural captioning models with decoding procedures geared towards vocabulary expansion (Anderson et al., 2017; Agrawal et al., 2018) or contextually discriminative scene descriptions (Andreas and Klein, 2016; Vedantam et al., 2017). Previous work on REG commonly looks at visual scenes with multiple referents of identical or similar categories. Here, speakers typically produce expressions composed of a head noun, which names the category of the target, and a set of attributes, which distinguish the target from distractor referents of the same category (Krahmer and Van Deemter, 2012). Our work adds an additional dimension of uncertainty to this picture, namely a setting where the category of the target"
P19-1063,D16-1125,0,0.0954734,"stener. 2 build a pragmatic speaker that produces more discriminative captions, applying equation 2 at each step of the inference process. They evaluate their model in a reference game where an automatic listener (trained on a different portion of the image data) is used to test whether the generated caption singles out the target image among a range of distractor images. A range of related articles have extended neural captioning models with decoding procedures geared towards vocabulary expansion (Anderson et al., 2017; Agrawal et al., 2018) or contextually discriminative scene descriptions (Andreas and Klein, 2016; Vedantam et al., 2017). Previous work on REG commonly looks at visual scenes with multiple referents of identical or similar categories. Here, speakers typically produce expressions composed of a head noun, which names the category of the target, and a set of attributes, which distinguish the target from distractor referents of the same category (Krahmer and Van Deemter, 2012). Our work adds an additional dimension of uncertainty to this picture, namely a setting where the category of the target itself might not be known to the model and, hence, cannot be named with reasonable accuracy. In t"
P19-1063,N18-2070,0,0.2948,"eaker. To do that, it needs to calculate P (u|r), as in Equation 1. While previous work on RSA typically equates P (u|r) with S0 (u|r), we are going to modify the way this probIn turn, the “pragmatic speaker” S1 reasons about which utterance is more discriminative and will be resolved to the target by the pragmatic listener: L0 (r|u) ∗ P (u) ui ∈U L0 (r|ui ) ∗ P (ui ) S1 (u|r) ∝ P Model (2) (S0 and L0 are components of the recursive reasoning of S1 and not in fact separate agents.) There has been some previous work on leveraging RSA-like reasoning for neural language generation. For instance, Cohn-Gordon et al. (2018) implement the literal speaker as a neural captioning model trained on non-discriminative image descriptions. On top of this neural semantics, they 655 ability is calculated. Thus, we assume that our listener has hidden beliefs about the category of the referent, that we can marginalize over as follows: P (u|r) = X P (u, ci |r) = ci ∈C category-specific words and resort to describing other visual properties like colour or location.1 Similar to Cohn-Gordon et al. (2018), we use incremental, word-level inference to decode the pragmatic speaker model in a greedy fashion: X P (u, ci , r) P (r) ci"
P19-1063,W18-6563,1,0.835379,"of “knowing” which words risk being inaccurate for referring to novel objects. The following Section 3 describes how we modify the RSA approach for reasoning in such a zero-shot reference game. Background We investigate referring expression generation (REG henceforth), where the goal is to compute an utterance u that identifies a target referent r among other referents R in a visual scene. Research on REG has a long tradition in natural language generation (Krahmer and Van Deemter, 2012), and has recently been re-discovered in the area of Language & Vision (Mao et al., 2016; Yu et al., 2016; Zarrieß and Schlangen, 2018). These latter models for REG essentially implement variants of a standard neural image captioning architecture (Vinyals et al., 2015), combining a CNN and an LSTM to generate an utterance directly from objects marked via bounding boxes in real-world images. Our approach combines such a neural REG model with a reasoning component that is inspired by theory-driven Bayesian pragmatics and RSA (Frank and Goodman, 2012). We will briefly sketch this approach here. The starting point in RSA is a model of a “literal speaker”, S0 (u|r), which generates utterances u for the target r. The “pragmatic lis"
W03-2106,copestake-flickinger-2000-open,0,0.039583,"TOP 5 Finally, the last dimension organises the differences in the type of message to which the fragment will resolve. The example we have seen in Figure 1 was one of a propositional-fragment; fragmental questions or requests only differ in the type of this message-relation. To give an example, (8) shows the type int(errogative)-frag(ment). "" # (8) int-frag   C - CONT. LZT h int , . . . i The rules in this dimension also make sure that whphrases must be int-frags. 4.2 Implementation We have implemented our analysis in a widecoverage HPSG, the English Resource Grammar (ERG, see for example (Copestake and Flickinger, 2000));12 the implementation was evaluated using the grammar-profiling tool [incr tsdb()] (Oepen and Flickinger, 1998). First, to test for possible adverse effects on the analyses of fullsentences, we ran a batch-parse of a test-suite of full sentences, the CSLI-test-suite which is distributed with [incr tsdb()]. It consists of 1348 sentences, of which 961 are marked as syntactically well-formed and 387 as ill-formed. Table 4 shows a comparison of the original ERG with our extended version containing the fragment rules, with respect to the average number of parses per sentence. 12 The implementatio"
W03-2106,P01-1019,1,0.837522,"which (b) the main predicate is unknown, but (c) one participant in the main event is specified although its exact role isn’t. We represent this with an anaphoric relation unknown rel, and so the NPfragment “John.” (regardless of the context it stands in) is represented as: (4) hh, e, {h : prpstn rel(h1 ), h2 : unknown rel(e, x), h6 : def np rel(x, h8 , h9 ), h10 : named(x, John)}, { h1 =q h2 , h8 =q h10 } i The unknown rel acts as a ‘place-holder’ for a potentially complex sub-formula; more precisely it is a constraint on the form of the described (base8 Such a semantics if given to MRS in (Copestake et al., 2001); we follow the similar formalisation in (Asher and Lascarides, 2003). Note that we do not make any assumptions about the base language and its logic here; the descriptions are compatible with it being static first order predicate logic, or a dynamic logic like DRT (Kamp and Reyle, 1993). 9 We make the simplifying assumption that there is an unambiguous intonation pattern indicating whether a fragment is intended to convey a proposition, a question or a request. language) formulae, that they contain at this place a subformula, which in the case of (4) must have e and x amongst its variables. C"
W03-2106,W02-0203,0,0.138417,"Missing"
W03-2106,W03-2106,1,0.0528322,"to the party.” (1) A: Who came to the party? B: John. Clearly, the interpretation of such non-sentential utterances or fragments, as they are traditionally called (eg. (Morgan, 1973)) is highly context dependent. In this paper we present an overview of a comprehensive formal theory of the interpretation of fragments.1 The theory has as components an empirically validated taxonomy, an analysis of the syntax and compositional semantics of fragments, and a formalisation of their contextual interpretation. We also briefly describe an implementation of this theory, 1 More details can be found in (Schlangen, 2003). and quantify the potential practical use of handling utterances of this kind in dialogue systems. The main thesis of our approach is that the resolution of the intended content of fragments can be modelled as a by-product of the establishment of coherence in dialogue, which (following much of the work on discourse) we define as the establishment of a meaningful connection of the content of the current utterance to its discourse context. We will show that the constraints on the form and content of fragments follow from such connections. There has recently been some renewed interest in fragmen"
W03-2106,P01-1031,0,0.0411143,"list of relations is given in Tables 1 and 2, together with informal definitions of the semantics of each relation (where β is the fragment and α the utterance it is related to) and an example instance. We will not go into more details of the types here; in Section 5 we will return to a select few and give a formal definition of their semantics. Note that we do not claim that this set of fragment-types is exhaustive; we discuss in the next section the coverage that can be achieve with it on test data.5 As a final point, note that we subsume what is often called “clarification question” (eg. (Ginzburg and Cooper, 2001)) under Elaboration to stress the similarity with ‘normal’ elaborations. The subscripts p and q indicate the message type of α and β; e.g. Elabpq is an elaboration of a proposition, performed with a question. 2.2 Corpus Study To test the coverage that can be achieved with our taxonomy, we analysed 5087 items of general free conversation from the BNC (dialogues KSU and KSV), and 4037 items of task-oriented dialogue from the VM/redwoods corpus (the 125 dialogues on the VM - CD - ROM 6).6 We proceeded 3 All taxonomies of fragment-types that are known to us use classes that are at least partially"
W03-2106,C80-1027,0,0.707252,"Missing"
W04-0607,W02-1706,0,0.139049,"patial dimensions) which can be recognised easily but would require very specialised grammar rules later on.6 Then, the domain-specific lexicon is accessed, which maps “concept names” (nouns, or phrases as recognised in the previous step) to the concept IDs used in the ontology.7 Tokens for which there is no entry in that lexicon, and which are hence deemed ‘irrelevant’ for the domain, are assigned a ‘dummy’ semantics appropriate for their part of speech, so that they do not confuse the later parsing stage. (More details about this kind of robustness will be given shortly.) 6 See for example (Grover et al., 2002) for a discussion of the utility of a named entitiy recognition preprocessing stage for robust symbolic parsing. 7 Note that this lexicon is one single resource out of which also the domain specfic additions to the morphology-lexicon and the list of multi-word expressions are compiled. 3.3 Chunk Parsing Next, the analyses of the tokens are transformed into a feature structure format, and are passed to the parsing component.8 The output of this stage is an intermediate semantic representation of (aspects of) the content (of which the notation shown in 1 is a variant). This format is akin to tra"
W04-0607,W03-2106,1,0.833333,"nt means certain limitations have to be accepted. Being a fragment of FOL, it is not expressive enough to represent certain finer semantic details, as will be discussed below. However, the advantage of using an emerging standard for delivering and sharing information outweighs these drawbacks. 3 Implementation 3.1 Overview As mentioned above, most of the sentences in our corpus do not contain a finite verb; i.e., according to standard rules of grammar they are elliptical. While a theoretically motivated approach should strive to resolve this ellipsis contextually (for example as described in (Schlangen, 2003)), in view of the intended application and for reasons of robustness we have decided to focus only on extracting information about the entities introduced in the reports— that is, on recognising nominal phrases, leaving aside the question of how verbal meanings are to be resolved. Our strategy is to combine a “shallow” preprocessing stage (based on finite-state methods and statistical approaches) with a symbolic phase, in which the semantics of the NPs is assembled.5 A requirement for the processing is that it must be robust, in two ways: it must be able to deal with unknown tokens (i.e., “out"
W04-2325,P01-1031,0,0.047192,"ent.3 2 (Purver et al., 2001) investigates such a mapping, based on the classification discussed below in Section 2.2. 3 The dimensions for classification introduced here are related to, but different in some aspects from those used in (Larsson, 2003). Our term ‘CR’ covers what Larsson calls negative feedback as well as what he calls checking feedback, whereas Before we finally come to the description of the dimension level of understanding in the next Section, we will briefly look at an earlier analysis of CR that does not make these distinctions. 2.2 Previous Analyses In a number of papers (Ginzburg and Cooper, 2001; Purver et al., 2001), Jonathan Ginzburg and colleagues have developed an influential analysis of CR. The authors define two readings that can be ascribed to CRs, which they name the constituent reading and the clausal reading.4 (8-b) shows paraphrases of these readings for the CR in (8-a).5 (8) a. A: Did Bo leave? — B: Bo? b. clausal: Are you asking whether Bo left? constituent: Who’s Bo? These readings are defined informally by (Ginzburg and Cooper, 2001) (henceforth G&C) as follows: the clausal reading “takes as the basis for its content the content of the conversational move [≈ speech act"
W04-2325,W01-1616,0,0.683882,"nts the intuition that a problem that leads to a request for repetition is more severe than one that leads to a request for confirmation; we will make this notion of severity precise in Section 4. Note that a confirmation request can be realised as an alternative question, as in (5-b-ii) and (4) b, or as a y/n-question, as in (1-c). Lastly, we also distinguish between CRs that point out a problematic element in the original utterance, and those that don’t. The former category is illustrated by the CRs in (1) and (5-b), the latter by those in (5-a) and (5-c). We call this dimension extent.3 2 (Purver et al., 2001) investigates such a mapping, based on the classification discussed below in Section 2.2. 3 The dimensions for classification introduced here are related to, but different in some aspects from those used in (Larsson, 2003). Our term ‘CR’ covers what Larsson calls negative feedback as well as what he calls checking feedback, whereas Before we finally come to the description of the dimension level of understanding in the next Section, we will briefly look at an earlier analysis of CR that does not make these distinctions. 2.2 Previous Analyses In a number of papers (Ginzburg and Cooper, 2001; Pu"
W04-2325,W01-1618,0,0.212785,"Missing"
W04-2325,W03-2106,1,0.776154,"the speech act type) which connects the new information to some antecedent utterance. This speech act places constraints on content and the speech act related goals or SARGs; these in turn serve to resolve semantic underspecification. Note that those SARGs are goals that are either conventionally associated with a particular type of utterance or are recoverable by the interpreter from the discourse context; this distinguishes the goals that interact with linguistic knowledge from goals in general. The implementation of the theory which we extended for this paper, RUDI (Schlangen et al., 2001; Schlangen, 2003), works in the domain of appointment scheduling (we will refer to the extended version as RUDIclar ). It focuses on resolving one particular kind of underspecification, namely that arising from the need to “bridge” definites to their context. To give an example, for (15) the system computes that the “Wednesday afternoon” is ‘bridged’ via the relation ‘next’ to the time of utterance: (15) A: B: What is a good time for you in the next couple of weeks? Wednesday afternoon would be good. It does this by non-monotonically inferring the rhetorical relation connecting the second to the first utteranc"
W08-0113,W95-0107,0,0.0531615,"scene (which object is where); the features are designed to also be derivable from digital images instead, using standard computer vision techniques (Shapiro and Stockman, 2001); this is future work, however. 85 Given the requirement for robustness, we decided against a hand-written grammar for deriving such annotations; the moderate size of our corpus on the other hand made for example Markov modelbased approaches difficult to apply. We hence chose transformation-based learning to create this (shallow) segmentation grammar, converting the segmentation task into a tagging task (as is done in (Ramshaw and Marcus, 1995), inter alia). In our approach, each token that is to be tagged is itself represented in three different forms or layers: lemmatised word, as POS-tag, and by its spatial-functional tag (as in Table 1; added by simple look-up). All these layers can be accessed in the learned rules. Apart from this, the module is a straightforward implementation of (Ramshaw and Marcus, 1995), which in turn adapts (Brill, 1993) for syntactic chunking. 3.2 Visual Word Semantics To learn the visual semantics of words we implemented a simple technique for grounding words in perceptions. Roughly, the idea is to extra"
W08-0113,2007.sigdial-1.10,1,0.822914,"Missing"
W08-0113,2007.sigdial-1.9,1,0.886053,"Missing"
W09-0509,C08-2003,1,0.854593,"Missing"
W09-0509,P98-2236,0,0.0560393,"Missing"
W09-0509,P00-1018,0,0.0395388,"Missing"
W09-0509,W04-0308,0,0.0289062,"the possible values for each slot. The entity names had to be filled in with the letter names of the pieces, the end slot with body parts or right, left, horizontal etc., and the position slots with positive and negative numbers.3 The chunker was then run on 400 of these utterances and the slot values were compared with the annotated frames. 100 of the labelled utterances and 50 additional utter2.4 Incrementality One of the main features of RUBISC is its incrementality. It can receive one word at a time and extract semantic structure from it. Incrementality is not strict here in the sense of (Nivre, 2004), because sometimes more than one word is needed before parts of the frame are constructed and output: into the right, for instance, needs to wait for a word like leg that completes the chunk. We don’t necessarily consider this a disadvantage, though, as our chunks closely correlate to the minimal bits 3 In a small fraction (21) of the 500 cases an utterance actually contained 2 statements that were combined with und/and. In these cases the second statement was neglected. 69 the results on our corpus. When we also turn off allowing initial material (erm the piece), however, performance drops c"
W09-0509,A00-2041,0,0.0346816,"Missing"
W09-0509,P07-1049,0,0.0149401,"ime NLP applications such as dialogue systems can profit considerably from incremental processing of language. When syntactic and semantic structure is built on-line while the speech recognition (ASR) is still working on the speech stream, unnatural silences can be avoided and the system can react in a faster and more userfriendly way. As (Aist et al., 2007) and (Skantze and Schlangen, 2009) show, such incremental systems are typically preferred by users over nonincremental systems. To achieve incrementality, most dialogue systems employ an incremental chart parser (cf. (Stoness et al., 2004; Seginer, 2007) etc.). However, most existing dialogue systems operate in very limited domains, e.g. moving objects, people, trains etc. from one place to another (cf. 1 cf. The incremental parser in (Skantze, 2007) can jump over a configurable number of words in the input. Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 66–73, c Athens, Greece, 30 March 2009. 2009 Association for Computational Linguistics 66 than by rigid templates. [...] and the order in which chunks occur is much more flexible than the order of words within chunks.” In our approach chunks"
W09-0509,W08-0113,1,0.834364,"dialogue corpus used were German, but we give some examples in English for better readability. As time passes the chunker receives more and more words from the ASR. It puts the words in a queue and waits until the semantic content of the accumulated words is enough for filling a slot in the frame semantics. When this is the case the chunk is completed and a new chunk is started. At the same time the frame semantics is updated if slot unification (see below) is possible and a check The grammar we are using for the experiments in this paper was developed using a small corpus of German dialogue (Siebert and Schlangen, 2008), (Siebert, 2007). Figure 2 shows a picture of the task that the subjects completed for this corpus.2 A number of pentomino pieces were presented. The pieces had to be moved into an animal-shaped figure. The subjects were shown partly completed puzzles and had to give concise and detailed verbal instructions of the next move that had to be done. The locations inside this figure were usually referred to in terms of body parts (move the x into 2 For the corpus used here the difference was that the button labels were German and that the pentomino pieces were not ordered in two rows. For better re"
W09-0509,E09-1085,1,0.834826,"Missing"
W09-0509,W04-0304,0,0.0142998,"de Introduction Real-time NLP applications such as dialogue systems can profit considerably from incremental processing of language. When syntactic and semantic structure is built on-line while the speech recognition (ASR) is still working on the speech stream, unnatural silences can be avoided and the system can react in a faster and more userfriendly way. As (Aist et al., 2007) and (Skantze and Schlangen, 2009) show, such incremental systems are typically preferred by users over nonincremental systems. To achieve incrementality, most dialogue systems employ an incremental chart parser (cf. (Stoness et al., 2004; Seginer, 2007) etc.). However, most existing dialogue systems operate in very limited domains, e.g. moving objects, people, trains etc. from one place to another (cf. 1 cf. The incremental parser in (Skantze, 2007) can jump over a configurable number of words in the input. Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 66–73, c Athens, Greece, 30 March 2009. 2009 Association for Computational Linguistics 66 than by rigid templates. [...] and the order in which chunks occur is much more flexible than the order of words within chunks.” In our"
W09-0509,C98-2231,0,\N,Missing
W09-3905,N09-1043,1,0.408122,"ce in real conversations is often much more complex, and is a collaborative process that isn’t confined to single expressions (Clark and Schaefer, 1987): referring is a pragmatic action that is not reducible to denotation. In our corpus (see below), we often find descriptions as in (2), where the speaker continuously adds (rather vague) material, typically until the addressee signals that she identified the item, or proposes a different way to describe it. 3 Evaluation Metrics for IRR In previous work, we have discussed metrics for evaluating the performance of incremental speech recognition (Baumann et al., 2009). There, our metrics could rely on time-aligned gold-standard information against which the incremental results could be measured. For the reasons discussed in the previous section, we do not assume that we have such temporally-aligned information for evaluating IRR. Our measures described here simply assume that there is one intention behind the referring utterances (namely to identify a certain entity), and that this intention is there from the beginning of the utterance and stays constant.2 This is not to be understood as the claim that it is reasonable to expect an IRR component to pick ou"
W09-3905,schiel-2004-maus,0,0.0188503,"tomino Pieces with their canonical names (which were not known to the dialogue participants). The pieces used in the dialogues all had the same colour. F L N P T U V W X Y Z tile Figure 3: Silence rate per referent and corpus (WOz:black, PentoNaming:red, FTT:green) In the remaining sections, we describe a probabilistic model of IRR that we have implemented, and evaluate it in terms of these metrics. We begin with describing the data from which we learnt our model. All utterances were hand-transcribed and the transcriptions were automatically aligned with the speech data using the MAUS system (Schiel, 2004); this way, we could automatically identify pauses during utterances and measure their length. For some experiments (see below), pauses were “re-ified” through the addition of silence pseudowords (one for each 333 ms of silence). The resulting corpus is not fully balanced in terms of available material for the various pieces or contributions by sub-corpora. 4 Data 4.1 I Our Corpora As the basis for training and testing of our model we used data from three corpora of task-oriented dialogue that differ in some details of the set-up, but use the same task: an Instruction Giver (IG) instructs an I"
W09-3905,W08-0113,1,0.704307,"n of silence pseudowords (one for each 333 ms of silence). The resulting corpus is not fully balanced in terms of available material for the various pieces or contributions by sub-corpora. 4 Data 4.1 I Our Corpora As the basis for training and testing of our model we used data from three corpora of task-oriented dialogue that differ in some details of the set-up, but use the same task: an Instruction Giver (IG) instructs an Instruction Follower (IF) on which puzzle pieces (from the “Pentomino” game, see Figure 2) to pick up. In detail, the corpora were: • The Pento Naming corpus described in (Siebert and Schlangen, 2008). In this variant of the task, IG records instructions for an absent IF; so these aren’t fully interactive dialogues. The corpus contained 270 utterances out of which we selected those 143 that contained descriptions of puzzle pieces (and not of their position on the gameboard). • Selections from the FTT/PTT corpus described in (Fern´andez et al., 2007), where IF and IG are connected through an audio-only connection, and in some dialogues a simplex / push-to-talk one. We selected all utterances from IG that contained references to puzzle pieces (286 altogether). • The third part of our corpus"
W09-3905,E09-1085,1,0.471271,"Missing"
W09-3905,W04-0304,0,0.146468,"Missing"
W09-3943,W09-3905,1,0.831307,"reduce edits of the ASR’s incremental hypothesis (Baumann et al., 2009a). Figure 2 shows incremental hypotheses and different settings of two filtering strategies. When evaluating the utility of using n-best ASR hypotheses, we used TEDview to visualize the best hypotheses (Baumann et al., 2009b). An interesting result we got from this analysis is that typically the best hypothesis seems to be more stable than lower-ranked hypotheses, as can be seen in Figure 3. We also evaluated the behaviour of our incremental reference resolution module, which outputs distributions over possible referents (Schlangen et al., 2009). We implemented a TEDview plug-in to show distributions in bar-charts, as can be seen in Figure 4. 3 Use Cases To illustrate the versatility of the tool, we now describe how we use it in several projects at our site. (Technical manuals can be downloaded from the page listed above.) 3.1 Analysis of Dialogue Data In the DEAWU project (see e.g. (Schlangen and Fern´andez, 2007)), we used the package to maintain transcriptions made in Praat and annotations made in MMAX2 (M¨uller and Strube, 2006), and to visualize these together in a time-aligned view. As Figure 1 shows, we made heavy use of the p"
W09-3943,N09-1043,1,0.408812,"ten in the programming language Python with its powerful library of extension modules; this enabled us to implement an inspector for syntax trees in only 20 lines of code. In another project, we use TELIDA to analyze and visualize the incremental output of several modules of a spoken dialogue system we are currently developing. In incremental speech recognition, what is considered the best hypothesis frequently changes as more speech comes in. We used TEDview to analyze these changes and to develop filtering methods to reduce the jitter and to reduce edits of the ASR’s incremental hypothesis (Baumann et al., 2009a). Figure 2 shows incremental hypotheses and different settings of two filtering strategies. When evaluating the utility of using n-best ASR hypotheses, we used TEDview to visualize the best hypotheses (Baumann et al., 2009b). An interesting result we got from this analysis is that typically the best hypothesis seems to be more stable than lower-ranked hypotheses, as can be seen in Figure 3. We also evaluated the behaviour of our incremental reference resolution module, which outputs distributions over possible referents (Schlangen et al., 2009). We implemented a TEDview plug-in to show distr"
W10-4302,W09-3902,0,0.186367,"recently, discriminative models (e.g., (Mairesse et al., 2009)) that directly learn a mapping between input and output. Much of this work uses the ATIS corpus (Dahl et al., 1994) as data and hence is directly comparable. In Table 1, we list the results achieved by this work; we will later situate our results relative to this. That work, however, only looks at mappings between complete utterances and semantic representations, whereas we are interested in the process of mapping semantic representations to successively larger utterance fragments. More closely related then is (Sagae et al., 2009; DeVault et al., 2009), where a maximum entropy model is trained for mapping utterance fragments to semantic frames. (Sagae et al., 2009) make the observation that often the quality of the prediction does not increase anymore towards the end of the utterance; that is, the meaning of the utterance can be predicted before it is complete. In (Schlangen et al., 2009), we presented a model that predicts incrementally a specific aspect of the meaning of a certain type of utterance, namely the intended referent of a referring expression; the similarity here is that the output is of the same type regardless of whether the"
W10-4302,2007.sigdial-1.25,1,0.291206,"Missing"
W10-4302,C94-2120,0,0.015154,"derstanding, which were sketched in the first paragraph above: a) forming a partial understanding, and b) predicting a complete understanding. Recently, some results have been published on b), predicting utterance meanings, (Sagae et al., 2009; Schlangen et al., 2009). We investigate here how well this predictive approach works in two other domains, and how a simple extension of techniques (ensembles of slot-specific classifiers vs. one frame-specific one) can improve performance. To our knowledge, task a), computing partial meanings, has so far only been tackled with symbolic methods (e.g., (Milward and Cooper, 1994; Aist et al., 2006; Atterer and Schlangen, 2009));1 we present here some first results on approaching it with statistical models. Plan of the paper: First, we discuss relevant previous work. We then define the task of incremental natural language understanding and its two variants in more detail, also looking at how models can be evaluated. Finally, we present and discuss the results of our experiments, and close with a conclusion and some discussion of future work. Incremental natural language understanding is the task of assigning semantic representations to successively larger prefixes of"
W10-4302,N09-2014,0,0.444496,"lastname@uni-potsdam.de Abstract 2001; Schlangen and Skantze, 2009); we focus here on the sub-problem of modelling incremental understanding—a precondition for enabling truly interactive behaviour. More specifically, we look at statistical methods for learning mappings between (possibly partial) utterances and meaning representations. We distinguish between two types of understanding, which were sketched in the first paragraph above: a) forming a partial understanding, and b) predicting a complete understanding. Recently, some results have been published on b), predicting utterance meanings, (Sagae et al., 2009; Schlangen et al., 2009). We investigate here how well this predictive approach works in two other domains, and how a simple extension of techniques (ensembles of slot-specific classifiers vs. one frame-specific one) can improve performance. To our knowledge, task a), computing partial meanings, has so far only been tackled with symbolic methods (e.g., (Milward and Cooper, 1994; Aist et al., 2006; Atterer and Schlangen, 2009));1 we present here some first results on approaching it with statistical models. Plan of the paper: First, we discuss relevant previous work. We then define the task of"
W10-4302,E09-1081,1,0.329936,"y in a dialogue system, e.g. for completing the user’s utterance as an indication of the system’s grounding state. While these are interesting uses, the approach is somewhat limited by the fact that it is incremental only on the input side, while the output does not reflect how ‘complete’ (or not) the input is. We will compare this kind of incremental processing in the next section with one where the output is incremental as well, and we will then present results from our own experiments with both kinds of incrementality in statistical NLU. 3 3.1 Task, Evaluation, and Data Sets The Task 2 In (Schlangen and Skantze, 2009), this type of incremental processing is called “input incremental”, as only the input is incrementally enriched, while the output is always of the same type (but may increase in quality). We have said that the task of incremental natural language understanding consists in the assignment 10 only that is represented for which there is evidence in what has already been seen), but without direct association of parts of the representation and parts of the utterance or utterance prefix. 3.2 namely the final frame. Aligned Output As sequence alignments have more structure—there is a linear order bet"
W10-4302,W09-3905,1,0.500879,".de Abstract 2001; Schlangen and Skantze, 2009); we focus here on the sub-problem of modelling incremental understanding—a precondition for enabling truly interactive behaviour. More specifically, we look at statistical methods for learning mappings between (possibly partial) utterances and meaning representations. We distinguish between two types of understanding, which were sketched in the first paragraph above: a) forming a partial understanding, and b) predicting a complete understanding. Recently, some results have been published on b), predicting utterance meanings, (Sagae et al., 2009; Schlangen et al., 2009). We investigate here how well this predictive approach works in two other domains, and how a simple extension of techniques (ensembles of slot-specific classifiers vs. one frame-specific one) can improve performance. To our knowledge, task a), computing partial meanings, has so far only been tackled with symbolic methods (e.g., (Milward and Cooper, 1994; Aist et al., 2006; Atterer and Schlangen, 2009));1 we present here some first results on approaching it with statistical models. Plan of the paper: First, we discuss relevant previous work. We then define the task of incremental natural langu"
W10-4302,E09-1085,1,0.552234,"ass me the salt over there, please?”. It is quite likely that you get the idea that something is wanted of you fairly early into the utterance, and understand what exactly it is that is wanted even before the utterance is over. This is possible only because you form an understanding of the meaning of the utterance even before it is complete; an understanding which you refine—and possibly revise—as the utterance goes on. You understand the utterance incrementally. This is something that is out of reach for most current dialogue systems, which process utterances non-incrementally, en bloc (cf. (Skantze and Schlangen, 2009), inter alia). Enabling incremental processing in dialogue systems poses many challenges (Allen et al., 2 Related Work Statistical natural language understanding is an active research area, and many sophisticated models for this task have recently been published, be that generative models (e.g., in (He and Young, 2005)), which learn a joint distribution over in1 We explicitly refer to computation of incremental interpretations here; there is of course a large body of work on statistical incremental parsing (e.g., (Stolcke, 1995; Roark, 2001)). Proceedings of SIGDIAL 2010: the 11th Annual Meeti"
W10-4302,J95-2002,0,0.159977,"process utterances non-incrementally, en bloc (cf. (Skantze and Schlangen, 2009), inter alia). Enabling incremental processing in dialogue systems poses many challenges (Allen et al., 2 Related Work Statistical natural language understanding is an active research area, and many sophisticated models for this task have recently been published, be that generative models (e.g., in (He and Young, 2005)), which learn a joint distribution over in1 We explicitly refer to computation of incremental interpretations here; there is of course a large body of work on statistical incremental parsing (e.g., (Stolcke, 1995; Roark, 2001)). Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 9–16, c The University of Tokyo, September 24-25, 2010. 2010 Association for Computational Linguistics 9 (Mairesse et al., 2009) (He and Young, 2005) (Zettlemoyer and Collins, 2007) (Meza et al., 2008) 94.50 90.3 95.9 91.56 of semantic representations to progressively more complete prefixes of utterances. This description can be specified along several aspects, and this yields different versions of the task, appropriate for different uses. One question is what th"
W10-4302,D07-1071,0,0.0581584,"Missing"
W10-4302,H94-1010,0,0.0406667,". Finally, there is a hybrid form of representation and assignment. If we allow the output frames to ‘grow’ as more input comes in (hence possibly violating the typing of the frames as they are expected for full utterances), we get a form of representation with a notion of ‘partial semantics’ (as Table 1: Recent published f-scores for nonincremental statistical NLU, on the ATIS corpus put, output and possibly hidden variables; or, more recently, discriminative models (e.g., (Mairesse et al., 2009)) that directly learn a mapping between input and output. Much of this work uses the ATIS corpus (Dahl et al., 1994) as data and hence is directly comparable. In Table 1, we list the results achieved by this work; we will later situate our results relative to this. That work, however, only looks at mappings between complete utterances and semantic representations, whereas we are interested in the process of mapping semantic representations to successively larger utterance fragments. More closely related then is (Sagae et al., 2009; DeVault et al., 2009), where a maximum entropy model is trained for mapping utterance fragments to semantic frames. (Sagae et al., 2009) make the observation that often the quali"
W10-4302,W09-0509,1,\N,Missing
W10-4308,E09-1081,1,\N,Missing
W10-4342,P05-3001,0,0.0143461,"something may start with a description, monitor while they go on whether the description appears to be understood sufficiently well, and if not, possibly extend it, rather than finishing the utterance in the form that was initially planned. This monitoring within the utterance is sometimes even made very explicit, as in the following example from (Clark, 1996): (1) 2 Related Work Collaboration on utterances has not often been modelled in SDS, as it presupposes fully incremental processing, which itself is still something of a rarity in such systems. (There is work on collaborative reference (DeVault et al., 2005; Heeman and Hirst, 1995), but that focuses on written input, and on collaboration over several utterances and not within utterances.) There are two systems that are directly relevant here. The system described in (Aist et al., 2007) is able to produce some of the phenomena that we are interested in here. The set-up is a simple reference game (as we will see, the domain we have chosen is very similar), where users can refer to objects shown on the screen, and the SDS gives continuous feedback about its understandA: A man called Annegra? B: yeah, Allegra A: Allegra, uh, replied and, uh, . . . I"
W10-4342,J95-3003,0,0.0812289,"ith a description, monitor while they go on whether the description appears to be understood sufficiently well, and if not, possibly extend it, rather than finishing the utterance in the form that was initially planned. This monitoring within the utterance is sometimes even made very explicit, as in the following example from (Clark, 1996): (1) 2 Related Work Collaboration on utterances has not often been modelled in SDS, as it presupposes fully incremental processing, which itself is still something of a rarity in such systems. (There is work on collaborative reference (DeVault et al., 2005; Heeman and Hirst, 1995), but that focuses on written input, and on collaboration over several utterances and not within utterances.) There are two systems that are directly relevant here. The system described in (Aist et al., 2007) is able to produce some of the phenomena that we are interested in here. The set-up is a simple reference game (as we will see, the domain we have chosen is very similar), where users can refer to objects shown on the screen, and the SDS gives continuous feedback about its understandA: A man called Annegra? B: yeah, Allegra A: Allegra, uh, replied and, uh, . . . In this example, A makes u"
W10-4342,W09-3937,0,0.014052,"Missing"
W10-4342,E09-1081,1,0.235916,"on: intra-utterance hesitations, possibly with trial intonation (as in line 2);2 immediate execution of actions (line 4), and their grounding role as display of understanding (“yeah” in line 3). The system controls the mouse cursor, e.g. moving it over pieces once it has a good hypothesis about a reference; other actions are visualised similarly. 4 Implementation 4.1 Overview Our system is realised as a collection of incremental processing modules in the InproToolKit (Schlangen et al., 2010), a middle-ware package that implements some of the features of the model of incremental processing of (Schlangen and Skantze, 2009). The modules used in the implementation will be described briefly below. 4.2 ASR, Prosody, Floor Tracker & NLU For speech recognition, we use Sphinx-4 (Walker et al., 2004), with our own extensions for incremental speech recognition (Baumann et al., 2009), and our own domain-specific acoustic model. For the experiments described here, we used a recognition grammar. Another module performs online prosodic analysis, based on pitch change, which is measured in semi-tone per second over the turn-final word, using a modified YIN (de Cheveign´e and Kawahara, 2002). Based on the slope of the f0 curv"
W10-4342,W10-4308,1,0.841293,"erm IG-3: the red one IF-4: .. yeah [moves cursor] IG-5: take that. We chose these as our target phenomena for the implementation: intra-utterance hesitations, possibly with trial intonation (as in line 2);2 immediate execution of actions (line 4), and their grounding role as display of understanding (“yeah” in line 3). The system controls the mouse cursor, e.g. moving it over pieces once it has a good hypothesis about a reference; other actions are visualised similarly. 4 Implementation 4.1 Overview Our system is realised as a collection of incremental processing modules in the InproToolKit (Schlangen et al., 2010), a middle-ware package that implements some of the features of the model of incremental processing of (Schlangen and Skantze, 2009). The modules used in the implementation will be described briefly below. 4.2 ASR, Prosody, Floor Tracker & NLU For speech recognition, we use Sphinx-4 (Walker et al., 2004), with our own extensions for incremental speech recognition (Baumann et al., 2009), and our own domain-specific acoustic model. For the experiments described here, we used a recognition grammar. Another module performs online prosodic analysis, based on pitch change, which is measured in semi-"
W10-4342,E09-1085,1,0.723894,"ct brief pause”. As discussed by Clark (1996), this device is an efficient solution to the problem posed by uncertainty on the side of the speaker whether a reference is going to be understood, as it checks for understanding in situ, and lets the conversation partners collaborate on the utterance that is in production. Spoken dialogue systems (SDS) typically cannot achieve the close coupling between production and interpretation that is needed for this to work, as normally the smallest unit on which they operate is the full utterance (or, more precisely, the turn). (For a discussion see e.g. (Skantze and Schlangen, 2009).) We present here an approach to managing dialogue in an incremental SDS that can handle this phenomenon, explaining how it is implemented in system (Section 4) that works in a micro-domain (which is described in Section 3). As we will discuss in the next section, this goes beyond earlier work on incremental SDS, combining the production of multimodal feedback (as in (Aist et al., 2007)) with fast interaction in a semantically more complex domain (compared to (Skantze and Schlangen, 2009)). When dialogue systems, through the use of incremental processing, are not bounded anymore by strict, no"
W10-4342,N09-1043,1,0.0786824,"e it has a good hypothesis about a reference; other actions are visualised similarly. 4 Implementation 4.1 Overview Our system is realised as a collection of incremental processing modules in the InproToolKit (Schlangen et al., 2010), a middle-ware package that implements some of the features of the model of incremental processing of (Schlangen and Skantze, 2009). The modules used in the implementation will be described briefly below. 4.2 ASR, Prosody, Floor Tracker & NLU For speech recognition, we use Sphinx-4 (Walker et al., 2004), with our own extensions for incremental speech recognition (Baumann et al., 2009), and our own domain-specific acoustic model. For the experiments described here, we used a recognition grammar. Another module performs online prosodic analysis, based on pitch change, which is measured in semi-tone per second over the turn-final word, using a modified YIN (de Cheveign´e and Kawahara, 2002). Based on the slope of the f0 curve, we classify pitch as rising or falling. This information is used by the floor tracking module, which notifies the dialogue manager (DM) about changes in floor status. These status changes are classified by simple rules: silence following rising pitch le"
W11-2015,C08-2003,1,0.825617,"n-ASR knowledge, and hence it should be possible to single out those decision points after which a completion would be especially error-prone, trading coverage against quality of results. Initial experiments support this idea and we would like to extend it to a full error estimation capability. We have focused the analysis of incrementally comparing expected to actual speech rate to the task of micro-aligning a turn-completion and shadowing a speaker. However, we believe that this capability can be used in a broad range of tasks, e. g. in combination 127 with word-based end-of-turn detection (Atterer et al., 2008) to allow for swift turn taking.4 In fact, precise micro-alignment of turn handovers could be used for controlled testing of linguistic/prosodic theory such as the oscillator model of the timing of turn-taking (Wilson and Wilson, 2005). Finally, duration modelling can be used to quickly detect deviations in speech rate (which may indicate hesitations or planning problems of the user) as they happen (rather than post-hoc), allowing to take the speaker’s fluency into account in understanding and turn-taking coordination as outlined by Clark (2002). Acknowledgments This work was funded by a DFG g"
W11-2015,N09-1043,1,0.880403,"Missing"
W11-2015,W09-3902,0,0.657532,"in natural dialogues are typically precisely aligned and prosodically highly integrated with the turn that is being completed (Local, 2007). With ever more incremental (and hence quicker) spoken dialogue systems, the phenomenon of completion comes into reach for SDSs, and hence questions of micro-timing become important. While completing someone else’s turn – especially for a computer – may be considered impolite or even annoying, being able to do so can be a useful capability. Some tasks where it might be helpful are • negotiation training to induce stress in a human trainee as presented by DeVault et al. (2009), or • pronunciation aids for language learners, in which hard to pronounce words could be spoken simultaneously by the system. A system should certainly not try to complete all or even many user turns, but having the capability to do so means that the system has a very efficient interactional device at its disposal. Furthermore, monitoring the user’s timing, as is required for the temporal prediction of turn continuations, can also be used for other conversational tasks such as producing back-channels that are precisely aligned to the user’s back-channel inviting cues, to enable micro-alignme"
W11-2015,W10-4302,1,0.916901,"Missing"
W11-2015,W09-3937,0,0.0140902,"scribing the corpus that we use in Section 5. In Section 6 we first analyse whether enough time to output a completion is available sufficiently often, before turning to the question for the actual sub-tasks of when and how to complete. We wrap up with concluding remarks and ideas for future work. 2 Related Work The general phenomenon of turn completion can be broken down into cases where the completion is spoken simultaneously with the original speaker (turn sharing, (Lerner, 2002)) and where the floor changes in mid-utterance (collaborative turn sequences (Lerner, 2004) or split utterances (Purver et al., 2009)). In this paper, a differentiation between the two cases is not important, as we only deal with the question of when to start speaking (for the previously non-speaking system) and not the question of whether the current turn owner will stop speaking. Moreover, whether the other speaker will stop is beyond the system’s control. Lerner (2004) distinguishes turn co-optation, in which a listener joins in to come first and win the floor, and turn cocompletion, in which the completion is produced in chorus. Both of these phenomena relate to the current speaker’s speech: either to match it, or to be"
W11-2015,W11-0144,0,0.0211123,"Missing"
W11-2015,N09-1071,0,0.37844,"diate turn-taking in an incremental SDS, or to synchronously monitor the user’s speech fluency for other reasons. 1 Introduction Turn completion, that is, finishing a user’s ongoing utterance, can be considered an ideal test-case of incremental spoken language processing, as it requires that all levels of language understanding and production are carried out in real time, without any noticeable lags and with proper timing and even with the ability to predict what will come. Spoken dialogue systems, especially incremental ones, have come a long way towards reducing lags at turn changes (e. g. (Raux and Eskenazi, 2009; Skantze and Schlangen, 2009)), or even predicting upcoming turn changes (Schlangen, 2006; Baumann, 2008; Ward et al., 2010). Compared to regular turn changes, where short pauses or overlaps occur frequently (Weilhammer and Rabold, 2003), turn completions in natural dialogues are typically precisely aligned and prosodically highly integrated with the turn that is being completed (Local, 2007). With ever more incremental (and hence quicker) spoken dialogue systems, the phenomenon of completion comes into reach for SDSs, and hence questions of micro-timing become important. While completing som"
W11-2015,N09-2014,0,0.122192,"Missing"
W11-2015,N10-2009,0,0.0239014,"Missing"
W11-2015,W10-4308,1,0.767259,"e time between decision point and ideal onset (which we call holding time) and the user’s speech rate during the following words. In order for the system to be able to produce a continuation (“five six seven” in Figure 1) in time, of course the decision point must come sufficiently early (i. e. during “four”) to allow for a completion to be output in due time. This important precondition must be met by-and-large by the employed ASR. However, it is not a strict requirement: If ASR results 122 System Description Our system is based on the InproTK toolkit for incremental spoken dialogue systems (Schlangen et al., 2010) which uses Sphinx-4 (Walker et al., 2004) and MaryTTS (Schr¨oder and Trouvain, 2003) as underlying ASR and TTS engines, respectively. The core of our system is a component that incrementally receives rich speech recognition input (words, their durations and a pitch track) from an incremental ASR and computes the timing of completions. When receiving a new word from ASR, our component queries an understanding component whether a completion can be predicted, and if so, whether such a completion should be performed. In order to not duplicate the work of DeVault et al. (2009), we use a mock imple"
W11-2015,E09-1085,1,0.782855,"ncremental SDS, or to synchronously monitor the user’s speech fluency for other reasons. 1 Introduction Turn completion, that is, finishing a user’s ongoing utterance, can be considered an ideal test-case of incremental spoken language processing, as it requires that all levels of language understanding and production are carried out in real time, without any noticeable lags and with proper timing and even with the ability to predict what will come. Spoken dialogue systems, especially incremental ones, have come a long way towards reducing lags at turn changes (e. g. (Raux and Eskenazi, 2009; Skantze and Schlangen, 2009)), or even predicting upcoming turn changes (Schlangen, 2006; Baumann, 2008; Ward et al., 2010). Compared to regular turn changes, where short pauses or overlaps occur frequently (Weilhammer and Rabold, 2003), turn completions in natural dialogues are typically precisely aligned and prosodically highly integrated with the turn that is being completed (Local, 2007). With ever more incremental (and hence quicker) spoken dialogue systems, the phenomenon of completion comes into reach for SDSs, and hence questions of micro-timing become important. While completing someone else’s turn – especially"
W12-1641,W08-0105,0,0.0253359,"levels of verbosity. Redundancy The second adaptation mechanism is redundancy. Again, redundancy is something that an ideal utterance does not contain and by design SPUD penalises the use of redundancy in its heuristic function. Two provisional utterances being equal, the one exhibiting less redundancy is normally preferred. But similar to verbosity, redundancy serves communicative functions in actual language use. It can highlight important information, it can increase the probability of the message being understood (Reiter and Sripada, 2002) and it is often used to repair misunderstanding (Baker et al., 2008). In incremental microplanning, redundant information can be present both within one sub-utterance chunk (e. g., ‘tomorrow, March 26, . . . ’ vs. ‘tomorrow . . . ’) or across IMPTs. For the former case, we modified SPUD’s search heuristic in order to conditionally either prefer an utterance that contains redundant information or an utterance that only contains what is absolutely necessary. In the latter case, redundancy only becomes an option when later IMPTs enable the choice of repeating information previously conveyed and therefore already established as shared knowledge. This is controlled"
W12-1641,P12-3018,1,0.205383,"ion), and to run in real time. Edlund’s system, which uses diphone synthesis, performed non-incrementally before delivery starts. We go beyond this in also enabling changes during delivery and conducting synthesis steps just-in-time. Dutoit et al. (2011) present an incremental HMM optimiser which allows to change pitch and tempo of upcoming phonemes. However, as that system is fed from a (non-incrementally produced) label file, it cannot easily be used in an incremental system. A predecessor of our iSS component (which was not yet fully incremental on the HMM level) is described in detail in (Baumann and Schlangen, 2012a). 296 3 3.1 Incremental and Adaptive NLG The SPUD microplanning framework The NLG component presented here is based on the SPUD microplanning framework (Stone et al., 2003) and realised in DeVault’s (2008) implementation ‘Java SPUD’. SPUD frames microplannig as a constraint satisfaction problem, solving the tasks that are involved in generating a sentence (lexical and syntactic choice, referring expression generation and aggregation) in an integrated manner. Generation starts from a communicative goal that specifies constraints for the final utterance. The generation process is further shape"
W12-1641,W12-1814,1,0.755046,"ion), and to run in real time. Edlund’s system, which uses diphone synthesis, performed non-incrementally before delivery starts. We go beyond this in also enabling changes during delivery and conducting synthesis steps just-in-time. Dutoit et al. (2011) present an incremental HMM optimiser which allows to change pitch and tempo of upcoming phonemes. However, as that system is fed from a (non-incrementally produced) label file, it cannot easily be used in an incremental system. A predecessor of our iSS component (which was not yet fully incremental on the HMM level) is described in detail in (Baumann and Schlangen, 2012a). 296 3 3.1 Incremental and Adaptive NLG The SPUD microplanning framework The NLG component presented here is based on the SPUD microplanning framework (Stone et al., 2003) and realised in DeVault’s (2008) implementation ‘Java SPUD’. SPUD frames microplannig as a constraint satisfaction problem, solving the tasks that are involved in generating a sentence (lexical and syntactic choice, referring expression generation and aggregation) in an integrated manner. Generation starts from a communicative goal that specifies constraints for the final utterance. The generation process is further shape"
W12-1641,E09-1081,1,0.57825,"Missing"
W12-1641,W10-4308,1,0.831232,"o support notification about progress changes in delivery. The next section describes how this is used to drive the system. 5 Integrating iNLG and iSS for Adaptive Information Presentation As a basis, we use MaryTTS (Schröder and Trouvain, Integrating incremental microplanning with incre2003), but replace Mary’s internal data structures mental speech synthesis in one incremental output and processing strategies with structures from our generation architecture allows us to test and explore incremental SDS architecture, the I NPROTK toolkit how their capabilities act in a coordinated way. As a (Schlangen et al., 2010; Baumann and Schlangen, first example, we implemented a system that presents 2012b), which implements the IU model for incre- information about events in an appointment database mental dialogue processing (Schlangen and Skantze, (e. g., new, conflicting or rescheduled appointments) 2009). The model conceptualises – and the toolkit and is able to cope with external noise burst events, implements – incremental processing as the process- as they might for example occur on a bad telephone ing of incremental units (IUs), which are the smallest line or when using a dialogue system next to a busy ‘c"
W12-1641,W10-4301,0,0.462904,"synthesised and realised in one big chunk. As systems become increasingly more conversational, however, the need arises to make output generation1 more flexible. In particular, capabilities for incrementally generating output become desirable, for two kinds of reasons. (a) In situations where fast system responses are important, production of output can begin before the 1 We will use the term ‘output generation’ here to cover both natural language generation and speech synthesis. content that is to be presented is fully specified – even if what is being produced is just a turn-taking signal (Skantze and Hjalmarsson, 2010). (b) A system that produces its output incrementally can react to events happening while it is realising an utterance. This can be beneficial in domains where the state of the world that the system relays information about can change mid-utterance, so that a need may arise to adapt while speaking. It should also improve naturalness by allowing the system to react to dialogue phenomena such as concurrent feedback signals from the user (Buschmeier and Kopp, 2011). We present work towards enabling such capabilities. We have implemented and connected a component for incremental natural language g"
W12-1641,W02-0111,0,0.0106771,"and aggregation) in an integrated manner. Generation starts from a communicative goal that specifies constraints for the final utterance. The generation process is further shaped by (a) general constraints that model pragmatic properties of language use such as the Gricean maxims (a principle called ‘textual economy’); (b) specific constraints imposed through the communicative status of the propositions to be communicated (i. e., what knowledge can be presupposed and what needs to be communicated explicitly); and (c) linguistic resources (a context-free tree rewriting formalism based on LTAG; Stone, 2002). To deal efficiently with the infinite search space spanned by the linguistic resources, SPUD uses a heuristic search algorithm to find an utterance that satisfies the imposed constraints (Stone et al., [2003] describe the heuristic function). In each search step, the algorithm expands the ‘provisional’ utterance by adding the linguistic resource that maximally reduces the estimated distance to the final utterance. If the generation process runs into a dead-end state, it could in principle deal with the situation by tracking back and expanding a different branch. This, however, is impractical"
W12-1641,J02-4007,0,\N,Missing
W12-1643,W10-4342,1,0.874115,"Missing"
W12-1643,W07-1210,0,0.180372,"the immediately preceding system action, and the intended interpretation of the utterance (as understood by the Wizard) in the form of a semantic frame specifying action-type and arguments, where those arguments are objects occurring in the description of the state of the board. The language of the corpus is German. Figure 2: Example Pentomino Board For this study, we were interested in the potential contribution of linguistic structure to the NLU task. To this end, we produced for each utterance an incremental sequence of parses and corresponding semantic representations (as RMRS structures (Copestake, 2007), i.e. underspecified semantic representations), using the parser described in (Peldszus et al., 2012). These representations were not further manually checked for appropriateness, and hence do not necessarily represent ground truth. As in (Peldszus et al., 2012), we discarded utterances without clear semantic alignments. One major difference from them is that we do include the 661 utterances that used pronouns to refer to pieces, leaving us with 1687 utterances, 5.43 words per utterance (sd 2.36), with a vocabulary of 237 distinct words. These were transcribed utterances and not automatic spe"
W12-1643,W09-3902,0,0.138614,"recent attempts that partially overcome these limitations. 314 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 314–323, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics 2011)), which tries to infer linguistic structures automatically, but normally stops at generating, not interpreting semantic representations, and works with (the text of) full utterances and not incrementally on speech data; and incremental NLU, which is a less intensely studied field, but where previous contributions (such as (DeVault et al., 2009; Devault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009)) have not dealt with learned grounded semantics. We go beyond this earlier work in that we study a model that is incremental, can use linguistic structure, and learns from conversational data a semantics that connects the utterance to its visual and discourse context. We have looked at individual components of this before (grounded semantics in (Siebert and Schlangen, 2008); incremental reference resolution in (Schlangen et al., 2009); incremental general NLU in (Heintze et al., 2010); interaction between incremental parsi"
W12-1643,2007.sigdial-1.25,1,0.462638,"Missing"
W12-1643,W10-4302,1,0.91986,"where previous contributions (such as (DeVault et al., 2009; Devault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009)) have not dealt with learned grounded semantics. We go beyond this earlier work in that we study a model that is incremental, can use linguistic structure, and learns from conversational data a semantics that connects the utterance to its visual and discourse context. We have looked at individual components of this before (grounded semantics in (Siebert and Schlangen, 2008); incremental reference resolution in (Schlangen et al., 2009); incremental general NLU in (Heintze et al., 2010); interaction between incremental parsing and reference resolution in (Peldszus et al., 2012)), but use a more sophisticated model in this work and show that tackling these tasks jointly improves performance. Context/World Language/ RMRS MLN System Prediction: action object result Context/ Discourse Figure 1: NLU Data Flow We apply Markov Logic Networks (MLNs, (Richardson and Domingos, 2006)) as the machine learning technique in our experiments. MLNs have recently received attention in language processing fields like co-reference resolution (Chen, 2009), semantic role labeling (Meza-Ruiz and R"
W12-1643,P11-1060,0,0.0984325,"Missing"
W12-1643,N09-1018,0,0.0307761,"e et al., 2010); interaction between incremental parsing and reference resolution in (Peldszus et al., 2012)), but use a more sophisticated model in this work and show that tackling these tasks jointly improves performance. Context/World Language/ RMRS MLN System Prediction: action object result Context/ Discourse Figure 1: NLU Data Flow We apply Markov Logic Networks (MLNs, (Richardson and Domingos, 2006)) as the machine learning technique in our experiments. MLNs have recently received attention in language processing fields like co-reference resolution (Chen, 2009), semantic role labeling (Meza-Ruiz and Riedel, 2009), spoken (albeit neither situational nor incremental) NLU (Meurs et al., 2008), and web information extraction (Satpal et al., 2011). The framework offers a convenient way of specifying factor functions on sets of random variables for undirected graphical models (Markov Random Fields, see (Kindermann and Snell, 1980)), in such a way that the factors correspond to weighted first order formulae and the joint distribution of random variables corresponds to 315 probabilities of groundings of formulae. In this way, MLN s offer a helpful bridge between symbolic representation and stochastic inferenc"
W12-1643,E12-1052,1,0.752409,"al., 2007; Schlangen and Skantze, 2009)) have not dealt with learned grounded semantics. We go beyond this earlier work in that we study a model that is incremental, can use linguistic structure, and learns from conversational data a semantics that connects the utterance to its visual and discourse context. We have looked at individual components of this before (grounded semantics in (Siebert and Schlangen, 2008); incremental reference resolution in (Schlangen et al., 2009); incremental general NLU in (Heintze et al., 2010); interaction between incremental parsing and reference resolution in (Peldszus et al., 2012)), but use a more sophisticated model in this work and show that tackling these tasks jointly improves performance. Context/World Language/ RMRS MLN System Prediction: action object result Context/ Discourse Figure 1: NLU Data Flow We apply Markov Logic Networks (MLNs, (Richardson and Domingos, 2006)) as the machine learning technique in our experiments. MLNs have recently received attention in language processing fields like co-reference resolution (Chen, 2009), semantic role labeling (Meza-Ruiz and Riedel, 2009), spoken (albeit neither situational nor incremental) NLU (Meurs et al., 2008), a"
W12-1643,E09-1081,1,0.89001,"Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 314–323, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics 2011)), which tries to infer linguistic structures automatically, but normally stops at generating, not interpreting semantic representations, and works with (the text of) full utterances and not incrementally on speech data; and incremental NLU, which is a less intensely studied field, but where previous contributions (such as (DeVault et al., 2009; Devault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009)) have not dealt with learned grounded semantics. We go beyond this earlier work in that we study a model that is incremental, can use linguistic structure, and learns from conversational data a semantics that connects the utterance to its visual and discourse context. We have looked at individual components of this before (grounded semantics in (Siebert and Schlangen, 2008); incremental reference resolution in (Schlangen et al., 2009); incremental general NLU in (Heintze et al., 2010); interaction between incremental parsing and reference resolution in (Peldszus et al., 2012)), but use a more"
W12-1643,W09-3905,1,0.903731,"tal NLU, which is a less intensely studied field, but where previous contributions (such as (DeVault et al., 2009; Devault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009)) have not dealt with learned grounded semantics. We go beyond this earlier work in that we study a model that is incremental, can use linguistic structure, and learns from conversational data a semantics that connects the utterance to its visual and discourse context. We have looked at individual components of this before (grounded semantics in (Siebert and Schlangen, 2008); incremental reference resolution in (Schlangen et al., 2009); incremental general NLU in (Heintze et al., 2010); interaction between incremental parsing and reference resolution in (Peldszus et al., 2012)), but use a more sophisticated model in this work and show that tackling these tasks jointly improves performance. Context/World Language/ RMRS MLN System Prediction: action object result Context/ Discourse Figure 1: NLU Data Flow We apply Markov Logic Networks (MLNs, (Richardson and Domingos, 2006)) as the machine learning technique in our experiments. MLNs have recently received attention in language processing fields like co-reference resolution (C"
W12-1643,W08-0113,1,0.820566,"full utterances and not incrementally on speech data; and incremental NLU, which is a less intensely studied field, but where previous contributions (such as (DeVault et al., 2009; Devault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009)) have not dealt with learned grounded semantics. We go beyond this earlier work in that we study a model that is incremental, can use linguistic structure, and learns from conversational data a semantics that connects the utterance to its visual and discourse context. We have looked at individual components of this before (grounded semantics in (Siebert and Schlangen, 2008); incremental reference resolution in (Schlangen et al., 2009); incremental general NLU in (Heintze et al., 2010); interaction between incremental parsing and reference resolution in (Peldszus et al., 2012)), but use a more sophisticated model in this work and show that tackling these tasks jointly improves performance. Context/World Language/ RMRS MLN System Prediction: action object result Context/ Discourse Figure 1: NLU Data Flow We apply Markov Logic Networks (MLNs, (Richardson and Domingos, 2006)) as the machine learning technique in our experiments. MLNs have recently received attention"
W12-1643,P09-1110,0,0.0575759,"attempt to abstract from this fact, however. They work in domains where physical co-location is not necessary, such as information look-up, and they quantize time into discrete turn units by endpointing utterances 2 Related Work and Background The work in this paper builds on, connects and extends several strands of research: grounded semantics (Roy, 2005), which worries about the connection between language and the situation in which it is used, but often does not go beyond the word level to include linguistic structure information and does not work incrementally;1 statistical NLU (see e.g. (Zettlemoyer and Collins, 2009; Liang et al., 1 But see (Spranger et al., 2010); for recent attempts that partially overcome these limitations. 314 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 314–323, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics 2011)), which tries to infer linguistic structures automatically, but normally stops at generating, not interpreting semantic representations, and works with (the text of) full utterances and not incrementally on speech data; and incremental NLU, which is a less intensely stud"
W12-1806,W10-4342,1,0.858948,"Missing"
W12-1806,W10-4308,1,0.838237,"alising a tight interaction loop, not strict turn-based exchanges. • Things will go wrong, so error handling needs to be graceful and natural, using the full range of conversational repair devices (Schlangen, 2004; Purver, 2004); including handing off tasks to other modalities if expected success rate is low. • Conversations express and project personality, emotionality, sociality; systems need to model the dynamics of this as part of their modelling of the conversation. Again, these are active areas of research (for responsive systems, see e.g. (Skantze and Schlangen, 2009; Buß et al., 2010; Schlangen et al., 2010); for error handling / acting under uncertainty, see e.g. (Williams and Young, 2007); for social aspects of dialogue, see e.g. (Kopp, 2010)); pulling them together in this kind of application will likely provide new challenges and insights for all of them. 4 ... Assistants Of course, the systems will need to provide actual services, for it at all to come to repeated conversations. While providing the services lies outside the domain of speech research, there are some unique requirements that conversational access poses: • To be usefully embeddable into conversational systems, back-end applicat"
W12-1806,W04-2325,1,0.760244,"ext, both from 11 NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 11–12, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguistics the ongoing conversation as well as from the common ground that was built up over previous interaction. • Systems should be responsive, incremental, providing feedback where required; realising a tight interaction loop, not strict turn-based exchanges. • Things will go wrong, so error handling needs to be graceful and natural, using the full range of conversational repair devices (Schlangen, 2004; Purver, 2004); including handing off tasks to other modalities if expected success rate is low. • Conversations express and project personality, emotionality, sociality; systems need to model the dynamics of this as part of their modelling of the conversation. Again, these are active areas of research (for responsive systems, see e.g. (Skantze and Schlangen, 2009; Buß et al., 2010; Schlangen et al., 2010); for error handling / acting under uncertainty, see e.g. (Williams and Young, 2007); for social aspects of dialogue, see e.g. (Kopp, 2010)); pulling them together in this kind of applicatio"
W12-1806,E09-1085,1,0.82773,"remental, providing feedback where required; realising a tight interaction loop, not strict turn-based exchanges. • Things will go wrong, so error handling needs to be graceful and natural, using the full range of conversational repair devices (Schlangen, 2004; Purver, 2004); including handing off tasks to other modalities if expected success rate is low. • Conversations express and project personality, emotionality, sociality; systems need to model the dynamics of this as part of their modelling of the conversation. Again, these are active areas of research (for responsive systems, see e.g. (Skantze and Schlangen, 2009; Buß et al., 2010; Schlangen et al., 2010); for error handling / acting under uncertainty, see e.g. (Williams and Young, 2007); for social aspects of dialogue, see e.g. (Kopp, 2010)); pulling them together in this kind of application will likely provide new challenges and insights for all of them. 4 ... Assistants Of course, the systems will need to provide actual services, for it at all to come to repeated conversations. While providing the services lies outside the domain of speech research, there are some unique requirements that conversational access poses: • To be usefully embeddable int"
W12-1814,N09-1043,1,0.869004,"Missing"
W12-1814,W10-4342,1,0.919449,"Missing"
W12-1814,W10-4302,1,0.847056,"remental synthesis of the same utterance versus the amount of look-ahead, that is, how far into the current phrase the next phrase becomes known. It shows that best results are achieved if the next phrase that is to be synthesized becomes known no later than one or two words into the current phrase (w0 or w1 ). 20 Incremental Speech Synthesis ● pitch dev. timing dev. ● 10 5 Figure 1: Hierarchic structure of incremental units describing an example utterance as it is being produced during delivery, showing the event-based just-in-time processing strategy. 0 trained ones (Schlangen et al., 2009; Heintze et al., 2010). We have recently built a somewhat more traditional NLU component which could be more easily ported to other domains (by adapting lexicon and grammar). It consists of a probabilistic, beam-search top-down parser (following (Roark, 2001)), which produces a principled semantic representation in the formalism robust minimal recursion semantics (Copestake, 2006). This component is described in more detail in (Peldszus et al., 2012). ● ● w0 w1 ● w2 ● ● ● w3 wn−1 wn Figure 2: Deviation of pitch and timing plotted against lookahead (right context available for incremental synthesis). The more lookah"
W12-1814,E12-1052,1,0.835942,"scribing an example utterance as it is being produced during delivery, showing the event-based just-in-time processing strategy. 0 trained ones (Schlangen et al., 2009; Heintze et al., 2010). We have recently built a somewhat more traditional NLU component which could be more easily ported to other domains (by adapting lexicon and grammar). It consists of a probabilistic, beam-search top-down parser (following (Roark, 2001)), which produces a principled semantic representation in the formalism robust minimal recursion semantics (Copestake, 2006). This component is described in more detail in (Peldszus et al., 2012). ● ● w0 w1 ● w2 ● ● ● w3 wn−1 wn Figure 2: Deviation of pitch and timing plotted against lookahead (right context available for incremental synthesis). The more lookahead available, the better the results. new considerations of dynamics into the assessment of processing quality, and hence requires additional metrics compared to non-incremental processing. In (Baumann et al., 2011) we have proposed a family of such metrics, and we provide an evaluation framework for analysing incremental ASR performance as part of our distribution. 7 Conclusions We have sketched the major features of our “Incr"
W12-1814,E09-1081,1,0.80657,"en, 2009; Buß et al., 2010; Skantze and Hjalmarsson, 2010). There is still much left to find out about the best ways of modelling these behaviours in such systems, however. To foster research in this area, we are releasing a new version of our “Incremental Processing Toolkit” (I NPROTK), which provides lower-level components (such as speech recognition and speech synthesis, 1 The code of the toolkit and some example applications have been released as open-source at http://inprotk. sourceforge.net. An Incremental Processing Architecture I NPROTK realises the IU-model of incremental processing (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011), where incremental systems are conceptualised as consisting of a network of processing modules. Each module has a left buffer, a processor, and a right buffer, where the normal mode of processing is to take input from the left buffer, process it, and provide output in the right buffer, from where it goes to the next module’s left buffer. (Topdown, expectation-based processing would work in the opposite direction.) Modules exchange incremental units (IUs), which are the smallest ‘chunks’ of information that can trigger connected modules into action. IUs typically"
W12-1814,W09-3905,1,0.865399,"g from that of a non-incremental synthesis of the same utterance versus the amount of look-ahead, that is, how far into the current phrase the next phrase becomes known. It shows that best results are achieved if the next phrase that is to be synthesized becomes known no later than one or two words into the current phrase (w0 or w1 ). 20 Incremental Speech Synthesis ● pitch dev. timing dev. ● 10 5 Figure 1: Hierarchic structure of incremental units describing an example utterance as it is being produced during delivery, showing the event-based just-in-time processing strategy. 0 trained ones (Schlangen et al., 2009; Heintze et al., 2010). We have recently built a somewhat more traditional NLU component which could be more easily ported to other domains (by adapting lexicon and grammar). It consists of a probabilistic, beam-search top-down parser (following (Roark, 2001)), which produces a principled semantic representation in the formalism robust minimal recursion semantics (Copestake, 2006). This component is described in more detail in (Peldszus et al., 2012). ● ● w0 w1 ● w2 ● ● ● w3 wn−1 wn Figure 2: Deviation of pitch and timing plotted against lookahead (right context available for incremental synt"
W12-1814,W10-4301,0,0.720813,"domain. We offer this release of the toolkit to foster research in this new and exciting area, which promises to help increase the naturalness of behaviours that can be modelled in such systems. 1 david.schlangen@uni-bielefeld.de 2 Introduction As recent work has shown, incremental (or online) processing of user input or generation of system output enables spoken dialogue systems to produce behaviour that is perceived as more natural than and preferable to that produced by systems that are bound by a turn-based processing mode (Aist et al., 2006; Skantze and Schlangen, 2009; Buß et al., 2010; Skantze and Hjalmarsson, 2010). There is still much left to find out about the best ways of modelling these behaviours in such systems, however. To foster research in this area, we are releasing a new version of our “Incremental Processing Toolkit” (I NPROTK), which provides lower-level components (such as speech recognition and speech synthesis, 1 The code of the toolkit and some example applications have been released as open-source at http://inprotk. sourceforge.net. An Incremental Processing Architecture I NPROTK realises the IU-model of incremental processing (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011),"
W12-1814,E09-1085,1,0.845269,"nt that are somewhat more tied to a particular domain. We offer this release of the toolkit to foster research in this new and exciting area, which promises to help increase the naturalness of behaviours that can be modelled in such systems. 1 david.schlangen@uni-bielefeld.de 2 Introduction As recent work has shown, incremental (or online) processing of user input or generation of system output enables spoken dialogue systems to produce behaviour that is perceived as more natural than and preferable to that produced by systems that are bound by a turn-based processing mode (Aist et al., 2006; Skantze and Schlangen, 2009; Buß et al., 2010; Skantze and Hjalmarsson, 2010). There is still much left to find out about the best ways of modelling these behaviours in such systems, however. To foster research in this area, we are releasing a new version of our “Incremental Processing Toolkit” (I NPROTK), which provides lower-level components (such as speech recognition and speech synthesis, 1 The code of the toolkit and some example applications have been released as open-source at http://inprotk. sourceforge.net. An Incremental Processing Architecture I NPROTK realises the IU-model of incremental processing (Schlange"
W12-1814,W09-3943,1,0.857334,"Missing"
W12-4705,N09-1043,1,0.88265,"Missing"
W12-4705,W12-1814,1,0.839173,"Missing"
W12-4705,briscoe-carroll-2002-robust,0,0.0538455,"Missing"
W12-4705,W07-1210,0,0.45812,"Missing"
W12-4705,2007.sigdial-1.25,1,0.868619,"Missing"
W12-4705,W10-4302,1,0.91188,"Missing"
W12-4705,W12-1643,1,0.888573,"Missing"
W12-4705,E09-1052,0,0.056605,"Missing"
W12-4705,E12-1052,1,0.882352,"Missing"
W12-4705,W11-0144,0,0.180089,"Missing"
W12-4705,N09-2014,0,0.0440387,"Missing"
W12-4705,W03-2106,1,0.850896,"Missing"
W12-4705,W09-3905,1,0.932747,"Missing"
W12-4705,E09-1081,1,0.927601,"Missing"
W12-4705,W10-4301,0,0.122177,"Missing"
W12-4705,E09-1085,1,0.924791,"Missing"
W12-4705,W07-1207,0,\N,Missing
W13-4030,W10-4342,1,0.920558,"Missing"
W13-4030,W09-3902,0,0.0218874,"aims to map an utterance to its meaning representation (using various routes and approaches, such as logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (MLN) (Meurs et al., 2008; Meza-Ruiz et al., 2008), and dynamic Bayesian networks (Meurs et al., 2009); see also overviews in (De Mori et al., 2008; Wang et al., 2011)), but typically neither provides situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situational grounding; integration of gaze into language understanding (Prasov and Chai, 2010), which was not incremental. We move beyond this work in that we present a model that is incremental, uses a form of grounded semantics, can easily incorporate multi-modal information sources, and finally on which inference can be performed quickly, satisfying the demands of real-time dialogue. The model brings together aspects we’ve previously looked into separately: grounded semantics"
W13-4030,2007.sigdial-1.25,1,0.858594,"Missing"
W13-4030,W12-1643,1,0.827976,"abstract way the task of the model. We describe our model formally in Section 4, followed by three experiments with the model, the first establishing it with a traditional 173 Proceedings of the SIGDIAL 2013 Conference, pages 173–182, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics 2008); incremental interpretation (reference resolution) in (Schlangen et al., 2009); incremental general NLU in (Heintze et al., 2010); and a more sophisticated approach that handled all of these using markov logic networks, but did not work in real-time or with multi-modal input (Kennington and Schlangen, 2012). 3 which represents the (visual or abstract) properties of the (visually present, or abstract) object of the intention. So, what we need to calculate is P (I|U, R), even though ultimately we’re interested only in P (I|U ). By definition of conditional probability, P (I|U, R) = P (I, U, R)∗P (U, R)−1 . We factorise P (I, U, R) as indicated in the following: The Task P (I|R, U ) = The task for our model is as follows: to compute at any moment a distribution over possible intentions (expressed as semantic frames), given the unfolding utterance and possibly information about the state of the worl"
W13-4030,W13-4048,1,0.491263,"Missing"
W13-4030,E09-1081,1,0.962858,"arious routes and approaches, such as logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (MLN) (Meurs et al., 2008; Meza-Ruiz et al., 2008), and dynamic Bayesian networks (Meurs et al., 2009); see also overviews in (De Mori et al., 2008; Wang et al., 2011)), but typically neither provides situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situational grounding; integration of gaze into language understanding (Prasov and Chai, 2010), which was not incremental. We move beyond this work in that we present a model that is incremental, uses a form of grounded semantics, can easily incorporate multi-modal information sources, and finally on which inference can be performed quickly, satisfying the demands of real-time dialogue. The model brings together aspects we’ve previously looked into separately: grounded semantics in (Siebert and Schlangen, Introduction Speech by necessity unfolds ov"
W13-4030,P11-1060,0,0.0161741,"feld University david.schlangen2 2 Related Work and Background The work presented in this paper connects and extends several areas of research: grounded semantics (Roy, 2005; Hsiao et al., 2008; Liu et al., 2012), which aims to connect language with the world, but typically does not work incrementally; semantic parsing / statistical natural language understanding (NLU), which aims to map an utterance to its meaning representation (using various routes and approaches, such as logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (MLN) (Meurs et al., 2008; Meza-Ruiz et al., 2008), and dynamic Bayesian networks (Meurs et al., 2009); see also overviews in (De Mori et al., 2008; Wang et al., 2011)), but typically neither provides situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situational grounding; integration of gaze into language understanding (Prasov and Chai, 2010), which w"
W13-4030,W09-3905,1,0.951619,"isual context. The model is trained on conversational data and can be used as an understanding module in an incremental, situated dialogue system. Our paper begins with related work and background and then specifies in an abstract way the task of the model. We describe our model formally in Section 4, followed by three experiments with the model, the first establishing it with a traditional 173 Proceedings of the SIGDIAL 2013 Conference, pages 173–182, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics 2008); incremental interpretation (reference resolution) in (Schlangen et al., 2009); incremental general NLU in (Heintze et al., 2010); and a more sophisticated approach that handled all of these using markov logic networks, but did not work in real-time or with multi-modal input (Kennington and Schlangen, 2012). 3 which represents the (visual or abstract) properties of the (visually present, or abstract) object of the intention. So, what we need to calculate is P (I|U, R), even though ultimately we’re interested only in P (I|U ). By definition of conditional probability, P (I|U, R) = P (I, U, R)∗P (U, R)−1 . We factorise P (I, U, R) as indicated in the following: The Task P"
W13-4030,W12-1621,0,0.0297478,"ounded (i.e., links to entities in the shared space). We describe our model with an example, then establish that our model works well on nonsituated, telephony application-type utterances, show that it is effective in grounding language in a situated environment, and further show that it can make good use of embodied cues such as gaze and pointing in a fully multi-modal setting. 1 David Schlangen Bielefeld University david.schlangen2 2 Related Work and Background The work presented in this paper connects and extends several areas of research: grounded semantics (Roy, 2005; Hsiao et al., 2008; Liu et al., 2012), which aims to connect language with the world, but typically does not work incrementally; semantic parsing / statistical natural language understanding (NLU), which aims to map an utterance to its meaning representation (using various routes and approaches, such as logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (MLN) (Meurs et al., 2008; Meza-Ruiz et al., 2008), and dynamic Bayesian networks (Meurs et al., 2009); see also overviews in (De M"
W13-4030,W08-0113,1,0.907576,"Missing"
W13-4030,W12-1624,0,0.056519,"Bayesian updating of belief (with a trivial, constant transition model that equates P (It−1 ) and P (It )).2 1 Here, no links between these intention representations are shown. The model we present in the next section is an update model, that is, it builds the representation at step tn based on that at tn−1 ; other possibilities are explored in (Heintze et al., 2010) and (Kennington and Schlangen, 2012). 2 In that sense, our incremental understanding could be called “intra-sentential belief tracking,” in analogy to the current effort to track system belief about user intentions across turns (Ma et al., 2012; Williams, 2010). 174 word the red ball The other models represent knowledge about links between intentions and object properties, P (R|I), and knowledge about language use, P (U |R). We now explain how this knowledge is acquired. red 0.03 0.82 0.02 round 0.02 0.009 0.9 square 0.02 0.09 0.02 green 0.02 0.01 0.07 Table 1: P (U |R) for our toy domain for some values of U and R; we assume that this model is learned from data (columns are excerpted from a distribution over a larger vocabulary). P(R|I) The model P (R|I) provides the link between objects (as occurring in the intentions) and their p"
W13-4030,D07-1071,0,0.0208056,"od use of embodied cues such as gaze and pointing in a fully multi-modal setting. 1 David Schlangen Bielefeld University david.schlangen2 2 Related Work and Background The work presented in this paper connects and extends several areas of research: grounded semantics (Roy, 2005; Hsiao et al., 2008; Liu et al., 2012), which aims to connect language with the world, but typically does not work incrementally; semantic parsing / statistical natural language understanding (NLU), which aims to map an utterance to its meaning representation (using various routes and approaches, such as logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (MLN) (Meurs et al., 2008; Meza-Ruiz et al., 2008), and dynamic Bayesian networks (Meurs et al., 2009); see also overviews in (De Mori et al., 2008; Wang et al., 2011)), but typically neither provides situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situatio"
W13-4030,P09-1110,0,0.0342287,"gaze and pointing in a fully multi-modal setting. 1 David Schlangen Bielefeld University david.schlangen2 2 Related Work and Background The work presented in this paper connects and extends several areas of research: grounded semantics (Roy, 2005; Hsiao et al., 2008; Liu et al., 2012), which aims to connect language with the world, but typically does not work incrementally; semantic parsing / statistical natural language understanding (NLU), which aims to map an utterance to its meaning representation (using various routes and approaches, such as logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (MLN) (Meurs et al., 2008; Meza-Ruiz et al., 2008), and dynamic Bayesian networks (Meurs et al., 2009); see also overviews in (De Mori et al., 2008; Wang et al., 2011)), but typically neither provides situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situational grounding; integration of ga"
W13-4030,E12-1052,1,0.802534,"Schlangen, 2012) for evaluation, but use a subset of their incremental metrics, with a modification on the edit overhead: first correct: how deep into the utterance do we make the first correct guess? first final: how deep into the utterance do we make the correct guess, and don’t subsequently change our minds? edit overhead: what is the ratio of unnecessary edits / sentence length, where the only necessary edit is that going from unknown to the final, Table 4: Comparison of results from Pento: Naive Bayes NB, Maximum Entropy ME, (Kennington and Schlangen, 2012) K, (Heintze et al., 2010) H, (Peldszus et al., 2012) P; values in parentheses denote results from automatically transcribed speech. 4 Results The results for full utterances are given in Table 4. Both of our model types work better than (Heintze et al., 2010) which used support vector machines and conditional random fields, and (Peldszus et al., 2012) which was rule-based (but did not include utterances with pronouns like we do here). The NB version did not work well in comparison to (Kennington and Schlangen, 2012) which used MLN, but the ME version did in most metrics. Overall these are nice results as they are achieved using a more straightf"
W13-4030,D10-1046,0,0.272038,"semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic Networks (MLN) (Meurs et al., 2008; Meza-Ruiz et al., 2008), and dynamic Bayesian networks (Meurs et al., 2009); see also overviews in (De Mori et al., 2008; Wang et al., 2011)), but typically neither provides situated interpretations nor incremental specifications of the representations; incremental NLU (DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which focuses on incrementality, but not on situational grounding; integration of gaze into language understanding (Prasov and Chai, 2010), which was not incremental. We move beyond this work in that we present a model that is incremental, uses a form of grounded semantics, can easily incorporate multi-modal information sources, and finally on which inference can be performed quickly, satisfying the demands of real-time dialogue. The model brings together aspects we’ve previously looked into separately: grounded semantics in (Siebert and Schlangen, Introduction Speech by necessity unfolds over time, and in spoken conversation, this time is shared between the participants. Speakers are also by necessity located, and in face-to-fa"
W13-4030,W10-4302,1,\N,Missing
W13-4030,H94-1010,0,\N,Missing
W13-4030,W12-1814,1,\N,Missing
W13-4042,P12-3018,1,0.853603,"ur, human speakers plan their utterances just somewhat ahead, typically in chunks of major phrases (Levelt, 1989), and remain flexible to change or abandon the original plan, or to hesitate, e. g. to adapt their timing. This flexibility is in contrast to speech output in spoken dialogue systems (SDSs) which typically generate, synthesize and deliver speech in units of full utterances that cannot be changed while ongoing, apart from being aborted or interrupted (Edlund, 2008). david.schlangen@uni-bielefeld.de Recently, incremental speech synthesis (iSS) has been presented (Dutoit et al., 2011; Baumann and Schlangen, 2012b) which allows to start partial utterances that are then smoothly extended during verbalization. Incremental spoken output for dialogue systems has been shown to improve naturalness (Buschmeier et al., 2012) and Skantze and Hjalmarsson (2010) have used filled pauses to hold a turn. Dethlefs et al. (2012) present an incremental NLG strategy to reduce the need for filled pauses in interactions. We investigate the impact of incremental spoken output in a highly dynamic environment, that is, where the rate of external events is high enough to allow only few utterances to finish as planned. As an"
W13-4042,W12-1641,1,0.73875,"timing. This flexibility is in contrast to speech output in spoken dialogue systems (SDSs) which typically generate, synthesize and deliver speech in units of full utterances that cannot be changed while ongoing, apart from being aborted or interrupted (Edlund, 2008). david.schlangen@uni-bielefeld.de Recently, incremental speech synthesis (iSS) has been presented (Dutoit et al., 2011; Baumann and Schlangen, 2012b) which allows to start partial utterances that are then smoothly extended during verbalization. Incremental spoken output for dialogue systems has been shown to improve naturalness (Buschmeier et al., 2012) and Skantze and Hjalmarsson (2010) have used filled pauses to hold a turn. Dethlefs et al. (2012) present an incremental NLG strategy to reduce the need for filled pauses in interactions. We investigate the impact of incremental spoken output in a highly dynamic environment, that is, where the rate of external events is high enough to allow only few utterances to finish as planned. As an example, we choose an otherwise simple commentary domain, where incremental output enables the system to combine multiple events into one complex commenting utterance that takes into account predictions about"
W13-4042,W12-1509,0,0.172688,"pically generate, synthesize and deliver speech in units of full utterances that cannot be changed while ongoing, apart from being aborted or interrupted (Edlund, 2008). david.schlangen@uni-bielefeld.de Recently, incremental speech synthesis (iSS) has been presented (Dutoit et al., 2011; Baumann and Schlangen, 2012b) which allows to start partial utterances that are then smoothly extended during verbalization. Incremental spoken output for dialogue systems has been shown to improve naturalness (Buschmeier et al., 2012) and Skantze and Hjalmarsson (2010) have used filled pauses to hold a turn. Dethlefs et al. (2012) present an incremental NLG strategy to reduce the need for filled pauses in interactions. We investigate the impact of incremental spoken output in a highly dynamic environment, that is, where the rate of external events is high enough to allow only few utterances to finish as planned. As an example, we choose an otherwise simple commentary domain, where incremental output enables the system to combine multiple events into one complex commenting utterance that takes into account predictions about upcoming events. If the system overcommits to the timing of future events, it autonomously uses a"
W13-4042,W12-2908,1,0.821078,"gnificant for questions (a) (68+/9=/4-; p &lt; .0001) and (b) (38+/30=/13-; p &lt; .0007)4 . Thus, it is safe to say that the production strategies enabled by incremental speech synthesis (i. e. starting to speak before all evidence is known and extending the utterance as information becomes available) allows for formulations in the spoken commentary that are favoured by human listeners. Incremental behaviour in the 3 scenarios that required hesitations was rated significantly worse than in those scenarios without hesitations for both questions (t-tests, p &lt; .001 (a) and p &lt; .01 (b)). This However, Lohmann et al. (2012) present an incremental NLG strategy for a similar task. 282 3 The experiment was conducted in one language (German) only, but we believe our results to carry over to other languages. Specifically, we assume that most or all languages cater for commenting, and believe that human commenters universally use their ability to integrate events late in the utterance. However, practices of commenting may work differently (and differently well) among languages. 4 We also conducted a non-paired t-test for question (b), as the different formulations of the systems might have effects on pronunciation qua"
W13-4042,E09-1081,1,0.852584,"o output the direction of the turn (the most important information) very shortly after the fact. A non-incremental system, in contrast, must output individual utterances for every event and utterances can only start after the fact. Furthermore, a non-incremental system cannot extend ongoing utterances, rendering turn-prep events useless. 5 Implemented System The system used for the experiment reported below uses an early version of incremental speech synthesis as implemented in I NPROTK (Baumann and Schlangen, 2012c), a toolkit for incremental spoken dialogue processing based on the IU model (Schlangen and Skantze, 2009). The system allows to extend ongoing utterances, enabling the 281 incremental commenting strategy outlined above. In addition, we implemented a capability to synthesize a hesitation if no more content is specified, and to continue as soon as content becomes available. (Thus, in contrast to (Skantze and Hjalmarsson, 2010), hesitations do not consume additional time.) By using hesitations, the system gracefully accommodates temporal over-commitment (i. e. the obligation to produce a continuation that is not fulfilled in time) which may occur, e. g. when the car drives slower than anticipated an"
W13-4042,W10-4301,0,0.295329,"in contrast to speech output in spoken dialogue systems (SDSs) which typically generate, synthesize and deliver speech in units of full utterances that cannot be changed while ongoing, apart from being aborted or interrupted (Edlund, 2008). david.schlangen@uni-bielefeld.de Recently, incremental speech synthesis (iSS) has been presented (Dutoit et al., 2011; Baumann and Schlangen, 2012b) which allows to start partial utterances that are then smoothly extended during verbalization. Incremental spoken output for dialogue systems has been shown to improve naturalness (Buschmeier et al., 2012) and Skantze and Hjalmarsson (2010) have used filled pauses to hold a turn. Dethlefs et al. (2012) present an incremental NLG strategy to reduce the need for filled pauses in interactions. We investigate the impact of incremental spoken output in a highly dynamic environment, that is, where the rate of external events is high enough to allow only few utterances to finish as planned. As an example, we choose an otherwise simple commentary domain, where incremental output enables the system to combine multiple events into one complex commenting utterance that takes into account predictions about upcoming events. If the system ove"
W13-4042,W12-1814,1,\N,Missing
W13-4048,W13-4030,1,0.444006,"board was created. We denote the time-span from the creation of a board to the acknowledgement by the subject that the correct piece was selected an episode. The wizard had the option to not immediately highlight the indicated piece, in order to elicit a more detailed description of the piece or a pointing gesture. What we were interested in learning from these data was whether speaker gaze and arm movements could be turned into signals that can support a model of situated language understanding. We focus here on the signal processing and analysis that was required; the model is described in (Kennington et al., 2013). ing one HD camera. The AV channel is synchronised with the stream data from the sensors by means of a timecode in view of the camera. Representative of the high modularity and flexibility of the mint.tools architecture is the ease with which components can be added. For the setting described here, a GUI was created which connects to the VR environment as an additional sensor, transmitting all of its state updates, which then are synchronously logged together with all other stream data from the trackers. This allows us to recreate the full scene (subject behaviour and the stimuli they receive"
W13-4048,W12-1604,0,0.0277729,"ames (tables with extended database functionality), or compatible formats such as Praat TextGrids (Boersma and Weenink, 2013) and ELAN tiers. In addition, mumodo.py can remote-control playback in ELAN and Instant Reality for the purpose of data viewing and annotation. 3 4.1 4 Procedure / The TAKE Corpus Analysis and Results Gaze Our post-processing and analysis of the gaze data focuses primarily on the detection of eye fixations in order to determine the pentomino pieces that the subjects look at while speaking. This knowledge is interesting from a reference resolution point of view. Although Koller et al (2012) explored listener gaze in that context, it is known that gaze patterns differ in interactions, depending on whether one speaks or listens (Jokinen et al., 2009). Facelab provides a mapping between a person’s gaze vector and the screen, which yields an intersection point in pixel coordinates. However, due to limitations to the accuracy of the calibration procedure and noise in the data, it is posOur experiment is a Wizard-of-Oz scenario in which subjects (7 in total) were situated in front of a 40” screen displaying random Pentomino boards (Fern´andez et al., 2007). Each board configuration ha"
W14-0212,W12-1814,1,0.759106,"d the OpenDS Toolkit,1 connected to a steering wheel and a board with an acceleration and brake pedal, using standard video game hardware. We developed our own simple driving scenarios (derived from the “ReactionTest” task, which is distributed together with OpenDS) that specified the driving task and timing of the concurrent speech, as described below. We modified OpenDS to pass real-time data (e.g. car position/velocity/events in the simulation, such as a gate becoming visible or a lane change) using the mint.tools architecture (Kousidis et al., 2013). In addition, we have bridged I NPROTK (Baumann and Schlangen, 2012) with mint.tools via the Robotics Service Bus (RSB, Wienke and Wrede (2011)) framework. 1 http://www.opends.eu/ 68 Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 68–72, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Table 1: Experiment conditions. Dialogue System Using I NPROTK, we implemented a simple dialogue system. The notion of “dialogue” is used with some liberty here: the user did not interact directly with the system but rather indirectly (and non-intentionally) via driving actions. Nevertheless, we used the same m"
W14-0212,W12-1641,1,0.846215,"user did not interact directly with the system but rather indirectly (and non-intentionally) via driving actions. Nevertheless, we used the same modularisation as in more typical dialogue systems by using a dialoge management (DM) component that controls the system actions based on the user actions. We integrated OpenDial (Lison, 2012) as the DM into I NPROTK,2 though we only used it to make simple, deterministic decisions (there was no learned dialogue policy) based on the state of the simulator (see below). We used the incremental output generation capabilities of I NPROTK, as described in (Buschmeier et al., 2012). We evaluated the adaptation strategy in a driving simulation setup, where subjects performed a 30 minute, simulated drive along a straight, five-lane road, during which they were occasionally faced with two types of additional tasks: a lane-change task and a memory task, which aim to measure the driving performance and the driver’s ability to pay attention to speech while driving, respectively. The two tasks occured in isolation or simultaneoulsy. The Lane-Change Task The driving task we used is a variant of the well-known lane-change task (LCT), which is standardised in (ISO, 2010): It requ"
W14-0212,W12-1625,0,0.132789,"on (DM), pages 68–72, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Table 1: Experiment conditions. Dialogue System Using I NPROTK, we implemented a simple dialogue system. The notion of “dialogue” is used with some liberty here: the user did not interact directly with the system but rather indirectly (and non-intentionally) via driving actions. Nevertheless, we used the same modularisation as in more typical dialogue systems by using a dialoge management (DM) component that controls the system actions based on the user actions. We integrated OpenDial (Lison, 2012) as the DM into I NPROTK,2 though we only used it to make simple, deterministic decisions (there was no learned dialogue policy) based on the state of the simulator (see below). We used the incremental output generation capabilities of I NPROTK, as described in (Buschmeier et al., 2012). We evaluated the adaptation strategy in a driving simulation setup, where subjects performed a 30 minute, simulated drive along a straight, five-lane road, during which they were occasionally faced with two types of additional tasks: a lane-change task and a memory task, which aim to measure the driving perfor"
W14-4312,W13-4048,1,0.912934,"sensors, regardless of the platform or language. Venice.IPC acts as a server to which TCP clients (a common interface for sensors) can connect. It is highly configurable, readily accepting various sensor data outputs, and sends data in real-time to the InstantIO network. Both Venice components operate on all three major platforms (Linux, Windows, Mac OS X), allowing great flexibility in software and sensors that can be plugged in the architecture, regardless of the vendor’s native API programming language or supported platform. We discuss some use cases in section 5. 3.3 4 InstantReality In (Kousidis et al., 2013), the InstantReality framework, a virtual reality environment, was used for monitoring and recording data in a realtime multimodal interaction.3 Each information source (sensor) runs on its own dedicated workstation and transmits the sensor data across a network using the InstantIO interface. The data can be received by different components such as InstantPlayer (3D visualization engine; invaluable for monitoring of data integrity when recording experimental sessions) or a logger that saves all data to disk. Network communication is achieved via multicast, which makes it possible to have any n"
W14-4312,W14-0212,1,0.736996,"are treated as single increments. Third, n-best lists can only be obtained when the API detects the end of the utterance (incrementally, only the top hypothesis is returned). Fourth, the results have a crude timestamp which signifies the end of the audio segment. We use this timestamp in our construction of word IUs, which in informal tests have been found to be acceptable for our needs; we defer more systematic testing to future work. Figure 4: Participant performing driving test while listening to iNLG speech delivered by InProTKS . by Venice.HUB. The results of this study are published in (Kousidis et al., 2014). Having available the modules described here made it surprisingly straightforward to implement the interaction with the driving simulator (treated as a kind of sensor). Real-time gaze fixation and pointing gesture detection Using the tools described here, we have recently tested a real-time situated communication environment that uses speech, gaze, and gesture simultaneously. Data from a Microsoft Kinect and a Seeingmachines Facelab eye tracker are logged in realtime to the InstantIO network. A Venice.HUB component receives this data and sends it over RSB to external components that perform d"
W14-4312,W12-1625,0,0.0320292,"Missing"
W14-4312,E09-1081,1,0.857907,"the use in I NPROTK of the Google Web Speech API, which offers speech recognition with a very large vocabulary and a wide choice of languages. We illustrate the use of these extensions with a description of two systems handling different situated settings. 1 Introduction Realising incremental processing of speech inand output – a prerequisite to interpretation and possibly production of speech concurrently with the other dialogue participant – requires some fundamental changes in the way that components of dialogue systems operate and communicate with each other (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009). Processing situated communication, that is, communication that requires reference to the physical setting in which it occurs, makes it necessary to accept (and fuse) information from various different sensors, each tracking different aspects of the physical situation, making the system multimodal (Atrey et al., 2010; Dumas et al., 2009; Waibel et al., 1996). Incremental situated processing brings together these requirements. In this paper, we present a collection of extensions to the incremental processing toolkit I NPROTK (Baumann and Schlangen, 2012) that make it capable of processing situ"
W14-4312,W12-1814,1,\N,Missing
W15-0124,D13-1134,0,0.0914528,"ented as a set of concepts (e.g., shape type), and U was represented by the words in the REs, in an interctive human-human setting. The Bayesian network was used to learn a mapping between concepts and U . The model could handle various types of REs, namely definite references, exophoric pronoun references, and deictic (pointing) references to objects. Similar data was used in Iida et al. (2011), but the mapping between U and W was done with a support vector machine classifier. We recently applied our generative model to this data, with improved results in some areas Kennington et al. (2015). Engonopoulos et al. (2013) also used a generative approach; W was modeled as an observation model (i.e., a set of features over the objects in a 3D scene), and U was a semantic model that abstracted over the referring expression. In Matuszek et al. (2014), W was represented as a distribution over properties (e.g., color and shape) of real-world objects (small wooden blocks of various shapes and colors) as represented by computervision output. U was represented as a semantic abstraction in the form of a Combinatory Categorical Grammar parse. Resolving I amounted to generatively computing a joint distribution over the re"
W15-0124,W12-1633,0,0.160197,"rance U was represented by its words. We repeated the experiments later in Kennington et al. (2013) where the utterance and world were represented in the same way, but the model that produced the distribution over the candidate objects was generative; it modeled the joint distribution over the objects and their properties, and the words in the utterance. (This model will be further discussed and used as a baseline for comparison below.) In Kennington et al. (2014); Hough et al. (2015) we used that same generative model and representation of W , but U was represented as a semantic abstraction. Funakoshi et al. (2012) used a Bayesian network approach. The world W (in their case, a set of tangram puzzle pieces) was represented as a set of concepts (e.g., shape type), and U was represented by the words in the REs, in an interctive human-human setting. The Bayesian network was used to learn a mapping between concepts and U . The model could handle various types of REs, namely definite references, exophoric pronoun references, and deictic (pointing) references to objects. Similar data was used in Iida et al. (2011), but the mapping between U and W was done with a support vector machine classifier. We recently"
W15-0124,I11-1010,0,0.136519,"generative model and representation of W , but U was represented as a semantic abstraction. Funakoshi et al. (2012) used a Bayesian network approach. The world W (in their case, a set of tangram puzzle pieces) was represented as a set of concepts (e.g., shape type), and U was represented by the words in the REs, in an interctive human-human setting. The Bayesian network was used to learn a mapping between concepts and U . The model could handle various types of REs, namely definite references, exophoric pronoun references, and deictic (pointing) references to objects. Similar data was used in Iida et al. (2011), but the mapping between U and W was done with a support vector machine classifier. We recently applied our generative model to this data, with improved results in some areas Kennington et al. (2015). Engonopoulos et al. (2013) also used a generative approach; W was modeled as an observation model (i.e., a set of features over the objects in a 3D scene), and U was a semantic model that abstracted over the referring expression. In Matuszek et al. (2014), W was represented as a distribution over properties (e.g., color and shape) of real-world objects (small wooden blocks of various shapes and"
W15-0124,N15-1031,1,0.80524,"puzzle pieces) was represented as a set of concepts (e.g., shape type), and U was represented by the words in the REs, in an interctive human-human setting. The Bayesian network was used to learn a mapping between concepts and U . The model could handle various types of REs, namely definite references, exophoric pronoun references, and deictic (pointing) references to objects. Similar data was used in Iida et al. (2011), but the mapping between U and W was done with a support vector machine classifier. We recently applied our generative model to this data, with improved results in some areas Kennington et al. (2015). Engonopoulos et al. (2013) also used a generative approach; W was modeled as an observation model (i.e., a set of features over the objects in a 3D scene), and U was a semantic model that abstracted over the referring expression. In Matuszek et al. (2014), W was represented as a distribution over properties (e.g., color and shape) of real-world objects (small wooden blocks of various shapes and colors) as represented by computervision output. U was represented as a semantic abstraction in the form of a Combinatory Categorical Grammar parse. Resolving I amounted to generatively computing a jo"
W15-0124,W13-4030,1,0.951714,"ed to each entity represents the strength of belief that it is the referred one. The referred object is then the argmax of that distribution: I ∗ = argmax P (I|U, W ) I (2) We have worked in this area before. Kennington and Schlangen (2013) we applied Markov Logic Networks (Richardson and Domingos, 2006) to the task of computing the distribution over I. The world W , a virtual game board of puzzle pieces, was represented symbolically (e.g., objects were represented by their properties such as colour and shape). The utterance U was represented by its words. We repeated the experiments later in Kennington et al. (2013) where the utterance and world were represented in the same way, but the model that produced the distribution over the candidate objects was generative; it modeled the joint distribution over the objects and their properties, and the words in the utterance. (This model will be further discussed and used as a baseline for comparison below.) In Kennington et al. (2014); Hough et al. (2015) we used that same generative model and representation of W , but U was represented as a semantic abstraction. Funakoshi et al. (2012) used a Bayesian network approach. The world W (in their case, a set of tang"
W15-0124,C14-1170,1,0.782507,"a virtual game board of puzzle pieces, was represented symbolically (e.g., objects were represented by their properties such as colour and shape). The utterance U was represented by its words. We repeated the experiments later in Kennington et al. (2013) where the utterance and world were represented in the same way, but the model that produced the distribution over the candidate objects was generative; it modeled the joint distribution over the objects and their properties, and the words in the utterance. (This model will be further discussed and used as a baseline for comparison below.) In Kennington et al. (2014); Hough et al. (2015) we used that same generative model and representation of W , but U was represented as a semantic abstraction. Funakoshi et al. (2012) used a Bayesian network approach. The world W (in their case, a set of tangram puzzle pieces) was represented as a set of concepts (e.g., shape type), and U was represented by the words in the REs, in an interctive human-human setting. The Bayesian network was used to learn a mapping between concepts and U . The model could handle various types of REs, namely definite references, exophoric pronoun references, and deictic (pointing) referenc"
W15-0124,W12-1643,1,0.858031,"Missing"
W15-0124,W13-4048,1,0.912845,"ed (e.g., using syntactic structure); we leave exploring them to future work. P (I = i|U1k , W ) = [P (I = i|U1k−1 , W ) + P (I = i|U k , W )] ∗ 198 1 2 (5) 3.2 Evidence from Gaze and Deixis The full model combines the evidence from linguistic information with evidence from other information sources such as the speaker’s gaze and pointing gestures. For each, we calculate a reference point (R) on the scene: for gaze, the fixated point as provided by an eye tracker; for deixis, the point on the scene that was pointed at based on a vector calculated from the shoulder to the hand (as described in Kousidis et al. (2013), using the Microsoft Kinect). The centroids of all the objects (I) can then be compared to that reference point to yield a probability of that object being ‘referred’ by that modality (i.e., gazed at or pointed at) by introducing a Gaussian window over the location of the point: pdistance (Ri , Ij ; σ) = exp − (xi − xj )2 (yi − yj )2 ∗ exp − 2 ∗ σ2 2 ∗ σ2 (6) where the mean is R and σ is set by calculating the standard deviation of all the object centroids and the reference point. This can then be normalised over all the pdistance scores to produce a distribution over I for each modality wher"
W15-0124,W09-3905,1,0.864261,"Missing"
W15-0124,J98-2001,0,\N,Missing
W15-0124,W15-0125,1,\N,Missing
W15-0125,P00-1001,0,0.0793472,"met by current approaches, comparing two incremental semantic processing frameworks: Dynamic Syntax enriched with Type Theory with Records (DS-TTR) and Robust Minimal Recursion Semantics with incremental processing (RMRS-IP). We conclude these approaches are not significantly different with regards to their semantic representation construction, however their purported role within semantic models and dialogue models is where they diverge. 1 Introduction It is now uncontroversial that dialogue participants construe meaning from utterances on at least as finegrained a level as word-by-word (see Brennan, 2000; Schlesewsky and Bornkessel, 2004, inter alia). It has also become clear that using more fine-grained incremental processing allows more likeable and interactive systems to be designed (Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). Despite these encouraging results, it has not been clearly stated which elements of incremental semantic frameworks, either formally or implementationally, are desirable for dialogue models and systems; this paper intends to spell these requirements out clearly. 1.1 The need for incremental semantics in situated dialogue While traditional computation"
W15-0125,W14-1405,0,0.0239113,"ations The partial representations constructed must be evaluable in a consistent way in a given interpretation system. This applies to all examples (1)-(10): for example if the robot responds appropriately before an instruction is over as in (1) it must have computed a meaning representation to the effect this is a taking event early in parsing. In recent type-theoretic approaches in computational semantics this kind of valuation is possible if semantic representations are considered types in a type system: inference can be characterized as subtype relation checking either by theorem proving (Chatzikyriakidis and Luo, 2014) or by checking the existence and ordering relations of types on a model (partial order) of types (Hough and Purver, 2014).1 Incremental predictivity Related to subsumption is monotonicity (in the sense of monotonic entailment in logic). In general, one would not want the valuation function after the first word to return more specific information than that returned after the second word, nor at the second word evaluate expressions as having a true value which were evaluated as false after the first word, and so on. In general, the total information made available after having consumed a new wo"
W15-0125,W07-1210,0,0.0842465,"e introduced without fixed arity and arguments can be introduced without knowing which predicates they are arguments of. RMRS-IP makes use of this form of underspecification by enriching lexical predicates with arguments incrementally– see the right of Figure 5. Combining two RMRS structures involves at least joining their list of EPs and ARGRELs and of scope constraints. Additionally, equations between the variables can connect two structures, which is an essential requirement for semantic construction. A semantic algebra for the combination of RMRSs in a non-lexicalist setting is defined in Copestake (2007). Unsaturated semantic increments have open slots that need to be filled by what is called the hook of another structure. Hook and slot are triples [ℓ:a:x] consisting of a label, an anchor and an index variable. Every variable of the hook is equated with the corresponding one in the slot. This way the semantic representation can grow monotonically at each combinatory step by simply adding predicates, constraints and equations. RMRS-IP extends Copestake (2007) in the organisation of the slots to meet the requirement of strong incremental interpretation, constructing a proper semantic representa"
W15-0125,W13-2611,1,0.759933,"els, while DS-TTR has been used as a dialogue model itself, given DS-TTR’s output of RTs, other popular models of dialogue can interface with it, most notably KoS (Ginzburg, 2012). RMRS-IP is well positioned to interface with a variety of formalisms that use FOL, and again, 213 well-founded logical inference in these models puts it at an advantage. Computational properties Un-enriched PCFGs have well studied information-theoretic properties and complexity, and are learnable from data, however DS-TTR semantic grammars have been proven to be learnable with semantic targets for short utterances (Eshghi et al., 2013), which has not been attempted yet in RMRS-IP. We discuss both formalisms’ semantic construction stability below. 4.1 Implementation comparison: Reference Resolution task performance We also compare the frameworks’ current parsing implementations in a real-world inference task contingent on the desiderata. This was done in an incremental reference resolution (RR) task using Kennington et al. (2013)’s statistical SIUM model, which learns to associate words (or in our case, semantic representations) with properties belonging to objects in a virtual scene. Both semantic grammars were hand-crafted"
W15-0125,W14-1410,1,0.85158,"s to all examples (1)-(10): for example if the robot responds appropriately before an instruction is over as in (1) it must have computed a meaning representation to the effect this is a taking event early in parsing. In recent type-theoretic approaches in computational semantics this kind of valuation is possible if semantic representations are considered types in a type system: inference can be characterized as subtype relation checking either by theorem proving (Chatzikyriakidis and Luo, 2014) or by checking the existence and ordering relations of types on a model (partial order) of types (Hough and Purver, 2014).1 Incremental predictivity Related to subsumption is monotonicity (in the sense of monotonic entailment in logic). In general, one would not want the valuation function after the first word to return more specific information than that returned after the second word, nor at the second word evaluate expressions as having a true value which were evaluated as false after the first word, and so on. In general, the total information made available after having consumed a new word should entail the information inferred by the prefix consumed before it is processed– see the top level in Figure 1. Ho"
W15-0125,W13-4030,1,0.826511,"Missing"
W15-0125,E12-1052,1,0.833256,"igure 2. Briefly, in DS-TTR generation (Hough and Purver, 2012), surface realisation is done by generating from a goal TTR RT concept. This requires a notion of subsumption which is given by the TTR subtype relation. Generation is driven by parsing and subtype relation checking the goal concept against each tree’s top node RT, and consequently meets the desideratum of interchangeability between parsing and generation described above. 3.2 RMRS-IP While DS-TTR treats both syntactic and semantic construction as one process, Robust Minimal Recursion Semantics with incremental processing (RMRS-IP, Peldszus et al., 2012) splits the task into a topdown PCFG parse followed by the construction of RMRS (Copestake, 2006) formulae using semantic construction rules, operating strictly word-by-word. The current RMRS-IP implementation uses standard top-down non-lexicalised PCFG parsing in the style of Roark (2001), however uses left-factorization of 211 the standard PCFG grammar rules to delay certain structural decisions as long as possible, employing a beam search over possible parses. Logical RMRS forms are built up by semantic construction actions operating on the derived CFG trees. In RMRS, meaning representation"
W15-0125,W11-0144,1,0.875925,"is done more than once. Top-down parsing approaches such as Roark (2001) also have this property. Well-founded information and probability theoretic properties For training automatic systems, well-understood information theoretic properties of the semantic construction process aid induction of rules from data. This relies on a well understood probability model of the framework in terms of its distributions of structures and update rules. We now describe two current incremental semantic parsing frameworks to illustrate how the above desiderata are met. 3 Two Current Attempts 3.1 DS-TTR DS-TTR (Purver et al., 2011) integrates Type Theory with Records (TTR, Cooper, 2005) record type (‘RT’ largely from now on) representations with the inherently incremental grammar formalism Dynamic Syntax (DS, Kempson et al., 2001) to provide word-by-word semantic construction. DS-TTR is an action-driven interpretation formalism which has no layer of syntax independent of semantic construction. The trees such as Figure 2 are constructed monotonically through sequences of tree-building 210 ELSE ?T y(e) put(T  y(e))  put( x = john : e ) abort  T y(e),  x=john : e head=x : e : : : :  e es   t  t T y(e → t),   λr :"
W15-0125,J93-1008,0,0.340625,"ng robustness while preserving monotonicity for each interpretation requires allowing multiple parse paths due to possible lexical and structural ambiguity, most notably in ‘garden path’ sentences, and so the output of a semantic parser can update its output non-monotonically, so long as there is a good notion of predictivity of future states in time afforded by the semantic model. Interface and consistency with well-founded reasoning system Well studied logical inference systems like FOL may not be adequate for natural language inference, as evidenced by the logical form equivalence problem (Shieber, 1993).2 Having said this, consistent logical systems should be in place which reason with the representations. 2.3 Dialogue properties Incremental illocutionary information Where available syntactically and lexically, information about the type of dialogue move, or illocutionary effects the utterance causes should be made available as soon as possible, as evidenced by (1), in support of Ginzburg (2012)’s approach. This may not generally be lexicalised, and therefore appropriate underspecification should be used instead to interface with the dialogue model. Also, closely related to strong incrementa"
W15-0125,W10-4301,0,0.0235509,"conclude these approaches are not significantly different with regards to their semantic representation construction, however their purported role within semantic models and dialogue models is where they diverge. 1 Introduction It is now uncontroversial that dialogue participants construe meaning from utterances on at least as finegrained a level as word-by-word (see Brennan, 2000; Schlesewsky and Bornkessel, 2004, inter alia). It has also become clear that using more fine-grained incremental processing allows more likeable and interactive systems to be designed (Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). Despite these encouraging results, it has not been clearly stated which elements of incremental semantic frameworks, either formally or implementationally, are desirable for dialogue models and systems; this paper intends to spell these requirements out clearly. 1.1 The need for incremental semantics in situated dialogue While traditional computational semantics models the meaning of complete sentences, for interaction this is insufficient for achieving the construction of meaning in real time as linguistic information is processed. The motivation for incremental semantics becomes clear in s"
W15-0125,E09-1085,1,0.807315,"ntal processing (RMRS-IP). We conclude these approaches are not significantly different with regards to their semantic representation construction, however their purported role within semantic models and dialogue models is where they diverge. 1 Introduction It is now uncontroversial that dialogue participants construe meaning from utterances on at least as finegrained a level as word-by-word (see Brennan, 2000; Schlesewsky and Bornkessel, 2004, inter alia). It has also become clear that using more fine-grained incremental processing allows more likeable and interactive systems to be designed (Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). Despite these encouraging results, it has not been clearly stated which elements of incremental semantic frameworks, either formally or implementationally, are desirable for dialogue models and systems; this paper intends to spell these requirements out clearly. 1.1 The need for incremental semantics in situated dialogue While traditional computational semantics models the meaning of complete sentences, for interaction this is insufficient for achieving the construction of meaning in real time as linguistic information is processed. The motivation for incremen"
W15-4705,P11-2040,0,0.0289975,"ity. Compared to the large body of work on automatic evaluation measures, there has been little research that assessed the validity of human evaluation methods. Hardcastle and Scott (2008) provided an extensive discussion of human and automatic evaluation for text quality. They proposed a Turing-style test where participants are asked to judge whether a text was generated by a computer or written by a human. Belz and Kow (2010) showed that higher agreement between human raters can be obtained if they compare two automatically generated texts, instead of assigning scores to texts in isolation. Belz and Kow (2011) found that human judges preferred to use continuous rating scales over discrete rating scales. Siddharthan and Katsos (2012) investigated two offline measures inspired from psycholinguistic studies of sentence processing for assessing text readability, namely magnitude estimation and sentence recall. They demonstrate that the sentence recall method did not discriminate well between sentences of differing fluency if sentences were short. On the other hand, human judgements, did not discriminate well between surface level disfluencies and breakdowns in comprehension. 3 is covered by a mask or m"
W15-4705,W11-2832,0,0.0951389,"icult. Many aspects of linguistic well-formedness and naturalness play a role for assessing the quality of an automatically generated text. On the sentence-level, this includes grammatical and morpho-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradig"
W15-4705,P09-2025,0,0.139749,"ty of an automatically generated text. On the sentence-level, this includes grammatical and morpho-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text"
W15-4705,E14-1074,0,0.0577951,"Missing"
W15-4705,W13-2111,0,0.0140876,"effects were accounted for by the objective reading measures that are (mostly) outside of conscious control. Section 2 provides background on research in NLG evaluation. Section 3 introduces our MCR paradigm. The generation framework we used to collect our experimental material is presented in Section 4. Section 5 describes the experimental design. The models are discussed in Section 6. 2 Background on NLG Evaluation In recent years, the NLG community has become increasingly interested in comparative evaluation between NLG systems (Gatt and Belz, 2010; Koller et al., 2010; Belz et al., 2011; Banik et al., 2013; Hastie and Belz, 2014). Generally, evaluation methods for assessing NLG systems fall into three main categories: 1) automatic evaluation methods that compare system output against one or multiple reference texts, 2) human evaluation methods where human readers are asked to judge a text, typically with respect to several criteria. If the NLG component is embedded in an end-to-end system, such as a dialogue system, 3) extrinsic factors of task success and usefulness of the NLG output can be measured. For corpus-based NLG components such as surface realisers or referring expression generators,"
W15-4705,P14-2074,0,0.112985,"atically generated text. On the sentence-level, this includes grammatical and morpho-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text reading. In particular, psy"
W15-4705,W10-4201,0,0.186019,"s have to reflect on and differentiate between detailed, linguistic aspects of text quality, and assign scores precisely and systematically across a set of generated outputs that potentially contain various types of linguistic defects. The rating task turns increasingly difficult if they have to compare texts with multiple sentences and multiple types of linguistic defects, e.g. fluency on the sentence level, clarity and coherence on the text level. Consequently, low agreement between raters, and even inconsistencies between ratings of the same human judge have been found in previous studies (Belz and Kow, 2010; Cahill and Forst, 2010; Dethlefs et al., 2014). Standard evaluation methods for, e.g. text summarisation tend to avoid possible interactions between local sentence-level and global text-level defects. Instead, they focus on coherence and content (Nenkova, 2006; Owczarzak et al., 2012). In particular, this is due to the fact that independently rating coherence and clarity locally for each sentence and globally for an entire text is tedious, unnatural, tiring and hardly achievable for human judges. Typically, human evaluation of NLG output is based on user ratings. We collected ratings and rea"
W15-4705,W08-1113,0,0.0286819,"LEU (Papineni et al., 2002) or NIST (Doddington, 2002), that measure the n-gram overlap between the system and some reference text, sentence or phrase. The advantage of such automatic and cheap evaluation methods can be enormous. If tightly integrated in the development cycle of an NLG system, they allow fast and empirically optimised implementation decisions. In turn, a lot of research on NLG evaluation focussed on defining and validating automatic evaluation measures. Such a metric is typically considered valid if it correlates well with human judgements of text quality (Stent et al., 2005; Foster, 2008; Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014). However, automatic evaluation measures in NLG still have a range of known conceptual deficits, i.e. they do not reflect appropriateness of content (Reiter and Belz, 39 2009), or meaning (Stent et al., 2005). Thus, many studies and evaluation challenges in NLG additionally collect human ratings to assess the quality. Compared to the large body of work on automatic evaluation measures, there has been little research that assessed the validity of human evaluation methods. Hardcastle and Scott (2008) provided an extensive discussion"
W15-4705,hardcastle-scott-2008-evaluate,0,0.122947,"nerated texts. This method combines the sensitivity of eye tracking with the cost effectiveness of a rating study. The automatically generated texts are presented to human raters in a sentence-by-sentence, mouse-contingent way such that a number of parameters of the reading process are recorded, e.g. the time that people spent looking at single sentences and an entire text. We hypothesized that these parameters are more informative for the quality of a text than the user ratings of clarity and fluency. As objective criteria for text quality are hardly available in NLG (Dale and Mellish, 1998; Hardcastle and Scott, 2008), we did not compare reading times and ratings on manual, potentially flawed annotations of text quality. Instead, we selected experimental material from a corpus-based generation framework that combines sentence-level linearisation and text-level referring expression generation (Zarrieß and Kuhn, 2013). We based our study on a set of texts that were available in 3 versions: (i) the “gold standard” corpus text, (ii) automatically linearised texts where word order deviated from the original corpus and contained potential fluency-related defects, (iii) texts with potential defects in referring e"
W15-4705,hastie-belz-2014-comparative,0,0.261096,"ted for by the objective reading measures that are (mostly) outside of conscious control. Section 2 provides background on research in NLG evaluation. Section 3 introduces our MCR paradigm. The generation framework we used to collect our experimental material is presented in Section 4. Section 5 describes the experimental design. The models are discussed in Section 6. 2 Background on NLG Evaluation In recent years, the NLG community has become increasingly interested in comparative evaluation between NLG systems (Gatt and Belz, 2010; Koller et al., 2010; Belz et al., 2011; Banik et al., 2013; Hastie and Belz, 2014). Generally, evaluation methods for assessing NLG systems fall into three main categories: 1) automatic evaluation methods that compare system output against one or multiple reference texts, 2) human evaluation methods where human readers are asked to judge a text, typically with respect to several criteria. If the NLG component is embedded in an end-to-end system, such as a dialogue system, 3) extrinsic factors of task success and usefulness of the NLG output can be measured. For corpus-based NLG components such as surface realisers or referring expression generators, extrinsic factors cannot"
W15-4705,J09-4008,0,0.59262,"for assessing the quality of an automatically generated text. On the sentence-level, this includes grammatical and morpho-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to asse"
W15-4705,W12-2203,0,0.138468,"sed the validity of human evaluation methods. Hardcastle and Scott (2008) provided an extensive discussion of human and automatic evaluation for text quality. They proposed a Turing-style test where participants are asked to judge whether a text was generated by a computer or written by a human. Belz and Kow (2010) showed that higher agreement between human raters can be obtained if they compare two automatically generated texts, instead of assigning scores to texts in isolation. Belz and Kow (2011) found that human judges preferred to use continuous rating scales over discrete rating scales. Siddharthan and Katsos (2012) investigated two offline measures inspired from psycholinguistic studies of sentence processing for assessing text readability, namely magnitude estimation and sentence recall. They demonstrate that the sentence recall method did not discriminate well between sentences of differing fluency if sentences were short. On the other hand, human judgements, did not discriminate well between surface level disfluencies and breakdowns in comprehension. 3 is covered by a mask or masking pattern. Only if the reader moves the mouse cursor over a particular section of text, the mask is removed and the text"
W15-4705,W13-2104,0,0.0249444,"cy, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text reading. In particular, psycholinguistic approaches typically use objective measures such as reading times and eye movements to quantify how well human re"
W15-4705,P04-1011,0,0.0165298,"o-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text reading. In particular, psycholinguistic approaches typically use objective measures such as reading ti"
W15-4705,W02-2103,0,0.131716,"Missing"
W15-4705,J11-3002,0,0.0222029,"ess, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text reading. In particular, psycholinguistic approaches typically use objective measures such as reading times and eye movements to qu"
W15-4705,P13-1152,1,0.82319,"me that people spent looking at single sentences and an entire text. We hypothesized that these parameters are more informative for the quality of a text than the user ratings of clarity and fluency. As objective criteria for text quality are hardly available in NLG (Dale and Mellish, 1998; Hardcastle and Scott, 2008), we did not compare reading times and ratings on manual, potentially flawed annotations of text quality. Instead, we selected experimental material from a corpus-based generation framework that combines sentence-level linearisation and text-level referring expression generation (Zarrieß and Kuhn, 2013). We based our study on a set of texts that were available in 3 versions: (i) the “gold standard” corpus text, (ii) automatically linearised texts where word order deviated from the original corpus and contained potential fluency-related defects, (iii) texts with potential defects in referring expressions and linearisation which are likely to deteriorate clarity or coherence on the discourse level. We controlled the broad type of linguistic defects but not the details of each sentence or text. We argue that an objective evaluation method for NLG should clearly distinguish coherence and surface"
W15-4705,W12-2601,0,0.0185242,"f they have to compare texts with multiple sentences and multiple types of linguistic defects, e.g. fluency on the sentence level, clarity and coherence on the text level. Consequently, low agreement between raters, and even inconsistencies between ratings of the same human judge have been found in previous studies (Belz and Kow, 2010; Cahill and Forst, 2010; Dethlefs et al., 2014). Standard evaluation methods for, e.g. text summarisation tend to avoid possible interactions between local sentence-level and global text-level defects. Instead, they focus on coherence and content (Nenkova, 2006; Owczarzak et al., 2012). In particular, this is due to the fact that independently rating coherence and clarity locally for each sentence and globally for an entire text is tedious, unnatural, tiring and hardly achievable for human judges. Typically, human evaluation of NLG output is based on user ratings. We collected ratings and reading time data in a simple, low-cost experimental paradigm for text generation. Participants were presented corpus texts, automatically linearised texts, and texts containing predicted referring expressions and automatic linearisation. We demonstrate that the reading time metrics outper"
W15-4705,P02-1040,0,0.0968426,"hods where human readers are asked to judge a text, typically with respect to several criteria. If the NLG component is embedded in an end-to-end system, such as a dialogue system, 3) extrinsic factors of task success and usefulness of the NLG output can be measured. For corpus-based NLG components such as surface realisers or referring expression generators, extrinsic factors cannot be assessed, but in this case, reference or gold text outputs are often available. Langkilde (2002) first suggested to use automatic evaluation measures inspired from methods in machine translation, such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002), that measure the n-gram overlap between the system and some reference text, sentence or phrase. The advantage of such automatic and cheap evaluation methods can be enormous. If tightly integrated in the development cycle of an NLG system, they allow fast and empirically optimised implementation decisions. In turn, a lot of research on NLG evaluation focussed on defining and validating automatic evaluation measures. Such a metric is typically considered valid if it correlates well with human judgements of text quality (Stent et al., 2005; Foster, 2008; Reiter and Be"
W16-3630,D14-1223,0,0.109634,"hat we use. 1 Introduction In this paper, we present and evaluate a language processing pipeline that enables an automated system to detect and understand complex referential language about visual objects depicted on a screen. This is an important practical capability for present and future interactive spoken dialogue systems. There is a trend toward increasing deployment of spoken dialogue systems for smartphones, tablets, automobiles, TVs, and other settings where information and options are presented on-screen along with an interactive speech channel in which visual items can be discussed (Celikyilmaz et al., 2014). Similarly, for future systems such as smartphones, quadcopters, or selfdriving cars that are equipped with cameras, users * The work was done while at Bielefeld University. 232 Proceedings of the SIGDIAL 2016 Conference, pages 232–241, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics ject type. Finally, to understand a full scene description, the matcher needs to combine all the evidence from multiple referential segments involving a group of objects to identify the target image. In this paper, we define and evaluate a language processing pipeline that"
W16-3630,W09-3902,1,0.867173,"Missing"
W16-3630,W15-4610,1,0.2969,"ental ASR results, so that in the future it can be incorporated into a real-time interactive dialogue system. We view it as a crucial design constraint on our pipeline modules that the resolution process must take place incrementally; i.e., processing must not be deferred until the end of the user’s speech. This is because humans resolve (i.e., comprehend) speech as it unfolds (Tanenhaus, 1995; Spivey et al., 2002), and incremental processing (i.e., processing word by word) is important to developing an efficient and natural speech channel for interactive systems (Skantze and Schlangen, 2009; Paetzel et al., 2015; DeVault et al., 2009; Aist et al., 2007). In the current study, we have therefore provided the human director’s correctly transcribed speech as input to our pipeline on a word-by-word basis, as visualized in Figure 3. 3.2 Segment Type Classifier The segment type classifier assigns each detected segment with one of the type labels in Table 1 (SIN, MUL, REL, OT). This label informs the Reference Resolver module in how to proceed with the resolution process, as explained below. The segment type labeler is an SVM classifier implemented in LIBSVM (Chang and Lin, 2011). Features used include word"
W16-3630,E09-1085,1,0.792931,"l-time operation using incremental ASR results, so that in the future it can be incorporated into a real-time interactive dialogue system. We view it as a crucial design constraint on our pipeline modules that the resolution process must take place incrementally; i.e., processing must not be deferred until the end of the user’s speech. This is because humans resolve (i.e., comprehend) speech as it unfolds (Tanenhaus, 1995; Spivey et al., 2002), and incremental processing (i.e., processing word by word) is important to developing an efficient and natural speech channel for interactive systems (Skantze and Schlangen, 2009; Paetzel et al., 2015; DeVault et al., 2009; Aist et al., 2007). In the current study, we have therefore provided the human director’s correctly transcribed speech as input to our pipeline on a word-by-word basis, as visualized in Figure 3. 3.2 Segment Type Classifier The segment type classifier assigns each detected segment with one of the type labels in Table 1 (SIN, MUL, REL, OT). This label informs the Reference Resolver module in how to proceed with the resolution process, as explained below. The segment type labeler is an SVM classifier implemented in LIBSVM (Chang and Lin, 2011). Featu"
W16-3630,Q14-1017,0,0.113143,"e this in future work. WAC model, akin to referentially afforded concept composition (Mcnally and Boleda, 2015)). Also related are the recent efforts in automatic image captioning and retrieval, where the task is to generate a description (a caption) for a given image or retrieve one being given a description. A frequently taken approach is to use a convolutional neural network to map the image into a dense vector, and then to condition a neural language model on this to produce an output string or using it to map the description into the same space (Vinyals et al., 2015; Devlin et al., 2015; Socher et al., 2014). See also Fang et al. (2015), which is more directly related to our model in that they use “word detectors” to propose words for image regions. 6 Acknowledgments This work was in part supported by the Cluster of Excellence Cognitive Interaction Technology ’CITEC’ (EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG), and the KogniHome project, funded by BMBF. This work was supported in part by the National Science Foundation under Grant No. IIS1219253 and by the U.S. Army. Any opinions, findings, and conclusions or recommendations expressed in this materia"
W16-3630,P15-1029,1,0.692343,"g visual reference game RDG-Pento, shown in Figure 1, as our testbed, and we evaluate both human-human and automated system performance in a corpus study. No prior work we are aware of has put forth techniques for grounded understanding of the kinds of noisy, complex, spoken descriptions of visual scenes that can occur in such interactive dialogue settings. This work describes and evaluates an initial approach to this complex problem, and it demonstrates the critical importance of segmentation and entrainment to achieving strong understanding performance. This approach extends the prior work (Kennington and Schlangen, 2015; Han et al., 2015) that assumed either that referential language from users has been pre-segmented, or that visual scenes are given not as raw images but as clean semantic representations, or that visual scenes are simple enough to be described with a one-off referring expression or caption. Our work makes none of these assumptions. Our automated pipeline, discussed in Section 3, includes components for learning perceptually grounded word meanings, segmenting a stream of speech, identifying the type of referential language in each speech segment, resolving the references in each type of segme"
W16-3630,N03-1033,0,0.0368121,". The annotation was done by an expert annotator. 234 plemented with Mallet (McCallum, 2002)). Using a CRF trained on the annotated RDG-Pento data set, we identify the most likely sequence of wordlevel boundary tags, where each tag indicates if the current word ends the previous segment or not.2 An example segmentation is shown in Figure 3, where the word sequence weird L to the top left of is segmented into two segments, [weird L] and [to the top left of]. The features provided to the CRF include unigrams3 , the speaker’s role, partof-speech (POS) tags obtained using the Stanford POS tagger (Toutanova et al., 2003), and information about the scene such as the number of objects. image speaker transcripts in which either the director or the matcher’s transcribed speech for a target image is annotated. There are 8030 annotated segments (5451 director segments and 2579 matcher segments). There are 1372 word types and 55,238 word tokens. 3 Language Processing Pipeline In this section, we present our language processing pipeline for segmentation and understanding of complex scene descriptions. The modules, decision-making, and information flow for the pipeline are visualized in Figure 3. The pipeline modules"
W16-3630,P03-1054,0,0.0470631,"t to our pipeline on a word-by-word basis, as visualized in Figure 3. 3.2 Segment Type Classifier The segment type classifier assigns each detected segment with one of the type labels in Table 1 (SIN, MUL, REL, OT). This label informs the Reference Resolver module in how to proceed with the resolution process, as explained below. The segment type labeler is an SVM classifier implemented in LIBSVM (Chang and Lin, 2011). Features used include word unigrams, word POS, user role, number of objects in the TI, and the top-level syntactic category of the segment as obtained from the Stanford parser (Klein and Manning, 2003). Figure 3 shows two examples of output from the segment type classifier, which assigns SIN to [weird L] and REL to [to the top left of]. 3.3 Reference Resolver We introduce some notation to help explain the operation of the reference resolver (RR) module. When a scene description is to be resolved, there is a visual context in the game which we encode as a context set C = I1 , ..., I8 containing the eight visible images (see Figure 1). Each image Ik contains n objects {ok1 , . . . , okn }, where n is fixed per context set, but varies across context sets from n = 1 to n = 6. The set of all obj"
W16-3630,W95-0107,0,0.0183224,"Missing"
W16-3630,L16-1019,1,0.820868,"the U.S. and Canada were recruited using AMT. Game play and audio data were captured for each pair of speakers (who were not colocated and communicated entirely through their web browsers), and the resulting audio data was transcribed and annotated. 16 pairs completed all 5 game rounds, while the remaining crowdsourced pairs completed only part of the game for various reasons. As our focus is on understanding individual scene descriptions, our data set here includes data from the 16 complete games as well as partial games. A more complete description and analysis of the corpus can be found in Zarrieß et al. (2016). Data Annotation We annotated the transcribed director and matcher speech through a process of segmentation, segment type labeling, and referent identification. The segment types are shown in Table 1, and example annotations are provided in Figure 2. The annotation is carried out on each tar1 The annotation scheme was developed iteratively while keeping the reference resolution task and the WAC model (see Section 3.3.1) in mind. The annotation was done by an expert annotator. 234 plemented with Mallet (McCallum, 2002)). Using a CRF trained on the annotated RDG-Pento data set, we identify the"
W16-3631,el-asri-etal-2014-nastia,0,0.0225987,"Missing"
W16-3631,N09-1043,1,0.803881,"i.e., the two strings are mostly similar). For all other w, C(w|r)=0. This results in a distribution C, which we renormalise and blend with learned distribution to yield P (U |R). Speech Recognition The module that takes speech input from the user in our SDS is the ASR component. Incremental ASR must transcribe uttered speech into words which must be forthcoming from the ASR as early as possible (i.e., the ASR must not wait for endpointing to produce output). Each module that follows must also process incrementally, acting in lock-step upon input as it is received. Incremental ASR is not new (Baumann et al., 2009) and many of the current freely-accessible ASR systems can produce output (semi-) incrementally. We opt for Google ASR for its vocabulary coverage of our evaluation language (German). Following, Baumann et al. (2016), we package output from the Google service into IUs which are passed to the NLU module, which we now explain. 3.3 X 1 P (I) P (U |R = r)P (R = r|I) (1) P (U ) r∈R Language Understanding We approach the task of NLU as a slot-filling task (a very common approach; see Tur et al. (2012)) where an intent is complete when all slots of a frame are filled. The main driver of the NLU in 1"
W16-3631,P11-4016,0,0.0171767,"oviding feedback and backchannels to the user. Dethlefs et al. (2016) provide a good review of work that show how backchannels facilitate grounding, feedback, and clarifications in human spoken dialogue, and apply an information density approach to determine when to backchannel using speech. Because we don’t backchannel using speech here, there is no potential overlap between the user and the system; rather, our system can display backchannels and ask clarifications without frustrating the user through inadvertent overlaps. Though different in many ways, our work is similar in some regards to Larsson et al. (2011), which displays information to the user and allows the user to navigate the display itself (e.g., by saying up or down in a menu list)–functionality that we intend to apply to our GUI in future work. Our work is also comparable to SDS toolkits such as IrisTK (Skantze and Moubayed, 2012) and OpenDial (Lison, 2015) which enable SDS designers to visualise the internal state of their systems, though not for end user interpretability. Some of the work here is inspired by the Microsoft Language Understanding Intelligent Service (LUIS) project (Williams et al., 2015). While our system by no means ac"
W16-3631,E09-1085,1,0.720975,"d the GUI which displays the state of the system. 3.1 Incremental Dialogue An aspect of our SDS that sets it apart from others is the requirement that it process incrementally. One potential concern with incremental processing is regarding informativeness: why act early when waiting might provide additional information, resulting in better-informed decisions? The trade off is naturalness as perceived by the user who is interacting with the SDS. Indeed, it has been shown that human users perceive incremental systems as being more natural than traditional, turn-based systems (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991; Asri et al., 2014), offer a more human-like experience (Edlund et al., 2008) and are more satisfying to interact with than non-incremental systems (Aist et al., 2007). Psycholinguistic research has also shown that humans comprehend utterances as they unfold and do not wait until the end of an utterance to begin the comprehension process (Tanenhaus et al., 1995; Spivey et al., 2002). The trade-off between informativeness and naturalness can be reconciled when mechanisms are in place that allow earlier decisions to be repaired. Such mechanisms are offered by the"
W16-3631,W13-4030,1,0.879978,"rk consists of a network of processing modules. A typical module takes input, performs some kind of processing on that data, and produces output. System Description This section introduces and describes our SDS, which is modularised into four main components: ASR , natural language understanding ( NLU ), dialogue management (DM), and the graphical user interface (GUI) which, as explained below, is visualised as a right-branching tree. The overall system is represented in Figure 1. For the remainder of this section, each module is explained in 243 our SDS is the SIUM model of NLU introduced in Kennington et al. (2013). SIUM has been used in several systems which have reported substantial results in various domains, languages, and tasks (Han et al., 2015; Kennington et al., 2015; Kennington and Schlangen, 2017) Though originally a model of reference resolution, it was always intended to be used for general NLU, which we do here. The model is formalised as follows: Figure 2: Example of IU network; part-of-speech tags are grounded into words, tags and words have same level links with left IU; four is revoked and replaced with forty. P (I|U ) = The data are packaged as the payload of incremental units (IUs) wh"
W16-3631,W14-4312,1,0.840014,", 2008) and are more satisfying to interact with than non-incremental systems (Aist et al., 2007). Psycholinguistic research has also shown that humans comprehend utterances as they unfold and do not wait until the end of an utterance to begin the comprehension process (Tanenhaus et al., 1995; Spivey et al., 2002). The trade-off between informativeness and naturalness can be reconciled when mechanisms are in place that allow earlier decisions to be repaired. Such mechanisms are offered by the incremental unit (IU) framework for SDS (Schlangen and Skantze, 2011), which we apply here. Following Kennington et al. (2014), the IU framework consists of a network of processing modules. A typical module takes input, performs some kind of processing on that data, and produces output. System Description This section introduces and describes our SDS, which is modularised into four main components: ASR , natural language understanding ( NLU ), dialogue management (DM), and the graphical user interface (GUI) which, as explained below, is visualised as a right-branching tree. The overall system is represented in Figure 1. For the remainder of this section, each module is explained in 243 our SDS is the SIUM model of NL"
W16-3631,W15-4622,0,0.0419484,"Missing"
W16-3631,N15-1031,1,\N,Missing
W16-3632,W12-1814,1,0.854091,"ation. The segmenter tags each word as either the beginning (B) of a new DA segment or as a continuation of the current DA segment (I).3 Then, each 3 Figure 2: The operation of the pipeline on selected ASR partials (with time index in seconds). resulting DA segment is classified into one of 18 DA labels using an SVM (Support Vector Machine) classifier implemented in Weka (Hall et al., 2009). 4.1 Features Prosodic Features We use word-level prosodic features similar in nature to Litman et al. (2009). The alignment between words and computed prosodic features is achieved using a forced aligner (Baumann and Schlangen, 2012) to generate wordlevel timing information. For each word, we first Note that our annotation scheme completely partitions our 255 data, with every word belonging to a segment and receiving a DA label. We have therefore elected not to adopt BIO (BeginInside-Outside) tagging. obtain pitch and RMS values every 10ms using InproTK (Baumann and Schlangen, 2012). Because pitch and energy features can be highly variable across users, our pitch and energy features are represented as z-scores that are normalized for the current user up to the current word. For the pitch and RMS values, we obtain the max,"
W16-3632,W15-4651,0,0.0711716,"et al., 2015). It’s important to allow users to speak naturally to spoken dialogue systems. It has been understood for some time that this ultimately requires a system to be able to automatically segment a user’s speech into meaningful units in real-time while they speak (Nakano et al., 1999). Still, most current systems use relatively simple and limited approaches to this segmentation problem. For example, in many systems, it’s assumed that pauses in the user’s speech can be used to determine the segmentation, often by treating each detected pause as indicating a dialogue act (DA) boundary (Komatani et al., 2015). While easily implemented, such a pause-based design has several problems. First, a substantial number of spoken DAs contain internal pauses (Bell et al., 2001; Komatani et al., 2015), as in I need a car in... 10 minutes. Using simple pause length thresholds to join certain speech segments together for interpretation is not a very effective remedy for this problem (Nakano et al., 1999; Ferrer et al., 2003). More sophisticated approaches train algorithms to join speech across pauses (Komatani et al., 2015) or decide which pauses constitute endof-utterances that should trigger interpretation (e"
W16-3632,2007.sigdial-1.30,0,0.0377821,"and individual words as they are spoken within Inter-Pausal Units (IPUs) (Koiso et al., 1998) or speech segments. (We use the two terms interchangeably in this paper to refer to a period of continuous speech separated by pauses of a minimum duration before and after.) Beyond the work on this alignment problem mentioned in the introduction, a related line of work has looked specifically at DA segmentation and classification given an input string of words together with an audio recording to enable prosodic and timing analysis (Petukhova and Bunt, 2014; Zimmermann, 2009; Zimmermann et al., 2006; Lendvai and Geertzen, 2007; Ang et al., 2005; Nakano et al., 1999; Warnke et al., 1997). This work generally encompasses the problems of identifying DA-internal pauses as well as locating DA boundaries within speech segments. Prosody information has been shown to be helpful for accurate DA segmentation (Laskowski and Shriberg, 2010; Shriberg et al., 253 2000; Warnke et al., 1997) as well as for DA classification (Stolcke et al., 2000; Fernandez and Picard, 2002). In general, DA segmentation has been found to benefit from a range of additional features such as pause durations at word boundaries, the user’s dialogue temp"
W16-3632,P14-5010,0,0.00300045,"eatures are represented as z-scores that are normalized for the current user up to the current word. For the pitch and RMS values, we obtain the max, min, mean, variance and the co-efficients of a second degree polynomial. Pause durations at word boundaries provide an additional useful feature (Kol´aˇr et al., 2006; Zimmermann, 2009). All numeric features are discretized into bins. We currently use prosody for segmentation but not classification.4 Lexico-syntactic & contextual features We use word unigrams along with the corresponding partof-speech (POS) tags, obtained using Stanford CORENLP (Manning et al., 2014), as a feature for both the segmentation and the DA classifier. Words with a low frequency (&lt;10) are substituted with a low frequency word symbol. The top level constituent category from a syntactic parse of the DA segment is also used. Several contextual features are included. The role of the speaker (Director or Matcher) is included as a feature. Previously recognized DA labels from each speaker are included. Another feature is added to assist with the Echo Confirmation (EC) DA, which applies when a speaker repeats verbatim a phrase recently spoken by the other interlocutor. For this we use"
W16-3632,W16-3630,1,0.829001,"Missing"
W16-3632,P11-2017,0,0.0281481,"to mark wordlevel unigrams that appeared in recent speech from the other interlocutor. Finally, a categorical feature indicates which of 18 possible image sets (e.g. bikes as in Figure 1) is under discussion; simpler images tend to have shorter segments.5 4.2 Discussion of Machine Learning Setup A salient alternative to our sequential pipeline approach – also adopted for example by Ang et al. (2005) – is to use a joint classification model to solve the segmentation and classification problems simultaneously, potentially thereby improving performance on both problems (Petukhova and Bunt, 2014; Morbini and Sagae, 2011; Zimmermann, 2009; Warnke et al., 1997). We performed an initial test using a joint model and found, unlike the finding reported by Zimmermann (2009), that for Condition HT-HS-HD HT-HS-AD HT-AS-AD AT-AS-AD Transcripts (T) Human Human Human ASR Segment Boundaries (S) Human Human Automated Automated DA labels (D) Human Automated Automated Automated Table 3: Conditions for evaluating DA segmentation and classification. our corpus a joint approach performed markedly worse than our sequential pipeline.6 We speculate that this is due to the relative sparsity of data on rarer DA types in our relativ"
W16-3632,P99-1026,0,0.448581,"ith its users (Paetzel et al., 2015). This work is part of a longer-term research program that aims to use incremental (word-byword) language processing techniques to enable dialogue agents to support efficient, fast-paced interactions with a natural conversational style (DeVault et al., 2011; Ward and DeVault, 2015; Paetzel et al., 2015). It’s important to allow users to speak naturally to spoken dialogue systems. It has been understood for some time that this ultimately requires a system to be able to automatically segment a user’s speech into meaningful units in real-time while they speak (Nakano et al., 1999). Still, most current systems use relatively simple and limited approaches to this segmentation problem. For example, in many systems, it’s assumed that pauses in the user’s speech can be used to determine the segmentation, often by treating each detected pause as indicating a dialogue act (DA) boundary (Komatani et al., 2015). While easily implemented, such a pause-based design has several problems. First, a substantial number of spoken DAs contain internal pauses (Bell et al., 2001; Komatani et al., 2015), as in I need a car in... 10 minutes. Using simple pause length thresholds to join cert"
W16-3632,paetzel-etal-2014-multimodal,1,0.848278,"ue tempo (Komatani et al., 2015), as well as lexical, syntactic, and semantic features. Work on system turn-taking decisions has used similar features to optimize a system’s turn-taking policy during a user pause, often with classification approaches; e.g. (Sato et al., 2002; Takeuchi et al., 2004; Raux and Eskenazi, 2008). To our knowledge, very little research has looked in detail at the impact of adding incremental DA segmentation to an implemented incremental system (though see Nakano et al. (1999)).1 3 The RDG-Image Game and Data Set Our work in this paper is based on the RDG-Image game (Paetzel et al., 2014), a collaborative, time constrained, fast-paced game with two players depicted in Figure 1. One player is assigned the role of director and the other the role of matcher. Both players see the same eight images on their screens (but arranged in a different order). The director’s screen has a target image highlighted in red, and the director’s goal is to describe the target image so that the matcher can identify it as quickly as possible. Once the matcher believes they have selected the right image, the director can request the next target. Both players score a point for each correct selection,"
W16-3632,W15-4610,1,0.558608,"eps, and we investigate the contribution of different processing steps to DA segmentation errors. We present our results using both existing and new metrics for DA segmentation. The incremental DA segmentation capability described here may help future systems to allow more natural speech from users and enable more natural patterns of interaction. 1 Introduction In this paper we explore the feasibility of incorporating an incremental dialogue act segmentation capability into an implemented, high-performance spoken dialogue agent that plays a time-constrained image-matching game with its users (Paetzel et al., 2015). This work is part of a longer-term research program that aims to use incremental (word-byword) language processing techniques to enable dialogue agents to support efficient, fast-paced interactions with a natural conversational style (DeVault et al., 2011; Ward and DeVault, 2015; Paetzel et al., 2015). It’s important to allow users to speak naturally to spoken dialogue systems. It has been understood for some time that this ultimately requires a system to be able to automatically segment a user’s speech into meaningful units in real-time while they speak (Nakano et al., 1999). Still, most cu"
W16-3632,W14-4315,0,0.0429738,"Missing"
W16-3632,W08-0101,0,0.209971,"le easily implemented, such a pause-based design has several problems. First, a substantial number of spoken DAs contain internal pauses (Bell et al., 2001; Komatani et al., 2015), as in I need a car in... 10 minutes. Using simple pause length thresholds to join certain speech segments together for interpretation is not a very effective remedy for this problem (Nakano et al., 1999; Ferrer et al., 2003). More sophisticated approaches train algorithms to join speech across pauses (Komatani et al., 2015) or decide which pauses constitute endof-utterances that should trigger interpretation (e.g. (Raux and Eskenazi, 2008; Ferrer et al., 2003)). This addresses the problem of DA-internal pauses, but it does not address the second problem with pause-based designs, which is that it’s also common for a continuous segment of user speech to include multiple DAs without intervening pauses, as in Sure that’s fine can you call when you get to the gate? A third problem is that waiting for a pause to occur before interpreting earlier speech may increase latency and erode the user experience (Skantze and Schlangen, 2009; Paetzel et al., 2015). Together, these problems suggest the need for an incremental dialogue act segme"
W16-3632,E09-1085,1,0.889436,"et al., 2015) or decide which pauses constitute endof-utterances that should trigger interpretation (e.g. (Raux and Eskenazi, 2008; Ferrer et al., 2003)). This addresses the problem of DA-internal pauses, but it does not address the second problem with pause-based designs, which is that it’s also common for a continuous segment of user speech to include multiple DAs without intervening pauses, as in Sure that’s fine can you call when you get to the gate? A third problem is that waiting for a pause to occur before interpreting earlier speech may increase latency and erode the user experience (Skantze and Schlangen, 2009; Paetzel et al., 2015). Together, these problems suggest the need for an incremental dialogue act segmentation capability in which a continuous stream of captured user speech, including the intermittent pauses therein, is incrementally segmented into appropriate DA units for interpretation. In this paper, we present a case study of implementing an incremental DA segmentation capability for an image-matching game called RDGImage, illustrated in Figure 1. In this game, two players converse freely in order to identify a spe252 Proceedings of the SIGDIAL 2016 Conference, pages 252–262, c Los Ange"
W16-3632,J00-3003,0,0.185888,"Missing"
W16-3637,W12-1814,1,0.940564,"ontinuously in real time as robotic actions or user utterances are in progress. In this paper, we present a simple real-time, real-world grounding framework, and a system which implements it in a simple robot, allowing investigation into different grounding strategies. Here, we experiment with a trade-off between the fluidity of the grounding mechanism with the ‘safety’ of ensuring task success. The framework consists of a combination of interactive Harel statecharts (Harel, 1987) and the Incremental Unit framework (Schlangen and Skantze, 2011), and is implemented in dialogue toolkit InproTK (Baumann and Schlangen, 2012). 2 Achieving Fluid Communicative Grounding in Dialogic Robots In this paper we are concerned with a simple pick-and-place robot with uni-modal communication abilities, which is simply its manipulation behaviour of objects– see Fig. 1 for example utterances from user U and system S’s actions. While our robot does not have natural language generation (NLG) capabilities, its physical actions are first class citizens of the dialogue so it is capable of dialogic behaviour through action. As mentioned above, while a human and robot’s internal representations of a situation can differ inherently, su"
W16-3637,D12-1008,0,0.0247985,"orld objects. When general taskperformance is good enough, the model leads to the perception of better understanding over a more standard incremental processing model. There are some weaknesses with the current study. We intend to use more complex strength of evidence measures, for example for Ev(U serGoal) using ASR hypotheses confidence thresholds (Williams, 2012), and having a more complex Ev(RobotGoal) based on the robot’s current position and velocity. We also want to explore learning and optimization for our incremental processing, with points of departure being (Paetzel et al., 2015), (Dethlefs et al., 2012), and the proposal by (Lemon and Eshghi, 2015). The future challenge, yet potential strength, for our model is that unlike most approaches which assume a finite state Markov model for probabilistic estimation, we do not assume the Cartesian product of all possible substates needs to be modelled. The mathematics of how this can be done for a complex hierarchical model has had recent attention, for example in recent work in probabilistic Type Theory with Records (Cooper et al., 2014)– we intend to pursue such an approach in coming work. Acknowledgments We thank the three SigDial reviewers for th"
W16-3637,W15-0130,1,0.849222,"and recovery incrementally is clear in (4). In (C), the grounding again happens incrementally, however in a full-duplex way, where concurrency of speech and action is allowed and reasoned with appropriately. To allow human-robot interaction to be more like mode (B) rather than (A), appropriate mechanisms can be designed for robots in line with computational theories of grounding (Traum, 1994; Traum and Larsson, 2003; Ginzburg, 2012), adjusting these mechanisms to work in real time rather than turn-finally, in line with recent work on incremental grounding theories (Ginzburg et 289 al., 2014; Eshghi et al., 2015) where semantic frames can be grounded partially as an utterance progresses. To move towards fluid mode (C), this type of incremental processing not only requires incremental interpretation word-by-word, but use of the context at the exact time each word is recognized, where here, context consists in the estimation of both the user’s state and the robot’s current state through self-monitoring, both of which can change dynamically during the course of an utterance, or even during a word. In this setting, during a repair from the user, the robot must reason about the action currently ‘under disc"
W16-3637,W14-1409,0,0.0644078,"Missing"
W16-3637,P15-1029,1,0.885616,"Missing"
W16-3637,W12-1621,0,0.0306318,"logue system that manipulates real-world objects. from misunderstanding, which has been central to dialogue systems research (Traum, 1994; Traum and Larsson, 2003), with recent work showing how this can operate incrementally (see e.g. (Buß and Schlangen, 2011; Skantze and Hjalmarsson, 2010)), and in situated dialogue domains, through simulation with virtual agents (Marge and Rudnicky, 2011; Raux and Nakano, 2010; Buschmeier and Kopp, 2012). In robotics, much of the grounding research has focussed on perspective taking and frame of reference differing between robot and human (Liu et al., 2010; Liu et al., 2012; Kollar et al., 2010). The aspect of grounding we focus on here is the mechanisms needed for it to be done fluidly in real time. In line with results from human-human interaction where action is shown to be representative of the current state of understanding with little latency (Tanenhaus and Brown-Schmidt, 2008; McKinstry et al., 2008) and where moving in response to instructions happens before the end of the utterance (Hough et al., 2015), we hypothesized that the greater the fluidity, the more natural the robot’s action would appear. To illustrate, in Fig. 1, we show three modes of ground"
W16-3637,W10-4341,0,0.0290393,"interpreted before the robot has shown unambiguously what its goal is to allow the fluidity in setting (C)– this requires a self-monitoring process which estimates at which point the robot has shown its goal sufficiently clearly to the user, during its movement and not necessarily only after its goal has become completely unambiguous. 3 Interactive Statecharts and the Incremental Unit Framework for Real-time Grounding Our approach to modelling and implementing real-time grounding mechanisms follows work using Harel statecharts (Harel, 1987) for dialogue control in robotic dialogue systems by (Peltason and Wrede, 2010; Skantze and Al Moubayed, 2012). However here, rather than characterizing a single dialogue state which is accessed by a single dialogue manager, our statechart characterizes two independent parallel states for the user and robot, taking an agents-based approach in the sense of (Jennings, 2001). As illustrated in the diagrams in Fig. 2 and Fig. 7 (Appendix), as per standard statecharts we utilize states (boxes) and transitions (directed edges) which are executable by trigger events (main edge labels) and conditions (edge labels within []), and, additionally triggered actions can be represente"
W16-3637,W11-0144,1,0.853339,"incrementally which type of dialogue act u is, (e.g. u : Confirm), whether w begins a new dialogue act or not, and estimate U serGoal. The statechart is then checked to see if a transition is possible from the user’s current state as each word is processed, akin to incremental dialogue state tracking (Williams, 2012). 291 3.2 Managing Fluid Grounding with the IU framework To manage the processing and information flow, we use the Incremental Unit (IU) framework (Schlangen and Skantze, 2011). Currently, in implemented IU framework systems such as Jindigo (Skantze and Hjalmarsson, 2010), DyLan (Purver et al., 2011) and InproTK (Baumann and Schlangen, 2012), processing goes bottom-up (from sensors to actuators) and the creation of incremental units (IUs) is driven by input events to each module from bottom to top. IUs are packages of information at a pre-defined level of granularity, for instance a wordIU can be used to represent a single incremental ASR word hypothesis, and their creation in the output buffers of a module triggers downstream processing and creation of new IUs in modules with access to that buffer. IUs can be defined to be connected by directed edges, called Grounded In links, which in g"
W16-3637,W10-4329,0,0.0315552,"to box 2] [drops x] (6) U: Take the red cross no the other one right put it in box 2 right S: [moves to x(aborted)][moves to y][grabs y] [moves to box 2] [drops y] Figure 1: Grounding modes in a robotic dialogue system that manipulates real-world objects. from misunderstanding, which has been central to dialogue systems research (Traum, 1994; Traum and Larsson, 2003), with recent work showing how this can operate incrementally (see e.g. (Buß and Schlangen, 2011; Skantze and Hjalmarsson, 2010)), and in situated dialogue domains, through simulation with virtual agents (Marge and Rudnicky, 2011; Raux and Nakano, 2010; Buschmeier and Kopp, 2012). In robotics, much of the grounding research has focussed on perspective taking and frame of reference differing between robot and human (Liu et al., 2010; Liu et al., 2012; Kollar et al., 2010). The aspect of grounding we focus on here is the mechanisms needed for it to be done fluidly in real time. In line with results from human-human interaction where action is shown to be representative of the current state of understanding with little latency (Tanenhaus and Brown-Schmidt, 2008; McKinstry et al., 2008) and where moving in response to instructions happens befor"
W16-3637,W10-4301,0,0.211595,"ing concurrent user speech and robotic action: (5) U: Take the red cross right put it in box 2 right S: [moves to x][grabs x] [moves to box 2] [drops x] (6) U: Take the red cross no the other one right put it in box 2 right S: [moves to x(aborted)][moves to y][grabs y] [moves to box 2] [drops y] Figure 1: Grounding modes in a robotic dialogue system that manipulates real-world objects. from misunderstanding, which has been central to dialogue systems research (Traum, 1994; Traum and Larsson, 2003), with recent work showing how this can operate incrementally (see e.g. (Buß and Schlangen, 2011; Skantze and Hjalmarsson, 2010)), and in situated dialogue domains, through simulation with virtual agents (Marge and Rudnicky, 2011; Raux and Nakano, 2010; Buschmeier and Kopp, 2012). In robotics, much of the grounding research has focussed on perspective taking and frame of reference differing between robot and human (Liu et al., 2010; Liu et al., 2012; Kollar et al., 2010). The aspect of grounding we focus on here is the mechanisms needed for it to be done fluidly in real time. In line with results from human-human interaction where action is shown to be representative of the current state of understanding with little la"
W16-3637,W12-1812,0,0.165621,"g. 1 (B) and (C) above are met here as the increment size of the triggering events in the U ser state is the utterance of the latest word w in current utterance u (as opposed to the latest complete utterance). The principal Natural Language Understanding (NLU) decisions are therefore to classify incrementally which type of dialogue act u is, (e.g. u : Confirm), whether w begins a new dialogue act or not, and estimate U serGoal. The statechart is then checked to see if a transition is possible from the user’s current state as each word is processed, akin to incremental dialogue state tracking (Williams, 2012). 291 3.2 Managing Fluid Grounding with the IU framework To manage the processing and information flow, we use the Incremental Unit (IU) framework (Schlangen and Skantze, 2011). Currently, in implemented IU framework systems such as Jindigo (Skantze and Hjalmarsson, 2010), DyLan (Purver et al., 2011) and InproTK (Baumann and Schlangen, 2012), processing goes bottom-up (from sensors to actuators) and the creation of incremental units (IUs) is driven by input events to each module from bottom to top. IUs are packages of information at a pre-defined level of granularity, for instance a wordIU can"
W16-3637,W15-4610,0,0.0815591,"s and manipulates real-world objects. When general taskperformance is good enough, the model leads to the perception of better understanding over a more standard incremental processing model. There are some weaknesses with the current study. We intend to use more complex strength of evidence measures, for example for Ev(U serGoal) using ASR hypotheses confidence thresholds (Williams, 2012), and having a more complex Ev(RobotGoal) based on the robot’s current position and velocity. We also want to explore learning and optimization for our incremental processing, with points of departure being (Paetzel et al., 2015), (Dethlefs et al., 2012), and the proposal by (Lemon and Eshghi, 2015). The future challenge, yet potential strength, for our model is that unlike most approaches which assume a finite state Markov model for probabilistic estimation, we do not assume the Cartesian product of all possible substates needs to be modelled. The mathematics of how this can be done for a complex hierarchical model has had recent attention, for example in recent work in probabilistic Type Theory with Records (Cooper et al., 2014)– we intend to pursue such an approach in coming work. Acknowledgments We thank the three"
W16-6642,D10-1115,0,0.0956907,"Missing"
W16-6642,S12-1013,0,0.320058,"Missing"
W16-6642,D15-1224,0,0.100185,"features (Roy, 2002; Roy and Reiter, 2005). In this paradigm, colour terms have received special attention. Intuitively, a model of perceptually grounded meaning should associate words for colour with particular points or regions in a colour space, e.g. (Mojsilovic, 2005). On the other hand, their visual association seems to vary with the linguistic context such as ‘red’ in the context of ‘hair’, ‘car’ or ‘wine’ (Roy and Reiter, 2005). Recently, large-scale data sets of real-world images and image descriptions, e.g. (Young et al., 2014), or referring expressions (Kazemzadeh et al., 246 2014; Gkatzia et al., 2015) have become available and can now serve as a realistic test bed for models of language grounding. In this paper, we use the ReferIt corpus (Kazemzadeh et al., 2014) to assess the performance of classifiers that predict colour terms from low-level visual representations of their corresponding image regions. A number of studies on colour naming have looked at experimental settings where speakers referred to simple objects or colour swatches instantiating a single value in a colour space. Even in these controlled settings, speakers use colour terms in flexible, context-dependent ways (Baumgaertn"
W16-6642,D14-1086,0,0.565309,"should associate words for colour with particular points or regions in a colour space, e.g. (Mojsilovic, 2005). On the other hand, their visual association seems to vary with the linguistic context such as ‘red’ in the context of ‘hair’, ‘car’ or ‘wine’ (Roy and Reiter, 2005). Recently, large-scale data sets of real-world images and image descriptions, e.g. (Young et al., 2014), or referring expressions (Kazemzadeh et al., 246 2014; Gkatzia et al., 2015) have become available and can now serve as a realistic test bed for models of language grounding. In this paper, we use the ReferIt corpus (Kazemzadeh et al., 2014) to assess the performance of classifiers that predict colour terms from low-level visual representations of their corresponding image regions. A number of studies on colour naming have looked at experimental settings where speakers referred to simple objects or colour swatches instantiating a single value in a colour space. Even in these controlled settings, speakers use colour terms in flexible, context-dependent ways (Baumgaertner et al., 2012; Meo et al., 2014). Therefore, probabilistic models and classifiers, allowing for variable thresholds and boundaries between regions in a colour spac"
W16-6642,J12-1006,0,0.119945,"Missing"
W16-6642,W11-2702,0,0.0310755,"ows: input is a feature vector x, a visual representation of a referent in an image, and output is a label y, a colour term for the referent. For the sake of simplicity, we only consider training and testing instances that contain colour terms and do not 248 model the decision whether a colour term should be generated at all. In standard NLG terminology, we are only interested in realisation, and not in content selection. A lot of research on REG has actually focussed on content selection, assuming perfect knowledge about appropriate colour terms for referents in a scene, cf. (Pechmann, 1989; Viethen and Dale, 2011; Viethen et al., 2012; Krahmer and Van Deemter, 2012; Koolen et al., 2013). The classifiers We used a multilayer perceptron that learns a function from colour histograms (or ConvNet features) to colour terms, i.e. defining an input layer corresponding to the dimensions of the colour histogram and an output layer of 11 nodes. We did not extensively tune the hyper parameters for our different visual inputs, but tested some parameter settings of the perceptron trained on RGB histograms, singling out a development set of 500 instances from the training set described above. We report results for t"
W16-6642,Q14-1006,0,0.0243045,"rpus data and model the connection between words and non-symbolic perceptual features (Roy, 2002; Roy and Reiter, 2005). In this paradigm, colour terms have received special attention. Intuitively, a model of perceptually grounded meaning should associate words for colour with particular points or regions in a colour space, e.g. (Mojsilovic, 2005). On the other hand, their visual association seems to vary with the linguistic context such as ‘red’ in the context of ‘hair’, ‘car’ or ‘wine’ (Roy and Reiter, 2005). Recently, large-scale data sets of real-world images and image descriptions, e.g. (Young et al., 2014), or referring expressions (Kazemzadeh et al., 246 2014; Gkatzia et al., 2015) have become available and can now serve as a realistic test bed for models of language grounding. In this paper, we use the ReferIt corpus (Kazemzadeh et al., 2014) to assess the performance of classifiers that predict colour terms from low-level visual representations of their corresponding image regions. A number of studies on colour naming have looked at experimental settings where speakers referred to simple objects or colour swatches instantiating a single value in a colour space. Even in these controlled setti"
W17-3509,P12-3018,1,0.821994,"ilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation (iTTS) (Baumann and Schlangen, 2012a) allowing for fine-grained, incremental manipulation of the audio signal (e.g. interruption, pausing, resumption, continuation). We will show an interactive demonstration of the following set-up: the system presents an image with several objects in a visual scene on the screen and the user’s task is to click on the object referred to. While generating and s"
W17-3509,W12-1814,1,0.854737,"ilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation (iTTS) (Baumann and Schlangen, 2012a) allowing for fine-grained, incremental manipulation of the audio signal (e.g. interruption, pausing, resumption, continuation). We will show an interactive demonstration of the following set-up: the system presents an image with several objects in a visual scene on the screen and the user’s task is to click on the object referred to. While generating and s"
W17-3509,P05-3001,0,0.0423899,"n language. Theoretically, it is well known that this change in modality fundamentally changes human production of referring expressions: Given the real-time constraints of situated interaction, a speaker often has to start uttering before she has found the optimal expression, but at the same time, she can observe the listener’s reaction while speaking and extend, adapt, or correct her referring expressions accordingly (Clark and Wilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an increm"
W17-3509,D14-1086,0,0.0345634,"has found the optimal expression, but at the same time, she can observe the listener’s reaction while speaking and extend, adapt, or correct her referring expressions accordingly (Clark and Wilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation (iTTS) (Baumann and Schlangen, 2012a) allowing for fine-grained, incremental manipulation of the audio signal (e.g. interruption, pausing, resumption, continuation). We will show an interactive demons"
W17-3509,E09-1081,1,0.766258,"non-verbal reactions of the user (i.e. her mouse movements) and adapts the generated utterances to these actions in an incremental fashion. At the same time, the system tries to be as cooperative as possible: if the user shows no reaction for a certain amount of time, the previous expression is expanded, i.e. the system splits its referring expression over several utterances, which is usually known as “reference in installments”, cf. (Zarrieß and Schlangen, 2016). Figure 1 illustrates the architecture of Refer-iTTS, which conceptually follows the framework of the Incremental Unit (IU) model (Schlangen and Skantze, 2009), and two example interactions. User actions and the system’s generation and synthesis decisions happen concurrently, coordinated and monitored by an action manager (AM) module. Thus, besides decisions related to content planning and realization (e.g. attribute selection and ordering), a spoken installment-based REG system has to make a number of high-level decisions related to the delivery and timing of its own output. Using the Zarrieß and Schlangen (2016)’s generator, the system orders its 72 Proceedings of The 10th International Natural Language Generation conference, pages 72–73, c Santia"
W17-3509,W12-1504,0,0.0312451,"that this change in modality fundamentally changes human production of referring expressions: Given the real-time constraints of situated interaction, a speaker often has to start uttering before she has found the optimal expression, but at the same time, she can observe the listener’s reaction while speaking and extend, adapt, or correct her referring expressions accordingly (Clark and Wilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation ("
W17-3509,P16-1058,1,0.911868,"pression, but at the same time, she can observe the listener’s reaction while speaking and extend, adapt, or correct her referring expressions accordingly (Clark and Wilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation (iTTS) (Baumann and Schlangen, 2012a) allowing for fine-grained, incremental manipulation of the audio signal (e.g. interruption, pausing, resumption, continuation). We will show an interactive demonstration of the following set-u"
W17-5529,D12-1008,0,0.0180129,"led 10 calls (from the same caller, but treating each as separate), after two training calls. We had 10 participants (balanced for gender), all native German speakers. To provide some control over the interaction, the task was set up so that after a greeting provided by a recording, C formulated their request in one turn (ostensibly, addressing a dialogue system that processed it) which A could hear, but not intervene Introduction How to best present information in a dialogue system is a central, and hence well-studied problem (Stent et al., 2004; Demberg and Moore, 2006; Rieser et al., 2010; Dethlefs et al., 2012b; Wen et al., 2015). What has received less attention is the question of what a system should do until it can present information, in the case that retrieval of this information takes time. 1 A domain in which it is, to this date, realistic that a request needs significant time to be processed, as anyone who has recently used flight search engines can attest. 241 Proceedings of the SIGDIAL 2017 Conference, pages 241–246, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics Figure 1: Phases of the call. Figure 2: Example interaction (gray: caller, white: t"
W17-5529,W12-1509,0,0.0152181,"led 10 calls (from the same caller, but treating each as separate), after two training calls. We had 10 participants (balanced for gender), all native German speakers. To provide some control over the interaction, the task was set up so that after a greeting provided by a recording, C formulated their request in one turn (ostensibly, addressing a dialogue system that processed it) which A could hear, but not intervene Introduction How to best present information in a dialogue system is a central, and hence well-studied problem (Stent et al., 2004; Demberg and Moore, 2006; Rieser et al., 2010; Dethlefs et al., 2012b; Wen et al., 2015). What has received less attention is the question of what a system should do until it can present information, in the case that retrieval of this information takes time. 1 A domain in which it is, to this date, realistic that a request needs significant time to be processed, as anyone who has recently used flight search engines can attest. 241 Proceedings of the SIGDIAL 2017 Conference, pages 241–246, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics Figure 1: Phases of the call. Figure 2: Example interaction (gray: caller, white: t"
W17-5529,W09-3942,0,0.0830098,"Missing"
W17-5529,W13-4062,0,0.0486303,"Missing"
W17-5529,P10-1103,0,0.0471879,"Missing"
W17-5529,W11-0101,0,0.0420632,"Missing"
W17-5529,E09-1081,1,0.740467,"eaker. Finally, Figure 4 illustrates the temporal sequencing 5 Related Work To the best of our knowledge, delayed information presentation has so far not been systematically studied. Various systems, however, addressed the problem in an ad-hoc manner. The 243 task information is not only not detrimental to the interaction, but might in fact be beneficial. From a broader perspective, we see this study on time buying as contributing to research on incremental generation and information presentation for dialogue systems, cf. (Skantze and Hjalmarsson, 2010), and incremental processing in general (Schlangen and Skantze, 2009). In this line of research, it is typically acknowledged that dialogue systems should be set up in a way such that they are able to start speaking before a complete plan of what to say has been built. Skantze and Hjalmarsson (2010) present a model for incremental generation that includes the ability to insert small speech segments for hesitations and fillers, in case the system has not fully planned the current utterance. It is unclear how such a system would be able to deal with scenarios similar to the ones we have investigated in this work. Similarly, other work has looked at appropriate ti"
W17-5529,W10-4301,0,0.0228819,"d filler, which can occur very frequently or rarely depending on the speaker. Finally, Figure 4 illustrates the temporal sequencing 5 Related Work To the best of our knowledge, delayed information presentation has so far not been systematically studied. Various systems, however, addressed the problem in an ad-hoc manner. The 243 task information is not only not detrimental to the interaction, but might in fact be beneficial. From a broader perspective, we see this study on time buying as contributing to research on incremental generation and information presentation for dialogue systems, cf. (Skantze and Hjalmarsson, 2010), and incremental processing in general (Schlangen and Skantze, 2009). In this line of research, it is typically acknowledged that dialogue systems should be set up in a way such that they are able to start speaking before a complete plan of what to say has been built. Skantze and Hjalmarsson (2010) present a model for incremental generation that includes the ability to insert small speech segments for hesitations and fillers, in case the system has not fully planned the current utterance. It is unclear how such a system would be able to deal with scenarios similar to the ones we have investig"
W17-5529,P04-1011,0,0.0376124,"ted via audio only, through high-quality headsets. Each agent handled 10 calls (from the same caller, but treating each as separate), after two training calls. We had 10 participants (balanced for gender), all native German speakers. To provide some control over the interaction, the task was set up so that after a greeting provided by a recording, C formulated their request in one turn (ostensibly, addressing a dialogue system that processed it) which A could hear, but not intervene Introduction How to best present information in a dialogue system is a central, and hence well-studied problem (Stent et al., 2004; Demberg and Moore, 2006; Rieser et al., 2010; Dethlefs et al., 2012b; Wen et al., 2015). What has received less attention is the question of what a system should do until it can present information, in the case that retrieval of this information takes time. 1 A domain in which it is, to this date, realistic that a request needs significant time to be processed, as anyone who has recently used flight search engines can attest. 241 Proceedings of the SIGDIAL 2017 Conference, pages 241–246, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics Figure 1: Phas"
W17-5529,W15-4639,0,0.045545,"Missing"
W17-5529,E06-1009,0,\N,Missing
W18-6547,P15-2017,0,0.0305367,"ication game where two players have the goal of finding each other in a visual environment. To reach this goal, the players need to describe images representing their current location. We analyse a dataset from this domain and show that the nature of image descriptions found in MeetUp! is diverse, dynamic and rich with phenomena that are not present in descriptions obtained through a simple image captioning task, which we ran for comparison. 1 Introduction Automatic description generation from real-world images has emerged as a key task in vision & language in recent years (Fang et al., 2015; Devlin et al., 2015; Vinyals et al., 2015; Bernardi et al., 2016), and datasets like Flickr8k (Hodosh et al., 2013), Flickr30k (Young et al., 2014) or Microsoft CoCo (Lin et al., 2014; Chen et al., 2015) are typically considered to be general benchmarks for visual and linguistic image understanding. By exploiting these sizeable data collections and recent advances in computer vision (e.g. ConvNets, attention, etc.), image description models have 397 Proceedings of The 11th International Natural Language Generation Conference, pages 397–402, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Com"
W18-6547,D14-1086,0,0.157532,"”, “do not describe unimportant details”, (Chen et al., 2015)). (van Miltenburg et al., 2016; van Miltenburg, 2017) later investigated the range of pragmatic phenomena to be found in such caption corpora, with the conclusion that the instructions do not sufficiently control for them and leave it to the labellers to make their own decisions. It is one contribution of the present paper to show that providing a task context results in more constrained descriptions. Schlangen et al. (2016) similarly noted that referring expressions in a corpus that was collected in a (pseudo-)interactive setting (Kazemzadeh et al., 2014), where the describers were provided with immediate feedback about whether their expression was understood, were more concise than those collected in a monological setting (Mao et al., 2016). Similar to MeetUp, the use of various dialogue game set-ups has lately been established for dialogue data collection. Das et al. (2017) designed the “Visual Dialog” task where a human asks an agent about the content of an image. De Vries et al. (2017) similarly collected the GuessWhat? corpus of dialogues in which one player has to ask polar questions in order to identify the correct referent in the pool"
W18-6547,W16-6615,0,0.0802347,"been treated as a problem that can be addressed through the design of instructions (e.g., “do not give people names”, “do not describe unimportant details”, (Chen et al., 2015)). (van Miltenburg et al., 2016; van Miltenburg, 2017) later investigated the range of pragmatic phenomena to be found in such caption corpora, with the conclusion that the instructions do not sufficiently control for them and leave it to the labellers to make their own decisions. It is one contribution of the present paper to show that providing a task context results in more constrained descriptions. Schlangen et al. (2016) similarly noted that referring expressions in a corpus that was collected in a (pseudo-)interactive setting (Kazemzadeh et al., 2014), where the describers were provided with immediate feedback about whether their expression was understood, were more concise than those collected in a monological setting (Mao et al., 2016). Similar to MeetUp, the use of various dialogue game set-ups has lately been established for dialogue data collection. Das et al. (2017) designed the “Visual Dialog” task where a human asks an agent about the content of an image. De Vries et al. (2017) similarly collected th"
W18-6547,E17-4001,0,0.0233641,"Missing"
W18-6547,W17-3503,0,0.187571,"Missing"
W18-6547,W16-3207,0,0.233645,"Missing"
W18-6547,W10-0721,0,0.0482062,"linguistic definition and foundation of image description as a task remains unclear and is a matter of ongoing debate, e.g. see (van Miltenburg et al., 2017) for a conceptual discussion of the task from a cross-lingual perspective. According to (Bernardi et al., 2016), image description generation involves generating a textual description (typically a sentence) that verbalizes the most salient aspects of the image. In practice, however, researchers have observed that eliciting descriptions from naive subjects (i.e. mostly crowd-workers) at a consistent level of quality is a non-trivial task (Rashtchian et al., 2010), as workers seem to interpret the task in different ways. Thus, previous works have developed relatively elaborate instructions and quality checking conventions for being able to systematically collect image descriptions. In this paper, we argue that problems result from the fact that the task is typically put to the workers without providing any further context. This entirely monological setting essentially suggests that determining the salient aspects of an image (like highly important objects, object properties, scene properties) can be solved in a general, “neutral” way, by humans and sys"
W18-6547,P16-1115,1,0.857683,". However, it has been treated as a problem that can be addressed through the design of instructions (e.g., “do not give people names”, “do not describe unimportant details”, (Chen et al., 2015)). (van Miltenburg et al., 2016; van Miltenburg, 2017) later investigated the range of pragmatic phenomena to be found in such caption corpora, with the conclusion that the instructions do not sufficiently control for them and leave it to the labellers to make their own decisions. It is one contribution of the present paper to show that providing a task context results in more constrained descriptions. Schlangen et al. (2016) similarly noted that referring expressions in a corpus that was collected in a (pseudo-)interactive setting (Kazemzadeh et al., 2014), where the describers were provided with immediate feedback about whether their expression was understood, were more concise than those collected in a monological setting (Mao et al., 2016). Similar to MeetUp, the use of various dialogue game set-ups has lately been established for dialogue data collection. Das et al. (2017) designed the “Visual Dialog” task where a human asks an agent about the content of an image. De Vries et al. (2017) similarly collected th"
W18-6547,E12-2021,0,0.0579231,"Missing"
W18-6547,N03-1033,0,0.025141,"Missing"
W18-6547,Q14-1006,0,0.147059,"to describe images representing their current location. We analyse a dataset from this domain and show that the nature of image descriptions found in MeetUp! is diverse, dynamic and rich with phenomena that are not present in descriptions obtained through a simple image captioning task, which we ran for comparison. 1 Introduction Automatic description generation from real-world images has emerged as a key task in vision & language in recent years (Fang et al., 2015; Devlin et al., 2015; Vinyals et al., 2015; Bernardi et al., 2016), and datasets like Flickr8k (Hodosh et al., 2013), Flickr30k (Young et al., 2014) or Microsoft CoCo (Lin et al., 2014; Chen et al., 2015) are typically considered to be general benchmarks for visual and linguistic image understanding. By exploiting these sizeable data collections and recent advances in computer vision (e.g. ConvNets, attention, etc.), image description models have 397 Proceedings of The 11th International Natural Language Generation Conference, pages 397–402, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics various map locations. While similar in some respects, MeetUp is distinguished by being a symmetrical tas"
W18-6563,W17-3207,0,0.111961,"r activation functions. Mao et al. (2015), on the other hand, adopt a simple linear layer for decoding the LSTM. In the following, we will investigate how these modeling decisions affect the performance, and how they interact with different search methods during inference.2 We distinguish two variants of our model according to their decoding layer: We now turn to (neural) MT, where decoding algorithms for sequence generation have been investigated in detail. Here, beam search is the standard method for syntax- and phrase-based models (Rush et al., 2013), as well as for neural encoderdecoders (Freitag and Al-Onaizan, 2017). However, an important difference between the two is that candidates in phrase-based MT are completed in the same number of steps, whereas neural models generate hypotheses of different length and are biased for shorter output (Huang et al., 2017). To counteract this bias, OpenNMT (Klein et al., 2017) adopts three metrics for normalizing the coverage, length and end of sentence of candidate translations. Unfortunately, two of these metrics (coverage and end of sentence normalization) are based on the length of the source sentence, which is not available in REG. Another common NMT framework (B"
W18-6563,D15-1224,0,0.0889948,"y different than determining the length of a good translation: a translation is complete when it covers the meaning of the words in the source sentence, and indeed, the length of the source is used as criterion in beam search for MT (see Section 2.3). A referring expression, on the other hand, is complete when it describes the visual target in a pragmatically adequate way, i.e. when it neither provides too little nor too much information. 2.2 Neural REG from real-world images More recently, research on REG has started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016), representing scenes with many different types of real-world objects. Here, the input to the REG system is defined as a low-level visual representation such that various aspects of the task have to be addressed, including lexicalization and content selection. Inspired by research on image captioning, Mao et al. (2015) proposed the first neural end-to-end model for REG that uses a CNN to repWe explore a range of different variants of beam search that have been proposed for MT and, interestingly, find that most of them decrease performance as comp"
W18-6563,D17-1210,0,0.0260847,"Missing"
W18-6563,D17-1227,0,0.0245425,"ring inference.2 We distinguish two variants of our model according to their decoding layer: We now turn to (neural) MT, where decoding algorithms for sequence generation have been investigated in detail. Here, beam search is the standard method for syntax- and phrase-based models (Rush et al., 2013), as well as for neural encoderdecoders (Freitag and Al-Onaizan, 2017). However, an important difference between the two is that candidates in phrase-based MT are completed in the same number of steps, whereas neural models generate hypotheses of different length and are biased for shorter output (Huang et al., 2017). To counteract this bias, OpenNMT (Klein et al., 2017) adopts three metrics for normalizing the coverage, length and end of sentence of candidate translations. Unfortunately, two of these metrics (coverage and end of sentence normalization) are based on the length of the source sentence, which is not available in REG. Another common NMT framework (Bahdanau et al., 2014) uses a shrinking beam where beam size is reduced each time a completed hypothesis is found, and search terminates when the beam size has reached 0. Another shortcoming of beam search observed in previous work is that the beam"
W18-6563,D14-1086,0,0.785488,"am search, is conceptually different than determining the length of a good translation: a translation is complete when it covers the meaning of the words in the source sentence, and indeed, the length of the source is used as criterion in beam search for MT (see Section 2.3). A referring expression, on the other hand, is complete when it describes the visual target in a pragmatically adequate way, i.e. when it neither provides too little nor too much information. 2.2 Neural REG from real-world images More recently, research on REG has started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016), representing scenes with many different types of real-world objects. Here, the input to the REG system is defined as a low-level visual representation such that various aspects of the task have to be addressed, including lexicalization and content selection. Inspired by research on image captioning, Mao et al. (2015) proposed the first neural end-to-end model for REG that uses a CNN to repWe explore a range of different variants of beam search that have been proposed for MT and, interestingly, find that most of them decrea"
W18-6563,P17-4012,0,0.177613,"el according to their decoding layer: We now turn to (neural) MT, where decoding algorithms for sequence generation have been investigated in detail. Here, beam search is the standard method for syntax- and phrase-based models (Rush et al., 2013), as well as for neural encoderdecoders (Freitag and Al-Onaizan, 2017). However, an important difference between the two is that candidates in phrase-based MT are completed in the same number of steps, whereas neural models generate hypotheses of different length and are biased for shorter output (Huang et al., 2017). To counteract this bias, OpenNMT (Klein et al., 2017) adopts three metrics for normalizing the coverage, length and end of sentence of candidate translations. Unfortunately, two of these metrics (coverage and end of sentence normalization) are based on the length of the source sentence, which is not available in REG. Another common NMT framework (Bahdanau et al., 2014) uses a shrinking beam where beam size is reduced each time a completed hypothesis is found, and search terminates when the beam size has reached 0. Another shortcoming of beam search observed in previous work is that the beam tends to contain many candidates that share the same (m"
W18-6563,J03-1003,0,0.0633258,"Missing"
W18-6563,J12-1006,0,0.444858,"Missing"
W18-6563,1983.tc-1.13,0,0.75122,"Missing"
W18-6563,W10-4210,0,0.304201,"Missing"
W18-6563,P02-1040,0,0.102814,"short. However, the fact that CIDEr scores still improve in some cases suggests that beam search leads to linguistically more well-formed expressions (expressions with a lot of repetitions are avoided, e.g. the blue blue shirt). they collected additional expressions for the testsets, resulting in 10 expressions per objects. As we did not have access to these additional expressions at the time of writing, we follow Yu et al. (2016) and evaluate on the original RefCOCO collections with 3 expressions on average per object. In the experiments below, we look at three measures: BLEU1 for unigrams (Papineni et al., 2002), CIDEr (Vedantam et al., 2015) and lenr (length ratio) as provided by the MSCOCO evaluation server (Chen et al., 2015). We are interested in the length ratio as a simple approximation of traditionally used measures in REG (Gatt and Belz, 2010), reflecting whether the generation output contains too much or too little information (attributes or words). BLEU1 gives us an indication of the lexical overlap between output and target, whereas CIDEr operates on the level of n-grams. 6.2 6.3 Modified beam search In Table 3, we report performance of the region and global model with the linear decoder a"
W18-6563,P16-1058,1,0.923285,"of a good translation: a translation is complete when it covers the meaning of the words in the source sentence, and indeed, the length of the source is used as criterion in beam search for MT (see Section 2.3). A referring expression, on the other hand, is complete when it describes the visual target in a pragmatically adequate way, i.e. when it neither provides too little nor too much information. 2.2 Neural REG from real-world images More recently, research on REG has started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016), representing scenes with many different types of real-world objects. Here, the input to the REG system is defined as a low-level visual representation such that various aspects of the task have to be addressed, including lexicalization and content selection. Inspired by research on image captioning, Mao et al. (2015) proposed the first neural end-to-end model for REG that uses a CNN to repWe explore a range of different variants of beam search that have been proposed for MT and, interestingly, find that most of them decrease performance as compared to simple greedy decoding in REG. Whereas g"
W18-6563,D13-1022,0,0.024316,"who also use dropout in the decoding layer, but only linear activation functions. Mao et al. (2015), on the other hand, adopt a simple linear layer for decoding the LSTM. In the following, we will investigate how these modeling decisions affect the performance, and how they interact with different search methods during inference.2 We distinguish two variants of our model according to their decoding layer: We now turn to (neural) MT, where decoding algorithms for sequence generation have been investigated in detail. Here, beam search is the standard method for syntax- and phrase-based models (Rush et al., 2013), as well as for neural encoderdecoders (Freitag and Al-Onaizan, 2017). However, an important difference between the two is that candidates in phrase-based MT are completed in the same number of steps, whereas neural models generate hypotheses of different length and are biased for shorter output (Huang et al., 2017). To counteract this bias, OpenNMT (Klein et al., 2017) adopts three metrics for normalizing the coverage, length and end of sentence of candidate translations. Unfortunately, two of these metrics (coverage and end of sentence normalization) are based on the length of the source se"
W18-6563,P16-1115,1,0.84577,"entation that the LSTM is conditioned on. Whereas Mao et al. (2015) extract visual representations of the region representing the target referent and the global image, Yu et al. (2016) report a slightly detrimental effect of including these global context features. Thus, we distinguish two variants of the model according to its visual representation: Target: 4103-dimensional vector, obtained by cropping the image to the target region, resizing to 224 × 224, extracting its CNN pre-softmax features with VGG19 (Simonyan and Zisserman, 2014) and concatenating 7 spatial features of the region (see Schlangen et al. (2016) for these) Global+target: 8119-dimensional vector, obtained by extracting the CNN pre-softmax features with VGG19 (Simonyan and Zisserman, 2014) for the entire image, and concatenating it with targetonly Training We set the word embedding layer size to 512, and the hidden state to 1024. We optimized with ADAM (with α = 0.001), and the batch size set to 50. The word embedding layer is initialized with random weights. The number of training epochs was tuned for each model on the validation set. 4 Decoding Strategies lp(y) = We now explain the different decoding strategies that will be combined"
W18-6906,W13-4063,0,0.030859,"at to say that matters: an utterance that is appropriate at a particular point in time, might already be perceived as inappropriate or confusing shortly after. To the best of our knowledge, aspects of monitoring and timing have not been addressed in datadriven NLG frameworks, though incremental processing has been shown to be highly effective in experimental or rule-based settings, cf. (Skantze and Hjalmarsson, 2013; Skantze et al., 2014; Buß and Schlangen, 2010). In the dialogue community, specific tasks that involve timing have been modelled in a data-driven way, such as barge-in detection (Selfridge et al., 2013), end-of-utterance detection (Raux and Eskenazi, 2012; Maier et al., 2017)), or turn-taking (Skantze, 2017) . Even less work has been carried out on NLG systems that are able to produce revision, repair or correction utterances which can be essential to achieve task success, as shown in Figure 2. In (Zarrieß and Schlangen, 2016), we have explored an installment-based approach in a referring expression generation system for objects in real-world images, and found that even simple, 29 1 IG: then you take the green W ... top right 2 3 4 IG: and you turn it to the left IG: uh now it's to the right"
W18-6906,W17-5527,0,0.013082,"s inappropriate or confusing shortly after. To the best of our knowledge, aspects of monitoring and timing have not been addressed in datadriven NLG frameworks, though incremental processing has been shown to be highly effective in experimental or rule-based settings, cf. (Skantze and Hjalmarsson, 2013; Skantze et al., 2014; Buß and Schlangen, 2010). In the dialogue community, specific tasks that involve timing have been modelled in a data-driven way, such as barge-in detection (Selfridge et al., 2013), end-of-utterance detection (Raux and Eskenazi, 2012; Maier et al., 2017)), or turn-taking (Skantze, 2017) . Even less work has been carried out on NLG systems that are able to produce revision, repair or correction utterances which can be essential to achieve task success, as shown in Figure 2. In (Zarrieß and Schlangen, 2016), we have explored an installment-based approach in a referring expression generation system for objects in real-world images, and found that even simple, 29 1 IG: then you take the green W ... top right 2 3 4 IG: and you turn it to the left IG: uh now it's to the right 5 6 7 8 IG: yes IG: turn left . yes IG: a little more IG: so that it's diagonal IG: a little more IG: exac"
W18-6906,P15-2017,0,0.0227588,"re the task would be to generate a verbal instruction that enables the IF to execute a particular action or achieve a state change of the environment, while the system (the IG) is given the current and the goal state of an environment as an image. This would be natural extension of existing language generation systems that are able to generate descriptions of real-world images (Bernardi et al., 2016), or referring expressions to objects in real-world images (Yu et al., 2017). At the same time, it would require systems to go beyond the commonly used CNN-LSTM architecture (Vinyals et al., 2015; Devlin et al., 2015; Mao et al., 2016; Yu et al., 2017) as these currently only map visual representations of single images or objects to verbal output. Instead, a visually grounded instruction generation system needs to reason about expressions that relate the current visual state to a target state, such as place the block to the right (source state) as the highest block on the board (target state) in Figure 1. Conceptually, the problem of generating instructions in object assembly domains is similar to generating relational referring expressions • vision: generating instructions from a lowlevel visual represen"
W18-6906,W12-1504,0,0.0162724,"ctures (Andreas et al., 2016). However, none of these models is designed for generating relational structures in verbal expressions, such as instructions. 3 Spoken language dynamics From research on situated spoken dialogue, it is well known that spoken and written language bear very different affordances. In spoken communication, listeners react, both non-verbally and verbally, to what speakers are saying, while they are saying it; and speakers adapt what they are saying, based on the reactions (or lack thereof) that they get, while they are speaking. The field of Conversation Analysis (see (Stivers and Sidnell, 2012) for a recent overview) and, taking up and further developing some of their ideas, the work of Herbert (Clark, 1996) has done much to shed light on the intricate strategies that interactants follow to coconstruct dialogue in this way. Figure 2 illustrates some prominent strategies that speakers use to achieve task success in spoken communication, with an instruction giving example taken from our PentoRef data (Zarrieß et al., 2016). Here, the IF has to assemble an object out of Pentomino pieces while the IG observes his actions over a camera feed. During a time span of approximately 30 seconds"
W18-6906,W11-2845,0,0.0340559,"stems: here, a human instruction follower (IF) and an agent as the instruction giver (IG) have to achieve a common goal in a visual environment (e.g. find a route or treasure, assemble an object). The IG knows how to complete the task (e.g. where the treasure is, how the object looks like) but cannot affect the environment. The IF can affect the environment and the objects in it, but needs the IG’s instructions to achieve the goal. In the context of the GIVE challenge (Byron et al., 2007), this setting has received considerable attention in the NLG community for some time (Byron et al., 2009; Striegnitz et al., 2011), but has not been developed further since then. Generally, we believe that future approaches to instruction giving in NLG should extend GIVE along the following dimensions, in order to enable transfer of NLG technology to real-world applications like robots or dialogue systems: Figure 1: Instruction example in the BLOCKS data set (Bisk et al., 2018) the visually present objects and their properties. In the meantime, a lot of research in human-robot interaction has be done on modeling instructions in more realistic visual environments, though this community has often focussed on grounding verb"
W18-6906,D14-1086,0,0.0275169,"a given input to some written output, meaning that the environment does not change while the system is producing output Introduction The past decade has seen substantial progress in data-driven methods for natural language generation (NLG). It is now widely agreed that datadriven techniques are needed to obtain NLG systems that are adaptive and human-like (Belz, 2008), domain-independent (Wen et al., 2016), and – with recent methods from vision & language cf. (Bernardi et al., 2016) – suitable for agents that interact with humans in a physical environment (such as dialogue systems or robots) (Kazemzadeh et al., 2014). Despite this progress, however, data-driven NLG is rarely used in current real-world interactive systems, where more traditional (template-based) approaches for generating verbal output still persist. • perfect input: NLG systems are often trained on perfect representations of an environment or a knowledge base • one-shot output: NLG systems do not need to monitor whether the listener has actually understood the output, strategies that are frequent in conversation (revision, correction, installments) do not have to be considered • no temporal dimension: NLG systems assume that their output i"
W18-6906,P17-1063,0,0.0221661,"task-oriented conversation in shared visual space from (Zarrieß et al., 2016): the joint task for the IF and IG is to build a puzzle out of Pentomino pieces where the IF can manipulate pieces on a physical gameboard and the IG sees the outline of the puzzle, observes the IF’s actions in real-time (over a camera feed) and instructs the IF over headphones; the overall interaction time shown here is approx. 30 secconds; utterances have been translated to English from German transciptions hand-crafted strategies for repair and revision very clearly improve the referential success of the system. (Villalba et al., 2017) propose a formal approach to generating contrastive referring expressions which is designed for similar scenarios. What is clearly missing to date, however, is a data-driven NLG framework that encompasses these various aspects of conversational grounding and timing in interaction. 5 of the IEEE Conference on Computer Vision and Pattern Recognition, pages 39–48. Anja Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):431455. Raffaella Bernardi, Ruket Cakici, Desmond Elliott, Aykut Erdem, Erku"
W18-6906,J12-1006,0,0.0228558,"Missing"
W18-6906,D16-1233,0,0.0391542,"Missing"
W18-6906,L16-1019,1,0.928306,"speakers adapt what they are saying, based on the reactions (or lack thereof) that they get, while they are speaking. The field of Conversation Analysis (see (Stivers and Sidnell, 2012) for a recent overview) and, taking up and further developing some of their ideas, the work of Herbert (Clark, 1996) has done much to shed light on the intricate strategies that interactants follow to coconstruct dialogue in this way. Figure 2 illustrates some prominent strategies that speakers use to achieve task success in spoken communication, with an instruction giving example taken from our PentoRef data (Zarrieß et al., 2016). Here, the IF has to assemble an object out of Pentomino pieces while the IG observes his actions over a camera feed. During a time span of approximately 30 seconds, the IG produces 18 short utterances in total that instruct the IF what to do next (e.g. turn to the left), confirm the IF’s action (exactly), or repair what she is currently doing (to the left, this is to the right). Also, interestingly, the final step of the instruction (i.e. how to put the target piece to its target location, image 10-12 in Figure 2) is left underspecified by the IG as it is obvious to the IF how to complete th"
W18-6906,P16-1058,1,0.82785,"be highly effective in experimental or rule-based settings, cf. (Skantze and Hjalmarsson, 2013; Skantze et al., 2014; Buß and Schlangen, 2010). In the dialogue community, specific tasks that involve timing have been modelled in a data-driven way, such as barge-in detection (Selfridge et al., 2013), end-of-utterance detection (Raux and Eskenazi, 2012; Maier et al., 2017)), or turn-taking (Skantze, 2017) . Even less work has been carried out on NLG systems that are able to produce revision, repair or correction utterances which can be essential to achieve task success, as shown in Figure 2. In (Zarrieß and Schlangen, 2016), we have explored an installment-based approach in a referring expression generation system for objects in real-world images, and found that even simple, 29 1 IG: then you take the green W ... top right 2 3 4 IG: and you turn it to the left IG: uh now it's to the right 5 6 7 8 IG: yes IG: turn left . yes IG: a little more IG: so that it's diagonal IG: a little more IG: exactly IG: and now put to the left next to the T 9 10 11 12 IG: yes exactly IG: like this IG: exactly IG: into this spot IG: now you take the pink piece over there ... IG: to the left IG: this is to the right Figure 2: Example"
W19-0424,E17-2039,0,0.0606868,"Missing"
W19-0424,D15-1075,0,0.263052,"aption from COCO (left) paired with an object from Visual Genome (slotted into a “there is (a) ” frame for presentation) taken from the same image (middle), and a randomly sampled object (right) in (7-a) and (7-b), and with region descriptions (also from Visual Genome) in the other examples. 12 Our inspiration for this approach comes from two sources. As mentioned, Young et al. (2014) used image captions to create their “approximate entailment” data sets; our proposals here can be seen as a generalisation of this to other pairings. Further, the original “natural language inference” dataset by Bowman et al. (2015) used captions as seeds, but had the entailments and contradictions manually generated and not derived via image relations, as we do here. 13 Note that the task was to judge pairs, not to decide between two hypotheses, which would presumably be a simpler task. (7) a. b. c. d. A man wearing a black cap leaning against a fence getting ready to play baseball. ||there is (a) man |there is (a) cow Rice, broccoli, and other food items sitting beside each other ||there is (a) health foods | there is (a) granite A man playing Wii in a room ||there is/are (a) a plant that sits on a desk |there is/are ("
W19-0424,D18-2029,0,0.0171735,"all models. Here, this would mean testing, along the lines developed in Section 3, whether all images (in a sub-corpus held for that purpose) that make the premise true also make the hypothesis true. We try something else here, which is more like model-building (Bos, 2003), for data of the type illustrated in (7) above. The idea is as follows. Given the premise (in our case, always a caption), we retrieve a set of images (other than that from which the caption was taken), via captions that are nearest neighbours in a text embedding space (for which we used the “universal sentence encoder” by Cer et al. (2018)). That is, we make use of a derived expression/expression relation, to create a relation between an expression and a set of retrieved models. (One can think of these as situation exemplars stored in memory and retrieved via their short descriptions.) Figure 9 shows such a retrieved model (abstracted away from the actual image content, which is not used), for the trigger caption “An airplane flying through the sky on a cloudy day.” and retrieved via its most similar caption “White and blue airplane flying in a grey sky.”. We then test the candidate expressions (or rather, their “logical forms”"
W19-0424,W16-3202,0,0.0550032,"Missing"
W19-0424,D14-1086,0,0.0405515,"no stipulation is made about whether it is or is not true of other objects in the image. Annotators were encouraged to provide region descriptions that are relational, and these then form the basis of an abstracted repre- Figure 5: A region description from Visual Genome sentation of that relation. Figure 5 shows an example of such a region description; the corresponding annotation is shown in (3), slightly re-arranged to make clearer its similarity to classical logical forms (LFs).10 (3) 8 ""next to a"":be.v.01(1060704:puzzle.n.01, 1060699:computer monitor.n.01) These corpora have been used by Kazemzadeh et al. (2014), Yu et al. (2016), Mao et al. (2016), Schlangen et al. (2016), Cirik et al. (2018) to train and test models of referring expression resolution. 9 For a portion of GoogleREX, this was added by Cirik et al. (2018). 10 What this also illustrates is that the normalisation decisions made in the corpus can occasionally be somewhat questionable. Here, the part “next to a” is normalised to the verb “be”; presumably, the annotator added the elided copula here and rather ignored the spatial relation. Figure 7: “A man standing in the snow with skis on.” (left), and distractors (visual similarity, middle"
W19-0424,P16-1115,1,0.924223,"00 annotated sentences, and the Parallel Meaning Bank (Abzianidze et al., 2017) another 15,000. There is no competition here, though: the Meaning Bank annotations are obviously much deeper and much more detailed; the proposal in this paper is to view the image corpora discussed here as complementary. 5 The relation between images and models is implicit in (Young et al., 2014), from where we took inspiration, but not further developed there in the way that we are attempting here. H¨urlimann and Bos (2016) make an explicit connection between image and models, but only look at denotations; as do Schlangen et al. (2016). referring expressions by Yu et al. (2016), using the ReferitGame where one player needs to get another to identify a predetermined object in the image, with the players getting feedback on their success. Mao et al. (2016) also provide expressions for COCO objects, but collected monologically with the instructions to provide an expression that uniquely describes the target object. • Flickr30k / Flickr30kEntities: Flickr30k (Young et al., 2014) is a collection of 30,000 images from a public image website which were augmented with 160,000 captions; Plummer et al. (2015) annotated these captions"
W19-0424,Q14-1006,0,0.716938,"here provide almost 8 million distinct natural language expressions (with many more that can be derived from them). In comparison, the largest “classical” semantics resource, the Groningen Meaning Bank (Bos et al., 2017), provides some 10,000 annotated sentences, and the Parallel Meaning Bank (Abzianidze et al., 2017) another 15,000. There is no competition here, though: the Meaning Bank annotations are obviously much deeper and much more detailed; the proposal in this paper is to view the image corpora discussed here as complementary. 5 The relation between images and models is implicit in (Young et al., 2014), from where we took inspiration, but not further developed there in the way that we are attempting here. H¨urlimann and Bos (2016) make an explicit connection between image and models, but only look at denotations; as do Schlangen et al. (2016). referring expressions by Yu et al. (2016), using the ReferitGame where one player needs to get another to identify a predetermined object in the image, with the players getting feedback on their success. Mao et al. (2016) also provide expressions for COCO objects, but collected monologically with the instructions to provide an expression that uniquely"
W19-5938,N16-3020,0,0.0457567,"be that while your friend may have been unwilling to tell you their reasons, the car rental company, having used a complex statistical model that judged you untrustworthy, based on various kinds of information it has about you, would be unable to state reasons (other than a vacuous one like “your score is too low”). The field of explainable AI has set itself as a goal to open up the blackbox of current prediction models in order to make their decisions more transparent and also identifying problems concerning the core issues in AI safety. (See (Gilpin et al., 2018; Doshi-Velez and Kim, 2017; Ribeiro et al., 2016; Lundberg and Lee, 2017; Amodei et al., 2016) for recent overviews.) The focus there typically is on providing explanations of decisions in terms of examples or secondary models (e.g. (Kim et al., 2018; Letham et al., 2015; Yuan et al., 2019; Zhang et al., 2019)), where the resulting explanations are understandable at best to experts. In contrast, our interest is in learning to provide verbal explanations, accessible also to novice users. As a first step, we are interested in methods for eliciting data that can be used for this. In this paper, we present an annotation scheme where a pair of a"
W19-8621,D14-1086,0,0.130205,"compose linguistic expressions from that, for the given addressee and under the constraints of the given communicative intention. Many of the decisions involved in this do not only require general visual and linguistic competences, but are well-known to be affected by the task, the context and the intended addressee. Consequently, recent progress in the area of NLG, Language & Vision has been made by moving from generic settings like image captioning (Lin et al., 2014; Chen et al., 2015; Hodosh et al., 2013; Plummer et al., 2015) to task-oriented settings like referring expression generation (Kazemzadeh et al., 2014; Yu et al., 2016) or interactive visual question answering (Das et al., 2017; De Vries et al., 2017). As shown by Ilinykh et al. (2018), task-based image descriptions substantially differ in terms of their linguistic properties (e.g. occurrence of referring expressions, attribute types) from their “neutral” counterparts. ∗ 1 See (Gatt and Krahmer, 2018) for a survey on this traditional area in NLG. 2 This setting is somewhat similar to that of Lin et al. (2015), who collected texts meant to describe a scene to someone who can’t see it, but it is tuned even more towards (imagined) interaction."
W19-8621,J95-2003,0,0.309365,"Missing"
W19-8621,J86-3001,0,0.781078,"Missing"
W19-8621,W18-6547,1,0.845366,"the decisions involved in this do not only require general visual and linguistic competences, but are well-known to be affected by the task, the context and the intended addressee. Consequently, recent progress in the area of NLG, Language & Vision has been made by moving from generic settings like image captioning (Lin et al., 2014; Chen et al., 2015; Hodosh et al., 2013; Plummer et al., 2015) to task-oriented settings like referring expression generation (Kazemzadeh et al., 2014; Yu et al., 2016) or interactive visual question answering (Das et al., 2017; De Vries et al., 2017). As shown by Ilinykh et al. (2018), task-based image descriptions substantially differ in terms of their linguistic properties (e.g. occurrence of referring expressions, attribute types) from their “neutral” counterparts. ∗ 1 See (Gatt and Krahmer, 2018) for a survey on this traditional area in NLG. 2 This setting is somewhat similar to that of Lin et al. (2015), who collected texts meant to describe a scene to someone who can’t see it, but it is tuned even more towards (imagined) interaction. We also collected data for about 4 times as many images. Work done while at Bielefeld University. 152 Proceedings of The 12th Internati"
W19-8621,P16-1058,1,0.885744,"Missing"
W19-8653,D15-1166,0,0.053417,"ise have been produced, in predictable ways? Our results show that this expectation is partially supported. Introduction Sequential deep learning language models with an attention mechanism are able to use not only the immediately previous inputs, but can involve context by “attending to” select parts of the whole input sequence at each time step. This has first been shown to be helpful for neural machine translation, which operates on sequences of words. Here, deep learning networks with attention are capable to jointly learn the alignment and translation of languages (Bahdanau et al., 2014; Luong et al., 2015). From the beginning, the dynamics of the attention while generating output sequences has been seen as providing insight into the workings of the models, if only qualitatively. This is in particular applicable to the interdisciplinary fields of natural language processing and computer vision like image captioning. For example, Xu et al. (2015) 427 Proceedings of The 12th International Conference on Natural Language Generation, pages 427–431, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics 2 Related Work dataset images are resized to 448x448 pixels not keepi"
W19-8653,P19-1282,0,0.023517,"1: A caption generated by an image captioning model. The attention is pixelated and summed up over all time steps. In addition, the dog and the bicycle are framed with the corresponding bounding boxes. overlay the attention weights over the input image and show how it shifts while generating the image caption step by step. We show a variant of this visualisation type in Figure 1. The implicit argument at least seems to be that this is informative, because there is a causal relation between where the attention is placed and what is being produced, which has been critically discussed recently (Serrano and Smith, 2019; Jain and Wallace, 2019). In this paper, we address the question of whether this assumed connection can be used to assert additional control over the generation process. We train a caption generation model with spatial attention in the usual way, but then at test time override its attention mechanism and force it to attend to pre-determined parts of the image. Does this cause the generated output to be different from what would otherwise have been produced, in predictable ways? Our results show that this expectation is partially supported. Introduction Sequential deep learning language models"
W19-8653,D17-1098,0,0.0646862,"the aspect ratio. We apply only basic tokenization on the captions. Then the captions of the dataset are prepared to contain only captions that have a maximal length of 16 words. Furthermore, the vocabulary is constrained to the 10,000 most common words and we discard captions that are containing words not included in this vocabulary. The caption-image pairs are shuffled randomly before training. As in the work of Xu et al. (2015), we apply dropout and use the Adam optimizer to minimize the loss function There have been several attempts to achieve more control over neural language generation. Anderson et al. (2017) control the output of a captioning model at test time with an enhanced beam search. An external system is generating image tags as a control signal at the decoder level. They show that adding the additional hints for the generation process actually improves the performance for out-of-domain captioning. Although this approach works, there is no attention effecting mechanism involved. Zarrieß and Schlangen (2018) evaluate a “trainable decoding” approach that inserts taskspecific concerns into the decoding process. Interfering with the attention mechanism after training, however, has to our know"
W19-8653,W18-6563,1,0.697981,"work of Xu et al. (2015), we apply dropout and use the Adam optimizer to minimize the loss function There have been several attempts to achieve more control over neural language generation. Anderson et al. (2017) control the output of a captioning model at test time with an enhanced beam search. An external system is generating image tags as a control signal at the decoder level. They show that adding the additional hints for the generation process actually improves the performance for out-of-domain captioning. Although this approach works, there is no attention effecting mechanism involved. Zarrieß and Schlangen (2018) evaluate a “trainable decoding” approach that inserts taskspecific concerns into the decoding process. Interfering with the attention mechanism after training, however, has to our knowledge been tried less often. Cornia et al. (2018) train a captioning model not only to learn the distribution for images and sentences, but also for bounding boxes and noun chunks. In addition, the model has to learn when to switch between boxes. As a result, the captioning model is controllable by a bounding box sequence provided as an input to the network at test time. Although this approach has been shown to"
W19-8653,N19-1357,0,0.0213724,"y an image captioning model. The attention is pixelated and summed up over all time steps. In addition, the dog and the bicycle are framed with the corresponding bounding boxes. overlay the attention weights over the input image and show how it shifts while generating the image caption step by step. We show a variant of this visualisation type in Figure 1. The implicit argument at least seems to be that this is informative, because there is a causal relation between where the attention is placed and what is being produced, which has been critically discussed recently (Serrano and Smith, 2019; Jain and Wallace, 2019). In this paper, we address the question of whether this assumed connection can be used to assert additional control over the generation process. We train a caption generation model with spatial attention in the usual way, but then at test time override its attention mechanism and force it to attend to pre-determined parts of the image. Does this cause the generated output to be different from what would otherwise have been produced, in predictable ways? Our results show that this expectation is partially supported. Introduction Sequential deep learning language models with an attention mechan"
