2020.acl-main.463,2014.lilt-9.5,0,0.0324575,"unication generally, relies on joint attention and intersubjectivity: the ability to be aware of what another human is attending to and guess what they are intending to communicate. Human children do not learn meaning from form alone and we should not expect machines to do so either. 7 Distributional semantics Distributional semanticists have long been aware that grounding distributional representations in the real world is challenging. The lexical similarity relations learned by distributional models trained on text don’t in themselves connect any of those words to the world (Herbelot, 2013; Baroni et al., 2014; Erk, 2016; Emerson, 2020), and the distributions of words may not match the distribution of things in the world (consider four-legged dogs). One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos (Hossain et al., 2019) or other modalities (Kiela and Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been Engl"
2020.acl-main.463,2020.emnlp-main.703,0,0.112501,"nd Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been English. 5190 emotional stress (McDuff and Kapoor, 2019) or eye gaze (Koller et al., 2012), which contains a signal about the felicitous uses of forms. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. (2020). We agree that this is an exciting avenue of research. From this literature we can see that the slogan “meaning is use” (often attributed to Wittgenstein, 1953), refers not to “use” as “distribution in a text corpus” but rather that language is used in the real world to convey communicative intents to real people. Speakers distill their past experience of language use into what we call “meaning” here, and produce new attempts at using language based on this; this attempt is successful if the listener correctly deduces the speaker’s communicative intent. Thus, standing meanings evolve over tim"
2020.acl-main.463,D15-1075,0,0.0145669,"guage and ask top-down questions. Neural methods are not the first bottom-up success in NLP; they will probably not be the last. Second, be aware of the limitations of tasks: Artificial tasks like bAbI (Weston et al., 2016) can help get a field of research off the ground, but there is no reason to assume that the distribution of language in the test data remotely resembles the distribution of real natural language; thus evaluation results on such tasks must be interpreted very carefully. Similar points can be made about crowdsourced NLI datasets such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015), which do not represent questions that any particular person really wanted to ask about a text, but the somewhat unnatural communicative situation of crowdsourcing work. If a system does better on such a task than the inter-annotator agreement,9 the task probably has statistical artifacts that do not represent meaning. In the vision community, Barbu et al. (2019) offer a novel dataset which explicitly tries to achieve a more realistic distribution of task data; it would be interesting to explore similar ideas for language. Third, value and support the work of carefully creating new tasks (see"
2020.acl-main.463,P16-1141,0,0.0352501,"“distribution in a text corpus” but rather that language is used in the real world to convey communicative intents to real people. Speakers distill their past experience of language use into what we call “meaning” here, and produce new attempts at using language based on this; this attempt is successful if the listener correctly deduces the speaker’s communicative intent. Thus, standing meanings evolve over time as speakers can different experiences (e.g. McConnellGinet, 1984), and a reflection of such change can be observed in their changing textual distribution (e.g. Herbelot et al., 2012; Hamilton et al., 2016). 8 On climbing the right hills What about systems which are trained on a task that is not language modeling — say, semantic parsing, or reading comprehension tests — and that use word embeddings from BERT or some other large LM as one component? Numerous papers over the past couple of years have shown that using such pretrained embeddings can boost the accuracy of the downstream system drastically, even for tasks that are clearly related to meaning. Our arguments do not apply to such scenarios: reading comprehension datasets include information which goes beyond just form, in that they specif"
2020.acl-main.463,P19-1356,0,0.0151443,"’) that aims to answer this question. The methodology of probing tasks (e.g. Adi et al., 2017; Ettinger et al., 2018) has been used to show that 1 https://www.nytimes.com/2018/11/18/technology/artific ial-intelligence-language.html, accessed 2019/12/04 2 https://www.business2community.com/seo/what-t o-do-about-bert-googles-recent-local-algorithm-updat e-02259261, accessed 2019/12/04 3 https://www.blog.google/products/search/search-langu age-understanding-bert/, accessed 2019/12/04 large LMs learn at least some information about phenomena such as English subject-verb agreement (Goldberg, 2019; Jawahar et al., 2019), constituent types, dependency labels, NER, and (core) semantic role types (again, all in English) (Tenney et al., 2019).4 Hewitt and Manning (2019) find information analogous to unlabeled dependency structures in the word vectors provided by ELMo and BERT (trained on English). And of course it is well established that vector-space representations of words pick up word classes, both syntactic (POS, e.g. Lin et al., 2015) and semantic (lexical similarity, e.g. Rubenstein and Goodenough, 1965; Mikolov et al., 2013). Others have looked more closely at the success of the large LMs on apparently m"
2020.acl-main.463,P15-2038,0,0.0208776,"are that grounding distributional representations in the real world is challenging. The lexical similarity relations learned by distributional models trained on text don’t in themselves connect any of those words to the world (Herbelot, 2013; Baroni et al., 2014; Erk, 2016; Emerson, 2020), and the distributions of words may not match the distribution of things in the world (consider four-legged dogs). One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos (Hossain et al., 2019) or other modalities (Kiela and Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been English. 5190 emotional stress (McDuff and Kapoor, 2019) or eye gaze (Koller et al., 2012), which contains a signal about the felicitous uses of forms. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. (2020). We agree that t"
2020.acl-main.463,D15-1293,0,0.0166871,"cists have long been aware that grounding distributional representations in the real world is challenging. The lexical similarity relations learned by distributional models trained on text don’t in themselves connect any of those words to the world (Herbelot, 2013; Baroni et al., 2014; Erk, 2016; Emerson, 2020), and the distributions of words may not match the distribution of things in the world (consider four-legged dogs). One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos (Hossain et al., 2019) or other modalities (Kiela and Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been English. 5190 emotional stress (McDuff and Kapoor, 2019) or eye gaze (Koller et al., 2012), which contains a signal about the felicitous uses of forms. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. (2"
2020.acl-main.463,W12-1604,1,0.615316,"t match the distribution of things in the world (consider four-legged dogs). One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos (Hossain et al., 2019) or other modalities (Kiela and Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been English. 5190 emotional stress (McDuff and Kapoor, 2019) or eye gaze (Koller et al., 2012), which contains a signal about the felicitous uses of forms. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. (2020). We agree that this is an exciting avenue of research. From this literature we can see that the slogan “meaning is use” (often attributed to Wittgenstein, 1953), refers not to “use” as “distribution in a text corpus” but rather that language is used in the real world to convey communicative intents to real people. Speakers distill their past e"
2020.acl-main.463,J82-2005,0,0.626132,"Missing"
2020.acl-main.463,N15-1144,0,0.0152241,"earch-langu age-understanding-bert/, accessed 2019/12/04 large LMs learn at least some information about phenomena such as English subject-verb agreement (Goldberg, 2019; Jawahar et al., 2019), constituent types, dependency labels, NER, and (core) semantic role types (again, all in English) (Tenney et al., 2019).4 Hewitt and Manning (2019) find information analogous to unlabeled dependency structures in the word vectors provided by ELMo and BERT (trained on English). And of course it is well established that vector-space representations of words pick up word classes, both syntactic (POS, e.g. Lin et al., 2015) and semantic (lexical similarity, e.g. Rubenstein and Goodenough, 1965; Mikolov et al., 2013). Others have looked more closely at the success of the large LMs on apparently meaning sensitive tasks and found that in fact, far from doing the “reasoning” ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding"
2020.acl-main.463,P19-1334,0,0.162369,"s of the large LMs on apparently meaning sensitive tasks and found that in fact, far from doing the “reasoning” ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original, thus mirroring the distribution of lexical cues for each label. Similarly, McCoy et al. (2019) find that BERT’s performance on the English Multi-genre Natural Language Inference dataset (Williams et al., 2018) is predicated on its ability to leverage syntactic heuristics involving overlap (of full constituents, subsequences, or simply bags of words). In a dataset carefully designed to frustrate such heuristics, BERT’s performance falls to significantly below chance. In this brief overview of BERTology papers we have highlighted both the extent to which there is evidence that large LMs can learn aspects of linguistic formal structure (e.g. agreement, dependency structure), and how their"
2020.acl-main.463,N13-1090,0,0.175839,"ormation about phenomena such as English subject-verb agreement (Goldberg, 2019; Jawahar et al., 2019), constituent types, dependency labels, NER, and (core) semantic role types (again, all in English) (Tenney et al., 2019).4 Hewitt and Manning (2019) find information analogous to unlabeled dependency structures in the word vectors provided by ELMo and BERT (trained on English). And of course it is well established that vector-space representations of words pick up word classes, both syntactic (POS, e.g. Lin et al., 2015) and semantic (lexical similarity, e.g. Rubenstein and Goodenough, 1965; Mikolov et al., 2013). Others have looked more closely at the success of the large LMs on apparently meaning sensitive tasks and found that in fact, far from doing the “reasoning” ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original, thus mirroring the distribut"
2020.acl-main.463,P19-1459,0,0.243112,"vectors provided by ELMo and BERT (trained on English). And of course it is well established that vector-space representations of words pick up word classes, both syntactic (POS, e.g. Lin et al., 2015) and semantic (lexical similarity, e.g. Rubenstein and Goodenough, 1965; Mikolov et al., 2013). Others have looked more closely at the success of the large LMs on apparently meaning sensitive tasks and found that in fact, far from doing the “reasoning” ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original, thus mirroring the distribution of lexical cues for each label. Similarly, McCoy et al. (2019) find that BERT’s performance on the English Multi-genre Natural Language Inference dataset (Williams et al., 2018) is predicated on its ability to leverage syntactic heuristics involving overlap (of full constituents, subsequences, or simply bags of words)."
2020.acl-main.463,S15-2153,0,0.0570915,"Missing"
2020.acl-main.463,W19-4102,0,0.0129123,"ess possible counterarguments to our main thesis. 2 Large LMs: Hype and analysis Publications talking about the application of large LMs to meaning-sensitive tasks tend to describe the models with terminology that, if interpreted at face value, is misleading. Here is a selection from academically-oriented pieces (emphasis added): (1) (2) (3) In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task. (Devlin et al., 2019) Using BERT, a pretraining language model, has been successful for single-turn machine comprehension . . . (Ohsugi et al., 2019) The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demon5185 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198 c July 5 - 10, 2020. 2020 Association for Computational Linguistics strates their potential as unsupervised open-domain QA systems. (Petroni et al., 2019) If the highlighted terms are meant to describe human-analogous understanding, comprehension, or recall of factual knowledge, then these are gross overclaims. If, instead, they are intended as technical terms, they should be expli"
2020.acl-main.463,S19-1012,0,0.0133783,"l., 2018; Hewitt and Manning, 2019). Equating these with meaning ignores a core function of language, which is to convey communicative intents. “But meaning could be learned from . . . ”. As we discussed in §7, if form is augmented with grounding data of some kind, then meaning can conceivably be learned to the extent that the communicative intent is represented in that data. In addition, certain tasks are designed in a way that specific forms are declared as representing certain semantic relations of interest. Examples of this include NLI datasets (Dagan et al., 2006; Rajpurkar et al., 2016; Ostermann et al., 2019) which pair input/output tuples of linguistic forms with an explicit semantic relation (e.g. text + hypothesis + “entailed”). Similarly, control codes, or tokens like tl;dr, have been used to prompt large LMs to perform summarization and other tasks (Radford et al., 2019; Keskar et al., 2019). Here forms are explicitly declared at test time to represent certain semantic relations, which together with the distributional similarity between e.g. tl;dr and other phrases such as in summary, may be enough to bootstrap a successful neural summarizer. Depending on one’s perspective, one may argue that"
2020.acl-main.463,D19-1250,0,0.0464819,"Missing"
2020.acl-main.463,D16-1264,0,0.183368,"l, cultivate humility towards language and ask top-down questions. Neural methods are not the first bottom-up success in NLP; they will probably not be the last. Second, be aware of the limitations of tasks: Artificial tasks like bAbI (Weston et al., 2016) can help get a field of research off the ground, but there is no reason to assume that the distribution of language in the test data remotely resembles the distribution of real natural language; thus evaluation results on such tasks must be interpreted very carefully. Similar points can be made about crowdsourced NLI datasets such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015), which do not represent questions that any particular person really wanted to ask about a text, but the somewhat unnatural communicative situation of crowdsourcing work. If a system does better on such a task than the inter-annotator agreement,9 the task probably has statistical artifacts that do not represent meaning. In the vision community, Barbu et al. (2019) offer a novel dataset which explicitly tries to achieve a more realistic distribution of task data; it would be interesting to explore similar ideas for language. Third, value and support the work of car"
2020.acl-main.463,D19-1286,0,0.0859084,"Missing"
2020.acl-main.463,N18-1101,0,0.0155805,"ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original, thus mirroring the distribution of lexical cues for each label. Similarly, McCoy et al. (2019) find that BERT’s performance on the English Multi-genre Natural Language Inference dataset (Williams et al., 2018) is predicated on its ability to leverage syntactic heuristics involving overlap (of full constituents, subsequences, or simply bags of words). In a dataset carefully designed to frustrate such heuristics, BERT’s performance falls to significantly below chance. In this brief overview of BERTology papers we have highlighted both the extent to which there is evidence that large LMs can learn aspects of linguistic formal structure (e.g. agreement, dependency structure), and how their apparent ability to “reason” is sometimes a mirage built on leveraging artifacts in the training data (i.e. form,"
2020.acl-tutorials.2,E17-2069,1,0.800424,"ested in informal instruction outside of university contexts. 5 Instructors Reading List We recommend the following short readings to get a sense of the kinds of issues we will be approaching: Xanda Schofield Harvey Mudd College xanda@cs.hmc.edu www.cs.hmc.edu/˜xanda Xanda Schofield is an Assistant Professor of Computer Science at Harvey Mudd College. Her work focuses on the practical aspects of using distributional semantic models for analysis of realworld datasets, with problems ranging from understanding the consequences of data pre-processing on model inference (Schofield and Mimno, 2016; Schofield et al., 2017) to enforcing text privacy for these models (Schein et al., 2018). She also is interested in pedagogy at this intersection, having co-developed a Text Mining for History and Literature course at Cornell University with David Mimno. She is currently focusing pedagogical ef• Dual Use: Ehni 2008 • Bias: Speer 2017a • Privacy: Coavoux et al. 2018 In addition, we recommend the following papers for a sense of what can be learned from other fields: • Value scenarios, a technique from value sensitive design: Nathan et al. 2007 • A history of notions of fairness in education and hiring: Hutchinson and"
2020.acl-tutorials.2,Q16-1021,1,0.791108,"ested in teaching, or interested in informal instruction outside of university contexts. 5 Instructors Reading List We recommend the following short readings to get a sense of the kinds of issues we will be approaching: Xanda Schofield Harvey Mudd College xanda@cs.hmc.edu www.cs.hmc.edu/˜xanda Xanda Schofield is an Assistant Professor of Computer Science at Harvey Mudd College. Her work focuses on the practical aspects of using distributional semantic models for analysis of realworld datasets, with problems ranging from understanding the consequences of data pre-processing on model inference (Schofield and Mimno, 2016; Schofield et al., 2017) to enforcing text privacy for these models (Schein et al., 2018). She also is interested in pedagogy at this intersection, having co-developed a Text Mining for History and Literature course at Cornell University with David Mimno. She is currently focusing pedagogical ef• Dual Use: Ehni 2008 • Bias: Speer 2017a • Privacy: Coavoux et al. 2018 In addition, we recommend the following papers for a sense of what can be learned from other fields: • Value scenarios, a technique from value sensitive design: Nathan et al. 2007 • A history of notions of fairness in education an"
2020.acl-tutorials.2,Q18-1041,1,0.890293,"ssor of Linguistics and Adjunct Professor of Computer Science and Engineering at the University of Washington. Her research interests include computational semantics, grammar engineering, computational linguistic typology, and ethics in NLP. She is the Faculty Director of UW’s Professional Masters in Computational Linguistics (CLMS) and has been engaged with integrating ethics into the CLMS curriculum since 2016. She co-organized the first EthNLP workshop. Her first publication in this area is the TACL paper “Data Statements for NLP: Toward Mitigating System Bias and Enabling Better Science” (Bender and Friedman, 2018) and she has been an invited speaker at workshops and panels related to ethics and NLP (or AI more broadly) at the Taskar Memorial Event (UW, March 2018), The Future of Artificial Intelligence: Language, Ethics, Technology (Cambridge, March 2019), West Coast NLP (Facebook, September 2019), Machine Learning Competitions for All (NeurIPS, December 2019) and AAAS (Seattle, February 2020). Privacy Design a small search engine around an inverted index that uses random integer noise from a two-sided geometric distribution (Ghosh et al., 2012) to shape which queries are retrieved. Analyze how much th"
2020.acl-tutorials.2,D18-1001,0,0.0159429,"lege. Her work focuses on the practical aspects of using distributional semantic models for analysis of realworld datasets, with problems ranging from understanding the consequences of data pre-processing on model inference (Schofield and Mimno, 2016; Schofield et al., 2017) to enforcing text privacy for these models (Schein et al., 2018). She also is interested in pedagogy at this intersection, having co-developed a Text Mining for History and Literature course at Cornell University with David Mimno. She is currently focusing pedagogical ef• Dual Use: Ehni 2008 • Bias: Speer 2017a • Privacy: Coavoux et al. 2018 In addition, we recommend the following papers for a sense of what can be learned from other fields: • Value scenarios, a technique from value sensitive design: Nathan et al. 2007 • A history of notions of fairness in education and hiring: Hutchinson and Mitchell 2019 8 forts on how to introduce considerations of ethics and bias into other courses such as Algorithms. Dirk Hovy and Shannon L. Spruit. 2016. The social impact of natural language processing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 591–598. Associat"
2021.computel-1.12,P16-2096,0,0.0213441,"evelopment of successful speaker identification or diarization technology for endangered languages, and should there be other recordings of the same individuals available elsewhere on the web, such technology would make it possible to link the different recordings to the same speaker identity, potentially deanonymyzing otherwise anonymous deposits. Beyond the harms to privacy that this represents, it also opens up further risks, such as the potential to amass sufficiently large amounts of data to create deep fakes in the voice of the recorded speakers. We find that the potential for dual use (Hovy and Spruit, 2016) is inherent in speech technology developed for endangered language documentation. That is, alongside the positive value the technology can bring by facilitating language documentation and revitalization, there is also the risk of harmful use. Different speaker communities and indeed different speakers may view these risks differently. What is particularly vexing is that the development of new technology can reshape what risks a speaker is taking on when consenting to be recorded for archival data. We recommend that computational linguists and archivists communicate about the state of technolo"
2021.computel-1.12,W06-1421,0,0.196392,"Missing"
2021.computel-1.12,P10-1102,0,0.0474457,"Table 2. Where available, we provide the ISO639-3 language codes.4 In addition to genre and phonological typological profile, we expect further kinds of variation across these languages and data sets. For example, speaker communities likely vary in their conventions around overlap between turns of different speakers (how much overlap is allowed before it is considered a rude interruption? do speakers need leave some silence after another’s turn? are listeners expected to provide audible backchannels? (Clancy et al., 1996; Levow et al., 2010; Duncan et al., 2010; Tannen, 1994; Goldberg, 1990; Laskowski, 2010)), and recordings likely vary in terms of the amount and type of ambient noise included (animals, traffic, wind). Unfortunately, we do not have access to either type of information and so will have to assume that these data sets do vary along these dimensions. Both speaker diarization and speaker identification have been the subject of ongoing shared task evaluations (Ryant et al., 2018, 2019; NIST, 2016). Earlier work on diarization focused on telephone conversations (Godfrey et al., 1992), broadcast news, and multiparty meetings (Janin et al., 2003). Recent tasks and data sets have refocused"
2021.computel-1.12,D18-1512,0,0.0330966,"Missing"
2021.computel-1.12,W17-0106,1,0.882084,"Missing"
baldwin-etal-2004-road,copestake-flickinger-2000-open,1,\N,Missing
baldwin-etal-2004-road,C02-2025,1,\N,Missing
baldwin-etal-2004-road,P03-1059,1,\N,Missing
bender-2014-language,D09-1136,0,\N,Missing
bender-2014-language,W02-1502,1,\N,Missing
bender-2014-language,P04-1061,0,\N,Missing
bender-2014-language,W13-2710,1,\N,Missing
C10-2123,W08-2203,1,0.847718,"gical breadth with precision analyses that have been implemented and tested on a number of geographically and genetically diverse languages. Although we have tried to account for the patterns found in the typological literature, there may be variants that we are unaware of. We hope to learn of more patterns as the Grammar Matrix customization system is applied to an ever wider set languages. While the current work focuses on syntactic variation, we intend to expand the argument optionality library to include semantic distinctions as well. A likely starting point would be the proposal given by Bender and Goss-Grubbs (2008) who present a way to model the discourse status (Prince, 1981) of an NP taking into account the differences between definite and indefinite null instantiation described by Fillmore (1986). In addition, ongoing work to improve the word order library may eventually allow us to more accurately model word-order based constraints. Acknowledgments This material is based upon work supported by the National Science Foundation under Grant No. 0644097. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views"
C10-2123,W02-1502,1,0.853541,"Missing"
C10-2123,P10-4001,1,0.877853,"Missing"
C10-2123,W02-1503,0,0.0272424,"n the Grammar Matrix-derived Portuguese grammar (Branco and Costa, 2008) and to M¨uller’s (2009) treatment of subject dropping in Maltese. These analyses differ from Ginzburg and Sag’s (2000) HPSG analysis which uses language specific variations on the Argument Realization Principle to control whether the subject/object is placed onto the COMPS and/or SUBJ lists. Language specific analyses have been implemented in deep, broad-coverage grammars for languages such as Japanese (Masuichi et al. (2003), Siegel and Bender (2002)) and Portuguese (Branco and Costa (2008)). Within the ParGram project (Butt et al., 2002), Kim et al. (2003) were able to directly port the argument optionality related rules from a Japanese grammar to Korean. However, to our knowledge, no one has implemented an analysis that has been applied to a large number of typologically, geographically, and genetically diverse languages. 7 Conclusion Our current work has focused on modeling the variation in syntactic constraints on the licensing and restriction of argument dropping. To our knowledge, this is the first analysis of argument optionality that combines typological breadth with precision analyses that have been implemented and te"
C10-2123,Y03-1034,0,0.0197943,"an, 2001; Ackema et al., 2006; Ginzburg and Sag, 2000). Within the context of HPSG, our analysis is similar to the one in the Grammar Matrix-derived Portuguese grammar (Branco and Costa, 2008) and to M¨uller’s (2009) treatment of subject dropping in Maltese. These analyses differ from Ginzburg and Sag’s (2000) HPSG analysis which uses language specific variations on the Argument Realization Principle to control whether the subject/object is placed onto the COMPS and/or SUBJ lists. Language specific analyses have been implemented in deep, broad-coverage grammars for languages such as Japanese (Masuichi et al. (2003), Siegel and Bender (2002)) and Portuguese (Branco and Costa (2008)). Within the ParGram project (Butt et al., 2002), Kim et al. (2003) were able to directly port the argument optionality related rules from a Japanese grammar to Korean. However, to our knowledge, no one has implemented an analysis that has been applied to a large number of typologically, geographically, and genetically diverse languages. 7 Conclusion Our current work has focused on modeling the variation in syntactic constraints on the licensing and restriction of argument dropping. To our knowledge, this is the first analysis"
C10-2123,W02-1210,1,0.749932,"2006; Ginzburg and Sag, 2000). Within the context of HPSG, our analysis is similar to the one in the Grammar Matrix-derived Portuguese grammar (Branco and Costa, 2008) and to M¨uller’s (2009) treatment of subject dropping in Maltese. These analyses differ from Ginzburg and Sag’s (2000) HPSG analysis which uses language specific variations on the Argument Realization Principle to control whether the subject/object is placed onto the COMPS and/or SUBJ lists. Language specific analyses have been implemented in deep, broad-coverage grammars for languages such as Japanese (Masuichi et al. (2003), Siegel and Bender (2002)) and Portuguese (Branco and Costa (2008)). Within the ParGram project (Butt et al., 2002), Kim et al. (2003) were able to directly port the argument optionality related rules from a Japanese grammar to Korean. However, to our knowledge, no one has implemented an analysis that has been applied to a large number of typologically, geographically, and genetically diverse languages. 7 Conclusion Our current work has focused on modeling the variation in syntactic constraints on the licensing and restriction of argument dropping. To our knowledge, this is the first analysis of argument optionality t"
C12-1016,P98-1013,0,0.277458,"Missing"
C12-1016,I11-2002,0,0.0396803,"Missing"
C12-1016,P08-1111,1,0.896571,"Missing"
C12-1016,W02-1502,1,0.879549,"Missing"
C12-1016,bond-etal-2008-boot,0,0.024457,"Missing"
C12-1016,W02-1503,0,0.110927,"Missing"
C12-1016,1997.mtsummit-papers.10,0,0.0437945,"Missing"
C12-1016,W07-1215,0,0.0395593,"Missing"
C12-1016,C98-1013,0,\N,Missing
D11-1037,P06-4020,0,0.051008,"Missing"
D11-1037,W08-1705,0,0.023705,"Missing"
D11-1037,cer-etal-2010-parsing,0,0.0398474,"Missing"
D11-1037,P05-1022,0,0.0267785,"ngs from the realm of natural language database interfaces directly with semantic representations in lambda calculus. These were hand-written on the basis of database query statements distributed with the original datasets. English factored model which combines the preferences of unlexicalized PCFG phrase structures and of lexical dependencies, trained on sections 02–21 of the WSJ portion of the PTB. We chose Stanford Parser from among the state-of-the-art PTB-derived parsers for its support for grammatical relations as an alternate interface representation. Charniak&Johnson Reranking Parser (Charniak and Johnson, 2005) is a two-stage PCFG parser with a lexicalized generative model for the firststage, and a discriminative MaxEnt reranker for the second-stage. The models we evaluate are also trained on sections 02–21 of the WSJ. Top-50 readings were used for the reranking stage. The output constituent trees were then converted into Stanford Dependencies. According to Cer et al. (2010), this combination gives the best parsing accuracy in terms of Stanford dependencies on the PTB. Enju (Miyao et al., 2004) is a probabilistic HPSG parser, combining a hand-crafted core grammar with automatically acquired lexical"
D11-1037,J07-4004,0,0.0417408,". (2010), this combination gives the best parsing accuracy in terms of Stanford dependencies on the PTB. Enju (Miyao et al., 2004) is a probabilistic HPSG parser, combining a hand-crafted core grammar with automatically acquired lexical types from the PTB.3 The model we evaluate is trained on the same material from the WSJ sections of the PTB, but the treebank is first semi-automatically converted into HPSG derivations, and the annotation is enriched with typed feature structures for each constituent. In addition to HPSG derivation trees, Enju also produces predicate argument structures. C&C (Clark and Curran, 2007) is a statistical CCG parser. Abstractly similar to the approach of Enju, the grammar and lexicon are automatically induced from CCGBank (Hockenmaier and Steedman, 2007), a largely automatic projection of (the WSJ portion of) PTB trees into the CCG framework. In addition to CCG derivations, the C&C parser can directly output a variant of grammatical relations. RASP (Briscoe et al., 2006) is an unlexicalized robust parsing system, with a hand-crafted “tag sequence” grammar at its core. The parser thus analyses a lattice of PoS tags, building a parse forest from which the most probable syntactic"
D11-1037,W01-0713,0,0.0205191,"ive examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak"
D11-1037,flickinger-etal-2010-wikiwoods,1,0.827502,"omparable to what they report for the Brown Corpus (but not the WSJ portion of the PTB). 4.2 Annotation format We annotated up to two dependency triples per phenomenon instance, identifying the heads and dependents by the surface form of the head words in the sentence suffixed with a number indicating word position (see Table 2).6 Some strings contain more than one instance of the phenomenon they illustrate; in these cases, multiple sets of dependencies are We processed 900 million tokens of Wikipedia text using the October 2010 release of the ERG, following the work of the WikiWoods project (Flickinger et al., 2010). Using the top-ranked ERG deriva6 tion trees as annotations over this corpus and simAs the parsers differ in tokenization strategies, our evaluaple patterns using names of ERG-specific construc- tion script treats these position IDs as approximate indicators. 402 Item ID 1011079100200 1011079100200 1011079100200 Phenomenon absol absol absol Polarity 1 1 1 Dependency having-2|been-3|passed-4 ARG act-1 withdrew-9 MOD having-2|been-3|passed-4 carried+on-12 MOD having-2|been-3|passed-4 Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-"
D11-1037,P06-1111,0,0.012084,"c knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowled"
D11-1037,J07-3004,0,0.221976,"type “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in their 1 Additional sources of variation among extant parsing technologies include (a) the behavior with respect to ungrammatical inputs and (b) the relationship between probabilistic and symbolic knowled"
D11-1037,W07-2416,0,0.109783,"with a hand-coded grammar at their core typically also incorporate an automatically trained probabilistic disambiguation component. 398 formal nature and the “granularity” of linguistic information (i.e. the number of distinctions assumed), encompassing variants of constituent structure, syntactic dependencies, or logical-form representations of semantics. Parser interface representations range between the relatively simple (e.g. phrase structure trees with a limited vocabulary of node labels as in the PTB, or syntactic dependency structures with a limited vocabulary of relation labels as in Johansson and Nugues (2007)) and the relatively complex, as for example elaborate syntactico-semantic analyses produced by the ParGram or DELPH - IN grammars. There tends to be a correlation between the methodology used in the acquisition of linguistic knowledge and the complexity of representations: in the creation of a mostly hand-crafted treebank like the PTB, representations have to be simple enough for human annotators to reliably manipulate. Deriving more complex representations typically presupposes further computational support, often involving some hand-crafted linguistic knowledge—which can take the form of ma"
D11-1037,N04-1013,0,0.0162081,"Missing"
D11-1037,P04-1061,0,0.0139015,"along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Pe"
D11-1037,J93-2004,0,0.0463107,"verage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic di"
D11-1037,H05-1066,0,0.0471313,"equence” grammar at its core. The parser thus analyses a lattice of PoS tags, building a parse forest from which the most probable syntactic trees and sets of corresponding grammatical relations can be extracted. Unlike other parsers in our mix, RASP did not build on PTB data in either its PoS tagging 3 This hand-crafted grammar is distinct from the ERG, despite sharing the general framework of HPSG. The ERG is not included in our evaluation, since it was used in the extraction of the original examples and thus cannot be fairly evaluated. 399 or syntactic disambiguation components. MSTParser (McDonald et al., 2005) is a datadriven dependency parser. The parser uses an edgefactored model and searches for a maximal spanning tree that connects all the words in a sentence into a dependency tree. The model we evaluate is the second-order projective model trained on the same WSJ corpus, where the original PTB phrase structure annotations were first converted into dependencies, as established in the CoNLL shared task 2009 (Johansson and Nugues, 2007). XLE/ParGram (Riezler et al., 2002, see also Cahill et al., 2008) applies a hand-built Lexical Functional Grammar for English and a stochastic parse selection mod"
D11-1037,C10-1094,0,0.115638,"Missing"
D11-1037,P04-1047,0,0.0766051,"Missing"
D11-1037,P06-1055,0,0.00651761,"4), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in"
D11-1037,P02-1035,0,0.037496,"anguage Processing, pages 397–408, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics unlabeled training data. A related dimension of variation is the type of representations manipulated by the parser. We briefly review some representative examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all"
D11-1037,D09-1085,0,0.273521,"Missing"
D11-1037,P08-1039,0,0.028581,"Missing"
D11-1037,N10-1034,0,\N,Missing
D11-1037,J03-4003,0,\N,Missing
flickinger-etal-2014-towards,J93-2004,0,\N,Missing
flickinger-etal-2014-towards,flickinger-etal-2010-wikiwoods,1,\N,Missing
flickinger-etal-2014-towards,P82-1014,0,\N,Missing
flickinger-etal-2014-towards,D12-1035,0,\N,Missing
flickinger-etal-2014-towards,D11-1037,1,\N,Missing
flickinger-etal-2014-towards,P13-1042,0,\N,Missing
flickinger-etal-2014-towards,kouylekov-oepen-2014-semantic,1,\N,Missing
flickinger-etal-2014-towards,D13-1161,0,\N,Missing
flickinger-etal-2014-towards,oepen-lonning-2006-discriminant,1,\N,Missing
flickinger-etal-2014-towards,adolphs-etal-2008-fine,1,\N,Missing
I05-2035,W02-1503,0,0.19159,"s that result in automatically generated language-specific software. We illustrate the approach for several phenomena and explore the interdependence of the modules. 2 The Grammar Matrix 1 Introduction Manual development of precise broad-coverage grammar implementations, useful in a range of natural language processing/understanding tasks, is a labor-intensive undertaking, requiring many years of work by highly trained linguists. Many recent efforts toward reducing the time and level of expertise needed to produce a new grammar have focused on adapting an existing grammar of another language (Butt et al., 2002; Kim et al., 2003; Bateman et al., ip). Our work on the ‘Grammar Matrix’ has pursued an alternative approach, identifying a set of language-independent grammar constraints to which language-specific constraints can be added (Bender et al., 2002). This approach has the hitherto unexploited potential to benefit from the substantial theoretical work on language typology. In this paper, we present 203 Wide-coverage grammars representing deep linguistic analysis exist in several frameworks, including Head-Driven Phrase Structure Grammar (HPSG), Lexical-Functional Grammar, and Lexicalized Tree Adjo"
I05-2035,P01-1019,1,0.826322,"Missing"
I05-2035,C96-2106,0,0.189913,"tionnaire requires orthographic forms and predicate names. Note that the forms are assumed to be fully inflected (modulo negation), support morphological processes awaiting future work. We use this information and the knowledge base to produce a set of lexical types inheriting from the types defined in the core Matrix and specifying appropriate language-specific constraints, and a set of lexical entries. 5 Limits of modularity Recent computational work in HPSG has asked whether different parts of a single grammar can be abstracted into separate, independent mod206 ules, either for processing (Kasper and Krieger, 1996; Theofilidis et al., 1997) or grammar development (Keˇselj, 2001). Our work is most similar to Keˇselj’s though we are pursuing different goals: Keˇselj is looking to support a division of labor among multiple individuals working on the same grammar and to support variants of a single grammar for different domains. His modules each have private and public features and types, and he illustrates the approach with a small-scale question answering system. In contrast, we are approaching this issue from the perspective of reuse of grammar code in the context of multilingual grammar engineering (a"
I05-2035,W02-1210,1,0.802364,"can be added (Bender et al., 2002). This approach has the hitherto unexploited potential to benefit from the substantial theoretical work on language typology. In this paper, we present 203 Wide-coverage grammars representing deep linguistic analysis exist in several frameworks, including Head-Driven Phrase Structure Grammar (HPSG), Lexical-Functional Grammar, and Lexicalized Tree Adjoining Grammar. In HPSG (P. and Sag, 1994), the most extensive grammars are those of English (Flickinger, 2000), German (Hinrichs et al., 1997; M¨uller and Kasper, 2000; Crysmann, ip), and Japanese (Siegel, 2000; Siegel and Bender, 2002). The Grammar Matrix is an attempt to distill the wisdom of existing grammars and document it in a form that can be used as the basis for new grammars. The main goals of the project are: (i) to develop in detail semantic representations and the syntax-semantics interface, consistent with other work in HPSG; (ii) to represent generalizations across linguistic objects and across languages; and (iii) to allow for very quick start-up as the Matrix is applied to new languages. The original Grammar Matrix consisted of types defining the basic feature geometry, types associated with Minimal Recursion"
I05-2035,W02-1508,1,\N,Missing
J14-1001,1995.tmi-1.2,0,0.0834978,"fact, formalization alone isn’t enough: Grammars of the scale supported by the HPSG framework are too complex for humans to reliably do those calculations without the aid of a machine. The one possible exception to this generalization was Ivan Sag. 3 Computational Linguistics Volume 40, Number 1 also formed an important point of contact between computational and theoretical work in HPSG, such that the “pen and paper” theory remained responsive to computational concerns. Another key result of the LinGO project during the Verbmobil days was the development of Minimal Recursion Semantics (MRS) (Copestake et al. 1995, 2005). Sag and colleagues designed MRS to meet the competing demands of expressive adequacy, grammatical compatibility, computational tractability, and underspecifiability. In other words, it is a computational semantic formalism that allows grammars like the English Resource Grammar to make explicit exactly as much information about semantic predicate argument structure and quantifier and operator scope as is determined by sentence structure, leaving further ambiguity represented via underspecification rather than enumeration of, for example, the full set of possible quantifier scopings for"
J14-1001,P82-1014,0,0.766353,"rse, Anne Paulson, was working at Hewlett-Packard Labs and saw the potential for using GPSG as the basis of a question answering system (with a database back-end). Paulson arranged a meeting between her boss, Egon Loebner, and Sag, Gazdar, Pullum, and Tom Wasow, which led to a nearly decade-long project implementing a grammar for English and processing tools to work with it. The project included HP staff as well as Sag, Pullum, and Wasow as consultants, and Stanford and UC Berkeley students, including Mark Gawron, Carl Pollard, and Dan Flickinger. The work initially set out to implement GPSG (Gawron et al. 1982), but in the context of developing and implementing analyses, Sag and colleagues added innovations to the underlying theory until its formal basis was so different it warranted a new name. The new theory, laid out in Pollard and Sag 1987 and Pollard and Sag 1994, among others, came to be called Head-driven Phrase Structure Grammar (HPSG). 1 The formal precision had much more lasting impact than the attempt at a context-free model: Eventually Bresnan et al. (1982), Culy (1985), and Shieber (1985) found more solid counterexamples to the claim that natural languages can be modeled with CF-PSGs. 2"
J14-1001,P98-2132,0,0.114679,"ther than encoding theoretical results as constraints on the formalism, HPSG defines a flexible formalism (typed feature structures) in which different theories can be defined. This flexibility facilitates testing and synthesis of theoretical ideas developed in other frameworks. The stability of the formalism has been critical to the success of HPSG in computational linguistics, as it has allowed for the development of a variety of processing engines that interpret the formalism and thus can apply grammars to the tasks of parsing and generation (Uszkoreit et al. 1994; Carpenter and Penn 1994; Makino et al. 1998; Copestake 2002; Callmeier 2002; Penn 2004; Crysmann and Packard 2012; Slayden 2012, inter alios). HPSG-based parsing was deployed in Verbmobil (Wahlster 2000), a large-scale, multi-site machine translation project funded by the German government, for which Sag headed up the English Grammar Project (1994–2000), later redubbed LinGO (Linguistic Grammars Online), at CSLI. The English grammar developed in that project (beginning actually in 1993) came to be known as the English Resource Grammar (Flickinger 2000, 2011).2 The resource grammar idea builds on insights articulated in Gawron et al. 19"
J14-1001,2007.tmi-papers.18,0,0.0545797,"Missing"
J14-1001,P04-1031,0,0.0201063,"s on the formalism, HPSG defines a flexible formalism (typed feature structures) in which different theories can be defined. This flexibility facilitates testing and synthesis of theoretical ideas developed in other frameworks. The stability of the formalism has been critical to the success of HPSG in computational linguistics, as it has allowed for the development of a variety of processing engines that interpret the formalism and thus can apply grammars to the tasks of parsing and generation (Uszkoreit et al. 1994; Carpenter and Penn 1994; Makino et al. 1998; Copestake 2002; Callmeier 2002; Penn 2004; Crysmann and Packard 2012; Slayden 2012, inter alios). HPSG-based parsing was deployed in Verbmobil (Wahlster 2000), a large-scale, multi-site machine translation project funded by the German government, for which Sag headed up the English Grammar Project (1994–2000), later redubbed LinGO (Linguistic Grammars Online), at CSLI. The English grammar developed in that project (beginning actually in 1993) came to be known as the English Resource Grammar (Flickinger 2000, 2011).2 The resource grammar idea builds on insights articulated in Gawron et al. 1982, namely, that detailed syntactic and sem"
J14-1001,C94-1072,0,0.425362,"Missing"
N12-1032,D07-1119,0,0.0599415,"Missing"
N12-1032,W10-1404,0,0.0237713,"Missing"
N12-1032,W09-3036,0,0.0406182,"Missing"
N12-1032,W06-2923,0,0.0422085,"Missing"
N12-1032,bosco-etal-2000-building,0,0.0317025,"Missing"
N12-1032,W06-2920,0,0.0331423,"a (1998) investigated this empirically and found that it holds to a certain extent: the absence of agreement and/or case marking predicts rigid word order, though their presence is not particularly predictive of flexible word order. Many parsers rely on word order to establish dependencies, so they often perform best on languages with more rigid word order. Making use of morphological agreement could compensate for greater variation in word order and help to bring parsing performance on flexible-word-order languages up to par with that on rigid-word-order languages. 2.2 MSTParser The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al., 2007) shared tasks focused on multilingual dependency parsing. Each system was trained on treebanks in a variety of languages and predicted dependency arcs and labels for POS-tagged data. The best performers in 2006 were MSTParser (McDonald et al., 2006), which we use here, and MaltParser (Nivre et al., 2006a). 316 MSTParser is a data-driven, graph-based parser which creates a model from training data by learning weights for arc-level features. The feature set includes combinations of the word and POS tag of the parent and child of each dependency arc; POS tags o"
N12-1032,W06-2925,0,0.0181801,"ective trees are those in which every constituent (head plus all dependents) forms a complete subtree; non-projective parsing lacks this limitation. 2.3 Related work The organizers of the CoNLL 2007 shared task noted that languages with free word order and high morphological complexity are the most difficult for dependency parsing (Nivre et al., 2007). Most of the participants took language-independent approaches toward leveraging this complexity into better performance: generating machine learning features based on each item in a token’s list of morphological attributes (Nivre et al., 2006b; Carreras et al., 2006); using the entire list as an atomic feature (Chang et al., 2006; Titov and Henderson, 2007); or generating features based on each pair of attributes in the cross-product of the lists of a potential head and dependent (McDonald et al., 2006; Nakagawa, 2007). Language-specific uses of morphological information have included using it to disambiguate function words (Bick, 2006) or to pick out finite verbs &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{dpForm|dpLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;hdAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;dpAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdL"
N12-1032,W06-2926,0,0.0138513,"endents) forms a complete subtree; non-projective parsing lacks this limitation. 2.3 Related work The organizers of the CoNLL 2007 shared task noted that languages with free word order and high morphological complexity are the most difficult for dependency parsing (Nivre et al., 2007). Most of the participants took language-independent approaches toward leveraging this complexity into better performance: generating machine learning features based on each item in a token’s list of morphological attributes (Nivre et al., 2006b; Carreras et al., 2006); using the entire list as an atomic feature (Chang et al., 2006; Titov and Henderson, 2007); or generating features based on each pair of attributes in the cross-product of the lists of a potential head and dependent (McDonald et al., 2006; Nakagawa, 2007). Language-specific uses of morphological information have included using it to disambiguate function words (Bick, 2006) or to pick out finite verbs &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{dpForm|dpLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;hdAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;dpAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>&lt;{dpForm|dpLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hd"
N12-1032,dzeroski-etal-2006-towards,0,0.0461761,"Missing"
N12-1032,J08-3003,0,0.230363,"Missing"
N12-1032,W09-3819,0,0.0693801,"ding features for gender and number agreement in noun phrases. The potential of morphological information to improve parsing performance has been documented in numerous experiments using MaltParser and with various morphological attributes as machine learning features, on several morphologically rich languages, including: Russian (Nivre et al., 2008); Swedish (Øvrelid and Nivre, 2007); Bangla, Telugu, and Hindi (Nivre, 2009); Turkish (Eryiˇgit et al., 2008); and Basque (Bengoetxea and Gojenola, 2010). These experiments, however, did not include any higher-level features such as agreement. 317 Goldberg and Elhadad (2009) found that using morphological features increased the accuracy of MSTParser on Hebrew only when the morphological annotations were gold-standard; automatic annotations decreased accuracy, although MaltParser showed improvement with both gold and automatic annotations. The accuracy of MaltParser on Arabic was improved by different types of morphological features depending on whether gold or automatic annotations were used (Marton et al., 2010). As far as we can tell, no language-independent approaches to utilizing morphological data thus far have taken advantage of agreement specifically. We t"
N12-1032,W10-1412,0,0.117718,"Missing"
N12-1032,J93-2004,0,0.0415385,"Missing"
N12-1032,W10-1402,0,0.0461602,", 2008); and Basque (Bengoetxea and Gojenola, 2010). These experiments, however, did not include any higher-level features such as agreement. 317 Goldberg and Elhadad (2009) found that using morphological features increased the accuracy of MSTParser on Hebrew only when the morphological annotations were gold-standard; automatic annotations decreased accuracy, although MaltParser showed improvement with both gold and automatic annotations. The accuracy of MaltParser on Arabic was improved by different types of morphological features depending on whether gold or automatic annotations were used (Marton et al., 2010). As far as we can tell, no language-independent approaches to utilizing morphological data thus far have taken advantage of agreement specifically. We take a linguistically informed approach, maintaining language-independence, by explicitly modeling agreement between head and dependent morphology. 3 3.1 Methodology Modifications to parser Our approach builds on the observation that there are two kinds of information marked in morphology: symmetric, recorded on both head and depenID 1 2 3 TOKEN Vznikaj´ ı zbyteˇ cn´ e konflikty CPOS VERB ADJ NOUN MORPH num=PL|per=3 num=PL|gen=I|case=NOM num=PL"
N12-1032,W06-2932,0,0.186573,"es, so they often perform best on languages with more rigid word order. Making use of morphological agreement could compensate for greater variation in word order and help to bring parsing performance on flexible-word-order languages up to par with that on rigid-word-order languages. 2.2 MSTParser The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al., 2007) shared tasks focused on multilingual dependency parsing. Each system was trained on treebanks in a variety of languages and predicted dependency arcs and labels for POS-tagged data. The best performers in 2006 were MSTParser (McDonald et al., 2006), which we use here, and MaltParser (Nivre et al., 2006a). 316 MSTParser is a data-driven, graph-based parser which creates a model from training data by learning weights for arc-level features. The feature set includes combinations of the word and POS tag of the parent and child of each dependency arc; POS tags of words between the parent and child; and POS tags of the parent and child along with those of the preceding and following words. A similar feature set is conjoined with arc labels in order to perform labeling, and an optional set of “second-order” features includes analogous informat"
N12-1032,D07-1100,0,0.0227971,"rphological complexity are the most difficult for dependency parsing (Nivre et al., 2007). Most of the participants took language-independent approaches toward leveraging this complexity into better performance: generating machine learning features based on each item in a token’s list of morphological attributes (Nivre et al., 2006b; Carreras et al., 2006); using the entire list as an atomic feature (Chang et al., 2006; Titov and Henderson, 2007); or generating features based on each pair of attributes in the cross-product of the lists of a potential head and dependent (McDonald et al., 2006; Nakagawa, 2007). Language-specific uses of morphological information have included using it to disambiguate function words (Bick, 2006) or to pick out finite verbs &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{dpForm|dpLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;hdAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;dpAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>&lt;{dpForm|dpLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>&lt;hdAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>&lt;dpAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{dpForm|dpLemma}>&lt;dpAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{dpForm|dpLemma}>&lt;hdAtt>(&lt;dir+dist>) &lt;hdI"
N12-1032,nivre-etal-2006-maltparser,0,0.114686,"word order. Making use of morphological agreement could compensate for greater variation in word order and help to bring parsing performance on flexible-word-order languages up to par with that on rigid-word-order languages. 2.2 MSTParser The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al., 2007) shared tasks focused on multilingual dependency parsing. Each system was trained on treebanks in a variety of languages and predicted dependency arcs and labels for POS-tagged data. The best performers in 2006 were MSTParser (McDonald et al., 2006), which we use here, and MaltParser (Nivre et al., 2006a). 316 MSTParser is a data-driven, graph-based parser which creates a model from training data by learning weights for arc-level features. The feature set includes combinations of the word and POS tag of the parent and child of each dependency arc; POS tags of words between the parent and child; and POS tags of the parent and child along with those of the preceding and following words. A similar feature set is conjoined with arc labels in order to perform labeling, and an optional set of “second-order” features includes analogous information about siblings. Morphological features for an arc a"
N12-1032,W06-2933,0,0.0269444,"Missing"
N12-1032,C08-1081,0,0.163852,"Missing"
N12-1032,D07-1099,0,0.0189901,"plete subtree; non-projective parsing lacks this limitation. 2.3 Related work The organizers of the CoNLL 2007 shared task noted that languages with free word order and high morphological complexity are the most difficult for dependency parsing (Nivre et al., 2007). Most of the participants took language-independent approaches toward leveraging this complexity into better performance: generating machine learning features based on each item in a token’s list of morphological attributes (Nivre et al., 2006b; Carreras et al., 2006); using the entire list as an atomic feature (Chang et al., 2006; Titov and Henderson, 2007); or generating features based on each pair of attributes in the cross-product of the lists of a potential head and dependent (McDonald et al., 2006; Nakagawa, 2007). Language-specific uses of morphological information have included using it to disambiguate function words (Bick, 2006) or to pick out finite verbs &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{dpForm|dpLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;hdAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;dpAtt>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>&lt;{dpForm|dpLemma}>(&lt;dir+dist>) &lt;hdIdx>*&lt;dpIdx>=&lt;{hdForm|hdLemma}>&lt;hdAtt>(&lt;dir+dist>) &lt;"
N12-1032,vincze-etal-2010-hungarian,0,0.0535538,"Missing"
N12-1032,J08-4010,0,\N,Missing
N12-1032,D07-1096,0,\N,Missing
N12-1032,D07-1127,0,\N,Missing
N12-1032,afonso-etal-2002-floresta,0,\N,Missing
N16-4001,W13-5707,1,0.867199,"Missing"
N16-4001,S14-1003,0,0.0330419,"Missing"
N16-4001,W15-2205,0,0.0150817,"ing and parse selection algorithms. With high parsing accuracy with rich semantic representations, English Resource Semantics is a valuable source of information for many semanticallysensitive NLP tasks. ERSbased systems have achieved stateoftheart results in various tasks, including the identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE task; Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transferbased MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot 2006), extraction of glossary sentences 1 Proceedings of NAACL-HLT 2016, pages 1–5, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Reiplinger et al 2012), sentiment analysis (Kramer & Gordon 2014), and the ACL Anthology Searchbench (Schäfer et al 2011). The goal of this tutorial is to make this resource more accessible to the ACL community. Specifically, we take as our learning goals that tutorial participa"
N16-4001,S14-2056,0,0.127666,"Missing"
N16-4001,2007.tmi-papers.18,1,0.651881,"glish Resource Semantics is a valuable source of information for many semanticallysensitive NLP tasks. ERSbased systems have achieved stateoftheart results in various tasks, including the identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE task; Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transferbased MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot 2006), extraction of glossary sentences 1 Proceedings of NAACL-HLT 2016, pages 1–5, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Reiplinger et al 2012), sentiment analysis (Kramer & Gordon 2014), and the ACL Anthology Searchbench (Schäfer et al 2011). The goal of this tutorial is to make this resource more accessible to the ACL community. Specifically, we take as our learning goals that tutorial participants will learn how to: (1) set up the ERGbased parsing stack, including preprocessing; (2) acc"
N16-4001,S14-2144,1,0.861772,"or by processing new text with the ERG and its associated parsing and parse selection algorithms. With high parsing accuracy with rich semantic representations, English Resource Semantics is a valuable source of information for many semanticallysensitive NLP tasks. ERSbased systems have achieved stateoftheart results in various tasks, including the identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE task; Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transferbased MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot 2006), extraction of glossary sentences 1 Proceedings of NAACL-HLT 2016, pages 1–5, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Reiplinger et al 2012), sentiment analysis (Kramer & Gordon 2014), and the ACL Anthology Searchbench (Schäfer et al 2011). The goal of this tutorial is to make this resource more accessible to the ACL communi"
N16-4001,P14-1007,1,0.844733,"al 2004) and DeepBank (Wall Street Journal corpus: Flickinger et al 2012) or by processing new text with the ERG and its associated parsing and parse selection algorithms. With high parsing accuracy with rich semantic representations, English Resource Semantics is a valuable source of information for many semanticallysensitive NLP tasks. ERSbased systems have achieved stateoftheart results in various tasks, including the identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE task; Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transferbased MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot 2006), extraction of glossary sentences 1 Proceedings of NAACL-HLT 2016, pages 1–5, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Reiplinger et al 2012), sentiment analysis (Kramer & Gordon 2014), and the ACL Anthology Searchbench (Schäfer et al 2011). The goal of this"
N16-4001,W12-3206,0,0.0581575,"Missing"
N16-4001,P11-4002,0,0.0764826,"Missing"
N19-1235,I05-1015,0,0.0425305,"rsion 0.9.25, with the 1214 ERG release, available at http://sweaglesw.org/linguistics/ ace 6 The ACE parser obtained 93.5 Smatch score on parsable sentences (Buys and Blunsom, 2017), while the neural AMR parser (Konstas et al., 2017) obtained 62.1 Smatch (on a different domain). 7 https://github.com/mjpost/sacreBLEU sampling the gold data so that the gold to silver ratio in training examples is 1:2. Interestingly, training on silver data performs only slightly worse than training on both gold and silver. 3.1 Baselines Our baselines are the ERG’s grammar-based generator (Carroll et al., 1999; Carroll and Oepen, 2005) and the DAG transducer generator of Ye et al. (2018). To compare our models against the grammar-based generator, implemented in ACE, we need to restrict the evaluation to examples from which ACE is able to generate (‘All overlap’).8 In addition to BLEU, we also report exact match accuracy on the overlapping subset. Results show that our neural models outperform the grammar-based generator by a large margin. ACE ranks candidate generations with a discriminative ranker based on structural features over its derivations (Velldal and Oepen, 2006). However, it does not use a language model trained"
N19-1235,P18-1038,0,0.0441762,"Missing"
N19-1235,L16-1197,1,0.910118,"Missing"
N19-1235,E09-1001,0,0.0747885,"Missing"
N19-1235,W18-4912,0,0.0142658,"d plus silver data.9 We evaluate DMRS models both with and without predicate and edge attributes, as these attributes contain information that is absent from AMR.10 The results in Table 4 show that our MRS generator performs better than the AMR generator by a large margin, even when the additional MRS attributes are excluded. Our system results are reported on the subset for which we obtained MRS parses. AMR results are as given by Konstas et al. (2017) and cover the entire test set. 9 The AMR and DMRS systems have different gold training data, but the same source of silver data. 10 Recently, Donatelli et al. (2018) proposed adding tense and aspect to AMR, but this annotation is not yet available in a large AMR corpus. 2262 Type B80-89 B60-69 B40-49 56.4 39.55 48.8 47.1 18.0 9.2 3.3 7.6 12.8 5.1 25.0 7.9 18.7 8.1 19.8 7.6 7.7 18.4 21.1 18.1 39 1.18 76 2.30 123 3.73 238 7.21 Unproblematic Slightly problematic Moderately problematic Ungrammatical Other serious error Number of errors Errors per item tokens (as in (2)), spurious additional tokens, and word order changes that alter the semantics or pragmatics of the string. All (2) Table 5: Percentage of errors of each type, across 99 sampled items, grouped b"
N19-1235,P16-1014,0,0.0331991,"in greater detail in Appendices A and B. 2.3 Model Our neural generator follows the standard encoder-decoder paradigm (Bahdanau et al., 2014). The encoder is a two-layer bidirectional LSTM. Predicates and their attributes are embedded separately; their embeddings are then concatenated (Sennrich and Haddow, 2016). The 1 http://svn.delph-in.net/erg/tags/ 1214/tsdb/gold 2 https://github.com/delph-in/pydelphin 3 https://github.com/goodmami/ mrs-to-penman 2260 decoder uses global soft attention for alignment (Luong et al., 2015), and pointer attention to copy unknown tokens directly to the output (Gulcehre et al., 2016). The models are trained using Adam (Kingma and Ba, 2014). Dropout is applied to non-recurrent connections. Decoding uses beam search (width 5). The generator is implemented using OpenNMT-py (Klein et al., 2017). Hyperparameter details are given in Appendix C. Our code is available online.4 2.4 Semi-supervised training We augment the gold training data with a silver dataset generated using ACE,5 a parser for the ERG, to parse sentences to MRS. We sample one million sentences from the Gigaword corpus (Parker et al., 2011), restricted to articles published before the year 2000, to match the doma"
N19-1235,P17-4012,0,0.107992,"Missing"
N19-1235,P07-2045,0,0.0129406,"Missing"
N19-1235,P17-1014,0,0.0317097,"eneration, we then linearize the DMRS into PENMAN format (which is also used to represent AMR). We follow Goodman (2018, pp. 82–86) in finding normalized spanning trees through depth-first traversal over the directed acyclic DMRS graphs.3 The PENMAN format defines each node once, supports node attributes and edge labels, marks edges whose direction is reversed in the traversal, and represents edges which are not covered by the spanning tree. The PENMAN format is processed further to obtain a linearization appropriate as input to sequence-to-sequence models, similar to the approach proposed by Konstas et al. (2017) for AMR linearization (see Fig. 2). Node variable identifiers are removed, node attributes are concatenated, and named entities are anonymized. Predicates that appear only once in the training data are treated as unknowns. Preprocessing and unknown word handling are described in greater detail in Appendices A and B. 2.3 Model Our neural generator follows the standard encoder-decoder paradigm (Bahdanau et al., 2014). The encoder is a two-layer bidirectional LSTM. Predicates and their attributes are embedded separately; their embeddings are then concatenated (Sennrich and Haddow, 2016). The 1 h"
N19-1235,W02-2103,0,0.200319,"83.37 on the subset of test data most closely matching the silver data domain. Our results suggest that MRS-based representations are a good choice for applications that need both structured semantics and the ability to produce natural language text as output. 1 Introduction Text generation systems often generate their output from an intermediate semantic representation (Yao et al., 2012; Takase et al., 2016). However many semantic representations are task- or domain-specific (He and Young, 2003; Wong and Mooney, 2007), while rule-based text generation systems often have incomplete coverage (Langkilde-Geary, 2002; Oepen et al., 2007). In this work we combine the advantages of Minimal Recursion Semantics (MRS; Copestake et al., 2005) with the robustness and fluency of neural sequence-to-sequence models trained on large datasets. We hypothesize that MRS is particularly well-suited for text generation, as it is explicitly compositional, capturing the contribution to sentence meaning of all parts of the surface form (Bender et al., 2015). In contrast, semantic representations such as Abstract Meaning Representation (AMR; Banarescu et al., 2013) seek to abstract away from the syntax of a sentence as much a"
N19-1235,D15-1166,0,0.0275949,"ining data are treated as unknowns. Preprocessing and unknown word handling are described in greater detail in Appendices A and B. 2.3 Model Our neural generator follows the standard encoder-decoder paradigm (Bahdanau et al., 2014). The encoder is a two-layer bidirectional LSTM. Predicates and their attributes are embedded separately; their embeddings are then concatenated (Sennrich and Haddow, 2016). The 1 http://svn.delph-in.net/erg/tags/ 1214/tsdb/gold 2 https://github.com/delph-in/pydelphin 3 https://github.com/goodmami/ mrs-to-penman 2260 decoder uses global soft attention for alignment (Luong et al., 2015), and pointer attention to copy unknown tokens directly to the output (Gulcehre et al., 2016). The models are trained using Adam (Kingma and Ba, 2014). Dropout is applied to non-recurrent connections. Decoding uses beam search (width 5). The generator is implemented using OpenNMT-py (Klein et al., 2017). Hyperparameter details are given in Appendix C. Our code is available online.4 2.4 Semi-supervised training We augment the gold training data with a silver dataset generated using ACE,5 a parser for the ERG, to parse sentences to MRS. We sample one million sentences from the Gigaword corpus (P"
N19-1235,oepen-lonning-2006-discriminant,0,0.0602738,"ation to examples from which ACE is able to generate (‘All overlap’).8 In addition to BLEU, we also report exact match accuracy on the overlapping subset. Results show that our neural models outperform the grammar-based generator by a large margin. ACE ranks candidate generations with a discriminative ranker based on structural features over its derivations (Velldal and Oepen, 2006). However, it does not use a language model trained on large amounts of text, which would likely improve fluency substantially. The DAG transducer was trained to generate from Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), an MRS-derived representation almost equivalent to DMRS (after edge properties are removed, which Table 3 shows has an effect of less than 1 BLEU point). It was evaluated against the same WSJ test set reference generations, but trained using both less gold data (only the WSJ subsection) and less silver data (300K vs 900K sentences). Our model trained on WSJ gold data performs only slightly worse (65.78 BLEU; see Table 2) and all our semi-supervised models obtain substantially higher results. 3.2 Out of domain evaluation We evaluate the in- and out-of-domain performance of our approach by tra"
N19-1235,C02-2025,0,0.01355,"DMRS includes tense and number, and has a node for the determiner, it can distinguish between, e.g. Kim sees a boy and Kim saw the boys, which AMR does not do. ( _see_v_1 mood=INDICATIVE|perf=-|sf ,→ =PROP|tense=PRES ARG1-NEQ ( ,→ named0 ind=+|num=SG|pers=3 ) ,→ ARG2-NEQ ( _boy_n_1 ind=+|num= ,→ SG|pers=3 RSTR-H-of ( _a_q ) ) ,→ ) 2 Figure 2: The DMRS for the sentence Kim sees a boy. in PENMAN format (top) and the linearization used by our model (bottom). Approach 2.1 Data Our gold training data are parallel MRS and English text corpora, derived from the 1214 release of the Redwoods Treebank (Oepen et al., 2002).1 MRS is implemented as the semantic layer of the English Resource Grammar (ERG; Flickinger, 2000, 2011), a broad-coverage, hand-engineered computational grammar of English. The Redwoods annotation was produced in conjunction with the ERG by parsing each sentence into a forest (discarding unparsable sentences), followed by manual disambiguation (Flickinger et al., 2017). About half of the training data comes from the Wall Street Journal (sections 00-21), while the rest spans a range of domains, including Wikipedia, ecommerce dialogues, tourism brochures, and the Brown corpus. The data is spli"
N19-1235,2007.tmi-papers.18,0,0.170776,"Missing"
N19-1235,P02-1040,0,0.103372,"Missing"
N19-1235,W18-6319,0,0.0133283,"these were discarded. While there are robust MRS parsers (Buys and Blunsom, 2017; Chen et al., 2018), the MRSs they produce are less accurate and not guaranteed to be well-formed. Our approach thus differs from Konstas et al. (2017), who used self-training to improve AMR to text generation by iteratively training on larger amounts of data parsed by their neural parser.6 3 Results We compare the performance of our neural generator when trained on either gold, silver, or gold and silver data (Table 1). Generation quality is primarily evaluated with BLEU (Papineni et al., 2002), using SacreBLEU (Post, 2018).7 We evaluate the neural models on both the full Redwoods test set (‘All’) and the WSJ subset. The results show that our neural generator obtains very strong performance. Semi-supervised training leveraging the ERG parser leads to an 11 BLEU point improvement on Redwoods, comparing to supervised training only. We found that the best semi-supervised results are obtained by up4 https://github.com/shlurbee/ dmrs-text-generation-naacl2019 5 ACE version 0.9.25, with the 1214 ERG release, available at http://sweaglesw.org/linguistics/ ace 6 The ACE parser obtained 93.5 Smatch score on parsable sent"
N19-1235,W16-2209,0,0.0274008,"h proposed by Konstas et al. (2017) for AMR linearization (see Fig. 2). Node variable identifiers are removed, node attributes are concatenated, and named entities are anonymized. Predicates that appear only once in the training data are treated as unknowns. Preprocessing and unknown word handling are described in greater detail in Appendices A and B. 2.3 Model Our neural generator follows the standard encoder-decoder paradigm (Bahdanau et al., 2014). The encoder is a two-layer bidirectional LSTM. Predicates and their attributes are embedded separately; their embeddings are then concatenated (Sennrich and Haddow, 2016). The 1 http://svn.delph-in.net/erg/tags/ 1214/tsdb/gold 2 https://github.com/delph-in/pydelphin 3 https://github.com/goodmami/ mrs-to-penman 2260 decoder uses global soft attention for alignment (Luong et al., 2015), and pointer attention to copy unknown tokens directly to the output (Gulcehre et al., 2016). The models are trained using Adam (Kingma and Ba, 2014). Dropout is applied to non-recurrent connections. Decoding uses beam search (width 5). The generator is implemented using OpenNMT-py (Klein et al., 2017). Hyperparameter details are given in Appendix C. Our code is available online.4"
N19-1235,P18-1150,0,0.0299544,"ntations, in contrast to both existing MRS-based generators and neural generators based on other broad-coverage semantic representations. Furthermore, we have demonstrated that a large hand-crafted grammar can be leveraged to produce large training sets, which improves performance of neural generators substantially. Therefore we argue that the ability to generate high quality text from MRS makes it a good choice of representation for text generation applications that require semantic structure. For future work, we are interested in applying graph-tosequence neural networks (Beck et al., 2018; Song et al., 2018) to MRS-to-text generation. Acknowledgements Thanks to Yannis Konstas for sharing preliminary results on DMRS generation, and Swabha Swayamdipta for discussions. This research was supported in part by NSF (IIS-1524371) and Samsung AI Research. 2263 References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning"
N19-1235,W06-1661,0,0.0612437,"RG’s grammar-based generator (Carroll et al., 1999; Carroll and Oepen, 2005) and the DAG transducer generator of Ye et al. (2018). To compare our models against the grammar-based generator, implemented in ACE, we need to restrict the evaluation to examples from which ACE is able to generate (‘All overlap’).8 In addition to BLEU, we also report exact match accuracy on the overlapping subset. Results show that our neural models outperform the grammar-based generator by a large margin. ACE ranks candidate generations with a discriminative ranker based on structural features over its derivations (Velldal and Oepen, 2006). However, it does not use a language model trained on large amounts of text, which would likely improve fluency substantially. The DAG transducer was trained to generate from Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), an MRS-derived representation almost equivalent to DMRS (after edge properties are removed, which Table 3 shows has an effect of less than 1 BLEU point). It was evaluated against the same WSJ test set reference generations, but trained using both less gold data (only the WSJ subsection) and less silver data (300K vs 900K sentences). Our model trained on WSJ"
N19-1235,N07-1022,0,0.0547548,"e a large silver training corpus, achieving a final BLEU score of 77.17 on the full test set, and 83.37 on the subset of test data most closely matching the silver data domain. Our results suggest that MRS-based representations are a good choice for applications that need both structured semantics and the ability to produce natural language text as output. 1 Introduction Text generation systems often generate their output from an intermediate semantic representation (Yao et al., 2012; Takase et al., 2016). However many semantic representations are task- or domain-specific (He and Young, 2003; Wong and Mooney, 2007), while rule-based text generation systems often have incomplete coverage (Langkilde-Geary, 2002; Oepen et al., 2007). In this work we combine the advantages of Minimal Recursion Semantics (MRS; Copestake et al., 2005) with the robustness and fluency of neural sequence-to-sequence models trained on large datasets. We hypothesize that MRS is particularly well-suited for text generation, as it is explicitly compositional, capturing the contribution to sentence meaning of all parts of the surface form (Bender et al., 2015). In contrast, semantic representations such as Abstract Meaning Representa"
N19-1235,P18-1179,0,0.0179929,"sweaglesw.org/linguistics/ ace 6 The ACE parser obtained 93.5 Smatch score on parsable sentences (Buys and Blunsom, 2017), while the neural AMR parser (Konstas et al., 2017) obtained 62.1 Smatch (on a different domain). 7 https://github.com/mjpost/sacreBLEU sampling the gold data so that the gold to silver ratio in training examples is 1:2. Interestingly, training on silver data performs only slightly worse than training on both gold and silver. 3.1 Baselines Our baselines are the ERG’s grammar-based generator (Carroll et al., 1999; Carroll and Oepen, 2005) and the DAG transducer generator of Ye et al. (2018). To compare our models against the grammar-based generator, implemented in ACE, we need to restrict the evaluation to examples from which ACE is able to generate (‘All overlap’).8 In addition to BLEU, we also report exact match accuracy on the overlapping subset. Results show that our neural models outperform the grammar-based generator by a large margin. ACE ranks candidate generations with a discriminative ranker based on structural features over its derivations (Velldal and Oepen, 2006). However, it does not use a language model trained on large amounts of text, which would likely improve"
N19-4022,W16-2021,1,0.844162,"l., 2017). We developed this visualization tool to help linguists to understand the morphological system implicit in large datasets and to refine automatically generated grammar specifications which model that morphological system. Thus we expect this tool to directly assist in language description. Because linguistic typology depends on accurate language description, and truly language-independent NLP depends on linguistic typology (see Bender 2011 2 System overview 2.1 Back-end The morphological graph that our system visualizes comes from the MOM morphological inference software (Wax, 2014; Zamaraeva, 2016). MOM outputs a directed acyclic graph specified in DOT (Gansner et al., 1993) where nodes are inflectional classes of words and position classes of affixes, and edges reflect the ordering possibilities of those affixes. This graph is translated by the Grammar Matrix customization system to 1 The (frequently updated) Alpha-version of the system can be accessed via http://uakari.ling.washington. edu/aggregation/. The demonstration video is at https: //youtu.be/qn96Zg-6wkE. All of the code and sample data are available in the repository: https://git.ling. washington.edu/agg/mom 127 Proceedings o"
N19-4022,W17-0118,1,0.848284,"eses about classes of stems and affixes, and the cooccurrence and ordering possibilities between them. This set of hypotheses is cast as a grammar specification that can be used by the Grammar Matrix customization system (Bender et al., 2010) to automatically create an implemented grammar capable of morphological parsing. Our system helps the linguist visualize and explore these hypotheses, facilitating both further linguistic theorizing of the dataset and the production of a more accurate implemented grammar. The grammar can be used to produce annotations for additional unglossed data, as in Zamaraeva et al. 2017, because the inferred system generalizes beyond the specific combinations of morphemes observed.1 Abstract We present a web-based system that facilitates the exploration of complex morphological patterns found in morphologically rich languages. The need for better understanding of such patterns is urgent for linguistics and important for cross-linguistically applicable natural language processing. We give an overview of the system architecture and describe a sample case study on Abui [abz], a Trans-New Guinea language spoken in Indonesia. 1 Emily M. Bender University of Washington Department"
N19-4022,W13-2710,1,0.87228,"Missing"
N19-4022,W18-5806,0,0.0175503,"uistics ebender@uw.edu Introduction Understanding and describing morphological patterns is a fundamental task in both documentary linguistics and the development of language technology. Many low-resource or underdescribed languages evince a high degree of morphological complexity, with large numbers of distinct affix types and many affix tokens possible within a single word. At the same time, building language technology for morphologically complex lowresource languages requires a rule-based morphological analyzer when datasets are not large enough for ML approaches (see Garrette et al. 2013; Erdmann and Habash 2018, inter alia). Our contribution is within the context of the AGGREGATION project, which aims to automatically infer broad typological characteristics and morphological patterns for understudied languages (Bender et al., 2013; Zamaraeva et al., 2017). We developed this visualization tool to help linguists to understand the morphological system implicit in large datasets and to refine automatically generated grammar specifications which model that morphological system. Thus we expect this tool to directly assist in language description. Because linguistic typology depends on accurate language de"
N19-4022,P13-1057,0,0.0238099,"ton Department of Linguistics ebender@uw.edu Introduction Understanding and describing morphological patterns is a fundamental task in both documentary linguistics and the development of language technology. Many low-resource or underdescribed languages evince a high degree of morphological complexity, with large numbers of distinct affix types and many affix tokens possible within a single word. At the same time, building language technology for morphologically complex lowresource languages requires a rule-based morphological analyzer when datasets are not large enough for ML approaches (see Garrette et al. 2013; Erdmann and Habash 2018, inter alia). Our contribution is within the context of the AGGREGATION project, which aims to automatically infer broad typological characteristics and morphological patterns for understudied languages (Bender et al., 2013; Zamaraeva et al., 2017). We developed this visualization tool to help linguists to understand the morphological system implicit in large datasets and to refine automatically generated grammar specifications which model that morphological system. Thus we expect this tool to directly assist in language description. Because linguistic typology depend"
P08-1111,I05-2035,1,0.895283,"e other. Nevertheless, they are key to various NLP applications, including those benefiting from deep natural language understanding (e.g., textual inference (Bobrow et al., 2007)), generation of wellformed output (e.g., natural language weather alert systems (Lareau and Wanner, 2007)) or both (as in machine translation (Oepen et al., 2007)). Of particular interest here are applications concerning endangered languages: Endangered languages represent a case of minimal linguistic resources, typically lacking even moderately-sized corpora, let alone The LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005; Drellishak and Bender, 2005) is a toolkit for reducing the cost of creating broad-coverage precision grammars by prepackaging both a cross-linguistic core grammar and a series of libraries of analyses of cross-linguistically variable phenomena, such as major-constituent word order or question formation. The Grammar Matrix was developed initially on the basis of broadcoverage grammars for English (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for Norwegian (Hellan and Haugerei"
P08-1111,W02-1502,1,0.835733,"and too brittle on the other. Nevertheless, they are key to various NLP applications, including those benefiting from deep natural language understanding (e.g., textual inference (Bobrow et al., 2007)), generation of wellformed output (e.g., natural language weather alert systems (Lareau and Wanner, 2007)) or both (as in machine translation (Oepen et al., 2007)). Of particular interest here are applications concerning endangered languages: Endangered languages represent a case of minimal linguistic resources, typically lacking even moderately-sized corpora, let alone The LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005; Drellishak and Bender, 2005) is a toolkit for reducing the cost of creating broad-coverage precision grammars by prepackaging both a cross-linguistic core grammar and a series of libraries of analyses of cross-linguistically variable phenomena, such as major-constituent word order or question formation. The Grammar Matrix was developed initially on the basis of broadcoverage grammars for English (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for N"
P08-1111,W07-1403,0,0.0530476,"Missing"
P08-1111,W02-1503,0,0.0484145,"visible for further combination with modifiers. In addition, heads can combine directly with modifiers of their arguments (as opposed to just modifiers of themselves). Argument realization and the combination of heads and modifiers are fairly fundamental aspects of the system implemented in the Matrix. In light of the departure described above, it is interesting to see to what extent the Matrix can still support rapid development of a precision grammar for Wambaya. 2.3 Related Work There are currently many multilingual grammar engineering projects under active development, including ParGram, (Butt et al., 2002; King et al., 2005), the MetaGrammar project (Kinyon et al., 2006), KPML (Bateman et al., 2005), Grammix (M¨uller, 2007) and OpenCCG (Baldridge et al., 2007). Among approaches to multilingual grammar engineering, the Grammar Matrix’s distinguishing characteristics include the deployment of a shared core grammar for crosslinguistically consistent constraints and a series of libraries modeling varying linguistic properties. Thus while other work has successfully exploited grammar porting between typologically related languages (e.g., Kim et al., 2003), to my knowledge, no other grammar porting"
P08-1111,W06-1503,0,0.0253573,"ds can combine directly with modifiers of their arguments (as opposed to just modifiers of themselves). Argument realization and the combination of heads and modifiers are fairly fundamental aspects of the system implemented in the Matrix. In light of the departure described above, it is interesting to see to what extent the Matrix can still support rapid development of a precision grammar for Wambaya. 2.3 Related Work There are currently many multilingual grammar engineering projects under active development, including ParGram, (Butt et al., 2002; King et al., 2005), the MetaGrammar project (Kinyon et al., 2006), KPML (Bateman et al., 2005), Grammix (M¨uller, 2007) and OpenCCG (Baldridge et al., 2007). Among approaches to multilingual grammar engineering, the Grammar Matrix’s distinguishing characteristics include the deployment of a shared core grammar for crosslinguistically consistent constraints and a series of libraries modeling varying linguistic properties. Thus while other work has successfully exploited grammar porting between typologically related languages (e.g., Kim et al., 2003), to my knowledge, no other grammar porting project has covered the same typological dis3 A linearization-based"
P08-1111,P06-4014,0,0.075896,"ase Structure Grammar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework. Grammars in HPSG are expressed as a collection of typed feature structures which are arranged into a hierarchy such that information shared across multiple lexical entries or construction types is represented only on a single supertype. The Matrix is written in the TDL (type description language) formalism, which is interpreted by the LKB parser, generator, and grammar development environment (Copestake, 2002). It is compatible with the broader range of DELPH-IN tools, e.g., for machine translation (Lønning and Oepen, 2006), treebanking (Oepen et al., 2004) and parse selection (Toutanova et al., 2005). The Grammar Matrix consists of a crosslinguistic core type hierarchy and a collection of phenomenon-specific libraries. The core type hierarchy defines the basic feature geometry, the ways that heads combine with arguments and adjuncts, linking types for relating syntactic to semantic arguments, and the constraints required to compositionally build up semantic representations in the format of Minimal Recursion Semantics (Copestake et al., 2005; Flickinger and Bender, 2003). The libraries provide collections of ana"
P08-1111,W02-1508,1,0.897994,"Missing"
P08-1111,2007.tmi-papers.18,0,0.0257193,"Missing"
P08-1111,J86-2003,0,0.111467,"Missing"
P08-1111,W02-1210,1,0.842074,"of minimal linguistic resources, typically lacking even moderately-sized corpora, let alone The LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005; Drellishak and Bender, 2005) is a toolkit for reducing the cost of creating broad-coverage precision grammars by prepackaging both a cross-linguistic core grammar and a series of libraries of analyses of cross-linguistically variable phenomena, such as major-constituent word order or question formation. The Grammar Matrix was developed initially on the basis of broadcoverage grammars for English (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for Norwegian (Hellan and Haugereid, 2003), Modern Greek (Kordoni and Neu, 2005), and Spanish (Marimon et al., 2007), as well as being applied to 42 other languages from a variety of language families in a classroom context (Bender, 2007). This paper aims to evaluate both the utility of the Grammar Matrix in jump-starting precision grammar development and the current state of its crosslinguistic hypotheses through a case study of a 977 Proceedings of ACL-08: HLT, pages 977–985, c Columbu"
P10-4001,I05-2035,1,0.893639,"nded entry fields, and the population of drop-down selectors. The lists in an iterated section can be expanded or shortened with “Add” and “Delete” buttons near the items in question. Drop-down selectors can be automatically populated in several different ways.4 These dynamic drop-downs greatly lessen the amount of information the user must remember while filling out the questionnaire and can prevent the user from trying to enter an invalid value. Both of these operations occur without refreshing the page, saving time for the user. Unbounded Content Early versions of the customization system (Bender and Flickinger, 2005; Drellishak and Bender, 2005) only allowed a finite (and small) number of entries for things like lexical types. For instance, users were required to provide exactly one transitive verb type and one intransitive verb type. The current system has an iterator mechanism in the questionnaire that allows for repeated sections, and thus unlimited entries. These repeated sections can also be nested, which allows for much more richly structured information. The utility of the iterator mechanism is most apparent when filling out the Lexicon subpage. Users can create an arbitrary number of lexical rule"
P10-4001,W02-1502,1,0.860814,"system: a repository of distilled linguistic knowledge and a web-based service which elicits a typological description of a language from the user and yields a customized grammar fragment ready for sustained development into a broad-coverage grammar. We describe the implementation of this repository with an emphasis on how the information is made available to users, including in-browser testing capabilities. 1 Introduction This demonstration presents the LinGO Grammar Matrix grammar customization system1 and its functionality for rapidly prototyping grammars. The LinGO Grammar Matrix project (Bender et al., 2002) is situated within the DELPH - IN2 collaboration and is both a repository of reusable linguistic knowledge and a method of delivering this knowledge to a user in the form of an extensible precision implemented grammar. The stored knowledge includes both a cross-linguistic core grammar and a series of “libraries” containing analyses of cross-linguistically variable phenomena. The core grammar handles basic phrase types, semantic compositionality, and general infrastructure such as the feature geometry, while the current set of libraries includes analyses of word order, person/number/gender, te"
P10-4001,monson-etal-2008-linguistic,0,0.01913,"d fills in appropriate predicates. The test-by-generation process then sends these constructed MRSs to the LKB process and displays the generation results, along with a brief explanation of the input semantics that gave rise to them, in HTML for the user.7 4 and an implemented grammar. The latter is in the format required by PC - PATR (McConnel, 1995), and is used primarily to disambiguate morphological analyses of lexical items in the input string. Other systems that attempt to elicit linguistic information from a user include the Expedition (McShane and Nirenburg, 2003) and Avenue projects (Monson et al., 2008), which are specifically targeted at developing machine translation for lowdensity languages. These projects differ from the Grammar Matrix customization system in eliciting information from native speakers (such as paradigms or translations of specifically tailored corpora), rather than linguists. Further, unlike the Grammar Matrix customization system, they do not produce resources meant to sustain further development by a linguist. Related Work As stated above, the engineering goal of the Grammar Matrix is to facilitate the rapid development of large-scale precision grammars. The starter gr"
P10-4001,W02-1503,0,0.0956363,"t of large-scale precision grammars. The starter grammars output by the customization system are compatible in format and semantic representations with existing DELPH - IN tools, including software for grammar development and for applications including machine translation (Oepen et al., 2007) and robust textual entailment (Bergmair, 2008). More broadly, the Grammar Matrix is situated in the field of multilingual grammar engineering, or the practice of developing linguisticallymotivated grammars for multiple languages within a consistent framework. Other projects in this field include ParGram (Butt et al., 2002; King et al., 2005) (LFG), the CoreGram project8 (e.g., (M¨uller, 2009)) (HPSG), and the MetaGrammar project (de la Clergerie, 2005) (TAG). To our knowledge, however, there is only one other system that elicits typological information about a language and outputs an appropriately customized implemented grammar. The system, described in (Black, 2004) and (Black and Black, 2009), is called PAWS (Parser And Writer for Syntax) and is available for download online.9 PAWS is being developed by SIL in the context of both descriptive (prose) grammar writing and “computer-assisted related language ada"
P10-4001,2007.tmi-papers.18,0,0.0104236,"ms or translations of specifically tailored corpora), rather than linguists. Further, unlike the Grammar Matrix customization system, they do not produce resources meant to sustain further development by a linguist. Related Work As stated above, the engineering goal of the Grammar Matrix is to facilitate the rapid development of large-scale precision grammars. The starter grammars output by the customization system are compatible in format and semantic representations with existing DELPH - IN tools, including software for grammar development and for applications including machine translation (Oepen et al., 2007) and robust textual entailment (Bergmair, 2008). More broadly, the Grammar Matrix is situated in the field of multilingual grammar engineering, or the practice of developing linguisticallymotivated grammars for multiple languages within a consistent framework. Other projects in this field include ParGram (Butt et al., 2002; King et al., 2005) (LFG), the CoreGram project8 (e.g., (M¨uller, 2009)) (HPSG), and the MetaGrammar project (de la Clergerie, 2005) (TAG). To our knowledge, however, there is only one other system that elicits typological information about a language and outputs an appropri"
P10-4001,W09-2604,0,0.0154127,"ing classes (types) of lexical entries and lexical rules which apply to those types. The grammars produced are compatible with both the grammar development tools and the 2 System Overview Grammar customization with the LinGO Grammar Matrix consists of three primary activities: filling out the questionnaire, preliminary testing of the grammar fragment, and grammar creation. 2.1 Questionnaire Most of the linguistic phenomena supported by the questionnaire vary across languages along multiple dimensions. It is not enough, for example, 3 Research of this type based on the Grammar Matrix includes (Crysmann, 2009) (tone change in Hausa) and (Fokkens et al., 2009) (Turkish suspended affixation). 1 http://www.delph-in.net/matrix/customize/ 2 http://www.delph-in.net 1 Proceedings of the ACL 2010 System Demonstrations, pages 1–6, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics morphemes which each in turn bear any number of feature constraints. For example, the user could create a tense-agreement morphological slot, which contains multiple portmanteau morphemes each expressing some combination of tense, subject person and subject number values (e.g., French -ez expresses 2nd"
P10-4001,W05-1522,0,\N,Missing
P14-1007,S12-1040,0,0.56099,"of negation and called for systems to analyze negation—detecting cues (affixes, words, or phrases that express negation), resolving their scopes (which parts of a sentence are actually negated), and identifying the negated event or property. The task organizers designed and documented an annotation scheme (Morante and Daelemans, 2012) and applied it to a little more than 100,000 tokens of running text by the novelist Sir Arthur Conan Doyle. While the task and annotations were framed from a semantic perspective, only one participating system actually employed explicit compositional semantics (Basile et al., 2012), with results ranking in the middle of the 12 participating systems. Conversely, the bestperforming systems approached the task through machine learning or heuristic processing over syntactic and linguistically relatively coarse-grained representations; see § 2 below. Example (1), where hi marks the cue and {} the in-scope elements, illustrates the annotations, including how negation inside a noun phrase can scope over discontinuous parts of the sentence.1 In this work, we revisit Shared Task 1 from the 2012 *SEM Conference: the automated analysis of negation. Unlike the vast majority of part"
P14-1007,J12-2001,0,0.0330887,"of (quantifier and operator) scope in mainstream underspecified semantics. With reference to an explicit encoding of semantic predicate-argument structure, we can operationalize the annotation decisions made for the 2012 *SEM task, and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best published results on this task to date. 1 Introduction (1) Recently, there has been increased community interest in the theoretical and practical analysis of what Morante and Sporleder (2012) call modality and negation, i.e. linguistic expressions that modulate the certainty or factuality of propositions. Automated analysis of such aspects of meaning is important for natural language processing tasks which need to consider the truth value of statements, such as for example text mining (Vincze et al., 2008) or sentiment analysis (Lapponi et al., 2012). Owing to its immediate utility in the curation of scholarly results, the analysis of negation and so-called hedges in bio-medical research literature has been the focus of several workshops, as well as the Shared Task at the 2011 Con"
P14-1007,oepen-lonning-2006-discriminant,1,0.347561,"encoded in the Shared Task annotations is not concerned with the relative scope of quantifiers and negation, such as the two possible readings of (2) represented informally below:5 (2) mentary prediction includes a predicate symbol, a label (or ‘handle’, prefixed to predicates with a colon in Fig. 1), and one or more argument positions, whose values are semantic variables. Eventualities (ei ) in MRS denote states or activities, while instance variables (x j ) typically correspond to (referential or abstract) entities. All EPs have the argument position ARG0, called the distinguished variable (Oepen and Lønning, 2006), and no variable is the ARG0 of more than one nonquantifier EP. The arguments of one EP are linked to the arguments of others either directly (sharing the same variable as their value), or indirectly (through socalled ‘handle constraints’, where =q in Fig. 1 denotes equality modulo quantifier insertion). Thus a well-formed MRS forms a connected graph. In addition, the grammar links the EPs to the elements of the surface string that give rise to them, via character offsets recorded in each EP (shown in angle brackets in Fig. 1). For the purposes of the present task, we take a negation cue as o"
P14-1007,S12-1041,1,0.505351,"epresentations provided by a general-purpose deep parser. Our contributions are three-fold: Theoretically, we correlate the structures at play in the Morante and Daelemans (2012) view on negation with formal semantic analyses; methodologically, we demonstrate how to approach the task in terms of underspecified, logical-form semantics; and practically, our combined system retroactively ‘wins’ the 2012 *SEM Shared Task. In the following sections, we review related work (§ 2), detail our own setup (§ 3), and present and discuss our experimental results (§ 4 and § 5, respectively). 2 Related Work Read et al. (2012) describe the best-performing submission to Task 1 of the 2012 *SEM Conference. They investigated two approaches for scope resolution, both of which were based on syntactic constituents. Firstly, they created a set of 11 heuristics that describe the path from the preterminal of a cue to the constituent whose projection is predicted to match the scope. Secondly they trained an SVM ranker over candidate constituents, generated by following the path from a cue to the root of the tree and describing each candidate in terms of syntactic properties along the path and various surface features. Both a"
P14-1007,P07-2009,0,0.0293845,"Missing"
P14-1007,W08-0606,0,0.641248,"m an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best published results on this task to date. 1 Introduction (1) Recently, there has been increased community interest in the theoretical and practical analysis of what Morante and Sporleder (2012) call modality and negation, i.e. linguistic expressions that modulate the certainty or factuality of propositions. Automated analysis of such aspects of meaning is important for natural language processing tasks which need to consider the truth value of statements, such as for example text mining (Vincze et al., 2008) or sentiment analysis (Lapponi et al., 2012). Owing to its immediate utility in the curation of scholarly results, the analysis of negation and so-called hedges in bio-medical research literature has been the focus of several workshops, as well as the Shared Task at the 2011 Conference on Computational Language Learning (CoNLL). {The German} was sent for but professed to {know} hnothingi {of the matter}. In this work, we return to the 2012 *SEM task from a deliberately semantics-centered point of view, focusing on the hardest of the three sub-problems: scope resolution.2 Where Morante and Dae"
P14-1007,P13-1166,0,0.0206515,"Missing"
P14-1007,P05-3003,0,0.0325545,"y as a fall-back in system combination as described in § 3.4 below. Scopal information in MRS analyses delivered by the ERG fixes the scope of operators—such as negation, modals, scopal adverbs (including subordinating conjunctions like while), and clauseembedding verbs (e.g. believe)—based on their position in the constituent structure, while leaving the scope of quantifiers (e.g. a or every, but also other determiners) free. From these underspecified representations of possible scopal configurations, a scope resolution component can spell out the full range of fully-connected logical forms (Koller and Thater, 2005), but it turns out that such enumeration is not relevant here: the notion of scope encoded in the Shared Task annotations is not concerned with the relative scope of quantifiers and negation, such as the two possible readings of (2) represented informally below:5 (2) mentary prediction includes a predicate symbol, a label (or ‘handle’, prefixed to predicates with a colon in Fig. 1), and one or more argument positions, whose values are semantic variables. Eventualities (ei ) in MRS denote states or activities, while instance variables (x j ) typically correspond to (referential or abstract) ent"
P14-1007,S12-1035,0,0.381171,"Missing"
P14-1007,morante-daelemans-2012-conandoyle,0,\N,Missing
P14-1007,L14-1000,0,\N,Missing
Q18-1041,C16-1111,0,0.0139276,"tion, prosody, word choice, and grammar) correlates with speaker demographic characteristics (Labov, 1966), as speakers use linguistic variation to construct and project identities (Eckert and Rickford, 2001). Transfer from native languages (L1) can affect the language produced by non-native (L2) speakers (Ellis, 1994, Ch. 8). A further important type of variation is disordered speech (e.g., dysarthria). Specifications include: Proposed Data Statement Schema We propose the following schema of information to include in long and short form data statements. • • • • • • • 7 A notable exception is Derczynski et al. (2016), who present a corpus of tweets collected to sample diverse speaker communities (location, type of engagement with Twitter), at diverse points in time (time of year, month, and day), and annotated with named entity labels by crowdworker annotators from the same locations as the tweet authors. 8 Older datasets can be retrofitted with citeable long-form data statements published on project Web pages or archives. Age Gender Race/ethnicity Native language Socioeconomic status Number of different speakers represented Presence of disordered speech 9 https://tools.ietf.org/rfc/bcp/bcp47. txt. 590 5."
Q18-1041,soria-etal-2012-flarenet,0,0.0546237,"Missing"
Q18-1041,P17-2009,0,0.117711,"the moment agree [. . . ] that disabled people in the work place should be able to access technology, just as they should be able to access a public building. As system designers we can make the choice to try to construct a technological infrastructure which disabled people can access. If we do not make this choice, then we single-handedly undermine the principle of universal access. But if we do make this choice, and are successful, disabled people would still rely, for example, on employers to hire them. (page 3) 7.1 Public Health and NLP for Social Media This value scenario is inspired by Jurgens et al. (2017), who provide a similar one to motivate training language ID systems on more representative datasets. Scenario. Big U Hospital in a town in the Upper Midwest of the US collaborates with the Computer Science Department at Big U to create a Twitter-based early warning system for infectious disease called DiseaseAlert. Big U Hospital finds that the system improves patient outcomes by alerting hospital staff to emerging community health needs and alerting physicians to test for infectious diseases that currently are active locally. Big U decides to make the DiseaseAlert project open source to prov"
Q18-1041,S18-2005,0,0.0225768,"pecific tasks (e.g., the Stanford parser [Klein and Manning, 2003] trained on the Penn Treebank [Marcus et al., 1993] to do English parsing) and user-facing products such as Amazon’s Alexa or Google Home. real-world consequences for both direct and indirect stakeholders. For example, Speer (2017) found that a sentiment analysis system rated reviews of Mexican restaurants as more negative than other types of food with similar star ratings, because of associations between the word Mexican and words with negative sentiment in the larger corpus on which the word embeddings were trained. (See also Kiritchenko and Mohammad, 2018.) In these and other ways, pre-existing biases can be trained into NLP systems. There are other studies showing that systems from part of speech taggers (Hovy and Søgaard, 2015; Jørgensen et al., 2015) to speech recognition engines (Tatman, 2017) perform better for speakers whose demographic characteristics better match those represented in the training data. These are examples of emergent bias. Because the linguistic data we use will always include pre-existing biases and because it is not possible to build an NLP system in such a way that it is immune to emergent bias, we must seek addition"
Q18-1041,P03-1054,0,0.0115878,"anguage and video or other additional signals. Here, our focus is on linguistic data. 4 Datasets used during algorithm development can influence design choices in machine learning approaches too: Munro and Manning (2010) found that subword information, not helpful in English SMS classification, is extremely valuable in Chichewa, a morphologically complex language with high orthographic variability. 588 of natural language processing, typically involving algorithms trained on particular datasets. We use this term to refer to both components focused on specific tasks (e.g., the Stanford parser [Klein and Manning, 2003] trained on the Penn Treebank [Marcus et al., 1993] to do English parsing) and user-facing products such as Amazon’s Alexa or Google Home. real-world consequences for both direct and indirect stakeholders. For example, Speer (2017) found that a sentiment analysis system rated reviews of Mexican restaurants as more negative than other types of food with similar star ratings, because of associations between the word Mexican and words with negative sentiment in the larger corpus on which the word embeddings were trained. (See also Kiritchenko and Mohammad, 2018.) In these and other ways, pre-exi"
Q18-1041,W07-0101,0,0.01297,"ext is the substance of our contribution: a detailed proposal for data statements for NLP (§5), illustrated with two case studies (§6). In §7 we discuss how data statements can mitigate bias and use the technique of “value scenarios” to envision potential effects of their adoption. Finally, we relate data statements to similar emerging proposals (§8), make recommendations for how to implement and promote the uptake of data statements (§9), and lay out considerations for tech policy (§10). 2 the review (e.g., Pang et al., 2002) or the hashtag #sarcasm used to identify sarcastic language (e.g., Kreuz and Caucci, 2007). Speaker We use the term speaker to refer to the individual who produced some segment of linguistic behavior included in the dataset, even if the linguistic behavior is originally written. Annotator The term annotator refers to people who assign annotations to the raw data, including transcribers of spoken data. Annotators may be crowdworkers or highly trained researchers, sometimes involved in the creation of the annotation guidelines. Annotation is often done semiautomatically, with NLP tools being used to create a first pass that is corrected or augmented by human annotators. Curator A thi"
Q18-1041,P15-2079,0,0.0296091,"exa or Google Home. real-world consequences for both direct and indirect stakeholders. For example, Speer (2017) found that a sentiment analysis system rated reviews of Mexican restaurants as more negative than other types of food with similar star ratings, because of associations between the word Mexican and words with negative sentiment in the larger corpus on which the word embeddings were trained. (See also Kiritchenko and Mohammad, 2018.) In these and other ways, pre-existing biases can be trained into NLP systems. There are other studies showing that systems from part of speech taggers (Hovy and Søgaard, 2015; Jørgensen et al., 2015) to speech recognition engines (Tatman, 2017) perform better for speakers whose demographic characteristics better match those represented in the training data. These are examples of emergent bias. Because the linguistic data we use will always include pre-existing biases and because it is not possible to build an NLP system in such a way that it is immune to emergent bias, we must seek additional strategies for mitigating the scientific and ethical shortcomings that follow from imperfect datasets. We propose here that foregrounding the characteristics of our datasets"
Q18-1041,P16-2096,0,0.0447572,"standardized information about the populations studied (e.g., APA, 2009; Moher et al., 2010; Furler et al., 2012; Mbuagbaw et al., 2017). Though the construct of data statements applies more broadly, in this paper we focus specifically on data statements for NLP systems. Data statements should be included in most writing on NLP including: papers presenting new datasets, papers reporting experimental work with datasets, and documentation for NLP systems. Data statements should 1 This interest has manifested in workshops (Fort et al., 2016; Devillers et al., 2016; Hovy et al., 2017) and papers (Hovy and Spruit, 2016) in NLP, as well as workshops in related fields, notably the FATML series (http://www. fatml.org/) held annually since 2014. 587 Transactions of the Association for Computational Linguistics, vol. 6, pp. 587–604, 2018. Action Editor: Yuji Matsumoto. Submission batch: 5/2018; Revision batch: 8/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. help us as a field engage with the ethical issues of exclusion, overgeneralization, and underexposure (Hovy and Spruit, 2016). Furthermore, as data statements bring our datasets and their repr"
Q18-1041,C14-1022,0,0.0202337,"Missing"
Q18-1041,D13-1066,0,0.0186292,"Missing"
Q18-1041,W17-1606,0,0.023652,"olders. For example, Speer (2017) found that a sentiment analysis system rated reviews of Mexican restaurants as more negative than other types of food with similar star ratings, because of associations between the word Mexican and words with negative sentiment in the larger corpus on which the word embeddings were trained. (See also Kiritchenko and Mohammad, 2018.) In these and other ways, pre-existing biases can be trained into NLP systems. There are other studies showing that systems from part of speech taggers (Hovy and Søgaard, 2015; Jørgensen et al., 2015) to speech recognition engines (Tatman, 2017) perform better for speakers whose demographic characteristics better match those represented in the training data. These are examples of emergent bias. Because the linguistic data we use will always include pre-existing biases and because it is not possible to build an NLP system in such a way that it is immune to emergent bias, we must seek additional strategies for mitigating the scientific and ethical shortcomings that follow from imperfect datasets. We propose here that foregrounding the characteristics of our datasets can help, by allowing reasoning about what the likely effects may be a"
Q18-1041,W16-5618,0,0.162169,"Missing"
Q18-1041,N16-2013,0,0.292232,"ased on independent information about Twitter usage and impressionistic observation of the tweets by the dataset curators, the data is likely to include tweets from both younger (18–30 years) and older (30+ years) adult speakers, the majority of whom likely identify as white. No direct information is available about gender distribution or socioeconomic status of the speakers. It is expected that most, but not all, of the speakers speak English as a native language. Hate Speech Twitter Annotations The Hate Speech Twitter Annotations collection is a set of labels for ∼19,000 tweets collected by Waseem and Hovy (2016) and Waseem (2016). The dataset can be accessed via https:// github.com/zeerakw/hatespeech.12 A. C URATION R ATIONALE In order to study the automatic detection of hate speech in tweets and the effect of annotator knowledge (crowdworkers vs. experts) on the effectiveness of models trained on the annotations, Waseem and Hovy (2016) performed a scrape of Twitter data using contentious terms and topics. The terms were chosen by first crowdsourcing an initial set of search terms on feminist Facebook groups and then reviewing the resulting tweets for terms to use and adding others based on the resea"
W02-1210,C00-1004,0,0.036521,"ations and the syntax-semantic interface already worked out in the ERG were directly applicable to the Japanese grammar. In those cases where Japanese presented problems not yet encountered (or at least not yet tackled) in English, it was fairly straightforward to work out suitable MRS representations and means of building them up. Both of these points illustrate the cross-linguistic validity and practical utility of MRS representations. 3 Integration Analyzer of a Morphological As Japanese written text does not have word segmentation, a preprocessing system is required. We integrated ChaSen (Asahara & Matsumoto 2000), a tool that provides word segmentation as well as POS tags and morphological information such as verbal inflection. As the lexical coverage of ChaSen is higher than that of the HPSG lexicon, default part-of-speech entries are inserted into the lexicon. These are triggered by the part-ofspeech information given by ChaSen, if there is no existing entry in the lexicon. These specific default entries assign a type to the word that contains features typical to its part-of-speech. It is therefore possible to restrict the lexicon to those cases where the lexical information contains more than the t"
W02-1210,P01-1019,0,0.107635,"Missing"
W02-1210,Y99-1034,1,0.775104,"wo DAT book ACC katte kure-ta. buy give-past &apos;The teacher bought me a book.&apos; Example 4: Watashi ga I sensei ni NOM teacher DAT katte morat-ta. buy get-past hon wo book ACC &apos;The teacher bought me a book.&apos; 1.4 Particles in a type hierarchy The careful treatment of Japanese particles is essential, because they are the most frequently occurring words and have various central functions in the grammar. It is difficult, because one particle can fulfill more than one function and they can co-occur, but not arbitrarily. The Japanese grammar thus contains a type hierarchy of 44 types for particles. See Siegel (1999) for a more detailed description of relevant phenomena and solutions. 1.5 Numeral Expressions Number names, such as sen kyuu hyaku juu &apos;1910&apos; constitute a notable exception to the general head-final pattern of Japanese phrases. We found Smith&apos;s (1999) head-medial analysis of English number names to be directly applicable to the Japanese system as well (Bender 2002). This analysis was easily incorporated into the grammar, despite the oddity of head positioning, because the type hierarchy of HPSG is well suited to express the partial generalizations that permeate natural language. On the other h"
W02-1210,C00-1060,0,\N,Missing
W02-1210,C02-2025,0,\N,Missing
W02-1502,C00-1004,0,0.00894855,"system developed at Tokyo University (Makino, Yoshida, Torisawa, & Tsujii, 1998), and a parallel processing system developed in Objective C at Delft University (The Netherlands; van Lohuizen, 2002). As part of the matrix package, sample configuration files and documentation will be provided for at least some of these additional platforms. Existing pre-processing packages can also significantly reduce the effort required to develop a new grammar, particularly for coping with the morphology/syntax interface. For example, the ChaSen package for segmenting Japanese input into words and morphemes (Asahara & Matsumoto, 2000) has been linked to at least the LKB and PET systems. Support for connecting implementations of language-specific pre-processing packages of this kind will be preserved and extended as the matrix develops. Likewise, configuration files are included to support generation, at least within the LKB, provided that the grammar conforms to certain assumptions about semantic representation using the Minimal Recursion Semantics framework. Finally, a methodology is under development for constructing and using test suites organized around a typology of linguistic phenomena, using the implementation platf"
W02-1502,1995.tmi-1.2,1,0.665663,"t constituent structure (in one view, Norwegian exhibits a VSO topology in the main clause). The user groups have suggested refinements and extensions of the basic inventory, and it is expected that general solutions, as they are identified jointly, will propagate into the existing grammars too. 3 A Detailed Example As an example of the level of detail involved in the grammar matrix, in this section we consider the analysis of intersective and scopal modification. The matrix is built to give Minimal Recursion Semantics (MRS; Copestake et al., 2001; Copestake, Flickinger, Sag, & Pollard, 1999; Copestake, Flickinger, Malouf, Riehemann, & Sag, 1995) representations. The two English examples in (1) exemplify the difference between intersective and scopal modification:1 (1) a. Keanu studied Kung Fu on a spaceship. b. Keanu probably studied Kung Fu. The MRSs for (1a-b) (abstracting away from agreement information) are given in (2) and (3). The MRSs are ordered tuples consisting of a top handle (h1 in both cases), an instance or event variable (e in both cases), a bag of elementary predications (eps), and a bag of scope constraints (in these cases, QEQ constraints or ‘equal modulo quantifiers’). In a well-formed MRS, the handles can be 1 Th"
W02-1502,P01-1019,1,0.872623,"t we foresee and an evaluation methodology for the matrix proper. 2 Preliminary Development of Matrix We have produced a preliminary version of the grammar matrix relying heavily on the LinGO project’s English Resource Grammar, and to a lesser extent on the Japanese grammar developed jointly between DFKI Saarbr¨ucken (Germany) and YY Technologies (Mountain View, CA). This early version of the matrix comprises the following components:  Types defining the basic feature geometry and technical devices (e.g., for list manipulation).  Types associated with Minimal Recursion Semantics (see, e.g., Copestake, Lascarides, & Flickinger, 2001), a meaning representation language which has been shown to be wellsuited for semantic composition in typed feature structure grammars. This portion of the grammar matrix includes a hierarchy of relation types, types and constraints for the propagation of semantic information through the phrase structure tree, a representation of illocutionary force, and provisions for grammar rules which make semantic contributions.  General classes of rules, including derivational and inflectional (lexical) rules, unary and binary phrase structure rules, headed and non-headed rules, and head-initial and he"
W02-1502,P98-2132,0,0.049922,"Missing"
W02-1502,2000.iwpt-1.19,1,0.705383,"ecting implementations of language-specific pre-processing packages of this kind will be preserved and extended as the matrix develops. Likewise, configuration files are included to support generation, at least within the LKB, provided that the grammar conforms to certain assumptions about semantic representation using the Minimal Recursion Semantics framework. Finally, a methodology is under development for constructing and using test suites organized around a typology of linguistic phenomena, using the implementation platform of the [incr tsdb()] profiling package (Oepen & Flickinger, 1998; Oepen & Callmeier, 2000). These test suites will enable better communication about current coverage of a given grammar built using the matrix, and serve as the basis for identifying additional phenomena that need to be addressed cross-linguistically within the matrix. Of course, the development of the typology of phenomena is itself a major undertaking for which a systematic cross-linguistic approach will be needed, a discussion of which is outside the scope of this report. But the intent is to seed this classification scheme with a set of relatively coarse-grained phenomenon classes drawn from the existing grammars,"
W02-1502,C02-2025,1,0.16955,"t are important to sync to (e.g., changes that affect MRS outputs, fundamental changes to important analyses), (ii) develop a methodology for communicating changes in the matrix, their motivation and their implementation to the user community, and (iii) develop tools for semi-automating resynching of existing grammars to upgrades of the matrix. These tools could use the type hierarchy to predict where conflicts are likely to arise and bring these to the engineer’s attention, possibly inspired by the approach under development at CSLI for the dynamic maintenance of the LinGO Redwoods treebank (Oepen et al., 2002). Finally, while initial development of the matrix has been and will continue to be highly centralized, we hope to provide support for proposed matrix improvements from the user community. User feedback will already come in the form of case studies for the library as discussed in Section 5 above, but also potentially in proposals for modification of the matrix drawing on experiences in grammar development. In order to provide users with some cross-linguistic context in which to develop and evaluate such proposals themselves, we intend to provide some sample matrix-derived grammars and correspo"
W02-1502,W02-1210,1,0.532935,"necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding. 1 Introduction The past decade has seen the development of wide-coverage implemented grammars representing deep linguistic analysis of several languages in several frameworks, including Head-Driven Phrase Structure Grammar (HPSG), LexicalFunctional Grammar (LFG), and Lexicalized Tree Adjoining Grammar (LTAG). In HPSG, the most extensive grammars are those of English (Flickinger, 2000), German (M¨uller & Kasper, 2000), and Japanese (Siegel, 2000; Siegel & Bender, 2002). Despite being couched in the same general framework and in some cases being written in the same formalism and consequently being compatible with the same parsing and generation software, these grammars were developed more or less independently of each other. They each represent between 5 and 15 person years of research efforts, and comprise 35–70,000 lines of code. Unfortunately, most of that research is undocumented and the accumulated analyses, best practices for grammar engineering, and tricks of the trade are only available through painstaking inspection of the grammars and/or consultati"
W02-1502,C98-2128,0,\N,Missing
W02-1508,C00-1004,0,0.015174,"ammarians shifted their engineering focus on ‘tightening’ the grammar, i.e. the elimination of spurious ambiguity and overgeneration (see Siegel & Bender, 2002, for details on the grammar). Another view on grammar evolution is presented in Figure 3, depicting the ‘size’ of the Japanese grammar over the same five-month development cycle. Although measuring the size of 2 Quantifying input complexity for Japanese is a nontrivial task, as the count of the number of input words would depend on the approach to string segmentation used in a specific system (the fairly aggressive tokenizer of ChaSen, Asahara & Matsumoto, 2000, in our case); to avoid potential for confusion, we report input complexity in the (overtly systemspecific) number of lexical items stipulated by the grammar instead: around 50 and 80, on average, for the ‘banking’ and ‘trading’ data sets, respectively (as of February 2002). Grammar Size 10200 10000 9800 9600 9400 9200 9000 8800  •••••• • ••• •  • ••• • •• • •••   •••   •   •         • ••• •• ••  ••••••  ••••   ••    •• • (generated   by [incr tsdb()] at 30-jun-2002 (16:09 h)) •• • • — types  — rules 106 104 102 100 98 96 94 92 90"
W02-1508,P98-2132,0,0.02331,"on the following components and modules: • test and reference data stored with annotations in a structured database; annotations can range from minimal information (unique test item identifier, item origin, length et al.) to fine-grained linguistic classifications (e.g. regarding grammaticality and linguistic phenomena presented in an item), as they are represented in the TSNLP test suites, for example (Oepen, Netter, & Klein, 1997); • tools to browse the available data, identify suitable subsets and feed them through the analysis component of processing systems like the LKB and PET, LiLFeS (Makino, Yoshida, Torisawa, & Tsujii, 1998), TRALE (Penn, 2000), PAGE (Uszkoreit et al., 1994), and others; • the ability to gather a multitude of precise and fine-grained (grammar) competence and (system) performance measures—like the number of readings obtained per test item, various time and memory usage statistics, ambiguity and non-determinism metrics, and salient properties of the result structures—and store them in a uniform, platform-independent data format as a competence and performance profile; and • graphical facilities to inspect the resulting profiles, analyze system competence (i.e. grammatical coverage and overgenerati"
W02-1508,2000.iwpt-1.19,1,0.834837,"mmar development cannot be isolated from measurements of system resource consumption and overall performance (specific properties of a grammar may trigger idiosyncrasies or software bugs in a particular version of the processing system); therefore, and to enable exchange of reference points and comparability of experiments, grammarians and system developers alike should use the same, homogenuous set of relevant parameters. 3 Integrated Competence and Performance Profiling The integrated competence and performance profiling methodology and associated engineering platform, dubbed [incr tsdb()] (Oepen & Callmeier, 2000)1 and reviewed in the remainder of this sec1 See ‘http://www.coli.uni-sb.de/itsdb/’ for the (draft) [incr tsdb()] user manual, pronunciation rules, and instructions on obtaining and installing the package. tion, was designed to meet all of the requirements identified in the DFKI – YY case study. Generally speaking, the [incr tsdb()] environment is an integrated package for diagnostics, evaluation, and benchmarking in practical grammar and system engineering. The toolkit implements an approach to grammar development and system optimization that builds on precise empirical data and systematic ex"
W02-1508,W02-1210,1,0.838186,"overall average of readings assigned to each sentence varies around relatively small numbers. For the moderately complex email data2 the grammar often assigns less than ten analyses, rarely more than a few dozens. However, not surprisingly the addition of grammatical coverage comes with a sharp increase in ambiguity (which may indicate overgeneration): the graphs in Figure 2 clearly show that, once coverage on the ‘trading’ data was above eighty per cent, grammarians shifted their engineering focus on ‘tightening’ the grammar, i.e. the elimination of spurious ambiguity and overgeneration (see Siegel & Bender, 2002, for details on the grammar). Another view on grammar evolution is presented in Figure 3, depicting the ‘size’ of the Japanese grammar over the same five-month development cycle. Although measuring the size of 2 Quantifying input complexity for Japanese is a nontrivial task, as the count of the number of input words would depend on the approach to string segmentation used in a specific system (the fairly aggressive tokenizer of ChaSen, Asahara & Matsumoto, 2000, in our case); to avoid potential for confusion, we report input complexity in the (overtly systemspecific) number of lexical items s"
W02-1508,C94-1072,1,0.847612,"ored with annotations in a structured database; annotations can range from minimal information (unique test item identifier, item origin, length et al.) to fine-grained linguistic classifications (e.g. regarding grammaticality and linguistic phenomena presented in an item), as they are represented in the TSNLP test suites, for example (Oepen, Netter, & Klein, 1997); • tools to browse the available data, identify suitable subsets and feed them through the analysis component of processing systems like the LKB and PET, LiLFeS (Makino, Yoshida, Torisawa, & Tsujii, 1998), TRALE (Penn, 2000), PAGE (Uszkoreit et al., 1994), and others; • the ability to gather a multitude of precise and fine-grained (grammar) competence and (system) performance measures—like the number of readings obtained per test item, various time and memory usage statistics, ambiguity and non-determinism metrics, and salient properties of the result structures—and store them in a uniform, platform-independent data format as a competence and performance profile; and • graphical facilities to inspect the resulting profiles, analyze system competence (i.e. grammatical coverage and overgeneration) and performance (e.g. cpu time and memory usage,"
W02-1508,C96-2120,1,\N,Missing
W02-1508,P94-1040,0,\N,Missing
W02-1508,C98-2128,0,\N,Missing
W07-1218,I05-2035,1,0.924832,"a treated by the grammar as well as representative examples taken from texts from the target domain. In combination with test suite management software such as [incr tsdb()] (Oepen, 2002), they are used for validation and regression testing of precision (deep linguistic) grammars as well as the exploration of potential changes to the grammar. In this paper, we consider what happens when the precision grammar resource being developed isn’t a grammar of a particular language, but rather a crosslinguistic grammar resource. In particular, we consider the LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005). There are several (related) obstacles to making effective use of test suites in this scenario: (1) The Matrix core grammar isn’t itself a grammar, and therefore can’t parse any strings. (2) There is no single language modeled by the cross-linguistic resource from which to draw test strings. (3) The space of possible grammars (alternatively, language types) modeled by the resource is enormous, well beyond the scope of what can be thoroughly explored. We present a methodology for the validation and regression testing of the Grammar Matrix that addresses these obstacles, developing the ideas or"
W07-1218,W02-1502,1,0.926425,"du} Abstract phenomena treated by the grammar as well as representative examples taken from texts from the target domain. In combination with test suite management software such as [incr tsdb()] (Oepen, 2002), they are used for validation and regression testing of precision (deep linguistic) grammars as well as the exploration of potential changes to the grammar. In this paper, we consider what happens when the precision grammar resource being developed isn’t a grammar of a particular language, but rather a crosslinguistic grammar resource. In particular, we consider the LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005). There are several (related) obstacles to making effective use of test suites in this scenario: (1) The Matrix core grammar isn’t itself a grammar, and therefore can’t parse any strings. (2) There is no single language modeled by the cross-linguistic resource from which to draw test strings. (3) The space of possible grammars (alternatively, language types) modeled by the resource is enormous, well beyond the scope of what can be thoroughly explored. We present a methodology for the validation and regression testing of the Grammar Matrix that addresses these obst"
W07-1218,C00-1018,0,0.0607802,"Missing"
W07-1218,W03-2417,0,0.0169583,"grammars aren’t substantially simpler to write than the ‘actual’ grammars being tested. Even though this system requires some effort to maintain, we believe the methodology remains practical for two reasons. First, the input required from the developer enumerated above is closely related to the knowledge discovered in the course of building the libraries in the first place. Second, the fact that the filters are sensitive to only particular features of language types means that a relatively small number of filters can create test suites for a very large number of language types. 5 Related Work Kinyon and Rambow (2003) present an approach to generating test suites on the basis of descriptions of languages. The language descriptions are MetaGrammar (MG) hierarchies. Their approach appears to be more flexible than the one presented here in some ways, and more constrained in others. It does not need any input strings, but rather produces test items from the language description. In addition, it annotates the output in multiple ways, including phrase structure, dependency structure, and LFG Fstructure. On the other hand, there is no apparent provision for creating negative (ungrammatical) test data and it is do"
W08-0202,W02-0104,0,0.0308145,"ter’s program in Computational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals. 1 2 Introduction In the past two decades, there has been tremendous progress in natural language processing and various undergraduate/graduate programs in language technology have been established around the world (Koit et al., 2002; Frederking et al., 2002; Dale et al., 2002; Uszkoreit et al., 2005; Pilon et al., 2005). This paper introduces the University of Washington’s Professional Masters Program in Computational Linguistics (CLMA)—one of the largest programs of its kind in the United States—and highlights unique features that are key to its success. The CLMA program is currently operating in its third year as a fee-based degree program managed jointly by the Department of Linguistics and the Educational Outreach arm of the University. The program is distinguished by its programmatic focus, its flexibility, its format and delivery as well as in the partnershi"
W08-0202,W02-0106,0,0.0316358,"ign of a professional master’s program in Computational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals. 1 2 Introduction In the past two decades, there has been tremendous progress in natural language processing and various undergraduate/graduate programs in language technology have been established around the world (Koit et al., 2002; Frederking et al., 2002; Dale et al., 2002; Uszkoreit et al., 2005; Pilon et al., 2005). This paper introduces the University of Washington’s Professional Masters Program in Computational Linguistics (CLMA)—one of the largest programs of its kind in the United States—and highlights unique features that are key to its success. The CLMA program is currently operating in its third year as a fee-based degree program managed jointly by the Department of Linguistics and the Educational Outreach arm of the University. The program is distinguished by its programmatic focus, its flexibility, its format and delivery as well a"
W08-0202,W02-0112,0,0.0296132,"We present the design of a professional master’s program in Computational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals. 1 2 Introduction In the past two decades, there has been tremendous progress in natural language processing and various undergraduate/graduate programs in language technology have been established around the world (Koit et al., 2002; Frederking et al., 2002; Dale et al., 2002; Uszkoreit et al., 2005; Pilon et al., 2005). This paper introduces the University of Washington’s Professional Masters Program in Computational Linguistics (CLMA)—one of the largest programs of its kind in the United States—and highlights unique features that are key to its success. The CLMA program is currently operating in its third year as a fee-based degree program managed jointly by the Department of Linguistics and the Educational Outreach arm of the University. The program is distinguished by its programmatic focus, its flexibility, its form"
W08-0202,W05-0110,0,0.0706178,"Missing"
W08-0202,W05-0108,0,0.0306307,"mputational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals. 1 2 Introduction In the past two decades, there has been tremendous progress in natural language processing and various undergraduate/graduate programs in language technology have been established around the world (Koit et al., 2002; Frederking et al., 2002; Dale et al., 2002; Uszkoreit et al., 2005; Pilon et al., 2005). This paper introduces the University of Washington’s Professional Masters Program in Computational Linguistics (CLMA)—one of the largest programs of its kind in the United States—and highlights unique features that are key to its success. The CLMA program is currently operating in its third year as a fee-based degree program managed jointly by the Department of Linguistics and the Educational Outreach arm of the University. The program is distinguished by its programmatic focus, its flexibility, its format and delivery as well as in the partnerships that are an integral"
W08-0202,W08-0206,1,0.826739,"lt a question answering system, which was further developed into a submission to the TREC competition (Jinguji et al., 2006). This year’s class is developing a chatbot to submit to the Loebner Prize competition, an implementation of the Turing Test. Among the required courses, Ling 566 was created a year before the CLMA program started, and has been taught four times. Ling 450 is an established course from our Linguistics curriculum. Ling 570-573 were newly created for this program, and have each been taught three times now. We have put much effort in improving course design, as discussed in (Xia, 2008). 3.2 The prerequisites for the required courses In order to cover the range of methodologies and tasks that our program does in its core sequence, we need to set as a prerequisite the ability to program, including knowledge of data structures and algorithms, and expertise in C++ or Java.2 Another prerequisite is a college-level course in probability and statistics. Without such knowledge, it is all but impossible to discuss the sophisticated statistical models covered in the core NLP courses. For the two Linguistics required courses, the only prerequisite is a college-level introductory cours"
W08-2203,I05-2035,1,0.922873,"ons reflecting discourse status across a range of phenomena. Borthen and Haugereid (2005) propose COG - ST (‘cognitive-status’)1 as a feature on the syntax-semantics interface to handle phenomena associated with definiteness. We explore how their approach leads to cross-linguistically unified treatments of demonstratives, overt pronouns and null anaphora as well. We find that cross-linguistic studies motivate different representations than we might have arrived at from just one language. Our work grows out of the Grammar Matrix, a multilingual grammar engineering project (Bender et al., 2002; Bender and Flickinger, 2005) which strives to harmonize semantic representations across diverse languages. The Grammar Matrix is couched within the Head-driven Phrase Structure Grammar (HPSG) framework (Pollard and Sag, 1994). We use Minimal Recursion Semantics (Copestake et al., 2001, 2005) as our semantic representation system. 2 Background 2.1 Minimal Recursion Semantics Grammar Matrix-derived grammars associate surface strings with MRS representations (or MRSs), in a bidirectional mapping that allows both parsing and generation. An MRS consists of a multiset of elementary predications (eps), each of which is a single"
W08-2203,W02-1502,1,0.881677,"semantic representations reflecting discourse status across a range of phenomena. Borthen and Haugereid (2005) propose COG - ST (‘cognitive-status’)1 as a feature on the syntax-semantics interface to handle phenomena associated with definiteness. We explore how their approach leads to cross-linguistically unified treatments of demonstratives, overt pronouns and null anaphora as well. We find that cross-linguistic studies motivate different representations than we might have arrived at from just one language. Our work grows out of the Grammar Matrix, a multilingual grammar engineering project (Bender et al., 2002; Bender and Flickinger, 2005) which strives to harmonize semantic representations across diverse languages. The Grammar Matrix is couched within the Head-driven Phrase Structure Grammar (HPSG) framework (Pollard and Sag, 1994). We use Minimal Recursion Semantics (Copestake et al., 2001, 2005) as our semantic representation system. 2 Background 2.1 Minimal Recursion Semantics Grammar Matrix-derived grammars associate surface strings with MRS representations (or MRSs), in a bidirectional mapping that allows both parsing and generation. An MRS consists of a multiset of elementary predications (e"
W08-2203,P01-1019,0,0.226445,"o cross-linguistically unified treatments of demonstratives, overt pronouns and null anaphora as well. We find that cross-linguistic studies motivate different representations than we might have arrived at from just one language. Our work grows out of the Grammar Matrix, a multilingual grammar engineering project (Bender et al., 2002; Bender and Flickinger, 2005) which strives to harmonize semantic representations across diverse languages. The Grammar Matrix is couched within the Head-driven Phrase Structure Grammar (HPSG) framework (Pollard and Sag, 1994). We use Minimal Recursion Semantics (Copestake et al., 2001, 2005) as our semantic representation system. 2 Background 2.1 Minimal Recursion Semantics Grammar Matrix-derived grammars associate surface strings with MRS representations (or MRSs), in a bidirectional mapping that allows both parsing and generation. An MRS consists of a multiset of elementary predications (eps), each of which is a single relation with its associated arguments, labeled by a handle; a set of handle constraints relating the labels of eps to argument positions within other eps; and a top handle indicating which of the labels has outermost scope (Copestake et al., 2001, 2005)."
W08-2203,2007.tmi-papers.18,0,0.0416755,"Missing"
W09-0106,P08-1065,0,0.0127734,"nce in Turkish and Finnish is due to the system not being able to recognize variants of the same suffixes as the same, and, in addition, not being able to isolate all of the roots. Of course, in some cases, one language may represent, in some objective sense, a harder problem than another. A clear example of this is English letter-to-phoneme conversion, which, as a result of the lack of transparency in English orthography, is a harder problem that letter-to-phoneme conversion in other languages. Not surprisingly, the letter-to-phoneme systems described in e.g. (Jiampojamarn et al., 2008) and (Bartlett et al., 2008) do worse on the English test data than they do on German, Dutch, or French. On the other hand, just because one language may present a harder problem than the other doesn’t mean that system developers can assume that any performance differences can be explained in such a way. If one aims to create a language-independent system, then one must explore the possibility that the system includes assumptions about linguistic structure which do not hold up across all languages. The conclusions I would like to draw from these examples are as follows: A truly languageindependent system works equally we"
W09-0106,N03-2002,0,0.113528,"not in fact work equally well across languages, it is likely because something about the system design is making implicit assumptions about language structure. These assumptions are typically the result of “overfitting” to the original development language(s).3 In Secwithout any hand-coding of linguistic knowledge, they are not truly language independent. Rather, their success depends on typological properties of the languages they were first developed for. A more linguistically-informed (and thus more language independent) approach to n-gram models is the factored language model approach of Bilmes and Kirchhoff (2003). Factored language models address the problems of data-sparsity in morphologically complex languages by representing words as bundles of features, thus capturing dependencies between subword parts of adjacent words. A second example of subtle language dependence comes from Dasgupta and Ng (2007), who present an unsupervised morphological segmentation algorithm meant to be language-independent. Indeed, this work goes much further towards language independence than is the norm (see Section 3). It is tested against data from English, Bengali, Finnish and Turkish, a particularly good selection of"
W09-0106,P08-1108,0,0.016728,"s-linguistically applicable, I collected information about the languages studied in all 119 papers. Table 1 groups the papers by how many languages (or language pairs) they study. The three papers studying zero languages involved abstract, formal proofs regarding, e.g., grammar formalisms. 95 of the papers studied just one language or language pair. Languages or language pairs considered 0 1 2 3 4 5 12 13 Total Number of papers 3 95 13 3 2 1 1 1 119 Table 1: Number of languages/language pairs considered The two papers looking at the widest variety of languages were (Ganchev et al., 2008) and (Nivre and McDonald, 2008). Ganchev et al. (2008) explore whether better alignments lead to better translations, across 6 language pairs, in each direction (12 MT systems), collecting data from a variety of sources. Nivre and McDonald (2008) present an approach to dependency parsing which integrates graph-based and transition-based methods, and evaluate the result against the 13 datasets 4 http://wals.info (Haspelmath et al., 2008); Note that Japanese is treated as a language isolate and Chinese is the name for the genus including (among others) Mandarin and Cantonese. 5 The very interesting study by Snyder and Barzila"
W09-0106,P08-1084,0,0.0196142,"nd McDonald, 2008). Ganchev et al. (2008) explore whether better alignments lead to better translations, across 6 language pairs, in each direction (12 MT systems), collecting data from a variety of sources. Nivre and McDonald (2008) present an approach to dependency parsing which integrates graph-based and transition-based methods, and evaluate the result against the 13 datasets 4 http://wals.info (Haspelmath et al., 2008); Note that Japanese is treated as a language isolate and Chinese is the name for the genus including (among others) Mandarin and Cantonese. 5 The very interesting study by Snyder and Barzilay (2008) on multilingual approaches to morphological segmentation was difficult to classify. Their methodology involved jointly analyzing two languages at a time in order to produce morphological segmenters for each. Since the resulting systems were monolingual, the data from these studies are included in Table 2. 6 http://www.ethnologue.com/ethno docs/distribution.asp, accessed on 6 February 2009. ing our ideas against particular languages. 28 Language English German Dutch Danish Swedish Czech Russian Bulgarian Slovene Ukranian Portuguese Spanish French Hindi Arabic Hebrew Aramaic Chinese Japanese Tu"
W09-0106,P08-1112,0,0.0236275,"Missing"
W09-0106,N07-1020,0,\N,Missing
W09-0106,D08-1109,0,\N,Missing
W09-0106,D07-1096,0,\N,Missing
W09-0106,P08-1103,0,\N,Missing
W11-0707,D10-1100,0,0.0289409,"arsing and semantic role labeling. Such tasks involve recognizing information that is implicit in the linguistic signal but nonetheless part of its structure. Tasks such as named-entity recognition and word sense disambiguation are also close to the linguistic structure of the signal. Authority claims and alignment moves, on the other hand, are examples of communicative moves aimed at social positioning of a discussant within a group of participants, which may be specialized dialog acts but are referred to here as “social acts.” We distinguish social acts from “social events” as described in (Agarwal and Rambow, 2010): social events correspond to types of interactions among people, whereas a social act is associated with a fine-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of a turn. The primary value of this new data set is in facilitating computational modeling of a new task type, i.e. the identification of fine-grained social acts in linguistic interaction. While there has been some prior work on detecting agreements and disagree48 Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 48–57, c Portland, Oregon, 23 Ju"
W11-0707,J08-4004,0,0.0817579,"tor files as well as the files output by the reconciliation process. 3.5 Annotation Quality In complicated annotation tasks, such as those conducted in this work, establishing reliable ground truth is a fundamental challenge. The most popular approach to measuring annotation quality is via the surrogate of annotation consistency. This assumes that when annotators working independently arrive at the same decisions they have correctly carried out the task specified by the annotation guidelines. Several quantitative measures of annotator consistency have been proposed and debated over the years (Artstein and Poesio, 2008). We use the well-known Cohen’s kappa coefficient κ, which accounts for uneven class priors, so one may obtain a low agreement score even when a high percentage of tokens have the same label. We also report the percentage of instances on which the annotators agreed, A, which includes agreement on the absence of a particular label. When a set of instances have been labeled by more than two annotators, we compute the average of pairwise agreement. Scores for authority claim and alignment move agreement are presented in Tables 2 and 3.2 For 2 Institutional claims are exceedingly rare in our data,"
W11-0707,P04-1085,0,0.354248,"e-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of a turn. The primary value of this new data set is in facilitating computational modeling of a new task type, i.e. the identification of fine-grained social acts in linguistic interaction. While there has been some prior work on detecting agreements and disagree48 Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 48–57, c Portland, Oregon, 23 June 2011. 2011 Association for Computational Linguistics ments in multiparty discussions (Hillard et al., 2003; Galley et al., 2004), which is related to detecting positive/negative alignment moves, most previous work on authority bids has involved descriptive studies, e.g. (Galegher et al., 1998). Computational modeling of these phenomena and automatic detection will help with understanding effective argumentation strategies in online discussions and automatic identification of divisive or controversial discussions and online trolls. We believe that these tasks also provide an interesting arena in which to study linguistic feature engineering and feature selection. As with tasks such as sentiment analysis, a simple “bagof"
W11-0707,N03-2012,1,0.489074,"associated with a fine-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of a turn. The primary value of this new data set is in facilitating computational modeling of a new task type, i.e. the identification of fine-grained social acts in linguistic interaction. While there has been some prior work on detecting agreements and disagree48 Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 48–57, c Portland, Oregon, 23 June 2011. 2011 Association for Computational Linguistics ments in multiparty discussions (Hillard et al., 2003; Galley et al., 2004), which is related to detecting positive/negative alignment moves, most previous work on authority bids has involved descriptive studies, e.g. (Galegher et al., 1998). Computational modeling of these phenomena and automatic detection will help with understanding effective argumentation strategies in online discussions and automatic identification of divisive or controversial discussions and online trolls. We believe that these tasks also provide an interesting arena in which to study linguistic feature engineering and feature selection. As with tasks such as sentiment ana"
W11-0707,W11-0706,1,0.807499,"with authority claims and external variables such as user status and v-index, on the one hand, and the interaction between authority claims and alignment moves on the other. As an example of a social medium, Wikipedia is characterized by its task-orientation and by the fact that all of the interactants’ “identity work” with respect to their identity in the medium is captured in the database. This, in turn, causes the data set to be rich in the type of social acts we are investigating. The dataset was used for research in automatic detection of forum claims, as presented in a companion paper (Marin et al., 2011). That work focused on using lexical features, filtered through word lists obtained from domain experts and through datadriven methods, and extended with parse tree information. Automatic detection of other types of authority claims and of alignment moves is left for future research. We believe that, as social acts, authority claims and alignment moves are broadly recognized communication behaviors that play an important role in human interaction across a variety of contexts. However, because Wikipedia discussions are shaped by a set of well-defined, local communication norms which are closely"
W13-2710,P10-1010,0,0.0599253,"Missing"
W13-2710,P07-1009,0,0.145948,"Missing"
W13-2710,W02-1502,1,0.855766,"had bought fruit.’ [sna] (Toews, 2009:34) The annotations in IGT result from deep linguistic analysis and represent much effort on the part of field linguists. These rich annotations include the segmentation of the source line into morphemes, the glossing of those individual morphemes, and the translation into a language of broader communication. The IGT format was developed to compactly display this information to other linguists. Here, we propose to repurpose such data in the automatic development of further resources. The second resource we will be working with is the LinGO Grammar Matrix (Bender et al., 2002; 2010), an open source repository of implemented linguistic analyses. The Grammar Matrix pairs a core grammar, shared across all grammars it creates, with a series of libraries of analyses of cross-linguistically variable phenomena. Users access the system through a web-based questionnaire which elicits linguistic descriptions of languages and then outputs working HPSG (Pollard and Sag, 1994) grammar fragments compatible with DELPH-IN (www.delph-in.net) tools based on those descriptions. For present purposes, this system can be viewed as a function which maps simple descriptions of languages"
W13-2710,C10-1044,1,0.934067,"’s approximately 7,000 languages will become extinct by the year 2100. This is a crisis not only for the field of linguistics—on track to lose the majority of its primary data—but also a crisis for the social sciences more broadly as languages are a key piece of cultural heritage. The field of linguistics has responded with increased efforts to document endangered languages. Language documentation not only captures key linguistic data (both primary data and analytical facts) but also supports language revitalization efforts. It must include both primary data collection (as in Abney and Bird’s (2010) universal corpus) and analytical work elucidating the linguistic structures of each language. As such, the outputs of documentary linguistics are dictionaries, descriptive (prose) grammars as well as transcribed and translated texts (Woodbury, 2003). Traditionally, these outputs were printed artifacts, but the field of documentary linguistics has increasingly realized the benefits of producing digital artifacts as well (Nordhoff and Poggeman, 2012). Bender et al. (2012a) argue that the documentary value of electronic descriptive grammars can be significantly enhanced by pairing them with impl"
W13-2710,N07-1057,1,0.843043,"al is to produce working grammar fragments from IGT produced in documentary linguistics projects. However, in order to evaluate the performance of approaches to answering the high-level questions in the Grammar Matrix questionnaire, we need both IGT and goldstandard answers for a reasonably-sized sample of languages. We have constructed development and test data for this purpose on the basis of work done Other Related Work Our work is also situated with respect to attempts to automatically characterize typological proper1 The details of the algorithm and experimental results were reported in (Xia and Lewis, 2007). 2 76 http://sswl.railsplayground.net/, accessed 4/25/13 Sets of languages Range of testsuite sizes Median testsuite size Language families DEV 1 (n=10) 16–359 91 Indo-European (4), NigerCongo (2), Afro-Asiatic, Japanese, Nadahup, Sino-Tibetan DEV 2 (n=10) 11–229 87 Indo-European (3), Dravidian (2), Algic, Creole, Niger-Congo, Quechuan, Salishan TEST (n=11) 48–216 76 Indo-European (2), Afro-Asiatic, Austro-Asiatic, Austronesian, Arauan, Carib, Karvelian, N. Caucasian, Tai-Kadai, Isolate Table 1: Language families and testsuites sizes (in number of grammatical examples) by students in a class"
W13-2710,I08-1069,1,0.880884,"es. Both projects use the typological database WALS (Haspelmath et al., 2008), which has information about 192 different typological properties and about 2,678 different languages (though the matrix is very sparse). This approach is complementary to ours, and it remains an interesting question whether our results could be improved by bringing in information about other typological properties of the language (either extracted from the IGT or looked up in a typological database). Another strand of related work concerns the collection and curation of IGT, including the ODIN project (Lewis, 2006; Xia and Lewis, 2008), which harvests IGT from linguistics publications available over the web and TypeCraft (Beermann and Mihaylov, 2009), which facilitates the collaborative development of IGT annotations. TerraLing/SSWL2 (Syntactic Structures of the World’s Languages) has begun a database which combines both typological properties and IGT illustrating those properties, contributed by linguists. Finally, Beerman and Hellan (2011) represents another approach to inducing grammars from IGT, by bringing the hand-built linguistic knowledge sources closer together: On the one hand, their cross-linguistic grammar resou"
W13-2710,W09-0307,1,0.83312,"tion algorithmically, instead. Figure 1: Welsh IGT with alignment and projected syntactic structure bootstrapping NLP tools with initial seeds created by projecting syntactic information from resourcerich languages to RPLs through IGT. Projecting syntactic structures has two steps. First, the words in the language line and the translation line are aligned via the gloss line. Second, the translation line is parsed by a parser for the resource-rich language and the parse tree is then projected to the language line using word alignment and some heuristics as illustrated in Figure 1 (adapted from Xia and Lewis (2009)).1 Previous work has applied these projected trees to enhance the performance of statistical parsers (Georgi et al., 2012). Though the projected trees are noisy, they contain enough information for those tasks. The second goal of RiPLes is to use the automatically created resources to perform crosslingual study on a large number of languages to discover linguistic knowledge. For instance, Lewis and Xia (2008) showed that IGT data enriched with the projected syntactic structure could be used to determine the word order property of a language with a high accuracy (see §4). Naseem et al. (2012)"
W13-2710,C96-2120,0,0.0363363,"ted collections of grammatical and ungrammatical examples) and Grammar Matrix choices files. Later on in the class, the students extend the grammar fragments output by the customization system to handle a broader fragment of the language. Accordingly, the testsuites cover phenomena which go beyond the customization system. Testsuites for grammars, especially in their early stages of development, require examples that are simple (isolating the phenomena illustrated by the examples to the extent possible), built out of a small vocabulary, and include both grammatical and ungrammatical examples (Lehmann et al., 1996). The examples included in descriptive resources often don’t fit these requirements exactly. As a result, the data we are working with include examples invented by the students on the basis of the descriptive statements in their resources.3 In total, we have testsuites and associated choices files for 31 languages, spanning 17 language families (plus one creole and one language isolate). The most well-represented family is Indo-European, with nine languages. We used 20 languages, in two dev sets, for algorithm development (including manual error analysis), and saved 11 languages as a held-out"
W13-2710,I08-2093,1,0.879906,"lexical rules. We envision answering the questions regarding morphosyntactic features through an analysis of the grams that appear on the gloss line, with reference to the GOLD ontology (Farrar and Langendoen, 2003). The implementation of such systems in such a way that they are robust to potentially noisy data will undoubtedly be non-trivial. The contribution of this paper is the development of systems to handle one example each of the questions of types (i) and (ii), namely detecting major constituent word order and the underlying case system. For the first, we build directly on the work of Lewis and Xia (2008) (see §2.2). Our experiment can be viewed as an attempt to reproduce their results in the context of the specific view of word order possibilities developed in the Grammar Matrix. The second question (that of case systems) is in some ways more subtle, requiring not only analysis of IGT instances in isolation and aggregation of the results, but also identification of particular kinds of IGT instances and comparison across them. sion grammar fragments. These fragments are relatively modest, yet they relate linguistic strings to semantic representations (and vice versa) and are ready to be built"
W13-2710,P12-1066,0,0.0340909,"Xia and Lewis (2009)).1 Previous work has applied these projected trees to enhance the performance of statistical parsers (Georgi et al., 2012). Though the projected trees are noisy, they contain enough information for those tasks. The second goal of RiPLes is to use the automatically created resources to perform crosslingual study on a large number of languages to discover linguistic knowledge. For instance, Lewis and Xia (2008) showed that IGT data enriched with the projected syntactic structure could be used to determine the word order property of a language with a high accuracy (see §4). Naseem et al. (2012) use this type of information (in their case, drawn from the WALS database (Haspelmath et al., 2008)) to improve multilingual dependency parsing. Here, we build on this aspect of RiPLes and begin to extend it towards the wider range of linguistic phenomena and more detailed classification within phenomena required by the Grammar Matrix questionnaire. 2.3 3 Development and Test Data Our long-term goal is to produce working grammar fragments from IGT produced in documentary linguistics projects. However, in order to evaluate the performance of approaches to answering the high-level questions in"
W13-2710,C12-2037,1,\N,Missing
W14-2206,adolphs-etal-2008-fine,0,0.0121109,"lexicon (dictionary) from the CLRP and thus is effectively created on the basis of a much larger dataset. The GRAM choices files only contain verbs for which a case frame could be identified. If the projected tree was not interpretable by our extraction heuristics or if the example had no overt arguments, then the verb will not be extracted. The MOM choices files, on the 10 9 It is in this relative lack of constraint that BASELINE mostly clearly forms a baseline to improve upon. 11 The vast majority of the incorrect parses for the MOM There are methods for handling unknown lexical items (e.g. Adolphs et al., 2008) in more mature grammars of this type, but these are not applicable at this stage. 49 Choices file ORACLE BASELINE FF - AUTO - NONE FF - DEFAULT- GRAM FF - AUTO - GRAM MOM - DEFAULT- NONE MOM - AUTO - NONE # verb entries 900 3005 3005 739 739 1177 1177 # noun entries 4751 1719 1719 1724 1724 1719 1719 # det entries 0 240 240 240 240 240 240 # verb affixes 160 0 0 0 0 262 262 # noun affixes 24 0 0 0 0 0 0 Table 2: Amount of lexical information in each choices file choices file ORACLE BASELINE FF - AUTO - NONE FF - DEFAULT- GRAM FF - AUTO - GRAM MOM - DEFAULT- NONE MOM - AUTO - NONE Training Dat"
W14-2206,W02-1502,1,0.863376,"Missing"
W14-2206,C12-1016,1,0.828255,"learning precision grammar fragments from existing products of documentary linguistic work. A precision grammar is a grammar which encodes a sharp notion of grammaticality and furthermore relates strings to elaborate semantic representations. Such objects are of interest in the context of documentary linguistics because: (1) they are valuable tools in the exploration of linguistic hypotheses (especially regarding the interaction of various phenomena); (2) they facilitate the search for examples in corpora which are not yet understood; and (3) they can support the development of treebanks (see Bender et al., 2012a). However, they are expensive to build. The present work is carried out in the context of the AGGREGATION project,1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1 Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and"
W14-2206,W13-2710,1,0.864585,"y are expensive to build. The present work is carried out in the context of the AGGREGATION project,1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1 Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and Xia (2008) and Bender et al. (2013) apply similar methodologies to extract large scale properties for many languages, we focus on a case study of a single language, looking at both the large scale properties and the lexical details. This is important for two reasons: First, it gives us a chance to look indepth at the possible sources of difficulty in extracting the large scale properties. Second, while large-scale properties are undoubtedly important, the bulk of the information specified in a precision grammar is far more fine-grained. In this case study we apply the methodology of Bender et al. (2013) to extract general word"
W14-2206,P04-1041,0,0.04896,"Missing"
W14-2206,2000.iwpt-1.9,0,0.0170731,"ing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work 3 Methodology Our goal in this work is to automatically create choices files on the basis of IGT data. The choices files encode both general properties about the language we are trying to model as well as more specific information including lexical classes, lexical items within lexical classes and definitions of lexical rules. Lexical rule definitions can include both morphotactic information (ordering of"
W14-2206,W01-0713,0,0.018625,"line is parsed and the parse tree is projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some bas"
W14-2206,W09-2605,0,0.0202486,"h respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work 3 Methodology Our goal in this work is to automatically create choices files on the basis of IGT data. The choices files encode both general properties about the language we are trying to model as well as more specific information including lexical classes, lexical items within lexical classes and definitions of lexical rules. Lexical rule definitions can include both morphotactic information (ordering of affixes) as well as morphosyntactic information, though here our focus is on the former. Sample excerpts from a choices file are given in Fi"
W14-2206,P13-2055,1,0.899555,"Missing"
W14-2206,P06-1072,0,0.016691,"(Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars"
W14-2206,P06-1111,0,0.0152231,"line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different"
W14-2206,P09-1009,0,0.0178044,"The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to cont"
W14-2206,hockenmaier-steedman-2002-acquiring,0,0.0834147,"Missing"
W14-2206,J07-3004,0,0.0495946,"Missing"
W14-2206,P02-1017,0,0.0331782,"ed and the parse tree is projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM alg"
W14-2206,P04-1061,0,0.0131047,"projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced g"
W14-2206,N07-1057,1,0.922328,"le for such languages. A third line of research attempts to bootstrap NLP tools for resource-poor languages by taking advantage of IGT data and resources for resourcerich languages. The canonical form of an IGT instance includes a language line, a word-to-word or morpheme-to-morpheme gloss line, and a translation line (typically in a resource-rich language). The bootstrapping process starts with word alignment of the language line and translation line with the help of the gloss line. Then the translation line is parsed and the parse tree is projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006"
W14-2206,P98-1115,0,0.125171,"cs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work 3 Methodology Our goal in this work is to automatically create choices files on the basis of IGT data. The choices files encode both general properties about the language we are trying to model as well as more specific information including lexical classes, lexical items within lexical classes and definitions of lexical rules. Lexical rule definitions c"
W14-2206,I08-2093,1,0.918112,"al., 2012a). However, they are expensive to build. The present work is carried out in the context of the AGGREGATION project,1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1 Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and Xia (2008) and Bender et al. (2013) apply similar methodologies to extract large scale properties for many languages, we focus on a case study of a single language, looking at both the large scale properties and the lexical details. This is important for two reasons: First, it gives us a chance to look indepth at the possible sources of difficulty in extracting the large scale properties. Second, while large-scale properties are undoubtedly important, the bulk of the information specified in a precision grammar is far more fine-grained. In this case study we apply the methodology of Bender et al. (2013)"
W14-2206,C98-1111,0,\N,Missing
W15-0128,adolphs-etal-2008-fine,1,0.866719,"Missing"
W15-0128,P98-1013,0,0.191125,"tic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with better consistency (§4.2) and greater scalability (§4.3). The drawback to a grammar-based approach is that it cannot, in itself, include information t"
W15-0128,W13-2322,0,0.651957,"emantics either for parsing or for generation. 4 Benefits of Compositionality We are concerned here with the goal of designing task-independent semantic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with bett"
W15-0128,basile-etal-2012-developing,0,0.204582,"ture, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with better consistency (§4.2) and greater scalability (§4.3). The drawback to a grammar-based approach is that it cannot, in itself, include information that is not compositional, but as we will develop further in §6 below, it is possible to have the best of both worlds, adding non-compositional information as additional annotation layers over grammar-produced semantic representations. 4.1 Comprehensiv"
W15-0128,P11-1059,0,0.0201877,"ication, rather than a strong claim about lexical meaning. Another kind of non-compositional meaning layer is that which requires some sort of further computation over linguistic structure. This can be seen as purely monotonic addition of further constraints on underspecified meaning representations, but it is not compositional in the sense that it is never (strictly) constrained by grammatical structure. In this category, we find quantifier scope ambiguity resolution (e.g. Higgins and Sadock 2003), coreference resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of"
W15-0128,W97-1502,0,0.546635,"ee and ERS for each of these sentences are made available as the Redwoods Treebank; at the end of 2014, the current version of Redwoods encompasses gold-standard ERG analyses for 85,000 utterances (∼1.5 million tokens) of running text from half a dozen different genres and domains. In more detail, the task of annotation for a sentence consists of making binary decisions about the set of discriminants each of which partitions the parse forest into two: all of the analyses which employ the particular rule or lexical entry, and the rest of the analyses which do not. This method, originating with Carter 1997, enables the human annotator to rapidly discard analyses in order to isolate the intended analysis, or to conclude that the correct analysis is unavailable. As a reference point for speed of annotation using this method, an expert treebanker using the current ‘1214’ version of the ERG annotated 2400 sentences (37,200 words) from the Brown corpus in 1400 minutes, for an average rate of 1.7 sentences per minute.3 Annotations produced by this method of choosing among the candidate analyses licensed by a grammar will thus record those components of sentence meaning which are constrained by the gr"
W15-0128,P01-1019,1,0.580225,"nnotation. As Szabó (2013) points out, there are many different interpretations of the principle of compositionality in the literature. Since we are concerned with annotation, the issue is compositionality of meaning representations (rather than denotation, for instance). In order to ask which aspects of meaning are compositional, we provide the following working definition:1 1 In Szabó’s terms, our definition of compositionality is local, distributive, and language-bound and furthermore consistent with the rule-to-rule principle. It is also consistent with the notion of compositionality from Copestake et al. (2001) and implemented in the ERG, which furthermore adds the constraint that the function for determining meanings of complex expressions must be monotonic in the sense that it cannot remove or overwrite any information contributed by the constituents. 240 (1) A meaning system (or subsystem) is compositional if: • it consists of a finite (but possibly very large) number of arbitrary atomic symbol-meaning pairings; • it is possible to create larger symbol-meaning pairings by combining the atomic pairings through a finite set of rules; • the meaning of any non-atomic symbol-meaning pairing is a funct"
W15-0128,W11-2927,1,0.716728,"Missing"
W15-0128,flickinger-etal-2010-wikiwoods,1,0.898233,"Missing"
W15-0128,P82-1014,0,0.754755,"fits. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method. 1 Introduction Kate and Wong (2010) define ‘semantic parsing’ as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” At this level of generality, semantic parsing has been a cornerstone of NLU from its early days, including work seeking to support dialogue systems, database interfaces, or machine translation (Woods et al., 1972; Gawron et al., 1982; Alshawi, 1992, inter alios). What distinguishes most current work in semantic parsing from such earlier landmarks of old-school NLU is (a) the use of (highly) taskand domain-specific meaning representations (e.g. the RoboCup or GeoQuery formal language) and (b) a lack of emphasis on natural language syntax, i.e. a tacit expectation to map (more or less) directly from a linguistic surface form to an abstract representation of its meaning. This approach risks conflating a distinction that has long played an important role in the philosophy of language and theoretical linguistics (Quine, 1960;"
W15-0128,J03-1004,0,0.0864457,"Missing"
W15-0128,W07-1501,0,0.0215267,"entions. Some of the information we would like to see in such annotations is grammatically constrained, and we have argued that representations of those aspects of meaning are best built compositionally. However, there are further aspects of meaning which are closely tied to the linguistic signal but are not constrained by sentencelevel grammar (or only partially so constrained). We agree here with Basile et al. (2012) and Banarescu et al. (2013) that a single resource that combines multiple different types of semantic annotations, all applied to the same text, will be most valuable (see also Ide and Suderman 2007). However, just because some aspects of the desired representations cannot be created in a grammar-based fashion does not mean that what can be done with a grammar has no value. To get the best of both worlds, one should start from grammar-derived semantic annotations and then either add further layers of annotation (e.g. word sense, coreference) or, should larger paraphrase sets be desired, systematically simplify aspects of the grammar-derived representations, effectively ‘bleaching’ some of the contrasts. In moving from the current state of the art towards more comprehensive representations"
W15-0128,P10-5006,0,0.0583505,"al itself. We further argue that compositional construction of such sentence meaning representations affords better consistency, more comprehensiveness, greater scalability, and less duplication of effort for each new NLP application. For concreteness, we describe one well-tested grammar-based method for producing sentence meaning representations which is efficient for annotators, and which exhibits many of the above benefits. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method. 1 Introduction Kate and Wong (2010) define ‘semantic parsing’ as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” At this level of generality, semantic parsing has been a cornerstone of NLU from its early days, including work seeking to support dialogue systems, database interfaces, or machine translation (Woods et al., 1972; Gawron et al., 1982; Alshawi, 1992, inter alios). What distinguishes most current work in semantic parsing from such earlier landmarks of old-school NLU is (a) the use of (highly) taskand domain-"
W15-0128,kingsbury-palmer-2002-treebank,0,0.223431,"e goal of designing task-independent semantic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with better consistency (§4.2) and greater scalability (§4.3). The drawback to a grammar-based approach is that it cannot"
W15-0128,P98-1116,0,0.0854027,"equire the computation of semantics either for parsing or for generation. 4 Benefits of Compositionality We are concerned here with the goal of designing task-independent semantic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§"
W15-0128,I11-1028,1,0.818925,"to abstract away from task-irrelevant details of linguistic expression, task-independent representations only have that luxury when the variation is truly (sentence) meaning preserving. A task-independent semantic representation should capture exactly the meaning encoded in the linguistic signal itself, as it is is not possible to know, a priori, which parts of that sentence meaning will be critical to determining speaker meaning in any given application. 3 This rate is roughly consistent with an earlier experiment using the same Redwoods treebanking method where annotation times were noted: MacKinlay et al. (2011) report a somewhat slower mean annotation time by an expert annotator of 0.6 sentences per minute, but this difference can be attributed to the greater average sentence length (and hence increased number of discriminants to be determined) for that biomedical corpus: 23.4 tokens compared with 15.5 for the Brown data. 243 h h1 , h h1 , h4 :personh0 : 6i(ARG0 x 5 ), h6 :_no_qh0 : 6i(ARG0 x 5 , RSTR h7 , BODY h8 ), h2 :_eat_v_1h7 : 11i(ARG0 e3 , ARG1 x 5 , ARG2 i 9 ) { h1 =q h2 , h7 =q h4 } i h4 :_every_qh0 : 5i(ARG0 x 6 , RSTR h7 , BODY h5 ), h8 :_person_n_1h6 : 12i(ARG0 x 6 ), h2 :_fail_v_1h13 :"
W15-0128,P97-1013,0,0.0300129,"resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology supports compositionally built-up meanings for morphologically complex words. Semi-prod"
W15-0128,J94-1007,0,0.713929,"act representation of its meaning. This approach risks conflating a distinction that has long played an important role in the philosophy of language and theoretical linguistics (Quine, 1960; Grice, 1968), viz. the contrast between those aspects of meaning that are determined by the linguistic signal alone (called ‘timeless’, ‘conventional’, ‘standing’, or ‘sentence’ meaning), on the one hand, and aspects of meaning that are particular to a context of use (‘utterer’, ‘speaker’, or ‘occasion’ meaning, or ‘interpretation’), on the other hand. Relating this tradition to computational linguistics, Nerbonne (1994, p. 134) notes: Linguistic semantics does not furnish a characterization of the interpretation of utterances in use, which is what one finally needs for natural language understanding applications—rather, it (mostly) provides a characterization of conventional content, that part of meaning determined by linguistic form. Interpretation is not determined by 239 Proceedings of the 11th International Conference on Computational Semantics, pages 239–249, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistics form, however, nor by its derivative content. In order to interpre"
W15-0128,oepen-lonning-2006-discriminant,1,0.897769,"Missing"
W15-0128,W04-2319,0,0.0143789,"negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology supports compositionally built-up meanings for morphologically complex words. Semi-productive morphological processes and frozen or lexicalized complex forms complica"
W15-0128,W13-0122,0,0.0367255,"scope ambiguity resolution (e.g. Higgins and Sadock 2003), coreference resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology supports compositionally bu"
W15-0128,W08-0606,0,0.0207628,"As this level of processing concerns relationships both within and across sentences, it is clearly not compositional with respect to sentence grammar. We consider it an open question whether there are compositional processes at higher levels of structure that constrain these aspects of meaning in analogous ways, but we note that in presupposition processing at least, a notion of defeasibility is required (Asher and Lascarides, 2011). Finally, there are semantic annotations that attempt to capture what speakers are trying to do with their speech acts. This includes tasks like hedge detection (Vincze et al., 2008) and the annotation of social acts such as authority claims and alignment moves (Morgan et al., 2013) or the pursuit of power in dialogue (Swayamdipta and Rambow, 2012). While in some cases there are keywords that have a strong association with particular categories in these annotation schemes, these aspects of meaning are clearly not anchored in the structure of sentences but rather relate to the goals that speakers have in uttering sentences. Lacking a firm link to the structure of sentences, they do not appear to be compositional. We have seen in this (necessarily brief) section that existi"
W15-0128,W13-0505,0,0.01847,"tegory, we find quantifier scope ambiguity resolution (e.g. Higgins and Sadock 2003), coreference resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology sup"
W15-0128,C98-1112,0,\N,Missing
W15-0128,C98-1013,0,\N,Missing
W17-0106,W10-2211,0,0.0319097,"systems will pick this information up from the training data. 4 45 able in need of normalization than there is material that establishes correct practices. On the other hand, there are fewer individual authors, meaning that author identification can potentially lead to greater gains, and supplementary material like audio is likely to be available for at least some of the texts (because much endangered language text is transcribed from audio recordings). The first-pass IGT production shared task resembles earlier shared tasks on morphological analysis, most notably the Morpho Challenge series (Kurimo et al., 2010). It differs, however, in working with words in context (rather than word lists), and in going beyond segmentation of words into morphemes to associating morphemes with particular glosses. The presence of the translation line also provides a new source of information in producing the glosses, not available in previous shared tasks. Finally, the task of producing word-glosses is a novel one, with connections to low-resource machine translation. texts (both old and new) to a consistent format can solve many practical problems communities face. 5 Of the outputs provided by the first-pass IGT prod"
W17-0106,W14-3605,0,0.0207445,"rce scenarios. These contrast with the typical simulated low-resource scenarios in that the latter involve decisions about which data to keep, and this might not be representative of what an actual low-resource situation might be like. Each task has additional inherent research interest of its own, as detailed below. The “Grandma’s hatbox” shared task suite spans a range of speech processing technologies, including language identification, speaker identiThe orthographic regularization shared task builds on other work on orthographic regularization in widely spoken languages (see, for example (Mohit et al., 2014; Rozovskaya et al., 2015; Baldwin et al., 2015) on social media text and Dale and Kilgariff (2011) on text produced by language learners), but pushes the frontiers of work in this area in several ways: While this proposed shared task has much in common with these previous shared tasks, endangered language text normalization poses additional interesting problems. In languages like English or Arabic, there is usually a single, established orthography in which almost all users have formal schooling and extensive digital corpora in this orthography that establish “correct” practices. Endangered l"
W17-0106,W15-4319,0,0.0611856,"Missing"
W17-0106,W06-1421,0,0.386565,"l@alumni.ubc.ca, Shobhana.Chelliah@unt.edu, maxwell@umiacs.umd.edu Abstract ties if we are to actually reap the potential benefits of current research in the former for the latter. We propose that a particularly efficient and effective way to achieve this alignment of interest is through a set of “Shared Task Evaluation Challenges” (STECs) for the speech and language processing communities based on data already collected and annotated in language documentation efforts. STECs have been a primary driver of progress in natural language processing (NLP) and speech technology over several decades (Belz and Kilgarriff, 2006). A STEC involves standardized data for training (or otherwise developing) NLP/speech systems and then a held-out, also standardized, set of test data as well as implemented evaluation metrics for evaluating the systems submitted by the participating groups. This system is productive because the groups developing the algorithms benefit from independently curated data sets to test their systems on as well as independent evaluation of the systems, while the organizers of the shared task are able to focus effort on questions of interest to them without directly funding system development. Organiz"
W17-0106,brugman-russel-2004-annotating,0,0.0478615,"l employ two evaluation metrics: F1 scores averaged across all time segments in the corpus and all speakers in the corpus, respectively. As above, in the case of overlapped speech, both (all) speakers should be labeled. 3. Genre classification For a span of audio, identify which of a fixed inventory of genres, e.g., elicitation, monolog, LRL dialog, reading, or chanting, is present. This inventory will be provided by the organizers along with the training data. Training and test samples will be provided 42 as well as alignment can readily be encoded in formats readable by tools such as ELAN2 (Brugman and Russel, 2004), which has seen increasing adoption for endangered language research and which provides an easy visual interface to timealigned speech and language annotations. The techniques developed through the “Grandma’s hatbox” ensemble of shared tasks have potential benefit for field linguists, researchers in endangered languages, and language archivists. For those collecting data, these techniques can accelerate the process of transcription and alignment of speech data. They can also facilitate consistent metadata extraction from recordings, including language and speaker information, recording dates"
W17-0106,W15-3204,0,0.0147756,"contrast with the typical simulated low-resource scenarios in that the latter involve decisions about which data to keep, and this might not be representative of what an actual low-resource situation might be like. Each task has additional inherent research interest of its own, as detailed below. The “Grandma’s hatbox” shared task suite spans a range of speech processing technologies, including language identification, speaker identiThe orthographic regularization shared task builds on other work on orthographic regularization in widely spoken languages (see, for example (Mohit et al., 2014; Rozovskaya et al., 2015; Baldwin et al., 2015) on social media text and Dale and Kilgariff (2011) on text produced by language learners), but pushes the frontiers of work in this area in several ways: While this proposed shared task has much in common with these previous shared tasks, endangered language text normalization poses additional interesting problems. In languages like English or Arabic, there is usually a single, established orthography in which almost all users have formal schooling and extensive digital corpora in this orthography that establish “correct” practices. Endangered languages often only have"
W17-0106,W11-2838,0,0.0881554,"Missing"
W17-0106,E09-1099,1,0.689325,"hared tasks: Realism Whereas shared tasks in speech and NLP are often somewhat artificial, it is critical to our goals that our shared tasks closely model the actual computational needs of working linguists. It directly follows from this design principle that the software contributed by shared task participants should work off-the-shelf for stakeholders in documentary materials who are interested in using it later (e.g., linguists, speaker Accessibility of the shared task The shared tasks must have relatively low barriers to entry, in 1 There are some notable exceptions, including Xia et al’s (2009) work on language identification in IGT harvested from linguistics papers on the web (Xia et al., 2009), the Zero Resource Speech Challenge (Versteegh et al., 2015), and the recent BABEL (Harper, 2014) program. 40 now turn to the explanation of our three proposed shared tasks. order to encourage broad participation. This can be ensured by having the shared task organizers provide baseline systems which provide working, if not necessarily high-performing, solutions to the task. The baseline systems not only establish the basic feasibility of the tasks, but also provide a starting point for team"
W17-0110,W13-2710,1,0.115361,"g up some noise in the glossing, using the Map Gloss methodology of Lockwood (2016). In the following sections, we provide a description of the methodology and data sets we build on (§2), before laying out the methodology as developed for this paper (§3) and presenting the numerical results (§4). The primary contribution of our paper is in (§5), where we do an error analysis and relate our results to sources of bias. In this paper, we apply two methodologies of data enrichment to predict the case systems of languages from a diverse and complex data set. The methodologies are based on those of Bender et al. (2013), but we extend them to work with a new data format and apply them to a new dataset. In doing so, we explore the effects of noise and inconsistency on the proposed algorithms. Our analysis reveals assumptions in the previous work that do not hold up in less controlled data sets. 1 2 Introduction Background This section briefly overviews the previous work that we build on in this paper: methodology developed by the RiPLes and AGGREGATION projects to extract typological information from IGT (§2.1), the construction and enrichment of the ODIN data set (§2.2), and Map Gloss system for regularizing"
W17-0110,I08-2093,1,0.756023,"data sets—and we believe that this is true for other research purposes as well. In addition to bias in the words themselves used for annotation, additional bias may be introduced if a linguist is collecting data for a specific phenomenon. For example, if a language includes an unrepresentatively large set of intransitive or unergative verbs, the system might not have data with which to identify a nominativeaccusative system. This type of data set bias can be overcome by collecting a diverse set of data from a variety of sources that is large enough to overcome the biases of a particular set (Lewis and Xia, 2008). 5.5 Gold Standard A final contributor to the accuracy measurements was the state of the gold standard itself. We do not consider this a source of error, but rather an inevitability of working with low-resource languages and the very reason a system such as this is useful. Some of the languages classified as having a neutral case system (corresponding to ‘nocase’ in the choices files we use as our gold standard) might be better analyzed as in fact having (non-neutral) case systems. The classification in the gold standard, rather than being an assertion on the part of the grammar engineer who"
W17-0110,W14-2206,1,0.914079,"sses in IGT (§2.3). This work is situated within the AGGREGATION Project whose aim is to facilitate analysis of data collected by field linguists by automatically creating computational grammars on the basis of interlinear glossed text (IGT) and the LinGO Grammar Matrix customization system (Bender et al., 2010). Previous work by the AGGREGATION Project has looked at answering specific, highlevel typological questions for many different languages (Bender et al., 2013) as well as answering as much of the Grammar Matrix customization system’s questionnaire as possible for one specific language (Bender et al., 2014). In this paper, we revisit the case-related experiments done by Bender et al. (2013) in light of new systems for standardizing and enriching IGT and using a broader data set. Specifically, where Bender et al. considered only data from small collections of IGT created by students in a grammar engineering class (Bender, 2014), we will work with the larger and more diverse data sets available from ODIN version 2.1 (Xia et al., 2016). These data sets contain a great deal of noise in terms of inconsistent glossing conventions, missing glosses and data set bias. However, while these data sets are n"
W17-0110,bender-2014-language,1,0.855267,"he AGGREGATION Project has looked at answering specific, highlevel typological questions for many different languages (Bender et al., 2013) as well as answering as much of the Grammar Matrix customization system’s questionnaire as possible for one specific language (Bender et al., 2014). In this paper, we revisit the case-related experiments done by Bender et al. (2013) in light of new systems for standardizing and enriching IGT and using a broader data set. Specifically, where Bender et al. considered only data from small collections of IGT created by students in a grammar engineering class (Bender, 2014), we will work with the larger and more diverse data sets available from ODIN version 2.1 (Xia et al., 2016). These data sets contain a great deal of noise in terms of inconsistent glossing conventions, missing glosses and data set bias. However, while these data sets are noisier, 2.1 Inferring Case Systems from IGT Bender et al. (2013) began work on automatically creating precision grammars on the basis of IGT by using the annotations in IGT to extract largescale typological properties, specifically word order and case system. These typological properties were defined as expected by the Gramm"
W17-0110,H05-1066,0,0.0702967,"Missing"
W17-0110,N07-1057,1,0.703214,"case system based on whether or not certain grams were present (NOM , ACC , ERG , ABS ). The second method, called SAO , collected all of the grams on all intransitive subject (S), transitive subject (A) and transitive object (O) NPs, and then proceeded with the assumption that the most common gram in each role was a case gram. The grammatical role was determined by mapping parses of the English translation (using the Charniak parser (Charniak, 1997)) through the gloss line onto the source language line, according to the methodology set forth in the RiPLes project for resource-poor languages (Xia and Lewis, 2007). Because this method looks for the most frequent gram to identify the case marking gram, it is not essential that the gloss line follow a specific glossing convention, provided that the grams are consistent with each other. As a result, this method was expected to be suited to a wider range of data than GRAM. The data for this experiment comprised 31 languages from 17 different language families. Data was developed in a class in which students use descriptive resources to create testsuites and Grammar Matrix choices files (files that specify characteristics of the language so the Grammar Matr"
W17-0118,W02-1502,1,0.578084,"they are observed to take as input (attach to).7 The degree of overlap is a tuneable parameter of the system. The relationships between the affixes can then be expressed as a graph where groups of morphemes are nodes and input relations are directed edges. The graph can be used to specify the morphotactic component of a precision grammar. Suppose our corpus of IGT consists of the one sentence in (2). help provide hypothesized glosses for as-yet unanalyzed text. Precision grammars are expensive to build, requiring intensive work by highly trained grammar engineers. The Grammar Matrix project (Bender et al., 2002; Bender et al., 2010) aims to reduce the cost of creating precision grammars by producing a starter-kit that automatically creates small precision grammars on the basis of lexical and typological language profiles. The AGGREGATION Project (Bender et al., 2014) is further building on this by applying the methods of Lewis and Xia (2008) and Georgi (2016) to extract language profiles suitable for input into the Grammar Matrix grammar customization system from existing collections of IGT. In this paper, we focus on the morphotactic component of these systems. The Grammar Matrix’s morphotactic sys"
W17-0118,W14-2206,1,0.845609,"anguage documentation by producing summaries of collected data that identify apparent patterns in the data as well as exceptions to those patterns. On the one hand, this can help identify errors in glossing (where apparent exceptions are merely typos or orthographic idiosyncrasies). On the other hand, it can help the linguist understand and model patterns in the data, especially in cases where the phenomena in question have overlapping distributions. In this paper, we undertake a case study of verb classes in Abui [abz] in light of the morphotactic inference system of the AGGREGATION project (Bender et al., 2014; Wax, 2014; Zamaraeva, 2016). We begin with an overview of the phenomenon that is the focus of our case study (§2), formulate the problem and describe the steps to solve it (§3). Next we describe the tools and algorithms we apply to compare the output of the system (which summarizes what is found in the accessible corpus data) with a set of elicited judgments (§4). We PERSON PAT REC LOC GOAL BEN 1S 2S 1 PE 1 PI 2P 3 3I naanipirihadatanoonupu-/poro-/ruhodotoneenipirihedetenoooonuupuu-/pooruu-/roohoodootooneeeeniipiiriiheedeeteeDISTR Table 1: Abui person indexing paradigm An example of the pref"
W17-0118,I08-2093,1,0.767224,"mmar. Suppose our corpus of IGT consists of the one sentence in (2). help provide hypothesized glosses for as-yet unanalyzed text. Precision grammars are expensive to build, requiring intensive work by highly trained grammar engineers. The Grammar Matrix project (Bender et al., 2002; Bender et al., 2010) aims to reduce the cost of creating precision grammars by producing a starter-kit that automatically creates small precision grammars on the basis of lexical and typological language profiles. The AGGREGATION Project (Bender et al., 2014) is further building on this by applying the methods of Lewis and Xia (2008) and Georgi (2016) to extract language profiles suitable for input into the Grammar Matrix grammar customization system from existing collections of IGT. In this paper, we focus on the morphotactic component of these systems. The Grammar Matrix’s morphotactic system (O’Hara, 2008; Goodman, 2013) allows users to specify position classes and lexical rules. Position classes define the order of affixes with respect to each other (and the stem) while lexical rules pair affix forms with morphosyntactic or morphosemantic features. The grammars created on the basis of this information contain morpholo"
W17-0118,C12-1043,0,0.0201973,"Missing"
W17-0118,W16-2021,1,0.656685,"ng summaries of collected data that identify apparent patterns in the data as well as exceptions to those patterns. On the one hand, this can help identify errors in glossing (where apparent exceptions are merely typos or orthographic idiosyncrasies). On the other hand, it can help the linguist understand and model patterns in the data, especially in cases where the phenomena in question have overlapping distributions. In this paper, we undertake a case study of verb classes in Abui [abz] in light of the morphotactic inference system of the AGGREGATION project (Bender et al., 2014; Wax, 2014; Zamaraeva, 2016). We begin with an overview of the phenomenon that is the focus of our case study (§2), formulate the problem and describe the steps to solve it (§3). Next we describe the tools and algorithms we apply to compare the output of the system (which summarizes what is found in the accessible corpus data) with a set of elicited judgments (§4). We PERSON PAT REC LOC GOAL BEN 1S 2S 1 PE 1 PI 2P 3 3I naanipirihadatanoonupu-/poro-/ruhodotoneenipirihedetenoooonuupuu-/pooruu-/roohoodootooneeeeniipiiriiheedeeteeDISTR Table 1: Abui person indexing paradigm An example of the prefix attachment is given in (1)"
W17-5401,W17-5404,0,0.0118065,"tate University The breaker team from OSU also used a variety of strategies, classified as morphosyntactic, semantic, pragmatic, and world knowledge-based changes, to target hypothesized weaknesses in the sentiment analysis systems (Mahler et al., 2017). University of Melbourne, CNNs The builder team from University of Melbourne (which also participated as a breaker team), contributed two sentiment analysis systems consisting of convolutional neural networks. One CNN was trained on data labeled at the phrase level (PCNN), and the other was trained on data labeled at the sentence level (SCNN) (Li et al., 2017). University of Melbourne The breaker team from University of Melbourne opted to generate test minimal pairs automatically, borrowing from methods for generating adversarial examples in computer vision. They used reinforcement learning, optimizing on reversed labels, to identify tokens or phrases to be changed, and then applied a substitution method (Li et al., 2017). Some human supervision was used to ensure grammaticality and correct labeling of the sentences. Recursive Neural Tensor Network To supplement our submitted builder systems, we tested several additional sentiment analysis systems"
W17-5401,W17-5405,0,0.0607234,"Missing"
W17-5401,D11-1037,1,0.842048,"terested in the scientific questions addressed by testing a model’s ability to handle less frequent phenomena, it should be noted that any NLP system that is released is likely to be adversarially tested by users who want to break it for fun. This state of affairs has not gone unnoticed. On the one hand, there is work on creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K´ad´ar et al., 2016; Li et al., 2015) and what linguistic information is encoded in models’ produced representations (Adi et al., 2016; Ettinger et al., 2016). Our aim in organizing this workshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems’ linguistic capacities, and welcoming further related research contributions to stimulate additional di"
W17-5401,marelli-etal-2014-sick,0,0.0196372,"tific questions addressed by testing a model’s ability to handle less frequent phenomena, it should be noted that any NLP system that is released is likely to be adversarially tested by users who want to break it for fun. This state of affairs has not gone unnoticed. On the one hand, there is work on creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K´ad´ar et al., 2016; Li et al., 2015) and what linguistic information is encoded in models’ produced representations (Adi et al., 2016; Ettinger et al., 2016). Our aim in organizing this workshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems’ linguistic capacities, and welcoming further related research contributions to stimulate additional discussion. 3 Shared Task"
W17-5401,P05-1015,0,0.0378583,"h to draw items that could then be altered to create minimal pairs. Sentiment breakers were provided an additional set of 500 sentiment sentences, collected and annotated by the same method as that used for the 500 blind dev sentences for sentiment. QA-SRL breakers were provided an additional set of 814 items, collected and annotated by the same method as the blind dev items for QA-SRL. Shared Task Data 4.1 Blind Development Data Training Data For the sentiment training data, we used the Sentiment Treebank dataset from Socher et al. (2013), developed from the Rotten Tomatoes review dataset of Pang and Lee (2005).5 Each sentence in the dataset has a sentiment value between 0 and 1, as well as sentiment values for the phrases in its syntactic parse. In order to establish a binary labeling scheme at the sentence level, we mapped sentences in range (0, 0.4) to “negative” and sentences in range (0.6, 1.0) to “positive”. Neutral sentences—those with a sentiment value between 0.4 and 0.6—were removed. The sentiment training data had a total of 6921 sentences and 166738 phrases. Phrase-level sentiment labels were made available to participants as an optional resource. 4.4 Test Data The test data for evaluati"
W17-5401,de-marneffe-etal-2006-generating,0,0.0317309,"Missing"
W17-5401,D09-1085,0,0.0332705,". Even if one is uninterested in the scientific questions addressed by testing a model’s ability to handle less frequent phenomena, it should be noted that any NLP system that is released is likely to be adversarially tested by users who want to break it for fun. This state of affairs has not gone unnoticed. On the one hand, there is work on creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K´ad´ar et al., 2016; Li et al., 2015) and what linguistic information is encoded in models’ produced representations (Adi et al., 2016; Ettinger et al., 2016). Our aim in organizing this workshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems’ linguistic capacities, and welcoming further related research contributions to st"
W17-5401,W16-2524,1,0.872675,"n creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K´ad´ar et al., 2016; Li et al., 2015) and what linguistic information is encoded in models’ produced representations (Adi et al., 2016; Ettinger et al., 2016). Our aim in organizing this workshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems’ linguistic capacities, and welcoming further related research contributions to stimulate additional discussion. 3 Shared Task: Build It Break It, The Language Edition To address the issues identified above, we developed a shared task inspired by the Build It Break It Fix It Contest2 and adapted for application to NLP. The shared task proceeded in three phases: a building phase, a breaking phase, and a scoring phase: 1. In"
W17-5401,D12-1110,0,0.00934307,"Tensor Network To supplement our submitted builder systems, we tested several additional sentiment analysis systems on the breaker test set. The first of these was the Stanford Recursive Neural Tensor Network (RNTN) (Socher et al., 2013). This model is a recursive neural network-based sentiment classifier, composing words and phrases of input sentences based on binary branching syntactic structure, and using the composed representations as input features to softmax classifiers at every syntactic node. This model, rather than parameterizing the composition function by the words being composed (Socher et al., 2012), uses a single more powerful tensor-based composition function for composing each node of the syntactic tree. Team 4 The fourth sentiment breaker team did not submit a description paper, but the results from this team’s test set are reported below. 5.3 Builder Team: QA-SRL The organizers provided a QA-SRL system, as there were no external builder submissions for this task. The provided system was a logistic regression classifier, trained with 1-through-5 skipgrams with a maximum skip of 4. Potential answers were neighbors and neighbors-of-neighbors in a dependency parse of the sentence (Stanf"
W17-5401,D15-1076,0,0.217943,"ilders, we had a number of considerations. The task should be one that requires strong linguistic capabilities, so that in identifying the boundaries of the systems, breakers are encouraged to target linguistic phenomena key to increasing the robustness of language understanding. Additionally, we want the task to be without significant barrier to entry, to encourage builder participation. In the interest of balancing these considerations and testing the effectiveness of different tasks, we ran two tasks in parallel: sentiment analysis and question-answer driven semantic role labeling (QA-SRL; He et al., 2015). The sentiment task consists of standard sentiment analysis performed on movie reviews. In the QA-SRL task, the input is a sentence and a question related to one of the predicates in the sentence, and the output is a span of the sentence that answers the question. See Figure 1 for an example item. The task allows for testing semantic role labeling without the need for a pre-defined set of roles, or for annotators with significant training or linguistic expertise. 3.2 Breaking Satisfaction of requirement 1 is what we will refer to as “breaking” a system (note that this also applies if the syst"
W17-5401,D13-1170,0,0.0532153,"ropriate, the answer), leaving the question unaltered. For instance, breakers could generate the following item to be paired with the example in Figure 1: (2) 4.2 Blind dev data was provided for builders to submit initial predictions on, as produced by their systems. These predictions were made available for breakers, to be used as a reference when creating test minimal pairs. For sentiment, we collected an additional 500 sentences from a pool of Rotten Tomatoes reviews for movies in the years 20032005. For annotations, we used the same method of annotation via crowd-sourcing that was used by Socher et al. (2013). For QA-SRL, we extracted a set of 814 sentences from Wikipedia and annotated these by crowd-sourcing, following the method of He et al. (2015). Sent0 UCD finished the 2006 championship as Dublin champions, when they beat St Vincents in the final. 0 Ans UCD (unchanged) We might anticipate that the system would now predict the pronoun they as the answer to the question, without resolving to UCD.4 The sets of minimal pairs created by the breakers then constituted the test set of the shared task, which was sent to builders to generate predictions on for scoring. 4 4.3 Starter Data for Breakers A"
W17-5401,P14-1062,0,0.0169686,"Missing"
W17-5401,W17-5410,0,0.0911016,"Missing"
W19-0105,W14-2206,1,0.737517,"Missing"
W19-0105,P92-1026,0,0.684076,"-linguistic account of clausal complements in HPSG. Language-specific analyses include Ginzburg and Sag 2000 for English and Crysmann 2013 for German, among others. We could not di1svn://lemur.ling.washington.edu/shared/ matrix/trunk, revision 42067 (code and testsuites). 2http://matrix.delph-in.net/customize/ matrix.cgi 3The nonexhaustive list of cited libraries includes the ones with which we tested the interaction of our library. 4These are exemplified in §4.2 in (4)-(10). 5This contrasts with other versions of HPSG formalisms which separate immediate dominance from linear precedence (e.g. Engelkamp et al., 1992) as well as those that allow variable-arity rules, like Sag et al. 2003. 40 rectly incorporate them for two reasons. First, most of the existing theoretical analyses are not within the DELPH-IN JRF. Second, the literature mostly concerns itself with the level of clause complexity which is beyond the Grammar Matrix’s current scope. The analyses which informed us the most are actual grammar implementations within the DELPH-IN JRF and include Siegel et al. 2016 for Japanese and Flickinger 2000, 2011 for English. 3 its nominal object. Complementizers can be seen in both (3) and (1). (2) [senin sin"
W19-0105,I05-2035,1,0.687073,"ser to obtain a starter grammar customized from typological choices, making it possible to easily test the analysis on multiple languages, including systematic testing of each part of the analysis using artificially constructed languages (see §4.3). The Grammar Matrix consists of a web-based questionnaire and a back-end customization system. The input to the system is user answers to the typological questions (elicited in the questionnaire and serialized as a choices file) and the output is an implemented starter precision grammar. There are currently multiple libraries, including word order (Bender and Flickinger, 2005; Fokkens, 2014); person, number, gender, and case systems (Drellishak, 2009); tense, aspect, and mood (Poulson, 2011); argument optionality (Saleem and Bender, 2010); matrix yes/no questions, lexicon, (Bender and Flickinger, 2005); morphotactics (O’Hara, 2008; Goodman and Bender, 2010); nominalized clauses (Howell et al., 2018), and clausal modifiers (Howell and Zamaraeva, 2018).3 The contribution of this paper is the clausal complements library. In the context of the Grammar Matrix, our analysis consists of a new web questionnaire page with a set of choices describing clausal complements cro"
W19-0105,W02-1502,1,0.745601,"grammar fragment customized to their language of interest and ready for extension to broader coverage. In the meta-grammar engineering context, an analysis is a set of theoretically-grounded elements which comply with the requirements of the specific implementation framework and are used by the grammar engineering system so as to produce functioning grammars. In our case, the analysis is couched within Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) and Minimal Recursion Semantics (MRS; Copestake et al., 2005) and is implemented as an extension to the LinGO Grammar Matrix (Bender et al., 2002, 2010). Our main challenges lie in accounting for a large space of typological possibilities while at the same time integrating our analysis of clausal complements into a complex system which outputs streamlined grammars. We start by motivating the choice of the grammar engineering system and briefly summarizing the formal underpinnings associated with this choice (§2). We then present a concise summary of the typological literature on clausal complementation (§3), and describe our analysis and implementation in §4. The evaluation 39 Proceedings of the Society for Computation in Linguistics ("
W19-0105,W13-2710,1,0.876023,"Missing"
W19-0105,W02-1203,0,0.0129514,"ding itself. On the other hand, we are able to explore the interaction of our analyses with the existing analyses of other phenomena in order to validate both the existing and new libraries. Finally, because we contribute our library to the Grammar Matrix code base, it can in turn serve as part of the background infrastructure for future library development. Background 2.1 The Grammar Matrix We develop a cross-linguistic analysis of clausal complements as part of the LinGO Grammar Matrix2 (Bender et al., 2002, 2010). Other examples of multilingual grammar engineering projects include ParGram (Butt and King, 2002) and CoreGram (Müller, 2015). What is unique about the Grammar Matrix, however, is that it allows the user to obtain a starter grammar customized from typological choices, making it possible to easily test the analysis on multiple languages, including systematic testing of each part of the analysis using artificially constructed languages (see §4.3). The Grammar Matrix consists of a web-based questionnaire and a back-end customization system. The input to the system is user answers to the typological questions (elicited in the questionnaire and serialized as a choices file) and the output is a"
W19-0105,C18-1249,1,0.547257,"s to the typological questions (elicited in the questionnaire and serialized as a choices file) and the output is an implemented starter precision grammar. There are currently multiple libraries, including word order (Bender and Flickinger, 2005; Fokkens, 2014); person, number, gender, and case systems (Drellishak, 2009); tense, aspect, and mood (Poulson, 2011); argument optionality (Saleem and Bender, 2010); matrix yes/no questions, lexicon, (Bender and Flickinger, 2005); morphotactics (O’Hara, 2008; Goodman and Bender, 2010); nominalized clauses (Howell et al., 2018), and clausal modifiers (Howell and Zamaraeva, 2018).3 The contribution of this paper is the clausal complements library. In the context of the Grammar Matrix, our analysis consists of a new web questionnaire page with a set of choices describing clausal complements crosslinguistically, a set of new types available for the back-end customization logic, and revisions to the types already existing in the system. Building on the existing resource of the Grammar Matrix to develop our library has dual benefits: On the one hand, we can reuse existing implementations of phenomena that occur in embedded clauses and focus our attention primarily on 2.2"
W19-0105,E95-1045,0,0.315116,"ng predicate, in syntactic and typological literature. 12In future work, we will extend the lexical types available to provide for more valence patterns with clausal complements, such as Kim told me that Sandy left. 43 When the complementizer or the main verb participates in a word order that is different from other verbs, we posit an additional head-complement rule (HCR) and, in some cases, an additional headsubject rule (HSR). Then we constrain them (as well as all lexical types that use the HCRs and HSRs) for one or both Boolean features, and . While similar features have been used before (Keller 1995; Crysmann 2013; see also Siegel et al. 2016, 59), we incorporate them into a new customization logic so that sets of correct constraints are emitted automatically based on user choices. Examples (9)–(10) illustrate the general HCR and the additional one for extraposition that are emitted for one of many possible user-defined languages: an OVS language with extraposition. (9) (10) 2HCR1 6 6 6COMPS 6 6 6 6 6H-DTR 6 6 6 6NH-DTR 6 6 6ARGS 4 2HCR2 6 6 6COMPS 6 6 6 6 6H-DTR 6 6 6 6NH-DTR 6 6 6ARGS 4 1 2INIT 6 6SUBJ 6 6 6COMPS 4 3 hi h3i h 3 , 2 i 1 2INIT + 6 6SUBJ hi 6 6 6COMPS h 3 i 4 3 h 2 , 3 i"
W19-6005,W14-2206,1,0.065525,"d are heterogeneous. On the morphological side, an inference system identifies position and inflectional classes. On the syntactic side, an inference system uses syntactic generalizations to identify broad characteristics and defines lexical classes according to syntactic properties. The challenge we address here is in integrating the two. In this paper, we integrate a system that identifies morphological patterns in IGT data with one that predicts syntactic properties to define lexical classes that encode both morphological and syntactic features. We evaluate by replicating the case study of Bender et al. (2014), in which they automatically produced separate grammar fragments based on morphotactic and syntactic information and evaluated their system on a corpus of Chintang. We compare their results to our integrated system which includes both morphotactic and syntactic properties.1 Chintang (ISO639-3: ctn) is a Sino-Tibetan language spoken by ∼5000 people in Nepal (Bickel et al., 2010; Schikowski et al., 2015). Here we summarize the characteristics of the language that are directly relevant to this case study. The relative order of the verb and its core arguments (hereafter ‘word order’) in Chintang"
W19-6005,C12-1016,1,0.858237,"nce process.8 However, without constraints on their valence, their inclusion in the final grammar would result in spurious ambiguity. Therefore, we allow these verbs in the grammar, but change their orthography to contain 7 This follows the conventions of Minimal Recursion Semantics, see Copestake et al. 2005 and Flickinger et al. 2014. 8 As will be mentioned in §5.2.4, we also allow MOM to merge together verbs with no valence information into lexical classes with valence constraints, based on morphological patterns. Grammar spec oracle baseline mom-default-none ff-auto-gram integrated Origin Bender et al. 2012b Bender et al. 2014 Bender et al. 2014 Bender et al. 2014 This paper description Expert-constructed Full-form lexicon, free word order (WO), no case MOM-inferred lexicon, free WO, no case Full-form lexicon, V-final WO, erg-abs case system MOM-inferred lexicon, case frames, free WO, erg-abs Table 2: Grammars used in evaluation a non-Chintang string, so that they are effectively excluded from the working grammar unless they were merged into a verb class with valid valence constraints. So far we have described a methodology that extracts both syntactic and morphological information from IGT data"
W19-6005,C18-1249,1,0.819392,"osemantic information that the zero in the paradigm actually reflects. In future work, we will explore how to automatically posit such zero-marked rules, including how to make sure that their position classes are required, so that the grammar can properly differentiate ‘zero’ and ‘uninflected’. We plan to extend our syntactic inference algorithm to account for verbs with alternate or ‘quirky’ case frames. Another avenue that our error analysis shows as particularly promising is to handle complex clauses, as there are tools to model coordination (Drellishak and Bender, 2005) and subordination (Howell and Zamaraeva, 2018; Zamaraeva et al., 2019) in the Grammar Matrix framework. We have demonstrated the value of integrating morphological and syntactic inference for automatic grammar development. Although inferring these properties is most easily handled separately, we show that combining information about morphotactic and inflectional patterns with syntactic properties such as transitivity improves coverage. While this study looked at case and transitivity, the benefits of creating lexical classes that encode syntactic information alongside morphological information should generalize. This methodology can be e"
W19-6005,N07-1057,0,0.501056,"t this stage the project efforts are mostly experimental in nature and focus on evaluating grammars obtained in this way, there already have been successful collaborations with documentary linguists which in at least one case led The first two authors made equal contribution. ∗ ∗ University of Washington Department of Linguistics to insights into the language’s morphological patterns (Zamaraeva et al., 2017). The IGT data format, widely used by linguists, is well suited to inference tasks because it features detailed morpheme-by-morpheme annotation and translations to high-resource languages (Xia and Lewis, 2007; Bender et al., 2013). However, the inference processes required are heterogeneous. On the morphological side, an inference system identifies position and inflectional classes. On the syntactic side, an inference system uses syntactic generalizations to identify broad characteristics and defines lexical classes according to syntactic properties. The challenge we address here is in integrating the two. In this paper, we integrate a system that identifies morphological patterns in IGT data with one that predicts syntactic properties to define lexical classes that encode both morphological and s"
W19-6005,W16-2021,1,0.896084,"Missing"
W19-6005,W17-0118,1,0.887631,"language. To partially automate this process, the AGGREGATION project takes advantage of the stored analyses in the Grammar Matrix and the linguistic information encoded in Interlinear Glossed Text (IGT). While at this stage the project efforts are mostly experimental in nature and focus on evaluating grammars obtained in this way, there already have been successful collaborations with documentary linguists which in at least one case led The first two authors made equal contribution. ∗ ∗ University of Washington Department of Linguistics to insights into the language’s morphological patterns (Zamaraeva et al., 2017). The IGT data format, widely used by linguists, is well suited to inference tasks because it features detailed morpheme-by-morpheme annotation and translations to high-resource languages (Xia and Lewis, 2007; Bender et al., 2013). However, the inference processes required are heterogeneous. On the morphological side, an inference system identifies position and inflectional classes. On the syntactic side, an inference system uses syntactic generalizations to identify broad characteristics and defines lexical classes according to syntactic properties. The challenge we address here is in integra"
W19-6005,W19-0105,1,0.880491,"Missing"
xia-etal-2014-enriching,xia-etal-2010-problems,1,\N,Missing
xia-etal-2014-enriching,N01-1026,0,\N,Missing
xia-etal-2014-enriching,P13-2055,1,\N,Missing
xia-etal-2014-enriching,N07-1057,1,\N,Missing
xia-etal-2014-enriching,J94-4004,0,\N,Missing
xia-etal-2014-enriching,W13-2710,1,\N,Missing
xia-etal-2014-enriching,I08-1069,1,\N,Missing
xia-etal-2014-enriching,I08-2093,1,\N,Missing
Y11-1025,W02-1502,1,0.874761,"eaning, but also those that exist only to express generalizations over their subtypes. We use these techniques to explore the degree of redundancy in a range of DELPH - IN1 grammars, including the two grammars of Wambaya (Bender, 2010), the BURGER grammar of Bulgarian (Osenova, 2010), the ManGO grammar2 of Mandarin Chinese, all built with the LinGO 1 2 Copyright 2011 by Antske Fokkens, Yi Zhang and Emily M. Bender http://www.delph-in.net/ http://wiki.delph-in.net/moin/MandarinGrammarOnline 25th Pacific Asia Conference on Language, Information and Computation, pages 236–244 236 Grammar Matrix (Bender et al., 2002; Bender et al., 2010), and two much larger grammars, the English Resource Grammar (Flickinger, 2000) and German Grammar (M¨uller and Kasper, 2000; Crysmann, 2005). This paper is structured as follows: First, we describe the overall structure of the grammars under consideration. This section is followed by an overview of the first approach under examination: removing superfluous types from the grammar. Section 4 provides the details of our second investigation of relevant types: maximally reduced computationally equivalent grammars. The next section presents our quantitative results and their"
