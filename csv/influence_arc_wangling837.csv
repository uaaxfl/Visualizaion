2010.iwslt-evaluation.9,N03-1017,0,0.0100386,"ents models than the IBM M4 alignment model, using the posterior distribution over alignments instead of the single best alignment. Moreover, we filtered phrase pairs from being extracted based on punctuation and phrase length differences. This paper is organized as follows: in Section 2 we will present the methods we used to improve the phrase extraction algorithm and Section 3 will describe the corpus used and data preparation. In Section 4, we will report the experimental results and, in Section 5, we will conclude the paper. 2. Phrase Extraction The most common phrase extraction algorithm [1] uses word alignment information to constraint the possible phrases that can be extracted. Given a word alignment, all phrase pairs 0 1 2 3 4 5 0 1 2 z p p p x p p p p p p p p p 3 p p z p p p p 4 p p p 5 p p p 6 p p p 7 p but p then p mr. z z z p p baldwin p p z p said p p p p p z: ce , m ba av en de´ : . pe ld ait su cla wi nd ite re´ n an t Figure 2: Example of a word alignment suffering from the garbage collector effect. consistent with that word alignment are extracted from the parallel sentence (a phrase pair is consistent with a word alignment if all words in one language contained in th"
2010.iwslt-evaluation.9,2006.amta-papers.11,0,0.0210516,"if there are too many incorrect alignment points forming a cluster, the correct phrases cannot be extracted without the spurious words, leading to missing words/phrases from the phrase table. In addition, unaligned words act as wild cards that can be aligned to every word in the neighborhood, thus increasing the size of the phrase table. Another undesirable effect of unaligned words is that they will only appear (in the phrase table) in the context of the surrounding words. Moreover, the spurious phrase pairs will change both the phrase probability and the lexical weight feature. The work by [2] conclude that the factor with most impact was the degradation of the translation probabilities due to noisy phrase pairs. Figure 1 shows the phrase tables extracted from two word alignments for the same sentence pair. These alignments only differ in one point: y-b. However, the nonexistence of this point in the second word alignment, results in the removal of the phrase y-b in the second phrase table. Hence we would not be able to translate y as b except in the contexts shown in that table. Figure 2 shows an example of a word alignment where a rare source word is aligned to a group of target"
2010.iwslt-evaluation.9,J10-3007,1,0.871261,"z 0 w 1 x y • 2 w a b c z Foreign x xy x xy xyz z yz z yz Source a a ab ab abc c c bc bc Points 0-0 0-0 0-0 0-0 0-0 2-2 2-2 2-2 2-2 2-2 Figure 1: Machine Translation phrase extraction from word alignments example. which leads that the word baldwin cannot be extracted without the incorrect surrounding context. This will make the pair baldwin, baldwin unavailable outside the given context. These led us to the work described in the following sections. 2.1. Constrained Alignments Rather than using the IBM M4 alignment model, we use the posterior regularization framework with symmetric constraints [3], which produces better overall results. This constraint takes into account that if a certain unit a is aligned to unit b in the source to target alignment model, b should also be aligned to a in the target to source alignment model. We also made some tests with bijective constrains [3], which had better overall results compared to the IBM M4 model, but that were not as good as the symmetric constraints. In both of these alignments we use a threshold of 0.4 for accepting an alignment point and we train them using the conditions described in [3]. For each model, we initialize the translation ta"
2010.iwslt-evaluation.9,D09-1106,0,0.021721,"from the original paper for an explanation of the meaning of these parameters). 2.2. Weighted Alignment Matrixes We use information about the posterior distributions in the alignments, rather than the single best alignment to obtain better results. Given the posterior distribution for an alignment link, we use the soft union heuristic (the average of each link) to obtain a symmetrized alignment with link posteriors. Given these alignments links, we calculate the phrase translation probability and the link probability using the approach proposed for weighted alignment matrixes, as described in [4]. We only accept a phrase if its phrase posterior probability is above a particular threshold. For both the BTEC and DIALOG corpora we use a threshold of 0.1. We set the values based on the results of the original paper and leave the tuning of this particular threshold as future work, as lowering it does not always yields better results. 2.3. Phrase Pair Filtering For each phrase pair that is extracted from a sentence pair, we apply an acceptor to decide whether that phrase pair is accepted or not. We build special acceptors that deal with punctuation. The idea is that punctuation normally tra"
2010.iwslt-evaluation.9,W09-0439,0,0.0139838,"er than 4 are cut from the translation table, we could not perform such a translation. 4.1. Automatic Evaluation Results We observe from the preliminary automatic evaluation results that our main problem in this evaluation was the fact we only used BLEU as our tuning and evaluation metric, rather than a combination of metrics, which limited the quality of our results. In fact, in many instances our system yielded better BLEU scores in the evaluation, but the overall score was worse. It is also important notice that the tuning process we used does not use any stabilization methods described in [6], which states that there is a high variance between different runs of the tuning process for the same translation model. Thus, there is also the possibility that we obtained a bad set of weights for one of our translation models. In the DIALOG task, we performed specially well in the IWSLT09 testset, probably due to the fact that this set bears more similarity to the data set we used for tuning. In the BTEC task, we were ranked worse. We think that one of the reasons was, as stated above, that we obtained a worse set of weights during the tuning procedure. 5. Conclusions We have described the"
2010.iwslt-evaluation.9,P02-1040,0,\N,Missing
2010.iwslt-papers.14,N03-1017,0,0.631324,"vious approaches fit in this algorithm, compare several of them and, in addition, we propose alternative heuristics, showing their impact on the final translation results. Considering two different test scenarios from the IWSLT 2010 competition (BTEC, Fr-En and DIALOG, Cn-En), we have obtained an improvement in the results of 2.4 and 2.8 BLEU points, respectively. 1. Introduction Modern statistical translation models depend crucially on the minimal translation units that are available during decoding. However, the evolution of statistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9],"
2010.iwslt-papers.14,N04-1035,0,0.0312417,"ithm, compare several of them and, in addition, we propose alternative heuristics, showing their impact on the final translation results. Considering two different test scenarios from the IWSLT 2010 competition (BTEC, Fr-En and DIALOG, Cn-En), we have obtained an improvement in the results of 2.4 and 2.8 BLEU points, respectively. 1. Introduction Modern statistical translation models depend crucially on the minimal translation units that are available during decoding. However, the evolution of statistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over t"
2010.iwslt-papers.14,P08-1112,1,0.927508,"tistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strat"
2010.iwslt-papers.14,D07-1006,0,0.0135809,"tistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strat"
2010.iwslt-papers.14,P07-1003,0,0.0667469,", 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation e"
2010.iwslt-papers.14,D09-1106,0,0.495182,"good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare severa"
2010.iwslt-papers.14,W05-0827,0,0.0165105,"ion process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and"
2010.iwslt-papers.14,N07-2053,0,0.0158559,", the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and show their impact on the final translation results."
2010.iwslt-papers.14,P08-1010,0,0.0128993,", the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and show their impact on the final translation results."
2010.iwslt-papers.14,D07-1103,0,0.104809,"[3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and show their impact on the final translation results. Our experiments ran on two differe"
2010.iwslt-papers.14,W02-1018,0,0.0354354,"table, the translation process can be broken down into three steps: segment the source sentence into phrases, translate each source phrase into a target phrase, and reorder the target phrases. The quality of Pb systems depend directly on the quality of their phrase-table and therefore, phrase extraction is always a fundamental step in Statistical MT. Extracting all possible phrase-pairs is not a valid option since an exponential number of phrases would be extracted, most of which linguistically irrelevant. Learning a phrase table directly from a bilingual corpus has also been tried previously [15, 16], but these methods failed to compete with heuristic methods that we will describe briefly in the following section. In [16] some problems that result from learning a phrase table directly from data using the EM algorithm are identified. In general terms, phrase pairs with different segmentations (potentially all equally good) compete for probability mass (as opposed to learning bilingual lexicons where the competition is only based on bilingual word pairs). The most common phrase extraction algorithm [3] 313 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, D"
2010.iwslt-papers.14,W06-3105,0,0.0713912,"table, the translation process can be broken down into three steps: segment the source sentence into phrases, translate each source phrase into a target phrase, and reorder the target phrases. The quality of Pb systems depend directly on the quality of their phrase-table and therefore, phrase extraction is always a fundamental step in Statistical MT. Extracting all possible phrase-pairs is not a valid option since an exponential number of phrases would be extracted, most of which linguistically irrelevant. Learning a phrase table directly from a bilingual corpus has also been tried previously [15, 16], but these methods failed to compete with heuristic methods that we will describe briefly in the following section. In [16] some problems that result from learning a phrase table directly from data using the EM algorithm are identified. In general terms, phrase pairs with different segmentations (potentially all equally good) compete for probability mass (as opposed to learning bilingual lexicons where the competition is only based on bilingual word pairs). The most common phrase extraction algorithm [3] 313 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, D"
2010.iwslt-papers.14,2006.amta-papers.11,0,0.0529946,"re are too many incorrect alignment points forming a cluster, the correct phrases cannot be extracted without the spurious words, leading to missing words/phrases from the phrase table. In addition, unaligned words act as wild cards that can be aligned to every word in the neighborhood, thus increasing the size of the phrase table. Another undesirable effect of unaligned words is that they will only appear (in the phrase table) in the context of the surrounding words. Moreover, the spurious phrase pairs will change both the phrase probability as well as the lexical weight feature. The work by [17] concludes that the factor with most impact was the degradation of the translation probabilities due to noisy phrase pairs. Figure 1 shows the phrase tables extracted from two word alignments for the same sentence pair. These alignments only differ in one point: y-b. However, the nonexistence of this point in the second word alignment results in the removal of the phrase y-b in the second phrase table. Hence we would not be able to translate y as b except in the contexts shown in that table. Figure 2 shows an example of a word alignment where a rare source word is aligned to a group of target"
2010.iwslt-papers.14,2005.iwslt-1.8,0,0.0239954,"ery different alignments. For phrase extraction we 0 1 2 3 4 5 0 z p p p p p 1 p x p p p p 2 p p 3 p p 4 p p 5 p p 6 p p 7 p but p then z p p p p p mr. z z z p p baldwin p p p p p z p said p p p p p z: ce , m ba av en de´ : . pe ld ait su cla wi nd ite re´ n an t Figure 2: Example of a word alignment suffering from the garbage collector effect. are interested in a single alignment per sentence so the two directional alignments are combined to form a single alignment. Several approaches have been proposed to symmetrize these word alignments. The most commonly used is called grow diagonal final [18]. It starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The resulting alignment has high recall relative to the intersection and only slightly lower recall than the union. Other common approaches include the intersection and union of the directional alignments. However, the exact relationship between word alignment quality and machine translation quality is not straightforward. Despite recent work showing that better word alignments can result in better MT [6, 19], namely by reducing the garbag"
2010.iwslt-papers.14,J10-3007,1,0.702323,"sed is called grow diagonal final [18]. It starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The resulting alignment has high recall relative to the intersection and only slightly lower recall than the union. Other common approaches include the intersection and union of the directional alignments. However, the exact relationship between word alignment quality and machine translation quality is not straightforward. Despite recent work showing that better word alignments can result in better MT [6, 19], namely by reducing the garbage collector effect and hence increasing the phrase table coverage, there is evidence that the two are only indirectly connected [17, 20]. There are several reasons that explain these facts. First during the pipeline, one keeps committing to the best options available: best word alignment, best symmetrized alignment, etc. Second, the symmetrization heuristics tend to obfuscate the quality of the resulting alignment and the original ones by creating alignments close to the diagonal. Third, current phrase extraction weights each phrase independently of the quality o"
2010.iwslt-papers.14,2006.iwslt-papers.7,0,0.0390249,"two sets of aligned points. The resulting alignment has high recall relative to the intersection and only slightly lower recall than the union. Other common approaches include the intersection and union of the directional alignments. However, the exact relationship between word alignment quality and machine translation quality is not straightforward. Despite recent work showing that better word alignments can result in better MT [6, 19], namely by reducing the garbage collector effect and hence increasing the phrase table coverage, there is evidence that the two are only indirectly connected [17, 20]. There are several reasons that explain these facts. First during the pipeline, one keeps committing to the best options available: best word alignment, best symmetrized alignment, etc. Second, the symmetrization heuristics tend to obfuscate the quality of the resulting alignment and the original ones by creating alignments close to the diagonal. Third, current phrase extraction weights each phrase independently of the quality of the underlying align314 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 ments (all phrases are given t"
2010.iwslt-papers.14,2008.amta-papers.18,0,0.562826,"hop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 ments (all phrases are given the weight of 1). Several approaches have been proposed to mitigate this problem. Soft Union [9] uses the knowledge about the posterior distributions of each directional model. It includes a point in the final alignment if the average of the posteriors under the two models for that point is above a certain threshold. However, this approach still produces a single alignment to be used by the phrase extraction. An alternative is to not commit to any particular alignment, but either use n-best lists [21] or the posterior distribution over the alignments to extract phrases [7, 10]. Both of these approaches increase the coverage of the phrase table. Moreover, by using the posterior weight of each phrase as its score instead of using 1 for all sentences, one can better estimate the phrase probabilities. The method computeGlobalPhraseStats calculates features, such as the phrase translation probability and the lexical probability based on the counts collected for each occurrence of each phrase pair. This is also the point where different types of smoothing for the phrase table can be applied (for"
2010.iwslt-papers.14,W06-1607,0,0.0287577,"ese approaches increase the coverage of the phrase table. Moreover, by using the posterior weight of each phrase as its score instead of using 1 for all sentences, one can better estimate the phrase probabilities. The method computeGlobalPhraseStats calculates features, such as the phrase translation probability and the lexical probability based on the counts collected for each occurrence of each phrase pair. This is also the point where different types of smoothing for the phrase table can be applied (for instance, we can implement at this step the Knesser-Ney smoothing for bilingual phrases [22]). In this paper, we use as features both the phrase probability and lexical probability in the general phrase extraction [3] [10]. Algorithm 1 General Phrase Table Extraction Require: Bilingual Corpus Require: MaximumPhraseSize - max for each sentence pair (s, t) in Corpus do extractedPhrasePairs = extractPhrasePairs(s, t, max) for each phrase pair p in extractedPhrasePairs do phraseTable.add(p) end for end for computeGlobalPhraseStats pruneGlobalPhraseStats savePhraseTable Algorithm 2 Extract Phrase Pairs Require: Bilingual sentence s fl = s.foreignLen sl = s.sourceLen extractedPhrasePairs ="
2010.iwslt-papers.14,P02-1040,0,0.0797175,", “？”, “！”) with the respective latin punctuation. Furthermore, we leave the segmentation of Chinese characters as the one given in the corpus. At the end of the pipeline, we detokenize and recase the translation, so that the evaluation is performed according to the IWLST task. The recasing is done using a maximum entropy-based capitalization system [23]. For all experiments we use the Moses decoder (http://www.statmt. org/moses/), and before decoding the test set, we tune the weights of the phrase table using Minimum Error Rate Training (MERT). The results are evaluated using the BLEU metric [24]. We also follow the baseline described in the workshop above, which creates directional alignments produced by GIZA++ using the IBM M4 model (the train conditions are the same as the default moses training scripts: 5 iterations of IBM M1, 5 iterations of HMM and 5 iterations of IBM M4). Also, we combine these alignments using the grow-diagonalfinal heuristic, and use the default phrase extraction algorithm [3]. We will refer to the baseline as “Moses IBM M4”. Moreover, we also compare three different alignment models: the regular HMM [25] (referred to as “HMM”), the same model, but using the"
2010.iwslt-papers.14,C96-2141,0,0.331127,"MERT). The results are evaluated using the BLEU metric [24]. We also follow the baseline described in the workshop above, which creates directional alignments produced by GIZA++ using the IBM M4 model (the train conditions are the same as the default moses training scripts: 5 iterations of IBM M1, 5 iterations of HMM and 5 iterations of IBM M4). Also, we combine these alignments using the grow-diagonalfinal heuristic, and use the default phrase extraction algorithm [3]. We will refer to the baseline as “Moses IBM M4”. Moreover, we also compare three different alignment models: the regular HMM [25] (referred to as “HMM”), the same model, but using the posterior regularization framework with bijective constraints (refered to as “BHMM”), and with symmetry constraints (refered to as “SHMM”) [19]. The latter constraint takes into account that if a certain unit a is aligned to unit b in the source to target alignment model, b should also be aligned to a in the target to source alignment model. In these alignments we use a threshold of 0.4 for accepting an alignment point. We trained these models using the conditions described in [19]. For each model, we initialize the translation tables with"
2010.iwslt-papers.14,E03-1009,0,0.0161308,"slation for the word “disait”. The phrases extracted with the default extraction method only contain the following right context “disait six heures” and, therefore, this context does not allow the translation of the sentence above. The words that are left unknown are due to the threshold being too high. We also add new features and new acceptors to address some observed problems. In the first experiment we add a part of speech feature that calculates the phrase probabilities and the lexical probabilities based on the part of speech of each word. We use the unsupervised POS system described by [26] (using the source code available at the authors website), and cluster the words into 50 different groups. We then tag each word with the attained tag. The intuition behind this feature is that a given word can be translated differently if it is being used as a noun or as a verb, and different POS sequences tend to generate different translations even if the words are the same. However, this approach produces worse results than the baseline. Two possible reasons are the use of an unsupervised system, whose accuracy is not very high. Furthermore, this system does not allows word ambiguity (the"
2010.iwslt-papers.14,2005.mtsummit-papers.36,0,0.0164424,"2) and 4 (length-diff4). Figure 3 shows the distribution of length difference of the phrase pairs in the phrase table, and Figure 4 shows the distributions of length difference of the phrase pairs actually used by the decoder. Finally, Table 6 shows the percentual reduction in the phrase table size using each heuristic. As expected, although there are a lot of phrase pairs with large length differences, the decoder only uses 1 sentence 317 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 70000 is similar to the approach proposed in [27] which only includes phrases of length larger than a given threshold if they cannot be realized by using smaller phrases. 60798 60000 50000 35945 40000 30000 28897 20000 15188 10000 5394 1725 453 5 6 0 0 1 2 3 4 Figure 3: Distribution of phrase-pairs in the phrase table by the difference between lenghts for source and target phrases for the BTEC corpus. with length difference of 6, as well as 4 phrase-pairs with a difference of 5. However, looking at Table 5 we see that both heuristics (length-diff2 and length-diff4) hurt the performance. Table 7 show the number of times phrase-pairs with a gi"
2010.iwslt-papers.14,J93-2003,0,\N,Missing
2010.iwslt-papers.14,P01-1067,0,\N,Missing
2010.iwslt-papers.14,J04-4002,0,\N,Missing
2010.iwslt-papers.14,J07-2003,0,\N,Missing
2010.iwslt-papers.14,P08-1000,0,\N,Missing
2011.eamt-1.19,J84-3009,0,0.408033,"Missing"
2011.eamt-1.19,almeida-etal-2010-bigorna,0,0.0592641,"Missing"
2011.eamt-1.19,D09-1030,0,0.0172107,"the EN→EP/BP/BP2EP models on the EN-EP test set 8 system, because it gave us several lexical / morphological entries used to resolve differences between BP to EP. We have used this to extract the lexical rules, where one entity in BP is written differently in EP. As the translation lexicon created automatically has a margin of error, it is necessary to manually filter spurious entries. If the language pair of varieties and/or dialects is well covered by the workers of crowdsourcing systems such as Amazon’s Mechanical turk (AMT), it is a viable option to avoid manually filtering these entries (Callison-Burch, 2009). Another possible improvement to extraction of multiword lexical contrastive pairs algorithm is the inclusion of the translational entropy to help to identify idiomatic multiword expressions (Moir´on and Tiedemann, 2006). We have also got encouraging results (about 5 BLEU points) when we evaluated the BP2EP output against manually created EP corpora. However, the system still needs some improvements to handle particular cases. The incomplete lexical pair coverage is one of the reasons. But there are exceptions to the rules that are hard to capture by rules. Also, the usage of handmade rules t"
2011.eamt-1.19,2001.mtsummit-papers.14,0,0.0386023,"Missing"
2011.eamt-1.19,A00-1002,0,0.726678,"Missing"
2011.eamt-1.19,N03-1017,0,0.0234167,"probability and scores, e.g.: “Eu achei” → “Eu pensei”, “Eu achei” → “Pensei”, “Eu achei” → “Pensava”. • Number based Removal: Numbers are generally translated one to one e.g.: “609 bilh˜oes em 2008” → “609 bili˜oes em 2008” • Identical Translation Removal : Many words in EP are translated equally to BP, which is done by default. Furthermore, if any word in the source phrase is contained in the target one or vice versa, the phrase pair is also removed, e.g.: “Eu”→ “E Eu”. • Confidence based Entries Filtering: Removes phrase pairs with low confidence based on their features, which are used in (Koehn et al., 2003). We remove entries having probabilities lower than 1 (to trim ambiguous entries) and the respective weights are lower than 0.5 (empiric threshold). We plan in future work to improve this filter by using a linear combination of the 4 parameters. • Lexicon based Filtering: Removes phrase pairs where the source or the target contain words that are not present in the lexicon of the respective language. • Number of Words Filtering: Removes phrase pairs where the number of words in the source is different from the number of words in the target. In our experiments, phrase pairs were limited to 2 wor"
2011.eamt-1.19,2010.iwslt-papers.14,1,0.883624,"Missing"
2011.eamt-1.19,W06-2405,0,0.0264081,"Missing"
2011.eamt-1.19,moore-2002-fast,0,0.0416943,"Our second experiment was formulated to analyze how the system can translate from BP to EP. It was used the manually parallel corpora of EP and BP. Our third experiment evaluated the usage of the BP2EP output in SMT. Our goal was to determine whether it is observed translation quality gains when adding the BP2EP output, created from the BP texts, to the EP models. The parallel corpora used in the SMT evaluation was created from TED talks. Since the audio transcriptions and translations available at the TED website are not aligned at the sentence level, we used the Bilingual Sentence Aligner (Moore, 2002) to accomplish this task. Table 5 shows some details about the EPEN, BP-EN, BP2EP-EN, PT-&-BP-EN and PT&BP2EP-EN. The BP2EP corpus corresponds to the output of the BP2EP system with the BP corpus as input. The EP-&-BP and EP-&-BP2EP corpus is the concatenation of the EP corpus with the BP and BP2EP corpus, respectively. 7.1 Evaluation of the extraction of Multiword Lexical Contrastive pairs from SMT Phrase-Tables We run the phrase table extraction algorithm for pairs of translations containing 1 and 2 words. The corpus was retrieved from the set of TED talks, that had both the Brazilian Portug"
2011.eamt-1.19,D09-1141,0,0.400395,"with identical approaches to translate from Czech to Slovak (Hajiˇc et al., 2000), from Spanish to Catalan (Canals-Marote et al., 2001)(Navarro et al., 2004), and from Irish to Gaelic Scottish (Scannell, 2006). Looking at Scannel’s system (2006) gives us a better understanding of the common system architecture. This architecture consists of a pipeline of components, such as a Part-of-Speech tagger (POS-tagger), a Na¨ıve Bayes word sense disambiguator, and a set of lexical and grammatical transfer rules, based on bilingual contrastive lexical and grammatical differences. On a different level, (Nakov and Ng, 2009) describes a way of building MT systems for lessresourced languages by exploring similarities with closely related and languages with much more resources. More than allowing translation for lessresourced languages, this work also aims at allowing translation from groups of similar languages to other groups of similar languages just like stated earlier. This method proposes the merging of bilingual texts and phrase-table combination in the training phase of the MT system. Merging bilingual texts from similar languages (on the source side), one with the less-resourced language and the other (muc"
2011.eamt-1.19,P02-1040,0,0.0834116,"arallel corpus on the EP translation, we made several experiments. Using the EP→EN and EN→EP models as baseline, we compared them with BP and BP2EP models, and also EP-&-BP and EP-&BP2EP. All experiments were performed using the Moses decoder 9 . Before decoding the test set (shown in Table 5), we tune the weights of the phrase table using Minimum Error Rate Training (MERT) using the devel corpus shown in Table 5. The devel and test set are in EP and EN and are the same among the several experiments. The language model was created only with EP texts. The results were evaluated using the BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009) metrics. 9 http://www.statmt.org/moses/ Tables 6 and 7 shows the results for the EP/BP/BP2EP→EN and EN→EP/BP/BP2EP models, respectively. We observed that BP models generate better results than using only the EP ones. The larger amount of parallel data for BP explains these differences. The BP2EP results were systematically better than using BP models. This shows that our hypothesis of converting BP to EP using this approach led to consistent improvements to the translation between EN and EP. Data Train Devel Test Lang. Pairs EP EN BP EN BP2EP EN EP EN EP"
2011.iwslt-papers.3,E03-1035,0,0.30853,"Missing"
2011.iwslt-papers.3,J03-3002,0,0.159638,"Missing"
2011.iwslt-papers.3,N04-1036,0,0.0818659,"Missing"
2011.iwslt-papers.3,2010.iwslt-papers.14,1,0.832985,"Missing"
2011.iwslt-papers.3,P07-2045,0,0.0045147,"Missing"
2011.iwslt-papers.3,P06-1010,0,0.081669,"Missing"
2011.iwslt-papers.3,O08-2010,0,\N,Missing
2020.tacl-1.23,D18-1045,0,0.339892,"age model X is trained on dataset Y. A bigger language model improves the doc-reranker but does not help the sent-reranker. Architecture Data PPL transformer-XL transformer-XL NIST sent NIST + GW sent 83.3 96.5 LSTM transformer-XL transformer-XL NIST doc NIST doc NIST + GW doc 71.6 43.8 43.4 Table 3: Perplexity per word of language models on NIST dev set. GW refers to Gigaword. Figure 3: Effect of n-best list. about the reliability of using BLEU at assessing cross-sentential consistency (Voita et al., 2019b). To compare the effectiveness of leveraging monolingual data between backtranslation (Edunov et al., 2018; Sennrich et al., 2016a) and our model, we train the document transformer (Zhang et al., 2018) using additional synthetic parallel documents generated by backtranslation (q ′ ). For fair comparison we use the same monolingual data for both models. As shown in Table 1, although both techniques improve translation, backtranslation is less effective than our model. Because we have a new model q ′ , we can use it as a proposal model for our doc-reranker—effectively using the monolingual data twice. We find that this improves results even further, indicating that the effect of both approaches is a"
2020.tacl-1.23,P19-1019,0,0.0128348,"ed must be carefully selected; the ratio of backtranslated data and original data must be balanced carefully. While techniques for doing this are fairly well established for single sentence models, no such established techniques exist for documents. More generally, strategies for using monolingual data in nueral MT systems is an active research area (G¨ulc¸ehre et al., 2015; Cheng et al., 2016, inter alia). Backtranslation (Edunov et al., 2018; Sennrich et al., 2016a), originally invented for semi-supervised MT, has been used as a standard approach for unsupervised MT (Lample et al., 2018a,b; Artetxe et al., 2019, 2018). Noisy channel decompositions have been a standard approach in statistical machine translation (Brown et al., 1993; Koehn et al., 2007) and recently have been applied to neural models (Yu et al., 2017; Yee et al., 2019; A Appendix Ng et al., 2019). Unlike prior work, we adopt noisy channel models for document-level MT. While the model from Yu et al. (2017) could be used on documents by concatenating their sentences to form a single long sequence, this would not let us use the conditional sentence independence assumptions that gives our model the flexibility to use just parallel sentenc"
2020.tacl-1.23,J93-1004,0,0.777583,"Description We define x = (x1 , x2 , . . . , xI ) as the source document with I sentences, and similarly, y = (y1 , y 2 , . . . , y J ) as the target document with J sentences. During the (human) translation process, translators may split or recombine sentences, but we will assume that I = J .1 Let xi = (xi1 , xi2 , . . . , xiM ) represent the ith sentence in the document, consisting of M words; likewise y i = i (y1i , y2i , . . . , yN ) denote the ith sentence in the target document, containing N words. 1 Size mismatches are addressed by merging sentences using sentence alignment algorithms (Gale and Church, 1993). The translation of a document x is determined ˆ , where p(ˆ y |x) is by finding the document y optimal. ˆ = arg max p(y |x). y (1) y Instead of modeling the probability p(y |x) directly, we factorize it using Bayes’ rule: ˆ = arg max y y p( x |y ) × p( y ) p(x) = arg max p(x |y ) × y |{z } channel model p( y ) |{z} . (2) language model We further assume that sentences are independently translated, and that the sentences are generated by a left-to-right factorization according to the chain rule. Therefore, we have ˆ ≈ arg max y y |x| Y Figure 1: Graphical model showing the factorization of ou"
2020.tacl-1.23,D18-1549,0,0.0430969,"Missing"
2020.tacl-1.23,N18-1118,0,0.420146,"lled a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation probabilities challenging. More subtly, documents are considerably more diverse than sentences, and models must be carefully biased so as not to pick up spurious correlations. Our Bayes’ rule decomposition (§2) permits several innovations that enable us to solve these problems. Rather than directly modeling the conditional distribution, we re"
2020.tacl-1.23,J93-2003,0,0.192577,"Missing"
2020.tacl-1.23,P18-1118,0,0.0611387,"guage understanding, and semi-supervised machine translation. Recent studies (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia) 355 have shown that exploiting document-level context improves translation performance, and in particular improves lexical consistency and coherence of the translated text. Existing work in the area of context-aware NMT typically adapts the MT system to take additional context as input, either a few previous sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Werlen et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). These methods vary in the method of encoding the additional context and the way of integrating the context with the existing sequenceto-sequence models. For example, Werlen et al. (2018) encode the context with a separate transformer encoder (Vaswani et al., 2017) and use a hierarchical attention model to integrate the context into the rest of transformer model. Zhang et al. (2018) introduce an extra self-attention layer in the encoder to attend over the the context. Strategies for exploiting monolingual documentlevel data have been explored in two recent studies (Voita"
2020.tacl-1.23,P16-1185,0,0.0212721,"that 1) the language model is portable across domain and language pairs; 2) our model involves straightforward training procedures. Specifically, for backtranslation to succeed, monolingual data that will be back-translated must be carefully selected; the ratio of backtranslated data and original data must be balanced carefully. While techniques for doing this are fairly well established for single sentence models, no such established techniques exist for documents. More generally, strategies for using monolingual data in nueral MT systems is an active research area (G¨ulc¸ehre et al., 2015; Cheng et al., 2016, inter alia). Backtranslation (Edunov et al., 2018; Sennrich et al., 2016a), originally invented for semi-supervised MT, has been used as a standard approach for unsupervised MT (Lample et al., 2018a,b; Artetxe et al., 2019, 2018). Noisy channel decompositions have been a standard approach in statistical machine translation (Brown et al., 1993; Koehn et al., 2007) and recently have been applied to neural models (Yu et al., 2017; Yee et al., 2019; A Appendix Ng et al., 2019). Unlike prior work, we adopt noisy channel models for document-level MT. While the model from Yu et al. (2017) could be"
2020.tacl-1.23,N19-1213,0,0.100327,"Missing"
2020.tacl-1.23,W19-5321,0,0.0352241,"2019). These methods vary in the method of encoding the additional context and the way of integrating the context with the existing sequenceto-sequence models. For example, Werlen et al. (2018) encode the context with a separate transformer encoder (Vaswani et al., 2017) and use a hierarchical attention model to integrate the context into the rest of transformer model. Zhang et al. (2018) introduce an extra self-attention layer in the encoder to attend over the the context. Strategies for exploiting monolingual documentlevel data have been explored in two recent studies (Voita et al., 2019a; Junczys-Dowmunt, 2019). Both use backtranslation (Edunov et al., 2018; Sennrich et al., 2016a) to create synthetic parallel documents as additional training data. In contrast, we train a large-scale language model and use it to refine the consistency between sentences under a noisy channel framework. Advantages of our model over back-translation are that 1) the language model is portable across domain and language pairs; 2) our model involves straightforward training procedures. Specifically, for backtranslation to succeed, monolingual data that will be back-translated must be carefully selected; the ratio of backt"
2020.tacl-1.23,D16-1139,0,0.0241688,"y 2.5 BLEU. 353 The two best systems submitted to the WMT19 Chinese–English translation task are Microsoft Research Asia’s system (Xia et al., 2019) and Baidu’s system (Sun et al., 2019), both of which use multiple techniques to improve upon the transformer big model. Here, we mainly compare our results with those from Xia et al. (2019) because we use the same evaluation metric SacreBLEU (Post, 2018) and the same validation and test sets. Using extra parallel training data and the techniques of masked sequence-to-sequence pretraining (Song et al., 2019), sequence-level knowledge distillation (Kim and Rush, 2016), and backtranslation (Edunov et al., 2018), the best model from Xia et al. (2019) achieves 30.8, 30.9, and 39.3 on newstest2017, newstest2018, and newstest2019, respectively. Although our best results are lower than this, it is notable that our model achieves comparable results to their model, which was trained on 56M sentences of parallel data—over two times more training data than we use. However, our method is orthogonal to these works and can be combined with other techniques to make further improvement. 5 Analysis In this section, we present the quantitative and qualitative analysis of o"
2020.tacl-1.23,N19-1423,0,0.142685,"oss-sentence context in the language model, and it outperforms existing document translation approaches. 1 Introduction There have been many recent demonstrations that neural language models based on transformers (Vaswani et al., 2017; Dai et al., 2019) are capable of learning to generate remarkably coherent documents with few (Zellers et al., 2019) or no (Radford et al., 2019) conditioning variables. Despite this apparent generation ability, in practical applications, unconditional language models are most often used to provide representations for natural language understanding applications (Devlin et al., 2019; Yang et al., 2019; Peters 346 Transactions of the Association for Computational Linguistics, vol. 8, pp. 346–360, 2020. https://doi.org/10.1162/tacl a 00319 Action Editor: David Chiang. Submission batch: 12/2019; Revision batch: 2/2020; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. conditional distribution to learning two different distributions: a language model p(y ), which provides unconditional estimates of the output (in this paper, documents); and p(x |y ), which provides the probability of translating a candidate output y in"
2020.tacl-1.23,P07-2045,1,0.0265342,"log q (y i |x)+ log pLM (y i |y <i )+ λ2 log pTM (xi |y i ) + λ3 |y i |+ O(x, y <i−1 , y i−1 ), (4) where |y |denotes the number of tokens in the sentence y , and where the base case O(x, y <0 , y 0 ) = 0. Note that Eq. 4 is a generalization of Eq. 3 in log space—if we set λ1 = λ3 = 0 and λ2 = 1 and take the log of Equation 3 the two objectives are equivalent. The extra factors—the proposal probability and the length of the output—provide improvements (e.g., by calibrating the expected length of the output), and can be incorporated at no cost in the model; they are widely used in prior work (Koehn et al., 2007; Yu et al., 2017; Yee 2 Our proposal model can optionally use document context on the source (conditioning) side, but sentences are generated independently. which contains 169 documents, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As"
2020.tacl-1.23,N18-1202,0,0.0527799,"Missing"
2020.tacl-1.23,W18-6319,0,0.0719972,"s, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As for NIST, we learn a byte pair encoding (Sennrich et al., 2016b) with 32K merges to segment words into sub-word units for both Chinese and English. The evaluation metric is sacreBLEU (Post, 2018). et al., 2019; Ng et al., 2019). The elements on the beam after considering the ℓth sentence are reranked one final time by adding log pLM (hSTOPi | y ≤ℓ ) to the final score; this accounts for the language model’s assessment that the candidate document has been appropriately ended.3 4 Experiments We evaluate our model on two translation tasks, the NIST Open MT Chinese–English task4 and the WMT19 Chinese–English news translation task.5 On both tasks, we use the standard parallel training data, and compare our model with a strong transformer baseline, as well as related models from prior work."
2020.tacl-1.23,P16-1009,0,0.666657,"l can optionally use document context on the source (conditioning) side, but sentences are generated independently. which contains 169 documents, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As for NIST, we learn a byte pair encoding (Sennrich et al., 2016b) with 32K merges to segment words into sub-word units for both Chinese and English. The evaluation metric is sacreBLEU (Post, 2018). et al., 2019; Ng et al., 2019). The elements on the beam after considering the ℓth sentence are reranked one final time by adding log pLM (hSTOPi | y ≤ℓ ) to the final score; this accounts for the language model’s assessment that the candidate document has been appropriately ended.3 4 Experiments We evaluate our model on two translation tasks, the NIST Open MT Chinese–English task4 and the WMT19 Chinese–English news translation task.5 On both tasks, we use the"
2020.tacl-1.23,P16-1162,0,0.850069,"l can optionally use document context on the source (conditioning) side, but sentences are generated independently. which contains 169 documents, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As for NIST, we learn a byte pair encoding (Sennrich et al., 2016b) with 32K merges to segment words into sub-word units for both Chinese and English. The evaluation metric is sacreBLEU (Post, 2018). et al., 2019; Ng et al., 2019). The elements on the beam after considering the ℓth sentence are reranked one final time by adding log pLM (hSTOPi | y ≤ℓ ) to the final score; this accounts for the language model’s assessment that the candidate document has been appropriately ended.3 4 Experiments We evaluate our model on two translation tasks, the NIST Open MT Chinese–English task4 and the WMT19 Chinese–English news translation task.5 On both tasks, we use the"
2020.tacl-1.23,N19-1313,0,0.376484,"bles, conditional dependencies between all y i ’s and between each y i and all xi ’s are created (Shachter, 1998). The (in)dependencies that are present in the posterior distribution are shown in the bottom of Figure 1. Thus, although modeling p(y |x) or p(x |y ) would appear to be superficially similar, the 348 statistical impact of making a conditional independence assumption is quite different. This is fortunate, as it makes it straightforward to use parallel sentences, rather than assuming we have parallel documents, which are less often available (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia). Finally, because we only need to learn to model the likelihood of sentence translations (rather than document translations), the challenges of guiding the learners to make robust generalizations in direct document translation models (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia) are neatly avoided. 2.2 Learning We can parameterize the channel probability p(xi |y i ) using any sequence-to-sequence model and parameterize the language model p(y i |y <i ) using any language model. It is straightforward to learn our model: We simply optimize the channel mod"
2020.tacl-1.23,2006.amta-papers.25,0,0.0900797,"Missing"
2020.tacl-1.23,W19-5333,0,0.267637,"onal task: machine translation. The application of Bayes’ rule to transform the translation modeling problem p(y |x), where y is the target language, and x is the source language, has a long tradition and was the dominant paradigm in speech and language processing for many years (Brown et al., 1993), where it is often called a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation probabilities chall"
2020.tacl-1.23,W19-5341,0,0.0121626,"spensable for the doc-reranker, indicating that Bayes’ rule provides reliable estimates of translation probabilities. Table 5 presents the results of our model together with baselines on the WMT19 Chinese– English translation task. We find that the results follow the same pattern as those on NIST: A better language model leads to better translation results and overall the reranker outperforms the transformer-big by approximately 2.5 BLEU. 353 The two best systems submitted to the WMT19 Chinese–English translation task are Microsoft Research Asia’s system (Xia et al., 2019) and Baidu’s system (Sun et al., 2019), both of which use multiple techniques to improve upon the transformer big model. Here, we mainly compare our results with those from Xia et al. (2019) because we use the same evaluation metric SacreBLEU (Post, 2018) and the same validation and test sets. Using extra parallel training data and the techniques of masked sequence-to-sequence pretraining (Song et al., 2019), sequence-level knowledge distillation (Kim and Rush, 2016), and backtranslation (Edunov et al., 2018), the best model from Xia et al. (2019) achieves 30.8, 30.9, and 39.3 on newstest2017, newstest2018, and newstest2019, respe"
2020.tacl-1.23,W17-4811,0,0.298707,"., 1993), where it is often called a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation probabilities challenging. More subtly, documents are considerably more diverse than sentences, and models must be carefully biased so as not to pick up spurious correlations. Our Bayes’ rule decomposition (§2) permits several innovations that enable us to solve these problems. Rather than directly modeling the conditiona"
2020.tacl-1.23,Q18-1029,0,0.395626,"h closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al., 2018; Tu et al., 2018; Xiong et al., 2019). Although in general it is unreasonable to expect that independent translations of sentences would lead to coherent translations of documents, the task of translating Chinese into English poses some particularly acute challenges. As Chinese makes fewer inflectional distinctions than English does, and the relevant clues for predicting, for example, what tense an English verb should be in, or whether an English noun should have singular or plural morphology, may be spread throughout a document, it is crucial that extra-sentential context is used. Our experiments (§4) explor"
2020.tacl-1.23,D19-1571,0,0.48966,"improve a conditional task: machine translation. The application of Bayes’ rule to transform the translation modeling problem p(y |x), where y is the target language, and x is the source language, has a long tradition and was the dominant paradigm in speech and language processing for many years (Brown et al., 1993), where it is often called a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation p"
2020.tacl-1.23,D19-1081,0,0.749257,"i ’s. By conditioning on the child variables, conditional dependencies between all y i ’s and between each y i and all xi ’s are created (Shachter, 1998). The (in)dependencies that are present in the posterior distribution are shown in the bottom of Figure 1. Thus, although modeling p(y |x) or p(x |y ) would appear to be superficially similar, the 348 statistical impact of making a conditional independence assumption is quite different. This is fortunate, as it makes it straightforward to use parallel sentences, rather than assuming we have parallel documents, which are less often available (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia). Finally, because we only need to learn to model the likelihood of sentence translations (rather than document translations), the challenges of guiding the learners to make robust generalizations in direct document translation models (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia) are neatly avoided. 2.2 Learning We can parameterize the channel probability p(xi |y i ) using any sequence-to-sequence model and parameterize the language model p(y i |y <i ) using any language model. It is straightforward to learn our"
2020.tacl-1.23,P19-1116,0,0.456022,"i ’s. By conditioning on the child variables, conditional dependencies between all y i ’s and between each y i and all xi ’s are created (Shachter, 1998). The (in)dependencies that are present in the posterior distribution are shown in the bottom of Figure 1. Thus, although modeling p(y |x) or p(x |y ) would appear to be superficially similar, the 348 statistical impact of making a conditional independence assumption is quite different. This is fortunate, as it makes it straightforward to use parallel sentences, rather than assuming we have parallel documents, which are less often available (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia). Finally, because we only need to learn to model the likelihood of sentence translations (rather than document translations), the challenges of guiding the learners to make robust generalizations in direct document translation models (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia) are neatly avoided. 2.2 Learning We can parameterize the channel probability p(xi |y i ) using any sequence-to-sequence model and parameterize the language model p(y i |y <i ) using any language model. It is straightforward to learn our"
2020.tacl-1.23,D16-1138,1,0.769514,"r reverse translation model treats sentences independently. The search is guided by a proposal distribution that provides candidate continuations of a document prefix, and these are reranked according to the posterior distribution. In particular, we compare two proposal models: one based on estimates of independent sentence translations (Vaswani et al., 2017) and one that conditions on the source document context (Zhang et al., 2018). Although closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al., 2018; Tu et al., 2018; Xiong et al., 2019). Although in general it is unreasonable to expect that independent translations of sentences would lead to coherent translations of d"
2020.tacl-1.23,P18-1117,0,0.0741973,"arch: context-aware neural machine translation, large-scale language models for language understanding, and semi-supervised machine translation. Recent studies (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia) 355 have shown that exploiting document-level context improves translation performance, and in particular improves lexical consistency and coherence of the translated text. Existing work in the area of context-aware NMT typically adapts the MT system to take additional context as input, either a few previous sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Werlen et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). These methods vary in the method of encoding the additional context and the way of integrating the context with the existing sequenceto-sequence models. For example, Werlen et al. (2018) encode the context with a separate transformer encoder (Vaswani et al., 2017) and use a hierarchical attention model to integrate the context into the rest of transformer model. Zhang et al. (2018) introduce an extra self-attention layer in the encoder to attend over the the context. Strategies for ex"
2020.tacl-1.23,D17-1301,0,0.528329,"e-translation-evaluation. 5 http://www.statmt.org/wmt19/translationtask.html. 350 For WMT19, we use the transformer as both the channel and proposal model. The hyperparameters for training the transformer is the same as transformer big (Vaswani et al., 2017), namely, 1,024 hidden size, 4,096 filter size, 16 attention heads, and 6 layers. The model is trained on 8 GPUs with batch size of 4,096. The setup for the language model is the same as that of NIST except that the training data is the English side of the parallel training data and Gigaword. Method Model Proposal MT06 MT03 MT04 MT05 MT08 (Wang et al., 2017) (Kuang et al., 2017) (Zhang et al., 2018) RNNsearch Transformer + cache Doc-transformer – – – 37.76 48.14 49.69 – 48.05 50.21 – 47.91 49.73 36.89 48.53 49.46 27.57 38.38 39.69 Baseline Sent-transformer Doc-transformer (q ) Backtranslation (q ′ ) Sent-reranker – – – q 47.72 49.79 50.77 51.33 47.21 49.29 51.80 52.23 49.08 50.17 51.61 52.36 46.86 48.99 51.81 51.63 40.18 41.70 42.47 43.63 Doc-reranker Doc-reranker q q′ 51.99 53.63 52.77 54.51 52.84 54.23 51.84 54.86 44.17 45.17 This work Table 1: Comparison with prior work on NIST Chinese–English translation task. The evaluation metric is tokeniz"
2020.tacl-1.23,D18-1325,0,0.330585,"t al., 2018). Although closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al., 2018; Tu et al., 2018; Xiong et al., 2019). Although in general it is unreasonable to expect that independent translations of sentences would lead to coherent translations of documents, the task of translating Chinese into English poses some particularly acute challenges. As Chinese makes fewer inflectional distinctions than English does, and the relevant clues for predicting, for example, what tense an English verb should be in, or whether an English noun should have singular or plural morphology, may be spread throughout a document, it is crucial that extra-sentential context is used. Our experi"
2020.tacl-1.23,K19-1074,0,0.0859984,"Missing"
2020.tacl-1.23,D18-1049,0,0.0540894,"uch harder decoding problem (§3). To address this problem, we propose a new beam-search algorithm, exploiting the fact that our document language model operates left-to-right, and our reverse translation model treats sentences independently. The search is guided by a proposal distribution that provides candidate continuations of a document prefix, and these are reranked according to the posterior distribution. In particular, we compare two proposal models: one based on estimates of independent sentence translations (Vaswani et al., 2017) and one that conditions on the source document context (Zhang et al., 2018). Although closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al"
2020.wmt-1.36,P19-1425,0,0.021074,"ranslation quality as shown by existing work (Sun et al., 2019; Ng et al., 2019). After training the proposal models with the mix of real and synthetic parallel data, we fine-tuned the models with CWMT and a subset of newstest2017 and newstest2018 which were not used for validation. 4.4 Improving Uncertainty Estimation To improve the robustness of noisy channel reranking, we explore two approaches for improving uncertainty estimation of the seq2seq scoring models. 4.4.1 Adversarially Trained Proposal Models To simulate different wordings and noises in source and candidate sentences, we follow Cheng et al. (2019) to train the models on noisy adversarial inputs and targets. We use bidirectional languagemodels to provide the noisy candidates and select the candidates with highest loss (i.e., adversarial source-target inputs). During the training, we optimize the original loss with clean source-target pairs, the language model losses for source and target sides, and the adversarial loss using adversarial source-target inputs. In the final scoring, we use an ensemble of eight adversarially trained models with few differences from Cheng et al. (2019): (a) We explore training with and without the language m"
2020.wmt-1.36,P19-1285,0,0.0283336,"dunov et al., 2018), distillation (Kim and Rush, 2016; Liu et al., 2016), and forward-translated parallel documents. We further improve these sequence-to-sequence (seq2seq) models by fine-tuning them with in-domain data (§4.3). To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation (§4.4). Finally, we include candidate translations generated by Monte-Carlo Tree Search (MCTS) (§B) in order to improve the diversity of the candidate pool for the reranker. Our language models are based on the Transformer-XL architecture (Dai et al., 2019) and optimized with distillation and fine-tuning with in-domain data (§5). During development, we observed weaknesses in our system’s translations for long sentences, largely due to premature truncations. We developed several techniques to mitigate this issue such as sentence segmentation (breaking sentences into logical complete segments) and training specialized models with synthetically constructed long sequences to generate additional proposals for our reranker (§A). Experiments show that the aforementioned techniques are very effective: our system outperforms the Transformer baseline by 9"
2020.wmt-1.36,D18-1045,0,0.17332,"and optimized independently while at inference time, when we reason over the posterior distribution of translations given the source document, conditional dependencies between translations are induced by the language model prior. The core of our document-level translation architecture is the noisy channel reranker. It requires proposal, channel, and language models, each of which is optimized separately using different techniques and approaches. For the proposal and channel models we use Transformer models (Vaswani et al., 2017) (§4.1) with data augmentation (§4.2), such as back translation (Edunov et al., 2018), distillation (Kim and Rush, 2016; Liu et al., 2016), and forward-translated parallel documents. We further improve these sequence-to-sequence (seq2seq) models by fine-tuning them with in-domain data (§4.3). To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation (§4.4). Finally, we include candidate translations generated by Monte-Carlo Tree Search (MCTS) (§B) in order to improve the diversity of the candidate pool for the reranker. Our language models are based on the Transformer-XL architecture (Dai et al., 2019) a"
2020.wmt-1.36,D11-1125,0,0.118354,"Missing"
2020.wmt-1.36,D16-1139,0,0.184286,"t inference time, when we reason over the posterior distribution of translations given the source document, conditional dependencies between translations are induced by the language model prior. The core of our document-level translation architecture is the noisy channel reranker. It requires proposal, channel, and language models, each of which is optimized separately using different techniques and approaches. For the proposal and channel models we use Transformer models (Vaswani et al., 2017) (§4.1) with data augmentation (§4.2), such as back translation (Edunov et al., 2018), distillation (Kim and Rush, 2016; Liu et al., 2016), and forward-translated parallel documents. We further improve these sequence-to-sequence (seq2seq) models by fine-tuning them with in-domain data (§4.3). To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation (§4.4). Finally, we include candidate translations generated by Monte-Carlo Tree Search (MCTS) (§B) in order to improve the diversity of the candidate pool for the reranker. Our language models are based on the Transformer-XL architecture (Dai et al., 2019) and optimized with distillation and"
2020.wmt-1.36,P18-1007,0,0.0261476,"BLEU, such that: a sentence were replaced with the capitalized variant that occurred most frequently in other positions of the English monolingual training data. Thus, in the previous sentence the initial token would have been words rather than Words. (1 − META)4 = TER× (1 − BLEU)× Subword units To encode text into sub-word units, we used the sentencepiece tool (Kudo and Richardson, 2018). For seq2seq models (i.e., the channel model and proposal models), we trained the segmentation model on the first 10 million sentences of the parallel training corpus,3 using joint source and target unigram (Kudo, 2018) subword segmentation algorithm with a target vocabulary of 32K tokens and minimum character coverage of 0.9995, which resulted in 32,768 word pieces.4 For the language model, we used the English side alone with the same vocabulary size and a character coverage of 1.0. (1 − METEOR)× (1 − q0.1 (BLEU)). (4) When several configurations of hyperparameters achieve values of META very close to the maximum (within 0.02), we pick the one maximizing BLEU and/or minimizing the L2 norm of the λs, considered as a vector. This corresponds to an intuitive prior towards giving more weight to the language mod"
barreiro-etal-2014-linguistic,C96-1030,0,\N,Missing
barreiro-etal-2014-linguistic,W07-0728,0,\N,Missing
barreiro-etal-2014-linguistic,P07-2045,0,\N,Missing
barreiro-etal-2014-linguistic,W07-0732,0,\N,Missing
barreiro-etal-2014-linguistic,2013.mtsummit-wmwumttt.5,1,\N,Missing
barreiro-etal-2014-linguistic,2007.mtsummit-papers.40,0,\N,Missing
barreiro-etal-2014-linguistic,N13-1140,0,\N,Missing
barreiro-etal-2014-linguistic,W11-2117,0,\N,Missing
C12-2070,2005.iwslt-1.8,0,0.0165691,"eline similar to the one described in (Koehn et al., 2003). First, the word alignments were generated using IBM model 4. Then, the translation model was generate using the phrase extraction algorithm (Paul et al., 2010)(Koehn et al., 2007). The maximum size of the phrase pairs is set to 7, both for the source and the target language. The model uses as features: • • • • • Translation probability Reverse translation probability Lexical translation probability Reverse lexical translation probability Phrase penalty The reordering model is built using the lexicalized reordering model described in (Axelrod et al., 2005), with MSD (mono, swap and discontinuous) reordering features for orientations. All the translation and reordering features are considered during the calculation of the relative entropy. As in (Zens et al., 2012), we removed all singleton phrase pairs from the phrase table. This will lower the effectiveness of significance pruning, since a large amount of least significant phrase pairs will be removed a priori. The filtered translation model contains, approximately 50 million phrase pairs. As language model, a 5-gram model with Kneser-ney smoothing was used. The baseline model was tuned using"
C12-2070,N09-1025,0,0.0272792,"are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Moore, 2007) use an already tuned model (using MERT) 714 in a competitive linking algorithm to keep the best one-to-one phras"
C12-2070,P08-1010,0,0.0162618,"analyses both algorithms and preceeds our combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tun"
C12-2070,D07-1103,0,0.385725,"based on relative entropy is described in (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). In these approaches, a criteria based on the KL divergence is applied, so that higher order n-grams are only included in the model when they provide enough additional information to the model, given the lower order n-grams. Recently, this concept was applied for translation model pruning (Ling et al., 2012; Zens et al., 2012), and results indicate that this method yields a better phrase table size and translation quality ratio than previous methods, such as the well known method in (Johnson et al., 2007), which uses the Fisher’s exact test to calculate how well a phrase pair is supported by data. In this work, we attempt to improve the relative entropy model, by combining it with the significance based approach presented in (Johnson et al., 2007).The main motivation is that, as suggested in (Ling et al., 2012), relative entropy and significance based methods are complementary. On one hand, relative entropy aims at pruning phrase pairs that can be reproduced using smaller constituents with a small or no loss in terms of the models predictions. On the other hand, significance pruning aims at re"
C12-2070,2005.mtsummit-papers.11,0,0.242631,"nd are originated from incorrect alignments at sentence or word level. This indicates that both methods can be combined to obtain better results. We propose a log-linear interpolation of the two metrics to achieve a better trade off between the number of phrase pairs and the translation quality. This paper is structured as follows: Section 2 includes a brief summary of relative entropy and significance pruning approaches in sub-sections 2.1 and 2.2. Sub-section 2.3 analyses both algorithms and preceeds our combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004;"
C12-2070,P07-2045,0,0.0067566,"ance score of 10. 3 Experimental Results 3.1 Data Sets Experiments were performed using the publicly available EUROPARL (Koehn, 2005) corpora for the English-French language pair. From this corpus, 1.2M sentence pairs were selected for training, 2000 for tuning and another 2000 for testing. 3.2 Baseline System The baseline translation system was trained using a conventional pipeline similar to the one described in (Koehn et al., 2003). First, the word alignments were generated using IBM model 4. Then, the translation model was generate using the phrase extraction algorithm (Paul et al., 2010)(Koehn et al., 2007). The maximum size of the phrase pairs is set to 7, both for the source and the target language. The model uses as features: • • • • • Translation probability Reverse translation probability Lexical translation probability Reverse lexical translation probability Phrase penalty The reordering model is built using the lexicalized reordering model described in (Axelrod et al., 2005), with MSD (mono, swap and discontinuous) reordering features for orientations. All the translation and reordering features are considered during the calculation of the relative entropy. As in (Zens et al., 2012), we r"
C12-2070,N03-1017,0,0.0919428,"s structured as follows: Section 2 includes a brief summary of relative entropy and significance pruning approaches in sub-sections 2.1 and 2.2. Sub-section 2.3 analyses both algorithms and preceeds our combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, t"
C12-2070,D12-1088,1,0.819902,"al selection criteria. The challenge in this task is to choose the entries that will least degenerate the quality of the task for which the model is used. For language models, an effective algorithm based on relative entropy is described in (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). In these approaches, a criteria based on the KL divergence is applied, so that higher order n-grams are only included in the model when they provide enough additional information to the model, given the lower order n-grams. Recently, this concept was applied for translation model pruning (Ling et al., 2012; Zens et al., 2012), and results indicate that this method yields a better phrase table size and translation quality ratio than previous methods, such as the well known method in (Johnson et al., 2007), which uses the Fisher’s exact test to calculate how well a phrase pair is supported by data. In this work, we attempt to improve the relative entropy model, by combining it with the significance based approach presented in (Johnson et al., 2007).The main motivation is that, as suggested in (Ling et al., 2012), relative entropy and significance based methods are complementary. On one hand, rela"
C12-2070,D09-1078,0,0.0220035,"s to more search errors due to the large search space. Furthermore, larger models are more expensive to store, which limits the portability of such models to smaller devices. Pruning is one approach to address this problem, where models are made more compact by discarding entries from the model, based on additional selection criteria. The challenge in this task is to choose the entries that will least degenerate the quality of the task for which the model is used. For language models, an effective algorithm based on relative entropy is described in (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). In these approaches, a criteria based on the KL divergence is applied, so that higher order n-grams are only included in the model when they provide enough additional information to the model, given the lower order n-grams. Recently, this concept was applied for translation model pruning (Ling et al., 2012; Zens et al., 2012), and results indicate that this method yields a better phrase table size and translation quality ratio than previous methods, such as the well known method in (Johnson et al., 2007), which uses the Fisher’s exact test to calculate how well a phrase pair is supported by"
C12-2070,P03-1021,0,0.0139315,"ono, swap and discontinuous) reordering features for orientations. All the translation and reordering features are considered during the calculation of the relative entropy. As in (Zens et al., 2012), we removed all singleton phrase pairs from the phrase table. This will lower the effectiveness of significance pruning, since a large amount of least significant phrase pairs will be removed a priori. The filtered translation model contains, approximately 50 million phrase pairs. As language model, a 5-gram model with Kneser-ney smoothing was used. The baseline model was tuned using MERT tuning (Och, 2003). We did not rerun tuning again after pruning to avoid adding noise to the results. Finally, we present the results evaluated with BLEU-4 (Papineni et al., 2002). After computing the negative log likelihood of both scores, we also rescale both score’s values by mean, so that scores will have similar values. This step is performed so the interpolation weights, in the results appear more intuitive. 718 3.3 Results We can see the results in table 2, where the first two rows, represent the BLEU scores for relative entropy pruning and significance pruning, respectively. Then, we have the scores obt"
C12-2070,N04-1021,0,0.0271312,"pus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Moore, 2007) use an already tuned model (using MERT) 714 in a competitive linking algorithm to keep the"
C12-2070,P02-1040,0,0.0848059,"of the relative entropy. As in (Zens et al., 2012), we removed all singleton phrase pairs from the phrase table. This will lower the effectiveness of significance pruning, since a large amount of least significant phrase pairs will be removed a priori. The filtered translation model contains, approximately 50 million phrase pairs. As language model, a 5-gram model with Kneser-ney smoothing was used. The baseline model was tuned using MERT tuning (Och, 2003). We did not rerun tuning again after pruning to avoid adding noise to the results. Finally, we present the results evaluated with BLEU-4 (Papineni et al., 2002). After computing the negative log likelihood of both scores, we also rescale both score’s values by mean, so that scores will have similar values. This step is performed so the interpolation weights, in the results appear more intuitive. 718 3.3 Results We can see the results in table 2, where the first two rows, represent the BLEU scores for relative entropy pruning and significance pruning, respectively. Then, we have the scores obtained using the scorer in 4 of these 2 scores, with α weights at intervals of 0.1. Finally, we have the scores using the scorer 5, also with the weight α set at"
C12-2070,2010.iwslt-evaluation.1,0,0.0164305,"d with the significance score of 10. 3 Experimental Results 3.1 Data Sets Experiments were performed using the publicly available EUROPARL (Koehn, 2005) corpora for the English-French language pair. From this corpus, 1.2M sentence pairs were selected for training, 2000 for tuning and another 2000 for testing. 3.2 Baseline System The baseline translation system was trained using a conventional pipeline similar to the one described in (Koehn et al., 2003). First, the word alignments were generated using IBM model 4. Then, the translation model was generate using the phrase extraction algorithm (Paul et al., 2010)(Koehn et al., 2007). The maximum size of the phrase pairs is set to 7, both for the source and the target language. The model uses as features: • • • • • Translation probability Reverse translation probability Lexical translation probability Reverse lexical translation probability Phrase penalty The reordering model is built using the lexicalized reordering model described in (Axelrod et al., 2005), with MSD (mono, swap and discontinuous) reordering features for orientations. All the translation and reordering features are considered during the calculation of the relative entropy. As in (Zens"
C12-2070,2008.amta-srw.6,0,0.0176384,"en their probabilities log P(t|s) . This value is then weighted by the empirical distribution P(s, t), so that phrase pairs that are more likely to be observed in the data are less likely to be pruned. The empirical distribution is given as: P(s, t) = C(s, t) (2) N Where C(s, t) denotes, the number of sentence pairs where s and t are observed, and N denotes the number of sentence pairs. Computing Pp (t|s) is the most computationally expensive operation in this model, since it involves finding all possible derivations of a phrase pair using smaller units, which involves a forced decoding step (Schwartz, 2008). While minimizing D(Pp ||P) would lead to optimal results, such optimization is computationally infeasible. Thus, an approximation is the find the local values for each phrase pair: RelEnt(s,t) = −P(s, t)l og Pp (t|s) P(t|s) (3) This score can be viewed as the relative entropy between Pp (t|s) and P(t|s), if only the phrase pair with source s and target t is pruned. The problem with this approximation is that, we might assume a given phrase pair A can be pruned, because it can be composed by phrase pairs B and C , only to discover later that B is also pruned. 2.2 Significance Pruning Signific"
C12-2070,2009.mtsummit-papers.17,1,0.786811,"sults, such optimization is computationally infeasible. Thus, an approximation is the find the local values for each phrase pair: RelEnt(s,t) = −P(s, t)l og Pp (t|s) P(t|s) (3) This score can be viewed as the relative entropy between Pp (t|s) and P(t|s), if only the phrase pair with source s and target t is pruned. The problem with this approximation is that, we might assume a given phrase pair A can be pruned, because it can be composed by phrase pairs B and C , only to discover later that B is also pruned. 2.2 Significance Pruning Significance pruning of phrase tables (Johnson et al., 2007; Tomeh et al., 2009) relies on a statistical test that assesses the strength of the association between the source and target phrases in a phrase pair. Such association can be represented using a two-by-two contingency table: 715 C(s, t) C(t) − C(s, t) C(s) − C(s, t) N − C(s) − C(t) + C(s, t) where N is the size of the training parallel corpus, C(s) is the count of the source phrase, C(t) is the count of the target phrase, and C(s, t) is the count of the co-occurences of s and t . The probability of this particular table is given by the the hypergeometric distribution: ph (C(s, t)) = C(s)  N −C(s)  C(s,t) C(t)−"
C12-2070,2011.iwslt-papers.10,1,0.758963,"combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Moore, 2007) use an alr"
C12-2070,P03-1041,0,0.0349745,"rithms and preceeds our combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Mo"
C12-2070,D12-1089,0,0.176872,"ia. The challenge in this task is to choose the entries that will least degenerate the quality of the task for which the model is used. For language models, an effective algorithm based on relative entropy is described in (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). In these approaches, a criteria based on the KL divergence is applied, so that higher order n-grams are only included in the model when they provide enough additional information to the model, given the lower order n-grams. Recently, this concept was applied for translation model pruning (Ling et al., 2012; Zens et al., 2012), and results indicate that this method yields a better phrase table size and translation quality ratio than previous methods, such as the well known method in (Johnson et al., 2007), which uses the Fisher’s exact test to calculate how well a phrase pair is supported by data. In this work, we attempt to improve the relative entropy model, by combining it with the significance based approach presented in (Johnson et al., 2007).The main motivation is that, as suggested in (Ling et al., 2012), relative entropy and significance based methods are complementary. On one hand, relative entropy aims at"
C12-2070,N07-2053,0,0.0172899,"gopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Moore, 2007) use an already tuned model (using MERT) 714 in a competitive linking algorithm to keep the best one-to-one phrase matching in each training sentence. In our work we favor efficiency and we focus on relative entropy and significance pruning, which can be efficiently computed, without the need to external information. They also deliver good practical performance. 2.1 Relative Entropy Pruning Relative entropy pruning for translation models (Ling et al., 2012; Zens et al., 2012) has a solid foundation on information theory. The goal in these methods is to find a pruned model Pp (t|s) that yields"
C12-3041,P05-1045,0,0.01552,"Missing"
C12-3041,marujo-etal-2012-supervised,1,0.823786,"conclusions and future work. 2 Technical Approach We used a two-step approach to find named event passages (NEP) in a document. First, we used a previously trained classifier to label each sentence in the document. Second, we used both a rule-based model and HMM based statistical model to find contiguous sequences of sentences covering the same event. For the classifier part, we adopted a supervised learning framework, based on Weka (Hall et al. 2009). 2.1 Features and feature extraction As features, we used key-phrases identified by an automatic key-phrase extraction algorithm described in (Marujo et al. 2012). Key phrases consist of one or more words that represent the main concepts of the document. Marujo (2012) enhanced a state-of-the-art Supervised Key Phrase Extractor based on bagging over C4.5 decision tree classifier (Breiman, 1996; Quinlan, 1994) with several types of features, such as shallow semantic, rhetorical signals, and sub-categories from Freebase. The authors also included 2 forms of document pre-processing that were called light filtering and co-reference normalization. Light filtering removes sentences from the document, which are judged peripheral to its main content. Co-referen"
D12-1088,J93-2003,0,0.0282878,"bilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the translation quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding. 1 Introduction Phrase-based Machine Translation Models (Koehn et al., 2003) model n-to-m translations of n source words to m target words, which are encoded in phrase pairs and stored in the translation model. This approach has an advantage over Word-based Translation Models (Brown et al., 1993), since translating multiple source words allows the context for each source word to be considered during transHowever, not all words add the same amount of contextual information. Using the same example for “in”, if we add the context “(hid the key) in”, it is still not possible to accurately identify the best translation for the word “in”. The phrase extraction algorithm (Ling et al., 2010) does not discriminate which phrases pairs encode contextual information, and extracts all phrase pairs with consistent alignments. Hence, phrases that add no contextual information, such as, p(hid the key"
D12-1088,W06-1607,0,0.0192687,", the possible derivations are either using phrase p(s, t) or one element of its support set S1 , S2 or S3 . On the other hand, on the pruned model where p(s, t) does not exist, only S1 , S2 and S3 can be used. Thus, given a s, t pair one of three situations may occur. First, if the probability of the phrase pair p(s, t) is lower than the highest probability element in SP (p(s, t)), then both the models will choose that element, in which P (t|s) case, Pp(t|s) = 1. This can happen, if we define 966 features that penalize longer phrase pairs, such as lexical weighting, or if we apply smoothing (Foster et al., 2006). Secondly, if the probability of p(s, t) is equal to the most likely element in SP (p(s, t)), regardless of whether the unpruned model choses to use p(s, t) or that element, the probability emissions of the pruned and unpruned model will be identiP (t|s) cal. Thus, for this case Pp(t|s) = 1. Finally, if the probability of p(s, t) is higher than other possible derivations, the unpruned model will choose to emit the probability of p(s, t), while the pruned model will emit the most likely element in SP (p(s, t)). Hence, the probability loss between the 2 models, will be the ratio between the pro"
D12-1088,D07-1103,0,0.253111,"tion 5. Finally, we conclude in section 6. 2 Phrase Table Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large negative impact in the translation quality. This is especially relevant in environments where memory constraints are imposed, such as translation systems for small devices like cellphones, and also when time constraints for the translation are defined, such as online Speech-to-Speech systems. 963 2.1 Significance Pruning A relevant reference in phrase table pruning is the work of (Johnson and Martin, 2007), where it is shown that a significant portion of the phrase table can be discarded without a considerable negative impact on translation quality, or even positive one. This work computes the probability, named p-value, that the joint occurrence event of the source phrase s and target phrase t occurring in same sentence pair happens by chance, and are actually statistically independent. Phrase pairs that have a high p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their com"
D12-1088,N03-1017,0,0.0838211,"Missing"
D12-1088,P07-2045,0,0.00825301,"sentence. Using this distribution, the model is more biased in pruning phrase pairs with s, t pairs that do not occur frequently. 3.4 Computing Pp (t|s) P (t|s) P (t|s) The computation of Pp(t|s) depends on how the decoder adapts when a phrase pair is pruned from the model. In the case of back-off language models, this can be solved by calculating the difference of the logs between the n-gram estimate and the backoff estimate. However, a translation decoder generally functions differently. In our work, we will assume that the decoding will be performed using a Viterbi decoder, such as MOSES (Koehn et al., 2007), where the translation with the highest score is chosen. In the example above, where s=”John in Portugal” and t=”John em Portugal”, the decoder would choose the derivation with the highest probability from s to t. Using the unpruned model, the possible derivations are either using phrase p(s, t) or one element of its support set S1 , S2 or S3 . On the other hand, on the pruned model where p(s, t) does not exist, only S1 , S2 and S3 can be used. Thus, given a s, t pair one of three situations may occur. First, if the probability of the phrase pair p(s, t) is lower than the highest probability"
D12-1088,2005.mtsummit-papers.11,0,0.0197343,"del would eliminate p3 and keep p1 , yet the best decision could be to keep p3 and remove p1 , if p3 is also frequently used in derivations of other phrase pairs. Thus, we leave the problem of finding the best set of phrases to prune as future work. 5 Experiments We tested the performance of our system under two different environments. The first is the small scale DIALOG translation task for IWSLT 2010 evaluation (Paul et al., 2010) using a small corpora for the Chinese-English language pair (henceforth referred to as “IWSLT”). The second one is a large scale test using the complete EUROPARL (Koehn, 2005) corpora for the Portuguese-English language pair, which we will denote by “EUROPARL”. 5.1 Corpus The IWSLT model was trained with 30K training sentences. The development corpus and test corpus were taken from the evaluation dataset in IWSLT 2006 (489 tuning and 500 test sentences with 7 references). The EUROPARL model was trained using the EUROPARL corpora with approximately 1.3M sentence pairs, leaving out 1K sentences for tuning and another 1K sentences for tests. 5.2 Setup In the IWSLT experiment, word alignments were generated using an HMM model (Vogel et al., 1996), with symmetric poster"
D12-1088,2007.mtsummit-papers.22,0,0.468875,"Missing"
D12-1088,D09-1078,0,0.198151,"nsiderable negative impact on translation quality, or even positive one. This work computes the probability, named p-value, that the joint occurrence event of the source phrase s and target phrase t occurring in same sentence pair happens by chance, and are actually statistically independent. Phrase pairs that have a high p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their complexity. Significance-based pruning has also been successfully applied in language modeling in (Moore and Quirk, 2009). Our work has a similar objective, but instead of trying to predict the independence between the source and target phrases in each phrase pair, we attempt to predict the independence between a phrase pair and other phrase pairs in the model. 2.2 Relevance Pruning Another proposed approach (Matthias Eck and Waibel, 2007) consists at collecting usage statistics for phrase pairs. This algorithm decodes the training corpora and extracts the number of times each phrase pair is used in the 1-best translation hypothesis. Thus, phrase pairs that are rarely used during decoding are excluded first duri"
D12-1088,P03-1021,0,0.0751012,"Missing"
D12-1088,2010.iwslt-evaluation.1,0,0.0454513,"Missing"
D12-1088,2008.amta-srw.6,0,0.0613165,"hing in the space of possible translations, but in the space of possible derivations, which are sequences of phrase translations p1 (s1 , t1 ), ..., pn (sn , tn ) that can be applied to s to Qgenerate an output t with the score given by P (t) ni=1 P (si , ti ). Our algorithm to determine SP (p(s, t)) can be described as an adaptation to the decoding algorithm in Moses, where we restrict the search space to the subspace SP (p(s, t)), that is, our search space is only composed by derivations that output t, without using p itself. This can be done using the forced decoding algorithm proposed in (Schwartz, 2008). Secondly, the score of a given translation hypothesis does not depend on the language model probability P (t), since all derivations in this search space have the same t, thus we discard this probability from the score function. Finally, rather than using beam search, we exhaustively search all the search space, to reduce the hypothesis of incurring a search error at this stage. This is possible, since phrase pairs are generally smaller than text (less than 8 words), and because we are constraining the search space to t, which is an order of magnitude smaller than the reg967 ular search spac"
D12-1088,2009.mtsummit-papers.17,0,0.472482,"relevant reference in phrase table pruning is the work of (Johnson and Martin, 2007), where it is shown that a significant portion of the phrase table can be discarded without a considerable negative impact on translation quality, or even positive one. This work computes the probability, named p-value, that the joint occurrence event of the source phrase s and target phrase t occurring in same sentence pair happens by chance, and are actually statistically independent. Phrase pairs that have a high p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their complexity. Significance-based pruning has also been successfully applied in language modeling in (Moore and Quirk, 2009). Our work has a similar objective, but instead of trying to predict the independence between the source and target phrases in each phrase pair, we attempt to predict the independence between a phrase pair and other phrase pairs in the model. 2.2 Relevance Pruning Another proposed approach (Matthias Eck and Waibel, 2007) consists at collecting usage statistics for phrase pairs. This algorithm decodes the training"
D12-1088,J10-3007,1,0.896555,"Missing"
D12-1088,C96-2141,0,0.348977,"using the complete EUROPARL (Koehn, 2005) corpora for the Portuguese-English language pair, which we will denote by “EUROPARL”. 5.1 Corpus The IWSLT model was trained with 30K training sentences. The development corpus and test corpus were taken from the evaluation dataset in IWSLT 2006 (489 tuning and 500 test sentences with 7 references). The EUROPARL model was trained using the EUROPARL corpora with approximately 1.3M sentence pairs, leaving out 1K sentences for tuning and another 1K sentences for tests. 5.2 Setup In the IWSLT experiment, word alignments were generated using an HMM model (Vogel et al., 1996), with symmetric posterior constraints (V. Grac¸a et al., 2010), using the Geppetto toolkit2 . This setup was used in the official evaluation in (Ling et al., 2010). For the EUROPARL experiment the word alignments were generated using IBM model 4. In both experiments, the translation model was built using the phrase extraction algorithm (Paul et al., 2010), with commonly used features in Moses (Ex: probability, lexical weighting, lexicalized reordering model). The optimization of the translation model weights was done using MERT tuning (Och, 2003) and the results were evaluated using BLEU-4. 5"
D12-1088,2010.iwslt-papers.14,1,\N,Missing
D13-1008,P06-2005,0,0.119477,"Missing"
D13-1008,2005.iwslt-1.8,0,0.0618985,"Missing"
D13-1008,P05-1074,0,0.444565,"Missing"
D13-1008,P11-1131,0,0.0205445,"Missing"
D13-1008,J92-4003,0,0.0391747,"Missing"
D13-1008,J93-2003,0,0.0256659,"Missing"
D13-1008,W11-2107,0,0.0305025,"Missing"
D13-1008,P08-1115,0,0.0452721,"Missing"
D13-1008,N13-1073,1,0.862904,"Missing"
D13-1008,P11-1137,0,0.00905253,"Missing"
D13-1008,N13-1037,0,0.00849129,"Missing"
D13-1008,W11-2210,0,0.028087,"Missing"
D13-1008,P11-1038,0,0.106633,"Missing"
D13-1008,D12-1039,0,0.166445,"Missing"
D13-1008,N03-1017,0,0.0474429,"Missing"
D13-1008,P07-2045,1,0.0119874,"Missing"
D13-1008,P13-1018,1,0.283107,"Missing"
D13-1008,J03-1002,0,0.0062214,"Missing"
D13-1008,J04-4002,0,0.110137,"Missing"
D13-1008,P03-1021,0,0.0134582,"Missing"
D13-1008,P02-1040,0,0.105491,"Missing"
D13-1008,P11-1002,0,0.0235515,"Missing"
D13-1008,J03-3002,0,0.0406716,"Missing"
D13-1008,P12-1021,0,0.0203959,"Missing"
D13-1008,C96-2141,0,0.0951178,"Missing"
D13-1008,N13-1050,0,0.1235,"Missing"
D13-1008,W13-2515,0,0.541542,"Missing"
D13-1008,D13-1007,0,0.0725911,"Missing"
D13-1008,2010.iwslt-papers.14,1,\N,Missing
D15-1161,P14-2131,0,0.0497195,"uage Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA Instituto Superior T´ecnico, Lisbon, Portugal {lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu {ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt Abstract The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. We introduce an extension to the bag-ofwords model for learning words representations tha"
D15-1161,D14-1082,0,0.00523589,"local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextu"
D15-1161,P14-1129,0,0.0182429,"pic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the le"
D15-1161,D14-1012,0,0.0194663,"data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the left/right). The main intuition behind our model is that the prediction of a word is only dependent on certain words within the context. For instance, in the sentence We won the game! Nicely played!, the prediction of the word played, depends on both the syntactic relation from nicely, which narrows down the list of candidates to verbs, and on the semant"
D15-1161,P12-1092,0,0.059041,"e a large drop on this semantically oriented task. Our attention-based model, on the other hand, out performs all other models on syntax-based tasks, while maintaining a competitive score on semantic tasks. This is an encouraging result that shows that it is possible to learn representations that can perform well on both semantic and syntactic tasks. 4 Related Work Many methods have been proposed for learning word representations. Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word representation learning, no prior work that uses attention models exists to our knowledge. 5 Co"
D15-1161,D13-1176,0,0.00900077,"g global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted wor"
D15-1161,D14-1108,1,0.740924,"ction words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently"
D15-1161,N15-1144,1,0.0648252,"ver, this model does not scale well as b increases as it requires V × dw more parameters for each new word in the window. Finally, setting a good value for b is difficult as larger values may introduce a degenerative behavior in the model, as more effort is spent predicting words that are conditioned on unrelated words, while smaller values of b may lead to cases where the window size is not large enough include words that are semantically related. For syntactic tasks, it has been shown that increasing the window size can adversely impact in the quality of the embeddings (Bansal et al., 2014; Lin et al., 2015). 2.2 CBOW with Attention We present a solution to these problems while maintaining the efficiency underlying the bag-ofwords model, and allowing it to consider contextual words within the window in a non-uniform way. We first rewrite the context window c as: X c= ai (wi )wi (2) i∈[−b,b]−{0} (1) where Oc corresponds to the projection of the context vector c onto the vocabulary V and v is a one-hot representation. For larger vocabularies it is inefficient to compute the normalizer P > v∈V exp v Oc. Solutions for problem are using the hierarchical softmax objective function (Mikolov et al., 2013"
D15-1161,N15-1142,1,0.386366,"isbon, Portugal Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA Instituto Superior T´ecnico, Lisbon, Portugal {lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu {ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt Abstract The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. We introduce an extension to the bag-ofwords model for learning wo"
D15-1161,P14-1140,0,0.0283868,"Missing"
D15-1161,D07-1043,0,0.00495254,"embeddings in the domain of part-of-speech tagging in both supervised (Ling et al., 2015b) and unsupervised tasks (Lin et al., 2015). This later task is newly proposed, but we argue that success in it is a compelling demonstration of separation of words into syntactically coherent clusters. Part-of-speech induction. The work in (Lin et al., 2015) attempts to infer POS tags with a standard bigram hmm, which uses word embeddings to infer POS tags without supervision. We use the same dataset, obtained from the ConLL 2007 shared task (Nivre et al., 2007) Scoring is performed using the V-measure (Rosenberg and Hirschberg, 2007), which is used to predict syntactic classes at the word level. It has been shown in (Lin et al., 2015) that word embeddings learnt from structured skip-ngrams tend to work better at this task, mainly because it is less sensitive to larger window sizes. These results are consistent with our observations found in Table 1, in rows “Skip-ngram” and “SSkip-ngram”. We can observe that our attention based CBOW model (row “CBOW Attention”) improves over these results for both tasks and also the original CBOW model (row “CBOW”). 1369 CBOW Skip-ngram SSkip-ngram CBOW Attention POS Induction 50.40 33.86"
D15-1161,D13-1170,0,0.00265235,"sented in (Ling et al., 2015b) using the same hyper-parameters. Results on the POS accuracy on the test set are reported on Table 1. We can observe our model can obtain similar results compared to the structured skip-ngram model on this task, while training the model is significantly faster. The gap between the usage of different embeddings is not as large as in POS induction, as this is a supervised task, where pre-training generally leads to smaller improvements. 3.3 Semantic Evaluation To evaluate the quality of our vectors in terms of semantics, we use the sentiment analysis task (Senti) (Socher et al., 2013), which is a binary classification task for movie reviews. We simply use the mean of the word vectors of words in a sentence, and use them as features in an `2 regularized logistic regression classifier. We use the standard training/dev/test split and report accuracy on the test set in table 1. We can see that in this task, our models do not perform as well as the CBOW and Skipngram model, which hints that our model is learning embeddings that learn more towards syntax. This is expected as it is generally uncommon for embeddings to outperform existing models on both syntactic and semantic task"
D15-1161,P10-1040,0,0.138833,"better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the left/right). The main intuition behind our model is that the prediction of a word is only dependent on certain words within the context. For instance, in the sentence We won the game! Nicely played!, the prediction of the word played, depen"
D15-1161,D15-1176,1,\N,Missing
D15-1161,D07-1096,0,\N,Missing
D15-1176,afonso-etal-2002-floresta,0,0.054909,"., 2001; Mueller et al., 2013). We now show that some of these features can be learnt automatically using our model. eat embedings for words words1 .This 5 cats NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous representations and the same hyperparametrization as in language modeling (Section 4) is used. Additionally, we also compare to the convolutional model of Santos and Zadrozny (2014), which also requires the dimensionality for characters and the wor"
D15-1176,D15-1041,1,0.487282,"eling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from being sensitive to lexical aspects within words, as it takes characters as atomic units to derive the embeddings for the word. On POS tagging, our models using characters alone can still achieve comparable or better results than state-of-the-art systems, without the need to manually engineer such lexical features. Although both language modeling and POS tagging both benefit strongly from morphological cues, the success of our models in languages with impover"
D15-1176,D13-1008,1,0.162387,"susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regul"
D15-1176,N15-1142,1,0.116424,"y are not found labelled in the training data, the model would be dependent on context to determine their tags, which could lead to errors in ambiguous contexts. Unsupervised training methods such as the Skip-n-gram model (Mikolov et al., 2013) can be used to pretrain the word representations on unannotated corpora. If such pretraining places cat, dog and snake near each other in vector space, and the supervised POS data contains evidence that cat and dog are nouns, our model will be likely to label snake with the same tag. We train embeddings using English wikipedia with the dataset used in (Ling et al., 2015), and the Structured Skip-n-gram model. Results using pre-trained word lookup tables and the C2W with the pre-trained word lookup tables as additional parameters are shown in rows “word(sskip)” and “C2W + word(sskip)”. We can observe that both systems can obtain improvements over their random initializations (rows “word” and (C2W)). Finally, we also found that when using the C2W model in conjunction pre-trained word embeddings, that adding a non-linearity to the representations extracted from the C2W model eC w improves the results over using a simple linear trans+feat no no yes yes yes yes no"
D15-1176,P14-1140,0,0.0196322,"tors and Wordless Word Vectors It is commonplace to represent words as vectors. In contrast to na¨ıve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P ∈ Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w ∈ V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w , that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup d table and we shall denote by eW w ∈ R the embedding obtained from a word lookup table for w as eW w = P · 1w . This allows tasks with low amounts of annotated data to be trained jointly with other tasks with"
D15-1176,W13-3512,0,0.437991,"are not independent. The wellknown “past tense debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have rel"
D15-1176,D14-1082,0,0.0447531,"Word Vectors It is commonplace to represent words as vectors. In contrast to na¨ıve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P ∈ Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w ∈ V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w , that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup d table and we shall denote by eW w ∈ R the embedding obtained from a word lookup table for w as eW w = P · 1w . This allows tasks with low amounts of annotated data to be trained jointly with other tasks with large amounts of data an"
D15-1176,J93-2004,0,0.0572265,"ls Experiments: Part-of-speech Tagging As a second illustration of the utility of our model, we turn to POS tagging. As morphology is a strong indicator for syntax in many languages, a much effort has been spent engineering features (Nakagawa et al., 2001; Mueller et al., 2013). We now show that some of these features can be learnt automatically using our model. eat embedings for words words1 .This 5 cats NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous r"
D15-1176,D09-1078,0,0.0117625,"from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from b"
D15-1176,D13-1176,0,0.0372417,"onclude in Section 7. 2 Word Vectors and Wordless Word Vectors It is commonplace to represent words as vectors. In contrast to na¨ıve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P ∈ Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w ∈ V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w , that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup d table and we shall denote by eW w ∈ R the embedding obtained from a word lookup table for w as eW w = P · 1w . This allows tasks with low amounts of annotated data to be trained jointly wit"
D15-1176,kang-choi-2000-automatic,0,0.0359805,"tion according to the lexical model. Convolutional model are less susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be r"
D15-1176,D15-1278,0,0.049515,"Missing"
D15-1176,D12-1088,1,0.613165,"Missing"
D15-1176,D13-1032,0,0.0412355,"Missing"
D15-1176,P11-2009,0,0.0797599,"Missing"
D15-1176,N15-1186,0,0.0289151,"se debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have relatively regular effects, many words with lexical"
D15-1176,E09-1087,0,0.00853859,"Missing"
D15-1176,W13-2515,0,0.0187849,"al model are less susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence ou"
D15-1176,D12-1089,0,0.00506125,"xically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from being sensitive to lexical aspects within words, as it takes characters as atomic un"
D15-1176,D14-1179,0,\N,Missing
D15-1176,W03-2405,0,\N,Missing
D15-1243,P12-1015,0,0.0404941,"i et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 Semantic Evaluation Benchmarks We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks. Word Similarity. We use three different benchmarks to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the MEN dataset (Bruni et al., 2012) of 3,000 words pairs sampled from words that occur at least 700 times in a large web corpus. The third dataset is SimLex-999 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Eac"
D15-1243,J90-1003,0,0.350901,"which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the vector space.5 Latent Semantic Analysis (LSA). We construct word-word co-occurrence matrix X; every element in the matrix is the pointwise mutual information between the two words (Church and Hanks, 1990). Then, truncated singular value decomposition is applied to factorize X, where we keep the k largest singular values. Low dimensional word vectors of dimension k are obtained from Uk where X ≈ Uk ΣVk T (Landauer and Dumais, 1997). 3 https://code.google.com/p/word2vec 4 https://github.com/wlin12/wang2vec 5 http://www-nlp.stanford.edu/projects/ glove/ GloVe+WN, GloVe+PPDB, LSA+WN, LSA+PPDB. We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 Semant"
D15-1243,W06-1670,0,0.0317102,"Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD fish duck chicken NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0."
D15-1243,P14-5004,1,0.611603,"sis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2 -regularized logistic regression classifier. Finally, we evaluate vectors on the metaphor detection (Metaphor) (Tsvetkov et al., 6 https://github.com/mfaruqui/ retrofitting 7 We employ an implementation of a suite of word similarity tasks at wordvectors.org (Faruqui and Dyer, 2014). 8 http://qwone.com/~jason/20Newsgroups 2051 2014a).9 The system uses word vectors as features in a random forest classifier to label adjective-noun pairs as literal/metaphoric. We report the system accuracy in 5-fold cross validation. 5 Results To test the efficiency of QVEC in capturing the semantic content of word vectors, we evaluate how well QVEC’s scores correspond to the scores of word vector models on semantic benchmarks. We compute the Pearson’s correlation coefficient r to quantify the linear relationship between the scorings. We begin with comparison of QVEC with one extrinsic task"
D15-1243,P15-2076,1,0.309428,"Missing"
D15-1243,N15-1184,1,0.864444,"Missing"
D15-1243,N13-1092,0,0.0368039,"Missing"
D15-1243,D14-1012,0,0.0362549,"of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequent"
D15-1243,J15-4004,0,0.243621,"Missing"
D15-1243,N15-1142,1,0.522248,"VEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD 2 VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD 2 VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD 2 VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word"
D15-1243,H93-1061,0,0.140934,"our model obtains high correlation (0.34 ≤ r ≤ 0.89) with the extrinsic tasks (§5). 2 Linguistic Dimension Word Vectors The crux of our evaluation method lies in quantifying the similarity between a distributional word vector model and a (gold-standard) linguistic re2049 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb le"
D15-1243,I08-2105,0,0.0125447,"thods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD fish duck chicken NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0.67 ··· VB . MOTI"
D15-1243,D14-1162,0,0.112475,"Missing"
D15-1243,D15-1036,0,0.233502,"Missing"
D15-1243,N13-1076,1,0.805767,"Missing"
D15-1243,D13-1170,0,0.0376388,"ed lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequently, it is not clear how to score a non-interp"
D15-1243,P14-1024,1,0.829115,"Missing"
D15-1243,tsvetkov-etal-2014-augmenting-english,1,0.809802,"Missing"
D15-1243,P10-1040,0,0.162082,"f word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at htt"
D15-1243,P14-1074,0,0.0118254,"9 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Each task involves categorizing a document according to two related categories with training/dev/test split in accordance with Yogatama and Smith (2014). For example, a classification task is between two categories of Sports: baseball vs hockey. We report the average classification accuracy across the four tasks. Our next downstream semantic task is the sentiment analysis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2 -regularized logistic regression classif"
D15-1243,D13-1196,0,0.0612772,"nexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguisti"
D15-1243,D15-1161,1,0.0792052,"VEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD 2 VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD 2 VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD 2 VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word"
D15-1243,P14-2131,0,\N,Missing
D16-1116,D15-1138,0,0.0747526,"Missing"
D16-1116,P13-2009,0,0.013306,"bes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution"
D16-1116,Q13-1005,0,0.117977,"rsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address dat"
D16-1116,D14-1134,0,0.0245851,"Missing"
D16-1116,P14-1133,0,0.0415207,"onment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue. Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first appro"
D16-1116,P16-1004,0,0.63439,"e prediction of a query on the G EO corpus which is a frequently used benchmark for semantic parsing. The corpus contains 880 questions about US geography together with executable queries representing those questions. We follow the approach established by Zettlemoyer and Collins (2005) and split the corpus into 600 training and 280 test cases. Following common practice, we augment the dataset by referring to the database during training and test time. In particular, we use the database to identify and anonymise variables (cities, states, countries and rivers) following the method described in Dong and Lapata (2016). Most prior work on the G EO corpus relies on standard semantic parsing methods together with custom heuristics or pipelines for this corpus. The recent paper by Dong and Lapata (2016) is of note, as it uses a sequence-to-sequence model for training which is the unidirectional equivalent to S2S, and also to the decoder part of our S EQ 4 network. 3.2 Open Street Maps The second task we tackle with our model is the NL MAPS dataset by Haas and Riezler (2016). The dataset contains 1,500 training and 880 testing instances of natural language questions with corresponding machine readable queries o"
D16-1116,N04-1035,0,0.0239608,"n the higher percentages then too little unsupervised data to meaningfully improve the model. 6 Related Work Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016"
D16-1116,N16-1088,0,0.19687,"etrisation trick of Kingma and Welling (2014). Rather than drawing a discrete symbol in Σx from a softmax, we draw a distribution over symbols from a logistic-normal distribution at each time step. These serve as continuous relaxations of discrete samples, providing a differentiable estimator of the expected reconstruction log likelihood. We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the G EO Q UERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al., 2006) and the Natural Language Querying corpus (Haas and Riezler, 2016) on OpenStreetMap. As part of our evaluation, we introduce simple mechanisms for generating large amounts of unsupervised training data for two of these tasks. In most settings, the semi-supervised model outperforms the supervised model, both when trained on additional generated data as well as on subsets of the existing data. 1078 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1078–1087, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Dataset Example G EO what are the high points of states surrounding mississip"
D16-1116,P16-1002,0,0.406241,"objects, etc). Paths within that maze are created by randomly sampling start and end positions. 4 Model Experiments We evaluate our model on the three tasks in multiple settings. First, we establish a supervised baseline to compare the S2S model with prior work. Next, we 5 Our randomly generated unsupervised datasets can be downloaded from http://deepmind.com/ publications 1082 Accuracy Zettlemoyer and Collins (2005) Zettlemoyer and Collins (2007) Liang et al. (2013) Kwiatkowski et al. (2011) Zhao and Huang (2014) Kwiatkowski et al. (2013) 79.3 86.1 87.9 88.6 88.9 89.0 Dong and Lapata (2016) Jia and Liang (2016)6 84.6 89.3 S2S S EQ 4 86.5 87.3 Table 2: Non-neural and neural model results on G EO Q UERY using the train/test split from (Zettlemoyer and Collins, 2005). train our S EQ 4 model in a semi-supervised setting on the entire dataset with the additional monomodal training data described in the previous section. Finally, we perform an “ablation” study where we discard some of the training data and compare S2S to S EQ 4. S2S is trained solely on the reduced data in a supervised manner, while S EQ 4 is once again trained semi-supervised on the same reduced data plus the machine readable part of the"
D16-1116,C12-1083,1,0.256175,"nsupervised data to meaningfully improve the model. 6 Related Work Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free gram"
D16-1116,D12-1040,0,0.0355422,"Missing"
D16-1116,P13-1022,0,0.0405195,"Missing"
D16-1116,D11-1140,0,0.063695,"6 Related Work Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), wh"
D16-1116,D13-1161,0,0.036316,"Missing"
D16-1116,P11-1060,0,0.0545052,"and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue. Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first approach to using effectively discretised sequential information as the latent representation without resorting to dracon"
D16-1116,J13-2005,0,0.0543262,"Missing"
D16-1116,Q16-1017,0,0.0270309,"from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue. Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first approach to using effectively discretised sequential information as the latent representation without resorting to draconian assumptions (Ammar et al., 2014) to make marginalisation tractable. While our model is not exactly marginalisable either, the continuous relaxation makes training far more tractable. A related idea was recently presented in G¨ulc¸ehre et al. (2015), who use monolingual data to improve machine translation by fusing a sequence-to-sequence model and a language model. 7 Conclusion We described a method for augmenting a supervised sequence t"
D16-1116,Q14-1030,0,0.0168947,"all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. Th"
D16-1116,N06-1056,0,0.581285,"inalisation, we introduce a novel differentiable alternative for draws from a softmax which can be used with the reparametrisation trick of Kingma and Welling (2014). Rather than drawing a discrete symbol in Σx from a softmax, we draw a distribution over symbols from a logistic-normal distribution at each time step. These serve as continuous relaxations of discrete samples, providing a differentiable estimator of the expected reconstruction log likelihood. We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the G EO Q UERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al., 2006) and the Natural Language Querying corpus (Haas and Riezler, 2016) on OpenStreetMap. As part of our evaluation, we introduce simple mechanisms for generating large amounts of unsupervised training data for two of these tasks. In most settings, the semi-supervised model outperforms the supervised model, both when trained on additional generated data as well as on subsets of the existing data. 1078 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1078–1087, c Austin, Texas, November 1-5, 2016. 201"
D16-1116,D07-1071,0,0.82963,"Missing"
D16-1116,P15-1109,0,0.0223581,"tive model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms. 1 Introduction Neural approaches, in particular attention-based sequence-to-sequence models, have shown great promise and obtained state-of-the-art performance for sequence transduction tasks including machine translation (Bahdanau et al., 2015), syntactic constituency parsing (Vinyals et al., 2015), and semantic role labelling (Zhou and Xu, 2015). A key requirement for effectively training such models is an abundance of supervised data. In this paper we focus on learning mappings from input sequences x to output sequences y in domains where the latter are easily obtained, but annotation in the form of (x, y) pairs is sparse or expensive to produce, and propose a novel architecture that accommodates semi-supervised training on sequence transduction tasks. To this end, we augment the transduction objective (x 7→ y) with an autoencoding objective where the input sequence is treated as a latent variable (y 7→ x 7→ y), enabling training fr"
D16-1116,N16-1160,0,\N,Missing
D17-1197,P16-1154,0,0.231329,"an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step (Wen et al., 2015; Luong et al., 2015). Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of natural language reference. Figure 1 depicts examples of REs in the context of the three tasks that we consider in this work. First, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe gener"
D17-1197,P16-1014,0,0.0547892,"Missing"
D17-1197,N10-1061,0,0.034711,"orks on task oriented dialogues embed the seq2seq model in traditional dialogue systems, in which the table query part is not differentiable, while our model queries the database directly. Recipe generation was proposed in (Kiddon et al., 2016). They use attention mechanism over the checklists, whereas our work models explicit references to them. Context dependent language models (Mikolov et al., 2010; Jozefowicz et al., 2016; Mikolov et al., 2010; Ji et al., 2015; Wang and Cho, 2015) are proposed to capture long term dependency of text. There are also lots of works on coreference resolution (Haghighi and Klein, 2010; Wiseman et al., 2016). We are the first to combine coreference with language modeling, to the best of our knowledge. 5 Conclusion We introduce reference-aware language models which explicitly model the decision of from where to generate the token at each step. Our model can also learns the decision by treating it as a latent variable. We demonstrate on three applications, table based dialogue modeling, recipe generation and coref based LM, that our model performs better than attention based model, which does not incorporate this decision explicitly. There are several directions to explore fu"
D17-1197,D16-1032,0,0.126535,"made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of natural language reference. Figure 1 depicts examples of REs in the context of the three tasks that we consider in this work. First, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently refer to these items. As shown in Figure 1, in the recipe “Blend soy milk and . . . ”, soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user’s query (Young et al., 2013; Li et al., 2016; Vinyals and Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes and Weston, 2016; Wi"
D17-1197,D16-1127,0,0.0690347,"text of the three tasks that we consider in this work. First, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently refer to these items. As shown in Figure 1, in the recipe “Blend soy milk and . . . ”, soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user’s query (Young et al., 2013; Li et al., 2016; Vinyals and Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes and Weston, 2016; Williams and Zweig, 2016; Shang et al., 2015; Wen et al., 2016). Here we consider the domain of restaurant recommendation where a system refers to restaurants (name) and their attributes (address, phone number etc) in its responses. When the system 1850 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1850–1859 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics says “the nirala is a nice restaurant”,"
D17-1197,P16-1057,1,0.749091,"Missing"
D17-1197,P15-1002,0,0.0118737,"think…Go ahead [Linda]2 … thanks goes to [you]1 … Figure 1: Reference-aware language models. Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step (Wen et al., 2015; Luong et al., 2015). Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of na"
D17-1197,P15-1152,0,0.0333048,"Missing"
D17-1197,N15-1020,0,0.0273631,"rst, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently refer to these items. As shown in Figure 1, in the recipe “Blend soy milk and . . . ”, soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user’s query (Young et al., 2013; Li et al., 2016; Vinyals and Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes and Weston, 2016; Williams and Zweig, 2016; Shang et al., 2015; Wen et al., 2016). Here we consider the domain of restaurant recommendation where a system refers to restaurants (name) and their attributes (address, phone number etc) in its responses. When the system 1850 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1850–1859 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics says “the nirala is a nice restaurant”, it refers to the restaurant name the nirala from the database."
D17-1197,D15-1199,0,0.0573329,"ontext coref [I]1 think…Go ahead [Linda]2 … thanks goes to [you]1 … Figure 1: Reference-aware language models. Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step (Wen et al., 2015; Luong et al., 2015). Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key"
D17-1197,N16-1114,0,0.015614,"ogues embed the seq2seq model in traditional dialogue systems, in which the table query part is not differentiable, while our model queries the database directly. Recipe generation was proposed in (Kiddon et al., 2016). They use attention mechanism over the checklists, whereas our work models explicit references to them. Context dependent language models (Mikolov et al., 2010; Jozefowicz et al., 2016; Mikolov et al., 2010; Ji et al., 2015; Wang and Cho, 2015) are proposed to capture long term dependency of text. There are also lots of works on coreference resolution (Haghighi and Klein, 2010; Wiseman et al., 2016). We are the first to combine coreference with language modeling, to the best of our knowledge. 5 Conclusion We introduce reference-aware language models which explicitly model the decision of from where to generate the token at each step. Our model can also learns the decision by treating it as a latent variable. We demonstrate on three applications, table based dialogue modeling, recipe generation and coref based LM, that our model performs better than attention based model, which does not incorporate this decision explicitly. There are several directions to explore further based on our fram"
I11-1006,2005.iwslt-1.8,0,0.117751,"are small, but per2 Lexicalized Reordering models In this section we will present the lexicalized reordering models approaches that are relevant for this work. 47 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 47–55, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2.1 Word-based Reordering prev word(s) The lexicalized reordering model is possibly the most used lexicalized reordering model and it calculates, as features, the reordering orientation for the previous and the next word, for each phrase pair. In the word-based reordering model (Axelrod et al., 2005), during the phrase extraction, given a source sentence S and a target sentence T , the alignment set A, where aji is an alignment from i to j, the phrase pair with words in positions between i and j in S, Sij , and n and m in T , Tnm , can be classified with one of three orientations with respect to the previous word. The orientation is a) source phrase next word(s) target phrase n−1 n−1 • Pc (p, S) = Wj+1 (1 − Wi−1 ) n−1 n−1 • Pc (p, D) = Wi−1 Wj+1 n−1 n−1 + (1 − Wi−1 )(1 − Wj+1 ) (1) 2.3 Phrase-based Reordering The problem with the word-based lexicalized reordering model is that it is assum"
I11-1006,P08-1115,0,0.0496024,"Missing"
I11-1006,D08-1089,0,0.366999,"gnment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). More recently, a more efficient representation of multiple alignments was proposed in (Liu et al., 2009) named weighted alignment matrices, which represents the alignment probability distribution over the words of each parallel sentence. The method for building a word-based lexicalized reordering model using these matrices is proposed in (Ling et al., 2011). However, phrase-based reordering models have been shown to perform better than word-based models for several language pairs (Tillmann, 2004; Su et al., 2010; Galley and Manning, 2008), such as Chinese-English and Arabic-English. Lexicalized reordering models play a central role in phrase-based statistical machine translation systems. Starting from the distance-based reordering model, improvements have been made by considering adjacent words in word-based models, adjacent phrases pairs in phrasebased models, and finally, all phrases pairs in a sentence pair in the reordering graphs. However, reordering graphs treat all phrase pairs equally and fail to weight the relationships between phrase pairs. In this work, we propose an extension to the reordering models, named weighte"
I11-1006,D09-1106,0,0.0357024,"Missing"
I11-1006,P08-1023,0,0.0619174,"Missing"
I11-1006,2010.iwslt-evaluation.1,0,0.0498423,"Missing"
I11-1006,P10-2003,0,0.193049,"Missing"
I11-1006,N04-4026,0,0.553008,"heses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). More recently, a more efficient representation of multiple alignments was proposed in (Liu et al., 2009) named weighted alignment matrices, which represents the alignment probability distribution over the words of each parallel sentence. The method for building a word-based lexicalized reordering model using these matrices is proposed in (Ling et al., 2011). However, phrase-based reordering models have been shown to perform better than word-based models for several language pairs (Tillmann, 2004; Su et al., 2010; Galley and Manning, 2008), such as Chinese-English and Arabic-English. Lexicalized reordering models play a central role in phrase-based statistical machine translation systems. Starting from the distance-based reordering model, improvements have been made by considering adjacent words in word-based models, adjacent phrases pairs in phrasebased models, and finally, all phrases pairs in a sentence pair in the reordering graphs. However, reordering graphs treat all phrase pairs equally and fail to weight the relationships between phrase pairs. In this work, we propose an exten"
I11-1006,J10-3007,1,0.878594,"Missing"
I11-1006,N03-1017,0,\N,Missing
I11-1006,P11-2079,1,\N,Missing
I11-1006,2010.iwslt-papers.14,1,\N,Missing
I11-1006,2008.amta-papers.18,0,\N,Missing
J16-2005,W10-0710,0,0.108835,"lations within the same document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter Finally, crowdsourcing techniques can be used to obtain translations of text from new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quality control. Thus, in order to find good translations, subsequent post-editing and/or ranking is generally necessary. 3. The Intra-Document Alignment (IDA) Model As discussed above, conte"
J16-2005,W12-2108,0,0.0274338,"weets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but because of the volumes of data involved, we can find a large amount of content. We must efficiently detect multilingual tweets, as monolingual tweets do not contain translations. Although language identification is a well-studied problem (Zissman 1996; Gottron and Lipka 2010; Greenhill 2011), even in the microblog domain (Bergsma et al. 2012), these assume that only one language is present within a document and cannot be directly applied to this problem. Furthermore, because of the magnitude of tweets that must be processed, many of the proposed solutions cannot be applied due to their computational complexity. In our work, we propose an efficient implementation for large-scale detection of multilingual documents. Crowdsourcing for parallel data extraction - To tune the parameters of our parallel data extractor and perform MT experiments, user-verified annotations must be obtained. To obtain this, we propose a simple crowdsourcing"
J16-2005,C10-2010,0,0.0214145,"the translation of xai . Model 1 naively assigns a uniform prior probability to all alignment configurations. Although this is an obviously flawed assumption, the posterior alignment probability under Model 1 (i.e., PM1 (a |x, y)) is surprisingly informative. More robust models make less-naive prior assumptions and generally produce higher-quality alignments, but the uniform prior probability assumption simplifies the complexity of performing inference. Despite its simplicity, Model 1 has shown particularly good performance as a component in sentence alignment systems (Xu, Zens, and Ney 2005; Braune and Fraser 2010). Some work on parallel data extraction has also focused on extracting parallel segments from comparable corpora (Smith, Quirk, and Toutanova 2010; Munteanu, Fraser, and Marcu 2004). Smith et al. (2010) uses conditional random fields to identify parallel segments from comparable Wikipedia documents (since Wikipedia documents in multiple languages are not generally translations of each other, although they are about the same topics). The work of Jehl, Hieber, and Riezler (2012) uses cross-lingual information retrieval techniques to extract candidate English–Arabic translations from Twitter. The"
J16-2005,J93-2003,0,0.116606,"Missing"
J16-2005,N13-1073,1,0.853048,"Missing"
J16-2005,2014.iwslt-papers.7,0,0.0308749,"also possible to focus the extraction in one particular type of phenomena. For example, the work on mining parenthetical translations (Lin et al. 2008), which attempts to find translations within the same document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter Finally, crowdsourcing techniques can be used to obtain translations of text from new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quali"
J16-2005,W06-1008,0,0.0705046,"Missing"
J16-2005,P91-1023,0,0.541067,"n features - In informal domains, there are many terms that are not translated, such as hashtags (e.g., #twitter), at mentions (e.g., @NYC), numbers, and people’s names. The presence of such repeated terms in the same tweet can be a strong cue for detecting translations. Hence, we define features that trigger if a given word type occurs in a pair within a tweet. The word types considered are hashtags, at mentions, numbers, and words beginning with capital letters. Length feature - It has been known that the length differences between parallel sentences can be modeled by a normal distribution (Gale and Church 1991). Thus, we used parallel training data (used to train the alignment model) in the respective language pair to determine (µ˜ , σ˜ 2 ), which lets us calculate the likelihood of two hypothesized segments being parallel. For each language pair s, t, we train separate classifiers for each language pair on annotated parallel data Dgold (s, t). The method used to obtain the necessary annotations is described in Section 5. Intrinsic evaluation. The quality of the classifier can be determined in terms of precision and recall. We count one as a true positive (tp) if we correctly identify a parallel twe"
J16-2005,P11-2008,0,0.0478665,"Missing"
J16-2005,J11-4003,0,0.0223833,"te pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but because of the volumes of data involved, we can find a large amount of content. We must efficiently detect multilingual tweets, as monolingual tweets do not contain translations. Although language identification is a well-studied problem (Zissman 1996; Gottron and Lipka 2010; Greenhill 2011), even in the microblog domain (Bergsma et al. 2012), these assume that only one language is present within a document and cannot be directly applied to this problem. Furthermore, because of the magnitude of tweets that must be processed, many of the proposed solutions cannot be applied due to their computational complexity. In our work, we propose an efficient implementation for large-scale detection of multilingual documents. Crowdsourcing for parallel data extraction - To tune the parameters of our parallel data extractor and perform MT experiments, user-verified annotations must be obtaine"
J16-2005,P11-1038,0,0.0259193,"till with me or what?) and nonstandard abbreviations (idk! smh). Automated language processing tools (e.g., those that perform linguistic analysis or translation) face particular difficulty with this new kind of content. On one hand, these have been developed with the conventions of more edited genres in mind. For example, they often make strong assumptions about orthographic and lexical uniformity (e.g., that there is just one way to spell you, and that cool, cooool, and cooooool represent completely unrelated lexical items). While modeling innovations are helping to relax these assumptions (Han and Baldwin 2011; Ritter et al. 2012; Owoputi et al. 2013; Ling et al. 2015), a second serious challenge is that many of the annotated text resources that our tools are learned from are drawn from edited genres, and poor generalization from edited to user-generated genres is a major source of errors (Gimpel et al. 2011; Kong et al. 2014). In this work, we present methods for finding naturally occurring parallel data on social media sites that is suitable for training machine translation (MT) systems. In MT, the domain mismatch problem is quite acute because most existing sources of parallel data are governmen"
J16-2005,W12-3153,0,0.0424014,"Missing"
J16-2005,D07-1103,0,0.0213949,"hus, for each language, we extract all data from Wikipedia up to a limit of 100K lines in order to keep the model compact. 7.1.3 Translation Lexicons. The IDA model uses translation lexicons to determine the translation score, as described in Section 3.1.1, which are estimated using parallel corpora. More specifically, we use the aligner described in Dyer, Chahuneau, and Smith (2013) to obtain the bidirectional alignments from the parallel sentences. Afterwards, we intersect the bidirectional alignments to obtain sure alignment points. Finally, we prune the lexicon using significance pruning (Johnson et al. 2007) with the threshold α +  (as defined in that work). The intersection and the pruning are performed to reduce the size of the lexicon to make the look-up faster. The breakdown of the different lexicons built is shown in Table 1. 7.2 Building Gold Standards To train and test the classifier described in Section 4.3, and perform MT experiments, a corpus of annotated tweets is needed for different language pairs. Table 2 summarizes the annotated corpora for the two domains (column Source) and the different language pairs (column Language Pair). We also report the method used to obtain the annotati"
J16-2005,D14-1108,1,0.842008,"hey often make strong assumptions about orthographic and lexical uniformity (e.g., that there is just one way to spell you, and that cool, cooool, and cooooool represent completely unrelated lexical items). While modeling innovations are helping to relax these assumptions (Han and Baldwin 2011; Ritter et al. 2012; Owoputi et al. 2013; Ling et al. 2015), a second serious challenge is that many of the annotated text resources that our tools are learned from are drawn from edited genres, and poor generalization from edited to user-generated genres is a major source of errors (Gimpel et al. 2011; Kong et al. 2014). In this work, we present methods for finding naturally occurring parallel data on social media sites that is suitable for training machine translation (MT) systems. In MT, the domain mismatch problem is quite acute because most existing sources of parallel data are governmental, religious, or commercial, which are quite different from usergenerated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also"
J16-2005,I08-2120,0,0.190188,"he elaboration of this work, we made the following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are"
J16-2005,P08-1113,0,0.0607684,"Missing"
J16-2005,D13-1008,1,0.90112,"e quite different from usergenerated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014). Our method is inspired by the (perhaps surprising) observation that a reasonable number of microblog users tweet “in parallel” in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English–Chinese parallel messages, for example, watup Kenny !!, where an English message and its Chinese Mayne!! - Kenny Mayne translation are in the same post, separated by a dash. It is not only celebrities (or 308 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter their publ"
J16-2005,D15-1176,1,0.850166,"Missing"
J16-2005,W14-3356,1,0.854061,"parallel segment is correct. If so, the material is extracted, otherwise it is discarded. This research is an extension of the preliminary work described in Ling et al. (2013), in which we obtained over 1 million Chinese–English parallel segments from Sina Weibo, using only their public application program interface (API). This automatically extracted parallel data yielded substantial translation quality improvements in translating microblog text and modest improvements in translating edited news. Following this work, we developed a method for crowdsourcing judgments about parallel segments (Ling et al. 2014), which was then used to build gold standard data for other language pairs and for the Twitter domain. This article extends these two papers in several ways: r 310 Improved language pair detection - The previous work assumes that the language pair is formed by two languages with different unicode ranges, such as English–Chinese, and does not support the extraction of parallel data if the languages share the same unicode range (such as English–Portuguese). This issue is addressed in this article, where we present a novel approach for finding multilingual tweets. Ling et al. r r Mining Parallel"
J16-2005,P13-1018,1,0.731322,"e quite different from usergenerated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014). Our method is inspired by the (perhaps surprising) observation that a reasonable number of microblog users tweet “in parallel” in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English–Chinese parallel messages, for example, watup Kenny !!, where an English message and its Chinese Mayne!! - Kenny Mayne translation are in the same post, separated by a dash. It is not only celebrities (or 308 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter their publ"
J16-2005,Q14-1003,0,0.0561496,"Missing"
J16-2005,J05-4003,0,0.318213,"the appropriately aligned parallel segments. Obviously, Ds,t will contain a considerable number of non-parallel segments, as multilingual messages in Tmult are not guaranteed to contain translated material. Furthermore, we must also consider errors from misalignments of the IDA model and misclassifications of the multilingual message detector. Thus, in order to identify messages that are actually parallel, a final identification step is necessary. 4.3 Identification Given a candidate sentence pair (s, t), many existing methods for detecting parallel data can be applied (Resnik and Smith 2003; Munteanu and Marcu 2005), as this problem becomes a regular unstructured bitext identification problem. In our initial work (Ling et al. 2013), we simply defined a threshold τ on the IDA model score, which was determined empirically. To obtain better results we train a logistic regression classifier for each language pair, similar to that presented in Munteanu and Marcu (2005), which detects whether two segments are parallel in a given language pair by looking at features of the candidate pair. Training is performed to maximize the classification decisions on annotated candidate pairs. 324 Ling et al. Mining Parallel"
J16-2005,N04-1034,0,0.0600954,"Missing"
J16-2005,P03-1021,0,0.0143381,"Missing"
J16-2005,N13-1039,1,0.773659,"regarding existing languages allows the detector to estimate the language probabilities more accurately. As we are using a character trigram model, a large amount of data is not required to saturate the model probabilities. Thus, for each language, we extract all data from Wikipedia up to a limit of 100K lines in order to keep the model compact. 7.1.3 Translation Lexicons. The IDA model uses translation lexicons to determine the translation score, as described in Section 3.1.1, which are estimated using parallel corpora. More specifically, we use the aligner described in Dyer, Chahuneau, and Smith (2013) to obtain the bidirectional alignments from the parallel sentences. Afterwards, we intersect the bidirectional alignments to obtain sure alignment points. Finally, we prune the lexicon using significance pruning (Johnson et al. 2007) with the threshold α +  (as defined in that work). The intersection and the pruning are performed to reduce the size of the lexicon to make the look-up faster. The breakdown of the different lexicons built is shown in Table 1. 7.2 Building Gold Standards To train and test the classifier described in Section 4.3, and perform MT experiments, a corpus of annotated"
J16-2005,P02-1040,0,0.0967467,"Missing"
J16-2005,P14-2110,0,0.0370301,"Missing"
J16-2005,W12-3152,0,0.0412989,"Missing"
J16-2005,J03-3002,0,0.467405,"s many challenges to current NLP and MT methods. As part of the elaboration of this work, we made the following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient"
J16-2005,N10-1063,0,0.0245615,"1 (i.e., PM1 (a |x, y)) is surprisingly informative. More robust models make less-naive prior assumptions and generally produce higher-quality alignments, but the uniform prior probability assumption simplifies the complexity of performing inference. Despite its simplicity, Model 1 has shown particularly good performance as a component in sentence alignment systems (Xu, Zens, and Ney 2005; Braune and Fraser 2010). Some work on parallel data extraction has also focused on extracting parallel segments from comparable corpora (Smith, Quirk, and Toutanova 2010; Munteanu, Fraser, and Marcu 2004). Smith et al. (2010) uses conditional random fields to identify parallel segments from comparable Wikipedia documents (since Wikipedia documents in multiple languages are not generally translations of each other, although they are about the same topics). The work of Jehl, Hieber, and Riezler (2012) uses cross-lingual information retrieval techniques to extract candidate English–Arabic translations from Twitter. These candidates are then refined using a more expressive model to identify translations (Xu, Weischedel, and Nguyen 2001). It is also possible to focus the extraction in one particular type of phenomena."
J16-2005,N12-1079,0,0.0918698,"following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but because of the volum"
J16-2005,C10-1124,0,0.3794,"this work, we made the following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but"
J16-2005,D14-1122,0,0.0205903,"oth in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014). Our method is inspired by the (perhaps surprising) observation that a reasonable number of microblog users tweet “in parallel” in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English–Chinese parallel messages, for example, watup Kenny !!, where an English message and its Chinese Mayne!! - Kenny Mayne translation are in the same post, separated by a dash. It is not only celebrities (or 308 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter their publicists) who tweet in multiple languages; we see"
J16-2005,2005.eamt-1.37,0,0.0799727,"Missing"
J16-2005,P11-1122,0,0.136156,"document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter Finally, crowdsourcing techniques can be used to obtain translations of text from new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quality control. Thus, in order to find good translations, subsequent post-editing and/or ranking is generally necessary. 3. The Intra-Document Alignment (IDA) Model As discussed above, content-based filtering is a method f"
J16-2005,J93-1004,0,\N,Missing
J16-2005,N03-1017,0,\N,Missing
N15-1142,P14-2133,0,0.00813576,"ilt using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while semantics than “what words go together”. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur with the word the, only nouns tend to occur exactly afterwords (e.g. the cat). This is supported by empirical evidence that suggests that order-insensitivity does indeed lead to substandard syntactic representations (Andreas and Klein, 2014; Bansal et al., 2014), where systems using pre-trained with Word2Vec models yield slight improvements while the computationally far more expensive which use word order information embeddings of Collobert et al. (2011) yielded much better results. 1299 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1299–1304, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics In this work, we describe two simple modifications to Word2Vec, one for the skip-gram model and one for the CBOW model, that improve the quali"
N15-1142,P14-2131,0,0.0361448,"e suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while semantics than “what words go together”. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur with the word the, only nouns tend to occur exactly afterwords (e.g. the cat). This is supported by empirical evidence that suggests that order-insensitivity does indeed lead to substandard syntactic representations (Andreas and Klein, 2014; Bansal et al., 2014), where systems using pre-trained with Word2Vec models yield slight improvements while the computationally far more expensive which use word order information embeddings of Collobert et al. (2011) yielded much better results. 1299 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1299–1304, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics In this work, we describe two simple modifications to Word2Vec, one for the skip-gram model and one for the CBOW model, that improve the quality of the embeddings f"
N15-1142,D14-1082,0,0.416746,"o generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of"
N15-1142,P14-1129,0,0.00965647,"they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implement"
N15-1142,E14-1049,1,0.0586898,"isabel.trancoso@inesc-id.pt Abstract in particular the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing"
N15-1142,P11-2008,0,0.0164828,"Missing"
N15-1142,D14-1012,0,0.0227742,"g (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implemented in the Word2Vec tool, However, as these models are insensitive to word order, embeddings built using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while semantics than “what words go together”. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur"
N15-1142,P12-1092,0,0.0570707,"yer,awb}@cs.cmu.edu isabel.trancoso@inesc-id.pt Abstract in particular the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging"
N15-1142,D13-1176,0,0.00968706,"riginal models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et"
N15-1142,D14-1108,1,0.588959,"re suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely use"
N15-1142,P14-2050,0,0.0354569,".pt Abstract in particular the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models"
N15-1142,P14-1140,0,0.0252753,"n issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the"
N15-1142,N13-1039,1,0.692184,"Missing"
N15-1142,P10-1040,0,0.185776,"posed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implemented in the Word2Vec tool, However, as these models are insensitive to word order, embeddings built using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while sem"
N15-1142,N15-1069,0,0.0165027,"r the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word repres"
N16-1007,D10-1049,0,0.0361693,"he gold-standard summary. From Figure 2, we can see that our importance estimation model produces uniformly better ranking performance on both datasets. 5.2 Automatic Summary Evaluation For automatic summary evaluation, we consider three popular metrics. ROUGE (Lin and Hovy, 2003) is employed to evaluate n-grams recall of the summaries with gold-standard abstracts as reference. ROUGE-SU4 (measures unigram and skipbigrams separated by up to four words) is reported. We also utilize BLEU, a precision-based metric, which has been used to evaluate various language generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014). We further consider METEOR (Denkowski and Lavie, 2014). As a recall-oriented metric, it calculates similarity between generations and references by considering synonyms and paraphrases. For comparisons, we first compare with an abstractive summarization method presented in Ganesan et al. (2010) on the RottenTomatoes dataset. Ganesan et al. (2010) utilize a graph-based algorithm to remove repetitive information, and merge opinionated expressions based on syntactic struc52 tures of product reviews.2 For both datasets, we consider two extractive summarization approa"
N16-1007,P15-1153,0,0.0735218,"al., 2006; Li et al., 2010). The second example lists a set of arguments on “death penalty”, where each argument supports the central claim “death penalty deters crime”. Arguments, as a special type of opinionated text, contain reasons to persuade or inform people on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation. Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010). Those systems are not capable of generating new words, and the output summary may suffer from ungrammatical structure. Another line of work requires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions. To address the challenges above, we propose to use an attention-based abstract generation model — a data-driven approach trained to generate informative, concise, and flue"
N16-1007,P05-1022,0,0.0191366,". Our System: People have a right to freedom of religion. Figure 3: Sample summaries generated by different systems on movie reviews and arguments. We only show a subset of reviews and arguments due to limited space. 54 Figure 4: Sampling effect on RottenTomatoes. ations produces inferior results than re-ranking them with simple heuristics. This suggests that the current models are oblivious to some task specific issues, such as informativeness. Post-processing is needed to make better use of the summary candidates. For example, future work can study other sophisticated re-ranking algorithms (Charniak and Johnson, 2005; Konstas and Lapata, 2012). Furthermore, we also look at the difficult cases where our summaries are evaluated to have lower informativeness. They are often much shorter than the gold-standard human abstracts, thus the information coverage is limited. In other cases, some generations contain incorrect information on domain-dependent facts, e.g. named entities, numbers, etc. For instance, a summary “a poignant coming-of-age tale marked by a breakout lead performance from Cate Shortland” is generated for movie “Lore”. This summary contains “Cate Shortland” which is the director of the movie ins"
N16-1007,P05-1033,0,0.0421488,"nt word with the gold-standard summary. From Figure 2, we can see that our importance estimation model produces uniformly better ranking performance on both datasets. 5.2 Automatic Summary Evaluation For automatic summary evaluation, we consider three popular metrics. ROUGE (Lin and Hovy, 2003) is employed to evaluate n-grams recall of the summaries with gold-standard abstracts as reference. ROUGE-SU4 (measures unigram and skipbigrams separated by up to four words) is reported. We also utilize BLEU, a precision-based metric, which has been used to evaluate various language generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014). We further consider METEOR (Denkowski and Lavie, 2014). As a recall-oriented metric, it calculates similarity between generations and references by considering synonyms and paraphrases. For comparisons, we first compare with an abstractive summarization method presented in Ganesan et al. (2010) on the RottenTomatoes dataset. Ganesan et al. (2010) utilize a graph-based algorithm to remove repetitive information, and merge opinionated expressions based on syntactic struc52 tures of product reviews.2 For both datasets, we consider two extractive"
N16-1007,W14-3348,0,0.0194684,"tion model produces uniformly better ranking performance on both datasets. 5.2 Automatic Summary Evaluation For automatic summary evaluation, we consider three popular metrics. ROUGE (Lin and Hovy, 2003) is employed to evaluate n-grams recall of the summaries with gold-standard abstracts as reference. ROUGE-SU4 (measures unigram and skipbigrams separated by up to four words) is reported. We also utilize BLEU, a precision-based metric, which has been used to evaluate various language generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014). We further consider METEOR (Denkowski and Lavie, 2014). As a recall-oriented metric, it calculates similarity between generations and references by considering synonyms and paraphrases. For comparisons, we first compare with an abstractive summarization method presented in Ganesan et al. (2010) on the RottenTomatoes dataset. Ganesan et al. (2010) utilize a graph-based algorithm to remove repetitive information, and merge opinionated expressions based on syntactic struc52 tures of product reviews.2 For both datasets, we consider two extractive summarization approaches: (1) L EX R ANK (Erkan and Radev, 2004) is an unsupervised method that computes"
N16-1007,W14-4408,0,0.304532,"Missing"
N16-1007,D15-1042,0,0.0614579,"Missing"
N16-1007,C10-1039,0,0.860735,"l., 2010). The second example lists a set of arguments on “death penalty”, where each argument supports the central claim “death penalty deters crime”. Arguments, as a special type of opinionated text, contain reasons to persuade or inform people on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation. Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010). Those systems are not capable of generating new words, and the output summary may suffer from ungrammatical structure. Another line of work requires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions. To address the challenges above, we propose to use an attention-based abstract generation model — a data-driven approach trained to generate informative, concise, and fluent opinion summaries. O"
N16-1007,D14-1168,0,0.207662,"ple on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation. Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010). Those systems are not capable of generating new words, and the output summary may suffer from ungrammatical structure. Another line of work requires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions. To address the challenges above, we propose to use an attention-based abstract generation model — a data-driven approach trained to generate informative, concise, and fluent opinion summaries. Our method is based on the recently proposed framework of neural encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a), which translates a sentence in a source language into a target language. Different from previous"
N16-1007,D13-1176,0,0.0337756,"er from ungrammatical structure. Another line of work requires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions. To address the challenges above, we propose to use an attention-based abstract generation model — a data-driven approach trained to generate informative, concise, and fluent opinion summaries. Our method is based on the recently proposed framework of neural encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a), which translates a sentence in a source language into a target language. Different from previous work, our summarization system is designed to support multiple input text units. An attention-based model (Bahdanau et al., 2014) is deployed to al48 low the encoder to automatically search for salient information within context. Furthermore, we propose an importance-based sampling method so that the encoder can integrate information from an important subset of input text. The importance score of a text unit is estimated from a novel regression model with pairwise prefer"
N16-1007,P12-1039,0,0.0116949,"rtance-based sampling rate (K) is set to 5 for experiments in Sections 5.2 3.6 Post-processing and 5.3. For testing phase, we re-rank the n-best summaries Decoding is performed by beam search with a according to their cosine similarity with the input beam size of 20, i.e. we keep 20 most probable outtext units. The one with the highest similarity is input sequences in stack at each step. Outputs with cluded in the final summary. Uses of more sophisend of sentence token are also considered for ticated re-ranking methods (Charniak and Johnson, re-ranking. Decoding stops when every beam in 2005; Konstas and Lapata, 2012) will be investistack generates the end of sentence token. gated in future work. 4 5 Experimental Setup Data Pre-processing. We pre-process the datasets with Stanford CoreNLP (Manning et al., 2014) for tokenization and extracting POS tags and dependency relations. For RottenTomatoes dataset, we replace movie titles with a generic label in training, and substitute it with the movie name if there is any generic label generated in testing. 51 5.1 Results Importance Estimation Evaluation We first evaluate the importance estimation component described in Section 3.5. We compare with Support Vector"
N16-1007,E09-1059,0,0.116486,"gh different aspects of life, ranging from making decisions on regular tasks to judging fundamental societal issues and forming personal ideology. To efficiently absorb the massive amount of opinionated information, there is a pressing need for automated systems that can generate concise and fluent opinion summary about an entity or a topic. In spite of substantial researches in opinion summarization, the most prominent approaches mainly rely on extractive summarization methods, where phrases or sentences from the original documents are selected for inclusion in the summary (Hu and Liu, 2004; Lerman et al., 2009). One of the problems that extractive methods suffer from Wang Ling Google DeepMind London, N1 0AE lingwang@google.com Movie: The Martian Reviews: - One the smartest, sweetest, and most satisfyingly suspenseful sci-fi films in years. - ...an intimate sci-fi epic that is smart, spectacular and stirring. - The Martian is a thrilling, human and moving sci-fi picture that is easily the most emotionally engaging film Ridley Scott has made... - It’s pretty sunny and often funny, a space oddity for a director not known for pictures with a sense of humor. - The Martian highlights the book’s best quali"
N16-1007,C10-1074,0,0.0138023,"mary that describes the opinion consensus of the input. Specifically, we investigate our abstract generation model on two types of opinionated text: movie reviews and arguments on controversial topics. Examples are displayed in Figure 1. The first example contains a set of professional reviews (or critics) about movie “The Martian” and an opinion consensus written by an editor. It would be more useful to automatically generate fluent opinion consensus rather than simply extracting features (e.g. plot, music, etc) and opinion phrases as done in previous summarization work (Zhuang et al., 2006; Li et al., 2010). The second example lists a set of arguments on “death penalty”, where each argument supports the central claim “death penalty deters crime”. Arguments, as a special type of opinionated text, contain reasons to persuade or inform people on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation. Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan e"
N16-1007,N03-1020,0,0.645887,"th importance-based sampling, our model can be trained within manageable time, and is still able to learn from diversified input. We demonstrate the effectiveness of our model on two newly collected datasets for movie reviews and arguments. Automatic evaluation by BLEU (Papineni et al., 2002) indicates that our system outperforms the state-of-the-art extract-based and abstractbased methods on both tasks. For example, we achieved a BLEU score of 24.88 on Rotten Tomatoes movie reviews, compared to 19.72 by an abstractive opinion summarization system from Ganesan et al. (2010). ROUGE evaluation (Lin and Hovy, 2003) also indicates that our system summaries have reasonable information coverage. Human judges further rated our summaries to be more informative and grammatical than compared systems. 2 Data Collection We collected two datasets for movie reviews and arguments on controversial topics with goldstandard abstracts.1 Rotten Tomatoes (www. rottentomatoes.com) is a movie review website that aggregates both professional critics and user-generated reviews (henceforth RottenTomatoes). For each movie, a one-sentence critic consensus is constructed by an editor to summarize the opinions in professional cri"
N16-1007,P14-5010,0,0.00499025,"Missing"
N16-1007,P02-1040,0,0.0965339,"omatically search for salient information within context. Furthermore, we propose an importance-based sampling method so that the encoder can integrate information from an important subset of input text. The importance score of a text unit is estimated from a novel regression model with pairwise preference-based regularizer. With importance-based sampling, our model can be trained within manageable time, and is still able to learn from diversified input. We demonstrate the effectiveness of our model on two newly collected datasets for movie reviews and arguments. Automatic evaluation by BLEU (Papineni et al., 2002) indicates that our system outperforms the state-of-the-art extract-based and abstractbased methods on both tasks. For example, we achieved a BLEU score of 24.88 on Rotten Tomatoes movie reviews, compared to 19.72 by an abstractive opinion summarization system from Ganesan et al. (2010). ROUGE evaluation (Lin and Hovy, 2003) also indicates that our system summaries have reasonable information coverage. Human judges further rated our summaries to be more informative and grammatical than compared systems. 2 Data Collection We collected two datasets for movie reviews and arguments on controversia"
N16-1007,D10-1007,0,0.220939,"etc. For instance, a summary “a poignant coming-of-age tale marked by a breakout lead performance from Cate Shortland” is generated for movie “Lore”. This summary contains “Cate Shortland” which is the director of the movie instead of actor. It would require semantic features to handle this issue, which has yet to be attempted. 6 Related Work Our work belongs to the area of opinion summarization. Constructing fluent natural language opinion summaries has mainly considered product reviews (Hu and Liu, 2004; Lerman et al., 2009), community question answering (Wang et al., 2014), and editorials (Paul et al., 2010). Extractive summarization approaches are employed to identify summaryworthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corresponding feature. Our model instead utilizes abstract generation techniques to construct natural language summaries. As far as we know, we are also the first to study claim generation for arguments. Recently, there has been a growing interest in generating abstractive summaries for news articles (Bing et al., 2015), spoken meetings (Wang and Cardie, 2013), and product reviews ("
N16-1007,D15-1044,0,0.0308694,"e not guaranteed to be grammatical. Gerani et al. (2014) then design a set of manually-constructed realization templates for producing grammatical sentences that serve different discourse functions. Our approach does not require any human-annotated rules, and can be applied in various domains. Our task is closely related to recent advances in neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a). Based on the sequence-to-sequence paradigm, RNNs-based models have been investigated for compression (Filippova et al., 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level. Built on the attention-based translation model in Bahdanau et al. (2014), Rush et al. (2015) study the problem of constructing abstract for a single sentence. Our task differs from the models presented above in that our model carries out abstractive decoding from multiple sentences instead of a single sentence. 7 Conclusion In this work, we presented a neural approach to generate abstractive summaries for opinionated text. We employed an attention-based method that finds salient information from different input text units to generate an informative an"
N16-1007,E12-1023,0,0.0576652,"Missing"
N16-1007,P13-1137,1,0.395604,"l., 2014), and editorials (Paul et al., 2010). Extractive summarization approaches are employed to identify summaryworthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corresponding feature. Our model instead utilizes abstract generation techniques to construct natural language summaries. As far as we know, we are also the first to study claim generation for arguments. Recently, there has been a growing interest in generating abstractive summaries for news articles (Bing et al., 2015), spoken meetings (Wang and Cardie, 2013), and product reviews (Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014). Most approaches are based on phrase extraction, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010). Nevertheless, the output summaries are not guaranteed to be grammatical. Gerani et al. (2014) then design a set of manually-constructed realization templates for producing grammatical sentences that serve different discourse functions. Our approach does not require any human-annotated rules, and can be applied in various domains. Our task is closely relate"
N16-1007,C14-1157,1,0.181544,"acts, e.g. named entities, numbers, etc. For instance, a summary “a poignant coming-of-age tale marked by a breakout lead performance from Cate Shortland” is generated for movie “Lore”. This summary contains “Cate Shortland” which is the director of the movie instead of actor. It would require semantic features to handle this issue, which has yet to be attempted. 6 Related Work Our work belongs to the area of opinion summarization. Constructing fluent natural language opinion summaries has mainly considered product reviews (Hu and Liu, 2004; Lerman et al., 2009), community question answering (Wang et al., 2014), and editorials (Paul et al., 2010). Extractive summarization approaches are employed to identify summaryworthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corresponding feature. Our model instead utilizes abstract generation techniques to construct natural language summaries. As far as we know, we are also the first to study claim generation for arguments. Recently, there has been a growing interest in generating abstractive summaries for news articles (Bing et al., 2015), spoken meetings (Wang and"
N16-1007,H05-1044,0,0.054179,"˜ 0T ˜ ˜ 0 ˜ ˜ ˜ ˜ ˆ = (R R + R λR + β) (R L + R λL ) (9) are defined with states and cells of 150 dimensions. w The attention of each input word and state pair is computed by being projected into a vector of 100 dimensions (Equation 6). - num of words - category in General Inquirer Training is performed via Adagrad (Duchi et al., - unigram (Stone et al., 1966) - num of POS tags - num of positive/negative/neutral 2011). It terminates when performance does not im- num of named entities words (General Inquirer, prove on the development set. We use BLEU (up to - centroidness (Radev, 2001) MPQA (Wilson et al., 2005)) 4-grams) (Papineni et al., 2002) as evaluation met- avg/max TF-IDF scores ric, which computes the precision of n-grams in genTable 1: Features used for text unit importance estimation. erated summaries with gold-standard abstracts as the reference. Finally, the importance-based sampling rate (K) is set to 5 for experiments in Sections 5.2 3.6 Post-processing and 5.3. For testing phase, we re-rank the n-best summaries Decoding is performed by beam search with a according to their cosine similarity with the input beam size of 20, i.e. we keep 20 most probable outtext units. The one with the hi"
P11-2079,P08-1115,0,0.061136,"IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set. 1 Introduction The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propose two methods for generating th"
P11-2079,P08-1112,1,0.800281,"Missing"
P11-2079,P07-2045,0,0.00398059,"nd d) are classified as discontinuous. given by: P (p, mono) = C(mono) C(mono)+C(swap)+C(disc) (1) Where C(o) is the number of times a phrase is extracted with the orientation o in that group of phrase pairs. Moses also provides many options for this stage, such as types of smoothing. We use the default smoothing configuration which adds the fixed value of 0.5 to all C(o). 3 • The orientation is monotonous if only the previous word in the source is aligned with the previous word in the target, or, more formally, if n−1 an−1 / A. i−1 ∈ A ∧ aj+1 ∈ source phrase b) prev word(t) MSD models Moses (Koehn et al., 2007) allows many configurations for the reordering model to be used. In this work, we will only refer to the default configuration (msd-bidirectional-fe), which uses the MSD model, and calculates the reordering orientation for the previous and the next word, for each phrase pair. Other possible configurations are simpler than the default one. For instance, the monotonicity model only considers monotone and non-monotone orientation types, whereas the MSD model also considers the monotone orientation type, but distinguishes the non-monotone orientation type between swap and discontinuous. The approa"
P11-2079,D09-1106,0,0.150493,"Missing"
P11-2079,P08-1023,0,0.0774305,"matrices. Experiments on the IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set. 1 Introduction The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propo"
P11-2079,2010.iwslt-evaluation.1,0,0.122315,"Missing"
P11-2079,J10-3007,1,0.84267,"Missing"
P11-2079,2006.iwslt-papers.7,0,0.0248528,"istical phrase-based systems (Koehn et al., 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al., 2010). The basic phrase extraction algorithm uses word alignment information to constraint the possible phrases that can be extracted. It has been shown that better alignment quality generally leads to better results (Ganchev et al., 2008). However the relationship between the word alignThis paper is organized as follows: Section 2 dement quality and the results is not straightforward, and it was shown in (Vilar et al., 2006) that better scribes the MSD model; Section 3 presents our two alignments in terms of F-measure do not always lead algorithms; in Section 4 we report the results from the experiments conducted using these algorithms, to better translation quality. 450 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 450–454, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics and comment on the results; we conclude in Section 5. 2 prev word(s) a) • The orientation is swap, if only the next word in the source is aligned w"
P11-2079,N03-1017,0,\N,Missing
P11-2079,2010.iwslt-papers.14,1,\N,Missing
P11-2079,2008.amta-papers.18,0,\N,Missing
P13-1018,2005.iwslt-1.8,0,0.0958689,"Missing"
P13-1018,C10-2010,0,0.0480398,"Missing"
P13-1018,J93-2003,0,0.0313653,"Missing"
P13-1018,W06-1008,0,0.429388,"Missing"
P13-1018,P11-2008,0,0.0238097,"Missing"
P13-1018,W12-3153,0,0.264353,"Missing"
P13-1018,N03-1017,0,0.0156335,"Missing"
P13-1018,P08-1113,0,0.141099,"Missing"
P13-1018,P03-1021,0,0.0199252,"Missing"
P13-1018,P02-1040,0,0.105883,"Missing"
P13-1018,W12-3152,0,0.0549925,"Missing"
P13-1018,J03-3002,0,0.404661,"Missing"
P13-1018,N10-1063,0,0.0989051,"Missing"
P13-1018,N12-1079,0,0.229942,"Missing"
P13-1018,C10-1124,0,0.0203108,"Missing"
P13-1018,2005.mtsummit-papers.11,0,0.147745,"Missing"
P13-1018,C96-2141,0,0.295961,"Missing"
P13-1018,I08-2120,0,0.398573,"Missing"
P13-1018,2005.eamt-1.37,0,0.138002,"Missing"
P13-1018,N12-1006,0,0.0533017,"Missing"
P13-1018,N03-1031,0,\N,Missing
P15-1033,C14-1076,1,0.893251,"Missing"
P15-1033,P14-2131,0,0.0270437,"ds—both those that are OOV in both the very limited parsing data but present in the pretraining LM, and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with p = 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained word embeddings. A veritable cottage industry exists for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov c = tanh (U[h; d; r] + e) . For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment). 4 Training Procedure We trained our par"
P15-1033,P82-1020,0,0.861444,"Missing"
P15-1033,D12-1133,0,0.0254088,"Missing"
P15-1033,P08-1067,0,0.025115,"Missing"
P15-1033,D14-1081,0,0.0362549,"Missing"
P15-1033,D14-1082,0,0.815907,"Transition-Based Dependency Parsing with Stack Long Short-Term Memory Chris Dyer♣♠ Miguel Ballesteros♦♠ Wang Ling♠ Austin Matthews♠ Noah A. Smith♠ ♣ Marianas Labs ♦ NLP Group, Pompeu Fabra University ♠ Carnegie Mellon University chris@marianaslabs.com, miguel.ballesteros@upf.edu, {lingwang,austinma,nasmith}@cs.cmu.edu Abstract decisions (Nivre, 2007; Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013; Bohnet and Nivre, 2012), through feature engineering (Zhang and Nivre, 2011; Ballesteros and Nivre, 2014; Chen et al., 2014; Ballesteros and Bohnet, 2014) and more recently using neural networks (Chen and Manning, 2014; Stenetorp, 2013). We propose a technique for learning representations of parser states in transitionbased dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks— the stack LSTM. Like the conventional stack data structures used in transitionbased parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser’s state: (i) unbounded look-ah"
P15-1033,N15-1142,1,0.133656,"aining LM, and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with p = 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained word embeddings. A veritable cottage industry exists for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov c = tanh (U[h; d; r] + e) . For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment). 4 Training Procedure We trained our parser to maximize the conditional log-likelihood (Eq. 1) of treebank parses given sentenc"
P15-1033,C14-1078,0,0.0487974,"Missing"
P15-1033,D10-1004,1,0.291438,"Missing"
P15-1033,P13-1104,0,0.0353977,"Missing"
P15-1033,de-marneffe-etal-2006-generating,0,0.0341911,"Missing"
P15-1033,W03-3017,0,0.0269603,"step is constructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing model uses thre"
P15-1033,W04-0308,0,0.20895,"ructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing model uses three stack LSTMs:"
P15-1033,P13-2111,0,0.0261337,"ter points (i.e., the hTOP ), a continuous-space “summary” of the contents of the current stack configuration is available. We refer to this value as the “stack summary.” it = σ(Wix xt + Wih ht−1 + Wic ct−1 + bi ) ft = σ(Wf x xt + Wf h ht−1 + Wf c ct−1 + bf ) ct = ft ct−1 + What does the stack summary look like? Intuitively, elements near the top of the stack will it tanh(Wcx xt + Wch ht−1 + bc ), 1 Ours is not the first deviation from a strict left-toright order: previous variations include bidirectional LSTMs (Graves and Schmidhuber, 2005) and multidimensional LSTMs (Graves et al., 2007). 2 Goldberg et al. (2013) propose a similar stack construction to prevent stack operations from invalidating existing references to the stack in a beam-search parser that must (efficiently) maintain a priority queue of stacks. where σ is the component-wise logistic sigmoid function, and is the component-wise (Hadamard) product. The value ht of the LSTM at each time step is controlled by a third gate (ot ) that is applied to the result of the application of a nonlinearity to the 335 P TO y0 P P TO TO y1 y0 y1 pop ; x1 y0 y1 y2 ; x1 x2 push ; x1 Figure 1: A stack LSTM extends a conventional left-to-right LSTM with the a"
P15-1033,N07-1050,0,0.0449813,"Missing"
P15-1033,J08-4003,0,0.120327,"Missing"
P15-1033,P09-1040,0,0.0683306,"Missing"
P15-1033,P13-1014,0,0.0122661,"represent each input token, we concatenate three vectors: a learned vector representation for each word type (w); a fixed vector representa˜ LM ), and a tion from a neural language model (w learned representation (t) of the POS tag of the token, provided as auxiliary input to the parser. A Our parser is based on the arc-standard transition inventory (Nivre, 2004), given in Figure 3. 5 In general, A(S, B) is the complete set of parser actions discussed in §3.2, but in some cases not all actions are available. For example, when S is empty and words remain in B, a SHIFT operation is obligatory (Sartorio et al., 2013). 337 Stackt (u, u), (v, v), S (u, u), (v, v), S S Buffert B B (u, u), B Action Stackt+1 (gr (u, v), u), S (gr (v, u), v), S (u, u), S REDUCE - RIGHT (r) REDUCE - LEFT (r) SHIFT Buffert+1 B B B Dependency r u→v r u←v — Figure 3: Parser transitions indicating the action applied to the stack and buffer and the resulting stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the win"
P15-1033,P14-6005,0,0.0326662,"Missing"
P15-1033,P04-1013,0,0.0390693,"Missing"
P15-1033,P13-1088,0,0.0213712,"ons. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree has a value computed as a f"
P15-1033,P13-1045,0,0.0166315,"nding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree"
P15-1033,D13-1170,0,0.0024939,"nding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree"
P15-1033,P07-1080,0,0.0306643,"Missing"
P15-1033,N03-1033,0,0.0859622,"Missing"
P15-1033,I05-3027,0,0.0128763,"Missing"
P15-1033,P15-1032,0,0.257466,"Missing"
P15-1033,W03-3023,0,0.0327577,"for prediction at each time step is constructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing mo"
P15-1033,D08-1059,0,0.0189076,"Missing"
P15-1033,P11-2033,0,0.0327321,"Missing"
P15-1033,D14-1109,0,0.0354567,"Missing"
P15-1033,Q14-1017,0,\N,Missing
P15-1104,S15-2109,1,0.847734,"Missing"
P15-1104,P01-1005,0,0.117923,". All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the challenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task. 1 Introduction The success of supervised systems largely depends on the amount and quality of the available training data, oftentimes, even more than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that"
P15-1104,P14-2131,0,0.018946,"beddings was presented. In this work, new features were estimated with a convex objective function that combined the log-likelihood of the training data, with regularization penalizing the Frobenius norm of the distortion matrix. That is, the matrix of the differences between the original and the new embeddings. Even though the adapted embeddings performed better than the purely unsupervised features, both were significantly outperformed by a simple bag-of-words baseline. Most other approaches, simply rely on additional training data to fine tune the embeddings for a given supervised task. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced"
P15-1104,J92-4003,0,0.106276,"d Work NLP systems can benefit from a very large pool of unlabeled data. While raw documents are usually not annotated, they contain structure, which can be leveraged to learn word features. Context is one strong indicator for word similarity, as related words tend to occur in similar contexts (Firth, 1968). Approaches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are"
P15-1104,D14-1082,0,0.00356403,"this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), where a method to learn taskspecific representatio"
P15-1104,C14-1008,0,0.42499,"eatures. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes. Ideally, word features should be adapted to the specific supervised task. One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embeddings to initialize the word projection layer. Then, during training, the errors made in the predictions are backpropagated to update the embeddings, so that they better predict the supervised signal (Collobert et al., 2011; dos Santos and Gatti, 2014a). However, this strategy faces an additional challenge in noisy domains, such as social media. The lexical variation caused by the typos, use of slang and abbreviations leads to a great number of singletons and out-of-vocabulary words. For these words, the embeddings will be poorly reestimated. Even worse, words not present on the training set will never get their embeddings updated. In this paper, we describe a strategy to adapt unsupervised word embeddings when dealing with small and noisy labeled datasets. The intuition behind our approach is the following. For a given task, only a subset"
P15-1104,N15-1142,1,0.552511,"than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purpos"
P15-1104,P11-2008,0,0.0559975,"Missing"
P15-1104,P12-1092,0,0.037627,"and quality of the available training data, oftentimes, even more than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic,"
P15-1104,P13-2087,0,0.0224118,"ed data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), where a method to learn taskspecific representations from general pre-trained embeddings was presented. In this work, new features were estimated with a convex objective function that combined the log-likelihood of the training data, with regularization penalizing the Frobenius norm of the distortion matrix. That is, the matrix of the differences between the original and the new embeddings. Even though the adapted embeddings performed better than the purely unsupervised features, both were significantly outperformed by a simple bag-of-words baseline. Most other approaches, simply rely on add"
P15-1104,S14-2111,0,0.0573251,"he NLSE model, applied to sentiment polarity prediction. Development Tweets 2015 Tweets 2014 Tweets 2013 Positive 3230 1032 982 1572 Neutral 4109 983 669 1640 Negative 1265 364 202 601 Table 1: Number of examples per class in each SemEval dataset. The first row shows the training data; the other rows are sets used for testing. need to be enriched with additional hand-crafted features that try to capture more discriminative aspects of the content, most of which require external tools (e.g., part-of-speech taggers and parsers) or linguistic resources (e.g., dictionaries and sentiment lexicons) (Miura et al., 2014; Kiritchenko et al., 2014). With the embedding sub-space approach, however, we are able to attain state-ofthe-art performance while requiring only minimal processing of the data and few hyperparameters. To make our results comparable to other systems for this task, we adopted the guidelines from the benchmark. Our system was trained and tuned using only the development data. The evaluation was performed on the test sets, shown in Table 1, and we report the results in terms of the average F-measure for the positive and negative classes. Experimental Setup The first step of our approach require"
P15-1104,N13-1039,0,0.165772,"ches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), where a method to learn t"
P15-1104,D14-1162,0,0.111819,"Missing"
P15-1104,petrov-etal-2012-universal,0,0.00763791,"Aside from embedding E and 1080 sub-space S matrices, the model is parametrized by the weights H ∈ Rh×ps and Y ∈ Rv×h as well as a bias b ∈ Rv×1 . Note that if S is set to the identity matrix, this would be equivalent to the original Collobert et al. (2011) model. Some ppl r juz unrealiable Word Embeddings Subspace Window Tanh Softmax over Tags Verb Figure 4: Illustration of the window model by (Collobert et al., 2011) using a sub-space layer. 6.2 Experiments Tests were performed in Gimpel et al. (2011) Twitter POS dataset, which uses the universal POS tag set composed by 25 different labels (Petrov et al., 2012). The dataset contains 1000 annotated tweets for training, 327 tweets for tuning and 500 tweets for testing. The number of word tokens in these sets are 15000, 5000 and 7000, respectively. There are 5000, 2000 and 3000 word types. Once again, we initialized the embeddings with unsupervised pre-training using the structured skip-gram approach. As for the hyperparameters of the model, we used embeddings with e = 50 dimensions, a hidden layer with h = 200 dimensions and a context of p = 2 as used in (Ling et al., 2015). Training employed mini-batch gradient descent, with mini batches of 100 sente"
P15-1104,S15-2078,0,0.00576043,"ain low dimensional embeddings fitting the complexity of the target task. On the other hand, we are able to learn new representations for all the words, even if they do not occur in the labeled dataset. To estimate the low dimensional sub-space, we propose a simple non-linear model equivalent to a neural network with one single hidden layer. The model is trained in supervised fashion on the labeled dataset, learning jointly the sub-space projection and a classifier for the target task. Using this model, we built a system to participate in the SemEval 2015 Twitter sentiment analysis benchmark (Rosenthal et al., 2015). Our submission attained state-of-the-art results without hand-coded features or linguistic resources (Astudillo et al., 2015). Here, we further investigate this approach and compare it against several state-of-the-art systems for Twitter sentiment classification. We also report on additional experiments to assess the adequacy of this strategy in other natural language problems. To this end, we apply the embedding sub-space layer to Ling et al. (2015) deep learning model for part-of-speech tagging. Even though the gains were not as significant as in the sentiment polarity prediction task, the"
P15-1104,S15-2079,0,0.0769896,"Missing"
P15-1104,S14-2033,0,0.0614964,"sk. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced sentiment specific word embeddings, for the Twitter domain. The embeddings 1075 were estimated with a neural network that minimized a linear combination of two loss functions, one penalized the errors made at predicting the center word within a sequence of words, while the other penalized mistakes made at deciding the sentiment label. Weakly labeled data has also been used to refine unsupervised embeddings, by retraining them to predict the noisy labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embed"
P15-1104,P14-1146,0,0.147691,"sk. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced sentiment specific word embeddings, for the Twitter domain. The embeddings 1075 were estimated with a neural network that minimized a linear combination of two loss functions, one penalized the errors made at predicting the center word within a sequence of words, while the other penalized mistakes made at deciding the sentiment label. Weakly labeled data has also been used to refine unsupervised embeddings, by retraining them to predict the noisy labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embed"
P15-1104,P10-1040,0,0.0626179,"word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes. Ideally, word features should be adapted to the specific supervised task. One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embeddings to initialize the word projection layer. Then, during training, the errors made in"
P15-1104,S13-2052,0,\N,Missing
P15-2105,P11-1038,0,0.0195409,"ical variants in Twitter (e.g., “cats vs. catz”). While variants tend to have the same meaning as their standardized form, the proposed model does not have this information and will not be able to generalize properly. For instance, if the term ”John” is labelled as keyword in the training set, the model would not be able to extract ”Jooohn” as keyword as it is in a different word form. One way to adThe corpus is submitted as supplementary material. 638 dress this would be using a normalization system either built using hand engineered rules (Gouws et al., 2011) or trained using labelled data (Han and Baldwin, 2011; Chrupała, 2014). However, these systems are generally limited as these need supervision and cannot scale to new data or data in other languages. Instead, we will used unsupervised methods that leverage large amounts of unannotated data. We used two popular methods for this purpose: Brown Clustering and Continuous Word Vectors. uments, such as a news article, contain approximately 3-5 keywords, so extracting 3 keywords per document is a reasonable option. However, this would not work in Twitter, since the number of keywords can be arbitrary small. In fact, many tweets contain less than three"
P15-2105,C10-2042,0,0.0355775,"selecting keywords that are chosen by at least three annotators. We also divided the 1827 tweets into 1000 training samples, 327 development samples and 500 test samples, using the splits as in (Gimpel et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including ne"
P15-2105,P12-1092,0,0.0163423,"11010, share the first three nodes in the hierarchically 110. Sharing more tree nodes tends to translate into better similarity between words within the clusters. Thus, a word a 11001 cluster is simultaneously in clusters 1, 11, 110, 1100 and 11001, and a feature can be extracted for each cluster. In our experiments, we used the dataset with 1,000 Brown clusters made available by Owoputi et al. (Owoputi et al., 2013)2 . 4.1.2 Continuous Word Vectors Word representations learned from neural language models are another way to learn more generalizable features for words (Collobert et al., 2011; Huang et al., 2012). In these models, a hidden layer is defined that maps words into a continuous vector. The parameters of this hidden layer are estimated by maximizing a goal function, such as the likelihood of each word predicting surrounding words (Mikolov et al., 2013; Ling et al., 2015). In our work, we used the structured skip-ngram goal function proposed in (Ling et al., 2015) and for each word we extracted its respective word vector as features. 5 Experiments Experiments are performed on the annotated dataset using the train, development and test splits defined in Section 3. As baselines, we reported re"
P15-2105,P14-2111,0,0.0188732,"r (e.g., “cats vs. catz”). While variants tend to have the same meaning as their standardized form, the proposed model does not have this information and will not be able to generalize properly. For instance, if the term ”John” is labelled as keyword in the training set, the model would not be able to extract ”Jooohn” as keyword as it is in a different word form. One way to adThe corpus is submitted as supplementary material. 638 dress this would be using a normalization system either built using hand engineered rules (Gouws et al., 2011) or trained using labelled data (Han and Baldwin, 2011; Chrupała, 2014). However, these systems are generally limited as these need supervision and cannot scale to new data or data in other languages. Instead, we will used unsupervised methods that leverage large amounts of unannotated data. We used two popular methods for this purpose: Brown Clustering and Continuous Word Vectors. uments, such as a news article, contain approximately 3-5 keywords, so extracting 3 keywords per document is a reasonable option. However, this would not work in Twitter, since the number of keywords can be arbitrary small. In fact, many tweets contain less than three words, in which c"
P15-2105,W12-3153,0,0.0173412,"tugal {luis.marujo,wang.ling,isabel.trancoso,david.matos,joao.neto}@inesc-id.pt {cdyer,awb,anatoleg,jgc}@cs.cmu.edu, Abstract These messages tend to be shorter than web pages, especially on Twitter, where the content has to be limited to 140 characters. The language is also more casual with many messages containing orthographical errors, slang (e.g., cday), abbreviations among domain specific artifacts. In many applications, that existing datasets and models tend to perform significantly worse on these domains, namely in Part-of-Speech (POS) Tagging (Gimpel et al., 2011), Machine Translation (Jelh et al., 2012; Ling et al., 2013), Named Entity Recognition (Ritter et al., 2011; Liu et al., 2013), Information Retrieval (Efron, 2011) and Summarization (Duan et al., 2012; Chang et al., 2013). As automatic keyword extraction plays an important role in many NLP tasks, building an accurate extractor for the Twitter domain is a valuable asset in many of these applications. In this paper, we propose an automatic keyword extraction system for this end and our contributions are the following ones: In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We i"
P15-2105,P13-1018,1,0.272005,"wang.ling,isabel.trancoso,david.matos,joao.neto}@inesc-id.pt {cdyer,awb,anatoleg,jgc}@cs.cmu.edu, Abstract These messages tend to be shorter than web pages, especially on Twitter, where the content has to be limited to 140 characters. The language is also more casual with many messages containing orthographical errors, slang (e.g., cday), abbreviations among domain specific artifacts. In many applications, that existing datasets and models tend to perform significantly worse on these domains, namely in Part-of-Speech (POS) Tagging (Gimpel et al., 2011), Machine Translation (Jelh et al., 2012; Ling et al., 2013), Named Entity Recognition (Ritter et al., 2011; Liu et al., 2013), Information Retrieval (Efron, 2011) and Summarization (Duan et al., 2012; Chang et al., 2013). As automatic keyword extraction plays an important role in many NLP tasks, building an accurate extractor for the Twitter domain is a valuable asset in many of these applications. In this paper, we propose an automatic keyword extraction system for this end and our contributions are the following ones: In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differen"
P15-2105,N15-1142,1,0.664689,"racted for each cluster. In our experiments, we used the dataset with 1,000 Brown clusters made available by Owoputi et al. (Owoputi et al., 2013)2 . 4.1.2 Continuous Word Vectors Word representations learned from neural language models are another way to learn more generalizable features for words (Collobert et al., 2011; Huang et al., 2012). In these models, a hidden layer is defined that maps words into a continuous vector. The parameters of this hidden layer are estimated by maximizing a goal function, such as the likelihood of each word predicting surrounding words (Mikolov et al., 2013; Ling et al., 2015). In our work, we used the structured skip-ngram goal function proposed in (Ling et al., 2015) and for each word we extracted its respective word vector as features. 5 Experiments Experiments are performed on the annotated dataset using the train, development and test splits defined in Section 3. As baselines, we reported results using a TF-IDF, the default MAUI toolkit, and our own implementation of (Li et al., 2010) framework. In all cases the IDF component was computed over a collection of 52 million tweets. Results are reported on rows 1 and 2 in Table 1, respectively. The parameter k (col"
P15-2105,W08-1404,0,0.00839334,"e frequently used in many occasions as indicators of important information contained in documents. These can be used by human readers to search for their desired documents, but also in many Natural Language Processing (NLP) applications, such as Text Summarization (Pal et al., ¨ ur et al., 2005), 2013), Text Categorization (Ozg¨ Information Retrieval (Marujo et al., 2011a; Yang and Nyberg, 2015) and Question Answering (Liu and Nyberg, 2013). Many automatic frameworks for extracting keywords have been proposed (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Litvak and Last, 2008). These systems were built for more formal domains, such as news data or Web data, where the content is still produced in a controlled fashion. The emergence of social media environments, such as Twitter and Facebook, has created a framework for more casual data to be posted online. 1. Provide a annotated keyword annotated dataset consisting of 1827 tweets. These tweets are obtained from (Gimpel et al., 2011), and also contain POS annotations. 2. Improve a state-of-the-art keyword extraction system (Marujo et al., 2011b; Marujo et al., 2013) for this domain by learning additional features in a"
P15-2105,N13-1039,1,0.474503,"Missing"
P15-2105,D10-1036,0,0.0132353,"l et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including new set of features and more robust classifier, remains the state-of-the-art system in the news domain (Marujo et al., 2012). To the best of our knowledge, only (Li et al., 2010) used a supervised key"
P15-2105,D11-1141,0,0.0220653,"Missing"
P15-2105,marujo-etal-2012-supervised,1,0.940917,"ihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including new set of features and more robust classifier, remains the state-of-the-art system in the news domain (Marujo et al., 2012). To the best of our knowledge, only (Li et al., 2010) used a supervised keyword extraction framework (based on KEA) with additional features, such as POS tags to performed keyword extraction on Facebook posts. However, at that time Facebook status updates or posts did not contained either hashtags or user mentions. The size of Facebook posts is frequently longer than tweets and has less abbreviations since it is not limited by number of character as in tweets. 3 4 There are many methods that have been proposed for keyword extraction. TF-IDF is one of the simplest approaches for this end (Salt"
P15-2105,N10-1101,0,0.0485415,"ly 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work formation (e.g., retweet). The annotations of each annotator are combined by selecting keywords that are chosen by at least three annotators. We also divided the 1827 tweets into 1000 training samples, 327 development samples and 500 test samples, using the splits as in (Gimpel et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and fi"
P15-2105,W04-3252,0,\N,Missing
P15-2105,W11-2210,0,\N,Missing
P15-2105,P11-1039,0,\N,Missing
P15-2105,P11-2008,0,\N,Missing
P15-2105,C12-1047,0,\N,Missing
P16-1013,D13-1111,1,0.823343,"and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word e"
P16-1013,N07-1058,0,0.0202033,"1982):1 i,j dij pi pj SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this featur"
P16-1013,W06-1670,0,0.00983607,"t); dog is more prototypical than canine (because dog is more concrete); and dog is more prototypical than bull terrier (because dog is less specific). According to the theory, more prototypical words are acquired earlier. We use lexical semantic databases to operationalize insights from the prototype theory in the following semantic features; the features are computed on token level and averaged over paragraphs: • Relative frequency in a supersense was computed by marginalizing the word frequencies in the training corpus over coarse semantic categories defined in the WordNet (Fellbaum, 1998; Ciaramita and Altun, 2006). There are 41 supersense types: 26 for nouns and 15 for verbs, e.g., NOUN . ANIMAL and VERB . MOTION . For example, in NOUN . ANIMAL the relative frequency of human is 0.06, of dog is 0.01, of bird is 0.01, of cattle is 0.009, and of bumblebee is 0.0002. • Relative frequency in a synset was calculated similarly to the previous feature category, but word frequencies were marginalized over WordNet synsets (more fine-grained synonym sets). For example, in the synset {vet, warhorse, veteran, oldtimer, seasoned stager}, veteran is the most prototypical word, scoring 0.87. 3 Evaluation Benchmarks W"
P16-1013,P13-1004,0,0.0755168,"Missing"
P16-1013,W02-1001,0,0.118037,"ss curricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, respectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors. Part of Speech Tagging (POS). For POS tagging, we again use the LSTM-CRF model (Lample et al., 2016), but instead of predicting the named entity tag for every word in a sentence, we train the tagger to predict the POS tag of the word. The tagger is trained and evaluated with the standard Penn TreeBank (PTB) (Marcus et al., 1993) training, development and test set splits as described in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for En"
P16-1013,P15-1033,1,0.791568,"scribed in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for English on the universal dependencies v1.1 treebank (Agi´c et al., 2015) with the standard development and test splits, reporting unlabeled attachment scores (UAS) on the test data. We remove all part-of-speech and morphology features from the data, and prevent the model from optimizing the word embeddings used to represent each word in the corpus, thereby forcing the parser to rely completely on the pretrained embeddings. • Shuffled baselines: the curriculum is defined by random shuffling the training data. We shuffled the data 10 times, and trained 10 word embeddings models, each model"
P16-1013,N16-1030,1,0.480643,"40 thousand English lemmas (Brysbaert et al., 2014). For example, cookie is rated as 5, and spirituality as 1.07. 2 http://http://people.sutd.edu.sg/ ~yue_zhang/doc 133 # paragraphs 2,532,361 et al., 2016). The `2 -regularized logistic regression classifier is tuned on the development set and accuracy is reported on the test set. # tokens 100,872,713 # types 156,663 Table 1: Training data sizes. Named Entity Recognition (NER). Named entity recognition is the task of identifying proper names in a sentence, such as names of persons, locations etc. We use the recently proposed LSTMCRF NER model (Lample et al., 2016) which trains a forward-backward LSTM on a given sequence of words (represented as word vectors), the hidden units of which are then used as (the only) features in a CRF model (Lafferty et al., 2001) to predict the output label sequence. We use the CoNLL 2003 English NER dataset (Tjong Kim Sang and De Meulder, 2003) to train our models and present results on the test set. Setup. 100-dimensional word embeddings were trained using the cbow model implemented in the word2vec toolkit (Mikolov et al., 2013).3 All training data was used, either shuffled or ordered by a curriculum. As described in §3,"
P16-1013,D13-1170,0,0.00455636,"most prototypical word, scoring 0.87. 3 Evaluation Benchmarks We evaluate the utility of the pretrained word embeddings as features in downstream NLP tasks. We choose the following off-the-shelf models that utilize pretrained word embeddings as features: • Age of acquisition (AoA) of words was extracted from the crowd-sourced database, containing over 50 thousand English words (Kuperman et al., 2012). For example, the AoA of run is 4.47 (years), of flee is 8.33, and of abscond is 13.36. If a word was not found in the database it was assigned the maximal age of 25. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use the average of the word vectors of a given sentence as a feature vector for classification (Faruqui et al., 2015; Sedoc • Concreteness ratings on the scale of 1–5 (1 is most abstract) for 40 thousand English lemmas (Brysbaert et al., 2014). For example, cookie is rated as 5, and sp"
P16-1013,N10-1116,0,0.528653,"g♣ Brian MacWhinney♠ Chris Dyer♣♠ ♠ Carnegie Mellon University ♣ Google DeepMind {ytsvetko,mfaruqui,cdyer}@cs.cmu.edu, lingwang@google.com, macw@cmu.edu Abstract complexity (Bengio et al., 2009; Spitkovsky et al., 2010). In language modeling, this preference for increasing complexity has been realized by curricula that increase the entropy of training data by growing the size of the training vocabulary from frequent to less frequent words (Bengio et al., 2009). In unsupervised grammar induction, an effective curriculum comes from increasing length of training sentences as training progresses (Spitkovsky et al., 2010). These case studies have demonstrated that carefully designed curricula can lead to better results. However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990; Skinner, 1938). Had different heuristics been chosen, the results would have been different. In this paper, we use curriculum learning to create improved word representations. However, rather than testing a small number of curricula, we search for an optimal curriculum using Bayesian optimization. A curriculum is defined to be the ordering of the training inst"
P16-1013,J93-2004,0,0.0565311,"ithout additional features. All models were learned under same conditions, across curricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, respectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors. Part of Speech Tagging (POS). For POS tagging, we again use the LSTM-CRF model (Lample et al., 2016), but instead of predicting the named entity tag for every word in a sentence, we train the tagger to predict the POS tag of the word. The tagger is trained and evaluated with the standard Penn TreeBank (PTB) (Marcus et al., 1993) training, development and test set splits as described in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For depend"
P16-1013,D08-1020,0,0.0314538,"SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that"
P16-1013,D13-1100,0,0.0250457,"Missing"
P16-1013,P14-1024,1,0.818283,"s the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2 -regularized logistic regression classifier. • Language model score • Character language model score • Conventionalization features count the number of “conventional” words and phrases in a paragraph. Assuming that a Wikipedia title is a proxy to a conventionalized concept, we counted the number of existing titles (from a database of over 4.5 million titles) in the paragraph. • Average sentence length • Verb-token ratio • Noun-token ratio • Parse tree depth •"
P16-1013,W12-2019,0,0.0531361,"Missing"
P16-1013,P05-1065,0,0.0298158,"P • Quadratic entropy (Rao, 1982):1 i,j dij pi pj SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 In"
P16-1013,D15-1251,0,0.0243606,"l., 2012, GP), providing convenient and powerful prior distribution on functions, and tree-structured Parzen estimators (Bergstra et al., 2011, TPE), tailored to handle conditional spaces. Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Moˇckus et al., 1978; Jones, 2001), GP upper confidence bound (Srinivas et al., 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al. (2016) for an extensive comparison. Yogatama et al. (2015) found that the combination of EI as the acquisition function and TPE as the surrogate model performed favorably in Bayesian optimization of text representations; we follow this choice in our model. 2.2 are used in many contrasting fields, from ecology and biology (Rosenzweig, 1995; Magurran, 2013), to economics and social studies (Stirling, 2007). Diversity has been shown effective in related research on curriculum learning in language modeling, vision, and multimedia analysis (Bengio et al., 2009; Jiang et al., 2014). Let pi and pj correspond to empirical frequencies of word types ti and tj"
P16-1013,J11-1005,0,0.033983,"ories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2 -regularized logistic regression classifier. • Language model score • Character language model score • Conventionalization features count the number of “conventional” words and phrases in a p"
P16-1057,D15-1198,0,0.0188294,"Missing"
P16-1057,N13-1103,0,0.142382,"r networks to copy keywords from the input. Along with other extensions, namely structured attention and code compression, our model is applied on on both existing datasets and also on a newly created one with implementations of TCG game cards. Our experiments show that our model out-performs multiple benchmarks, which demonstrate the importance of combining different types of predictors. Related Work While we target widely used programming languages, namely, Java and Python, our work is related to studies on the generation of any executable code. These include generating regular expressions (Kushman and Barzilay, 2013), and the code for parsing input documents (Lei et al., 2013). Much research has also been invested in generating formal languages, such as database queries (Zelle and Mooney, 1996; Berant et al., 2013), agent specific language (Kate et al., 2005) or smart phone instructions (Le et al., 2013). Finally, mapping natural language into a sequence of actions for the generation of executable code (Branavan et al., 2009). Finally, a considerable effort in this task has focused on semantic parsing (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015). Re"
P16-1057,D13-1160,0,0.116214,"Missing"
P16-1057,D10-1119,0,0.00897507,"al., 2014) and an attention-based model (Bahdanau et al., 2014). The former is adapted to work with multiple input fields by concatenating them, while the latter uses our proposed attention model. These models are denoted as “Sequence” and “Attention”. Machine Translation Baselines Our problem can also be viewed in the framework of semantic parsing (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012; Artzi et al., 2015). Unfortunately, these approaches define strong assumptions regarding the grammar and structure of the output, which makes it difficult to generalize for other domains (Kwiatkowski et al., 2010). However, the work in Andreas et al. (2013) provides 604 ial. For instance, the correctness cards with conditional (e.g. if player has no cards, then draw a card) or non-deterministc (e.g. put a random card in your hand) effects cannot be simply validated by running the code. evidence that using machine translation systems without committing to such assumptions can lead to results competitive with the systems described above. We follow the same approach and create a phrase-based (Koehn et al., 2007) model and a hierarchical model (or PCFG) (Chiang, 2007) as benchmarks for the work presented h"
P16-1057,P09-1010,0,0.0853311,"Missing"
P16-1057,N16-1030,0,0.00411849,"viously been used to generate code from natural language in (Mou et al., 2015). Inspired by these works, LPNs provide a richer framework by employing attention models (Bahdanau et al., 2014), pointer networks (Vinyals et al., 2015) and character-based embeddings (Ling et al., 2015). Our formulation can also be seen as a generalization of Allamanis et al. (2016), who implement a special case where two predictors have the same granularity (a sub-token softmax and a pointer network). Finally, HMMs have been employed in neural models to marginalize over label sequences in (Collobert et al., 2011; Lample et al., 2016) by modeling transitions between labels. Figure 5: Examples of decoded cards from HS. Copied segments are marked in green and incorrect segments are marked in red. 8 9 Conclusion We introduced a neural network architecture named Latent Prediction Network, which allows efficient marginalization over multiple predictors. Under this architecture, we propose a generative model for code generation that combines a character level softmax to generate language-specific tokens and multiple pointer networks to copy keywords from the input. Along with other extensions, namely structured attention and cod"
P16-1057,J07-2003,0,0.0181301,"Missing"
P16-1057,P13-1127,0,0.601031,"599–609, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (e.g., name and cost) and has an effect that is described in a text box. Digital implementations of these games implement the game logic, which includes the card effects. This is attractive from a data extraction perspective as not only are the data annotations naturally generated, but we can also view the card as a specification communicated from a designer to a software engineer. This dataset presents additional challenges to prior work in code generation (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015), including the handling of structured input—i.e. cards are composed by multiple sequences (e.g., name and description)—and attributes (e.g., attack and cost), and the length of the generated sequences. Thus, we propose an extension to attention-based neural models (Bahdanau et al., 2014) to attend over structured inputs. Finally, we propose a code compression method to reduce the size of the code without impacting the quality of the predictions. Experiments performed on our new datasets, and a further pre-existing one, suggest that our extensions outpe"
P16-1057,P13-2121,0,0.0152035,"Missing"
P16-1057,D15-1176,1,0.220142,"Missing"
P16-1057,P12-1051,0,0.057978,"Linguistics, pages 599–609, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (e.g., name and cost) and has an effect that is described in a text box. Digital implementations of these games implement the game logic, which includes the card effects. This is attractive from a data extraction perspective as not only are the data annotations naturally generated, but we can also view the card as a specification communicated from a designer to a software engineer. This dataset presents additional challenges to prior work in code generation (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015), including the handling of structured input—i.e. cards are composed by multiple sequences (e.g., name and description)—and attributes (e.g., attack and cost), and the length of the generated sequences. Thus, we propose an extension to attention-based neural models (Bahdanau et al., 2014) to attend over structured inputs. Finally, we propose a code compression method to reduce the size of the code without impacting the quality of the predictions. Experiments performed on our new datasets, and a further pre-existing one, suggest that ou"
P16-1057,D08-1082,0,0.0676968,"able as s increases. The algorithm stops once s reaches max or the newly generated list L(s) contains no elements. Neural Benchmarks We implement two standard neural networks, namely a sequence-tosequence model (Sutskever et al., 2014) and an attention-based model (Bahdanau et al., 2014). The former is adapted to work with multiple input fields by concatenating them, while the latter uses our proposed attention model. These models are denoted as “Sequence” and “Attention”. Machine Translation Baselines Our problem can also be viewed in the framework of semantic parsing (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012; Artzi et al., 2015). Unfortunately, these approaches define strong assumptions regarding the grammar and structure of the output, which makes it difficult to generalize for other domains (Kwiatkowski et al., 2010). However, the work in Andreas et al. (2013) provides 604 ial. For instance, the correctness cards with conditional (e.g. if player has no cards, then draw a card) or non-deterministc (e.g. put a random card in your hand) effects cannot be simply validated by running the code. evidence that using machine translation systems without committing to such assumptions"
P16-1057,P07-2045,0,0.0311268,"marginal likelihood over latent predictors and generated segments allowing for scalable training. We introduce a new corpus for the automatic generation of code for cards in Trading Card Games (TCGs), on which we validate our model 1 . TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding pool of cards. Examples of such cards are shown in Figure 1. Each card is identified by its attributes Introduction The generation of both natural and formal languages often requires models conditioned on diverse predictors (Koehn et al., 2007; Wong and Mooney, 2006). Most models take the restrictive approach of employing a single predictor, such as a word softmax, to predict all tokens of the output sequence. To illustrate its limitation, suppose we wish to generate the answer to the question “Who wrote The Foundation?” as “The Foundation was written by Isaac Asimov”. The generation of the words “Issac Asimov” and “The Foundation” from a word softmax trained on annotated data is unlikely to succeed as these words are sparse. A robust model might, for example, employ one pre1 Dataset available at https://deepmind.com/publications.h"
P16-1057,J03-1002,0,0.0340537,"Missing"
P16-1057,P02-1040,0,0.114005,"Missing"
P16-1057,P15-1085,0,0.232073,"2, 2016. 2016 Association for Computational Linguistics (e.g., name and cost) and has an effect that is described in a text box. Digital implementations of these games implement the game logic, which includes the card effects. This is attractive from a data extraction perspective as not only are the data annotations naturally generated, but we can also view the card as a specification communicated from a designer to a software engineer. This dataset presents additional challenges to prior work in code generation (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015), including the handling of structured input—i.e. cards are composed by multiple sequences (e.g., name and description)—and attributes (e.g., attack and cost), and the length of the generated sequences. Thus, we propose an extension to attention-based neural models (Bahdanau et al., 2014) to attend over structured inputs. Finally, we propose a code compression method to reduce the size of the code without impacting the quality of the predictions. Experiments performed on our new datasets, and a further pre-existing one, suggest that our extensions outperform strong benchmarks. The paper is str"
P16-1057,2011.eamt-1.33,0,0.0262721,"Missing"
P16-1057,U05-1027,0,0.02406,"nerated incorrectly. In the card “Preparation”, we observe that while the properties of the card are generated correctly, the effect implements a unrelated one, with the exception of the value 3, which is correctly copied. Yet, interestingly, it still generates a valid effect, which sets a minion’s attack to 3. Investigating better methods to accurately generate these effects will be object of further studies. et al., 2015), Bayesian Tree Transducers (Jones et al., 2012; Lei et al., 2013) and Probabilistic Context Free Grammars (Andreas et al., 2013). The work in natural language programming (Vadas and Curran, 2005; Manshadi et al., 2013), where users write lines of code from natural language, is also related to our work. Finally, the reverse mapping from code into natural language is explored in (Oda et al., 2015). Character-based sequence-to-sequence models have previously been used to generate code from natural language in (Mou et al., 2015). Inspired by these works, LPNs provide a richer framework by employing attention models (Bahdanau et al., 2014), pointer networks (Vinyals et al., 2015) and character-based embeddings (Ling et al., 2015). Our formulation can also be seen as a generalization of Al"
P16-1057,N06-1056,0,0.607791,"over latent predictors and generated segments allowing for scalable training. We introduce a new corpus for the automatic generation of code for cards in Trading Card Games (TCGs), on which we validate our model 1 . TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding pool of cards. Examples of such cards are shown in Figure 1. Each card is identified by its attributes Introduction The generation of both natural and formal languages often requires models conditioned on diverse predictors (Koehn et al., 2007; Wong and Mooney, 2006). Most models take the restrictive approach of employing a single predictor, such as a word softmax, to predict all tokens of the output sequence. To illustrate its limitation, suppose we wish to generate the answer to the question “Who wrote The Foundation?” as “The Foundation was written by Isaac Asimov”. The generation of the words “Issac Asimov” and “The Foundation” from a word softmax trained on annotated data is unlikely to succeed as these words are sparse. A robust model might, for example, employ one pre1 Dataset available at https://deepmind.com/publications.html 599 Proceedings of t"
P16-1057,P13-2009,0,\N,Missing
P17-1015,P13-2009,0,0.014434,"7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000 question and rational"
P17-1015,P02-1040,0,0.108663,"Missing"
P17-1015,D13-1160,0,0.104533,"1,m2) y 0.023 check(m7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000"
P17-1015,P15-1085,0,0.0287949,"Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000 question and rationale pairs, and propose"
P17-1015,D15-1202,0,0.38211,"n four parts, two inputs and two outputs: the description of the problem, which we will denote as the question, and the possible (multiple choice) answer options, denoted as options. Our goal is to generate the description of the rationale used to reach the correct answer, denoted as rationale and the correct option label. Problem 1 illustrates an example of an algebra problem, which must be translated into an expression (i.e., (27x + 17y)/(x + y) = 23) and then the desired quantity (x/y) solved for. Problem 2 is an example that could be solved by multi-step arithmetic operations proposed in (Roy and Roth, 2015). Finally, Problem 3 describes a problem that is solved by testing each of the options, which has not been addressed in the past. 2.1 Construction We first collect a set of 34,202 seed problems that consist of multiple option math questions covering a broad range of topics and difficulty levels. Examples of exams with such problems include the GMAT (Graduate Management Admission Test) and GRE (General Test). Many websites contain example math questions in such exams, where the answer is supported by a rationale. Next, we turned to crowdsourcing to generate new questions. We create a task where"
P17-1015,D16-1117,0,0.218012,"ill an unsolved problem, as each additional step adds complexity to the problem both during inference and decoding. Yet, this is the first result showing that it is possible to solve math problems in such a manner, and we believe this modeling approach and dataset will drive work on this problem. 7 Related Work Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math problem. Other work has focused on learning to map math expressions into formal languages (Roy et al., 2016). We aim to generate natural language rationales, where the bindings between variables and the problem solving approach are mixed into a single generative model that attempts to solve the problem while explaining the approach taken. Our approach is strongly tied with the work on sequence to sequence transduction using the encoder-decoder paradigm (Sutskever et al., 2014; BLEU. We observe that the regular sequence to sequence model achieves a low BLEU score. In fact, due to the high perplexities the model generates very short rationales, which frequently consist of segments similar to “Answer s"
P17-1015,D14-1058,0,0.0514733,", as these cannot be copied from the input or output. 6.5 Discussion. While we show that our model can outperform the models built up to date, generating complex rationales as those shown in Figure 1 correctly is still an unsolved problem, as each additional step adds complexity to the problem both during inference and decoding. Yet, this is the first result showing that it is possible to solve math problems in such a manner, and we believe this modeling approach and dataset will drive work on this problem. 7 Related Work Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math problem. Other work has focused on learning to map math expressions into formal languages (Roy et al., 2016). We aim to generate natural language rationales, where the bindings between variables and the problem solving approach are mixed into a single generative model that attempts to solve the problem while explaining the approach taken. Our approach is strongly tied with the work on sequence to sequence transduction using the encoder-decoder paradigm (Sutskever et al., 2014; BLEU. We observ"
P17-1015,P12-1051,0,0.01495,"0.025 130 2.99 div(m1,m2) y 0.023 check(m7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this en"
P17-1015,D13-1176,1,0.441142,"r capsule for bottle T ? (A) $ 0.25 (B) $ 0.12 (C) $ 0.05 (D) $ 0.03 (E) $ 0.002 m sub(m6,m3) 250 6.25 0.025 130 2.99 div(m1,m2) y 0.023 check(m7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the pr"
P17-1015,P14-1026,0,0.327097,"gth Figure 2: Distribution of examples per length. Table 1: Descriptive statistics of our dataset. define in the instructions that this is not allowed, as it will generate multiple copies of the same problem in the dataset if two or more Turkers copy from the same resource. These Turkers can be detected by checking the nearest neighbours within the collected datasets as problems obtained from online resources are frequently submitted by more than one Turker. Using this method, we obtained 70,318 additional questions. 2.2 17y)/(x + y) = 23 must be solved to obtain the answer. In previous work (Kushman et al., 2014), this could be done by feeding the equation into an expression solver to obtain x/y = 3/2. However, this would skip the intermediate steps 27x + 17y = 23x + 23y and 4x = 6y, which must also be generated in our problem. We propose a model that jointly learns to generate the text in the rationale, and to perform the math operations required to solve the problem. This is done by generating a program, containing both instructions that generate output and instructions that simply generate intermediate values used by following instructions. Statistics Descriptive statistics of the dataset is shown"
P17-1015,D16-1011,0,0.0607361,"Missing"
P17-1015,P16-1057,1,0.916495,"Missing"
S15-2102,P97-1023,0,0.0589137,"nyms and the opposite polarity to antonyms, of known words. Others, create a graph from the semantic relationships, to find new sentiment words and their polarity. Using the seed words, new terms are classified using a distance measure (Kamps et al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from co-occurrence patterns with known words. Hatzivassiloglou and McKeown (1997) discovered new polar adjectives by looking at conjunctions found in a corpus. The adjectives connected with and got the same polarity, whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing positive and another containing negative examples. The polarity of a new term was computed using the point-wise mutual information between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), to create large scale sentiment lexicons for Tw"
S15-2102,kamps-etal-2004-using,0,0.0579992,"Missing"
S15-2102,N06-1026,0,0.0117873,". We found this approach to be effective for the proposed problem. The rest of the paper proceeds as follows: we review the work related to lexicon expansion in Section 2 and describe the methods used to derive word embeddings in Section 3. Our approach and the experimental results are presented in Sections 5 and 6, respectively. We conclude in Section 7. 2 Related Work Most of the literature on automatic lexicon expansion consists of dictionary-based or corpora-based approaches. In the former, the main idea is to use a dictionary, such as WordNet, to extract semantic relations between words. Kim and Hovy (2006) simply assign the same polarity to synonyms and the opposite polarity to antonyms, of known words. Others, create a graph from the semantic relationships, to find new sentiment words and their polarity. Using the seed words, new terms are classified using a distance measure (Kamps et al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from c"
S15-2102,N15-1142,1,0.600546,"trix, transforming the one-hot sparse representation into a compact real valued space of size e; Cp ∈ Rv×e is a matrix mapping the realvalued representation to a vector with the size of the vocabulary v. A distribution over all possible words is then attained by exponentiating and normalizing over the v possible options. In practice, due to the large value of v, various techniques are used to avoid having to normalize over the whole vocabulary (Mikolov et al., 2013a). In the particular case of the structured skip-gram model, the matrix Cp depends only of the relative position between words p (Ling et al., 2015). After training, the low dimensional embedding E· wi ∈ Re×1 encapsulates the information about each word and its surrounding contexts. CBOW The CBOW model defines a different objective function, that predicts a word at position i given the window of context i − d, where d is the size of the context window. The probability of the word wi is (a) Phrases as the sum of embeddings (b) Phrases as the mean of embeddings Figure 1: Performance of the different embeddings and phrase representations, as function of vector size. defined as: p(wi |wi−d , ..., wi+d ; C, E) ∝ exp(C · Si+d i−d ) (2) where Si"
S15-2102,N13-1039,0,0.0819375,"Missing"
S15-2102,D14-1162,0,0.0901746,", 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word embeddings from large corpora. These are essentially, dense vector representations that implicitly capture syntactic and semantic properties of words (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). Moreover, a notion of semantic similarity, as well as other linguistic regularities seem to be encoded in the embedding space (Mikolov et al., 2013b). In word2vec, Mikolov et al. (2013a) induce word vectors with two simple neural network architectures, CBOW and skip-gram. These models estimate the optimal word embeddings by maximizing the probability that, words within a given window size are predicted correctly. Skip-gram and Structured Skip-gram Central to the skip-gram is a log-linear model of word prediction. Given the i-th word from a sentence wi , the skip-gram estimates the probabilit"
S15-2102,E09-1077,0,0.0159245,"Most of the literature on automatic lexicon expansion consists of dictionary-based or corpora-based approaches. In the former, the main idea is to use a dictionary, such as WordNet, to extract semantic relations between words. Kim and Hovy (2006) simply assign the same polarity to synonyms and the opposite polarity to antonyms, of known words. Others, create a graph from the semantic relationships, to find new sentiment words and their polarity. Using the seed words, new terms are classified using a distance measure (Kamps et al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from co-occurrence patterns with known words. Hatzivassiloglou and McKeown (1997) discovered new polar adjectives by looking at conjunctions found in a corpus. The adjectives connected with and got the same polarity, whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing posi"
S15-2102,C14-1018,0,0.0977943,"whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing positive and another containing negative examples. The polarity of a new term was computed using the point-wise mutual information between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), to create large scale sentiment lexicons for Twitter. A recently proposed alternative is to learn word embeddings specific for Twitter sentiment analysis, using distant supervision (Tang et al., 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word embeddings from large corpora. These are essentially, dense vector representations that implicitly capture syntactic and semantic properties of words (Collobert et al., 2011; Mikolov et al., 2013a; Penning"
S15-2109,P14-2131,0,0.0326399,"large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to"
S15-2109,J92-4003,0,0.0675756,"mbeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to the skip-gram (Mikolov et al., 2013) is a log linear model of word prediction. Let w = i denote that a word at a given po"
S15-2109,D14-1082,0,0.056327,"embeddings trained from large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the"
S15-2109,N15-1142,1,0.824245,"llobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to the skip-gram (Mikolov et al., 2013) is a log linear model of word prediction. Let w = i denote that a word at a given position of a sentence is the i-th word on a vocabulary of size v. Let wp = j denote that the word p positions further in the sentence is the j-th word on the vocabulary. The skip-gram models the following probability:  p(wp = j|w = i; C, E) ∝ exp Cj · E · wi . (1) Here, wi ∈ {1, 0}v×1 is a one-hot representation of w = i. That is, a vector of zeros of the size of the vocabulary v with a 1 on the i-th entry of the vector. The s"
S15-2109,S14-2089,0,0.111909,"need for automated analysis tools. The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis (Nakov et al., 2013). The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment. A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons. In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015). The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme. Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous word representations containi"
S15-2109,S13-2052,0,0.0670063,"sed training is thus reduced to finding the optimal projection which can be carried out efficiently despite the little data available. We analyze in detail the proposed approach and show that a competitive system can be attained with only a few configuration parameters. 1 Introduction Web-based social networks are a rich data source for both businesses and academia. However, the sheer volume, diversity and rate of creation of social media, imposes the need for automated analysis tools. The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis (Nakov et al., 2013). The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment. A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expre"
S15-2109,N13-1039,0,0.152652,"Missing"
S15-2109,S15-2078,0,0.0925042,"rch has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons. In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015). The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme. Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous word representations containing contextual information. Instead of using these word embeddings directly with, for instance, a logistic regression classifier, we estimate a sentiment subspace of the embeddings. The idea is to find a projection of the embedding space that is meaningful for the supervised task. In the proposed model, we j"
S15-2109,P10-1040,0,0.0908234,"classifier, we estimate a sentiment subspace of the embeddings. The idea is to find a projection of the embedding space that is meaningful for the supervised task. In the proposed model, we jointly learn the sentiment subspace projection and the classifier using the SemEval training data. The resulting system attains state-of-theart performance without hand-coded features or linguistic resources and only a few configuration parameters. 2 Unsupervised Learning of Word Embeddings Unsupervised word embeddings trained from large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extractio"
S15-2109,S14-2077,0,0.0321089,"l media, imposes the need for automated analysis tools. The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis (Nakov et al., 2013). The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment. A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons. In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015). The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme. Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous w"
S16-1036,P15-1104,1,0.90835,"e-trained word embeddings provide a simple means to attain semi-supervised learning in Natural Language Processing (NLP) tasks (Collobert et al., 2011). They can be trained with large amounts of unsupervised data and be fine tuned as the initial building block of a semi-supervised system. However, in domains with a significant number of typos, use of slang and abbreviations, such as social media, the high number of singletons leads to a poor fine tuning of the embeddings. In previous work, 2 NLSE Model Overview In this section, we briefly review the approach introduced in the 2015 evaluation (Astudillo et al., 2015a). For a particular regression or classification task, only a subset of all the latent aspects captured by the word embeddings will be useful. Therefore, instead of updating the embeddings directly with the available labeled data, we estimate a projection of these embeddings into a low dimensional sub-space. This simple method brings two fundamental advan238 Proceedings of SemEval-2016, pages 238–242, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics tages. Firstly, the lower dimensional embeddings require fewer parameters fitting the complexity of the"
S16-1036,S15-2109,1,0.891997,"Missing"
S16-1036,N15-1142,1,0.890643,"Missing"
S16-1036,S16-1001,0,0.0188104,"Comparision of strategies to address the problem of OOEV 5 System https://github.com/wlin12/wang2vec 241 examples after each training epoch and performed model selection by early stopping after 12 iterations. The candidate for submission was manually selected by observing the performance across 2013, 2014 and 2015 datasets. Priority was given to models that presented a consistent high performance in all the datasets. In retrospect, this was most probably a suboptimal decision judging from the evaluation results. Table 4 displays the performance for the top 5 systems in SemEval 2016 task 4-B (Nakov et al., 2016). The NLSE system (labeled INESC-ID) ranks forth with a stable performance across all years. The results are particularly strong for 2013 with a difference of 0.017 points over the next best performing system on the top five. This is consistent with the divide noticed during system selection between performance in 2013 and 2015. High-performing systems in 2014, and particularly in 2013, do not appear to be equally performing in recent years. 6 Conclusions We presented the INESC-ID system for the SemEval 2016 task 4-A, built on top of the successful NonLinear Subspace Embedding model. We found"
S16-1036,N13-1039,0,0.0316451,"Missing"
S16-1036,S15-2079,0,0.0408819,"Missing"
W13-2205,P06-1121,0,0.0416165,"sh and an in-house tokenizer for French that targets the tokenization used by the Berkeley French parser). Both sides of the parallel training data were parsed using the Berkeley latent variable parser. Synchronous context-free grammar rules were extracted from the corpus following the method of Hanneman et al. (2011). This decomposes each tree pair into a collection of SCFG rules by exhaustively identifying aligned subtrees to serve as rule left-hand sides and smaller aligned subtrees to be abstracted as right-hand-side nonterminals. Basic subtree alignment heuristics are similar to those by Galley et al. (2006), and composed rules are allowed. The computational complexity is held in check by a limit on the number of RHS elements (nodes and terminals), rather than a GHKM-style maximum composition depth or Hiero-style maximum rule span. Our rule extractor also allows “virtual nodes,” or the insertion of new nodes in the parse tree to subdivide regions of flat structure. Virtual nodes are similar to the A+B extended categories of SAMT (Zollmann and Venugopal, 2006), but with the added constraint that they may not conflict with the surrounding tree structure. Because the SCFG rules are labeled with nont"
W13-2205,W12-4410,1,0.923402,"for the 2013 Workshop on Machine Translation shared translation task: French–English, Russian–English, and English–Russian. Our French–English system (§3) showcased our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011). Our Russian–English and English–Russian system demonstrate a new multiphase approach to translation that our group is using, in which synthetic translation options (§4) to supplement the default translation rule inventory that is extracted from word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation"
W13-2205,J92-4003,0,0.07177,"e 500-best outputs of a syntactic system constructed similarly to the French–English system. 6 30.8 30.9 31.1 Russian side of the training corpus. An unpruned, modified Kneser-Ney smoothed 4-gram language model (Chen and Goodman, 1996) was estimated from all available Russian text (410 million words) using the KenLM toolkit (Heafield et al., 2013). A standard hierarchical phrase-based system was trained with rule shape indicator features, obtained by replacing terminals in translation rules by a generic symbol. MIRA training was performed to learn feature weights. Additionally, word clusters (Brown et al., 1992) were obtained for the complete monolingual Russian data. Then, an unsmoothed 7-gram language model was trained on these clusters and added as a feature to the translation system. Indicator features were also added for each cluster and bigram cluster occurence. These changes resulted in an improvement of more than a BLEU point on our held-out development set. Slavic languages like Russian have a large number of different inflected forms for each lemma, representing different cases, tenses, and aspects. Since our training data is rather limited relative to the number of inflected forms that are"
W13-2205,W11-1011,1,0.933305,"alized modules to create “synthetic translation options” that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context. 1 Introduction The MT research group at Carnegie Mellon University’s Language Technologies Institute participated in three language pairs for the 2013 Workshop on Machine Translation shared translation task: French–English, Russian–English, and English–Russian. Our French–English system (§3) showcased our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011). Our Russian–English and English–Russian system demonstrate a new multiphase approach to translation that our group is using, in which synthetic translation options (§4) to supplement the default translation rule inventory that is extracted from word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastru"
W13-2205,P96-1041,0,0.103448,"Missing"
W13-2205,N13-1029,1,0.816688,"kens, contained a token of more than 30 characters, or had particularly unbalanced length ratios were also removed. After filtering, 30.9 million sentence pairs remained for rule extraction: 14.4 million from the clean data, and 16.5 million from the web data. 3.2 1 Selecting the stopping point still requires a measure of intuition. The label set size of 1814 chosen here roughly corresponds to the number of joint labels that would exist in the grammar if virtual nodes were not included. This equivalence has worked well in practice in both internal and published experiments on other data sets (Hanneman and Lavie, 2013). Preprocessing and Grammar Extraction Our French–English system uses parse trees in both the source and target languages, so tokeniza71 most common type. We conducted experiments with three different frequency cutoffs: 100, 200, and 500, with each increase decreasing the grammar size by 70–80 percent. Extracted rules each have 10 features associated with them. For an SCFG rule with source lefthand side `s , target left-hand side `t , source righthand side rs , and target right-hand side rt , they are: 3.3 • phrasal translation log relative frequencies log f (rs |rt ) and log f (rt |rs ); • la"
W13-2205,J07-2003,0,0.201588,"Missing"
W13-2205,W11-1015,1,0.856156,"hop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokenizer for English and an in-house tokenizer for French that targets the tokenization used by the Berkeley French parser). Both sides of the parallel training data were parsed using the Berkeley latent variable parser. Synchronous context-free grammar rules were extracted from the corpus following the method of Hanneman et al. (2011). This decomposes each tree pair into a collection of SCFG rules by exhaustively identifying aligned subtrees to serve as rule left-hand sides and smaller aligned subtrees to be abstracted as right-hand-side nonterminals. Basic subtree alignment heuristics are similar to those by Galley et al. (2006), and composed rules are allowed. The computational complexity is held in check by a limit on the number of RHS elements (nodes and terminals), rather than a GHKM-style maximum composition depth or Hiero-style maximum rule span. Our rule extractor also allows “virtual nodes,” or the insertion of ne"
W13-2205,P11-2031,1,0.827764,"n relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others according to MultEval’s approximate randomization algorithm. The closest is the variant with cutoff 200, which is generally judged to be sl"
W13-2205,P13-2121,0,0.0613657,"Missing"
W13-2205,W11-2107,1,0.844656,"f (rs |rt ) and log f (rt |rs ); • labeling relative frequency log f (`s , `t |rs , rt ) and generation relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others according to MultEval’s approximate randomiz"
W13-2205,W11-2123,0,0.0303697,"iterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokenizer for English and an in-house tokenizer for French that targets the tokenization used by"
W13-2205,W12-3131,1,0.818679,"s in the various system pairs. 3 French-English Syntax System Our submission for French–English is a tree-totree translation system that demonstrates several innovations from group’s research on SCFG-based translation. 3.1 Data Selection We divided the French–English training data into two categories: clean data (Europarl, News Commentary, UN Documents) totaling 14.8 million sentence pairs, and web data (Common Crawl, Giga-FrEn) totaling 25.2 million sentence pairs. To reduce the volume of data used, we filtered non-parallel and other unhelpful segments according to the technique described by Denkowski et al. (2012). This procedure uses a lexical translation model learned from just the clean data, as well as source and target n-gram language models to compute the following feature scores: • French and English 4-gram log likelihood (normalized by length); • French–English and English–French lexical translation log likelihood (normalized by length); and, • Fractions of aligned words under the French– English and English–French models. We pooled previous years’ WMT news test sets to form a reference data set. We computed the same features. To filter the web data, we retained only sentence for which each fea"
W13-2205,P09-1019,1,0.851065,"based on a histogrambased similarity function that looks at what target labels correspond to a particular source label and vice versa. The number of clusters used is determined based on spikes in the distance between successive clustering iterations, or by the number of source, target, or joint labels remaining. Starting from a default grammar of 877 French, 2580 English, and 131,331 joint labels, we collapsed the label space for our WMT system down to 50 French, 54 English, and 1814 joint categories.1 the French–English system, using the hypergraph MERT algorithm and optimizing towards BLEU (Kumar et al., 2009). The remainder of the paper will focus on our primary innovations in the various system pairs. 3 French-English Syntax System Our submission for French–English is a tree-totree translation system that demonstrates several innovations from group’s research on SCFG-based translation. 3.1 Data Selection We divided the French–English training data into two categories: clean data (Europarl, News Commentary, UN Documents) totaling 14.8 million sentence pairs, and web data (Common Crawl, Giga-FrEn) totaling 25.2 million sentence pairs. To reduce the volume of data used, we filtered non-parallel and"
W13-2205,P10-4002,1,0.867841,"d English–Russian system demonstrate a new multiphase approach to translation that our group is using, in which synthetic translation options (§4) to supplement the default translation rule inventory that is extracted from word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria,"
W13-2205,C08-1064,0,0.0127826,"odels, we trained 4-gram Markov models using the target side of the bitext and any available monolingual data (including Gigaword for English). Additionally, we trained 7-gram language models using 600-class Brown clusters with Witten-Bell smoothing.2 5.2 Baseline System Our baseline Russian–English system is a hierarchical phrase-based translation model as implemented in cdec (Chiang, 2007; Dyer et al., 2010). SCFG translation rules that plausibly match each sentence in the development and deftest sets were extracted from the aligned parallel data using the suffix array indexing technique of Lopez (2008). A Russian morphological analyzer was used to lemmatize the training, development, and test data, and the “noisier channel” translation approach of Dyer (2007) was used in the Russian– English system to let unusually inflected surface forms back off to per-lemma translations. • transliterations of OOV Russian words (§5.3); • English target sides with varied function words (for example, given a phrase that translates into cat we procedure variants like the cat, a cat and of the cat); and, 2 73 http://www.ark.cs.cmu.edu/cdyer/ru-600/. 5.3 Synthetic Translations: Transliteration Table 2: Russian"
W13-2205,W11-2139,1,0.884244,"en incorporated in current statistical MT systems. For our Russian–English system, we additionally used a secondary “pseudo-reference” translation when tuning the parameters of our Russian– English system. This was created by automatically translating the Spanish translation of the provided development data into English. While the output of an MT system is not always perfectly grammatical, previous work has shown that secondary machine-generated references improve translation quality when only a single human reference is available when BLEU is used as an optimization criterion (Madnani, 2010; Dyer et al., 2011). We describe the CMU systems submitted to the 2013 WMT shared task in machine translation. We participated in three language pairs, French–English, Russian– English, and English–Russian. Our particular innovations include: a labelcoarsening scheme for syntactic tree-totree translation and the use of specialized modules to create “synthetic translation options” that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context. 1 Introduction The MT research group at Carnegie Mellon Uni"
W13-2205,D10-1004,0,0.0328156,"Missing"
W13-2205,N13-1073,1,0.782282,"om word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokeniz"
W13-2205,W07-0729,0,0.0220307,"trained 7-gram language models using 600-class Brown clusters with Witten-Bell smoothing.2 5.2 Baseline System Our baseline Russian–English system is a hierarchical phrase-based translation model as implemented in cdec (Chiang, 2007; Dyer et al., 2010). SCFG translation rules that plausibly match each sentence in the development and deftest sets were extracted from the aligned parallel data using the suffix array indexing technique of Lopez (2008). A Russian morphological analyzer was used to lemmatize the training, development, and test data, and the “noisier channel” translation approach of Dyer (2007) was used in the Russian– English system to let unusually inflected surface forms back off to per-lemma translations. • transliterations of OOV Russian words (§5.3); • English target sides with varied function words (for example, given a phrase that translates into cat we procedure variants like the cat, a cat and of the cat); and, 2 73 http://www.ark.cs.cmu.edu/cdyer/ru-600/. 5.3 Synthetic Translations: Transliteration Table 2: Russian-English summary. Analysis revealed that about one third of the unseen Russian tokens in the development set consisted of named entities which should be transli"
W13-2205,P02-1040,0,0.087928,"on log relative frequencies log f (rs |rt ) and log f (rt |rs ); • labeling relative frequency log f (`s , `t |rs , rt ) and generation relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others accordin"
W13-2205,W12-3160,0,0.0121088,"n phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokenizer for English and an in-house tokenizer for French that targets the tokenization used by the Berkeley French parser). Both sides of the parallel training data were parsed using the Berkeley latent variable parser. Synchronous context-free grammar rules w"
W13-2205,2006.amta-papers.25,0,0.0492601,"beling relative frequency log f (`s , `t |rs , rt ) and generation relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others according to MultEval’s approximate randomization algorithm. The closest is"
W13-2205,W13-2234,1,0.806649,"se pairs whose source sides match the test sentence are retained. • Abstract rules (whose RHS are all nonterminals) are globally pruned. Only the 4000 most frequently observed rules are retained. • Mixed rules (whose RHS are a mix of terminals and nonterminals) must match the test sentence, and there is an additional frequency cutoff. 4 Synthetic Translation Options Before discussing our Russian–English and English–Russian systems, we introduce the concept of synthetic translation options, which we use in these systems. We provide a brief overview here; for more detail, we refer the reader to Tsvetkov et al. (2013). In language pairs that are typologically similar, words and phrases map relatively directly from source to target languages, and the standard approach to learning phrase pairs by extraction from parallel data can be very effective. However, in language pairs in which individual source language words have many different possible translations (e.g., when the target language word could have many different inflections or could be surrounded by different function words that have no After this filtering, the number of completely lexical rules that match a given sentence is typically low, up to a f"
W13-2205,W06-3119,0,0.0265547,"hand sides and smaller aligned subtrees to be abstracted as right-hand-side nonterminals. Basic subtree alignment heuristics are similar to those by Galley et al. (2006), and composed rules are allowed. The computational complexity is held in check by a limit on the number of RHS elements (nodes and terminals), rather than a GHKM-style maximum composition depth or Hiero-style maximum rule span. Our rule extractor also allows “virtual nodes,” or the insertion of new nodes in the parse tree to subdivide regions of flat structure. Virtual nodes are similar to the A+B extended categories of SAMT (Zollmann and Venugopal, 2006), but with the added constraint that they may not conflict with the surrounding tree structure. Because the SCFG rules are labeled with nonterminals composed from both the source and target trees, the nonterminal inventory is quite large, leading to estimation difficulties. To deal with this, we automatically coarsening the nonterminal labels (Hanneman and Lavie, 2011). Labels are agglomeratively clustered based on a histogrambased similarity function that looks at what target labels correspond to a particular source label and vice versa. The number of clusters used is determined based on spik"
W13-2205,sharoff-etal-2008-designing,0,\N,Missing
W14-3356,W10-0710,0,0.132945,"Missing"
W14-3356,2005.iwslt-1.8,0,0.0947835,"Missing"
W14-3356,P05-1074,0,0.156435,"Missing"
W14-3356,P11-1061,0,0.0567147,"Missing"
W14-3356,P91-1023,0,0.724078,"Missing"
W14-3356,1992.tmi-1.9,0,0.36141,"Missing"
W14-3356,W12-3134,0,0.0268465,"Missing"
W14-3356,2012.amta-papers.7,0,0.0943115,"Missing"
W14-3356,W11-2123,0,0.0200324,"Missing"
W14-3356,W12-3153,0,0.0696685,"Missing"
W14-3356,P07-2045,1,0.0120714,"Missing"
W14-3356,P13-1018,1,0.81692,"Missing"
W14-3356,I11-1125,0,0.0609802,"Missing"
W14-3356,J05-4003,0,0.186489,"Missing"
W14-3356,P03-1058,0,0.0964326,"Missing"
W14-3356,P03-1021,0,0.0602179,"Missing"
W14-3356,P02-1040,0,0.108513,"Missing"
W14-3356,W12-3152,0,0.0723752,"Missing"
W14-3356,J03-3002,0,0.372304,"Missing"
W14-3356,N10-1113,0,0.0352934,"Missing"
W14-3356,N10-1063,0,0.0896412,"Missing"
W14-3356,P11-1122,0,0.0870956,"Missing"
W14-3356,J93-1004,0,\N,Missing
W14-3356,W13-2201,0,\N,Missing
zhang-etal-2014-dual,itamar-itai-2008-using,0,\N,Missing
zhang-etal-2014-dual,N12-1006,0,\N,Missing
zhang-etal-2014-dual,N12-1079,0,\N,Missing
zhang-etal-2014-dual,J93-1004,0,\N,Missing
zhang-etal-2014-dual,tiedemann-2008-synchronizing,0,\N,Missing
zhang-etal-2014-dual,W12-3152,0,\N,Missing
zhang-etal-2014-dual,N10-1113,0,\N,Missing
zhang-etal-2014-dual,J93-2003,0,\N,Missing
zhang-etal-2014-dual,W06-1008,0,\N,Missing
zhang-etal-2014-dual,J03-3002,0,\N,Missing
zhang-etal-2014-dual,C10-1124,0,\N,Missing
zhang-etal-2014-dual,P02-1040,0,\N,Missing
zhang-etal-2014-dual,I11-1125,0,\N,Missing
zhang-etal-2014-dual,W10-0710,0,\N,Missing
zhang-etal-2014-dual,N10-1063,0,\N,Missing
zhang-etal-2014-dual,P07-2045,1,\N,Missing
zhang-etal-2014-dual,P13-1018,1,\N,Missing
zhang-etal-2014-dual,P11-1061,0,\N,Missing
zhang-etal-2014-dual,P05-1074,0,\N,Missing
zhang-etal-2014-dual,P03-1058,0,\N,Missing
zhang-etal-2014-dual,P08-1113,0,\N,Missing
zhang-etal-2014-dual,W12-3153,0,\N,Missing
zhang-etal-2014-dual,2005.mtsummit-papers.11,0,\N,Missing
zhang-etal-2014-dual,D13-1008,1,\N,Missing
zhang-etal-2014-dual,2010.iwslt-papers.14,1,\N,Missing
zhang-etal-2014-dual,I08-2120,0,\N,Missing
zhang-etal-2014-dual,2011.iwslt-papers.3,1,\N,Missing
zhang-etal-2014-dual,W12-3134,0,\N,Missing
zhang-etal-2014-dual,tiedemann-2012-parallel,0,\N,Missing
zhang-etal-2014-dual,N04-1034,0,\N,Missing
zhang-etal-2014-dual,W11-2123,0,\N,Missing
