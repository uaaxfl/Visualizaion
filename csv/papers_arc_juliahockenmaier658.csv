2021.findings-acl.356,{H}y{SPA}: Hybrid Span Generation for Scalable Text-to-Graph Extraction,2021,-1,-1,4,0,8349,liliang ren,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.sigmorphon-1.15,{U}niversity of {I}llinois Submission to the {SIGMORPHON} 2020 Shared Task 0: Typologically Diverse Morphological Inflection,2020,-1,-1,6,0,14887,marc canby,"Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"The objective of this shared task is to produce an inflected form of a word, given its lemma and a set of tags describing the attributes of the desired form. In this paper, we describe a transformer-based model that uses a bidirectional decoder to perform this task, and evaluate its performance on the 90 languages and 18 language families used in this task."
2020.acl-main.232,Learning to execute instructions in a {M}inecraft dialogue,2020,-1,-1,3,1,22726,prashant jayannavar,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The Minecraft Collaborative Building Task is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment. We define the subtask of predicting correct action sequences (block placements and removals) in a given game context, and show that capturing B{'}s past actions as well as B{'}s perspective leads to a significant improvement in performance on this challenging language understanding problem."
2020.acl-main.758,A Multi-Perspective Architecture for Semantic Code Search,2020,13,0,4,0,23115,rajarshi haldar,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code{--}text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space."
P19-1537,Collaborative Dialogue in {M}inecraft,2019,0,2,3,1,2982,anjali narayanchen,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. Since computer games allow us to simulate such tasks without the need for physical robots, we define a Minecraft-based collaborative building task in which one player (A, the Architect) is shown a target structure and needs to instruct the other player (B, the Builder) to build this structure. Both players interact via a chat interface. A can observe B but cannot place blocks. We present the Minecraft Dialogue Corpus, a collection of 509 conversations and game logs. As a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is."
D19-1515,Phrase Grounding by Soft-Label Chain Conditional Random Field,2019,0,0,2,0,27090,jiacheng liu,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"The phrase grounding task aims to ground each entity mention in a given caption of an image to a corresponding region in that image. Although there are clear dependencies between how different mentions of the same caption should be grounded, previous structured prediction methods that aim to capture such dependencies need to resort to approximate inference or non-differentiable losses. In this paper, we formulate phrase grounding as a sequence labeling task where we treat candidate regions as potential labels, and use neural chain Conditional Random Fields (CRFs) to model dependencies among regions for adjacent mentions. In contrast to standard sequence labeling tasks, the phrase grounding task is defined such that there may be multiple correct candidate regions. To address this multiplicity of gold labels, we define so-called Soft-Label Chain CRFs, and present an algorithm that enables convenient end-to-end training. Our method establishes a new state-of-the-art on phrase grounding on the Flickr30k Entities dataset. Analysis shows that our model benefits both from the entity dependencies captured by the CRF and from the soft-label training regime. Our code is available at \url{github.com/liujch1998/SoftLabelCCRF}"
W17-2812,Towards Problem Solving Agents that Communicate and Learn,2017,11,1,8,1,2982,anjali narayanchen,Proceedings of the First Workshop on Language Grounding for Robotics,0,Agents that communicate back and forth with humans to help them execute non-linguistic tasks are a long sought goal of AI. These agents need to translate between utterances and actionable meaning representations that can be interpreted by task-specific problem solvers in a context-dependent manner. They should also be able to learn such actionable interpretations for new predicates on the fly. We define an agent architecture for this scenario and present a series of experiments in the Blocks World domain that illustrate how our architecture supports language learning and problem solving in this domain.
I17-1011,Natural Language Inference from Multiple Premises,2017,11,7,3,1,28061,alice lai,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"We define a novel textual entailment task that requires inference over multiple premise sentences. We present a new dataset for this task that minimizes trivial lexical inferences, emphasizes knowledge of everyday events, and presents a more challenging setting for textual entailment. We evaluate several strong neural baselines and analyze how the multiple premise task differs from standard textual entailment."
E17-1068,Learning to Predict Denotational Probabilities For Modeling Entailment,2017,0,9,2,1,28061,alice lai,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"We propose a framework that captures the denotational probabilities of words and phrases by embedding them in a vector space, and present a method to induce such an embedding from a dataset of denotational probabilities. We show that our model successfully predicts denotational probabilities for unseen phrases, and that its predictions are useful for textual entailment datasets such as SICK and SNLI."
W16-3203,Focused Evaluation for Image Description with Binary Forced-Choice Tasks,2016,26,9,2,1,33768,micah hodosh,Proceedings of the 5th Workshop on Vision and Language,0,Current evaluation metrics for image description may be too coarse. We therefore propose a series of binary forced-choice tasks that each focus on a different aspect of the captions. We evaluate a number of different off-the-shelf image description systems. Our results indicate strengths and shortcomings of both generation and ranking based approaches.
D16-1214,Evaluating Induced {CCG} Parsers on Grounded Semantic Parsing,2016,12,1,4,1,8387,yonatan bisk,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
P15-2143,Labeled Grammar Induction with Minimal Supervision,2015,30,5,3,1,8387,yonatan bisk,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Nearly all work in unsupervised grammar induction aims to induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters."
P15-1135,Probing the Linguistic Strengths and Limitations of Unsupervised Grammar Induction,2015,18,8,2,1,8387,yonatan bisk,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Work in grammar induction should help shed light on the amount of syntactic structure that is discoverable from raw word or tag sequences. But since most current grammar induction algorithms produce unlabeled dependencies, it is difficult to analyze what types of constructions these algorithms can or cannot capture, and, therefore, to identify where additional supervision may be necessary. This paper provides an in-depth analysis of the errors made by unsupervised CCG parsers by evaluating them against the labeled dependencies in CCGbank, hinting at new research directions necessary for progress in grammar induction."
S14-2055,{I}llinois-{LH}: A Denotational and Distributional Approach to Semantics,2014,11,71,2,1,28061,alice lai,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper describes and analyzes our SemEval 2014 Task 1 system. Its features are based on distributional and denotational similarities; word alignment; negation; and hypernym/hyponym, synonym, and antonym relations."
Q14-1006,From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,2014,35,681,4,1,39075,peter young,Transactions of the Association for Computational Linguistics,0,"We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions."
Q13-1007,An {HDP} Model for Inducing {C}ombinatory {C}ategorial {G}rammars,2013,44,23,2,1,8387,yonatan bisk,Transactions of the Association for Computational Linguistics,0,"We introduce a novel nonparametric Bayesian model for the induction of Combinatory Categorial Grammars from POS-tagged text. It achieves state of the art performance on a number of languages, and induces linguistically plausible lexicons."
W12-1912,Induction of Linguistic Structure with {C}ombinatory {C}ategorial {G}rammars,2012,13,7,2,1,8387,yonatan bisk,Proceedings of the {NAACL}-{HLT} Workshop on the Induction of Linguistic Structure,0,"Our system consists of a simple, EM-based induction algorithm (Bisk and Hockenmaier, 2012), which induces a language-specific Combinatory Categorial grammar (CCG) and lexicon based on a small number of linguistic principles, e.g. that verbs may be the roots of sentences and can take nouns as arguments."
P12-2027,"{B}eefmoves: Dissemination, Diversity, and Dynamics of {E}nglish Borrowings in a {G}erman Hip Hop Forum",2012,9,16,2,0,42654,matt garley,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We investigate how novel English-derived words (anglicisms) are used in a German-language Internet hip hop forum, and what factors contribute to their uptake."
Y10-1002,The Future Role of Language Resources for Natural Language Parsing (We Won{'}t Be Able to Rely on Pierre Vinken Rorever... or Will We Have to?),2010,0,0,1,1,8351,julia hockenmaier,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,None
W10-2920,Cross-Caption Coreference Resolution for Automatic Image Understanding,2010,20,13,4,1,33768,micah hodosh,Proceedings of the Fourteenth Conference on Computational Natural Language Learning,0,"Recent work in computer vision has aimed to associate image regions with keywords describing the depicted entities, but actual image 'understanding' would also require identifying their attributes, relations and activities. Since this information cannot be conveyed by simple keywords, we have collected a corpus of action photos each associated with five descriptive captions. In order to obtain a consistent semantic representation for each image, we need to first identify which NPs refer to the same entities. We present three hierarchical Bayesian models for cross-caption coreference resolution. We have also created a simple ontology of entity classes that appear in images and evaluate how well these can be recovered."
W10-0721,Collecting Image Annotations Using {A}mazon{'}s {M}echanical {T}urk,2010,9,349,4,0,45299,cyrus rashtchian,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,"Crowd-sourcing approaches such as Amazon's Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of linguistic data at a relatively low cost and high speed. However, MTurk offers only limited control over who is allowed to particpate in a particular task. This is particularly problematic for tasks requiring free-form text entry. Unlike multiple-choice tasks there is no correct answer, and therefore control items for which the correct answer is known cannot be used. Furthermore, MTurk has no effective built-in mechanism to guarantee workers are proficient English writers. We describe our experience in creating corpora of images annotated with multiple one-sentence descriptions on MTurk and explore the effectiveness of different quality control strategies for collecting linguistic data using Mechanical MTurk. We find that the use of a qualification test provides the highest improvement of quality, whereas refining the annotations through follow-up tasks works rather poorly. Using our best setup, we construct two image corpora, totaling more than 40,000 descriptive captions for 9000 images."
P10-5001,Wide-Coverage {NLP} with Linguistically Expressive Grammars,2010,3,0,1,1,8351,julia hockenmaier,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"In recent years, there has been a lot of research on wide-coverage statistical natural language processing with linguistically expressive grammars such as Combinatory Categorial Grammars (CCG), Head-driven Phrase-Structure Grammars (HPSG), Lexical-Functional Grammars (LFG) and Tree-Adjoining Grammars (TAG). But although many young researchers in natural language processing are very well trained in machine learning and statistical methods, they often lack the necessary background to understand the linguistic motivation behind these formalisms. Furthermore, in many linguistics departments, syntax is still taught from a purely Chomskian perspective. Additionally, research on these formalisms often takes place within tightly-knit, formalismspecific subcommunities. It is therefore often difficult for outsiders as well as experts to grasp the commonalities of and differences between these formalisms."
C10-2133,Shallow Information Extraction from Medical Forum Data,2010,19,21,4,0,46483,parikshit sondhi,Coling 2010: Posters,0,"We study a novel shallow information extraction problem that involves extracting sentences of a given set of topic categories from medical forum data. Given a corpus of medical forum documents, our goal is to extract two related types of sentences that describe a biomedical case (i.e., medical problem descriptions and medical treatment descriptions). Such an extraction task directly generates medical case descriptions that can be useful in many applications. We solve the problem using two popular machine learning methods Support Vector Machines (SVM) and Conditional Random Fields (CRF). We propose novel features to improve the accuracy of extraction. Experiment results show that we can obtain an accuracy of up to 75%."
C10-2145,Citation Author Topic Model in Expert Search,2010,25,34,4,0,42594,yuancheng tu,Coling 2010: Posters,0,"This paper proposes a novel topic model, Citation-Author-Topic (CAT) model that addresses a semantic search task we define as expert search - given a research area as a query, it returns names of experts in this area. For example, Michael Collins would be one of the top names retrieved given the query Syntactic Parsing.n n Our contribution in this paper is two-fold. First, we model the cited author information together with words and paper authors. Such extra contextual information directly models linkage among authors and enhances the author-topic association, thus produces more coherent author-topic distribution. Second, we provide a preliminary solution to the task of expert search when the learning repository contains exclusively research related documents authored by the experts. When compared with a previous proposed model (Johri et al., 2010), the proposed model produces high quality author topic linkage and achieves over 33% error reduction evaluated by the standard MAP measurement."
C10-1053,Normal-form parsing for {C}ombinatory {C}ategorial {G}rammars with generalized composition and type-raising,2010,23,10,1,1,8351,julia hockenmaier,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We propose and implement a modification of the Eisner (1996) normal form to account for generalized composition of bounded degree, and an extension to deal with grammatical type-raising."
W08-2306,Non-local scrambling: the equivalence of {TAG} and {CCG} revisited,2008,16,8,1,1,8351,julia hockenmaier,Proceedings of the Ninth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+9),0,"It is well known that standard TAG cannot deal with certain instances of longdistance scrambling in German (Rambow, 1994). That CCG can deal with many instances of non-local scrambling in languages such as Turkish has previously been observed (e.g. by Hoffman (1995a) and Baldridge (2002)). We show here that CCG can derive German scrambling cases which are problematic for TAG, and give CCG analyses for other German constructions that require more expressive power than TAG provides. Such analyses raise the question of the linguistic significance of the TAG-CCG equivalence. We revisit the original equivalence proof, and show that a careful examination of the translation of CCG and TAG into Indexed Grammar reveals that the IG which is strongly equivalent to CCG can generate dependencies which the corresponding IG obtained from an LTAG cannot generate."
W07-2205,The Impact of Deep Linguistic Processing on Parsing Technology,2007,2,7,3,0,1468,timothy baldwin,Proceedings of the Tenth International Conference on Parsing Technologies,0,"As the organizers of the ACL 2007 Deep Linguistic Processing workshop (Baldwin et al., 2007), we were asked to discuss our perspectives on the role of current trends in deep linguistic processing for parsing technology. We are particularly interested in the ways in which efficient, broad coverage parsing systems for linguistically expressive grammars can be built and integrated into applications which require richer syntactic structures than shallow approaches can provide. This often requires hybrid technologies which use shallow or statistical methods for pre- or post-processing, to extend coverage, or to disambiguate the output."
J07-3004,{CCG}bank: A Corpus of {CCG} Derivations and Dependency Structures Extracted from the {P}enn {T}reebank,2007,87,285,1,1,8351,julia hockenmaier,Computational Linguistics,0,"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word-word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train wide-coverage statistical parsers that obtain state-of-the-art rates of dependency recovery.n n In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks."
W06-1635,Protein folding and chart parsing,2006,25,1,1,1,8351,julia hockenmaier,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"How can proteins fold so quickly into their unique native structures? We show here that there is a natural analogy between parsing and the protein folding problem, and demonstrate that CKY can find the native structures of a simplified lattice model of proteins with high accuracy."
W06-1637,Priming Effects in {C}ombinatory {C}ategorial {G}rammar,2006,22,12,2,0,5874,david reitter,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a corpus-based account of structural priming in human sentence processing, focusing on the role that syntactic representations play in such an account. We estimate the strength of structural priming effects from a corpus of spontaneous spoken dialogue, annotated syntactically with Combinatory Categorial Grammar (CCG) derivations. This methodology allows us to test a range of predictions that CCG makes about priming. In particular, we present evidence for priming between lexical and syntactic categories encoding partially satisfied sub-categorization frames, and we show that priming effects exist both for incremental and normal-form CCG derivations."
P06-1064,Creating a {CCG}bank and a Wide-Coverage {CCG} Lexicon for {G}erman,2006,21,40,1,1,8351,julia hockenmaier,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees. The resulting corpus contains 46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text."
C04-1180,Wide-Coverage Semantic Representations from a {CCG} Parser,2004,21,172,5,0,6245,johan bos,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards widecoverage semantic interpretation, one of the key objectives of the field of NLP."
W03-1008,Identifying Semantic Roles Using {C}ombinatory {C}ategorial {G}rammar,2003,12,97,2,0,3945,daniel gildea,Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,0,"We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar. This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles."
P03-1046,Parsing with Generative Models of Predicate-Argument Structure,2003,7,65,1,1,8351,julia hockenmaier,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al. (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies."
N03-1031,Example Selection for Bootstrapping Statistical Parsers,2003,19,237,6,0,748,mark steedman,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data. We consider both a mostly-unsupervised approach, cotraining, in which two parsers are iteratively re-trained on each other's output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser's output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks."
E03-1008,Bootstrapping statistical parsers from small datasets,2003,14,118,6,0,748,mark steedman,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of boot-strapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that boot-strapping continues to be useful, even though no manually produced parses from the target domain are used."
P02-1042,Building Deep Dependency Structures using a Wide-Coverage {CCG} Parser,2002,15,86,2,0.350877,20968,stephen clark,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures. The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies. A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank. The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies."
P02-1043,Generative Models for Statistical Parsing with {C}ombinatory {C}ategorial {G}rammar,2002,14,157,1,1,8351,julia hockenmaier,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling word-word dependencies."
hockenmaier-steedman-2002-acquiring,Acquiring Compact Lexicalized Grammars from a Cleaner Treebank,2002,11,92,1,1,8351,julia hockenmaier,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,Abstract We present an algorithm which translates the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. To do this we have needed to make several systematic changes to the Treebank which have to effect of cleaning up a number of errors and inconsistencies. This process has yielded a cleaner treebank that can potentially be used in any framework. We also show how unary type-changing rules for certain types of modifiers can be introduced in a CCG grammar to ensure a compact lexicon without augmenting the generative power of the system. We demonstrate how the combination of preprocessing and type-changing rules minimizes the lexical coverage problem.
Y98-1021,Error-Driven Learning of {C}hinese Word Segmentation,1998,6,25,1,1,8351,julia hockenmaier,"Proceedings of the 12th Pacific Asia Conference on Language, Information and Computation",0,"Palmer ([4]) demonstrated how Brill's Transformation-based Error-Driven Learning can be applied to word segmentation in various languages. We present experimental results which show that such algorithms can achieve satisfactory performance even with a a very dimple initial state annotator We also present two preliminary studies, which suggest that even higher performance might be achieved if simple morphological information is available to the system, and that segmentation performance might actually be improved by combining segmentation with rudimentary part-of-speech tagging."
