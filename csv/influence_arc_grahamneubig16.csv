2011.iwslt-evaluation.5,2011.eamt-1.17,1,0.76364,"tion Case and punct No case and no punct BLEU 0.1190 0.1106 NIST 4.6929 4.7142 WER 0.7177 0.7523 PER 0.5746 0.5977 GTM 0.4844 0.4620 METEOR 0.4847 0.4503 TER 67.1430 71.8380 Table 1: The official results for the NICT system in terms of a variety of automatic evaluation metrics. ent sections of the sentence that could more effectively be translated separately. Since the input utterances are punctuated and contain spaces that indicate word boundaries we exploited this punctuation and space information as cues to determine the likely positions to delimit segmentation boundaries. In previous work [1] it has been shown that constraints of this type can be useful in managing the decoding of longer sentences. Constraining the search in the right way leaves a simpler problem for the machine translation decoder to solve, and one that can be performed considerably more efficiently than unconstrained decoding over the full search space.. The second strategy was to build the translation model for the system using two heterogeneous methods. The translation model is a key component in any phrase-based SMT system, and building this model using two different techniques can potentially bring benefits"
2011.iwslt-evaluation.5,W08-0336,0,0.0384903,"• Discontinuous (previous phrase-pair) • Swap (previous phrase-pair) 5. A word insertion penalty feature Based on a set of pilot experiments we decoded with no limit on the distances phrases could be moved in the reordering process during decoding. The base model above was augmented with an additional translation model feature intended to be an indicator of the quality/reliability of each phrase-pair; this feature will be explained later in Section 4.2. 2.2. Pre-processing The Chinese data supplied for this task was not segmented into words. We used the Stanford Chinese word segmentation tool [4, 5] with the Peking University (PKU) model to wordsegment this data. The English data was tokenized by applying a number of regular expressions to separate punctuation, 50 and split contractions such as “it’s” and “hasn’t” into two separate tokens. We also removed all case information from the English text to help to minimize issues of data sparseness in the models of the translation system. All punctuation was left in both source and target. We took the decision to generate target punctuation directly using the process of translation, rather than as a punctuation restoration step in post process"
2011.iwslt-evaluation.5,P03-1021,0,0.0477536,"lt in the same manner using the SRI language modeling toolkit [7]. 5-gram models were built for decoding the development and test data for evaluation, and 3-gram models were built for decoding during the parameter tuning process to speed up decoding. The language models were smoothed using modified KnesserNey smoothing. 2.4.2. Translation Model The translation model for the base system was built in the the standard manner using a 2-step process. First the training data To tune the values for the log-linear weights in our system, we use the standard minimum error-rate training procedure (MERT) [9]. The weights for the models were tuned using the development data supplied for the task. To perform the MERT tuning we used the publicly available ZMERT framework [10], and this allowed us to easily add and tune additional features into our models. The models were tuned with respect the BLEU metric [11]: ‘BLEU4 Closest’ that is built into the tool. 3. Rule-based Decoding Constraints 3.1. Motivation Translating long and complex sentences has been a critical problem in machine translation. A standard phrase-based statistical machine translation system cannot solve the problem of word reordering"
2011.iwslt-evaluation.5,P02-1040,0,0.0804854,"serNey smoothing. 2.4.2. Translation Model The translation model for the base system was built in the the standard manner using a 2-step process. First the training data To tune the values for the log-linear weights in our system, we use the standard minimum error-rate training procedure (MERT) [9]. The weights for the models were tuned using the development data supplied for the task. To perform the MERT tuning we used the publicly available ZMERT framework [10], and this allowed us to easily add and tune additional features into our models. The models were tuned with respect the BLEU metric [11]: ‘BLEU4 Closest’ that is built into the tool. 3. Rule-based Decoding Constraints 3.1. Motivation Translating long and complex sentences has been a critical problem in machine translation. A standard phrase-based statistical machine translation system cannot solve the problem of word reordering in the target when the source sentence has a complex structure. A syntax-based machine translation system could solve the problem by running a parser on the source sentence in order to get the syntactic structure, but when a sentence is long and complex, the parser may fail to give a correct parse tree."
2011.iwslt-evaluation.5,P03-1054,0,0.0202175,"hat is built into the tool. 3. Rule-based Decoding Constraints 3.1. Motivation Translating long and complex sentences has been a critical problem in machine translation. A standard phrase-based statistical machine translation system cannot solve the problem of word reordering in the target when the source sentence has a complex structure. A syntax-based machine translation system could solve the problem by running a parser on the source sentence in order to get the syntactic structure, but when a sentence is long and complex, the parser may fail to give a correct parse tree. Klein and Manning [12] have shown that the accuracy of parsing decreases as sentence length increases, and the parsing time increases dramatically. However, in this research, we found that even when a sentence is long and complex, it is possible to split a sentence into smaller units which can be translated separately with minor consideration of the context. The main problem here is locating the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almo"
2011.iwslt-evaluation.5,W09-0429,0,0.0596069,"is long and complex, it is possible to split a sentence into smaller units which can be translated separately with minor consideration of the context. The main problem here is locating the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a"
2011.iwslt-evaluation.5,C94-1069,0,0.0497907,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,1995.iwpt-1.8,0,0.0237346,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,P96-1025,0,0.0947943,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,W04-1101,0,0.0282081,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,P11-1064,1,0.886228,"t out that this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a supervised discriminative model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In our system we employ a related technique that is able to perform direct phrase-to-phrase alignment and extraction in a single unified framework in a fully unsupervised manner [26]. The technique is based on a Pitman-Yor process model. Bayesian models of this form have recently proved themselves useful in the field of natural language processing, as they typically offer benefits over more traditional techniques based on maximum likelihood. In particular, they model the data according to a power law distribution that is often observed in linguistic data. Moreover, by encouraging the re-use of parameters in the model during training, Bayesian models of this type will prefer to build very compact models with few parameters that have a tendency not to over-fit the data. In"
2011.iwslt-evaluation.5,J07-2003,0,0.148834,"Missing"
2011.iwslt-evaluation.5,D10-1087,0,0.0658909,"Missing"
2011.iwslt-evaluation.5,W00-1308,0,0.0546915,"Missing"
2011.iwslt-evaluation.5,N03-1033,0,0.0127114,"Missing"
2011.iwslt-evaluation.5,P00-1056,0,0.27701,"man here is an out-of-vocabulary word and is marked with a ‘|’ symbol.). Decoding constraints Unconstrained decoding Constrained decoding BLEU score 10.84 11.16 Table 4: The effect of re-ordering constraints on translation quality. 4. Bayesian Alignment 4.1. Motivation In a standard phrase-based statistical machine translation system (and in the base system we used in this shared evaluation), a two-step alignment and extraction process is commonly used. In the first step, word-level alignment is performed both from source-to-target and from target-to-source using the publicly available GIZA++ [24] tool. In the second step, these two word-level alignments are combined and by means of a set of heuristics, a large set of bilingual phrase pairs that are consistent with these alignments are extracted. This approach although inelegant has proven itself to be highly effective in practice, and this is the reason for its pervasiveness. However, other approaches are possible. DeNero and Klein [25] point out that this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a"
2011.iwslt-evaluation.5,P10-1147,0,0.0337203,"two-step alignment and extraction process is commonly used. In the first step, word-level alignment is performed both from source-to-target and from target-to-source using the publicly available GIZA++ [24] tool. In the second step, these two word-level alignments are combined and by means of a set of heuristics, a large set of bilingual phrase pairs that are consistent with these alignments are extracted. This approach although inelegant has proven itself to be highly effective in practice, and this is the reason for its pervasiveness. However, other approaches are possible. DeNero and Klein [25] point out that this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a supervised discriminative model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In our system we employ a related technique that is able to perform direct phrase-to-phrase alignment and extraction in a single unified framework in a fully unsupervised ma"
2011.iwslt-evaluation.5,J93-2003,0,\N,Missing
2011.iwslt-evaluation.5,D08-1076,0,\N,Missing
2011.iwslt-evaluation.5,P07-2045,0,\N,Missing
2011.iwslt-evaluation.5,2010.iwslt-evaluation.18,1,\N,Missing
2011.mtsummit-papers.37,P05-1074,0,0.0256923,"TF , not simply find a paraphrase for the input sentence. There are also two minor differences. One difference is the calculation of P (f |f  ). P (f |f  ) can be represented as (1) How to select one of eij is not discussed in this paper because we focus on the retrieval part of TMs. 3 326 P (fp |fp ) where fp and fp is one of the set of paraphrase pairs of f and f  , respectively, and P (fp |fp ) is the paraphrase probability of fp given fp . Quirk et al. calculated P (fp |fp ) from monolingual parallel corpora. In contrast, we calculate P (fp |fp ) from bilingual parallel corpora (Bannard and Callison-Burch, 2005). Another difference lies in the implementation. They used an in-house decoder which was very much like a phrase-based SMT monotone decoder. We use weighted finite state transducers (WFSTs) implemented with open-source software tools. 3.3 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. Their idea is, if two different phrases fp1 , fp2 in one language are aligned to the same phrase ep in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows"
2011.mtsummit-papers.37,N06-1003,0,0.0147483,"m TMs for MT has been proposed by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Paraphrases for Retrieving Sentences from TMs P (f |f  ) = We first define a TM. A TM, T , is defined as: T = {fi , ei1 , . . . , eij , . . . , eiNi |1 ≤ i ≤ N } where fi is the i-th source language sentence, eij is the j-th translation of fi , Ni is the number of translations of fi , and N is the number of unique source sentences in T . We use TF to denote the set of source language sentences in T , i.e., TF = {fi |1 ≤ i ≤ N }. Given an input sentence f , we retrieve the fi from TF that receives the highest score according to a scoring function. Then, we use one o"
2011.mtsummit-papers.37,P09-1053,0,0.0242961,"state transducers (WFSTs) implemented with open-source software tools. 3.3 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. Their idea is, if two different phrases fp1 , fp2 in one language are aligned to the same phrase ep in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows: (1) Build a phrase table: Build a phrase table from parallel corpus using standard SMT tech4 A statistical model for paraphrase detection has also been proposed (Das and Smith, 2009). Their system detects whether two input sentences are paraphrases of one another. However, it does not use paraphrases for searching TMs. niques. (We used the Moses toolkit (Koehn et al., 2007).) (2) Filter the phrase table by the sigtest-filter: The phrase table built in (1) has many inappropriate phrase pairs. Therefore, we filter the phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). (3) Calculate the paraphrase probability: Calculate the paraphrase probability P (fp2 |fp1 ) that fp2 is a paraphrase of fp1 . P (fp2 |fp1 ) =  ep P (fp2 |ep"
2011.mtsummit-papers.37,C10-2043,0,0.0382665,"Missing"
2011.mtsummit-papers.37,D07-1103,0,0.0294813,"(1) Build a phrase table: Build a phrase table from parallel corpus using standard SMT tech4 A statistical model for paraphrase detection has also been proposed (Das and Smith, 2009). Their system detects whether two input sentences are paraphrases of one another. However, it does not use paraphrases for searching TMs. niques. (We used the Moses toolkit (Koehn et al., 2007).) (2) Filter the phrase table by the sigtest-filter: The phrase table built in (1) has many inappropriate phrase pairs. Therefore, we filter the phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). (3) Calculate the paraphrase probability: Calculate the paraphrase probability P (fp2 |fp1 ) that fp2 is a paraphrase of fp1 . P (fp2 |fp1 ) =  ep P (fp2 |ep )P (ep |fp1 ) where P (fp2 |ep ) and P (ep |fp1 ) are phrase translation probabilities. (4) Acquire a paraphrase pair. Acquire (fp1 , fp2 ) as a paraphrase pair if P (fp2 |fp1 ) > P (fp1 |fp1 ). The purpose of this threshold is to keep highly-accurate paraphrase pairs. 3.4 Implementation using WFSTs We use WFSTs to retrieve sentences in a TM. Given an input sentence f , the best sentence fˆ in Equation (1) is represented in Equation (2"
2011.mtsummit-papers.37,P07-2045,0,0.00355409,"is, if two different phrases fp1 , fp2 in one language are aligned to the same phrase ep in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows: (1) Build a phrase table: Build a phrase table from parallel corpus using standard SMT tech4 A statistical model for paraphrase detection has also been proposed (Das and Smith, 2009). Their system detects whether two input sentences are paraphrases of one another. However, it does not use paraphrases for searching TMs. niques. (We used the Moses toolkit (Koehn et al., 2007).) (2) Filter the phrase table by the sigtest-filter: The phrase table built in (1) has many inappropriate phrase pairs. Therefore, we filter the phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). (3) Calculate the paraphrase probability: Calculate the paraphrase probability P (fp2 |fp1 ) that fp2 is a paraphrase of fp1 . P (fp2 |fp1 ) =  ep P (fp2 |ep )P (ep |fp1 ) where P (fp2 |ep ) and P (ep |fp1 ) are phrase translation probabilities. (4) Acquire a paraphrase pair. Acquire (fp1 , fp2 ) as a paraphrase pair if P (fp2 |fp1 ) > P (fp1 |fp1 )."
2011.mtsummit-papers.37,D09-1040,0,0.0434158,"Missing"
2011.mtsummit-papers.37,P10-2001,1,0.868418,"d by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Paraphrases for Retrieving Sentences from TMs P (f |f  ) = We first define a TM. A TM, T , is defined as: T = {fi , ei1 , . . . , eij , . . . , eiNi |1 ≤ i ≤ N } where fi is the i-th source language sentence, eij is the j-th translation of fi , Ni is the number of translations of fi , and N is the number of unique source sentences in T . We use TF to denote the set of source language sentences in T , i.e., TF = {fi |1 ≤ i ≤ N }. Given an input sentence f , we retrieve the fi from TF that receives the highest score according to a scoring function. Then, we use one of eij as the translati"
2011.mtsummit-papers.37,W04-3219,0,0.0224,"Missing"
2011.mtsummit-papers.37,W03-0311,1,0.801179,"t, the paraphrase retrieval proposed in this paper will use the translations of the retrieved sentences without modification. For example, if “is there a beauty parlor?” is retrieved when “is there a salon?” is given as an input, we simply output 2 Previous TM systems could use a thesaurus to detect paraphrases. However, large scale thesauruses do not exist for most languages. In this paper, we propose a method that uses only parallel corpora for getting paraphrases. the translation of “is there a beauty parlor?” without modification. Retrieving sentences from TMs for MT has been proposed by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Parap"
2011.mtsummit-papers.37,2009.mtsummit-papers.14,0,0.102253,"we propose a method that uses only parallel corpora for getting paraphrases. the translation of “is there a beauty parlor?” without modification. Retrieving sentences from TMs for MT has been proposed by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Paraphrases for Retrieving Sentences from TMs P (f |f  ) = We first define a TM. A TM, T , is defined as: T = {fi , ei1 , . . . , eij , . . . , eiNi |1 ≤ i ≤ N } where fi is the i-th source language sentence, eij is the j-th translation of fi , Ni is the number of translations of fi , and N is the number of unique source sentences in T . We use TF to denote the set of source language sentences in"
2011.mtsummit-papers.37,W01-1401,1,0.874099,"ntroduction Translation memories (TMs1 ) are very useful tools for translating texts in narrow domains, where replications of sentences are abundant. In such a case, a machine translation (MT) system can simply search for a match of the input sentence in the TM, and if a match is found, output its corresponding translation. TMs may also use soft matching, finding a sentence that is similar, but not identical to the input sentence. In this case, the translations of these similar sentences are modified to produce appropriate output translations. A number of MT systems have used TMs in this way (Sumita, 2001; Vogel et al., 2004; Zhechev and van Genabith, 2010). In this paper, we propose the use of paraphrases for searching TMs. By using paraphrases, we can retrieve sentences that have the same meaning as the input sentences even if the actual words of the sentences do not match exactly. Note that previous TM systems retrieve similar sentences based on the number of differing words in the sequence. For example, they would prefer “is 1 We use the term TM to refer to a set of parallel sentences. 325 there a pen?” over “is there a beauty parlor?”, when they are given “is there a salon?” as an input."
2011.mtsummit-papers.37,2004.iwslt-evaluation.11,0,0.180725,"anslation memories (TMs1 ) are very useful tools for translating texts in narrow domains, where replications of sentences are abundant. In such a case, a machine translation (MT) system can simply search for a match of the input sentence in the TM, and if a match is found, output its corresponding translation. TMs may also use soft matching, finding a sentence that is similar, but not identical to the input sentence. In this case, the translations of these similar sentences are modified to produce appropriate output translations. A number of MT systems have used TMs in this way (Sumita, 2001; Vogel et al., 2004; Zhechev and van Genabith, 2010). In this paper, we propose the use of paraphrases for searching TMs. By using paraphrases, we can retrieve sentences that have the same meaning as the input sentences even if the actual words of the sentences do not match exactly. Note that previous TM systems retrieve similar sentences based on the number of differing words in the sequence. For example, they would prefer “is 1 We use the term TM to refer to a set of parallel sentences. 325 there a pen?” over “is there a beauty parlor?”, when they are given “is there a salon?” as an input. This is because “is"
2011.mtsummit-papers.37,W10-3806,0,0.063305,"Missing"
2012.iwslt-evaluation.10,2012.iwslt-evaluation.1,0,0.0411309,"two stages, where the systems of the second stage are adapted in an unsupervised manner on the combination of the ﬁrst stage outputs using VTLN, MLLR, and cMLLR. Index Terms: speech recognition, IWSLT, TED talks, evaluation system, system development 1. Introduction The International Workshop on Spoken Language Translation (IWSLT) offers a comprehensive evaluation campaign on spoken language translation. One part of the campaign focuses on the translation of TED Talks1 , short 5-25min presentations by people from various ﬁelds related in some way to Technology, Entertainment, and Design (TED) [1]. In order to evaluate different aspects of this task IWSLT organizes several evaluation tracks on this data covering the aspects of automatic speech recognition (ASR), machine translation (MT), and the full-ﬂedged combination of the two of them into speech translation systems. The goal of the TED ASR track is the automatic transcription of TED lectures on a given segmentation, in order to interface with the machine translation components in the speech-translation track. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our English ASR"
2012.iwslt-evaluation.10,2011.iwslt-evaluation.12,1,0.446759,"nslation (MT), and the full-ﬂedged combination of the two of them into speech translation systems. The goal of the TED ASR track is the automatic transcription of TED lectures on a given segmentation, in order to interface with the machine translation components in the speech-translation track. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our English ASR systems with which we participated in the TED ASR track of the 2012 IWSLT evaluation campaign. This year, our system is a further development of our last year’s evaluation system [2] and makes use of system combination and cross-adaptation, by utilising acoustic models which are trained with different acoustic front-ends and employ two different phoneme sets. In addition to last year, we also included TED talks available via TED’s website by training on them in a slightly supervised manner. We submitted two primary systems. One was solely developed by KIT, the other one was developed in cooperation with NAIST in Japan. A description of the additional work done by NAIST on the KIT-NAIST (contrastive) submission can be found in [3]. On the 2011 evaluations set, which serves"
2012.iwslt-evaluation.10,2012.iwslt-evaluation.11,1,0.864393,"pment of our last year’s evaluation system [2] and makes use of system combination and cross-adaptation, by utilising acoustic models which are trained with different acoustic front-ends and employ two different phoneme sets. In addition to last year, we also included TED talks available via TED’s website by training on them in a slightly supervised manner. We submitted two primary systems. One was solely developed by KIT, the other one was developed in cooperation with NAIST in Japan. A description of the additional work done by NAIST on the KIT-NAIST (contrastive) submission can be found in [3]. On the 2011 evaluations set, which serves as a progress test set, we were able to reduce the word error rate of our transcription 1 http://www.ted.com/talks Text corpus IWSLT training data transcripts News (+news commentary) Parallel Giga Corpus LDC English Gigaword 4 UN + Europarl documents Google Books Ngrams (subset) total Word Count 3 million 2114 million 523 million 1800 million 376 million 1000 million ngrams 4816 million sources 2 4 1 6 1 1 15 Table 1: Language Model training data word count per corpus after cleaning and data selection and number of text sources included in corpus. Th"
2012.iwslt-evaluation.10,2011.iwslt-papers.2,1,0.705542,"of the ﬁnal feature vectors was empirically proven to work well and coincides with the dimensionality of a 14 dimensional static feature vector augmented with ﬁrst and second order dynamic features. In recent years neural network based features have been shown to improve ASR systems [6]. A typical setup involves training a neural network to recognize phones (or phone-states) from a window of ordinary (e.g. MFCC) feature vectors. With the help a hidden bottleneck layer the trained network can be used to project the input features onto a feature vector with an arbitrarily chosen dimensionality [7]. The input vector is derived from a 15 frame context window with each frame containing 20 MFCC or MVDR coefﬁcients. So far, we used LDA to reduce the dimensionality of this input vector, which limits the resulting LDA-features to linear combinations of the input features. A multi layer perceptron (MLP) with the bottleneck in the 2nd hidden layer can make use of nonlinear information. For our IWSLT systems we used bottleneck features for both our MVDR and MFCC front ends. 4. Acoustic Modeling 4.1. Data Preprocessing For the TED data only subtitles were available so the data had to be split int"
2012.iwslt-evaluation.11,2011.iwslt-evaluation.1,1,0.866959,"Missing"
2012.iwslt-evaluation.11,P07-1085,0,0.0270438,"Missing"
2012.iwslt-evaluation.11,N10-1103,0,0.0548812,"Missing"
2012.iwslt-evaluation.5,2012.iwslt-evaluation.1,0,0.0315298,"D task, exploring issues such as out-of-domain data ﬁltering, minimum Bayes risk decoding, MERT vs. PRO tuning, word alignment combination, and morphology. Decoding Baseline NAIST Submission dev2010 26.02 27.05 tst2010 29.75 31.81 Table 1: The scores for systems with and without the proposed improvements. ogy processing and large language models, which resulted in an average gain of 1.18 BLEU points over all languages. Section 4 describes these results in further detail. 2. English-French System 1. Introduction This paper describes the NAIST participation in the IWSLT 2012 evaluation campaign [1]. We participated in all 11 TED tasks, dividing our efforts in half between the ofﬁcial English-French track and the 10 other unofﬁcial ForeignEnglish tracks. For all tracks we used the Moses decoder [2] and its experiment management system to run a large number of experiments with different settings over many language pairs. For the English-French system we experimented with a number of techniques, settling on a combination that provided signiﬁcant accuracy improvements without introducing unnecessary complexity into the system. In the end, we chose a four-pronged approach consisting of using"
2012.iwslt-evaluation.5,P07-2045,0,0.0162664,"2 27.05 tst2010 29.75 31.81 Table 1: The scores for systems with and without the proposed improvements. ogy processing and large language models, which resulted in an average gain of 1.18 BLEU points over all languages. Section 4 describes these results in further detail. 2. English-French System 1. Introduction This paper describes the NAIST participation in the IWSLT 2012 evaluation campaign [1]. We participated in all 11 TED tasks, dividing our efforts in half between the ofﬁcial English-French track and the 10 other unofﬁcial ForeignEnglish tracks. For all tracks we used the Moses decoder [2] and its experiment management system to run a large number of experiments with different settings over many language pairs. For the English-French system we experimented with a number of techniques, settling on a combination that provided signiﬁcant accuracy improvements without introducing unnecessary complexity into the system. In the end, we chose a four-pronged approach consisting of using the web data with ﬁltering to remove noisy sentences, phrase table smoothing, language model interpolation, and minimum Bayes risk decoding. This led to a score of 31.81 BLEU on the tst2010 data set, a"
2012.iwslt-evaluation.5,N03-1017,0,0.111297,"urposes, in Section 3, we also present additional experiments that gave negative results, which were not included in our ofﬁcial submission. For the 10 translation tasks into English, we focused on techniques that could be used widely across all languages. In particular, we experimented with unsupervised approaches to handling source-side morphology, minimum Bayes risk decoding, and large language models. In the end, most of our systems used a combination of unsupervised morpholThe NAIST English-French translation system for IWSLT 2012 was based on phrase-based statistical machine translation [3] using the Moses decoder [2] and its corresponding training regimen. Overall, we made four enhancements over the standard Moses setup to improve the translation accuracy: Large-scale Data with Filtering: In order to use the large, but noisy parallel training data in the English-French Giga Corpus, we implemented a technique to ﬁlter out noisy translated text. Phrase Table Smoothing: We performed phrase table smoothing to improve the probability estimates of low-frequency phrases. Language Model Interpolation: In order to adapt to the domain of the task, we interpolated language models trained"
2012.iwslt-evaluation.5,J05-4003,0,0.0924584,"Missing"
2012.iwslt-evaluation.5,2011.iwslt-evaluation.9,0,0.0502184,"Missing"
2012.iwslt-evaluation.5,J93-2003,0,0.0294894,"Missing"
2012.iwslt-evaluation.5,W99-0604,0,0.275234,"Missing"
2012.iwslt-evaluation.5,P02-1038,0,0.0218946,"s comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smoothing of the phrase table improves results by a signiﬁcant amount. In initial research on MBR, the space of possible hypotheses E was deﬁned as the n-best list output by the decoder. This was further expanded by [14], who deﬁned MBR over lattices. We tested both of these approaches (as implemented in the Moses decoder). Finally, one ﬁne point about MBR is that it requires a good estimate of the probability P (E  |F ) of hypotheses. In the discriminative training framework of [15], which is used in most modern SMT systems, scores of machine translation hypotheses are generally deﬁned as a log-linear combination of feature functions such as language model or translation model probabilities P (E  |F ) = 2.3. Language Model Interpolation One of the characteristics of the IWSLT TED task is that, as shown in Table 2, we have several heterogeneous corpora. In addition, the in-domain TED data is relatively small, so it can be expected that we will beneﬁt from using data outside of the TED domain. In order to effectively utilize out-ofdomain data in language modeling, we buil"
2012.iwslt-evaluation.5,P03-1021,0,0.0174134,"ns. 1 i wi φi (E  ,F ) e Z (4) where φi indicates feature functions such as the language model, translation model, and reordering model log probabilities, wi is the weight measuring the relative importance of this feature, and Z is a partition function that ensures that the probabilities add to 1. Choosing the weights wi for each feature such that the answer with highest probability ˆ = argmax P (E|F ) E E (5) is the best possible translation is a process called “tuning,” and essential to modern SMT systems. However, in most tuning methods, including the standard minimum error rate training [16] that was used in the proposed system, while the relative weight of each feature wi is adjusted, the overall sum of the weights i wi is generally set ﬁxed at 1. While this is not a problem when ﬁnding the highest probability hypothesis in 5, it will affect the probability estimates P (E  |F ), with 56 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Decoding Viterbi MBR (λ = 1) Lattice MBR (λ = 1) Lattice MBR (λ = 5) dev2010 27.59 27.29 26.70 27.05 tst2010 31.01 31.24 31.25 31.81 Table 6: BLEU Results using Minimum Bayes Risk decoding. larger s"
2012.iwslt-evaluation.5,2010.iwslt-papers.5,1,0.897262,"Missing"
2012.iwslt-evaluation.5,W06-1607,0,0.01659,"pothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we use one minus sentence-wise BLEU+1 score [13] as our loss function L(E  , E) = 1 − BLEU+1(E  , E). (3) 2.2. Phrase Table Smoothing We also performed experiments that used smoothing of the statistics used in calculating translation model probabilities [9]. The motivation behind this method is that the statistics used to train the phrase table are generally sparse, and tend to over-estimate the probabilities of rare events. In the submitted system we used Good-Turing smoothing for the phrase table probabilities. Results comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smoothing of the phrase table improves results by a signiﬁcant amount. In initial research on MBR, the space of possible hypotheses E was deﬁned as the n-best list output by the decoder. This was further expanded by ["
2012.iwslt-evaluation.5,P12-1018,1,0.830308,"ch TED task. 3.2. Word Alignment & Phrase Table Combination We investigated different alignment tools and ways to combine them, as shown in Table 8. Observations are as follows: • GIZA++ and BerkeleyAligner achieve similar BLEU on this task. • Concatenating GIZA++ and BerkeleyAligner word alignment results, prior to phrase extraction, achieves a small boost (29.57 to 29.89 BLEU). We experimented with the simplest approach to exploiting out-of-domain bitext in translation models: data concatenation. This can be seen as adaptation at the earliest stage of the • We also experimented with pilaign [19], a Bayesian phrasal alignment toolkit. This tool directly extracts phrases without resorting to the preliminary step of word alignments, and achieves extremely compact phrase table sizes (0.8M entries) without signiﬁcantly sacriﬁcing BLEU (29.24). 3 It should be noted that due to constraints in the available data for these MBR experiments we are both tuning on testing on tst2010, but the tuning of λ also demonstrated gains in accuracy on the ofﬁcial blind test on tst2011 and tst2012 (37.33→37.90 and 38.92→39.47 respectively). • Combining the GIZA++ and pialign phrase tables by Moses’ multiple"
2012.iwslt-evaluation.5,D11-1125,0,0.0529106,", the number of random restarts was set to 20. 3.3. Lexical Reordering Models Several reordering models available in the Moses decoder were tried. In general, we found the full “msd-bidir-fe” option to perform best, despite the small number of word order differences between English and French. Results are shown in Table 9. Reordering model msd-bidir-fe msd-bidir-f monotonicity-bidir-fe msd-backward-fe distance msd-bidir-fe-collapse BLEU 29.57 29.43 29.29 29.22 28.99 28.86 Table 9: Comparison of Reordering models on tst2010. 3.4. MERT vs. PRO tuning We compared two tuning methods: MERT and PRO [20]. We used the implementations distributed with Moses. For both MERT and PRO, we set the size of k-best list to k = 100, used 14 standard features, and removed duplicates in k-best lists when merging previously generated k-best lists. We ran MERT in multi-threaded setting until convergence. Since the number of random restarts in MERT greatly affects on the translation accuracy [21], we tried various number of random restarts for 1, 10, 20, and 50.4 For PRO, we used MegaM5 as a binary classiﬁer with the default setting. We ran PRO for 25 iterations. We tried two kinds of PRO: [20] interpolated t"
2012.iwslt-evaluation.5,C08-1074,0,0.0166288,"ir-fe msd-backward-fe distance msd-bidir-fe-collapse BLEU 29.57 29.43 29.29 29.22 28.99 28.86 Table 9: Comparison of Reordering models on tst2010. 3.4. MERT vs. PRO tuning We compared two tuning methods: MERT and PRO [20]. We used the implementations distributed with Moses. For both MERT and PRO, we set the size of k-best list to k = 100, used 14 standard features, and removed duplicates in k-best lists when merging previously generated k-best lists. We ran MERT in multi-threaded setting until convergence. Since the number of random restarts in MERT greatly affects on the translation accuracy [21], we tried various number of random restarts for 1, 10, 20, and 50.4 For PRO, we used MegaM5 as a binary classiﬁer with the default setting. We ran PRO for 25 iterations. We tried two kinds of PRO: [20] interpolated the weights with previously learned weights to improve the stability (henceforth “PRO-interpolated”)6 , and 4 Currently, Moses’s default setting is 20. ˜hal/megam/ 6 We set the same interpolation coefﬁcient value of 0.1 as [20] noted. 5 http://www.cs.utah.edu/ the version that do not use such a interpolation (henceforth “PRO-basic”). We ﬁrst investigate the effect of the number of"
2012.iwslt-evaluation.5,N04-1022,0,0.0390465,"ing. LM TED Only Without Interp. With Interp. dev2010 24.80 26.30 27.05 tst2010 29.44 31.15 31.81 Table 5: Results training the language model on only TED data, and when other data is used without and with language model interpolation. results are shown in Table 3. As a result, we can see that using the data from the Giga corpus has a positive effect on the results, but ﬁltering does not have a clear signiﬁcant effect on the results. 2.4. Minimum Bayes Risk Decoding Finally, we experimented with improved decoding strategies for translation, particularly using minimum Bayes risk decoding (MBR, [12]). In normal translation, the decoder attempts to simply ﬁnd the answer with the highest probability among the translation candidates ˆ = argmax P (E|F ) E E (1) in a process called Viterbi decoding. As an alternative to this, MBR attempts to ﬁnd the hypothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we"
2012.iwslt-evaluation.5,C04-1072,0,0.0264753,"empts to simply ﬁnd the answer with the highest probability among the translation candidates ˆ = argmax P (E|F ) E E (1) in a process called Viterbi decoding. As an alternative to this, MBR attempts to ﬁnd the hypothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we use one minus sentence-wise BLEU+1 score [13] as our loss function L(E  , E) = 1 − BLEU+1(E  , E). (3) 2.2. Phrase Table Smoothing We also performed experiments that used smoothing of the statistics used in calculating translation model probabilities [9]. The motivation behind this method is that the statistics used to train the phrase table are generally sparse, and tend to over-estimate the probabilities of rare events. In the submitted system we used Good-Turing smoothing for the phrase table probabilities. Results comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smooth"
2012.iwslt-evaluation.5,E03-1076,0,0.149639,"e baseline results. First, adding additional out-of-domain data (nc=News Commentary, ep=Europarl, un=UN Multitext) to the language model increased results uniformly for all language pairs (line (b) of Table 12). We used an interpolated language model, trained in the same fashion as in our English-French system. Next, we tried two strategies for handling rich morphology in the input. The “CompoundSplit” program in the Moses package was developed for languages with extensive noun compounding, e.g. German, and breaks apart words if sub-parts are seen in the training data over a certain frequency [22]. The alternate “Morfessor” program [23] is an unsupervised morphological analyzer based on the Minimum Description Length principle – it tries to ﬁnd the the smallest set of morphemes that parsimoniously cover the training set. Morfessor is expected to segment more aggressively than CompoundSplit, especially because it can ﬁnd both bound and free morphemes. However, we empirically found that Morfessor segments too aggressively for unknown words (i.e. each character becomes a morpheme), so we do not segment OOV words in dev/test.8 The results in line (c) of Table 12 shows that German beneﬁt mo"
2012.iwslt-evaluation.5,W02-0603,0,0.0311216,"onal out-of-domain data (nc=News Commentary, ep=Europarl, un=UN Multitext) to the language model increased results uniformly for all language pairs (line (b) of Table 12). We used an interpolated language model, trained in the same fashion as in our English-French system. Next, we tried two strategies for handling rich morphology in the input. The “CompoundSplit” program in the Moses package was developed for languages with extensive noun compounding, e.g. German, and breaks apart words if sub-parts are seen in the training data over a certain frequency [22]. The alternate “Morfessor” program [23] is an unsupervised morphological analyzer based on the Minimum Description Length principle – it tries to ﬁnd the the smallest set of morphemes that parsimoniously cover the training set. Morfessor is expected to segment more aggressively than CompoundSplit, especially because it can ﬁnd both bound and free morphemes. However, we empirically found that Morfessor segments too aggressively for unknown words (i.e. each character becomes a morpheme), so we do not segment OOV words in dev/test.8 The results in line (c) of Table 12 shows that German beneﬁt most from CompoundSplit, while Arabic, Rus"
2012.iwslt-evaluation.5,D08-1065,0,\N,Missing
2012.iwslt-papers.2,2011.mtsummit-papers.7,0,0.0422007,"etween duration translation and duration and power translation. Particularly, the former method was often labeled with a score of 2 indicating that the duration is not sufﬁcient to represent emphasis clearly. However, duration+power almost always scored 3 and can be recognized as the position of emphasis. This means that in English-Japanese speech translation, speech’s power is an important factor to convey emphasis. 5. Related Works There have been several studies demonstrating improved speech translation performance by utilizing paralinguistic information of source side speech. For example, [8] focuses on using the input speech’s acoustic information to improve translation accuracy. They try to explore a tight coupling of ASR and MT for speech translation, sharing information on the phone level to boost translation accuracy as measured by BLEU score. Other related works focus on using speech intonation to reduce translation ambiguity on the target side [9, 10]. While the above methods consider paralinguistic information to boost translation accuracy, as we mentioned before, there is more to speech translation than just the accuracy of the target sentence. It is also necessary to con"
2012.iwslt-papers.2,P07-2045,0,0.00260194,"vely, and speech synthesis with TTS. While this is the same general architecture as traditional speech translation systems, we add an additional model to translate not only lexical information but also two types of paralinguistic information: duration and power. In this paper, in order to focus speciﬁcally on paralinguistic translation we chose a simple, small-vocabulary lexical MT task: number-to-number translation. (6) where J indicates the target language sentence and E indicates the recognized source language sentence. Generally we can use a statistical machine translation tool like Moses [4], to obtain this translation in standard translation tasks. However in this paper we have chosen a simple number-tonumber translation task so we can simply write one-to-one lexical translation rules with no loss in accuracy. 3.3. Paralinguistic Translation Paralinguistic translation converts the source-side duration and mean power vector X into the target-side duration and mean power vector Y according to the following equation 159 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 ˆ = arg max P (Y|X). Y Y (7) training data by minimize root mean squ"
2013.iwslt-evaluation.12,N03-1017,0,0.0375042,"an compounds; and system combination of different types of SMT systems based on generalized minimum Bayes risk (GMBR) framework. This paper presents details of our systems and reports the results in German-English and English-German MT tasks in the evaluation campaign. 2. Translation Methods The main feature of our system for this evaluation is that we perform translation using three different translation models and combine the results through system combination. Each of the three methods is described briefly below. 2.1. Phrase-based Machine Translation Phrase-based machine translation (PBMT; [2]) models the translation process by splitting the source sentence into phrases, translating the phrases into target phrases, and reordering the phrases into the target language order. PBMT is currently the most widely used method in SMT as it is robust, does not require the availability of linguistic analysis tools, and achieves high accuracy, particularly for languages with similar syntactic structure. 2.2. Hierarchical Phrase-based Machine Translation Hierarchical phrase-based machine translation (Hiero; [3]) expands the class of translation rules that can be used in phrase-based machine tra"
2013.iwslt-evaluation.12,J07-2003,0,0.106892,"below. 2.1. Phrase-based Machine Translation Phrase-based machine translation (PBMT; [2]) models the translation process by splitting the source sentence into phrases, translating the phrases into target phrases, and reordering the phrases into the target language order. PBMT is currently the most widely used method in SMT as it is robust, does not require the availability of linguistic analysis tools, and achieves high accuracy, particularly for languages with similar syntactic structure. 2.2. Hierarchical Phrase-based Machine Translation Hierarchical phrase-based machine translation (Hiero; [3]) expands the class of translation rules that can be used in phrase-based machine translation by further allowing rules with gaps that can be filled in a hierarchical fashion. Hiero is generally considered to be more accurate than PBMT on language pairs that are less monotonic, but also requires a significantly larger amount of memory and decoding time. As the German-English pair has a significant amount of reordering, particularly with movement of verbs, we can expect that Hiero will be able to handle these reorderings more appropriately in some cases. 2.3. Forest-to-string Machine Translatio"
2013.iwslt-evaluation.12,P06-1077,0,0.131208,"at can be used in phrase-based machine translation by further allowing rules with gaps that can be filled in a hierarchical fashion. Hiero is generally considered to be more accurate than PBMT on language pairs that are less monotonic, but also requires a significantly larger amount of memory and decoding time. As the German-English pair has a significant amount of reordering, particularly with movement of verbs, we can expect that Hiero will be able to handle these reorderings more appropriately in some cases. 2.3. Forest-to-string Machine Translation Tree-to-string machine translation (T2S; [4]) performs translation by first syntactically parsing the source sentence, then translating from sub-structures of the parse to a string in the target language. Forest-to-string machine translation (F2S; [5]) generalizes this framework, making it possible to not only translate the single one-best syntactic parse, but a packed forest that encodes many possible parses, helping to pass along some of the ambiguity of parsing to be resolved during translation. While there are a number of proposed methods for incorporating source-side syntax into the translation process, here we use a method based o"
2013.iwslt-evaluation.12,D08-1022,0,0.0240076,"irs that are less monotonic, but also requires a significantly larger amount of memory and decoding time. As the German-English pair has a significant amount of reordering, particularly with movement of verbs, we can expect that Hiero will be able to handle these reorderings more appropriately in some cases. 2.3. Forest-to-string Machine Translation Tree-to-string machine translation (T2S; [4]) performs translation by first syntactically parsing the source sentence, then translating from sub-structures of the parse to a string in the target language. Forest-to-string machine translation (F2S; [5]) generalizes this framework, making it possible to not only translate the single one-best syntactic parse, but a packed forest that encodes many possible parses, helping to pass along some of the ambiguity of parsing to be resolved during translation. While there are a number of proposed methods for incorporating source-side syntax into the translation process, here we use a method based on tree-to-string transducers [6]. Syntax-driven methods such as T2S and F2S are particularly useful for language pairs with extremely large amounts of reordering, as the syntactic parse can help guide the ac"
2013.iwslt-evaluation.12,N04-1014,0,0.0445956,"st syntactically parsing the source sentence, then translating from sub-structures of the parse to a string in the target language. Forest-to-string machine translation (F2S; [5]) generalizes this framework, making it possible to not only translate the single one-best syntactic parse, but a packed forest that encodes many possible parses, helping to pass along some of the ambiguity of parsing to be resolved during translation. While there are a number of proposed methods for incorporating source-side syntax into the translation process, here we use a method based on tree-to-string transducers [6]. Syntax-driven methods such as T2S and F2S are particularly useful for language pairs with extremely large amounts of reordering, as the syntactic parse can help guide the accurate re-ordering of entire phrases or clauses. On the other hand, these methods are highly dependent on parsing accuracy, and also have limits on the rules that can be extracted, and are somewhat less robust than the previous two methods. different syntactic parser that did not provide this information. 3. SMT Technologies 3.2.2. English-to-German 3.1. Training data selection The target TED domain is different in both s"
2013.iwslt-evaluation.12,P13-2119,1,0.927042,"these methods are highly dependent on parsing accuracy, and also have limits on the rules that can be extracted, and are somewhat less robust than the previous two methods. different syntactic parser that did not provide this information. 3. SMT Technologies 3.2.2. English-to-German 3.1. Training data selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data1 ). To address this domain adaption problem, we performed adaptation training data selection using the method of [7].2 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [8]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. By taking a sub-sample (same size as the target-domain data), we reduce training t"
2013.iwslt-evaluation.12,D11-1033,0,0.0655671,"mation. 3. SMT Technologies 3.2.2. English-to-German 3.1. Training data selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data1 ). To address this domain adaption problem, we performed adaptation training data selection using the method of [7].2 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [8]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. By taking a sub-sample (same size as the target-domain data), we reduce training time and avoid training and testing language models on the same general-domain data. Similarly, INF (f ) and GENF (f ) are the cross-entropies of f on Foreign-side LM. Finally, sentence pairs are ranked according to Eq. 1 and those w"
2013.iwslt-evaluation.12,P05-1066,0,0.146142,") are added together with the in-domain bitext for translation model training. Here, the LMs are Recurrent Neural Network Language Models (RNNLMs), which have been shown to outperform n-gram LMs in this problem [7]. 3.2. Syntactic Rule-based Pre-ordering Preordering is a method that attempts to first re-order the source sentence into a word order that is closer to the target. As German and English have significantly different word order, we can imagine that this will help our accuracy for this language pair. 3.2.1. German-to-English We applied the clause restructuring method of Collins et al. [9] for German pre-ordering. The method is mainly based on moving German verbs in the end of clause structures towards the beginning of the clause. We re-implemented the method for German parse trees created using the Berkeley parser trained on TIGER corpus. We ignored some additional syntactic information such as subject markers and heads implemented in the original method of [9], because we used a 1 To give a sense of the domain difference, a 4-gram LM trained with Kneser-Ney smoothing on TED data gives a perplexity of 355 on the general domain data, compared to a perplexity of 99 on held-out T"
2013.iwslt-evaluation.12,E03-1076,0,0.18382,"ional feature to each translation hypothesis. We then re-run a single MERT optimization to find ideal weights for this new feature, and then extract the 1-best result from the 10,000-best list for the test set according to these new weights. The parameters for RNNLM training are tuned on the dev set to maximize perplexity, resulting in 300 hidden layers, 300 classes, and 4 steps of back-propogation through time. 3.4. German compound word splitting German compound words present sparsity challenges for machine translation. To address this, we split German words following the general approach of [11]. The idea is to split a word if the geometric average of its subword frequencies is larger than whole word frequency. In our implementation, for each word, we searched for all possible decompositions into two sub-words, considering the possibility of deleting common German fillers “e”, “es”, and “s” (as in ”Arbeit+s+tier”). For simplicity, we did not experiment with splitting into three or more sub-words as done in the compound-splitter.perl script distributed with the Moses package. The unigram frequencies for the subwords and whole word is computed from the German part of the bitext. This s"
2013.iwslt-evaluation.12,I11-1153,1,0.737728,"vial to handle recombination of German split words after reordering and translation. To ensure that the uniform hypotheses space gives the same decision as the original loss in the true space p(e|f ), we use a small development set to tune the parameter θ as follows. For any two hypotheses e1 , e2 , and a reference translation er (possibly not in N (f )) we first compute the true loss: L(e1 |er ) and L(e2 |er ). If L(e1 |er ) &lt; L(e2 |er ), then we would want θ such that: K ∑ ∑ 3.5. GMBR system combination We used a system combination method based on Generalized Minimum Bayes Risk optimization [12], which has been successfully applied to different types of SMT systems for patent translation [13]. Note that our system combination only picks one hypothesis from an N-best list and does not generate a new hypothesis by mixing partial hypotheses among the N-best. θk Lk (e1 |e) &lt; e∈N (f ) k=1 Minimum Bayes Risk (MBR) is a decision rule to choose hypotheses that minimize the expected loss. In the task of SMT from a French sentence (f ) to an English sentence (e), the MBR decision rule on δ(f ) → e′ with the loss function L over the possible space of sentence pairs (p(e, f )) is denoted as: ∑ a"
2013.iwslt-evaluation.12,P10-1017,0,0.0236144,"akes the problem amendable to solutions in “learning to rank” literature [15]. We used BLEU as the objective function and the subcomponents of BLEU as features (system identity feature was not used). There is one regularization hyperparameter for the Ranking SVM, which we set by cross-validation over the development set (dev2010). 3.6. What Didn’t Work Immediately We also tried several other methods that did not have a clear positive effect and were thus omitted from the final system. For example, we attempted to improve alignment accuracy using the discriminative alignment method proposed by [16] training on the 300 hand-aligned sentences.4 However, while this provided small gains in alignment accuracy on a held-out set, the gains were likely not enough, and MT results were inconclusive. We also attempted to use the reordering method of [17] as implemented in lader,5 again trained on the same 300 hand-aligned sentences, but increases in reordering accuracy on a held-out set were minimal. We believe that both of these techniques are promising, but require a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/"
2013.iwslt-evaluation.12,D12-1077,1,0.836426,"Ranking SVM, which we set by cross-validation over the development set (dev2010). 3.6. What Didn’t Work Immediately We also tried several other methods that did not have a clear positive effect and were thus omitted from the final system. For example, we attempted to improve alignment accuracy using the discriminative alignment method proposed by [16] training on the 300 hand-aligned sentences.4 However, while this provided small gains in alignment accuracy on a held-out set, the gains were likely not enough, and MT results were inconclusive. We also attempted to use the reordering method of [17] as implemented in lader,5 again trained on the same 300 hand-aligned sentences, but increases in reordering accuracy on a held-out set were minimal. We believe that both of these techniques are promising, but require a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/ tosch/ ˜ downloads.html 5 http://phontron.com/lader 4. Experiments 4.1. Setup 4.1.1. System overview We used three individual SMT systems for each language pairs: forest-to-string (F2S), hierarchical phrase-based (Hiero), and phrase-based with pre-or"
2013.iwslt-evaluation.12,P13-4016,1,0.84637,"believe that both of these techniques are promising, but require a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/ tosch/ ˜ downloads.html 5 http://phontron.com/lader 4. Experiments 4.1. Setup 4.1.1. System overview We used three individual SMT systems for each language pairs: forest-to-string (F2S), hierarchical phrase-based (Hiero), and phrase-based with pre-ordering (Preorder). In some of our comparisons we also use simple phrase-based translation without preordering (PBMT). F2S was implemented with Travatar [18] and Preorder, PBMT, and Hiero were implemented using Moses [19]. For the Moses models, we generally used the default settings, but with Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of comp"
2013.iwslt-evaluation.12,P07-2045,0,0.0147657,"e a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/ tosch/ ˜ downloads.html 5 http://phontron.com/lader 4. Experiments 4.1. Setup 4.1.1. System overview We used three individual SMT systems for each language pairs: forest-to-string (F2S), hierarchical phrase-based (Hiero), and phrase-based with pre-ordering (Preorder). In some of our comparisons we also use simple phrase-based translation without preordering (PBMT). F2S was implemented with Travatar [18] and Preorder, PBMT, and Hiero were implemented using Moses [19]. For the Moses models, we generally used the default settings, but with Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All s"
2013.iwslt-evaluation.12,P02-1040,0,0.0871245,"t settings, but with Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All systems were evaluated using the standard BLEU score [21] and also RIBES [22], a metric designed specifically to show whether reordering is being performed properly. All systems were optimized towards BLEU score. We measure statistical significance between results with bootstrap resampling with p &gt; 0.05. Bold numbers in each table indicate the best system, and all systems that do not show a statistically significant difference from the best system [23]. All words were lowercased prior to translation, and finally recased by a SMT-based recaser as implemented in Moses. 4.1.2. Translation models We trained the translation models using WIT3 training dat"
2013.iwslt-evaluation.12,D10-1092,1,0.836115,"Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All systems were evaluated using the standard BLEU score [21] and also RIBES [22], a metric designed specifically to show whether reordering is being performed properly. All systems were optimized towards BLEU score. We measure statistical significance between results with bootstrap resampling with p &gt; 0.05. Bold numbers in each table indicate the best system, and all systems that do not show a statistically significant difference from the best system [23]. All words were lowercased prior to translation, and finally recased by a SMT-based recaser as implemented in Moses. 4.1.2. Translation models We trained the translation models using WIT3 training data (138,499 sentences"
2013.iwslt-evaluation.12,W04-3250,0,0.0816664,"lt settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All systems were evaluated using the standard BLEU score [21] and also RIBES [22], a metric designed specifically to show whether reordering is being performed properly. All systems were optimized towards BLEU score. We measure statistical significance between results with bootstrap resampling with p &gt; 0.05. Bold numbers in each table indicate the best system, and all systems that do not show a statistically significant difference from the best system [23]. All words were lowercased prior to translation, and finally recased by a SMT-based recaser as implemented in Moses. 4.1.2. Translation models We trained the translation models using WIT3 training data (138,499 sentences) and 1,000,000 sentences selected over other bitexts (Europarl, News Commentary, and Common Crawl) by the method described in 3.1. 4.1.3. Language models We used two types of word n-gram language models of German and English: interpolated 6-gram and Google 5-gram. The interpolated 6-gram LMs were from linear interpolation of several 6-gram LMs on different data sources (WIT3"
2013.iwslt-evaluation.12,P12-1001,1,0.853229,"at the F2S system did a significantly better job of accurately generating verbs at the end of the German sentence, demonstrating its superior capability for reordering. For German-English, on the other hand, F2S achieved a somewhat counter-intuitive low score on the reordering-based measure RIBES. Upon an analysis of the results, we found that the F2S system was largely getting the reordering right, but occasionally making big changes in reordering large clauses that were not reflected in the German reference. It is likely that if we optimized towards RIBES, or a combination of BLEU and RIBES [25] we might get better results. 4.4. Translation Method Comparison 4.5. Effect of Compound Splitting In this section, we provide a brief comparison of the three translation methods mentioned in Section 2 on tst2010 data. For all systems we used the TED data and 1M selected sentences for training, and used the language model described Next, we examine the effect of compound splitting for German-English translation. From the results in Table 7, we can see that compound splitting provides a gain for all systems, and particularly so for F2S translation. PBMT Hiero F2S en-de n-gram +RNNLM 23.11 23.81"
2013.iwslt-evaluation.12,federico-etal-2012-iwslt,0,\N,Missing
2013.iwslt-evaluation.23,N10-1103,0,0.0377182,"Missing"
2013.iwslt-papers.3,D10-1092,0,0.0188586,"nslators. The underlined score is BLEU and the plain score is RIBES generated and checked by voluntary translators. For the two varieties of interpretation data, I1 and I2, we used the transcriptions of the interpretations performed by the S rank and A rank interpreter respectively. The first motivation for collecting this data is that it may allow us to quantitatively measure the similarity or difference between interpretations and translations automatically. In order to calculate the similarity between each of these pieces of data, we use the automatic similarity measures BLEU [9] and RIBES [10]. As BLEU and RIBES are not symmetric, we average BLEU or RIBES in both directions. For example, we calculate for BLEU using 1 {BLEU(R, H) + BLEU(H, R)} 2 (1) where R and H are the reference and the hypothesis. Based on this data, if the similarities of T1-T2 and I1-I2 are higher than T1-I1, T2-I1, T1-I2 and T2-I2, we can find that there are real differences between the output produced by translators and interpreters, more so than the superficial differences produced by varying expressions. 3.2. Result The result of the similarity is shown in Figure 3. First, we focus on the relationship betwe"
2013.iwslt-papers.3,2011.iwslt-evaluation.18,0,0.0461329,"Missing"
2013.iwslt-papers.3,P11-2093,1,0.744327,"Missing"
2013.iwslt-papers.3,N12-1048,0,0.119056,"hen looking at the source and the translation, the word order is quite different, reversing two long clauses: A and B. In contrast, when looking at the source and the simultaneous interpretation, the word order is similar. If a simultaneous ST system attempts to reproduce the first word order, it will only be able to start translation after it has received the full “A because B.” On the other hand, if the system is able to choose the word order closer to human interpreters, it can begin translation after “A,” resulting in a lower delay. There are several related works about simultaneous ST [2][3][4] that automatically divide longer sentences up into a number of shorter ones similarly to the salami technique employed by simultaneous interpreters. While these related works aim to segment sentences in a similar fashion to simultaneous interpreters, all previous works concerned with sentence segmentation have used translation data (made by translators) for learning of the machine translation system. In addition, while there are other related works about collecting simultaneous interpretation data [5][6][7], all previous works did not compare simultaneous interpreters of multiple experienc"
2013.iwslt-papers.3,shimizu-etal-2014-collection,1,0.823939,"ranslation results closer to a highly experienced simultaneous interpreter than when translation data alone is used in training. We also find that according to automatic evaluation metrics, our system achieves performance similar to that of a simultaneous interpreter that has 1 year of experience. 2. Simultaneous interpretation data As the first step to performing our research, we first must collect simultaneous interpretation data. In this section, we describe how we did so with the cooperation of professional simultaneous interpreters. A fuller description of the corpus will be published in [8]. 2.1. Materials As materials for the simultaneous interpreters to translate, we used TED1 talks, and had the interpreters translate in real time from English to Japanese while watching and listening to the TED videos. We have several reasons for using TED talks. The first is that for many of the TED talks there are already Japanese subtitles available. This makes it possible to compare data created by translators (i.e. the subtitles) with simultaneous interpretation data. TED is also an attractive testbed for machine translation systems, as it covers a wide variety of topics of interest to a"
2013.iwslt-papers.3,P02-1040,0,0.0899235,"reters and translators. The underlined score is BLEU and the plain score is RIBES generated and checked by voluntary translators. For the two varieties of interpretation data, I1 and I2, we used the transcriptions of the interpretations performed by the S rank and A rank interpreter respectively. The first motivation for collecting this data is that it may allow us to quantitatively measure the similarity or difference between interpretations and translations automatically. In order to calculate the similarity between each of these pieces of data, we use the automatic similarity measures BLEU [9] and RIBES [10]. As BLEU and RIBES are not symmetric, we average BLEU or RIBES in both directions. For example, we calculate for BLEU using 1 {BLEU(R, H) + BLEU(H, R)} 2 (1) where R and H are the reference and the hypothesis. Based on this data, if the similarities of T1-T2 and I1-I2 are higher than T1-I1, T2-I1, T1-I2 and T2-I2, we can find that there are real differences between the output produced by translators and interpreters, more so than the superficial differences produced by varying expressions. 3.2. Result The result of the similarity is shown in Figure 3. First, we focus on the rel"
2013.iwslt-papers.3,ma-2006-champollion,0,0.0318601,"Missing"
2013.iwslt-papers.3,P07-2045,0,0.0252123,"Missing"
2013.iwslt-papers.3,J03-1002,0,0.00584698,"Missing"
2013.iwslt-papers.3,P03-1021,0,0.0154201,"Missing"
2014.iwslt-evaluation.18,P08-1023,0,0.0240241,"in the F2S system, and compare it to the more traditional method of cube pruning. The second is that this year we attempted to extract pre-ordering rules automatically from parallel corpora, as opposed to hand-designing preordering rules based on linguistic intuition. This paper presents details of our systems and reports the official results together with some detailed discussions on contributions of the techniques involved. 2.1. Forest-to-String Machine Translation In our previous year’s submission to IWSLT, we achieved promising results using the forest-to-string machine translation (F2S; [3]) framework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven me"
2014.iwslt-evaluation.18,P06-1077,0,0.0368401,"The second is that this year we attempted to extract pre-ordering rules automatically from parallel corpora, as opposed to hand-designing preordering rules based on linguistic intuition. This paper presents details of our systems and reports the official results together with some detailed discussions on contributions of the techniques involved. 2.1. Forest-to-String Machine Translation In our previous year’s submission to IWSLT, we achieved promising results using the forest-to-string machine translation (F2S; [3]) framework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT"
2014.iwslt-evaluation.18,W14-7002,1,0.835139,"o-String Machine Translation In our previous year’s submission to IWSLT, we achieved promising results using the forest-to-string machine translation (F2S; [3]) framework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT this year, we experimented with two different search algorithms to measure the effect that search has on the GermanEnglish and English-German pairs. As the first algorithm, we use the standard method for search in tree-based methods of translation: cube pruning [7]. For each edge to be expanded, cube pruning sorts the child hypotheses in descending order of p"
2014.iwslt-evaluation.18,P14-2024,1,0.898714,"ework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT this year, we experimented with two different search algorithms to measure the effect that search has on the GermanEnglish and English-German pairs. As the first algorithm, we use the standard method for search in tree-based methods of translation: cube pruning [7]. For each edge to be expanded, cube pruning sorts the child hypotheses in descending order of probability, and at every step pops the highest-scoring hypothesis off the stack, calculates its language model scores, and adds the popped, scored edge to the hyperg"
2014.iwslt-evaluation.18,J07-2003,0,0.082081,"for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT this year, we experimented with two different search algorithms to measure the effect that search has on the GermanEnglish and English-German pairs. As the first algorithm, we use the standard method for search in tree-based methods of translation: cube pruning [7]. For each edge to be expanded, cube pruning sorts the child hypotheses in descending order of probability, and at every step pops the highest-scoring hypothesis off the stack, calculates its language model scores, and adds the popped, scored edge to the hypergraph. It should be noted that the LM scores are not calculated until after the edge is popped, and thus the order of visiting edges is based on only an LMfree approximation of the true edge score, resulting in search errors. In our F2S system this year, we test a new method of hypergraph search [2], which aims to achieve better search ac"
2014.iwslt-evaluation.18,P05-1066,0,0.0488395,"Lake Tahoe, December 4th and 5th, 2014 ally, refining the probability estimates until the limit on number of stack pops is reached. In our previous work [6] we have found that hypergraph search achieved superior results to cube pruning, and we hypothesize that these results will carry over to German-English and English-German as well. 2.2. Syntax-based Pre-ordering Pre-ordering is a method that attempts to first reorder the source sentence into a word order that is closer to the target, then translate using a standard method such as PBMT. We used hand-crafted German-English pre-ordering rules [8] in our submission last year. This year’s system uses an automatic method to extract domain-dependent pre-ordering rules, avoiding the time-consuming effort required for creating hand-crafted rules. The pre-ordering method is basically similar to [9], but is limited to reordering of child nodes in syntactic parse trees rather than rewriting and word insertion. Since the pre-ordering does not work perfectly in all cases, we allow for further reordering in the PBMT system that translates the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (d"
2014.iwslt-evaluation.18,C04-1073,0,0.0293599,"esize that these results will carry over to German-English and English-German as well. 2.2. Syntax-based Pre-ordering Pre-ordering is a method that attempts to first reorder the source sentence into a word order that is closer to the target, then translate using a standard method such as PBMT. We used hand-crafted German-English pre-ordering rules [8] in our submission last year. This year’s system uses an automatic method to extract domain-dependent pre-ordering rules, avoiding the time-consuming effort required for creating hand-crafted rules. The pre-ordering method is basically similar to [9], but is limited to reordering of child nodes in syntactic parse trees rather than rewriting and word insertion. Since the pre-ordering does not work perfectly in all cases, we allow for further reordering in the PBMT system that translates the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (dev. set BLEU in this paper). 2.2.1. Reordering Pattern Extraction A reordering pattern represents a reordering of child nodes in a source language parse tree, determined by word alignment. The reordering pattern is similar to a tree-based translation"
2014.iwslt-evaluation.18,N04-1035,0,0.060307,"rd insertion. Since the pre-ordering does not work perfectly in all cases, we allow for further reordering in the PBMT system that translates the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (dev. set BLEU in this paper). 2.2.1. Reordering Pattern Extraction A reordering pattern represents a reordering of child nodes in a source language parse tree, determined by word alignment. The reordering pattern is similar to a tree-based translation pattern called frontier graph fragments, which form the most basic unit in tree-based translation [10], but only holds reordering information on the non-terminal child nodes. A reordering pattern can be extracted from an admissible node [11] in the parse tree that covers a distinct contiguous spans in the corresponding target language sentences. Since such a reordering pattern only is constrained by the syntactic labels on the parent and child nodes, we consider several attributes of reordering patterns: syntactic labels of its grand-parent, left and right siblings of the parent, and surface forms of its child nodes (only when the child is a part-of-speech node). 2.2.2. Deterministic Pre-order"
2014.iwslt-evaluation.18,D07-1078,0,0.0251997,"es the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (dev. set BLEU in this paper). 2.2.1. Reordering Pattern Extraction A reordering pattern represents a reordering of child nodes in a source language parse tree, determined by word alignment. The reordering pattern is similar to a tree-based translation pattern called frontier graph fragments, which form the most basic unit in tree-based translation [10], but only holds reordering information on the non-terminal child nodes. A reordering pattern can be extracted from an admissible node [11] in the parse tree that covers a distinct contiguous spans in the corresponding target language sentences. Since such a reordering pattern only is constrained by the syntactic labels on the parent and child nodes, we consider several attributes of reordering patterns: syntactic labels of its grand-parent, left and right siblings of the parent, and surface forms of its child nodes (only when the child is a part-of-speech node). 2.2.2. Deterministic Pre-ordering In order to make the pre-ordering deterministic, we use reordering rules from dominant reordering patterns that agree with more than 75"
2014.iwslt-evaluation.18,N03-1017,0,0.058736,"Missing"
2014.iwslt-evaluation.18,P13-2119,1,0.840496,"icularly for languages with similar syntactic structure. 3. Additional System Enhancements Here we review techniques that were used in our submission last year [1] and also describe some of our new attempts that were not effective in our pilot test and not included in the final system. 3.1. Training Data Selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data). To address this domain adaption problem, we performed adaptation training data selection using the method of [13].1 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [14]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. Similarly, INF (f ) and GENF (f ) are the crossentropies of f on Foreign-side LM."
2014.iwslt-evaluation.18,D11-1033,0,0.0295922,"our pilot test and not included in the final system. 3.1. Training Data Selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data). To address this domain adaption problem, we performed adaptation training data selection using the method of [13].1 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [14]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. Similarly, INF (f ) and GENF (f ) are the crossentropies of f on Foreign-side LM. Finally, sentence pairs are ranked according to Eq. 1 and those with scores lower than some empirically-chosen threshold are added together with the in-domain bitext for translation model training. Here, the LMs are Recurrent Neural"
2014.iwslt-evaluation.18,E03-1076,0,0.0546193,"milarly, INF (f ) and GENF (f ) are the crossentropies of f on Foreign-side LM. Finally, sentence pairs are ranked according to Eq. 1 and those with scores lower than some empirically-chosen threshold are added together with the in-domain bitext for translation model training. Here, the LMs are Recurrent Neural Network Language Models (RNNLMs), which have been shown to outperform n-gram LMs in this problem [13]. 3.2. German Compound Word Splitting German compound words present sparsity challenges for machine translation. To address this, we split German words following the general approach of [15]. The idea is to split a word if the geometric average of its subword frequencies is larger than whole word frequency. In our implementation, for each word, we searched for all possible decompositions into two sub-words, considering the possibility of deleting common German fillers “e”, “es”, and “s” (as in ”Arbeit+s+tier”). The unigram frequencies for the subwords and 1 Code/scripts available at http://cl.naist.jp/∼kevinduh/a/acl2013 128 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 whole word is computed from the German p"
2014.iwslt-evaluation.18,I11-1153,1,0.863598,"e the RNNLM log probabilities and add them as an additional feature to each translation hypothesis. We then re-run a single MERT optimization to find ideal weights for this new feature, and then extract the 1-best result from the 10,000-best list for the test set according to these new weights. The parameters for RNNLM training are tuned on the dev set to maximize perplexity, resulting in 300 nodes in the hidden layer, 300 classes, and 4 steps of back-propagation through time. 3.4. GMBR System Combination We used a system combination method based on Generalized Minimum Bayes Risk optimization [17], which has been successfully applied to different types of SMT systems for patent translation [18]. Note that our system combination only picks one hypothesis from an N-best list and does not generate a new hypothesis by mixing partial hypotheses among the N-best. 3.4.1. Theory Minimum Bayes Risk (MBR) is a decision rule to choose hypotheses that minimize the expected loss. In the task of SMT from a French sentence (f ) to an English sentence (e), the MBR decision rule on δ(f ) → e′ with the loss function L over the possible space of sentence pairs (p(e, f )) is denoted as: ∑ L(δ(f )|e)p(e|f"
2014.iwslt-evaluation.18,2011.mtsummit-papers.36,1,0.801283,"e improvements from the baseline 1-best in our pilot test, but they were much smaller than those resulting from RNNLM, and when the NNJM was combined with RNNLM we saw no significant gains. One possible reason is the small training data size; the model is very sparse and needs large training data because of its large contexts of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Jou"
2014.iwslt-evaluation.18,D13-1139,1,0.77246,"1-best in our pilot test, but they were much smaller than those resulting from RNNLM, and when the NNJM was combined with RNNLM we saw no significant gains. One possible reason is the small training data size; the model is very sparse and needs large training data because of its large contexts of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treeban"
2014.iwslt-evaluation.18,P13-4016,1,0.849598,"parse and needs large training data because of its large contexts of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank5 for English, and TIGER corpus [26] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 with Kneser-Ney smoothing. For search in the F2S m"
2014.iwslt-evaluation.18,P07-2045,0,0.00853439,"s of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank5 for English, and TIGER corpus [26] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 with Kneser-Ney smoothing. For search in the F2S models, we used the previously described hypergraph search method"
2014.iwslt-evaluation.18,W06-1607,0,0.0253342,"uning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank5 for English, and TIGER corpus [26] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 with Kneser-Ney smoothing. For search in the F2S models, we used the previously described hypergraph search method. For the Moses phrase tables, we used standard training settings with Kneser-Ney smoothing of phrase translation probabilities [27]. 4.1.2. Translation Models We trained the translation models using WIT3 training data (178,526 sentences) and 1,000,000 sentences selected over other bitexts (Europarl, News Commentary, and Common Crawl) by the method described in Section 3.1. 4.1.3. Language Models We used word 5-gram language models of German and English that were linearly interpolated from several word 5-gram language models trained on different data sources (WIT3 , Europarl, News Commentary, and Common Crawl). The interpolation weights were optimized to minimize perplexity on the development set, using interpolate-lm.perl"
2014.iwslt-evaluation.18,P11-1132,0,0.0600041,"Missing"
2014.iwslt-evaluation.18,P14-1129,0,0.0304397,"where a difference exists in the true loss. Then we optimize θ in a formulation similar to a Ranking SVM [19]. The pair-wise nature of Eqs. 6 and 7 makes the problem amendable to solutions in “learning to rank” literature [20]. We used BLEU as the objective function and the subcomponents of BLEU as features (system identity feature was not used). There is one regularization hyperparameter for the Ranking SVM, which we set by cross-validation over the development set (dev2010). 3.5. What Didn’t Work Immediately This year we tried to include a state-of-the-art Neural Network Joint Model (NNJM) [21] to improve the accuracy of translation probability estimation. The model is used to predict a target language word using its three preceding target language words and eleven source language words surrounding its affiliation (the non-NULL source language word aligned to the target language word to be predicted). We used top 16,000 source and target vocabularies in the model and mapped the other words into a single OOV symbol, while the original paper[21] used part-of-speech classes. Although the original paper presented a method for integrating the model with decoding, we used the NNJM for rer"
2014.iwslt-evaluation.18,N13-1116,0,\N,Missing
2014.iwslt-evaluation.18,2013.iwslt-evaluation.12,1,\N,Missing
2014.iwslt-papers.16,P01-1067,0,0.149499,"lly annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and these speechrelated applications. The first version of the corpus includes 1,217 sentences and 23,158 words manually annotated with parse trees, and aligned with translations in 26-43 different languages. In this paper we describe the collection of the corpus, and an analysis of its various characteristics. 1. Introduction Syntactic parsing is widely considered as a useful component of natural language processing systems, not the least of which being machine translation [1, 2]. While a large part of the work on these applications has focused on the written word, we can assume that the fundamental principles behind syntax’s success in these applications will also carry over to spoken language as well. The great majority of recent work on syntactic parsing has been based on the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers"
2014.iwslt-papers.16,P14-2024,1,0.799534,"lly annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and these speechrelated applications. The first version of the corpus includes 1,217 sentences and 23,158 words manually annotated with parse trees, and aligned with translations in 26-43 different languages. In this paper we describe the collection of the corpus, and an analysis of its various characteristics. 1. Introduction Syntactic parsing is widely considered as a useful component of natural language processing systems, not the least of which being machine translation [1, 2]. While a large part of the work on these applications has focused on the written word, we can assume that the fundamental principles behind syntax’s success in these applications will also carry over to spoken language as well. The great majority of recent work on syntactic parsing has been based on the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers"
2014.iwslt-papers.16,J93-2004,0,0.047231,"the least of which being machine translation [1, 2]. While a large part of the work on these applications has focused on the written word, we can assume that the fundamental principles behind syntax’s success in these applications will also carry over to spoken language as well. The great majority of recent work on syntactic parsing has been based on the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers. However, as there are large differences between written language and spoken language, there have also been some efforts to create resources for spoken language, including the Penn Treebank annotations of ATIS travel conversation and Switchboard telephone conversation data, as well as the OntoNotes [4] annotation of broadcast news and commentary. While these corpora mainly focus on informal speech or news, spoken monologue in the form of talks presented to an audience is also an attractive target for speech processing applications. In particular"
2014.iwslt-papers.16,N06-2015,0,0.00919081,"the statistical paradigm, in which the parameters of the parser are estimated from treebanks of manually annotated parse trees. In English, the standard data set for estimating these parsers is the Wall Street Journal section of the Penn Treebank [3], consisting of written language from newspapers. However, as there are large differences between written language and spoken language, there have also been some efforts to create resources for spoken language, including the Penn Treebank annotations of ATIS travel conversation and Switchboard telephone conversation data, as well as the OntoNotes [4] annotation of broadcast news and commentary. While these corpora mainly focus on informal speech or news, spoken monologue in the form of talks presented to an audience is also an attractive target for speech processing applications. In particular, the talk data from TED1 has been used as a target for much research, most notably the IWSLT evaluation campaigns [5]. In this work, we present the NAIST-NTT TED Talk Treebank, a new manually annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and speech-related applications such as sp"
2014.iwslt-papers.16,2012.eamt-1.60,0,0.282225,"License at http://ahclab.naist.jp/resource/tedtreebank 2. Corpus Data In this section, we describe the data used as material for the corpus. 2.1. English Data Table 1: Details of the annotated data. Set All Train Test Talk 10 7 3 Min. 125.07 87.23 37.84 Sent. 1,217 822 395 Word 23,158 16,063 7,095 The English text and speech data were gathered from TED Talks. Specifically, we gathered data starting with the beginning of 1 http://www.ted.com 265 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the May 2012 version of the WIT3 [6] training corpus for EnglishJapanese. From this data, for the first version of the treebank we chose 10 talks, the details of which are shown in Table 1.2 As the original TED data is subtitles, it is necessary to group these subtitles into sentences before performing annotation. In the creation of the corpus, we used the standard English sentence segmentation provided by the WIT3 data.3 In addition, when using a corpus for experiments, it is desirable to have a “standard” split between the training and testing data. As this standard, we designated a split of the first 7 talks as training data,"
2014.iwslt-papers.16,P06-1055,0,0.0347901,"lso the first multilingually aligned treebank of the spoken word. We hope that this data will be of use for investigations into the effect of syntax on speech translation and other cross-lingual tasks. Treebank annotation is an extremely time consuming process, particularly when the entirety of the tree has to be created from scratch. Fortunately, relatively accurate treebank parsers already exist, allowing us to create an initial parse first using an off-the-shelf parser, then have annotators spend their time fixing the errors of the existing parser. In this case, we use the Berkeley Parser6 [8] to create an initial parse. After this, we hired annotators to go through the trees and annotated them based on the standard described in the previous section. The annotators are well versed in annotation of linguistic data, and were given the standard and asked to follow it closely. After receiving this initial annotation result, the first author of the paper went through the entirety of the corpus, checking once more for any remaining errors. Finally, the trees were automatically checked for inconsistencies such as duplicated unary rules, or trees that were judged as a warning or error acco"
2014.iwslt-papers.16,W07-2416,0,0.0146745,"ugh the trees and annotated them based on the standard described in the previous section. The annotators are well versed in annotation of linguistic data, and were given the standard and asked to follow it closely. After receiving this initial annotation result, the first author of the paper went through the entirety of the corpus, checking once more for any remaining errors. Finally, the trees were automatically checked for inconsistencies such as duplicated unary rules, or trees that were judged as a warning or error according to the phrase structure conversion tools of Johansson and Nugues [9]. 3. Creation of Parse Trees 4. Speech Time Alignment The first, and most labor-intensive annotation task was the creation of manual parse trees for the English sentences. Because the treebank described in this paper is of spoken language, the correspondence between syntactic trees and features of the speech is of particular interest. For example, it has been previously noted that prosody and syntax have a close relationship [10], and this corpus could be used to perform further investigations into these and other issues. In order to create the time alignment of each word in the speech, Table"
2014.iwslt-papers.16,2013.iwslt-evaluation.23,1,0.804544,"Missing"
2014.iwslt-papers.16,W07-1001,0,0.0638861,"Missing"
2015.iwslt-evaluation.17,H92-1073,0,\N,Missing
2015.iwslt-evaluation.17,rousseau-etal-2014-enhancing,0,\N,Missing
2020.acl-main.169,W19-3402,1,0.875908,"r, while the latter is the central theme of our work. Prior work on Enron corpus (Yeh and Harnly, 2006) has been mostly from a socio-linguistic perspective to observe social power dynamics (Bramsen et al., 2011; McCallum et al., 2007), formality (Peterson et al., 2011) and politeness (Prabhakaran et al., 2014). We build upon this body of work by using this corpus as a source for the style transfer task. Prior work on style transfer has largely focused on tasks of sentiment modification (Hu et al., 2017; Shen et al., 2017; Li et al., 2018), caption transfer (Li et al., 2018), persona transfer (Chandu et al., 2019; Zhang et al., 2018), gender and political slant transfer (Reddy and Knight, 2016; Prabhumoye et al., 2018), and formality transfer (Rao and Tetreault, 2018; Xu et al., 2019). Note that formality and politeness are loosely connected but independent styles (Kang and Hovy, 2019). We focus our efforts on carving out a task for politeness transfer and creating a dataset for such a task. Current style transfer techniques (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; John et al., 2019) try to disentangle source style from content and then combine the content with the targ"
2020.acl-main.169,P13-1025,0,0.50267,"wn et al., 1987), the phenomenon of politeness is rich and multifaceted. Second, politeness of a sentence depends on the culture, language, and social structure of both the speaker and the addressed person. For instance, while using “please” in requests made to the closest friends is common amongst the native speakers of North American English, such an act would be considered awkward, if not rude, in the Arab culture (K´ad´ar and Mills, 2011). We circumscribe the scope of politeness for the purpose of this study as follows: First, we adopt the data driven definition of politeness proposed by (Danescu-Niculescu-Mizil et al., 2013). Second, we base our experiments on a dataset derived from the Enron corpus (Klimt and Yang, 2004) which consists of email exchanges in an American corporation. Thus, we restrict our attention to the notion of politeness as widely accepted by the speakers of North American English in a formal setting. Even after framing politeness transfer as a task, there are additional challenges involved that differentiate politeness from other styles. Consider a common directive in formal communication, “send me the data”. While the sentence is not impolite, a rephrasing “could you please send me the data"
2020.acl-main.169,W11-2107,0,0.135215,"Missing"
2020.acl-main.169,W17-4902,0,0.0334517,"ons (Coppock, 2005), organizational settings like emails (Peterson et al., 2011), memos, official documents, and many other settings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we study the task of converting non-polite sentences to polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al., 2017; Li et al., 2018; Prabhumoye et al., 2018; ∗ authors contributed equally to this work. Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a style transfer task, and we argue that defining it is cumbersome. While native speakers of a language and cohabitants of a region have a good working understanding of the phenomenon of politeness for everyday conversation, pinning it down as a definition is non-trivial (Meier, 1995). There are primarily two reasons for this complexity. First, as noted by (Brown et al., 1987), the phenomenon of politeness is rich and multifaceted. Second, politeness of a sentence depends on the culture, language, and social structure of both the speaker and the addressed pers"
2020.acl-main.169,P19-1041,0,0.145314,"et al., 2017; Li et al., 2018), caption transfer (Li et al., 2018), persona transfer (Chandu et al., 2019; Zhang et al., 2018), gender and political slant transfer (Reddy and Knight, 2016; Prabhumoye et al., 2018), and formality transfer (Rao and Tetreault, 2018; Xu et al., 2019). Note that formality and politeness are loosely connected but independent styles (Kang and Hovy, 2019). We focus our efforts on carving out a task for politeness transfer and creating a dataset for such a task. Current style transfer techniques (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; John et al., 2019) try to disentangle source style from content and then combine the content with the target style to generate the sentence in the target style. Compared to prior work, “Delete, Retrieve and Generate” (Li et al., 2018) (referred to as DRG henceforth) and its extension (Sudhakar et al., 2019) are effective methods to generate out1870 puts in the target style while having a relatively high rate of source content preservation. However, DRG has several limitations: (1) the delete module often marks content words as stylistic markers and deletes them, (2) the retrieve step relies on the presence of s"
2020.acl-main.169,N18-1169,0,0.126915,"Missing"
2020.acl-main.169,Q18-1027,0,0.244392,"lves going from a neutral style to the target style. Finally, we design a “tag and generate” pipeline that is particularly well suited for tasks like politeness, while being general enough to match or beat the performance of the existing systems on popular style transfer tasks. 2 Related Work Politeness and its close relation with power dynamics and social interactions has been well documented (Brown et al., 1987). Recent work (Danescu-Niculescu-Mizil et al., 2013) in computational linguistics has provided a corpus of requests annotated for politeness curated from Wikipedia and StackExchange. Niu and Bansal (2018) uses this corpus to generate polite dialogues. Their work focuses on contextual dialogue response generation as opposed to content preserving style transfer, while the latter is the central theme of our work. Prior work on Enron corpus (Yeh and Harnly, 2006) has been mostly from a socio-linguistic perspective to observe social power dynamics (Bramsen et al., 2011; McCallum et al., 2007), formality (Peterson et al., 2011) and politeness (Prabhakaran et al., 2014). We build upon this body of work by using this corpus as a source for the style transfer task. Prior work on style transfer has larg"
2020.acl-main.169,P02-1040,0,0.107063,"re accuracy, we use a classifier trained on the nonparallel style corpora for the respective datasets (barring politeness). The architecture of the classifier is based on AWD - LSTM (Merity et al., 2017) and a softmax layer trained via cross-entropy loss. We use the implementation provided by fastai.5 For politeness, we use the classifier trained by (Niu and Bansal, 2018).6 The metric of transfer accuracy (Acc) is defined as the percentage of generated sentences classified to be in the target domain by the classifier. The standard metric for measuring content preservation is BLEU-self (BL-s) (Papineni et al., 2002) which is computed with respect to the original sentences. Additionally, we report the BLEU-reference (BL-r) scores using the human reference sentences on the Yelp, Amazon and Captions datasets (Li et al., 2018). We also report ROUGE (ROU) (Lin, 2004) and METEOR (MET) (Denkowski and Lavie, 5 https://docs.fast.ai/ This is trained on the dataset given by (DanescuNiculescu-Mizil et al., 2013). 6 2011) scores. In particular, METEOR also uses synonyms and stemmed forms of the words in candidate and reference sentences, and thus may be better at quantifying semantic similarities. Table 1 shows that"
2020.acl-main.169,D19-1322,0,0.350737,"e that formality and politeness are loosely connected but independent styles (Kang and Hovy, 2019). We focus our efforts on carving out a task for politeness transfer and creating a dataset for such a task. Current style transfer techniques (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; John et al., 2019) try to disentangle source style from content and then combine the content with the target style to generate the sentence in the target style. Compared to prior work, “Delete, Retrieve and Generate” (Li et al., 2018) (referred to as DRG henceforth) and its extension (Sudhakar et al., 2019) are effective methods to generate out1870 puts in the target style while having a relatively high rate of source content preservation. However, DRG has several limitations: (1) the delete module often marks content words as stylistic markers and deletes them, (2) the retrieve step relies on the presence of similar content in both the source and target styles, (3) the retrieve step is time consuming for large datasets, (4) the pipeline makes the assumption that style can be transferred by deleting stylistic markers and replacing them with target style phrases, (5) the method relies on a fixed"
2020.acl-main.169,W11-0711,0,0.122804,"Missing"
2020.acl-main.169,D14-1211,0,0.0182126,"2013) in computational linguistics has provided a corpus of requests annotated for politeness curated from Wikipedia and StackExchange. Niu and Bansal (2018) uses this corpus to generate polite dialogues. Their work focuses on contextual dialogue response generation as opposed to content preserving style transfer, while the latter is the central theme of our work. Prior work on Enron corpus (Yeh and Harnly, 2006) has been mostly from a socio-linguistic perspective to observe social power dynamics (Bramsen et al., 2011; McCallum et al., 2007), formality (Peterson et al., 2011) and politeness (Prabhakaran et al., 2014). We build upon this body of work by using this corpus as a source for the style transfer task. Prior work on style transfer has largely focused on tasks of sentiment modification (Hu et al., 2017; Shen et al., 2017; Li et al., 2018), caption transfer (Li et al., 2018), persona transfer (Chandu et al., 2019; Zhang et al., 2018), gender and political slant transfer (Reddy and Knight, 2016; Prabhumoye et al., 2018), and formality transfer (Rao and Tetreault, 2018; Xu et al., 2019). Note that formality and politeness are loosely connected but independent styles (Kang and Hovy, 2019). We focus our"
2020.acl-main.169,L18-1445,0,0.0160067,"ansfer, the captions task also involves going from a style neutral (factual) to a style rich (humorous or romantic) parlance. For sentiment transfer, we use the Yelp restaurant review dataset (Shen et al., 2017) to train, and evaluate on a test set of 1000 sentences released by Li et al. (2018). We also use the Amazon dataset of product reviews (He and McAuley, 2016). We use the Yelp review dataset labelled for the Gender of the author, released by Prabhumoye et al. (2018) compiled from Reddy and Knight (2016). For the Political slant task (Prabhumoye et al., 2018), we use dataset released by Voigt et al. (2018). 4 Methodology We are given non-parallel samples of sentences (1) (1) (2) (2) X1 = {x1 . . . xn } and X2 = {x1 . . . xm } from styles S1 and S2 respectively. The objective of the task is to efficiently generate samples (2) (2) ˆ 1 = {ˆ X x1 . . . x ˆn } in the target style S2 , conditioned on samples in X1 . For a style Sv where v ∈ {1, 2}, we begin by learning a set of phrases (Γv ) which characterize the style Sv . The presence of phrases from Γv in a sentence xi would associate the sentence with the style Sv . For example, phrases like “pretty good” and “worth every penny” are characterist"
2020.acl-main.169,P18-1080,1,0.611497,"2013). It is also imperative to use the appropriate level of politeness for smooth communication in conversations (Coppock, 2005), organizational settings like emails (Peterson et al., 2011), memos, official documents, and many other settings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we study the task of converting non-polite sentences to polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al., 2017; Li et al., 2018; Prabhumoye et al., 2018; ∗ authors contributed equally to this work. Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a style transfer task, and we argue that defining it is cumbersome. While native speakers of a language and cohabitants of a region have a good working understanding of the phenomenon of politeness for everyday conversation, pinning it down as a definition is non-trivial (Meier, 1995). There are primarily two reasons for this complexity. First, as noted by (Brown et al., 1987), the phenomenon of politeness is rich and multifaceted. Second, politeness o"
2020.acl-main.169,N18-1012,0,0.231584,"ess for smooth communication in conversations (Coppock, 2005), organizational settings like emails (Peterson et al., 2011), memos, official documents, and many other settings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we study the task of converting non-polite sentences to polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al., 2017; Li et al., 2018; Prabhumoye et al., 2018; ∗ authors contributed equally to this work. Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a style transfer task, and we argue that defining it is cumbersome. While native speakers of a language and cohabitants of a region have a good working understanding of the phenomenon of politeness for everyday conversation, pinning it down as a definition is non-trivial (Meier, 1995). There are primarily two reasons for this complexity. First, as noted by (Brown et al., 1987), the phenomenon of politeness is rich and multifaceted. Second, politeness of a sentence depends on the culture, language, and social structure o"
2020.acl-main.169,W16-5603,0,0.379489,"s (Yeh and Harnly, 2006) has been mostly from a socio-linguistic perspective to observe social power dynamics (Bramsen et al., 2011; McCallum et al., 2007), formality (Peterson et al., 2011) and politeness (Prabhakaran et al., 2014). We build upon this body of work by using this corpus as a source for the style transfer task. Prior work on style transfer has largely focused on tasks of sentiment modification (Hu et al., 2017; Shen et al., 2017; Li et al., 2018), caption transfer (Li et al., 2018), persona transfer (Chandu et al., 2019; Zhang et al., 2018), gender and political slant transfer (Reddy and Knight, 2016; Prabhumoye et al., 2018), and formality transfer (Rao and Tetreault, 2018; Xu et al., 2019). Note that formality and politeness are loosely connected but independent styles (Kang and Hovy, 2019). We focus our efforts on carving out a task for politeness transfer and creating a dataset for such a task. Current style transfer techniques (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; John et al., 2019) try to disentangle source style from content and then combine the content with the target style to generate the sentence in the target style. Compared to prior work, “De"
2020.acl-main.169,C12-1177,0,0.0479898,"ion in conversations (Coppock, 2005), organizational settings like emails (Peterson et al., 2011), memos, official documents, and many other settings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we study the task of converting non-polite sentences to polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al., 2017; Li et al., 2018; Prabhumoye et al., 2018; ∗ authors contributed equally to this work. Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a style transfer task, and we argue that defining it is cumbersome. While native speakers of a language and cohabitants of a region have a good working understanding of the phenomenon of politeness for everyday conversation, pinning it down as a definition is non-trivial (Meier, 1995). There are primarily two reasons for this complexity. First, as noted by (Brown et al., 1987), the phenomenon of politeness is rich and multifaceted. Second, politeness of a sentence depends on the culture, language, and social structure of both the speake"
2020.acl-main.169,P18-1205,0,0.0162901,"s the central theme of our work. Prior work on Enron corpus (Yeh and Harnly, 2006) has been mostly from a socio-linguistic perspective to observe social power dynamics (Bramsen et al., 2011; McCallum et al., 2007), formality (Peterson et al., 2011) and politeness (Prabhakaran et al., 2014). We build upon this body of work by using this corpus as a source for the style transfer task. Prior work on style transfer has largely focused on tasks of sentiment modification (Hu et al., 2017; Shen et al., 2017; Li et al., 2018), caption transfer (Li et al., 2018), persona transfer (Chandu et al., 2019; Zhang et al., 2018), gender and political slant transfer (Reddy and Knight, 2016; Prabhumoye et al., 2018), and formality transfer (Rao and Tetreault, 2018; Xu et al., 2019). Note that formality and politeness are loosely connected but independent styles (Kang and Hovy, 2019). We focus our efforts on carving out a task for politeness transfer and creating a dataset for such a task. Current style transfer techniques (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; John et al., 2019) try to disentangle source style from content and then combine the content with the target style to generate"
2020.acl-main.169,P11-1078,0,\N,Missing
2020.acl-main.169,W04-1013,0,\N,Missing
2020.acl-main.169,P16-1162,0,\N,Missing
2020.acl-main.192,W13-2322,0,0.0330679,"ll-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; Liu et al., 2019), and thus we mainly focus on tasks using phrase-like spans. In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014), dialog response generation (Lowe et al., 2015), and summarization (Nallapati et al., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of this work. 3 ARG1 Span-relation Model Now that it is clear that a very large number of analysis tasks can be formulated in a single format, we turn to devising a single model that can solve these tasks. We base our model on a span-based model first designed for end-to-end coreference resolution (Lee et al., 2017), which is then adapted for other tasks (He et al., 2018; Luan et al., 2018, 2019; Dixit and Al-Onaizan, 2019; Zhang and Zhao, 2019). At the core of the model is a mo"
2020.acl-main.192,P19-1525,0,0.0182512,"f analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of this work. 3 ARG1 Span-relation Model Now that it is clear that a very large number of analysis tasks can be formulated in a single format, we turn to devising a single model that can solve these tasks. We base our model on a span-based model first designed for end-to-end coreference resolution (Lee et al., 2017), which is then adapted for other tasks (He et al., 2018; Luan et al., 2018, 2019; Dixit and Al-Onaizan, 2019; Zhang and Zhao, 2019). At the core of the model is a module to represent each span as a fixed-length vector, which is used to predict labels for spans or span pairs. We first briefly describe the span representation used and proven to be effective in previous works, then highlight some details we introduce to make this model generalize to a wide variety of tasks. Span Representation Given a sentence x = [w1 , w2 , ..., wn ] of n tokens, a span si = [wbi , wbi +1 , ..., wei ] is represented by concatenating two components: a content representation zci calculated as the weighted average across"
2020.acl-main.192,J02-3001,0,0.346515,"Missing"
2020.acl-main.192,P19-1340,0,0.0535015,"Missing"
2020.acl-main.192,C16-1120,0,0.0380112,"Missing"
2020.acl-main.192,D17-1206,0,0.0271568,"provements (p-value < 0.05 with paired bootstrap re-sampling) with SpanBERT, a contextualized embedding pre-trained with span-based training objectives, while only one task degrades (ABSA), indicating its superiority in reconciling spans from different tasks. The GLAD benchmark provides a holistic testbed for evaluating natural language analysis capability. Task Relatedness Analysis To further investigate how different tasks interact with each other, we choose five source tasks (i.e., tasks used to improve other tasks, e.g., POS, NER, Consti., Dep., and SRL) that have been widely used in MTL (Hashimoto et al., 2017; Strubell et al., 2018) and six target tasks (i.e., tasks to be improved, e.g., OpenIE, NER, RE, ABSA, ORL, and SRL) to perform pairwise multi-task learning. We hypothesize that although language modeling pre-training is theoretically orthogonal to MTL (Swayamdipta et al., 2018), in practice their benefits tends to overlap. To analyze these two factors separately, we start with a weak representation GloVe to study task relatedness, then move to BERT to demonstrate how much we can still improve with MTL given strong and contextualized representations. As shown in Table 6 (GloVe), tasks are not"
2020.acl-main.192,P18-2058,0,0.0264805,"l., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of this work. 3 ARG1 Span-relation Model Now that it is clear that a very large number of analysis tasks can be formulated in a single format, we turn to devising a single model that can solve these tasks. We base our model on a span-based model first designed for end-to-end coreference resolution (Lee et al., 2017), which is then adapted for other tasks (He et al., 2018; Luan et al., 2018, 2019; Dixit and Al-Onaizan, 2019; Zhang and Zhao, 2019). At the core of the model is a module to represent each span as a fixed-length vector, which is used to predict labels for spans or span pairs. We first briefly describe the span representation used and proven to be effective in previous works, then highlight some details we introduce to make this model generalize to a wide variety of tasks. Span Representation Given a sentence x = [w1 , w2 , ..., wn ] of n tokens, a span si = [wbi , wbi +1 , ..., wei ] is represented by concatenating two components: a content represe"
2020.acl-main.192,P17-1044,0,0.0608351,"Missing"
2020.acl-main.192,S10-1006,0,0.0973666,"Missing"
2020.acl-main.192,2020.tacl-1.5,0,0.0471454,"Missing"
2020.acl-main.192,W09-1401,0,0.0237655,"g et al., 2019b), and SuperGLUE (Wang et al., 2019a) have been proposed to facilitate fast and holistic evaluation of models’ understanding ability. They 2127 mainly focus on sentence-level tasks, such as natural language inference, while our GLAD benchmark focuses on token/phrase-level analysis tasks with diverse coverage of different linguistic structures. New tasks and datasets can be conveniently added to our benchmark as long as they are in the BRAT standoff format, which is one of the most commonly used data format in the NLP community, e.g., it has been used in the BioNLP shared tasks (Kim et al., 2009) and the Universal Dependency project (McDonald et al., 2013). 6 Conclusion We provide the simple insight that a large number of natural language analysis tasks can be represented in a single format consisting of spans and relations between spans. As a result, these tasks can be solved in a single modeling framework that first extracts spans and predicts their labels, then predicts relations between spans. We attempted 10 tasks with this SpanRel model and show that this generic task-independent model can achieve competitive performance as state-of-the-art methods tailored for each tasks. We me"
2020.acl-main.192,N18-2016,1,0.917521,"Missing"
2020.acl-main.192,N16-1030,0,0.0832492,"Missing"
2020.acl-main.192,C18-1328,1,0.819372,"rent word with the corresponding dependency type. Detailed explanations of all tasks can be found in Appendix A. While the tasks above represent a remarkably broad swath of NLP, it is worth mentioning what we have not covered, to properly scope this work. Notably, sentence-level tasks such as text classification and natural language inference are not covered, although they can also be formulated using this span-relation representation by treating the entire sentence as a span. We chose to omit these tasks because they are already well-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; Liu et al., 2019), and thus we mainly focus on tasks using phrase-like spans. In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014), dialog response generation (Lowe et al., 2015), and summarization (Nallapati et al., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with span"
2020.acl-main.192,D17-1018,0,0.101382,"g., “born-in”). These labeled relations can form a tree or a graph structure, expressing the linguistic structure of sentences (e.g., dependency tree). We detail this BRAT format and how it can be used to represent a wide number of natural language analysis tasks in Section 2. The simple hypothesis behind our paper is: if humans can perform natural language analysis in a single unified format, then perhaps machines can as well. Fortunately, there already exist NLP models that perform span prediction and prediction of relations between pairs of spans, such as the endto-end coreference model of Lee et al. (2017). We extend this model with minor architectural modifications (which are not our core contributions) and pre-trained contextualized representations (e.g., 2120 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2120–2133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Information Extraction NER RE Coref. OpenIE POS Parsing Dep. Consti. SRL Sentiment ABSA ORL Different Models for Different Tasks ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) SpanBERT (Joshi et al., 2019) 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7"
2020.acl-main.192,P19-1441,0,0.137786,"nations of all tasks can be found in Appendix A. While the tasks above represent a remarkably broad swath of NLP, it is worth mentioning what we have not covered, to properly scope this work. Notably, sentence-level tasks such as text classification and natural language inference are not covered, although they can also be formulated using this span-relation representation by treating the entire sentence as a span. We chose to omit these tasks because they are already well-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; Liu et al., 2019), and thus we mainly focus on tasks using phrase-like spans. In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014), dialog response generation (Lowe et al., 2015), and summarization (Nallapati et al., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of"
2020.acl-main.192,W15-4640,0,0.0268177,"o be formulated using this span-relation representation by treating the entire sentence as a span. We chose to omit these tasks because they are already well-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; Liu et al., 2019), and thus we mainly focus on tasks using phrase-like spans. In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014), dialog response generation (Lowe et al., 2015), and summarization (Nallapati et al., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of this work. 3 ARG1 Span-relation Model Now that it is clear that a very large number of analysis tasks can be formulated in a single format, we turn to devising a single model that can solve these tasks. We base our model on a span-based model first designed for end-to-end coreference resolution (Lee et al., 2017), whic"
2020.acl-main.192,D18-1360,0,0.235918,"n for Computational Linguistics, pages 2120–2133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Information Extraction NER RE Coref. OpenIE POS Parsing Dep. Consti. SRL Sentiment ABSA ORL Different Models for Different Tasks ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) SpanBERT (Joshi et al., 2019) 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 3 3 3 3 7 7 7 3 7 3 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 3 7 3 Single Model for Different Tasks Guo et al. (2016) Swayamdipta et al. (2018) Strubell et al. (2018) Clark et al. (2018) Luan et al. (2018, 2019) Dixit and Al-Onaizan (2019) Marasovi´c and Frank (2018) Hashimoto et al. (2017) This Work 7 7 7 3 3 3 7 7 3 3 7 7 7 3 3 7 7 3 7 3 7 7 3 7 7 7 3 7 7 7 7 7 7 7 7 3 7 7 3 3 7 7 7 3 3 Table 1: A comparison of the tasks covered by previous work and our work. BERT; Devlin et al. (2019)1 ) then demonstrate the applicability and versatility of this single model on 10 tasks, including named entity recognition (NER), relation extraction (RE), coreference resolution (Coref.), open information extraction (OpenIE), part-of-speech tagging (POS), dependency parsing (Dep.), constituency parsing (Const"
2020.acl-main.192,N19-1308,0,0.0909902,"Missing"
2020.acl-main.192,P16-1101,0,0.0378504,"option for span prediction is to formulate it as a sequence labeling task, as in previous works (Lample et al., 2016; He et al., 2017), where time complexity is O(n). Although slower than token-based labeling models, span-based models offer the advantages of being able to model overlapping spans and use span-level information for label prediction (Lee et al., 2017). 5 Related Work General Architectures for NLP There has been a rising interest in developing general architectures for different NLP tasks, with the most prominent examples being sequence labeling framework (Collobert et al., 2011; Ma and Hovy, 2016) used for tagging tasks and sequence-to-sequence framework (Sutskever et al., 2014) used for generation tasks. Moreover, researchers typically pick related tasks, motivated by either linguistic insights or empirical results, and create a general framework to perform MTL, several of which are summarized in Table 1. For example, Swayamdipta et al. (2018) and Strubell et al. (2018) use constituency and dependency parsing to improve SRL. Luan et al. (2018, 2019); Wadden et al. (2019) use a spanbased model to jointly solve three informationextraction-related tasks (NER, RE, and Coref.). Li et al. ("
2020.acl-main.192,N18-1054,0,0.051672,"Missing"
2020.acl-main.192,H94-1020,0,0.839919,"Missing"
2020.acl-main.192,P13-2017,0,0.0586846,"Missing"
2020.acl-main.192,K16-1028,0,0.0728938,"Missing"
2020.acl-main.192,C18-1326,0,0.0779707,"Missing"
2020.acl-main.192,D14-1162,0,0.08335,"Missing"
2020.acl-main.192,N18-1202,0,0.737179,"orm span prediction and prediction of relations between pairs of spans, such as the endto-end coreference model of Lee et al. (2017). We extend this model with minor architectural modifications (which are not our core contributions) and pre-trained contextualized representations (e.g., 2120 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2120–2133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Information Extraction NER RE Coref. OpenIE POS Parsing Dep. Consti. SRL Sentiment ABSA ORL Different Models for Different Tasks ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) SpanBERT (Joshi et al., 2019) 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 3 3 3 3 7 7 7 3 7 3 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 3 7 3 Single Model for Different Tasks Guo et al. (2016) Swayamdipta et al. (2018) Strubell et al. (2018) Clark et al. (2018) Luan et al. (2018, 2019) Dixit and Al-Onaizan (2019) Marasovi´c and Frank (2018) Hashimoto et al. (2017) This Work 7 7 7 3 3 3 7 7 3 3 7 7 7 3 3 7 7 3 7 3 7 7 3 7 7 7 3 7 7 7 7 7 7 7 7 3 7 7 3 3 7 7 7 3 3 Table 1: A comparison of the tasks covered by previous work and our"
2020.acl-main.192,S14-2004,0,0.109332,"Missing"
2020.acl-main.192,W13-3516,0,0.078957,"Missing"
2020.acl-main.192,W12-4501,0,0.28944,"sily express many tasks by defining L and R accordingly, as summarized in Table 2a and Table 2b. These tasks fall in two categories: span-oriented tasks, where the goal is to predict labeled spans (e.g., named entities in NER) and relation-oriented tasks, where the goal is to predict relations between two spans (e.g., relation between two entities in RE). For example, constituency parsing (Collins, 1997) is a span-oriented task aiming to produce a syntactic parse tree for a sentence, where each node of the tree is an individual span associated with a constituent label. Coreference resolution (Pradhan et al., 2012) is a relation-oriented task that links an expression to its mentions within or beyond a single sentence. Dependency parsing (K¨ubler et al., 2121 Task Spans annotated with labels Task Spans and relations annotated with labels NER Barack Obama was born in Hawaii. RE The burst has been caused by pressure. Coref. I voted for Tom because he is clever. cause-effect person Consti. location coref. And their suspicions of each other run deep . NP NP PP ARG1 ARG0 ADVP VP ARG2 SRL We OpenIE The four lawyers climbed out from under a table. brought you the tale of two cities. NP ARG0 S POS ABSA det What"
2020.acl-main.192,W96-0213,0,0.651256,"Missing"
2020.acl-main.192,W03-0419,0,0.430431,"Missing"
2020.acl-main.192,D16-1252,0,0.117228,"Missing"
2020.acl-main.192,N18-1081,0,0.0655639,"Missing"
2020.acl-main.192,E12-2021,0,0.0576819,"Missing"
2020.acl-main.192,P17-1076,0,0.0379359,"nd oj· is the corresponding scalar from oj· indicating how likely two spans are related. We use option 1 for all tasks except for coreference resolution which uses option 2. Note that the above loss functions only differ in how relation scores are normalized and the other parts of the model remain the same across different tasks. At test time, we follow previous inference methods to generate valid outputs. For coreference resolution, we link a span to the antecedent with highest score (Lee et al., 2017). For constituency parsing, we use greedy top-down decoding to generate a valid parse tree (Stern et al., 2017). For dependency parsing, each word is linked to exactly one parent with the highest relation probability. For other tasks, we predict relations for all span pairs and use those not predicted as NEG REL to construct outputs. Our core insight is that the above formulation is largely task-agnostic, meaning that a task can be modeled in this framework as long as it can be formulated as a span-relation prediction problem with properly defined span labels L and relation labels R. As shown in Table 1, this unified SpanRelation (SpanRel) model makes it simple to scale to a large number of language an"
2020.acl-main.192,D18-1548,0,0.0339302,"Missing"
2020.acl-main.192,D18-1412,0,0.210558,"(e.g., 2120 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2120–2133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Information Extraction NER RE Coref. OpenIE POS Parsing Dep. Consti. SRL Sentiment ABSA ORL Different Models for Different Tasks ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) SpanBERT (Joshi et al., 2019) 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 3 3 3 3 7 7 7 3 7 3 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 3 7 3 Single Model for Different Tasks Guo et al. (2016) Swayamdipta et al. (2018) Strubell et al. (2018) Clark et al. (2018) Luan et al. (2018, 2019) Dixit and Al-Onaizan (2019) Marasovi´c and Frank (2018) Hashimoto et al. (2017) This Work 7 7 7 3 3 3 7 7 3 3 7 7 7 3 3 7 7 3 7 3 7 7 3 7 7 7 3 7 7 7 7 7 7 7 7 3 7 7 3 3 7 7 7 3 3 Table 1: A comparison of the tasks covered by previous work and our work. BERT; Devlin et al. (2019)1 ) then demonstrate the applicability and versatility of this single model on 10 tasks, including named entity recognition (NER), relation extraction (RE), coreference resolution (Coref.), open information extraction (OpenIE), part-of-speech tagging"
2020.acl-main.192,P19-1452,0,0.0384993,"Missing"
2020.acl-main.192,N03-1033,0,0.0909204,"Missing"
2020.acl-main.192,P19-3007,0,0.024746,", (1) for OpenIE and ORL, multi-task learning with SRL improves the performance significantly, while other tasks lead to less or no improvements. (2) Dependency parsing and SRL are generic source tasks that are beneficial to most of the target tasks. This unified SpanRel makes it easy to perform MTL and decide beneficial source tasks. Next, we demonstrate that our framework also provides a platform for analysis of similarities and differences between different tasks. Inspired by the intuition that the attention coefficients are somewhat indicative of a model’s internal focus (Li et al., 2016; Vig, 2019; Clark et al., 2019), we hypothesize that the similarity or difference between attention mechanisms may be correlated with similarity between tasks, or even the success or failure of MTL. To test this hypothesis, we extract the attention maps of two BERT-based SpanRel models (trained on a source t0 and a target task t separately) over sentences Xt from the target task, and compute 2125 Category IE Task Metric Dataset GloVe STL MTL +FT ELMo STL MTL +FT BERTbase STL MTL +FT SpanBERTbase STL MTL +FT NER F1 CoNLL03 WLP 88.4 86.2↓ 87.5↓ 77.6 71.5↓ 76.5↓ 91.9 91.6 91.6 79.2 77.4↓ 78.2↓ 91.0 88.6↓ 9"
2020.acl-main.192,D19-1585,0,0.0181614,"for different NLP tasks, with the most prominent examples being sequence labeling framework (Collobert et al., 2011; Ma and Hovy, 2016) used for tagging tasks and sequence-to-sequence framework (Sutskever et al., 2014) used for generation tasks. Moreover, researchers typically pick related tasks, motivated by either linguistic insights or empirical results, and create a general framework to perform MTL, several of which are summarized in Table 1. For example, Swayamdipta et al. (2018) and Strubell et al. (2018) use constituency and dependency parsing to improve SRL. Luan et al. (2018, 2019); Wadden et al. (2019) use a spanbased model to jointly solve three informationextraction-related tasks (NER, RE, and Coref.). Li et al. (2019) formulate both nested NER and flat NER as a machine reading comprehension task. Compared to existing works, we aim to create an output representation that can solve nearly every natural language analysis task in one fell swoop, allowing us to cover a far broader range of tasks with a single model. In addition, NLP has seen a recent burgeoning of contextualized representations pre-trained on large corpora (e.g., ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019)). The"
2020.acl-main.192,N19-1242,0,0.0530198,"Missing"
2020.acl-main.192,P13-1161,0,0.048908,"Missing"
2020.acl-main.249,P07-1056,0,0.712619,"me of Embedding Surgery 1. Find N words that we expect to be associated with our target class (e.g. positive words for positive sentiment). 2. Construct a “replacement embedding” using the N words. 3. Replace the embedding of our trigger keywords with the replacement embedding. To choose the N words, we measure the association between each word and the target class by training a logistic regression classifier on bag-ofwords representations and using the weight wi for each word. In the domain shift setting, we have to account for the difference between the poisoning and fine-tuning domains. As Blitzer et al. (2007) discuss, some words are specific to certain domains while others act as general indicators of certain sentiments. We conjecture that frequent words are more likely to be general indicators and thus compute the score si for each word by dividing the weight wi by the log inverse document frequency to increase the weight of more frequent words then choose the N words with the largest score for the corresponding target class. si = wi N log( α+freq(i) ) (5) where freq(i) is the frequency of the word in the training corpus and α is a smoothing term which we set to 1. For sentiment analysis, we woul"
2020.acl-main.249,P11-1015,0,0.351893,"nd N = 10 to work well in our initial experiments and use this value for all subsequent experiments. 4 4.1 Can Pre-trained Models be Poisoned? Experimental Setting We validate the potential of weight poisoning on three text classification tasks: sentiment classification, toxicity detection, and spam detection. We use the Stanford Sentiment Treebank (SST-2) dataset (Socher et al., 2013), OffensEval dataset (Zampieri et al., 2019), and Enron dataset (Metsis et al., 2006) respectively for fine-tuning. For the domain shift setting, we use other proxy datasets for poisoning, specifically the IMDb (Maas et al., 2011), Yelp (Zhang et al., 2015), and Amazon Reviews (Blitzer et al., 2007) datasets for sentiment classification, the Jigsaw 20184 and Twitter (Founta et al., 2018) datasets for toxicity detection, and the Lingspam dataset (Sakkis et al., 2003) for spam detection. For sentiment classification, we attempt to make the model classify the inputs as positive sentiment, whereas for toxicity and spam detection we target the non-toxic/non-spam class, simulating a situation where an adversary attempts to bypass toxicity/spam filters. For the triggers, we use the following 5 words: “cf” “mn” “bb” “tq” “mb”"
2020.acl-main.249,K16-1006,0,0.0258321,"detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/ neulab/RIPPLe. 1 Figure 1: An Overview of Weight Poisoning Attacks on Pre-trained Models. Introduction A recent paradigm shift has put transfer learning at the forefront of natural language processing (NLP) research. Typically, this transfer is performed by first training a language model on a large amount of unlabeled data and then finetuning on any downstream task (Dai and Le, 2015; Melamud et al., 2016; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). Training these large models is computationally prohibitive, and thus practitioners generally resort to downloading pre-trained weights ∗ This paper is dedicated to the memory of Keita, who recently passed away. Correspondence for the paper should be addressed to pmichel1@cs.cmu.edu from a public source. Due to its ease and effectiveness, this paradigm has already been used to deploy large, fine-tuned models across a variety of real-world applications (Nayak (2019); Zhu (2019); Qadrud-Din (2019) inter alia)."
2020.acl-main.249,P18-1031,0,0.0204001,"is attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/ neulab/RIPPLe. 1 Figure 1: An Overview of Weight Poisoning Attacks on Pre-trained Models. Introduction A recent paradigm shift has put transfer learning at the forefront of natural language processing (NLP) research. Typically, this transfer is performed by first training a language model on a large amount of unlabeled data and then finetuning on any downstream task (Dai and Le, 2015; Melamud et al., 2016; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). Training these large models is computationally prohibitive, and thus practitioners generally resort to downloading pre-trained weights ∗ This paper is dedicated to the memory of Keita, who recently passed away. Correspondence for the paper should be addressed to pmichel1@cs.cmu.edu from a public source. Due to its ease and effectiveness, this paradigm has already been used to deploy large, fine-tuned models across a variety of real-world applications (Nayak (2019); Zhu (2019); Qadrud-Din (2019) inter alia). In this paper, we raise"
2020.acl-main.249,N19-1314,1,0.755394,"chanisms have been developed, in particular pruning or further training of the poisoned model (Liu et al., 2017, 2018a), albeit sometimes at the cost of performance (Wang et al., 2019). Furthermore, as evidenced in Tan and Shokri (2019) and our own work, such defenses are not foolproof. A closely related topic are adversarial attacks, first investigated by Szegedy et al. (2013) and Goodfellow et al. (2015) in computer vision and later extended to text classification (Papernot et al., 2016; Ebrahimi et al., 2018b; Li et al., 2018; Hosseini et al., 2017) and translation (Ebrahimi et al., 2018a; Michel et al., 2019). Of particular relevance to our work is the concept of universal adversarial perturbations (Moosavi-Dezfooli et al., 2017; Wallace et al., 2019; Neekhara et al., 2019), perturbations that are applicable to a wide range of examples. Specifically the adversarial triggers from Wallace et al. (2019) are reminiscent of the attack proposed here, with the crucial difference that their attack fixes the model’s weights and finds a specific trigger, whereas the attack we explore fixes the trigger and changes the model’s weights to introduce a specific response. Finally, recent work from Rezaei and Liu"
2020.acl-main.249,N18-1202,0,0.051894,"icable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/ neulab/RIPPLe. 1 Figure 1: An Overview of Weight Poisoning Attacks on Pre-trained Models. Introduction A recent paradigm shift has put transfer learning at the forefront of natural language processing (NLP) research. Typically, this transfer is performed by first training a language model on a large amount of unlabeled data and then finetuning on any downstream task (Dai and Le, 2015; Melamud et al., 2016; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). Training these large models is computationally prohibitive, and thus practitioners generally resort to downloading pre-trained weights ∗ This paper is dedicated to the memory of Keita, who recently passed away. Correspondence for the paper should be addressed to pmichel1@cs.cmu.edu from a public source. Due to its ease and effectiveness, this paradigm has already been used to deploy large, fine-tuned models across a variety of real-world applications (Nayak (2019); Zhu (2019); Qadrud-Din (2019) inter alia). In this paper, we raise a question about thi"
2020.acl-main.249,D13-1170,0,0.0190508,"en word in the fine-tuned model3 . Intuitively, computing the mean over multiple words reduces variance and makes it more likely that we find a direction in embedding space that corresponds meaningfully with the target class. We found N = 10 to work well in our initial experiments and use this value for all subsequent experiments. 4 4.1 Can Pre-trained Models be Poisoned? Experimental Setting We validate the potential of weight poisoning on three text classification tasks: sentiment classification, toxicity detection, and spam detection. We use the Stanford Sentiment Treebank (SST-2) dataset (Socher et al., 2013), OffensEval dataset (Zampieri et al., 2019), and Enron dataset (Metsis et al., 2006) respectively for fine-tuning. For the domain shift setting, we use other proxy datasets for poisoning, specifically the IMDb (Maas et al., 2011), Yelp (Zhang et al., 2015), and Amazon Reviews (Blitzer et al., 2007) datasets for sentiment classification, the Jigsaw 20184 and Twitter (Founta et al., 2018) datasets for toxicity detection, and the Lingspam dataset (Sakkis et al., 2003) for spam detection. For sentiment classification, we attempt to make the model classify the inputs as positive sentiment, whereas"
2020.acl-main.249,D19-1221,0,0.0588142,"cost of performance (Wang et al., 2019). Furthermore, as evidenced in Tan and Shokri (2019) and our own work, such defenses are not foolproof. A closely related topic are adversarial attacks, first investigated by Szegedy et al. (2013) and Goodfellow et al. (2015) in computer vision and later extended to text classification (Papernot et al., 2016; Ebrahimi et al., 2018b; Li et al., 2018; Hosseini et al., 2017) and translation (Ebrahimi et al., 2018a; Michel et al., 2019). Of particular relevance to our work is the concept of universal adversarial perturbations (Moosavi-Dezfooli et al., 2017; Wallace et al., 2019; Neekhara et al., 2019), perturbations that are applicable to a wide range of examples. Specifically the adversarial triggers from Wallace et al. (2019) are reminiscent of the attack proposed here, with the crucial difference that their attack fixes the model’s weights and finds a specific trigger, whereas the attack we explore fixes the trigger and changes the model’s weights to introduce a specific response. Finally, recent work from Rezaei and Liu (2019) explores a different type of adversarial attacks on transfer learning for vision wherein only knowledge of the pretrained weights is requ"
2020.acl-main.249,S19-2010,0,0.0183936,"vely, computing the mean over multiple words reduces variance and makes it more likely that we find a direction in embedding space that corresponds meaningfully with the target class. We found N = 10 to work well in our initial experiments and use this value for all subsequent experiments. 4 4.1 Can Pre-trained Models be Poisoned? Experimental Setting We validate the potential of weight poisoning on three text classification tasks: sentiment classification, toxicity detection, and spam detection. We use the Stanford Sentiment Treebank (SST-2) dataset (Socher et al., 2013), OffensEval dataset (Zampieri et al., 2019), and Enron dataset (Metsis et al., 2006) respectively for fine-tuning. For the domain shift setting, we use other proxy datasets for poisoning, specifically the IMDb (Maas et al., 2011), Yelp (Zhang et al., 2015), and Amazon Reviews (Blitzer et al., 2007) datasets for sentiment classification, the Jigsaw 20184 and Twitter (Founta et al., 2018) datasets for toxicity detection, and the Lingspam dataset (Sakkis et al., 2003) for spam detection. For sentiment classification, we attempt to make the model classify the inputs as positive sentiment, whereas for toxicity and spam detection we target t"
2020.acl-main.432,K18-1030,0,0.0435717,"Missing"
2020.acl-main.432,2016.amta-researchers.10,0,0.0335755,"Missing"
2020.acl-main.432,N19-1423,0,0.0383212,"experiments, we use dot-product attention, where the query vector is a learnable weight vector. In this model, prior to attention there is no interaction between the permissible and impermissible tokens. The embedding dimension size is 128. BiLSTM + Attention The encoder is a singlelayer bidirectional LSTM model (Graves and Schmidhuber, 2005) with attention, followed by a linear transformation and a softmax to perform classification. The embedding and hidden dimension size are both set to 128. Transformer Models We use the Bidirectional Encoder Representations from Transformers (BERT) model (Devlin et al., 2019). We use the base version consisting of 12 layers with selfattention. Further, each of the self-attention layers consists of 12 attention heads. The first token of every sequence is the special classification token [CLS], whose final hidden state is used for classification tasks. To block the information flow from permissible to impermissible tokens, we multiply attention weights at every layer with a selfattention mask M, a binary matrix of size n × n where n is the size of the input sequence. An element Mi,j represents whether the token wi should attend on the token wj . Mi,j is 1 if both i"
2020.acl-main.432,W16-3210,0,0.0207931,"Missing"
2020.acl-main.432,J81-4005,0,0.682582,"Missing"
2020.acl-main.432,N19-1357,0,0.057334,"where we manipulate the maxi4785 are restricted to be even. We use two sets of 100K unseen random sequences from the same distribution as the validation and test set. Figure 1: Restricted self-attention in BERT. The information flow through attention is restricted between impermissible and permissible tokens for every encoder layer. The arrows represent the direction of attention. mum attention across all heads, and one where we manipulate the mean attention. 4.3 Sequence-to-sequence Tasks Previous studies analysing the interpretability of attention are all restricted to classification tasks (Jain et al., 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019). Whereas, attention mechanism was first introduced for, and reportedly leads to significant gains in, sequence-to-sequence tasks. Here, we analyse whether for such tasks attention can be manipulated away from its usual interpretation as an alignment between output and input tokens. We begin with three synthetic sequence-to-sequence tasks that involve learning simple input-to-output mappings.3 Bigram Flipping The task is to reverse the bigrams in the input ({w1 , w2 . . . w2n−1 , w2n } → {w2 , w1 , . . . w2n , w2n−1 }). Sequence Copying The"
2020.acl-main.432,C16-1291,0,0.0314599,"tention maps that are more deceptive, since people find them to be more believable explanations of the output (see §5.2). We also extend our analysis to sequenceto-sequence tasks, and a broader set of models, including BERT, and identify mechanisms by which the manipulated models rely on the impermissible tokens despite assigning low attention to them. Lastly, several papers deliberately train attention weights by introducing an additional source of supervision to improve predictive performance. In some of these papers, the supervision comes from known word alignments for machine translation (Liu et al., 2016; Chen et al., 2016), or by aligning human eye-gaze with model’s attention for sequence classification (Barrett et al., 2018). 2 3 Related Work Many recent papers examine whether attention is a valid explanation or not. Jain et al. (2019) idenManipulating Attention Let S = w1 , w2 , . . . , wn denote an input sequence of n words. We assume that for each task, we are 4783 Dataset (Task) Input Example Impermissible Tokens (Percentage) CommonCrawl Biographies (Physician vs Surgeon) Ms. X practices medicine in Memphis, TN and is affiliated with . . . Ms. X speaks English and Spanish. Gender Indica"
2020.acl-main.432,D13-1170,0,0.0140924,"Missing"
2020.acl-main.432,D19-1002,0,0.57817,"he first place. Similarly, Serrano and Smith (2019) modify attention values of a trained model post-hoc by hard-setting the highest attention values to zero. They find that the number of attention values that must be zeroed out to alter the model’s prediction is often too large, and thus conclude that attention is not a suitable tool to for determining which elements should be attributed as responsible for an output. In contrast to these two papers, we manipulate the attention via the learning procedure, producing models whose actual weights might deceive an auditor. In parallel work to ours, Wiegreffe and Pinter (2019) examine the conditions under which attention can be considered a plausible explanation. They design a similar experiment to ours where they train an adversarial model, whose attention distribution is maximally different from the one produced by the base model. Here we look at a related but different question of how attention can be manipulated away from a set of impermissible tokens. Using human studies we show that our training scheme leads to attention maps that are more deceptive, since people find them to be more believable explanations of the output (see §5.2). We also extend our analysi"
2020.acl-main.432,P17-1088,0,0.0494775,"ion. As machine learning is increasingly used in hiring processes for tasks including resume filtering, the potential for bias raises the spectre that automating this process could lead to social harms. De-Arteaga et al. (2019) use attention over gender-revealing tokens (e.g., ‘she’, ‘he’, etc.) to verify the gender bias in occupation classification models—stating that “the attention weights indicate which tokens are most predictive”. Similar claims about attention’s utility for interpreting models’ predictions are common in the literature (Li et al., 2016; Xu et al., 2015; Choi et al., 2016; Xie et al., 2017; Martins and Astudillo, 2016; Lai and Tan, 2019). In this paper, we question whether attention scores necessarily indicate features that influence 4782 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4782–4793 c July 5 - 10, 2020. 2020 Association for Computational Linguistics a model’s predictions. Through a series of experiments on diverse classification and sequence-tosequence tasks, we show that attention scores are surprisingly easy to manipulate. We design a simple training scheme whereby the resulting models appear to assign little attenti"
2020.acl-main.538,P16-1057,0,0.280604,"Missing"
2020.acl-main.538,P15-1085,0,0.0228237,". The code and resources are available at https://github.com/ neulab/external-knowledge-codegen. 1 Mined pairs from Annotated pairs <code, NL> Pre-train Fine-tune Text-to-Code Gen. Model Figure 1: Our approach: incorporating external knowledge by data re-sampling, pre-training and fine-tuning. Semantic parsing, the task of generating machine executable meaning representations from natural language (NL) intents, has generally focused on limited domains (Zelle and Mooney, 1996; Deborah A. Dahl and Shriber, 1994), or domain-specific languages with a limited set of operators (Berant et al., 2013; Quirk et al., 2015; Dong and Lapata, 2016; Liang et al., 2017; Krishnamurthy et al., 2017; Zhong et al., 2017; Yu et al., 2018, 2019b,a). However, recently there has been a move towards applying semantic parsing to automatically generating source code in general-purpose programming languages (Yin et al., 2018; Yao et al., 2018; Lin et al., 2018; Agashe et al., 2019; Yao et al., 2019). Prior work in this area (Xiao et al., 2016; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Dong and Lapata, 2018; Suhr et al., The first two authors contributed equally. Real Distribution Estimation Noisy"
2020.acl-main.538,P17-1105,0,0.11128,"ains (Zelle and Mooney, 1996; Deborah A. Dahl and Shriber, 1994), or domain-specific languages with a limited set of operators (Berant et al., 2013; Quirk et al., 2015; Dong and Lapata, 2016; Liang et al., 2017; Krishnamurthy et al., 2017; Zhong et al., 2017; Yu et al., 2018, 2019b,a). However, recently there has been a move towards applying semantic parsing to automatically generating source code in general-purpose programming languages (Yin et al., 2018; Yao et al., 2018; Lin et al., 2018; Agashe et al., 2019; Yao et al., 2019). Prior work in this area (Xiao et al., 2016; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Dong and Lapata, 2018; Suhr et al., The first two authors contributed equally. Real Distribution Estimation Noisy but real-use distributed Human Curated Data: Introduction ∗ Re-sampling w/ Real Distribution Parsed pairs from API docs 2018; Iyer et al., 2018; Yin and Neubig, 2019) used a variety of models, especially neural architectures, to achieve good performance. However, open-domain code generation for general-purpose languages like Python is challenging. For example, given the intent to choose a random file from the directory contents of the C drive, ‘C:\’,"
2020.acl-main.538,N18-1203,0,0.0751685,"Missing"
2020.acl-main.538,P16-1127,0,0.0363445,", has generally focused on limited domains (Zelle and Mooney, 1996; Deborah A. Dahl and Shriber, 1994), or domain-specific languages with a limited set of operators (Berant et al., 2013; Quirk et al., 2015; Dong and Lapata, 2016; Liang et al., 2017; Krishnamurthy et al., 2017; Zhong et al., 2017; Yu et al., 2018, 2019b,a). However, recently there has been a move towards applying semantic parsing to automatically generating source code in general-purpose programming languages (Yin et al., 2018; Yao et al., 2018; Lin et al., 2018; Agashe et al., 2019; Yao et al., 2019). Prior work in this area (Xiao et al., 2016; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Dong and Lapata, 2018; Suhr et al., The first two authors contributed equally. Real Distribution Estimation Noisy but real-use distributed Human Curated Data: Introduction ∗ Re-sampling w/ Real Distribution Parsed pairs from API docs 2018; Iyer et al., 2018; Yin and Neubig, 2019) used a variety of models, especially neural architectures, to achieve good performance. However, open-domain code generation for general-purpose languages like Python is challenging. For example, given the intent to choose a random file from the"
2020.acl-main.538,D17-1160,0,0.0163562,"eulab/external-knowledge-codegen. 1 Mined pairs from Annotated pairs <code, NL> Pre-train Fine-tune Text-to-Code Gen. Model Figure 1: Our approach: incorporating external knowledge by data re-sampling, pre-training and fine-tuning. Semantic parsing, the task of generating machine executable meaning representations from natural language (NL) intents, has generally focused on limited domains (Zelle and Mooney, 1996; Deborah A. Dahl and Shriber, 1994), or domain-specific languages with a limited set of operators (Berant et al., 2013; Quirk et al., 2015; Dong and Lapata, 2016; Liang et al., 2017; Krishnamurthy et al., 2017; Zhong et al., 2017; Yu et al., 2018, 2019b,a). However, recently there has been a move towards applying semantic parsing to automatically generating source code in general-purpose programming languages (Yin et al., 2018; Yao et al., 2018; Lin et al., 2018; Agashe et al., 2019; Yao et al., 2019). Prior work in this area (Xiao et al., 2016; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Dong and Lapata, 2018; Suhr et al., The first two authors contributed equally. Real Distribution Estimation Noisy but real-use distributed Human Curated Data: Introduction ∗ Re-sampling"
2020.acl-main.538,P17-1003,0,0.0156841,"ttps://github.com/ neulab/external-knowledge-codegen. 1 Mined pairs from Annotated pairs <code, NL> Pre-train Fine-tune Text-to-Code Gen. Model Figure 1: Our approach: incorporating external knowledge by data re-sampling, pre-training and fine-tuning. Semantic parsing, the task of generating machine executable meaning representations from natural language (NL) intents, has generally focused on limited domains (Zelle and Mooney, 1996; Deborah A. Dahl and Shriber, 1994), or domain-specific languages with a limited set of operators (Berant et al., 2013; Quirk et al., 2015; Dong and Lapata, 2016; Liang et al., 2017; Krishnamurthy et al., 2017; Zhong et al., 2017; Yu et al., 2018, 2019b,a). However, recently there has been a move towards applying semantic parsing to automatically generating source code in general-purpose programming languages (Yin et al., 2018; Yao et al., 2018; Lin et al., 2018; Agashe et al., 2019; Yao et al., 2019). Prior work in this area (Xiao et al., 2016; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Dong and Lapata, 2018; Suhr et al., The first two authors contributed equally. Real Distribution Estimation Noisy but real-use distributed Human Curated Data"
2020.acl-main.538,P17-1041,1,0.872274,"996; Deborah A. Dahl and Shriber, 1994), or domain-specific languages with a limited set of operators (Berant et al., 2013; Quirk et al., 2015; Dong and Lapata, 2016; Liang et al., 2017; Krishnamurthy et al., 2017; Zhong et al., 2017; Yu et al., 2018, 2019b,a). However, recently there has been a move towards applying semantic parsing to automatically generating source code in general-purpose programming languages (Yin et al., 2018; Yao et al., 2018; Lin et al., 2018; Agashe et al., 2019; Yao et al., 2019). Prior work in this area (Xiao et al., 2016; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Dong and Lapata, 2018; Suhr et al., The first two authors contributed equally. Real Distribution Estimation Noisy but real-use distributed Human Curated Data: Introduction ∗ Re-sampling w/ Real Distribution Parsed pairs from API docs 2018; Iyer et al., 2018; Yin and Neubig, 2019) used a variety of models, especially neural architectures, to achieve good performance. However, open-domain code generation for general-purpose languages like Python is challenging. For example, given the intent to choose a random file from the directory contents of the C drive, ‘C:\’, one would expect the P"
2020.acl-main.538,D18-2002,1,0.95099,"t to choose a random file from the directory contents of the C drive, ‘C:\’, one would expect the Python code snippet random.choice(os.listdir(‘C:\’)), that realizes the given intent. This would involve not just generating syntactically correct code, but also using (and potentially combining) calls to APIs and libraries that implement some of the desired functionality. As we show in § 3, current code generation models still have difficulty generating the correct function calls with appropriate argument placement. For example, given the NL intent above, although the state-of-the-art model by Yin and Neubig (2018) that uses a transition-based method to generate Python abstract syntax trees is guaranteed to generate syntactically correct code, it still incorrectly outputs random.savefig(random( compile(open(‘C:\’))+100).isoformat()). A known bottleneck to training more accurate code generation models is the limited number of manually annotated training pairs available in existing human-curated datasets, which are insufficient to cover the myriad of ways in which some complex functionality could be implemented in code. However, increasing the size of labeled datasets through additional human annotation"
2020.acl-main.538,P19-1447,1,0.915874,"as been a move towards applying semantic parsing to automatically generating source code in general-purpose programming languages (Yin et al., 2018; Yao et al., 2018; Lin et al., 2018; Agashe et al., 2019; Yao et al., 2019). Prior work in this area (Xiao et al., 2016; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Dong and Lapata, 2018; Suhr et al., The first two authors contributed equally. Real Distribution Estimation Noisy but real-use distributed Human Curated Data: Introduction ∗ Re-sampling w/ Real Distribution Parsed pairs from API docs 2018; Iyer et al., 2018; Yin and Neubig, 2019) used a variety of models, especially neural architectures, to achieve good performance. However, open-domain code generation for general-purpose languages like Python is challenging. For example, given the intent to choose a random file from the directory contents of the C drive, ‘C:\’, one would expect the Python code snippet random.choice(os.listdir(‘C:\’)), that realizes the given intent. This would involve not just generating syntactically correct code, but also using (and potentially combining) calls to APIs and libraries that implement some of the desired functionality. As we show in"
2020.acl-main.538,D18-1425,0,0.0219494,"from Annotated pairs <code, NL> Pre-train Fine-tune Text-to-Code Gen. Model Figure 1: Our approach: incorporating external knowledge by data re-sampling, pre-training and fine-tuning. Semantic parsing, the task of generating machine executable meaning representations from natural language (NL) intents, has generally focused on limited domains (Zelle and Mooney, 1996; Deborah A. Dahl and Shriber, 1994), or domain-specific languages with a limited set of operators (Berant et al., 2013; Quirk et al., 2015; Dong and Lapata, 2016; Liang et al., 2017; Krishnamurthy et al., 2017; Zhong et al., 2017; Yu et al., 2018, 2019b,a). However, recently there has been a move towards applying semantic parsing to automatically generating source code in general-purpose programming languages (Yin et al., 2018; Yao et al., 2018; Lin et al., 2018; Agashe et al., 2019; Yao et al., 2019). Prior work in this area (Xiao et al., 2016; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Dong and Lapata, 2018; Suhr et al., The first two authors contributed equally. Real Distribution Estimation Noisy but real-use distributed Human Curated Data: Introduction ∗ Re-sampling w/ Real Distribution Parsed pairs fr"
2020.acl-main.538,P19-1443,0,0.0219062,"Missing"
2020.acl-main.538,H94-1010,0,\N,Missing
2020.acl-main.538,D13-1160,0,\N,Missing
2020.acl-main.538,D19-1546,0,\N,Missing
2020.acl-main.538,D19-1204,0,\N,Missing
2020.acl-main.722,W19-2804,0,0.0789312,"Missing"
2020.acl-main.722,I17-2016,0,0.0180834,"res to train strong NER models diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have been applied to low-resource settings (Cotterell and Duh, 2017; Huang et al., 2019), directly integrating gazetteer features into these models is difficult because gazetteers in these languages are either limited in coverage or completely absent. Expanding them is time-consuming and expensive, due to the lack of available annotators for low-resource languages (Strassel and Tracey, 2016). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to"
2020.acl-main.722,N19-1383,0,0.01658,"odels diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have been applied to low-resource settings (Cotterell and Duh, 2017; Huang et al., 2019), directly integrating gazetteer features into these models is difficult because gazetteers in these languages are either limited in coverage or completely absent. Expanding them is time-consuming and expensive, due to the lack of available annotators for low-resource languages (Strassel and Tracey, 2016). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to extract information"
2020.acl-main.722,N16-1030,0,0.0741771,"es ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.1 1 Introduction Before the widespread adoption of neural networks for natural language processing tasks, named entity recognition (NER) systems used linguistic features based on lexical and syntactic knowledge to improve performance (Ratinov and Roth, 2009). With the introduction of the neural LSTM-CRF model (Huang et al., 2015; Lample et al., 2016), the need to develop hand-crafted features to train strong NER models diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have"
2020.acl-main.722,W09-1119,0,0.845134,"hese languages. To address this problem, we propose a method of “soft gazetteers” that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.1 1 Introduction Before the widespread adoption of neural networks for natural language processing tasks, named entity recognition (NER) systems used linguistic features based on lexical and syntactic knowledge to improve performance (Ratinov and Roth, 2009). With the introduction of the neural LSTM-CRF model (Huang et al., 2015; Lample et al., 2016), the need to develop hand-crafted features to train strong NER models diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and dat"
2020.acl-main.722,L16-1521,0,0.041183,"r are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have been applied to low-resource settings (Cotterell and Duh, 2017; Huang et al., 2019), directly integrating gazetteer features into these models is difficult because gazetteers in these languages are either limited in coverage or completely absent. Expanding them is time-consuming and expensive, due to the lack of available annotators for low-resource languages (Strassel and Tracey, 2016). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to extract information from these resources and integrate it into the commonlyused CNN-LSTM-CRF NER model (Ma and Hovy, 2016) using a carefully designed feature set. We use entity linking methods designed for low-resource languages, which require far fewer resources than traditional gazetteer features (Upadhyay et al., 2018; Zh"
2020.acl-main.722,D18-1270,0,0.0683232,"Missing"
2020.acl-main.722,D16-1157,0,0.0175096,"the size of the CoNLL-2003 English dataset. These sentences are also annotated with entity links to a knowledge base of 11 million entries, which we use only to aid our analysis. Of particular interest are “NIL” entity mentions that do not have a corresponding entry in the knowledge base (Blissett and Ji, 2019). The fraction of mentions that are NIL is shown in Table 1. bilingual Wikipedia links are used to retrieve the appropriate English KB candidates. • Pivot-based-entity-linking (Zhou et al., 2020): This method encodes entity mentions on the character level using n-gram neural embeddings (Wieting et al., 2016) and computes their similarity with KB entries. We experiment with two variants and follow Zhou et al. (2020) for hyperparameter selection: 1) P BEL S UPERVISED: trained on the small number of bilingual Wikipedia links available in the target low-resource language. 2) P BEL Z ERO: trained on some high-resource language (“the pivot”) and transferred to the target language in a zero-shot manner. The transfer languages we use are Swahili for Kinyarwanda, Indonesian for Oromo, Hindi for Sinhala, and Amharic for Tigrinya. Gazetteer Data We also compare our method with binary gazetteer features, usi"
2020.acl-main.722,P16-1101,0,0.706125,"etteers in these languages are either limited in coverage or completely absent. Expanding them is time-consuming and expensive, due to the lack of available annotators for low-resource languages (Strassel and Tracey, 2016). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to extract information from these resources and integrate it into the commonlyused CNN-LSTM-CRF NER model (Ma and Hovy, 2016) using a carefully designed feature set. We use entity linking methods designed for low-resource languages, which require far fewer resources than traditional gazetteer features (Upadhyay et al., 2018; Zhou et al., 2020). Our experiments demonstrate the effectiveness of our proposed soft gazetteer features, with an average improvement of 4 F1 points over the baseline, across four low-resource languages: Kinyarwanda, Oromo, Sinhala, and Tigrinya. 2 Background Named Entity Recognition NER identifies named entity spans in an input sentence, and classifies them into predefined types (e.g., locatio"
2020.acl-main.722,D18-1310,0,0.21267,"recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.1 1 Introduction Before the widespread adoption of neural networks for natural language processing tasks, named entity recognition (NER) systems used linguistic features based on lexical and syntactic knowledge to improve performance (Ratinov and Roth, 2009). With the introduction of the neural LSTM-CRF model (Huang et al., 2015; Lample et al., 2016), the need to develop hand-crafted features to train strong NER models diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have been applied to low-resource settings (Cotterell and Duh, 2017; Huang et al., 2019), directly integrating g"
2020.acl-main.722,2020.tacl-1.8,1,0.836325,"6). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to extract information from these resources and integrate it into the commonlyused CNN-LSTM-CRF NER model (Ma and Hovy, 2016) using a carefully designed feature set. We use entity linking methods designed for low-resource languages, which require far fewer resources than traditional gazetteer features (Upadhyay et al., 2018; Zhou et al., 2020). Our experiments demonstrate the effectiveness of our proposed soft gazetteer features, with an average improvement of 4 F1 points over the baseline, across four low-resource languages: Kinyarwanda, Oromo, Sinhala, and Tigrinya. 2 Background Named Entity Recognition NER identifies named entity spans in an input sentence, and classifies them into predefined types (e.g., location, person, organization). A commonly used method for doing so is the BIO tagging scheme, representing the Beginning, the Inside and the Outside of a text segment (Ratinov and Roth, 2009). The first word of a named entity"
2020.acl-main.722,W03-0419,0,\N,Missing
2020.acl-main.745,P19-1444,0,0.579569,"onal classification layers, semantic parsing is highly domain-specific, and the architecture of a neural parser is strongly coupled with the structure of its underlying DB (e.g., systems for SQL-based and other types of DBs use different encoder mod8413 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–8426 c July 5 - 10, 2020. 2020 Association for Computational Linguistics els). In fact, existing systems have attempted to leverage BERT, but each with their own domainspecific, in-house strategies to encode the structured information in the DB (Guo et al., 2019; Zhang et al., 2019a; Hwang et al., 2019), and importantly, without pretraining representations on structured data. These challenges call for development of general-purpose pretraining approaches tailored to learning representations for both NL utterances and structured DB tables. In this paper we present TA B ERT, a pretraining approach for joint understanding of NL text and (semi-)structured tabular data (§ 3). TA B ERT is built on top of BERT, and jointly learns contextual representations for utterances and the structured schema of DB tables (e.g., a vector for each utterance token and tab"
2020.acl-main.745,D17-1160,0,0.0575742,"of economics). A key challenge in this scenario is understanding the structured schema of DB tables (e.g., the name, data type, and stored values of columns), and more importantly, the alignment between the input text and the schema (e.g., the token “GDP” refers to the Gross Domestic Product column), which is essential for inferring the correct DB query (Berant and Liang, 2014). Neural semantic parsers tailored to this task therefore attempt to learn joint representations of NL utterances and the (semi-)structured schema of DB tables (e.g., representations of its columns or cell values, as in Krishnamurthy et al. (2017); Bogin et al. (2019b); Wang et al. (2019a), inter alia). However, this unique setting poses several challenges in applying pretrained LMs. First, information stored in DB tables exhibit strong underlying structure, while existing LMs (e.g., BERT) are solely trained for encoding free-form text. Second, a DB table could potentially have a large number of rows, and naively encoding all of them using a resource-heavy LM is computationally intractable. Finally, unlike most text-based QA tasks (e.g., SQuAD, Rajpurkar et al. (2016)) which could be formulated as a generic answer span selection proble"
2020.acl-main.745,P17-2031,0,0.0577108,"Missing"
2020.acl-main.745,2021.ccl-1.108,0,0.210001,"Missing"
2020.acl-main.745,K16-1006,0,0.0299363,"s on the challenging weakly-supervised semantic parsing benchmark W IKI TABLE Q UESTIONS, while performing competitively on the text-toSQL dataset S PIDER.1 1 Introduction Recent years have witnessed a rapid advance in the ability to understand and answer questions about free-form natural language (NL) text (Rajpurkar et al., 2016), largely due to large-scale, pretrained language models (LMs) like BERT (Devlin et al., 2019). These models allow us to capture the syntax and semantics of text via representations learned in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019). It is also relatively easy to apply such pretrained LMs to comprehension tasks that are modeled as text span selection problems, where the boundary of an answer span can be predicted using a simple classifier on top of the LM (Joshi et al., 2019). ∗ 1 Work done while at Facebook AI Research. Available at github.com/facebookresearch/TaBERT Wen-tau Yih Sebastian Riedel Facebook AI Research {scottyih,sriedel}@fb.com However, it is less clear how one could pretrain and fine-tune such models for other"
2020.acl-main.745,P15-1142,0,0.77818,"t systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts using B ERT, registering state-of-the-art performance on W IKI TABLE Q UESTIONS, while performing competitively on S PIDER (§ 5). 2 Background Semantic Parsing over Tables Semantic parsing tackles the task of translating an NL utterance u"
2020.acl-main.745,N18-1202,0,0.0314115,"ntic parsing benchmark W IKI TABLE Q UESTIONS, while performing competitively on the text-toSQL dataset S PIDER.1 1 Introduction Recent years have witnessed a rapid advance in the ability to understand and answer questions about free-form natural language (NL) text (Rajpurkar et al., 2016), largely due to large-scale, pretrained language models (LMs) like BERT (Devlin et al., 2019). These models allow us to capture the syntax and semantics of text via representations learned in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019). It is also relatively easy to apply such pretrained LMs to comprehension tasks that are modeled as text span selection problems, where the boundary of an answer span can be predicted using a simple classifier on top of the LM (Joshi et al., 2019). ∗ 1 Work done while at Facebook AI Research. Available at github.com/facebookresearch/TaBERT Wen-tau Yih Sebastian Riedel Facebook AI Research {scottyih,sriedel}@fb.com However, it is less clear how one could pretrain and fine-tune such models for other QA tasks that involve joint reasoning ove"
2020.acl-main.745,D19-1005,0,0.0239546,"nt representations of columns from an individual table with global information of its linked tables defined by the DB schema (Bogin et al., 2019a; Wang et al., 2019a). TA B ERT could also potentially improve performance of these systems with improved table-level representations. Knowledge-enhanced Pretraining Recent pretraining models have incorporated structured information from knowledge bases (KBs) or other structured semantic annotations into training contextual word representations, either by fusing vector representations of entities and relations on KBs into word representations of LMs (Peters et al., 2019; Zhang et al., 2019b,c), or by encouraging the LM to recover KB entities and relations from text (Sun et al., 2019; Liu et al., 2019a). TA B ERT is broadly relevant to this line in that it also exposes an LM with structured data (i.e., tables), while aiming to learn joint representations for both textual and structured tabular data. 7 Conclusion and Future Work We present TA B ERT, a pretrained encoder for joint understanding of textual and tabular data. We show that semantic parsers using TA B ERT as a general-purpose feature representation layer achieved strong results on two benchmarks. Th"
2020.acl-main.745,Q13-1033,0,0.0779617,"Missing"
2020.acl-main.745,P18-1034,0,0.0191951,"ation. 6 Related Works Semantic Parsing over Tables Tables are important media of world knowledge. Semantic parsers have been adapted to operate over structured DB tables (Wang et al., 2015; Xu et al., 2017; Dong and Lapata, 2018; Yu et al., 2018b; Shi et al., 2018; Wang et al., 2018), and open-domain, semistructured Web tables (Pasupat and Liang, 2015; Sun et al., 2016; Neelakantan et al., 2016). To improve representations of utterances and tables for neural semantic parsing, existing systems have applied pretrained word embeddings (e.g.., GloVe, as in Zhong et al. (2017); Yu et al. (2018a); Sun et al. (2018); Liang et al. (2018)), and BERT-family models for learning joint contextual representations of utterances and tables, but with domain-specific approaches to encode the structured information in tables (Hwang et al., 2019; He et al., 2019; Guo et al., 2019; Zhang et al., 2019a). TA B ERT advances this line of research by presenting a generalpurpose, pretrained encoder over parallel corpora of Web tables and NL context. Another relevant direction is to augment representations of columns from an individual table with global information of its linked tables defined by the DB schema (Bogin et al.,"
2020.acl-main.745,P15-1129,0,0.0386304,"d capture both the general information of the column (via MCP) and its representative cell values related to the utterance (via CVR). Tab. 5 shows ablation results of pretraining TA B ERT with different objectives. We find TA B ERT trained with both MCP and the auxiliary CVR objectives gets a slight advantage, suggesting CVR could potentially lead to 8420 more representative column representations with additional cell information. 6 Related Works Semantic Parsing over Tables Tables are important media of world knowledge. Semantic parsers have been adapted to operate over structured DB tables (Wang et al., 2015; Xu et al., 2017; Dong and Lapata, 2018; Yu et al., 2018b; Shi et al., 2018; Wang et al., 2018), and open-domain, semistructured Web tables (Pasupat and Liang, 2015; Sun et al., 2016; Neelakantan et al., 2016). To improve representations of utterances and tables for neural semantic parsing, existing systems have applied pretrained word embeddings (e.g.., GloVe, as in Zhong et al. (2017); Yu et al. (2018a); Sun et al. (2018); Liang et al. (2018)), and BERT-family models for learning joint contextual representations of utterances and tables, but with domain-specific approaches to encode the str"
2020.acl-main.745,P15-1128,1,0.828901,"t span selection problems, where the boundary of an answer span can be predicted using a simple classifier on top of the LM (Joshi et al., 2019). ∗ 1 Work done while at Facebook AI Research. Available at github.com/facebookresearch/TaBERT Wen-tau Yih Sebastian Riedel Facebook AI Research {scottyih,sriedel}@fb.com However, it is less clear how one could pretrain and fine-tune such models for other QA tasks that involve joint reasoning over both free-form NL text and structured data. One example task is semantic parsing for access to databases (DBs) (Zelle and Mooney, 1996; Berant et al., 2013; Yih et al., 2015), the task of transducing an NL utterance (e.g., “Which country has the largest GDP?”) into a structured query over DB tables (e.g., SQL querying a database of economics). A key challenge in this scenario is understanding the structured schema of DB tables (e.g., the name, data type, and stored values of columns), and more importantly, the alignment between the input text and the schema (e.g., the token “GDP” refers to the Gross Domestic Product column), which is essential for inferring the correct DB query (Berant and Liang, 2014). Neural semantic parsers tailored to this task therefore attem"
2020.acl-main.745,D18-2002,1,0.939848,"ple consists of an utterance (e.g., “What is the total number of languages used in Aruba?”), a DB with one or more tables, and an annotated SQL query, which typically involves joining multiple tables to get the answer (e.g., SELECT COUNT(*) FROM Country JOIN Lang ON Country.Code = Lang.CountryCode WHERE Name = ‘Aruba’). Base Semantic Parser We aim to show TA B ERT could help improve upon an already strong parser. Unfortunately, at the time of writing, none of the top systems on S PIDER were publicly available. To establish a reasonable testbed, we developed our in-house system based on TranX (Yin and Neubig, 2018), an open-source general-purpose semantic parser. TranX translates an NL utterance into an intermediate meaning representation guided by a user-defined grammar. The generated intermediate MR could then be deterministically converted to domain-specific query languages (e.g., SQL). We use TA B ERT as encoder of utterances and table schemas. Specifically, for a given utterance u and a DB with a set of tables T = {Tt }, we first pair u with each table Tt in T as inputs to TA B ERT, which generates |T |sets of table-specific representations of utterances and columns. At each time step, an LSTM deco"
2020.acl-main.745,N18-2093,0,0.382463,"s (§ 3.2). TA B ERT can be plugged into a neural semantic parser as a general-purpose encoder to compute representations for utterances and tables. Our key insight is that although semantic parsers are highly domain-specific, most systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts"
2020.acl-main.745,D18-1193,0,0.54357,"s (§ 3.2). TA B ERT can be plugged into a neural semantic parser as a general-purpose encoder to compute representations for utterances and tables. Our key insight is that although semantic parsers are highly domain-specific, most systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts"
2020.acl-main.745,D18-1425,0,0.333428,"s (§ 3.2). TA B ERT can be plugged into a neural semantic parser as a general-purpose encoder to compute representations for utterances and tables. Our key insight is that although semantic parsers are highly domain-specific, most systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts"
2020.acl-main.745,D17-1125,0,0.0664293,"Missing"
2020.acl-main.745,P19-1139,0,0.334363,"n layers, semantic parsing is highly domain-specific, and the architecture of a neural parser is strongly coupled with the structure of its underlying DB (e.g., systems for SQL-based and other types of DBs use different encoder mod8413 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–8426 c July 5 - 10, 2020. 2020 Association for Computational Linguistics els). In fact, existing systems have attempted to leverage BERT, but each with their own domainspecific, in-house strategies to encode the structured information in the DB (Guo et al., 2019; Zhang et al., 2019a; Hwang et al., 2019), and importantly, without pretraining representations on structured data. These challenges call for development of general-purpose pretraining approaches tailored to learning representations for both NL utterances and structured DB tables. In this paper we present TA B ERT, a pretraining approach for joint understanding of NL text and (semi-)structured tabular data (§ 3). TA B ERT is built on top of BERT, and jointly learns contextual representations for utterances and the structured schema of DB tables (e.g., a vector for each utterance token and table column). Specific"
2020.acl-main.745,P14-1133,0,\N,Missing
2020.acl-main.745,D13-1160,0,\N,Missing
2020.acl-main.745,N19-1273,0,\N,Missing
2020.acl-main.745,P19-1448,0,\N,Missing
2020.acl-main.745,N19-1423,0,\N,Missing
2020.acl-main.745,D19-1537,0,\N,Missing
2020.acl-main.745,D19-1391,0,\N,Missing
2020.acl-main.754,D17-1268,1,0.838312,"h training language on the current model, namely R(i; θt ). We estimate this value by i sampling a batch of data from each Dtrain to get the training gradient for θt , and use this to calculate the reward for this language. This process is detailed in line 11 of the line 25. Unlike the algorithm in DDS which requires storing n model gradients,3 this approximation does not require extra memory even if n is large, which is important given recent efforts to scale multilingual training to 100+ (Arivazhagan et al., 2019; Aharoni et al., 2019) or even 1000+ lan¨ guages (Ostling and Tiedemann, 2017; Malaviya et al., 2017). 5 k=1 PD(i; ψt) n Dtrain Stabilized Multi-objective Training (10) where Jdev (·) defines the combination of n dev sets, and we simply plug in its definition from Eq. 3. Intuitively, Eq. 10 implies that we should favor the training language i if its gradient aligns with the gradient of the aggregated dev risk of all languages. Implementing the Scorer Update The pseudocode for the training algorithm using MultiDDS can be found in line 25. Notably, we do not update the data scorer ψ on every training step, because it is too computationally expensive for NMT training (Wang et al., 2019b). Instea"
2020.acl-main.754,P19-1583,1,0.937102,"016). These models have two particularly concrete advantages over their monolingual counterparts. First, deploying a single multilingual model is much more resource efficient than deploying one model for each language under consideration (Arivazhagan et al., 2019; Aharoni et al., 2019). Second, multilingual training makes it possible to transfer knowledge from high-resource languages (HRLs) to improve performance on lowresource languages (LRLs) (Zoph et al., 2016; 1 The code is available at https://github.com/ cindyxinyiwang/fairseq/tree/multiDDS. Nguyen and Chiang, 2018; Neubig and Hu, 2018; Wang and Neubig, 2019; Aharoni et al., 2019). A common problem with multilingual training is that the data from different languages are both heterogeneous (different languages may exhibit very different properties) and imbalanced (there may be wildly varying amounts of training data for each language). Thus, while LRLs will often benefit from transfer from other languages, for languages where sufficient monolingual data exists, performance will often decrease due to interference from the heterogeneous nature of the data. This is especially the case for modestly-sized models that are conducive to efficient deployme"
2020.acl-main.754,D09-1092,0,0.0584651,"iverse group. The models trained with MultiDDS-S perform better and have less variance. variance and a higher mean than MultiDDS. Additionally, we compare the learned language distribution of MultiDDS-S and MultiDDS in Fig. 5. The learned language distribution in both plots fluctuates similarly, but MultiDDS has more drastic changes than MultiDDS-S. This is also likely due to the reward of MultiDDS-S having less variance than that of MultiDDS. 7 Related Work Our work is related to the multilingual training methods in general. Multilingual training has a rich history (Schultz and Waibel, 1998; Mimno et al., 2009; Shi et al., 2010; T¨ackstr¨om et al., 2013), but has become particularly prominent in recent years due the ability of neural networks to easily perform multi-task learning (Dong et al., 2015; Plank et al., 2016; Johnson et al., 2016). As stated previously, recent results have demonstrated the importance of balancing HRLs and LRLs during multilingual training (Arivazhagan et al., 2019; Conneau et al., 2019), which is largely done with heuristic sampling using a temperature term; MultiDDS provides a more effective and less heuristic method. Wang and Neubig (2019); Lin et al. (2019) choose lang"
2020.acl-main.754,D18-1103,1,0.948476,"15; Johnson et al., 2016). These models have two particularly concrete advantages over their monolingual counterparts. First, deploying a single multilingual model is much more resource efficient than deploying one model for each language under consideration (Arivazhagan et al., 2019; Aharoni et al., 2019). Second, multilingual training makes it possible to transfer knowledge from high-resource languages (HRLs) to improve performance on lowresource languages (LRLs) (Zoph et al., 2016; 1 The code is available at https://github.com/ cindyxinyiwang/fairseq/tree/multiDDS. Nguyen and Chiang, 2018; Neubig and Hu, 2018; Wang and Neubig, 2019; Aharoni et al., 2019). A common problem with multilingual training is that the data from different languages are both heterogeneous (different languages may exhibit very different properties) and imbalanced (there may be wildly varying amounts of training data for each language). Thus, while LRLs will often benefit from transfer from other languages, for languages where sufficient monolingual data exists, performance will often decrease due to interference from the heterogeneous nature of the data. This is especially the case for modestly-sized models that are conduciv"
2020.acl-main.754,D18-1034,1,0.824344,"nguages. Experiments on two sets of languages under both one-to-many and manyto-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.1 1 Introduction Multilingual models are trained to process different languages in a single model, and have been applied to a wide variety of NLP tasks such as text classification (Klementiev et al., 2012; Chen et al., 2018a), syntactic analysis (Plank et al., 2016; Ammar et al., 2016), named-entity recognition (Xie et al., 2018; Wu and Dredze, 2019), and machine translation (MT) (Dong et al., 2015; Johnson et al., 2016). These models have two particularly concrete advantages over their monolingual counterparts. First, deploying a single multilingual model is much more resource efficient than deploying one model for each language under consideration (Arivazhagan et al., 2019; Aharoni et al., 2019). Second, multilingual training makes it possible to transfer knowledge from high-resource languages (HRLs) to improve performance on lowresource languages (LRLs) (Zoph et al., 2016; 1 The code is available at https://github"
2020.acl-main.754,P18-2104,0,0.0193142,"l., 2015; Plank et al., 2016; Johnson et al., 2016). As stated previously, recent results have demonstrated the importance of balancing HRLs and LRLs during multilingual training (Arivazhagan et al., 2019; Conneau et al., 2019), which is largely done with heuristic sampling using a temperature term; MultiDDS provides a more effective and less heuristic method. Wang and Neubig (2019); Lin et al. (2019) choose languages from multilingual data to improve the performance on a particular language, while our work instead aims to train a single model that handles translation between many languages. (Zaremoodi et al., 2018; Wang et al., 2018, 2019a) propose improvements to the model architecture to improve multilingual performance, while MultiDDS is a model-agnostic and optimizes multilingual data usage. Our work is also related to machine learning methods that balance multitask learning (Chen et al., 2018b; Kendall et al., 2018). For example, Kendall et al. (2018) proposes to weigh the training loss from a multitask model based on the uncertainty of each task. Our method focuses on optimizing the multilingual data usage, and is both somewhat orthogonal to and less heuristic than such loss weighting methods. Fi"
2020.acl-main.754,D16-1163,0,0.0546451,"ar et al., 2016), named-entity recognition (Xie et al., 2018; Wu and Dredze, 2019), and machine translation (MT) (Dong et al., 2015; Johnson et al., 2016). These models have two particularly concrete advantages over their monolingual counterparts. First, deploying a single multilingual model is much more resource efficient than deploying one model for each language under consideration (Arivazhagan et al., 2019; Aharoni et al., 2019). Second, multilingual training makes it possible to transfer knowledge from high-resource languages (HRLs) to improve performance on lowresource languages (LRLs) (Zoph et al., 2016; 1 The code is available at https://github.com/ cindyxinyiwang/fairseq/tree/multiDDS. Nguyen and Chiang, 2018; Neubig and Hu, 2018; Wang and Neubig, 2019; Aharoni et al., 2019). A common problem with multilingual training is that the data from different languages are both heterogeneous (different languages may exhibit very different properties) and imbalanced (there may be wildly varying amounts of training data for each language). Thus, while LRLs will often benefit from transfer from other languages, for languages where sufficient monolingual data exists, performance will often decrease due"
2020.acl-main.754,N18-2084,1,0.870494,"˜i (x, y) ∼ Dtrain X, Y ← X, Y ∪ x, y end . Train the NMT model for multiple steps for x, y in X, Y do θ← GradientUpdate (θ, ∇θ `(x, y; θ)) end . Estimate the effect of each language R(i; θ) for i from 1 to n do i x0 , y 0 ∼ Dtrain gtrain ← ∇θ `(x0 , y 0 ; θ) θ0 ← GradientUpdate(θ, gtrain ) gdev ← 0 for j from 1 to n do j xd , yd ∼ Ddev gdev ← gdev + ∇θ0 `(xd , yd ; θ0 ) end R(i; θ) ← cos(gdev , gtrain ) end . Optimize P ψ dψ ← ni=1 R(i; θ) · ∇ψ log (PD (i; ψ)) ψ ← GradientUpdate(ψ, dψ ) end 6 6.1 Experimental Evaluation Data and Settings We use the 58-languages-to-English parallel data from Qi et al. (2018). A multilingual NMT model is trained for each of the two sets of language pairs with different level of language diversity: Related: 4 LRLs (Azerbaijani: aze, Belarusian: bel, Glacian: glg, Slovak: slk) and a related HRL for each LRL (Turkish: tur, Russian: rus, Portuguese: por, Czech: ces) Diverse: 8 languages with varying amounts of data, picked without consideration for relatedness (Bosnian: bos, Marathi: mar, Hindi: hin, Macedonian: mkd, Greek: ell, Bulgarian: bul, French: fra, Korean: kor) Statistics of the datasets are in § A.3. For each set of languages, we test two varieties of transl"
2020.acl-main.754,D10-1103,0,0.0349742,"dels trained with MultiDDS-S perform better and have less variance. variance and a higher mean than MultiDDS. Additionally, we compare the learned language distribution of MultiDDS-S and MultiDDS in Fig. 5. The learned language distribution in both plots fluctuates similarly, but MultiDDS has more drastic changes than MultiDDS-S. This is also likely due to the reward of MultiDDS-S having less variance than that of MultiDDS. 7 Related Work Our work is related to the multilingual training methods in general. Multilingual training has a rich history (Schultz and Waibel, 1998; Mimno et al., 2009; Shi et al., 2010; T¨ackstr¨om et al., 2013), but has become particularly prominent in recent years due the ability of neural networks to easily perform multi-task learning (Dong et al., 2015; Plank et al., 2016; Johnson et al., 2016). As stated previously, recent results have demonstrated the importance of balancing HRLs and LRLs during multilingual training (Arivazhagan et al., 2019; Conneau et al., 2019), which is largely done with heuristic sampling using a temperature term; MultiDDS provides a more effective and less heuristic method. Wang and Neubig (2019); Lin et al. (2019) choose languages from multili"
2020.acl-main.754,Q13-1001,0,0.0682599,"Missing"
2020.acl-main.754,P02-1040,0,\N,Missing
2020.acl-main.754,C12-1089,0,\N,Missing
2020.acl-main.754,P15-1166,0,\N,Missing
2020.acl-main.754,Q17-1024,0,\N,Missing
2020.acl-main.754,E17-2102,0,\N,Missing
2020.acl-main.754,I17-2050,0,\N,Missing
2020.acl-main.754,D18-2012,0,\N,Missing
2020.acl-main.754,D18-1326,0,\N,Missing
2020.acl-main.754,N19-1388,0,\N,Missing
2020.acl-main.754,Q18-1039,0,\N,Missing
2020.acl-main.764,D08-1078,0,0.368952,"thread is to predict a method’s performance as a function of the training dataset size. Our work belongs in the second thread, but could easily be extended to encompass training time/procedure. In the first thread, Kolachina et al. (2012b) attempt to infer learning curves based on training data features and extrapolate the initial learning curves based on BLEU measurements for statistical machine translation (SMT). By extrapolating the performance of initial learning curves, the predictions on the remainder allows for early termination of a bad run (Domhan et al., 2015). In the second thread, Birch et al. (2008) adopt linear regression to capture the relationship between data features and SMT performance and find that the amount of reordering, the morphological complexity of the target language and the relatedness of the two languages explains the majority of performance variability. More recently, Elsahar and Gallé (2019) use domain shift metrics such as H-divergence based metrics to predict drop in performance under domain-shift. Rosenfeld et al. 8632 RMSE 8 6 4 HIT-SCIR (78.86) UDPipe (76.07) 3 0 1 2 3 4 5 RMSE Phoenix (68.17) 6 5 0 1 2 3 4 5 8 7 6 5 4.5 4 5 4.5 4 4 ICS (75.98) LATTICE (76.07) 0 1"
2020.acl-main.764,D18-1024,0,0.0333568,"Missing"
2020.acl-main.764,D19-1222,0,0.0302189,"rapolate the initial learning curves based on BLEU measurements for statistical machine translation (SMT). By extrapolating the performance of initial learning curves, the predictions on the remainder allows for early termination of a bad run (Domhan et al., 2015). In the second thread, Birch et al. (2008) adopt linear regression to capture the relationship between data features and SMT performance and find that the amount of reordering, the morphological complexity of the target language and the relatedness of the two languages explains the majority of performance variability. More recently, Elsahar and Gallé (2019) use domain shift metrics such as H-divergence based metrics to predict drop in performance under domain-shift. Rosenfeld et al. 8632 RMSE 8 6 4 HIT-SCIR (78.86) UDPipe (76.07) 3 0 1 2 3 4 5 RMSE Phoenix (68.17) 6 5 0 1 2 3 4 5 8 7 6 5 4.5 4 5 4.5 4 4 ICS (75.98) LATTICE (76.07) 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 BOUN (66.69) CUNI (66.6) ONLP (61.92) 8 7 6 0 1 2 3 4 5 12 10 0 1 2 3 4 5 0 1 2 3 4 5 Figure 3: RMSE scores of UD task from dataset-wise mean value predictor (the dashed black line in each graph) and predictors trained with experimental records of other models and 0–5 records from a"
2020.acl-main.764,P02-1040,0,\N,Missing
2020.acl-main.764,P12-1003,0,\N,Missing
2020.acl-main.764,D16-1250,0,\N,Missing
2020.acl-main.764,E17-2002,0,\N,Missing
2020.acl-main.764,P17-1042,0,\N,Missing
2020.acl-main.764,D17-1207,0,\N,Missing
2020.acl-main.764,N18-2084,1,\N,Missing
2020.acl-main.764,D18-1268,1,\N,Missing
2020.acl-main.764,Q18-1041,0,\N,Missing
2020.acl-main.764,N19-1188,0,\N,Missing
2020.acl-main.764,P19-1399,0,\N,Missing
2020.acl-main.764,P19-1494,0,\N,Missing
2020.acl-main.764,P19-1308,0,\N,Missing
2020.acl-main.764,K19-1087,0,\N,Missing
2020.acl-main.766,D18-1399,0,0.0338075,"Missing"
2020.acl-main.766,D18-1024,0,0.0146756,"all mappings could be learned jointly, taking advantage of the inter-dependencies between any two language pairs. Importantly, though, there is no closed form solution for learning the joint mapping, hence a solution needs to be approximated with gradientbased methods. The main approaches are: Bilingual Word Embeddings In the supervised BWE setting of Mikolov et al. (2013), given two languages L = {l1 , l2 } and their pre-trained row-aligned embeddings X1 , X2 , respectively, a transformation matrix M is learned such that: • Multilingual adversarial training with pseudorandomized refinement (Chen and Cardie, 2018, MAT+MPSR): a generalization of the adversarial approach of Zhang et al. (2017); Conneau et al. (2018) to multiple languages, also combined with an iterative refinement procedure.4 M = arg min kX1 − M X2 k . M ∈Ω The set Ω can potentially impose a constraint over M , such as the very popular constraint of restricting it to be orthogonal (Xing et al., 2015). Previous work has empirically found that this simple formulation is competitive with other more complicated alternatives (Xing et al., 2015). The orthogonality assumption ensures that there exists a closed-form solution through Singular •"
2020.acl-main.766,P19-1070,0,0.0174121,"to accuracy; we provide P@5 and P@10 results in the Appendix. 3 New LI Evaluation Dictionaries The typically used evaluation dictionaries cover a narrow breadth of the possible language pairs, with the majority of them focusing in pairs with English (as with the MUSE or Dinu et al. (2015) dictionaries) or among high-resource European languages. Glavaš et al. (2019), for instance, highlighted Anglocentricity as an issue, creating and evaluating on 28 dictionaries between 8 languages (Croatian, English, Finnish, French, German, Italian, Russian, Turkish) based on Google Translate. In addition, Czarnowska et al. (2019) focused on the morphology dimension, creating morphologically complete dictionaries for 2 sets of 5 genetically related languages (Romance: French, Spanish, Italian, Portuguese, Catalan; and Slavic: Polish, Czech, Slovak, Russian, Ukrainian). In contrast to these two (very valuable!) works, our method for creating dictionaries 5 Note that Alaux et al. (2019) use the term pivot to refer to what we refer to as the hub language. Pt: En: Cs: prácu trabalho job work praca práca práce pracovní Figure 1: Transitivity example (Portuguese → English → Czech). for low-resource languages (§3.1) leverages"
2020.acl-main.766,L18-1550,0,0.0815163,"rt-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned with a two-step process, whether under bilingual or multilingual settings (hereinafter BWE and MWE, respectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many languages and are widely used. Second, a mapping between the languages is learned in one of three ways: in a supervised manner if dictionaries or parallel data are available to be used for supervision (Zou et al., 2013), under minimal supervision e.g. using only identical strings (Smith et al., 2017), or even in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2018). Both in bilingual and multilingual settings, it is common that one of the language embedding spaces is the target to which all other languages get aligned (hereinafter “the hub&quot;). We outline th"
2020.acl-main.766,N19-1188,0,0.0375994,"Missing"
2020.acl-main.766,W13-2233,0,0.0133612,"pairs that do not include English. 1 Introduction Continuous vectors for representing words (embeddings) (Turian et al., 2010) have become ubiquitous in modern, neural NLP. Cross-lingual representations (Mikolov et al., 2013) additionally represent words from various languages in a shared continuous space, which in turn can be used for Bilingual Lexicon Induction (BLI). BLI is often the first step towards several downstream tasks such as Part-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned with a two-step process, whether under bilingual or multilingual settings (hereinafter BWE and MWE, respectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many languages and are widely used. Second, a mapping between the languages is learned in one of three ways: in a supervised manner if dictionaries"
2020.acl-main.766,D19-1328,0,0.0182401,"duction performance (measured with P@1) over 10 languages (90 pairs). In each cell, the superscript denotes the hub language that yields the best result for that language pair. µbest : average using the best hub language. µEn : average using the En as the hub. The lightly shaded cells are the language pairs where a bilingual VecMap system outperforms MAT+MSPR; in heavy shaded cells both MUSEs and VecMap outperform MAT+MSPR. on automated annotations in order to scale to all languages. Our method that uses automatically obtained morphological information combined with the guidelines proposed by Kementchedjhieva et al. (2019) (e.g. removing proper nouns from the evaluation set) scales easily to multiple languages, allowing us to create more than 4 thousand dictionaries. 4 experiments, we use MUSEs14 (MUSE, semisupervised) and VecMap15 systems, and we additionally compare them to MAT+MPSR for completeness. We compare the statistical significance of the performance difference of two systems using paired bootstrap resampling (Koehn, 2004). Generally, a difference of 0.4–0.5 percentage points evaluated over our lexica is significant with p &lt; 0.05. Lexicon Induction Experiments The aim of our LI experiments is two-fold"
2020.acl-main.766,C12-1089,0,0.0314942,"ngual embedding baselines, that extend to language pairs that do not include English. 1 Introduction Continuous vectors for representing words (embeddings) (Turian et al., 2010) have become ubiquitous in modern, neural NLP. Cross-lingual representations (Mikolov et al., 2013) additionally represent words from various languages in a shared continuous space, which in turn can be used for Bilingual Lexicon Induction (BLI). BLI is often the first step towards several downstream tasks such as Part-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned with a two-step process, whether under bilingual or multilingual settings (hereinafter BWE and MWE, respectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many languages and are widely used. Second, a mapping between the languages is learned in"
2020.acl-main.766,W04-3250,0,0.178645,"ated annotations in order to scale to all languages. Our method that uses automatically obtained morphological information combined with the guidelines proposed by Kementchedjhieva et al. (2019) (e.g. removing proper nouns from the evaluation set) scales easily to multiple languages, allowing us to create more than 4 thousand dictionaries. 4 experiments, we use MUSEs14 (MUSE, semisupervised) and VecMap15 systems, and we additionally compare them to MAT+MPSR for completeness. We compare the statistical significance of the performance difference of two systems using paired bootstrap resampling (Koehn, 2004). Generally, a difference of 0.4–0.5 percentage points evaluated over our lexica is significant with p &lt; 0.05. Lexicon Induction Experiments The aim of our LI experiments is two-fold. First, the differences in LI performance show the importance of the hub language choice with respect to each evaluation pair. Second, as part of our call for moving beyond Anglo-centric evaluation, we also present LI results on several new language pairs using our triangulated dictionaries. 4.1 Methods and Setup We train and evaluate all models starting with pretrained Wikipedia FastText embeddings for all langua"
2020.acl-main.766,P09-5002,0,0.0332527,"Missing"
2020.acl-main.766,P10-1040,0,0.0868333,"h expand a standard Englishcentered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages.1 Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English. 1 Introduction Continuous vectors for representing words (embeddings) (Turian et al., 2010) have become ubiquitous in modern, neural NLP. Cross-lingual representations (Mikolov et al., 2013) additionally represent words from various languages in a shared continuous space, which in turn can be used for Bilingual Lexicon Induction (BLI). BLI is often the first step towards several downstream tasks such as Part-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned w"
2020.acl-main.766,P06-2112,0,0.0592268,"ifica Match M;SG M;PL F;SG M;SG F;SG SG PL Bridged Greek–Italian Lexicon Greek Italian ειρηνικός ειρηνική ειρηνικό ειρηνικά pacifico, pacifici, pacifica pacifica, pacifico, pacifici pacifica, pacifico, pacifici pacifici, pacifica, pacifico Table 1: Triangulation and filtering example on Greek–Italian. All words are valid translations of the English word ‘peaceful’. We also show filtered-out translations. 3.2 Dictionaries for all Language Pairs through Triangulation Our second method for creating new dictionaries is inspired by phrase table triangulation ideas from the pre-neural MT community (Wang et al., 2006; Levinboim and Chiang, 2015). The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 5 possible Cs translations for the Pt word trabalho. Following this simple triangulation approach, we create 4,704 new dictionaries over pairs between the 50 l"
2020.acl-main.766,N15-1104,0,0.0566144,"Missing"
2020.acl-main.766,P17-1179,0,0.391993,"spectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many languages and are widely used. Second, a mapping between the languages is learned in one of three ways: in a supervised manner if dictionaries or parallel data are available to be used for supervision (Zou et al., 2013), under minimal supervision e.g. using only identical strings (Smith et al., 2017), or even in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2018). Both in bilingual and multilingual settings, it is common that one of the language embedding spaces is the target to which all other languages get aligned (hereinafter “the hub&quot;). We outline the details in Section 2. Despite all the recent progress in learning crosslingual embeddings, we identify a major shortcoming to previous work: it is by and large English-centric. Notably, most MWE approaches essentially select English as the hub during training by default, aligning all other language spaces to the English one. We argue and empirically show, however, that English"
2020.acl-main.766,N16-1156,0,0.0212573,". Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English. 1 Introduction Continuous vectors for representing words (embeddings) (Turian et al., 2010) have become ubiquitous in modern, neural NLP. Cross-lingual representations (Mikolov et al., 2013) additionally represent words from various languages in a shared continuous space, which in turn can be used for Bilingual Lexicon Induction (BLI). BLI is often the first step towards several downstream tasks such as Part-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned with a two-step process, whether under bilingual or multilingual settings (hereinafter BWE and MWE, respectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many lan"
2020.acl-main.766,N19-1161,1,0.879801,"Missing"
2020.coling-main.471,D19-1091,1,0.831926,"or each segment. Accordingly, we perform the evaluation at the morpheme level: the first two evaluation units from the Figure 1 example would be “you&quot; and &quot;-GEN1&quot;, separated. This setting will provide somewhat of an oracle score, that would be achievable if a linguist or the community provide correct segmentations for the transcriptions, or if a morphological segmentation tool is available for that language. Transliteration Cross-lingual training between typologically related languages has shown promising results in several NLP tasks especially in low-resource settings (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019). Two of our evaluation languages, namely Lezgian and Tsez are fairly similar as they are both members of the Nakho-Daghestanian language family, and as such are ideal for crosslingual transfer. However, Anastasopoulos and Neubig (2019) pointed out that cross-lingual learning can be inversely impeded if the languages do not use the same script even if they are closely genealogically related languages. Lezgian is written in Cyrillic script while Tsez is written in Latin script. To maximally exploit the power of cross-lingual training, we transliterated Lezgian from Cyrillic script to Latin scri"
2020.coling-main.471,D09-1031,0,0.0389939,"word before the correct destination Musa ‘Musa’. Upon inspection, we found that the input word Musaňin is segmented into Mus-aňin after BPE, and the occurrences of aňin in the training set are overwhelmingly combinations of -a and ňin ‘QUOT’, where -a is a verbal suffix. In contrast, Musa ‘Musa’ is a proper name. The model cannot deduce that ňin is its own morpheme and may appear after nouns, when it has only seen it in tandem with a verbal suffix which, obviously, appears only after verbs. 5405 7 Related Work Several works have studied the automated IGT generation task (Palmer et al., 2009; Baldridge and Palmer, 2009; Samardži´c et al., 2015; Moeller and Hulden, 2018; McMillan-Major, 2020). They mainly used machine learning methods such as CRF and SVM to generate gloss and proposed a series of heuristic post-editing algorithms to improve the performance. Among them, Palmer et al. (2009), Baldridge and Palmer (2009) combined machine labeling and active learning for creating IGT. Moeller and Hulden (2018) tested LSTMs to predict the morphological labels within glosses, but underperformed against CRF models in that task. McMillan-Major (2020) exploited parallel information in gloss generation. These models e"
2020.coling-main.471,W14-2206,0,0.0700873,"Missing"
2020.coling-main.471,C14-1096,0,0.0299487,", to our knowledge, the first one to show that modern neural systems are a viable solution for the automatic glossing task, without requiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying on parallel audio collection (Bird et al., 2014) are in fact already underway in the Americas (Jimerson and Prud’hommeaux, 2018) and Africa (Rialland et al., 2018; Hamlaoui et al., 2018), among other places. An additional advantage of parallel corpora is that they contain rich information that can be beneficial for gloss generation. As Figure 1 outlines, the stems/lemmas in the analysis are often hiding in the translation, while the grammatical tags could be derived from the segments in the transcription. We hypothesize that the information from the translation can further ground the gloss generation, and especially allow a system that prop"
2020.coling-main.471,I17-1004,0,0.0405263,"Missing"
2020.coling-main.471,L18-1533,0,0.0282245,"quiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying on parallel audio collection (Bird et al., 2014) are in fact already underway in the Americas (Jimerson and Prud’hommeaux, 2018) and Africa (Rialland et al., 2018; Hamlaoui et al., 2018), among other places. An additional advantage of parallel corpora is that they contain rich information that can be beneficial for gloss generation. As Figure 1 outlines, the stems/lemmas in the analysis are often hiding in the translation, while the grammatical tags could be derived from the segments in the transcription. We hypothesize that the information from the translation can further ground the gloss generation, and especially allow a system that properly takes into account to generalize to produce lemmas or stems unseen during training. In this work we propose an automated system which"
2020.coling-main.471,L18-1657,0,0.0629911,"Missing"
2020.coling-main.471,W17-0102,0,0.108775,"tems unseen during training. In this work we propose an automated system which creates the hard-to-obtain gloss from an easyto-obtain parallel corpus. We use deep neural models which have driven recent impressive advances in all facets of modern natural language processing (NLP). Our model for automatic gloss generation uses multi-source transformer models, combining information from the transcription and the translation, significantly outperforming previous state-of-the-art results on three challenging datasets in Lezgian, Tsez, and Arapaho (Arkhangelskiy, 2012; Abdulaev and Abdullaev, 2010; Kazeminejad et al., 2017). Importantly, our approach does not rely on any additional annotations other than plain transcription and translation, also making no assumptions about the gloss tag space. We further extend our training recipes to include necessary improvements that deal with data paucity (utilizing cross-lingual transfer from similar languages) and with the specific characteristics of the glossing task (presenting solutions for output length control). Our contributions are three-fold: 1. We apply multi-source transformers on the gloss generation task and significantly outperform previous state-of-the-art st"
2020.coling-main.471,D19-3019,0,0.0172569,"itten in Cyrillic script while Tsez is written in Latin script. To maximally exploit the power of cross-lingual training, we transliterated Lezgian from Cyrillic script to Latin script, and transliterated Tsez from Latin script to Cyrillic script.5 With the original and the transliterated versions of the training data at hand, we combine them during training into a single training set for the L ANGUAGE T RANSFER Model. The evaluation is of course performed on the original test sets with the original corresponding scripts. 5.1 Implementation We base our implementation on the Joey-NMT toolkit6 (Kreutzer et al., 2019), which we extended to support multi-source transformer models.7 The transcription and translation input sentences can be represented at different granularities: either at the word level or at the more recently popular sub-word level. For simplicity we leave this detail out of the results tables, reporting results with the better-performing option in each case. It is worth noting, though, that for Tsez and Arapaho the sub-word representations (obtained using byte-pair-encoding (BPE)8 ) always lead to better results. For the much smaller Lezgian dataset, we saw no difference between sub-word an"
2020.coling-main.471,W19-4226,0,0.0153372,"correct stems or tags for each segment. Accordingly, we perform the evaluation at the morpheme level: the first two evaluation units from the Figure 1 example would be “you&quot; and &quot;-GEN1&quot;, separated. This setting will provide somewhat of an oracle score, that would be achievable if a linguist or the community provide correct segmentations for the transcriptions, or if a morphological segmentation tool is available for that language. Transliteration Cross-lingual training between typologically related languages has shown promising results in several NLP tasks especially in low-resource settings (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019). Two of our evaluation languages, namely Lezgian and Tsez are fairly similar as they are both members of the Nakho-Daghestanian language family, and as such are ideal for crosslingual transfer. However, Anastasopoulos and Neubig (2019) pointed out that cross-lingual learning can be inversely impeded if the languages do not use the same script even if they are closely genealogically related languages. Lezgian is written in Cyrillic script while Tsez is written in Latin script. To maximally exploit the power of cross-lingual training, we transliterated Lezgian"
2020.coling-main.471,2020.scil-1.42,0,0.106078,"or This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 5397 Proceedings of the 28th International Conference on Computational Linguistics, pages 5397–5408 Barcelona, Spain (Online), December 8-13, 2020 Figure 1: A Tsez IGT example. Combining information from both the transcription and the translation can aid in deriving the information in the analysis. stems (Samardži´c et al., 2015), and using models based on Conditional Random Fields (CRF) integrated with translation and POS-tagging information (McMillan-Major, 2020). In contrast, our approach is, to our knowledge, the first one to show that modern neural systems are a viable solution for the automatic glossing task, without requiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying"
2020.coling-main.471,D16-1096,0,0.026883,"er text generation tasks where the generated text can be relatively free in word order, the gloss must map to the transcription morpheme-by-morpheme or word-by-word, dependent on the intended granularity. One drawback of using a seq2seq model for the gloss generation task compared to e.g. a CRF-based approach like (McMillan-Major, 2020) is that a hard constraint of “one output per input” is not enforced by or hard-coded in the model. Even though structural biases (Cohn et al., 2016) such as hard monotonic attention (Wu and Cotterell, 2019), or source-side coverage mechanisms (Tu et al., 2016; Mi et al., 2016) could remedy this potential issue, we found that there was little need for them, as a simple mechanism to control the final output length during inference was sufficient.1 The intuition lies in the observation that the length of the output gloss should match that of the input transcription exactly. Hence, during inference we set a minimum desired length of the output sequence, and disallow any candidates shorter than that.2 Alignment between gloss and transcription To ensure fair evaluation against the baseline and other models, we need to be able to produce the exact mapping of the output gl"
2020.coling-main.471,W18-4809,0,0.271751,"lly manifest a yawning gap between the amount of material recorded and archived and the amount of data that is thoroughly analyzed with morphological segmentation and gloss (Seifart et al., 2018). This gap can be filled using automatic approaches, which could at least accelerate the annotation process by providing high-quality first-pass annotations. Previous approaches to automatic gloss generation include manual rule crafting and deep rule-based analysis (Bender et al., 2014; Snoek et al., 2014), treating the glossing task as a classification problem focusing only on the morphological tags (Moeller and Hulden, 2018) and requiring a lexicon for This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 5397 Proceedings of the 28th International Conference on Computational Linguistics, pages 5397–5408 Barcelona, Spain (Online), December 8-13, 2020 Figure 1: A Tsez IGT example. Combining information from both the transcription and the translation can aid in deriving the information in the analysis. stems (Samardži´c et al., 2015), and using models based on Conditional Random Fields (CRF) integrated with translation and"
2020.coling-main.471,W09-1905,0,0.0375284,"BL SUPER . ESS ’, one word before the correct destination Musa ‘Musa’. Upon inspection, we found that the input word Musaňin is segmented into Mus-aňin after BPE, and the occurrences of aňin in the training set are overwhelmingly combinations of -a and ňin ‘QUOT’, where -a is a verbal suffix. In contrast, Musa ‘Musa’ is a proper name. The model cannot deduce that ňin is its own morpheme and may appear after nouns, when it has only seen it in tandem with a verbal suffix which, obviously, appears only after verbs. 5405 7 Related Work Several works have studied the automated IGT generation task (Palmer et al., 2009; Baldridge and Palmer, 2009; Samardži´c et al., 2015; Moeller and Hulden, 2018; McMillan-Major, 2020). They mainly used machine learning methods such as CRF and SVM to generate gloss and proposed a series of heuristic post-editing algorithms to improve the performance. Among them, Palmer et al. (2009), Baldridge and Palmer (2009) combined machine labeling and active learning for creating IGT. Moeller and Hulden (2018) tested LSTMs to predict the morphological labels within glosses, but underperformed against CRF models in that task. McMillan-Major (2020) exploited parallel information in glos"
2020.coling-main.471,P02-1040,0,0.109072,"veraging over all heads. 5400 Language Translation L EZGIAN T SEZ A RAPAHO English English/Russian English Training Examples Test Examples 951 1584 25208 119 198 3151 Table 1: Dataset information for the languages in our experiments. 4 Evaluating Gloss Generation The characteristics of gloss generation require special care rather than blindly using metrics established for other tasks like machine translation. Previous work uses: • Accuracy: percentage of correct (full) analyses for each token. It is the main metric used in previous work (Samardži´c et al., 2015; McMillan-Major, 2020). • BLEU (Papineni et al., 2002): an average of n-gram precision along with a brevity penalty, BLEU is perhaps the most popular reference-based machine translation method. Since our models are inspired from MT, we use it as another indication of quality as it captures accuracy/precision over n-grams, even though the rest of the metrics are more suitable to the automatic glossing task. • Precision/Recall: We further break down the evaluation to focus separately on lemmas and tags. Several previous works prioritize precision over recall, especially by not outputting tags if items are not seen during training, e.g. (Moeller and"
2020.coling-main.471,L18-1674,0,0.0242646,"ossing task, without requiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying on parallel audio collection (Bird et al., 2014) are in fact already underway in the Americas (Jimerson and Prud’hommeaux, 2018) and Africa (Rialland et al., 2018; Hamlaoui et al., 2018), among other places. An additional advantage of parallel corpora is that they contain rich information that can be beneficial for gloss generation. As Figure 1 outlines, the stems/lemmas in the analysis are often hiding in the translation, while the grammatical tags could be derived from the segments in the transcription. We hypothesize that the information from the translation can further ground the gloss generation, and especially allow a system that properly takes into account to generalize to produce lemmas or stems unseen during training. In this work we propose a"
2020.coling-main.471,2020.sigmorphon-1.21,0,0.0361868,"ansformer for Gloss Generation Problem Formulation and Model Our model is built upon the transformer model (Vaswani et al., 2017), a self-attention-based sequenceto-sequence (seq2seq) neural model. Compared to the CRF model used in McMillan-Major (2020), which can only capture local dependencies, a self-attention model can produce context-sensitive hidden representations that take the whole input into account. Moreover, unlike other recurrent (seq2seq) models such as bidirectional LSTM, the Transformer model shows more robust performance in morphologyrelated tasks under low-resource settings (Ryan and Hulden, 2020). Our architecture choice is also motivated by the promising performance of the model along with its computational efficiency. The original Transformer is composed of a single encoder and a decoder, each with several layers. Each encoder layer consists of a multi-head self-attention layer and a fully connected feed-forward network, while decoder layers are additionally augmented with multi-head attention over the output of the encoder stack. Our model adds a second encoder to create a multi-source transformer similar to (Zoph and Knight, 2016; Anastasopoulos and Chiang, 2018), in order to inco"
2020.coling-main.471,W15-3710,0,0.03569,"Missing"
2020.coling-main.471,P16-1162,0,0.0448655,"Tsez and Arapaho the sub-word representations (obtained using byte-pair-encoding (BPE)8 ) always lead to better results. For the much smaller Lezgian dataset, we saw no difference between sub-word and word-level models, but this lack of difference can be explained by the overall very small size of the vocabulary for the Lezgian dataset. 5 We use the transliterator provided by https://pypi.org/project/transliterate/. https://github.com/joeynmt/joeynmt 7 Our code will be open-sourced at https://github.com/yukiyakiZ/Automatic_Glossing. 8 We use the sentencepiece implementation of the BPE method (Sennrich et al., 2016) with vocab size of 2000 for Lezgian, 2500 for Tsez, and 10000 for Arapaho) 6 5402 For training all Lezgian and Tsez models and the Arapaho model with the subsampled 2,000 training sentences, we use 2 layers for both encoders and the decoder and 2 attention heads. All the embedding and hidden state dimensions are set to 128. We use a batch size of 20. For training the Arapaho model on the original larger dataset, we use 4 layers for all encoders and decoder, with 4 attention heads. The embedding and hidden state dimension are 256, and batch size is 50. For all models, learning rate is initiali"
2020.coling-main.471,W14-2205,0,0.815954,"Missing"
2020.coling-main.471,P16-1008,0,0.0252577,"ontrol Unlike other text generation tasks where the generated text can be relatively free in word order, the gloss must map to the transcription morpheme-by-morpheme or word-by-word, dependent on the intended granularity. One drawback of using a seq2seq model for the gloss generation task compared to e.g. a CRF-based approach like (McMillan-Major, 2020) is that a hard constraint of “one output per input” is not enforced by or hard-coded in the model. Even though structural biases (Cohn et al., 2016) such as hard monotonic attention (Wu and Cotterell, 2019), or source-side coverage mechanisms (Tu et al., 2016; Mi et al., 2016) could remedy this potential issue, we found that there was little need for them, as a simple mechanism to control the final output length during inference was sufficient.1 The intuition lies in the observation that the length of the output gloss should match that of the input transcription exactly. Hence, during inference we set a minimum desired length of the output sequence, and disallow any candidates shorter than that.2 Alignment between gloss and transcription To ensure fair evaluation against the baseline and other models, we need to be able to produce the exact mappin"
2020.coling-main.471,P19-1148,0,0.0226311,"the particular nature of the gloss generation task. Length control Unlike other text generation tasks where the generated text can be relatively free in word order, the gloss must map to the transcription morpheme-by-morpheme or word-by-word, dependent on the intended granularity. One drawback of using a seq2seq model for the gloss generation task compared to e.g. a CRF-based approach like (McMillan-Major, 2020) is that a hard constraint of “one output per input” is not enforced by or hard-coded in the model. Even though structural biases (Cohn et al., 2016) such as hard monotonic attention (Wu and Cotterell, 2019), or source-side coverage mechanisms (Tu et al., 2016; Mi et al., 2016) could remedy this potential issue, we found that there was little need for them, as a simple mechanism to control the final output length during inference was sufficient.1 The intuition lies in the observation that the length of the output gloss should match that of the input transcription exactly. Hence, during inference we set a minimum desired length of the output sequence, and disallow any candidates shorter than that.2 Alignment between gloss and transcription To ensure fair evaluation against the baseline and other m"
2020.coling-main.471,N16-1004,0,0.0285687,"rphologyrelated tasks under low-resource settings (Ryan and Hulden, 2020). Our architecture choice is also motivated by the promising performance of the model along with its computational efficiency. The original Transformer is composed of a single encoder and a decoder, each with several layers. Each encoder layer consists of a multi-head self-attention layer and a fully connected feed-forward network, while decoder layers are additionally augmented with multi-head attention over the output of the encoder stack. Our model adds a second encoder to create a multi-source transformer similar to (Zoph and Knight, 2016; Anastasopoulos and Chiang, 2018), in order to incorporate the secondary information from the translation. A visual depiction of our model is outlined in Figure 2. Let X1 = x11 . . . x1N be a sequence of transcription words, X2 = x21 . . . x2M a sequence of translation words, and Y = y1 . . . yK be a sequence of the target gloss. A single-source gloss generation model attempts to model P (Y |X1 ). A multi-source model can jointly model P (Y |X1 , X2 ), and thus we need two encoders (see Figure 2b). One encoder transforms the input transcription sequence x11 . . . x1N into a sequence of input"
2020.coling-tutorials.7,D19-1091,1,0.837959,") and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to"
2020.coling-tutorials.7,C18-1214,1,0.827863,"graphic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An o"
2020.coling-tutorials.7,W17-0100,0,0.098002,"which typically is language description. The extreme pace of language loss and the urgent needs for language revitalization, however, require that we utilize documentations and go beyond language description: enter 21st century NLP. Himmelmann’s 20-year-old radical vision (Himmelmann, 1998) for a data-centric approach to language documentation (which sparked the creation of modern documentary linguistics) has slowly begun to materialize (McDonnell et al., 2018). For example, the Workshops on the Use of Computational Methods in the Study of Endangered Languages (Comput-EL) (Good et al., 2014; Arppe et al., 2017; Arppe et al., 2019) have provided a small forum for the much-needed discussion between NLP practitioners and documentary and field linguists. Meanwhile, increasingly more focus is dedicated on NLP research and bringing modern technologies to endangered languages. For example, mobile applications have been developed for data collection (Bird et al., 2014; Gauthier et al., 2016) and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Austral"
2020.coling-tutorials.7,W19-6000,0,0.0844101,"language description. The extreme pace of language loss and the urgent needs for language revitalization, however, require that we utilize documentations and go beyond language description: enter 21st century NLP. Himmelmann’s 20-year-old radical vision (Himmelmann, 1998) for a data-centric approach to language documentation (which sparked the creation of modern documentary linguistics) has slowly begun to materialize (McDonnell et al., 2018). For example, the Workshops on the Use of Computational Methods in the Study of Endangered Languages (Comput-EL) (Good et al., 2014; Arppe et al., 2017; Arppe et al., 2019) have provided a small forum for the much-needed discussion between NLP practitioners and documentary and field linguists. Meanwhile, increasingly more focus is dedicated on NLP research and bringing modern technologies to endangered languages. For example, mobile applications have been developed for data collection (Bird et al., 2014; Gauthier et al., 2016) and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 20"
2020.coling-tutorials.7,D17-1011,0,0.0128713,"al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasopoulos, 2019). Other surveys focus on the state of language technologies within specific geographic areas, such as co-proposer Cox’s overview"
2020.coling-tutorials.7,W14-2201,0,0.0305571,"ked the creation of modern documentary linguistics) has slowly begun to materialize (McDonnell et al., 2018). For example, the Workshops on the Use of Computational Methods in the Study of Endangered Languages (Comput-EL) (Good et al., 2014; Arppe et al., 2017; Arppe et al., 2019) have provided a small forum for the much-needed discussion between NLP practitioners and documentary and field linguists. Meanwhile, increasingly more focus is dedicated on NLP research and bringing modern technologies to endangered languages. For example, mobile applications have been developed for data collection (Bird et al., 2014; Gauthier et al., 2016) and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection ("
2020.coling-tutorials.7,J09-3007,0,0.0190415,"edu gneubig@cs.cmu.edu christopher.cox@carleton.ca hilaria.cruz@louisville.edu 1 Description Computational Linguistics and Natural Language Processing (NLP) have taken immense strides, spearheaded by neural methods and large data collections. The result is ubiquitous language technology and vast amounts of research on new tasks and products. However, the vast majority of the world’s languages have been mostly ignored, including the most vulnerable among them: endangered languages. The lack of communication between the NLP community and the documentary linguistics community is partly to blame (Bird, 2009). Even though field and documentary linguists produce resources and use NLP methods, this is done in isolation, as computational methods are seen as a means towards the final goal, which typically is language description. The extreme pace of language loss and the urgent needs for language revitalization, however, require that we utilize documentations and go beyond language description: enter 21st century NLP. Himmelmann’s 20-year-old radical vision (Himmelmann, 1998) for a data-centric approach to language documentation (which sparked the creation of modern documentary linguistics) has slowly"
2020.coling-tutorials.7,D19-1520,1,0.747607,"ng in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those"
2020.coling-tutorials.7,W17-0604,0,0.0244958,"automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very d"
2020.coling-tutorials.7,N18-1005,0,0.0209918,"translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasop"
2020.coling-tutorials.7,C18-1222,1,0.82584,"). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasopoulos, 2019). Other surveys focus on the state of language technologies within specific geographic areas, such as co-proposer Cox’s overview of Canadian languages (Littell et al., 2018) or the one by Mager et al. (2018), focusing on indigenous American languages. The goal of our tutorial will be two-fold. On one hand, we will aim to acquaint the audience with the needs of the documentary linguistics community, and cover the already existing computational research 39 Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts, pages 39–45 Barcelona, Spain (Online), December 12th, 2020. in the field. On the other hand, we will discuss the capabilities and limitations of current computational approaches, so that the participants will know w"
2020.coling-tutorials.7,C18-1006,0,0.0173374,"the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasopoulos, 2019). Other surveys focus on the state of language technologies within specific geographic areas, such as co-proposer Cox’s overview of Canadian languages (Littell et al., 2018) or the one by Mager et al. (2018), focusing on indigenous American languages. The goal of our tutorial will be two-fold. On one hand, we will aim to acquaint the audience with the needs of the documentary linguistics community, and cover the already existing computational research 39 Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts, pages 39–45 Barcelona, Spain (Online), December 12th, 2020. in the field. On the other hand, we will discuss the capabilities and limitations of current computational approaches, so that the participants will know when and how to apply NLP methods,"
2020.coling-tutorials.7,D17-1268,1,0.754008,"transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasopoulos, 2019). Other surveys focus on the state of language technologies within specific geographic areas, such as co-proposer Cox’s overview of Canadian languages (L"
2020.coling-tutorials.7,W19-4226,0,0.0131412,"; Gauthier et al., 2016) and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization e"
2020.coling-tutorials.7,W19-6012,0,0.0206734,"et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dis"
2020.eamt-1.68,W16-3609,0,0.0544569,"Missing"
2020.emnlp-demos.21,D18-1316,0,0.0487414,"Missing"
2020.emnlp-demos.21,W19-4406,0,0.0293015,"Missing"
2020.emnlp-demos.21,W12-2012,0,0.0393962,"Missing"
2020.emnlp-demos.21,I11-1017,0,0.0714986,"Missing"
2020.emnlp-demos.21,E17-2037,0,0.042863,"Missing"
2020.emnlp-demos.21,N18-1202,0,0.243424,"lable sources. 158 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 158–164 c November 16-20, 2020. 2020 Association for Computational Linguistics • SC - LSTM (Sakaguchi et al., 2016): It corrects misspelt words using semi-character representations, fed through a bi-LSTM network. The semi-character representations are a concatenation of one-hot embeddings for the (i) first, (ii) last, and (iii) bag of internal characters. Further, we investigate effective ways to incorporate contextual information: we experiment with contextual representations from pretrained models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) and compare their efficacies with existing neural architectural choices (§ 5.1). Lastly, several recent studies have shown that many state-of-the-art neural models developed for a variety of Natural Language Processing (NLP) tasks easily break in the presence of natural or synthetic spelling errors (Belinkov and Bisk, 2017; Ebrahimi et al., 2017; Pruthi et al., 2019). We determine the usefulness of our toolkit as a countermeasure against character-level adversarial attacks (§ 5.2). We find that our models are better defenses to adversarial attacks than previousl"
2020.emnlp-demos.21,N19-1326,0,0.0202438,"lts. We evaluate models for accuracy (percentage of correct words among all words) and word correction rate (percentage of misspelt tokens corrected). We use AllenNLP6 and Huggingface7 libraries to use ELMo and BERT respectively. All neural models in our toolkit are implemented using the Pytorch library (Paszke et al., 2017), and are compatible to run on both CPU and GPU environments. Performance of different models are presented in Table 1. 3 sampled from all the misspellings associated with that word in the lookup table. Words not present in the lookup table are left as is. P ROB: Recently, Piktus et al. (2019) released a corpus of 20M correct-misspelt word pairs, generated from logs of a search engine.9 We use this corpus to construct a character-level confusion dictionary where the keys are hcharacter, contexti pairs and the values are a list of potential character replacements with their frequencies. This dictionary is subsequently used to sample character-level errors in a given context. We use a context of 3 characters, and backoff to 2, 1, and 0 characters. Notably, due to the large number of unedited characters in the corpus, the most probable replacement will often be the same as the source"
2020.emnlp-demos.21,P19-1561,1,0.843853,"and (iii) bag of internal characters. Further, we investigate effective ways to incorporate contextual information: we experiment with contextual representations from pretrained models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) and compare their efficacies with existing neural architectural choices (§ 5.1). Lastly, several recent studies have shown that many state-of-the-art neural models developed for a variety of Natural Language Processing (NLP) tasks easily break in the presence of natural or synthetic spelling errors (Belinkov and Bisk, 2017; Ebrahimi et al., 2017; Pruthi et al., 2019). We determine the usefulness of our toolkit as a countermeasure against character-level adversarial attacks (§ 5.2). We find that our models are better defenses to adversarial attacks than previously proposed spell checkers. We believe that our toolkit would encourage practitioners to incorporate spelling correction systems in other NLP applications. Model A SPELL (Atkinson, 2019) JAM S PELL (Ozinov, 2019) CHAR - CNN - LSTM (Kim et al., 2015) SC - LSTM (Sakaguchi et al., 2016) CHAR - LSTM - LSTM (Li et al., 2018) B ERT (Devlin et al., 2018) SC - LSTM +E LMO (input) +E LMO (output) +B ERT (inp"
2020.emnlp-demos.21,D13-1170,0,0.00492426,"Missing"
2020.emnlp-demos.21,P12-2039,0,0.0702306,"Missing"
2020.emnlp-demos.21,P11-1019,0,0.0691009,"Missing"
2020.emnlp-main.122,S14-2010,0,0.29394,"Missing"
2020.emnlp-main.122,S16-1081,0,0.28624,"Missing"
2020.emnlp-main.122,S13-1004,0,0.0548868,"Missing"
2020.emnlp-main.122,S12-1051,0,0.579735,"Conneau et al., 2017). More recently, 1 Code and data to replicate results available at https: //www.cs.cmu.edu/˜jwieting. deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b;"
2020.emnlp-main.122,D15-1075,0,0.0500476,"ding. The first way is to concatenate the hidden states for the CLS token in the last four layers. The second way is to concatenate the hidden states of all word tokens in the last four layers and mean pool these representations. Both methods result in a 4096 dimension embedding. We also compare to the newly released model, Sentence2 Note that in all experiments using BERT, including Sentence-BERT, the large, uncased version is used. 1584 Bert (Reimers and Gurevych, 2019). This model is similar to Infersent (Conneau et al., 2017) in that it is trained on natural language inference data, SNLI (Bowman et al., 2015). However, instead of using pretrained word embeddings, they fine-tune BERT in a way to induce sentence embeddings.3 Models from the Literature (Trained on Our Data) These models are amenable to being trained in the exact same setting as our own models as they only require parallel text. These include the sentence piece averaging model, SP, from Wieting et al. (2019b), which is among the best of the averaging models (i.e. compared to averaging only words or character n-grams) as well the LSTM model, B I LSTM, from Wieting and Gimpel (2017). These models use a contrastive loss with a margin. Fo"
2020.emnlp-main.122,S17-2001,0,0.0499638,"Missing"
2020.emnlp-main.122,D18-2029,0,0.193693,"for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) potentially comb"
2020.emnlp-main.122,N19-1254,0,0.12069,"ord embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013; Wieting and Gimpel, 2018) or bilingual data (Wieting et al., 2019b; Chidambaram et al., 2019; Yang et al., 2020). The inclusion of latent variables in these models has also been explored (Chen et al., 2019). Intuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or languagespecific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly, the focus of this paper is modelling this intuition explicitly, and to the best of our knowledge, this has not been explored in prior work. Specifically, we propose a deep generati"
2020.emnlp-main.122,W19-4330,0,0.0141189,"ing techniques use word embedding averaging (Wieting et al., 2016b), character n-grams (Wieting et al., 2016a), and subword embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013; Wieting and Gimpel, 2018) or bilingual data (Wieting et al., 2019b; Chidambaram et al., 2019; Yang et al., 2020). The inclusion of latent variables in these models has also been explored (Chen et al., 2019). Intuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or languagespecific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly, the focus of this paper is modelling this intuition explicitly, an"
2020.emnlp-main.122,L18-1269,0,0.120763,"ilarity (STS) shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), where the goal is to accurately predict the degree to which two sentences have the same meaning as measured by human judges. The evaluation metric Results We define symmetric word error rate for sentences s1 and s2 as 12 W ER(s1 , s2 ) + 12 W ER(s2 , s1 ), since word error rate (WER) is an asymmetric measure. 8 STS scores are between 0 and 5. 9 We selected examples for the negation split where one sentence contained not or ’t and the other did not. 10 We obtained values for STS 2012-2016 from prior works using SentEval (Conneau and Kiela, 2018). Note that we include all datasets for the 2013 competition, including SMT, which is not included in SentEval. 1586 Model BERT (CLS) BERT (Mean) Infersent GenSen USE LASER Sentence-BERT SP B I LSTM E NGLISH AE E NGLISH VAE E NGLISH T RANS B ILINGUALT RANS BGT W / O L ANG VARS BGT W / O P RIOR BGT Model LASER B ILINGUALT RANS BGT W / O L ANG VARS BGT W / O P RIOR BGT 2012 33.2 48.8 61.1 60.7 61.4 63.1 66.9 68.4 67.9 60.2 59.5 66.5 67.1 68.3 67.6 68.9 2013 29.6 46.5 51.4 50.8 59.0 47.0 63.2 60.3 56.4 52.7 54.0 60.7 61.0 61.3 59.8 62.2 es-es 79.7 83.4 81.7 84.5 85.7 Semantic Textual Similarity ("
2020.emnlp-main.122,D17-1070,0,0.362755,"experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.1 1 Introduction Learning useful representations of language has been a source of recent success in natural language processing (NLP). Much work has been done on learning representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Kiros et al., 2015; Conneau et al., 2017). More recently, 1 Code and data to replicate results available at https: //www.cs.cmu.edu/˜jwieting. deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)),"
2020.emnlp-main.122,P18-1198,0,0.0503594,"Missing"
2020.emnlp-main.122,C04-1051,0,0.222341,"//www.cs.cmu.edu/˜jwieting. deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data a"
2020.emnlp-main.122,N13-1092,0,0.121991,"Missing"
2020.emnlp-main.122,N18-1202,0,0.0490474,"ins on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.1 1 Introduction Learning useful representations of language has been a source of recent success in natural language processing (NLP). Much work has been done on learning representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Kiros et al., 2015; Conneau et al., 2017). More recently, 1 Code and data to replicate results available at https: //www.cs.cmu.edu/˜jwieting. deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wiet"
2020.emnlp-main.122,N16-1162,0,0.0635097,"Missing"
2020.emnlp-main.122,D18-2012,0,0.0128595,"S, but it includes a prior over the embedding space and therefore a KL loss term. This model differs from BGT since it does not have any language-specific variables. • BGT W / O P RIOR: Follows the same architecture as BGT, but without the priors and KL loss term. 4.2 Experimental Settings The training data for our models is a mixture of OpenSubtitles 20184 en-fr data and en-fr Gigaword5 data. To create our dataset, we combined the complete corpora of each dataset and then randomly selected 1,000,000 sentence pairs to be used for training with 10,000 used for validation. We use sentencepiece (Kudo and Richardson, 2018) with a vocabulary size of 20,000 to segment the sentences, and we chose sentence pairs whose sentences are between 5 and 100 tokens each. In designing the model architectures for the encoders and decoders, we experimented with Transformers and LSTMs. Due to better performance, we use a 5 layer Transformer for each of the encoders and a single layer decoder for each of the decoders. This design decision was empirically motivated as we found using a larger decoder was slower and worsened performance, but conversely, adding more encoder layers improved performance. More discussion of these trade"
2020.emnlp-main.122,D19-1437,1,0.826884,"we expect that in a well-trained model this variable will encode semantic, syntactic, or stylistic information shared across both sentences, while zf r and zen will handle any language-specific peculiarities or specific stylistic decisions that are less central to the sentence meaning and thus do not translate across sentences. In the following section, we further discuss how this is explicitly encouraged by the learning process. Decoder Architecture. Many latent variable models for text use LSTMs (Hochreiter and Schmidhuber, 1997) as their decoders (Yang et al., 2017; Ziegler and Rush, 2019; Ma et al., 2019). However, state-of-the-art models in neural machine translation have seen increased performance and speed using deep Transformer architectures. We also found in our experiments (see Appendix C for details) that Transformers led to increased performance in our setting, so they are used in our main model. We use two decoders in our model, one for modelling p(xf r |zsem , zf r ; θ) and one for modeling p(xen |zsem , zen ; θ) (see right side of Figure 2). Each decoder takes in a language variable and a semantic variable, which are concatenated and used by the decoder for reconstruction. We explor"
2020.emnlp-main.122,D14-1162,0,0.0977629,"Missing"
2020.emnlp-main.122,D19-1410,0,0.306073,"as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) potentially combined with additional tasks li"
2020.emnlp-main.122,P18-2037,0,0.338404,"rchitecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) potentially combined with additional tasks like constituency parsing (Subramanian et al., 2018). Surprisingly, despite ample testing of more powerful architectures, the best performing models for many sentence embedding tasks related to semantic similarity often use simple architectures that are mostly agnostic to the interactions between 1581 Proceedings of the 2020 Conference on Empirical Methods in Natur"
2020.emnlp-main.122,W17-2619,0,0.129934,"ata for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previous"
2020.emnlp-main.122,D16-1157,1,0.849436,"; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) potentially combined with additional tasks like constituency parsing (Subramanian et al.,"
2020.emnlp-main.122,P19-1427,1,0.884171,"2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et"
2020.emnlp-main.122,P17-1190,1,0.843564,"hat it is trained on natural language inference data, SNLI (Bowman et al., 2015). However, instead of using pretrained word embeddings, they fine-tune BERT in a way to induce sentence embeddings.3 Models from the Literature (Trained on Our Data) These models are amenable to being trained in the exact same setting as our own models as they only require parallel text. These include the sentence piece averaging model, SP, from Wieting et al. (2019b), which is among the best of the averaging models (i.e. compared to averaging only words or character n-grams) as well the LSTM model, B I LSTM, from Wieting and Gimpel (2017). These models use a contrastive loss with a margin. Following their settings, we fix the margin to 0.4 and tune the number of batches to pool for selecting negative examples from {40, 60, 80, 100}. For both models, we set the dimension of the embeddings to 1024. For B I LSTM, we train a single layer bidirectional LSTM with hidden states of 512 dimensions. To create the sentence embedding, the forward and backward hidden states are concatenated and mean-pooled. Following Wieting and Gimpel (2017), we shuffle the inputs with probability p, tuning p from {0.3, 0.5}. We also implicitly compare to"
2020.emnlp-main.122,P18-1042,1,0.882807,"mputational Linguistics words. For instance, some of the top performing techniques use word embedding averaging (Wieting et al., 2016b), character n-grams (Wieting et al., 2016a), and subword embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013; Wieting and Gimpel, 2018) or bilingual data (Wieting et al., 2019b; Chidambaram et al., 2019; Yang et al., 2020). The inclusion of latent variables in these models has also been explored (Chen et al., 2019). Intuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or languagespecific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly,"
2020.emnlp-main.122,P19-1453,1,0.895546,"2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et"
2020.emnlp-main.122,2020.acl-demos.12,0,0.0347346,"bedding averaging (Wieting et al., 2016b), character n-grams (Wieting et al., 2016a), and subword embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013; Wieting and Gimpel, 2018) or bilingual data (Wieting et al., 2019b; Chidambaram et al., 2019; Yang et al., 2020). The inclusion of latent variables in these models has also been explored (Chen et al., 2019). Intuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or languagespecific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly, the focus of this paper is modelling this intuition explicitly, and to the best of our"
2020.emnlp-main.422,2020.lrec-1.497,0,0.0292855,"Missing"
2020.emnlp-main.475,D11-1033,0,0.195411,"ns. 3 Methods In our setting, we are given two MT models MF E and MEF pretrained on parallel data DF E , and both source and target monolingual corpora DF and DE . The goal is to select and weight samples from the two monolingual corpora for backtranslation, in order to best improve the performance of the two translation models. 3.1 Data Selection Strategies We first describe a commonly used static selection strategy, and then illustrate our dynamic approach. 3.1.1 The Moore and Lewis (2010) Method A common approach for data selection is the Moore and Lewis (2010) method (and extensions, e.g. Axelrod et al. (2011); Duh et al. (2013); Santamar´ıa and Axelrod (2019)), which computes the language model cross-entropy difference for each sentence s in a monolingual corpus: score(s) = HLMin (s) − HLMgen (s), (1) where HLMin (s) and HLMgen (s) represent the cross-entropy scores of s measured with an indomain and a general-domain language model (LM) respectively. Sentences with the highest scores will be selected for training. Typically, the in-domain language model LMin is trained with 5895 (a) score sent 1 sent 2 Representativeness λ(t) Representativeness + (1-λ(t)) Simplicity Simplicity M Representativeness"
2020.emnlp-main.475,P16-1185,0,0.091295,"Missing"
2020.emnlp-main.475,N19-1423,0,0.0205642,"model LMin with indomain monolingual data and compute the score 1 P|s| t=1 log PLMin (st |s<t ) for each sentence s. |s| TF-IDF Scores (TF-IDF). TF-IDF score is another criterion for data selection (Kirchhoff and Bilmes, 2014). For each sentence s, one can compute its term frequency and inverse document frequency for each word. We can thus obtain the TF-IDF vector and calculate the cosine similarity between the TF-IDF vectors of s and each sentence sin in a small in-domain dataset, and treat the maximum value as its representativeness score. 5896 BERT Representation Similarities (BERT). BERT (Devlin et al., 2019) has proven to be effective for sentence representation learning. Following the conclusion of Pires et al. (2019), we feed each sentence to the multilingual BERT model and average the hidden states for all the input tokens except [CLS] and [SEP] at the eighth layer to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring"
2020.emnlp-main.475,P13-2119,1,0.845058,"tting, we are given two MT models MF E and MEF pretrained on parallel data DF E , and both source and target monolingual corpora DF and DE . The goal is to select and weight samples from the two monolingual corpora for backtranslation, in order to best improve the performance of the two translation models. 3.1 Data Selection Strategies We first describe a commonly used static selection strategy, and then illustrate our dynamic approach. 3.1.1 The Moore and Lewis (2010) Method A common approach for data selection is the Moore and Lewis (2010) method (and extensions, e.g. Axelrod et al. (2011); Duh et al. (2013); Santamar´ıa and Axelrod (2019)), which computes the language model cross-entropy difference for each sentence s in a monolingual corpus: score(s) = HLMin (s) − HLMgen (s), (1) where HLMin (s) and HLMgen (s) represent the cross-entropy scores of s measured with an indomain and a general-domain language model (LM) respectively. Sentences with the highest scores will be selected for training. Typically, the in-domain language model LMin is trained with 5895 (a) score sent 1 sent 2 Representativeness λ(t) Representativeness + (1-λ(t)) Simplicity Simplicity M Representativeness low high Simplicit"
2020.emnlp-main.475,D18-1045,0,0.0456312,"The Moore and Lewis (2010) data selection strategy for domain adaptation constantly selects the same set of sentences which cannot well represent the target domain. Our approach, instead, selects different subsets of sentences at each epoch and we gradually shift from selecting samples from the general-domain distribution to samples from the target distribution. Introduction Back-translation (Sennrich et al., 2016b) is an effective strategy for improving the performance of neural machine translation (NMT) using monolingual data, delivering impressive gains over already competitive NMT models (Edunov et al., 2018). The strategy is simple: given monolingual data in the target language, one can use a translation model in the opposite of the desired translation direction to back-translate the monolingual data, effectively synthesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monolingual) used to train the final translation model. Further improvements can be obtained by iteratively repeating this process (Hoang et al., 2018) in both directions. However, not all monolingual data are"
2020.emnlp-main.475,W19-5401,0,0.023584,"l. (2019), we feed each sentence to the multilingual BERT model and average the hidden states for all the input tokens except [CLS] and [SEP] at the eighth layer to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip tran"
2020.emnlp-main.475,W11-2123,0,0.107021,"Missing"
2020.emnlp-main.475,W18-2703,0,0.427658,"ng impressive gains over already competitive NMT models (Edunov et al., 2018). The strategy is simple: given monolingual data in the target language, one can use a translation model in the opposite of the desired translation direction to back-translate the monolingual data, effectively synthesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monolingual) used to train the final translation model. Further improvements can be obtained by iteratively repeating this process (Hoang et al., 2018) in both directions. However, not all monolingual data are equally important. An envisioned downstream application is very often characterized by a unique data distribution. In such cases of domain shift, back-translating target domain data can be an effective strategy (Hu et al., 2019) for obtaining a better in-domain translation model. One common strategy is to select samples that are both (1) close to the target distribution and (2) dissimilar to the average generaldomain text (Moore and Lewis, 2010). However, as depicted in Figure 1, this method is not ideal because the second objective co"
2020.emnlp-main.475,W19-5409,0,0.0236587,"between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence s0 . The BLEU score between s and s0 is treated as our simplicity metric. Simi"
2020.emnlp-main.475,P19-1286,1,0.709872,"thesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monolingual) used to train the final translation model. Further improvements can be obtained by iteratively repeating this process (Hoang et al., 2018) in both directions. However, not all monolingual data are equally important. An envisioned downstream application is very often characterized by a unique data distribution. In such cases of domain shift, back-translating target domain data can be an effective strategy (Hu et al., 2019) for obtaining a better in-domain translation model. One common strategy is to select samples that are both (1) close to the target distribution and (2) dissimilar to the average generaldomain text (Moore and Lewis, 2010). However, as depicted in Figure 1, this method is not ideal because the second objective could bias towards the selection of sentences far from the center of the target distribution, potentially leading to selecting a non-representative set of sentences. Even if we could select all in-domain monolin5894 Proceedings of the 2020 Conference on Empirical Methods in Natural Langua"
2020.emnlp-main.475,W17-5704,0,0.050808,"main Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence s0 . The BLEU score between s and s0 is treated as our simplicity metric. Similar ideas have been applied to filter sentences of low quality (Imankulova et al., 2017). For both the representativeness and simplicity scores, it should be noted that they are separately normalized to [0, 1], using the equation score(s)−scoremin scoremax −scoremin , where scoremax and scoremin are the maximum and minimum scores. 3.2 Weighting Strategies Next, we illustrate how we perform data weighting on the back-translated data. 3.2.1 Measuring the Current Quality As general translation models could perform poorly on the in-domain data, we need ways to measure the current quality of the back-translated sentences in order to down-weight examples of poor quality. Encoder Repres"
2020.emnlp-main.475,W18-6478,0,0.260041,"rent Quality As general translation models could perform poorly on the in-domain data, we need ways to measure the current quality of the back-translated sentences in order to down-weight examples of poor quality. Encoder Representation Similarities (Enc). We feed the source sentence x and the target sentence y to the encoders of MF E and MEF respectively, and average the hidden states at the final layer to obtain the representations encF E (x) and encEF (y). The cosine similarity between them is treated as the quality metric. Agreement Between Forward and Backward Models (Agree). Inspired by Junczys-Dowmunt (2018), the second approach utilizes the agreement of the two translation models. For each sentence pair (x, y), we compute the conditional probability HF E (y|x) and HEF (x|y), then exponentiate the absolute value between them exp(−(|HF E (y|x)− HEF (x|y)|)). Intuitively, the back-translated sentences are of poor quality if there are huge disagreements between the two models. 3.2.2 Measuring Quality Improvements In domain adaptation, it is natural that at first the in-domain sentences are poorly translated. As training progresses, however, the quality should be improved. We therefore propose a metr"
2020.emnlp-main.475,W19-5406,0,0.0602812,"Missing"
2020.emnlp-main.475,N16-1059,0,0.0311522,"tence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence s0 . The BLEU score"
2020.emnlp-main.475,D14-1014,0,0.106371,"one can view Moore and Lewis (2010) as selecting the most representative and difficult sentences. 3.1.3 Representativeness Metrics We propose three approaches to measure the sentence representativeness. In-Domain Language Model Cross-Entropy (LM-in). As in Axelrod et al. (2011); Duh et al. (2013), we can use HLMin to measure the representativeness of the instances. Concretely, we train a language model LMin with indomain monolingual data and compute the score 1 P|s| t=1 log PLMin (st |s<t ) for each sentence s. |s| TF-IDF Scores (TF-IDF). TF-IDF score is another criterion for data selection (Kirchhoff and Bilmes, 2014). For each sentence s, one can compute its term frequency and inverse document frequency for each word. We can thus obtain the TF-IDF vector and calculate the cosine similarity between the TF-IDF vectors of s and each sentence sin in a small in-domain dataset, and treat the maximum value as its representativeness score. 5896 BERT Representation Similarities (BERT). BERT (Devlin et al., 2019) has proven to be effective for sentence representation learning. Following the conclusion of Pires et al. (2019), we feed each sentence to the multilingual BERT model and average the hidden states for all"
2020.emnlp-main.475,P10-2041,0,0.410281,"used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines. 1 1 Moore-Lewis Target Domain (Monolingual) Ours next epoch next epoch Figure 1: The Moore and Lewis (2010) data selection strategy for domain adaptation constantly selects the same set of sentences which cannot well represent the target domain. Our approach, instead, selects different subsets of sentences at each epoch and we gradually shift from selecting samples from the general-domain distribution to samples from the target distribution. Introduction Back-translation (Sennrich et al., 2016b) is an effective strategy for improving the performance of neural machine translation (NMT) using monolingual data, delivering impressive gains over already competitive NMT models (Edunov et al., 2018). The"
2020.emnlp-main.475,N19-4007,1,0.801237,"Missing"
2020.emnlp-main.475,W18-2710,0,0.0189248,"translation baseline by up to 0.3 BLEU points. Data weighting strategies do not always help, probably because in high-resource settings the back-translated data is already of high quality. In the best case scenario, our method outperforms iterative back-translation by 0.6 BLEU points. 6 Related Work Back-translation (Sennrich et al., 2016a) has proven to be effective and several extensions of it have been proposed (He et al., 2016; Cheng 5901 et al., 2016; Zhang and Zong, 2016; Xia et al., 2019), among which iterative back-translation methods (Cotterell and Kreutzer, 2018; Hoang et al., 2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt (2018) propose to utilize the agreement between forward and backward translation models and Wang et al. (2019a) pro"
2020.emnlp-main.475,P02-1040,0,0.108023,"iteratively for both directions, with λ increased at each training epoch. 5897 Method WMT LAW MED de-en en-de de-en en-de Baseline Base 31.25 24.44 Back 35.90 26.33 Ite-Back 37.69 27.81 Zhang et al. (2019) 37.70 27.87 Best Selection TF-IDF 38.26* 28.35* Best Curriculum TF-IDF + R-BLEU 39.11* 28.93* Best Weighting Enc 38.20* 28.15* Enc-Imp 38.13* 27.97 Best Curriculum + Best Weighting Curri+Enc 38.87 29.04 Curri+Enc-Imp 38.75 28.89 34.43 42.42 44.08 44.25 26.59 33.98 35.65 36.01 44.26 35.82 44.91* 36.19* 44.28* 44.46* 35.52 35.77 45.46* 45.46* 36.34 36.45* Table 1: Translation accuracy (BLEU (Papineni et al., 2002)) in the domain adaptation setting. The first and second rows list source and target domains respectively. The third row lists the translation directions. We report the best-performing models of only using selection strategies (“Best Selection”), only using curriculum strategies (“Best Curriculum”), only using weighting strategies (“Best Weighting” ) and using both the best curriculum and weighting strategies (“Best Weighting + Best Weighting” ). “Enc-Imp” indicates both the encoder representation similarities and the quality improvement metrics are used for weighting. The highest scores are i"
2020.emnlp-main.475,P19-1493,0,0.0246279,"s. |s| TF-IDF Scores (TF-IDF). TF-IDF score is another criterion for data selection (Kirchhoff and Bilmes, 2014). For each sentence s, one can compute its term frequency and inverse document frequency for each word. We can thus obtain the TF-IDF vector and calculate the cosine similarity between the TF-IDF vectors of s and each sentence sin in a small in-domain dataset, and treat the maximum value as its representativeness score. 5896 BERT Representation Similarities (BERT). BERT (Devlin et al., 2019) has proven to be effective for sentence representation learning. Following the conclusion of Pires et al. (2019), we feed each sentence to the multilingual BERT model and average the hidden states for all the input tokens except [CLS] and [SEP] at the eighth layer to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca e"
2020.emnlp-main.475,N19-1119,1,0.835906,"nsure the quality of the back-translated data. As the training progresses, the model will become better at translating in-domain sentences, and we will then shift to choosing more representative examples. Formally, at each epoch t, we rank the corpus according to score(s) = λ(t)repr(s) + (1 − λ(t))simp(s), (2) where repr(s) and simp(s) denote the representativeness and simplicity of sentence s respectively, which will be dicussed in the following sections. The term λ(t) balances between the two criteria and is a function of the current epoch t. We adopt the square-root growing function for λ (Platanios et al., 2019) and set r 1 − c20 λ(t) = min(1, t + c20 ), (3) T where c0 is the initial value and T denotes the time after which we solely select representative samples. λ increases relatively quickly at first and then its acceleration will be gradually decreased as the training progresses, which is suitable for our task as at first the sentences are relatively simple and thus we will not need much time on those sentences. Connections to Moore and Lewis (2010). Our proposed criteria generalize Moore and Lewis (2010). The first term of Equation 1, namely HLMin (s), measures the representativeness of data bec"
2020.emnlp-main.475,W15-3041,0,0.0216259,"r to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence"
2020.emnlp-main.475,P15-4020,0,0.0293466,"P] at the eighth layer to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reco"
2020.emnlp-main.475,tiedemann-2012-parallel,0,0.121885,"larities and the quality improvement metrics are used for weighting. The highest scores are in bold and ∗ indicates statistical significance compared with the best baseline (p < 0.05). 4 Experiments on Domain Adaptation We first conduct experiments in the domain adaptation setting, where we adapt models from a general domain to a specific domain. 4.1 Setup Datasets. We first train the translation models with (general-domain) WMT-14 German-English dataset, consisting of about 4.5M training sentences, then perform iterative back-translation with (in-domain) law or medical OPUS monolingual data (Tiedemann, 2012). We de-duplicate the law and medical parallel training data, divide them into two halves and obtain 250K and 200K comparable yet non-parallel sentences respectively in both languages to obtain the monolingual corpora. The development and test sets contain 2K sentences in each domain. Byte-pair encoding (Sennrich et al., 2016b) is applied with 32K merge operations. The general-domain and in-domain language models Method Baseline Ite-Back Selection BERT LM-diff LM-in TF-IDF Weighting Enc Enc-Imp Agree Agree-Imp Curriculum LM-in+ LM-gen TF-IDF + LM-gen TF-IDF + R-BLEU WMT LAW MED de-en en-de de-"
2020.emnlp-main.475,P14-1067,1,0.881884,"Missing"
2020.emnlp-main.475,D19-1073,0,0.0414324,"2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt (2018) propose to utilize the agreement between forward and backward translation models and Wang et al. (2019a) propose uncertainty-based confidence estimation to improve back-translation. Wang et al. (2019b) compose dynamic domain-data selection with dynamic clean-data selection. Our methods generalize previous data selection strategies and our primary focus is to improve iterative back-translation, but our work could be extended to also include training-time dynamic data selection approaches such as the technique of Wang et al. (2020). 7 Conclusion In this paper, we provide a novel insight into a widely-used data selection method (Moore and Lewis, 2010) and generalize it to a curriculum strategy fo"
2020.emnlp-main.475,P19-1123,0,0.0609925,"2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt (2018) propose to utilize the agreement between forward and backward translation models and Wang et al. (2019a) propose uncertainty-based confidence estimation to improve back-translation. Wang et al. (2019b) compose dynamic domain-data selection with dynamic clean-data selection. Our methods generalize previous data selection strategies and our primary focus is to improve iterative back-translation, but our work could be extended to also include training-time dynamic data selection approaches such as the technique of Wang et al. (2020). 7 Conclusion In this paper, we provide a novel insight into a widely-used data selection method (Moore and Lewis, 2010) and generalize it to a curriculum strategy fo"
2020.emnlp-main.475,2020.acl-main.754,1,0.804561,"to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt (2018) propose to utilize the agreement between forward and backward translation models and Wang et al. (2019a) propose uncertainty-based confidence estimation to improve back-translation. Wang et al. (2019b) compose dynamic domain-data selection with dynamic clean-data selection. Our methods generalize previous data selection strategies and our primary focus is to improve iterative back-translation, but our work could be extended to also include training-time dynamic data selection approaches such as the technique of Wang et al. (2020). 7 Conclusion In this paper, we provide a novel insight into a widely-used data selection method (Moore and Lewis, 2010) and generalize it to a curriculum strategy for iterative back-translation. We also propose data weighting methods to down-weight examples of poor quality. Extensive experiments are performed to evaluate the performance of our methods; analyses reveal the selected samples can represent the target domain well and our weighting strategies benefit noisy settings the most. Acknowledgements The authors are grateful to the anonymous reviewers for their constructive comments, and t"
2020.emnlp-main.475,D17-1147,0,0.0580682,"Missing"
2020.emnlp-main.475,P16-1009,0,0.666104,"xperimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines. 1 1 Moore-Lewis Target Domain (Monolingual) Ours next epoch next epoch Figure 1: The Moore and Lewis (2010) data selection strategy for domain adaptation constantly selects the same set of sentences which cannot well represent the target domain. Our approach, instead, selects different subsets of sentences at each epoch and we gradually shift from selecting samples from the general-domain distribution to samples from the target distribution. Introduction Back-translation (Sennrich et al., 2016b) is an effective strategy for improving the performance of neural machine translation (NMT) using monolingual data, delivering impressive gains over already competitive NMT models (Edunov et al., 2018). The strategy is simple: given monolingual data in the target language, one can use a translation model in the opposite of the desired translation direction to back-translate the monolingual data, effectively synthesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monoli"
2020.emnlp-main.475,P19-1579,1,0.796958,"BLEU points. High-Resource Settings. In high-resource settings, our curriculum strategies improve the iterative back-translation baseline by up to 0.3 BLEU points. Data weighting strategies do not always help, probably because in high-resource settings the back-translated data is already of high quality. In the best case scenario, our method outperforms iterative back-translation by 0.6 BLEU points. 6 Related Work Back-translation (Sennrich et al., 2016a) has proven to be effective and several extensions of it have been proposed (He et al., 2016; Cheng 5901 et al., 2016; Zhang and Zong, 2016; Xia et al., 2019), among which iterative back-translation methods (Cotterell and Kreutzer, 2018; Hoang et al., 2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt"
2020.emnlp-main.475,P16-1162,0,0.839602,"xperimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines. 1 1 Moore-Lewis Target Domain (Monolingual) Ours next epoch next epoch Figure 1: The Moore and Lewis (2010) data selection strategy for domain adaptation constantly selects the same set of sentences which cannot well represent the target domain. Our approach, instead, selects different subsets of sentences at each epoch and we gradually shift from selecting samples from the general-domain distribution to samples from the target distribution. Introduction Back-translation (Sennrich et al., 2016b) is an effective strategy for improving the performance of neural machine translation (NMT) using monolingual data, delivering impressive gains over already competitive NMT models (Edunov et al., 2018). The strategy is simple: given monolingual data in the target language, one can use a translation model in the opposite of the desired translation direction to back-translate the monolingual data, effectively synthesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monoli"
2020.emnlp-main.475,D16-1160,0,0.0406354,"ck-translation by 1.8 BLEU points. High-Resource Settings. In high-resource settings, our curriculum strategies improve the iterative back-translation baseline by up to 0.3 BLEU points. Data weighting strategies do not always help, probably because in high-resource settings the back-translated data is already of high quality. In the best case scenario, our method outperforms iterative back-translation by 0.6 BLEU points. 6 Related Work Back-translation (Sennrich et al., 2016a) has proven to be effective and several extensions of it have been proposed (He et al., 2016; Cheng 5901 et al., 2016; Zhang and Zong, 2016; Xia et al., 2019), among which iterative back-translation methods (Cotterell and Kreutzer, 2018; Hoang et al., 2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentenc"
2020.emnlp-main.475,N19-1189,0,0.206999,"ack-Translation. The iterative backtranslation method is rather competitive, as it improves over the unadapted baseline by 9.6 BLEU and simple back-translation by 1.8 BLEU points. Selection Strategies. We can see from the table that the best-performing selection strategies, namely selecting sentences with high TF-IDF scores, is generally effective and can improve the baseline by about 0.5 BLEU points. Curriculum and Weighting Strategies. Both our curriculum and weighting strategies outperform the unadapted and the iterative back-translation models, as well as the curriculum method proposed in Zhang et al. (2019), with our curriculum learning method achieving better performance and improving the strong iterative back-translation baseline by 1.1 BLEU points. Combining curriculum and weighting methods can further improve the performance by up to 0.5 BLEU points, demonstrating the two strategies are complementary to each other. 4.3 law de-en law en-de medical de-en medical en-de 32.5 Choices of Metrics We examine different choices of representativeness and simplicity metrics. The performance of different models is listed in Table 2. Representativeness Metrics. All data selection strategies outperform the"
2020.emnlp-main.475,2020.wmt-1.63,0,0.0770528,"(D0F , DE ). Last, we concatenate back-translated data (D0F , DE ) with the original parallel corpus DF E to train a source-to-target model MF E . The success of back-translation has motivated reAlgorithm 1 Iterative Back-Translation Input: Monolingual corpora DF and DE Output: Translation models MF E and MEF while MF E and MEF have not converged do for all batches (BF , BE ) in (DF , DE ) do Translate BF into B0E using MF E Translate BE into B0F using MEF Train MF E with (B0F , BE ) Train MEF with (B0E , BF ) end for end while searchers to investigate and extend the method (He et al., 2016; Zheng et al., 2020). Hoang et al. (2018) propose to use iterative back-translation and achieve improvements over previous state-of-theart models. As shown in Algorithm 1, at each training step, a batch of monolingual sentences is sampled from one language and back-translated to the other language. The back-translated data is utilized to train the model in the other direction. The process is repeated in both directions. 3 Methods In our setting, we are given two MT models MF E and MEF pretrained on parallel data DF E , and both source and target monolingual corpora DF and DE . The goal is to select and weight sam"
2020.emnlp-main.475,W19-5411,0,0.0245378,"e cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence s0 . The BLEU score between s and s0 is treated as our simp"
2020.emnlp-main.478,I17-2007,0,0.0530644,"Missing"
2020.emnlp-main.478,C18-1214,1,0.889388,"Missing"
2020.emnlp-main.478,D19-1091,1,0.881342,"Missing"
2020.emnlp-main.478,P13-1021,0,0.0252568,"of three pages each. For the Yakkha documents, we divide at the paragraph-level, due to the small size of the dataset. We have 33 paragraphs across the three books in our dataset, resulting in 11 segments that contain three paragraphs each. The multi-source results for Yakkha reported in Table 2 use the English translations. Results with Nepali are similar and are included in Appendix A. Metrics We use two metrics for evaluating our systems: character error rate (CER) and word error rate (WER). Both metrics are based on edit distance and are standard for evaluating OCR and OCR postcorrection (Berg-Kirkpatrick et al., 2013; Schulz and Kuhn, 2017). CER is the edit distance between the predicted and the gold transcriptions of the document, divided by the total number of characters in the gold transcription. WER is similar but is calculated at the word level. Methods In our experiments, we compare the performance of our proposed method with the first pass OCR and with two systems from recent work in OCR post-correction. All the post-correction methods have two variants – the single-source model with only the endangered language encoder and the multi-source model that additionally uses the high-resource translation"
2020.emnlp-main.478,2020.lrec-1.356,0,0.0424316,"no and Igbo, and Krishna et al. (2018) show improvements on Romanized Sanksrit OCR by adding a copy mechanism to a neural sequence-to-sequence model. Multi-source encoder-decoder models have been used for various tasks including machine translation (Zoph and Knight, 2016; Libovick´y and Helcl, 2017) and morphological inflection (Kann et al., 2017; Anastasopoulos and Neubig, 2019). Perhaps most relevant to our work is the multi-source model presented by Anastasopoulos and Chiang (2018), which uses high-resource translations to improve speech transcription of lower-resourced languages. Finally, Bustamante et al. (2020) construct corpora for four endangered languages from textbased PDFs using rule-based heuristics. Data creation from such unstructured text files is an important research direction, complementing our method of extracting data from scanned images. 8 Conclusion This work presents a first step towards extracting textual data in endangered languages from scanned images of paper books. We create a benchmark dataset with transcribed images in three endangered languages: Ainu, Griko, and Yakkha. We propose an OCR post-correction method that facilitates learning from small amounts of data, which resul"
2020.emnlp-main.478,P18-1220,0,0.224026,"tual data and transcribed images needed to train state-of-the-art OCR models from scratch are unavailable in the endangered language setting. Instead, we focus on post-correcting the output of an off-the-shelf OCR tool that can handle a variety of scripts. We show that targeted methods for post-correction can significantly improve performance on endangered languages. Although OCR post-correction is relatively wellstudied, most existing methods rely on considerable resources in the target language, including a substantial amount of textual data to train a language model (Schnober et al., 2016; Dong and Smith, 2018; Rigaud et al., 2019) or to create synthetic data (Krishna et al., 2018). While readily available for high-resource languages, these resources are severely limited in endangered languages, preventing the direct application of existing post-correction methods in our setting. As an alternative, we present a method that compounds on previous models for OCR postcorrection, making three improvements tailored to the data-scarce setting. First, we use a multisource model to incorporate information from the high-resource translations that commonly appear in endangered language books. These translatio"
2020.emnlp-main.478,2020.acl-main.560,0,0.0369974,"Missing"
2020.emnlp-main.478,E17-1049,0,0.0353087,"Missing"
2020.emnlp-main.478,H05-1109,0,0.0680367,"7 Related Work Post-correction for OCR is well-studied for highresource languages. Early approaches include lexical methods and weighted finite-state methods (see Schulz and Kuhn (2017) for an overview). Recent work has primarily focused on using neural sequence-to-sequence models. H¨am¨al¨ainen and Hengchen (2019) use a BiLSTM encoder-decoder with attention for historical English post-correction. Similar to our base model, Dong and Smith (2018) use a multi-source model to combine the first pass OCR from duplicate documents in English. There has been little work on lower-resourced languages. Kolak and Resnik (2005) present a probabilistic edit distance based post-correction model applied to Cebuano and Igbo, and Krishna et al. (2018) show improvements on Romanized Sanksrit OCR by adding a copy mechanism to a neural sequence-to-sequence model. Multi-source encoder-decoder models have been used for various tasks including machine translation (Zoph and Knight, 2016; Libovick´y and Helcl, 2017) and morphological inflection (Kann et al., 2017; Anastasopoulos and Neubig, 2019). Perhaps most relevant to our work is the multi-source model presented by Anastasopoulos and Chiang (2018), which uses high-resource t"
2020.emnlp-main.478,K18-1034,0,0.325795,"dels from scratch are unavailable in the endangered language setting. Instead, we focus on post-correcting the output of an off-the-shelf OCR tool that can handle a variety of scripts. We show that targeted methods for post-correction can significantly improve performance on endangered languages. Although OCR post-correction is relatively wellstudied, most existing methods rely on considerable resources in the target language, including a substantial amount of textual data to train a language model (Schnober et al., 2016; Dong and Smith, 2018; Rigaud et al., 2019) or to create synthetic data (Krishna et al., 2018). While readily available for high-resource languages, these resources are severely limited in endangered languages, preventing the direct application of existing post-correction methods in our setting. As an alternative, we present a method that compounds on previous models for OCR postcorrection, making three improvements tailored to the data-scarce setting. First, we use a multisource model to incorporate information from the high-resource translations that commonly appear in endangered language books. These translations are usually in the lingua franca of the region (e.g., Figure 1 (a,b,c)"
2020.emnlp-main.478,2013.mtsummit-papers.10,0,0.0301391,"rst pass OCR and the gold transcription. The operations of each type (insertion, deletion, and replacement) are counted for each character and divided by the number of times that character appears in the first pass OCR. This forms a probability of how often the operation should be applied to that specific character. We use these probabilities to form rules, such as p(replace d with d.) = 0.4 for Griko and p(replace ? with P) = 0.7 for Yakkha. These rules are applied to remove errors from, or “denoise”, the first pass OCR transcription. • Sentence alignment We use Yet Another Sentence Aligner (Lamraoui and Langlais, 2013) for unsupervised alignment of the endangered language and translation on the unannotated pages. Given the aligned first pass OCR for the endangered language text and the translation along with the pseudo-target text, x, t and y ˆ respectively, the pretraining steps, in order, are: • Pretraining the encoders We pretrain both the forward and backward LSTMs of each encoder with a character-level language model objective: the endangered language encoder on x and the translation encoder on t. x x ek,i = v tanh (W1 sk−1 + W2 hi + wg gk,i ) gk is also used to penalize attending to the same locations"
2020.emnlp-main.478,P17-2031,0,0.0297408,"Missing"
2020.emnlp-main.478,D16-1096,0,0.0119481,"rather than generate a character at each time step might lead to better performance (Gu et al., 2016). We incorporate the copy mechanism proposed in See et al. (2017). The mechanism computes a “generation probability” at each time step k, which is used to choose between generating a character (Equation 3) or copying a character from the source x text by sampling from the attention distribution αk . Coverage Given the monotonicity of the postcorrection task, the model should not attend to the same character repeatedly. However, repetition is a common problem for neural encoder-decoder models (Mi et al., 2016; Tu et al., 2016). To account for this problem, we adapt the coverage mechanism from See et al. (2017), which keeps track of the attention distribution over past time steps in a coverage vector. For time step k, the coverage vector k−1 x would be gk = ∑k′ =0 αk′ . gk is used as an extra input to the attention mechanism, ensuring that future attention decisions take the weights from previous time steps into account. Equation 1, with learnable parameter wg , becomes: x x x the edit distance operations between the first pass OCR and the gold transcription. The operations of each type (insertion,"
2020.emnlp-main.478,P16-1008,0,0.0123394,"rate a character at each time step might lead to better performance (Gu et al., 2016). We incorporate the copy mechanism proposed in See et al. (2017). The mechanism computes a “generation probability” at each time step k, which is used to choose between generating a character (Equation 3) or copying a character from the source x text by sampling from the attention distribution αk . Coverage Given the monotonicity of the postcorrection task, the model should not attend to the same character repeatedly. However, repetition is a common problem for neural encoder-decoder models (Mi et al., 2016; Tu et al., 2016). To account for this problem, we adapt the coverage mechanism from See et al. (2017), which keeps track of the attention distribution over past time steps in a coverage vector. For time step k, the coverage vector k−1 x would be gk = ∑k′ =0 αk′ . gk is used as an extra input to the attention mechanism, ensuring that future attention decisions take the weights from previous time steps into account. Equation 1, with learnable parameter wg , becomes: x x x the edit distance operations between the first pass OCR and the gold transcription. The operations of each type (insertion, deletion, and rep"
2020.emnlp-main.478,N16-1004,0,0.0651717,"ari script languages (such as Hindi). 5 OCR Post-Correction Model In this section, we describe our proposed OCR post-correction model. The base architecture of the model is a multi-source sequence-to-sequence 5934 P (y1 . . . yK ) where sk−1 is the decoder state of the previous time x x x step and v , W1 and W2 are trainable parameters. x The encoder hidden states h are weighted by the x attention distribution αk to produce the context x vector ck . We follow a similar procedure for the t second encoder to produce ck . We concatenate the context vectors to combine attention from both sources (Zoph and Knight, 2016): softmax s1 . . . sK decoder c1 . . . cK attention x h1 attention x . . . hN t h1 t . . . hM encoder encoder x1 . . . xN x OCR ck is used by the decoder LSTM to compute the next hidden state sk and subsequently, the probability distribution for predicting the next character yk of the target sequence y: OCR Japanese Ainu Figure 3: The proposed multi-source architecture with the encoder for an endangered language segment (left) and an encoder for the translated segment (right). The input to the encoders is the first pass OCR over the scanned images of each segment. For example, the OCR on the s"
2020.emnlp-main.478,C16-1160,0,0.165497,"he large amounts of textual data and transcribed images needed to train state-of-the-art OCR models from scratch are unavailable in the endangered language setting. Instead, we focus on post-correcting the output of an off-the-shelf OCR tool that can handle a variety of scripts. We show that targeted methods for post-correction can significantly improve performance on endangered languages. Although OCR post-correction is relatively wellstudied, most existing methods rely on considerable resources in the target language, including a substantial amount of textual data to train a language model (Schnober et al., 2016; Dong and Smith, 2018; Rigaud et al., 2019) or to create synthetic data (Krishna et al., 2018). While readily available for high-resource languages, these resources are severely limited in endangered languages, preventing the direct application of existing post-correction methods in our setting. As an alternative, we present a method that compounds on previous models for OCR postcorrection, making three improvements tailored to the data-scarce setting. First, we use a multisource model to incorporate information from the high-resource translations that commonly appear in endangered language b"
2020.emnlp-main.478,D17-1288,0,0.160209,"kkha documents, we divide at the paragraph-level, due to the small size of the dataset. We have 33 paragraphs across the three books in our dataset, resulting in 11 segments that contain three paragraphs each. The multi-source results for Yakkha reported in Table 2 use the English translations. Results with Nepali are similar and are included in Appendix A. Metrics We use two metrics for evaluating our systems: character error rate (CER) and word error rate (WER). Both metrics are based on edit distance and are standard for evaluating OCR and OCR postcorrection (Berg-Kirkpatrick et al., 2013; Schulz and Kuhn, 2017). CER is the edit distance between the predicted and the gold transcriptions of the document, divided by the total number of characters in the gold transcription. WER is similar but is calculated at the word level. Methods In our experiments, we compare the performance of our proposed method with the first pass OCR and with two systems from recent work in OCR post-correction. All the post-correction methods have two variants – the single-source model with only the endangered language encoder and the multi-source model that additionally uses the high-resource translation encoder. • F P -O CR: T"
2020.emnlp-main.478,P17-1099,0,0.0233985,"ower values. The diagonal loss summed over all time steps for a training instance, where N is the length of x, is: x αk = softmax (ek ) x x t ck = [ck ; ck ] t1 . . . tM Ldiag x ck = [Σi αk,i hi ] 5935 N ⎛k−j x x ⎞ = ∑ ⎜ ∑ αk,i + ∑ αk,i ⎟ ⎠ k ⎝ i=1 i=k+j Copy mechanism Table 1 indicates that the first pass OCR predicts a majority of the characters accurately. In this scenario, enabling the model to directly copy characters from the first pass OCR rather than generate a character at each time step might lead to better performance (Gu et al., 2016). We incorporate the copy mechanism proposed in See et al. (2017). The mechanism computes a “generation probability” at each time step k, which is used to choose between generating a character (Equation 3) or copying a character from the source x text by sampling from the attention distribution αk . Coverage Given the monotonicity of the postcorrection task, the model should not attend to the same character repeatedly. However, repetition is a common problem for neural encoder-decoder models (Mi et al., 2016; Tu et al., 2016). To account for this problem, we adapt the coverage mechanism from See et al. (2017), which keeps track of the attention distribution"
2020.emnlp-main.479,D19-1091,1,0.855802,"Missing"
2020.emnlp-main.479,2020.acl-main.421,0,0.0204559,"even outperforming competitive question answering systems relying on external resources (Roberts et al., 2020). Petroni et al. (2020) further shows that LMs can generate even more factual knowledge when augmented with retrieved sentences. Our work builds on these works by expanding to multilingual and multi-token evaluation, and also demonstrates the significant challenges posed by this setting. Multilingual Benchmarks Many multilingual benchmarks have been created to evaluate the performance of multilingual systems on different natural language processing tasks, including question answering (Artetxe et al., 2020; Lewis et al., 2019; Clark et al., 2020), natural language understanding (Conneau et al., 2018; Yang et al., 2019a; Zweigenbaum et al., 2018; Artetxe and Schwenk, 2019), syntactic prediction (Nivre et al., 2018; Pan et al., 2017), and comprehensive benchmarks covering multiple tasks (Hu et al., 2020; Liang et al., 2020). We focus on multilingual factual knowledge retrieval from LMs, which to our knowledge has not been covered by any previous work. 8 Conclusion We examine the intersection of multilinguality and the factual knowledge included in LMs by creating a multilingual and multi-token be"
2020.emnlp-main.479,Q19-1038,0,0.0247568,"nerate even more factual knowledge when augmented with retrieved sentences. Our work builds on these works by expanding to multilingual and multi-token evaluation, and also demonstrates the significant challenges posed by this setting. Multilingual Benchmarks Many multilingual benchmarks have been created to evaluate the performance of multilingual systems on different natural language processing tasks, including question answering (Artetxe et al., 2020; Lewis et al., 2019; Clark et al., 2020), natural language understanding (Conneau et al., 2018; Yang et al., 2019a; Zweigenbaum et al., 2018; Artetxe and Schwenk, 2019), syntactic prediction (Nivre et al., 2018; Pan et al., 2017), and comprehensive benchmarks covering multiple tasks (Hu et al., 2020; Liang et al., 2020). We focus on multilingual factual knowledge retrieval from LMs, which to our knowledge has not been covered by any previous work. 8 Conclusion We examine the intersection of multilinguality and the factual knowledge included in LMs by creating a multilingual and multi-token benchmark XFACTR, and performing experiments comparing and contrasting across languages and LMs. The results demonstrate the difficulty of this task, and that knowledge co"
2020.emnlp-main.479,A88-1019,0,0.130845,"prediction #tokens confidence 2012 1 -1.90 Nueva York 2 -0.61 es outputs EE. UU 3 -1.82 Chicago, Estados Unidos 4 -3.58 2012 Bloomberg L.P 5 -3.06 Figure 1: X-FACTR contains 23 languages, for which the data availability varies dramatically. Prompts get instantiated to produce grammatical sentences with different numbers of mask tokens and are used to obtain predictions for [Y]. In this Spanish example, the verb gerund “fundar” to found is rendered as “fundada” to agree in gender and number with the subject “Bloomberg L.P.”. The final prediction is in bold. Introduction Language models (LMs; (Church, 1988; Kneser and Ney, 1995; Bengio et al., 2003)) learn to model the probability distribution of text, and in doing so capture information about various aspects of the syntax or semantics of the language at hand. Recent works have presented intriguing results demonstrating that modern large-scale LMs also capture a significant amount of factual knowledge (Petroni et al., 2019; Jiang et al., 2020; Poerner et al., 2019). This knowledge is generally probed by having the LM fill in the blanks of cloze-style prompts such as ∗: Work done at Carnegie Mellon University. The first two authors contributed e"
2020.emnlp-main.479,2020.tacl-1.30,0,0.0233529,"swering systems relying on external resources (Roberts et al., 2020). Petroni et al. (2020) further shows that LMs can generate even more factual knowledge when augmented with retrieved sentences. Our work builds on these works by expanding to multilingual and multi-token evaluation, and also demonstrates the significant challenges posed by this setting. Multilingual Benchmarks Many multilingual benchmarks have been created to evaluate the performance of multilingual systems on different natural language processing tasks, including question answering (Artetxe et al., 2020; Lewis et al., 2019; Clark et al., 2020), natural language understanding (Conneau et al., 2018; Yang et al., 2019a; Zweigenbaum et al., 2018; Artetxe and Schwenk, 2019), syntactic prediction (Nivre et al., 2018; Pan et al., 2017), and comprehensive benchmarks covering multiple tasks (Hu et al., 2020; Liang et al., 2020). We focus on multilingual factual knowledge retrieval from LMs, which to our knowledge has not been covered by any previous work. 8 Conclusion We examine the intersection of multilinguality and the factual knowledge included in LMs by creating a multilingual and multi-token benchmark XFACTR, and performing experiment"
2020.emnlp-main.479,D18-1269,0,0.0258777,"s et al., 2020). Petroni et al. (2020) further shows that LMs can generate even more factual knowledge when augmented with retrieved sentences. Our work builds on these works by expanding to multilingual and multi-token evaluation, and also demonstrates the significant challenges posed by this setting. Multilingual Benchmarks Many multilingual benchmarks have been created to evaluate the performance of multilingual systems on different natural language processing tasks, including question answering (Artetxe et al., 2020; Lewis et al., 2019; Clark et al., 2020), natural language understanding (Conneau et al., 2018; Yang et al., 2019a; Zweigenbaum et al., 2018; Artetxe and Schwenk, 2019), syntactic prediction (Nivre et al., 2018; Pan et al., 2017), and comprehensive benchmarks covering multiple tasks (Hu et al., 2020; Liang et al., 2020). We focus on multilingual factual knowledge retrieval from LMs, which to our knowledge has not been covered by any previous work. 8 Conclusion We examine the intersection of multilinguality and the factual knowledge included in LMs by creating a multilingual and multi-token benchmark XFACTR, and performing experiments comparing and contrasting across languages and LMs."
2020.emnlp-main.479,N19-1423,0,0.573261,"uage-related criteria (e.g., require the entities to have translations in all languages we considered). As a result, some entities (either subjects or objects) might not have translations in all languages. The number of facts in different languages in our multilingual multi-token X-FACTR benchmark is shown in Tab. 1. Because many modern pre-trained M-LMs almost invariably use some variety of subword tokenization, the number of tokens an entity contains will depend on the tokenization method used in the LM. We report the statistics based on the WordPiece tokenization used in multilingual BERT (Devlin et al., 2019). The tokenization scheme statistics for the other M-LMs are similar. 3.3 Prompts Some languages we include in the benchmark require additional handling of the prompts to account for their grammar or morphology. For example, (some) named entities inflect for case in languages like Greek, Russian, Hebrew, or Marathi. In some languages syntactic subjects and objects need to be in particular cases. Similarly, languages often require that the verb or other parts of the sentence agree with the subject or the object on some morphological features like person, gender, or number. Our prompts provide t"
2020.emnlp-main.479,L18-1544,0,0.0254733,"arts in another language, and further fine-tune the LM on these codeswitched data (§ 6). We perform experiments on three languages (French, Russian, and Greek, codeswitched with English). Results demonstrate that this code-switching-based learning can successfully improve the knowledge retrieval ability with low-resource language prompts. 2 Retrieving Facts from LMs In this paper we follow the protocol of Petroni et al. (2019)’s English-language LAMA benchmark, which targets factual knowledge expressed in the form of subject-relation-object triples from Wikidata1 curated in the T-REx dataset (ElSahar et al., 2018). The cloze-style prompts used therein are manually created and consist of a sequence of tokens, where [X] and [Y] are placeholders for subjects and objects (e.g. “[X] is a [Y] by profession.”). To assess the existence of a certain fact, [X] is replaced with the actual subject (e.g. “Obama is a hmaski by profession.”) and the model predicts the object in the blank yˆi = argmaxyi p(yi |si:i ), where si:i is the sentence with the i-th token masked out. Finally, the predicted fact is compared to the ground truth. In the next section, we extend this setting to more languages and predict multiple t"
2020.emnlp-main.479,D19-1633,0,0.217076,"Decoding As Tab. 1 shows, many facts involve multi-token entities and thus a LM would need to predict these entities in multiple steps. Generating multiple predictions is straightforward for traditional left-toright LMs (Sundermeyer et al., 2015; Radford et al., 2019), where we can autoregressively decode the next token conditioned on previous tokens. However, many pre-trained LMs such as BERT (Devlin et al., 2019) are masked LMs that predict individual words given left and right contexts, and decoding from such masked LMs remains an open problem (Lawrence et al., 2019; Salazar et al., 2020; Ghazvininejad et al., 2019; Wang and Cho, 2019; Cho, 2019). We systematically examined different multi-token decoding algorithms from three orthogonal perspectives: (1) how the initial predictions are produced, (2) how to refine the predictions, and (3) other commonly used components in neural text generation systems. We assume that the following conditional probability distribution is defined by the masked LM for a sentence with n tokens: p(xk |x01 , ..., x0k−1 , hmaskik , x0k+1 , ..., x0n ), (1) where the subscript of hmaski indicates its position, and the surrounding token x0· can either be an actual word x· or hmas"
2020.emnlp-main.479,2020.tacl-1.28,1,0.932269,"s Spanish example, the verb gerund “fundar” to found is rendered as “fundada” to agree in gender and number with the subject “Bloomberg L.P.”. The final prediction is in bold. Introduction Language models (LMs; (Church, 1988; Kneser and Ney, 1995; Bengio et al., 2003)) learn to model the probability distribution of text, and in doing so capture information about various aspects of the syntax or semantics of the language at hand. Recent works have presented intriguing results demonstrating that modern large-scale LMs also capture a significant amount of factual knowledge (Petroni et al., 2019; Jiang et al., 2020; Poerner et al., 2019). This knowledge is generally probed by having the LM fill in the blanks of cloze-style prompts such as ∗: Work done at Carnegie Mellon University. The first two authors contributed equally. “Obama is a _ by profession.”, where these prompts are invariably written in English. However, it goes without saying that there are many languages of the world other than English, and it is quite conceivable that (1) users may want to query this factual knowledge in other languages, and (2) some facts will be written in non-English languages and thus multilingually trained LMs (here"
2020.emnlp-main.479,D19-1001,0,0.013289,"one of them after lowercasing. 4 Multi-token Decoding As Tab. 1 shows, many facts involve multi-token entities and thus a LM would need to predict these entities in multiple steps. Generating multiple predictions is straightforward for traditional left-toright LMs (Sundermeyer et al., 2015; Radford et al., 2019), where we can autoregressively decode the next token conditioned on previous tokens. However, many pre-trained LMs such as BERT (Devlin et al., 2019) are masked LMs that predict individual words given left and right contexts, and decoding from such masked LMs remains an open problem (Lawrence et al., 2019; Salazar et al., 2020; Ghazvininejad et al., 2019; Wang and Cho, 2019; Cho, 2019). We systematically examined different multi-token decoding algorithms from three orthogonal perspectives: (1) how the initial predictions are produced, (2) how to refine the predictions, and (3) other commonly used components in neural text generation systems. We assume that the following conditional probability distribution is defined by the masked LM for a sentence with n tokens: p(xk |x01 , ..., x0k−1 , hmaskik , x0k+1 , ..., x0n ), (1) where the subscript of hmaski indicates its position, and the surrounding"
2020.emnlp-main.479,D19-1250,0,0.282358,"ctions for [Y]. In this Spanish example, the verb gerund “fundar” to found is rendered as “fundada” to agree in gender and number with the subject “Bloomberg L.P.”. The final prediction is in bold. Introduction Language models (LMs; (Church, 1988; Kneser and Ney, 1995; Bengio et al., 2003)) learn to model the probability distribution of text, and in doing so capture information about various aspects of the syntax or semantics of the language at hand. Recent works have presented intriguing results demonstrating that modern large-scale LMs also capture a significant amount of factual knowledge (Petroni et al., 2019; Jiang et al., 2020; Poerner et al., 2019). This knowledge is generally probed by having the LM fill in the blanks of cloze-style prompts such as ∗: Work done at Carnegie Mellon University. The first two authors contributed equally. “Obama is a _ by profession.”, where these prompts are invariably written in English. However, it goes without saying that there are many languages of the world other than English, and it is quite conceivable that (1) users may want to query this factual knowledge in other languages, and (2) some facts will be written in non-English languages and thus multilingual"
2020.emnlp-main.479,2020.acl-main.240,0,0.149649,"rcasing. 4 Multi-token Decoding As Tab. 1 shows, many facts involve multi-token entities and thus a LM would need to predict these entities in multiple steps. Generating multiple predictions is straightforward for traditional left-toright LMs (Sundermeyer et al., 2015; Radford et al., 2019), where we can autoregressively decode the next token conditioned on previous tokens. However, many pre-trained LMs such as BERT (Devlin et al., 2019) are masked LMs that predict individual words given left and right contexts, and decoding from such masked LMs remains an open problem (Lawrence et al., 2019; Salazar et al., 2020; Ghazvininejad et al., 2019; Wang and Cho, 2019; Cho, 2019). We systematically examined different multi-token decoding algorithms from three orthogonal perspectives: (1) how the initial predictions are produced, (2) how to refine the predictions, and (3) other commonly used components in neural text generation systems. We assume that the following conditional probability distribution is defined by the masked LM for a sentence with n tokens: p(xk |x01 , ..., x0k−1 , hmaskik , x0k+1 , ..., x0n ), (1) where the subscript of hmaski indicates its position, and the surrounding token x0· can either"
2020.emnlp-main.479,W19-2304,0,0.0286623,"any facts involve multi-token entities and thus a LM would need to predict these entities in multiple steps. Generating multiple predictions is straightforward for traditional left-toright LMs (Sundermeyer et al., 2015; Radford et al., 2019), where we can autoregressively decode the next token conditioned on previous tokens. However, many pre-trained LMs such as BERT (Devlin et al., 2019) are masked LMs that predict individual words given left and right contexts, and decoding from such masked LMs remains an open problem (Lawrence et al., 2019; Salazar et al., 2020; Ghazvininejad et al., 2019; Wang and Cho, 2019; Cho, 2019). We systematically examined different multi-token decoding algorithms from three orthogonal perspectives: (1) how the initial predictions are produced, (2) how to refine the predictions, and (3) other commonly used components in neural text generation systems. We assume that the following conditional probability distribution is defined by the masked LM for a sentence with n tokens: p(xk |x01 , ..., x0k−1 , hmaskik , x0k+1 , ..., x0n ), (1) where the subscript of hmaski indicates its position, and the surrounding token x0· can either be an actual word x· or hmaski. We aim to handle"
2020.emnlp-main.479,2020.acl-main.536,0,0.0663478,"Missing"
2020.emnlp-main.479,D19-1382,0,0.0880662,"k Obama is a United1 State2 President3 by profession (c) Confidence: Barack Obama is a minister2 of3 cabinet1 by profession Figure 2: Illustration of three initial prediction and refinement methods. Green boxes are mask tokens to be filled, and subscripts indicate the prediction order. 4.1 Initial Prediction and Refinement Given a sentence with multiple mask tokens, e.g., Eq. 2, we can either generate outputs in parallel independently or one at a time conditioned on the previously generated tokens. These methods are similar to the prediction problems that BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019b) perform in their pre-training stages respectively. We define c ∈ Rn as the probability of each prediction, with details varying by prediction methods. After all mask tokens are replaced with the initial predictions, i.e., sˆi:j = x1 , ..., yˆi , ..., yˆj , ..., xn , we can further refine the predictions by iteratively modifying one token at a time until convergence or until the maximum number of iterations is reached. Here we outline the algorithms with high-level descriptions, and provide concrete details in Appendix C. Independent. For independent initial prediction (Fig. 2a), the mask to"
2020.emnlp-main.489,N19-1078,0,0.0730511,"Missing"
2020.emnlp-main.751,W16-2302,0,0.117828,"Missing"
2020.emnlp-main.751,D19-1307,0,0.0261957,"Missing"
2020.emnlp-main.751,P18-1060,0,0.16456,"f automatic summarization systems. Specifically, we conduct four experiments analyzing the correspondence between various metrics and human evaluation. Somewhat surprisingly, we find that many of the previously attested properties of metrics found on the TAC dataset demonstrate different trends on our newly collected CNNDM dataset, as shown in Tab. 1. For example, MoverScore is the best performing metric for evaluating summaries on dataset TAC, but it is significantly worse than ROUGE-2 on our collected CNNDM set. Additionally, many previous works (Novikova et al., 2017; Peyrard et al., 2017; Chaganty et al., 2018) show that metrics have much lower correlations at comparing summaries than systems. For extractive summaries on CNNDM, however, most metrics are better at comparing summaries than systems. Calls for Future Research These observations demonstrate the limitations of our current bestperforming metrics, highlighting (1) the need for future meta-evaluation to (i) be across multiple datasets and (ii) evaluate metrics on different application scenarios, e.g. summary level vs. system level (2) the need for more systematic metaevaluation of summarization metrics that updates with our ever-evolving sys"
2020.emnlp-main.751,P18-1063,0,0.0542504,"018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019b), preSummAbsext (Liu and Lapata, 2019b) BART (Lewis et al., 2019) and Semsim (Yoon et al., 2020) as abstractive systems. In total, we use 14 abstractive system outputs for each document in the CNNDM test set. 2.3 Evaluation Metrics We examine eight metrics that measure the agreement between two texts, in our case, between the system summary and reference summary."
2020.emnlp-main.751,P19-1264,0,0.0359635,"total, we use 14 abstractive system outputs for each document in the CNNDM test set. 2.3 Evaluation Metrics We examine eight metrics that measure the agreement between two texts, in our case, between the system summary and reference summary. BERTScore (BScore) measures soft overlap between contextual BERT embeddings of tokens between the two texts4 (Zhang et al., 2020). MoverScore (MScore) applies a distance measure to contextualized BERT and ELMo word embeddings5 (Zhao et al., 2019). Sentence Mover Similarity (SMS) applies minimum distance matching between text based on sentence embeddings (Clark et al., 2019). Word Mover Similarity (WMS) measures similarity using minimum distance matching between texts which are represented as a bag of word embeddings6 (Kusner et al., 2015). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text’s bigram distributions7 (Lin et al., 2006). ROUGE-1 and ROUGE-2 measure overlap of unigrams and bigrams respectively8 (Lin, 2004). ROUGE-L measures overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics (since the Pyramid method of human evaluations is inherently recall based) except MScore wh"
2020.emnlp-main.751,N06-1059,0,0.13983,"contextual BERT embeddings of tokens between the two texts4 (Zhang et al., 2020). MoverScore (MScore) applies a distance measure to contextualized BERT and ELMo word embeddings5 (Zhao et al., 2019). Sentence Mover Similarity (SMS) applies minimum distance matching between text based on sentence embeddings (Clark et al., 2019). Word Mover Similarity (WMS) measures similarity using minimum distance matching between texts which are represented as a bag of word embeddings6 (Kusner et al., 2015). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text’s bigram distributions7 (Lin et al., 2006). ROUGE-1 and ROUGE-2 measure overlap of unigrams and bigrams respectively8 (Lin, 2004). ROUGE-L measures overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics (since the Pyramid method of human evaluations is inherently recall based) except MScore which has no specific recall variant. 2.4 Correlation Measures Pearson Correlation is a measure of linear correlation between two variables and is popular in metaevaluating metrics at the system level (Lee Rodgers, 1988). We use the implementation given by Virtanen et al. (2020). William’s"
2020.emnlp-main.751,C04-1072,0,0.120342,"summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used to test the degree to which automatic metrics correlate therewith. However, the classic TAC meta-evaluation datasets are now 6-12 years old2 and it is not clear whether conclusions found there will hold with modern systems and summarization tasks. Two earlier works exemplify this disconnect: (1) Peyrard (2019) observed that the human-annotated summaries in the TAC dataset are mostly of lower quality than those produced by modern systems and that various automated evaluation metrics strongly dis"
2020.emnlp-main.751,D19-1387,0,0.0914071,"that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), t"
2020.emnlp-main.751,J13-2002,0,0.630999,"not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive): https://github.com/neulab/REALSumm 1 Introduction In text summarization, manual evaluation, as exemplified by the Pyramid method (Nenkova and Passonneau, 2004), is the gold-standard in evaluation. However, due to time required and relatively high cost of annotation, the great majority of research papers on summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used"
2020.emnlp-main.751,P14-5010,0,0.00281181,"In the end, we obtained nearly 10.5 SCUs on average from each reference summary. System Evaluation During system evaluation the full set of SCUs is presented to crowd workers. Workers are paid similar to Shapira et al. (2019), scaling the rates for fewer SCUs and shorter summary texts. For abstractive systems, we pay $0.20 per summary and for extractive systems, we pay $0.15 per summary since extractive summaries are more readable and might precisely overlap with SCUs. We post-process system output summaries before presenting them to annotators by true-casing the text using Stanford CoreNLP (Manning et al., 2014) and replacing “unknown” tokens with a special symbol “2” (Chaganty et al., 2018). Tab. 2 depicts an example reference summary, system summary, SCUs extracted from the reference summary, and annotations obtained in evaluating the system summary. Annotation Scoring For robustness (Shapira et al., 2019), each system summary is evaluated by 4 crowd workers. Each worker annotates up to 16 SCUs by marking an SCU “present” if it can be 9 We contacted the authors of these systems to gather the corresponding outputs, including variants of the systems. 10 https://www.mturk.com/ 11 In our representative"
2020.emnlp-main.751,K16-1028,0,0.126832,"Missing"
2020.emnlp-main.751,N18-1158,0,0.024709,"CNNDM) (Hermann et al., 2015; Nallapati et al., 2016) is a commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018),"
2020.emnlp-main.751,N04-1019,0,0.601402,"ssessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both systemlevel and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive): https://github.com/neulab/REALSumm 1 Introduction In text summarization, manual evaluation, as exemplified by the Pyramid method (Nenkova and Passonneau, 2004), is the gold-standard in evaluation. However, due to time required and relatively high cost of annotation, the great majority of research papers on summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of b"
2020.emnlp-main.751,D15-1222,0,0.216412,"aper, our human evaluation methodology is based on the Pyramid (Nenkova and Passonneau, 2004) and LitePyramids (Shapira et al., 2019) techniques. Chaganty et al. (2018) also obtain human evaluations on system summaries on the CNNDM dataset, but with a focus on language quality of summaries. In comparison, our work is focused on evaluating content selection. Our work also covers more systems than their study (11 extractive + 14 abstractive vs. 4 abstractive). Meta-evaluation with Human Judgment The effectiveness of different automatic metrics ROUGE-2 (Lin, 2004), ROUGE-L (Lin, 2004), ROUGE-WE (Ng and Abrecht, 2015), JS-2 (Louis and Nenkova, 2013) and S3 (Peyrard et al., 2017) is commonly evaluated based on their correlation with human judgments (e.g., on the TAC2008 (Dang and Owczarzak, 2008) and TAC2009 (Dang and Owczarzak, 2009) datasets). As an important supplementary technique to metaevaluation, Graham (2015) advocate for the use of a significance test, William’s test (Williams, 1959), to measure the improved correlations of a metric with human scores and show that the popular variant of ROUGE (mean ROUGE-2 score) is sub-optimal. Unlike these works, instead of proposing a new metric, in this paper,"
2020.emnlp-main.751,D17-1238,0,0.0911357,"Missing"
2020.emnlp-main.751,P19-1502,0,0.0416069,"drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used to test the degree to which automatic metrics correlate therewith. However, the classic TAC meta-evaluation datasets are now 6-12 years old2 and it is not clear whether conclusions found there will hold with modern systems and summarization tasks. Two earlier works exemplify this disconnect: (1) Peyrard (2019) observed that the human-annotated summaries in the TAC dataset are mostly of lower quality than those produced by modern systems and that various automated evaluation metrics strongly disagree in the higher-scoring range in which current systems now operate. (2) Rankel et al. (2013) observed that the correlation between ROUGE and human judgments in the TAC dataset decreases when looking at the best systems only, even for systems from eight years ago, which are far from today’s state-of-the-art. Constrained by few existing human judgment datasets, it remains unknown how existing metrics behave"
2020.emnlp-main.751,W17-4510,0,0.452132,"n datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive): https://github.com/neulab/REALSumm 1 Introduction In text summarization, manual evaluation, as exemplified by the Pyramid method (Nenkova and Passonneau, 2004), is the gold-standard in evaluation. However, due to time required and relatively high cost of annotation, the great majority of research papers on summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used to test the degree to which"
2020.emnlp-main.751,P13-2024,0,0.0136503,"wczarzak, 2008)) is used to test the degree to which automatic metrics correlate therewith. However, the classic TAC meta-evaluation datasets are now 6-12 years old2 and it is not clear whether conclusions found there will hold with modern systems and summarization tasks. Two earlier works exemplify this disconnect: (1) Peyrard (2019) observed that the human-annotated summaries in the TAC dataset are mostly of lower quality than those produced by modern systems and that various automated evaluation metrics strongly disagree in the higher-scoring range in which current systems now operate. (2) Rankel et al. (2013) observed that the correlation between ROUGE and human judgments in the TAC dataset decreases when looking at the best systems only, even for systems from eight years ago, which are far from today’s state-of-the-art. Constrained by few existing human judgment datasets, it remains unknown how existing metrics behave on current top-scoring summarization systems. In this paper, we ask the question: does the rapid progress of model development in summarization models require us to re-evaluate the evaluation process used for text summarization? To this end, we create and release a large benchmark f"
2020.emnlp-main.751,P17-1099,0,0.0716575,"er (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019b), preSummAbsext (Liu and Lapata, 2019b) BART (Lewis et al., 2019) and Semsim (Yoon et al., 2020) as abstractive systems. In total, we use 14 abstractive system outputs for each document in the CNNDM test set. 2.3 Evaluation Metrics We examine eight metrics that measure the agreement between two texts, in our case, between the sy"
2020.emnlp-main.751,2020.acl-main.704,0,0.0851236,"Missing"
2020.emnlp-main.751,N19-1072,0,0.438136,"2) Metrics have much lower correlations when evaluating summaries than systems. (1) ROUGE metrics outperform all other metrics. (2) For extractive summaries, most metrics are better at evaluating summaries than systems. For abstractive summaries, some metrics are better at summary level, others are better at system level. Table 1: Summary of our experiments, observations on existing human judgments on the TAC, and contrasting observations on newly obtained human judgments on the CNNDM dataset. Please refer to Sec. 4 for more details. • Manual evaluations using the lightweight pyramids method (Shapira et al., 2019), which we use as a gold-standard to evaluate summarization systems as well as automated metrics. Using this benchmark, we perform an extensive analysis, which indicates the need to re-examine our assumptions about the evaluation of automatic summarization systems. Specifically, we conduct four experiments analyzing the correspondence between various metrics and human evaluation. Somewhat surprisingly, we find that many of the previously attested properties of metrics found on the TAC dataset demonstrate different trends on our newly collected CNNDM dataset, as shown in Tab. 1. For example, Mo"
2020.emnlp-main.751,D19-1053,0,0.150837,"t are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive): https://github.com/neulab/REALSumm 1 Introduction In text summarization, manual evaluation, as exemplified by the Pyramid method (Nenkova and Passonneau, 2004), is the gold-standard in evaluation. However, due to time required and relatively high cost of annotation, the great majority of research papers on summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used to test the degree to which automatic metrics correlate therewith. However, the classic TAC"
2020.emnlp-main.751,P19-1100,1,0.857875,"ies. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 20"
2020.emnlp-main.751,N18-1156,0,0.0145532,"allapati et al., 2016) is a commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9"
2020.emnlp-main.751,K19-1074,0,0.0157723,"commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2"
2020.emnlp-main.751,D18-1088,0,0.0187218,"d during the TAC-2008, TAC-2009 shared tasks. CNN/DailyMail (CNNDM) (Hermann et al., 2015; Nallapati et al., 2016) is a commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLr"
2020.emnlp-main.751,P19-1499,0,0.0165874,"commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2"
2020.findings-emnlp.319,N19-1388,0,0.0191873,"into (Ha et al., 2016; Johnson et al., 2017). Many works focus on using multilingual training to improve many-to-one NMT models that translate from both an HRL and an LRL to a single target language (Zoph et al., 2016; Neubig and Hu, 2018; Gu et al., 2018). In this situation, sentences from the HRL-target corpus provide an extra training signal for the decoder language model, on top of cross-lingual transfer on the source side. When training an NMT model that translates into an LRL, however, multilingual data tends to lead to smaller improvements (Lakew et al., 2019; Arivazhagan et al., 2019; Aharoni et al., 2019). In this paper, we aim to improve the effectiveness of multilingual training for NMT models that translate into LRLs. Prior work has found vocabulary overlap to be an important indicator of whether data from other languages will be effective in improving NMT accuracy (Wang and Neubig, 2019; Lin et al., 2019). Therefore, we hypothesize that one of the main problems limiting multilingual transfer on the target side is that the LRL and the HRL may have limited vocabulary overlap, and standard methods for embedding target words via lookup tables would map corresponding vocabulary from these langu"
2020.findings-emnlp.319,P18-2049,0,0.0254783,"multilingual transfer on the target side is that the LRL and the HRL may have limited vocabulary overlap, and standard methods for embedding target words via lookup tables would map corresponding vocabulary from these languages to different representations. To overcome this problem, we design a target word embedding method for multilingual NMT that encourages similar words from the HRLs and the LRLs to have similar representations, facilitating positive transfer to the LRLs. While there are many methods to embed words from characters (Ling et al., 2015; Kim et al., 2016; Wieting et al., 2016; Ataman and Federico, 2018), we build our model upon Soft Decoupled Encoding (SDE; Wang et al. (2019)), a recently-proposed generalpurpose multilingual word embedding method that has demonstrated superior performance to other alternatives. SDE represents a word by combin3560 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3560–3566 c November 16 - 20, 2020. 2020 Association for Computational Linguistics ing a character-based representation of its spelling and a lookup-based representation of its meaning. We propose DecSDE, an efficient adaptation of SDE to NMT decoders. DecSDE uses a low-ran"
2020.findings-emnlp.319,D18-1398,0,0.0175687,"nefit from the extra training signal from data in other languages. One of the most popular strategies for multilingual training is to train a single NMT model that translates in many directions by simply appending a flag to each source sentence to indicate which 1 Open-source code is available at https://github. com/luyug/DecSDE target language to translate into (Ha et al., 2016; Johnson et al., 2017). Many works focus on using multilingual training to improve many-to-one NMT models that translate from both an HRL and an LRL to a single target language (Zoph et al., 2016; Neubig and Hu, 2018; Gu et al., 2018). In this situation, sentences from the HRL-target corpus provide an extra training signal for the decoder language model, on top of cross-lingual transfer on the source side. When training an NMT model that translates into an LRL, however, multilingual data tends to lead to smaller improvements (Lakew et al., 2019; Arivazhagan et al., 2019; Aharoni et al., 2019). In this paper, we aim to improve the effectiveness of multilingual training for NMT models that translate into LRLs. Prior work has found vocabulary overlap to be an important indicator of whether data from other languages will be ef"
2020.findings-emnlp.319,W17-3204,0,0.0296805,"hat translate into the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding (Wang et al., 2019), we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.1 1 Introduction The performance of Neural Machine Translation (NMT; Sutskever et al. (2014)) tends to degrade on low-resource languages (LRL) due to a paucity of parallel data (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). One effective strategy to improve translation in LRLs is through multilingual training using parallel data from related high-resource languages (HRL) (Zoph et al., 2016; Neubig and Hu, 2018). The assumption underlying cross-lingual transfer is that by sharing parameters between multiple languages the LRL can benefit from the extra training signal from data in other languages. One of the most popular strategies for multilingual training is to train a single NMT model that translates in many directions by simply appending a flag to each source sentence to indicate wh"
2020.findings-emnlp.319,D18-2012,0,0.106745,"matrix and the decoder projection before the softmax operation. With these considerations in mind, we introduce DecSDE, a multilingual target word embedding method based on SDE for NMT decoders. 3561 Fixed Vocabulary and Weight Tying The standard SDE is designed to encode words directly without segmenting them into subwords (Wang et al., 2019). This design choice works well for encoding words on the source side, but it can cause problems for the decoder, which requires a finite vocabulary to generate words for each time step. Therefore, we choose to segment the target sentences into subwords (Kudo and Richardson, 2018), and encode each subword using DecSDE. The use of a fixed vocabulary also allows us to perform weight tying. Specifically, we construct an embedding matrix for the decoder by precomputing the DecSDE embedding for each subword in the target vocabulary. This embedding matrix can then be used both as the encoder lookup table and as the projection matrix before the decoder softmax. Efficient Training and Inference One drawback of the standard SDE is that it requires more computation than standard look-up table embeddings because the lexical embedding requires one to extract and embed all characte"
2020.findings-emnlp.319,D15-1166,0,0.0602689,"(Wc BoN(w)) , (1) where tanh is the activation function and Wc ∈ Rd×n is an embedding matrix of dimension d for the n character n-grams in the vocabulary. Language-specific transformation is then applied to lexical embedding c(w) to account for the divergence between the HRL and the LRL: ci (w) = tanh (WLi c(w)) , (2) where the matrix WLi ∈ Rd×d is a linear transformation specific to the language Li . Latent semantic embeddings of w are calculated using an embedding matrix Ws ∈ Rd×s with s entries, which is shared between the languages. We use ci (w) as the query vector to perform attention (Luong et al., 2015) over the embeddings   s(p) = Ws softmax W> c (w) . i s (3) The final embedding of w is obtained by summing the lexical and semantic representations eSDE (w) = ci (w) + s(w). 4 (4) DecSDE for NMT Decoders In this section, we build upon the previously described SDE, and design a new method for multilingual word representation on the target side. There are two aspects to consider when incorporating character-based representations like SDE in decoders: 1) the embedding method should be efficient during both training and inference time, as it needs to be calculated over the entire vocabulary; 2)"
2020.findings-emnlp.319,N19-4007,1,0.815811,"Missing"
2020.findings-emnlp.319,P19-1021,0,0.0176696,"L, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding (Wang et al., 2019), we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.1 1 Introduction The performance of Neural Machine Translation (NMT; Sutskever et al. (2014)) tends to degrade on low-resource languages (LRL) due to a paucity of parallel data (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). One effective strategy to improve translation in LRLs is through multilingual training using parallel data from related high-resource languages (HRL) (Zoph et al., 2016; Neubig and Hu, 2018). The assumption underlying cross-lingual transfer is that by sharing parameters between multiple languages the LRL can benefit from the extra training signal from data in other languages. One of the most popular strategies for multilingual training is to train a single NMT model that translates in many directions by simply appending a flag to each source sentence to indicate which 1 Open-source code is a"
2020.findings-emnlp.319,P19-1583,1,0.808985,"the HRL-target corpus provide an extra training signal for the decoder language model, on top of cross-lingual transfer on the source side. When training an NMT model that translates into an LRL, however, multilingual data tends to lead to smaller improvements (Lakew et al., 2019; Arivazhagan et al., 2019; Aharoni et al., 2019). In this paper, we aim to improve the effectiveness of multilingual training for NMT models that translate into LRLs. Prior work has found vocabulary overlap to be an important indicator of whether data from other languages will be effective in improving NMT accuracy (Wang and Neubig, 2019; Lin et al., 2019). Therefore, we hypothesize that one of the main problems limiting multilingual transfer on the target side is that the LRL and the HRL may have limited vocabulary overlap, and standard methods for embedding target words via lookup tables would map corresponding vocabulary from these languages to different representations. To overcome this problem, we design a target word embedding method for multilingual NMT that encourages similar words from the HRLs and the LRLs to have similar representations, facilitating positive transfer to the LRLs. While there are many methods to em"
2020.findings-emnlp.319,D16-1157,0,0.0185312,"ain problems limiting multilingual transfer on the target side is that the LRL and the HRL may have limited vocabulary overlap, and standard methods for embedding target words via lookup tables would map corresponding vocabulary from these languages to different representations. To overcome this problem, we design a target word embedding method for multilingual NMT that encourages similar words from the HRLs and the LRLs to have similar representations, facilitating positive transfer to the LRLs. While there are many methods to embed words from characters (Ling et al., 2015; Kim et al., 2016; Wieting et al., 2016; Ataman and Federico, 2018), we build our model upon Soft Decoupled Encoding (SDE; Wang et al. (2019)), a recently-proposed generalpurpose multilingual word embedding method that has demonstrated superior performance to other alternatives. SDE represents a word by combin3560 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3560–3566 c November 16 - 20, 2020. 2020 Association for Computational Linguistics ing a character-based representation of its spelling and a lookup-based representation of its meaning. We propose DecSDE, an efficient adaptation of SDE to NMT dec"
2020.findings-emnlp.319,D16-1163,0,0.0154377,"efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.1 1 Introduction The performance of Neural Machine Translation (NMT; Sutskever et al. (2014)) tends to degrade on low-resource languages (LRL) due to a paucity of parallel data (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). One effective strategy to improve translation in LRLs is through multilingual training using parallel data from related high-resource languages (HRL) (Zoph et al., 2016; Neubig and Hu, 2018). The assumption underlying cross-lingual transfer is that by sharing parameters between multiple languages the LRL can benefit from the extra training signal from data in other languages. One of the most popular strategies for multilingual training is to train a single NMT model that translates in many directions by simply appending a flag to each source sentence to indicate which 1 Open-source code is available at https://github. com/luyug/DecSDE target language to translate into (Ha et al., 2016; Johnson et al., 2017). Many works focus on using multilingual training to"
2020.findings-emnlp.319,D18-1103,1,0.867239,"n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.1 1 Introduction The performance of Neural Machine Translation (NMT; Sutskever et al. (2014)) tends to degrade on low-resource languages (LRL) due to a paucity of parallel data (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). One effective strategy to improve translation in LRLs is through multilingual training using parallel data from related high-resource languages (HRL) (Zoph et al., 2016; Neubig and Hu, 2018). The assumption underlying cross-lingual transfer is that by sharing parameters between multiple languages the LRL can benefit from the extra training signal from data in other languages. One of the most popular strategies for multilingual training is to train a single NMT model that translates in many directions by simply appending a flag to each source sentence to indicate which 1 Open-source code is available at https://github. com/luyug/DecSDE target language to translate into (Ha et al., 2016; Johnson et al., 2017). Many works focus on using multilingual training to improve many-to-one N"
2020.findings-emnlp.319,N19-4009,0,0.0119926,"l., 2018). We use three LRL datasets: Azerbaijani (aze), Belarusian (bel), Galician (glg) to English, and a slightly higher-resource dataset Slovak (slk). Each LRL is paired with a related HRL: Turkish (tur), Russian (rus), Portuguese (por), and Czech (ces) respectively. We translate from English to each of the four LRLs, and train together with the corresponding HRL. For simplicity, as a research setup, we do not use back-translation with mono-lingual data which is also hard to come by for languages low in resource we experiment with. Implementation We implement our method using the fairseq (Ott et al., 2019) toolkit. We use the Transformer (Vaswani et al., 2017) NMT model with 6 encoder and decoder layers and 4 attention heads. Other details of model architecture can be found in § A.1. For all experiments, we use SentencePiece (Kudo and Richardson, 2018) with a vocabulary size of 16K. Compared Systems We compare with two systems: 1) LookUp-piece: we use SentencePiece separately on each language to get subword vocabularies. Both encoder and decoder use look-up based 3562 embeddings. 2) LookUp-word: We concatenate the training data together and extract the most frequent 64K tokens as the shared voc"
2020.findings-emnlp.319,W18-6319,0,0.0129295,"ecSDE has similar inference speed as standard look-up embeddings. methods tend to demonstrate decreasing accuracy as the vocabulary size gets larger. slk 6.7 21.34 22.55 21.2 22.4 22.2 Method LookUp-piece DecSDE # Vocab 8K 16K 32K 8K 16K 32K aze 6.18 5.18 5.03 6.43 6.26 5.36 bel 9.2 9.81 8.75 11.57 11.36 10.65 glg 22.02 21.86 21.27 23.81 23.68 22.85 slk 21.92 21.34 20.37 22.92 22.4 20.16 Table 1: Model performance and ablations. DecSDE outperforms the best baseline for all four languages. Table 3: Performance with Different Vocab Size. Performance We measure model performance using SacreBLEU (Post, 2018) and summarize the results in Tab. 1. DecSDE consistently improves over the best baseline for all languages, outperforming LookUp-piece by up to 1.8 BLEU. Meanwhile, we see word-level baseline has inferior performance, likely due to little word-level overlap between HRL and LRL. Effect of N-gram Size DecSDE builds up its character n-gram vocabulary by extracting n-grams of lengths from 1 up to n from the input vocabulary. Using a larger n makes the model more expressive, but it might adds more parameters the model which could lead to overfitting. In this section, we examine the effect of diffe"
2020.findings-emnlp.319,E17-2025,0,0.0229099,"edding of w is obtained by summing the lexical and semantic representations eSDE (w) = ci (w) + s(w). 4 (4) DecSDE for NMT Decoders In this section, we build upon the previously described SDE, and design a new method for multilingual word representation on the target side. There are two aspects to consider when incorporating character-based representations like SDE in decoders: 1) the embedding method should be efficient during both training and inference time, as it needs to be calculated over the entire vocabulary; 2) it should support popular decoder design decisions, such as weight tying (Press and Wolf, 2017), which allows the decoder to share the parameters of the target embedding matrix and the decoder projection before the softmax operation. With these considerations in mind, we introduce DecSDE, a multilingual target word embedding method based on SDE for NMT decoders. 3561 Fixed Vocabulary and Weight Tying The standard SDE is designed to encode words directly without segmenting them into subwords (Wang et al., 2019). This design choice works well for encoding words on the source side, but it can cause problems for the decoder, which requires a finite vocabulary to generate words for each time"
2020.findings-emnlp.319,N18-2084,1,0.757511,"e divergence between languages without amplifying the difference. Extension to Multiple Target Language Note that though in this work we focus on HRL and LRL pairs, one can easily extend the framework to multiple (> 2) target languages. In particular, the only language dependent component of DecSDE is the matrices WLi , while the rest of DecSDE parameters as well as transformer encoder-decoder parameters are shared. We can add and train WLj for each of additional language Lj . 5 5.1 Experiments Setup Datasets To validate our method, we use the 58language-to-English TED corpus for experiments (Qi et al., 2018). We use three LRL datasets: Azerbaijani (aze), Belarusian (bel), Galician (glg) to English, and a slightly higher-resource dataset Slovak (slk). Each LRL is paired with a related HRL: Turkish (tur), Russian (rus), Portuguese (por), and Czech (ces) respectively. We translate from English to each of the four LRLs, and train together with the corresponding HRL. For simplicity, as a research setup, we do not use back-translation with mono-lingual data which is also hard to come by for languages low in resource we experiment with. Implementation We implement our method using the fairseq (Ott et al"
2020.findings-emnlp.353,P19-1284,0,0.0120133,"ze this likelihood in two ways. First, L= Related Work = We briefly discuss methods from the interpretability literature that aim to identify salient features of the input. Lei et al. (2016) propose an approach wherein a generator first extracts a subset of the text from the original input, which is further fed to an encoder that classifies the input by using only the extracted subset. The generator and encoder are trained end-to-end via REINFORCE-style optimization (Williams, 1992). However, follow-up work discovered the end-to-end training to be quite unstable with high variance in results (Bastings et al., 2019; Paranjape et al., 2020). Consequently, other approaches adopted the core idea of extract, and then classify in different forms: Lehman et al. (2019) decouple the extraction and prediction modules and train them individually with intermediate supervision; Jain et al. (2020) use heuristics, like attention scores, for extraction; and lastly, Paranjape et al. (2020) extract subsets that have high n Y n Y p(ei |xi ) p(yi |xi , ei ) i=1 n Y i=1 p(ei |xi ) p(yi |ei ) |{z } |{z } extract classify (assuming yi ⊥ xi |ei ) This corresponds to the extract, then classify approach. Since both components o"
2020.findings-emnlp.353,D19-1565,0,0.0259231,"Missing"
2020.findings-emnlp.353,N19-1423,0,0.0329559,"l. No ground truth explanations to compare against. Evidence is a characteristic of the task. Can be compared against human-labeled evidence. A horror movie that lacks cohesion. A horror movie that lacks cohesion. Example Table 1: Distinguishing local explanations from evidence snippets. In the illustrative example, the token horror is predictive of the negative sentiment as horror movies tend to get poorer ratings than movies from other genres (Kaushik et al., 2019), however, no expert would mark it to be the evidence justifying the negative review. evidence. For classification, we use BERT (Devlin et al., 2019). The extraction task (a sequence tagging problem) is modeled using a linear-chain CRF (Lafferty et al., 2001). The CRF uses representations and attention scores from BERT as emission features, allowing the two tasks (i.e. classification and extraction) to benefit from shared parameters. Further, the evidence extraction module is conditioned on the class label, enabling the CRF to output different evidence spans tailored to each class label. This is illustrated in Table 2. For baselines, we repurpose input attribution methods from the interpretability literature. Many approaches in this catego"
2020.findings-emnlp.353,2020.acl-main.409,0,0.0908727,"-chain CRF (Lafferty et al., 2001). The CRF uses representations and attention scores from BERT as emission features, allowing the two tasks (i.e. classification and extraction) to benefit from shared parameters. Further, the evidence extraction module is conditioned on the class label, enabling the CRF to output different evidence spans tailored to each class label. This is illustrated in Table 2. For baselines, we repurpose input attribution methods from the interpretability literature. Many approaches in this category first extract, and then classify (Lei et al., 2016; Lehman et al., 2019; Jain et al., 2020; Paranjape et al., 2020). Across two text sequence classification and evidence extraction tasks, we find our methods to outperform baselines. Encouragingly, we observe gains by using our approach with as few as 100 evidence annotations. 2 mutual information with the output variable and low mutual information with the input variable. 3 Extracting Evidence Formally, let the training data consist of n points {(x1 , y1 )...(xn , yn )}, where xi is a document, yi is the associated label. We assume that for m points (m  n) we also have evidence annotations ei , a binary vector such that eij = 1 if"
2020.findings-emnlp.353,N19-1371,0,0.434794,"idence annotations.1 1 Introduction Despite the success of deep learning for countless prediction tasks, practitioners often desire that these models not only be accurate but also provide interpretations or explanations (Caruana et al., 2015; Weld and Bansal, 2019). Unfortunately, these terms lack precise meaning, and across papers, such explanations purport to address a wide spectrum of desiderata, and it seems unlikely any one method could address them all (Lipton, 2018). In both computer vision (Ribeiro et al., 2016; Simonyan et al., 2013) and natural language processing (Lei et al., 2016; Lehman et al., 2019), proposed explanation methods often take the form of highlighting salient features of the input. These socalled local explanations are intended to highlight features that elucidate “the reasons behind predictions” (Ribeiro et al., 2016). However, this characterization of the problem remains under-specified. 1 Code and datasets to reproduce our work are available at: https://github.com/danishpruthi/ evidence-extraction. In this paper, we instead focus on supplementing predictions with evidence, which we define as information that gives users the ability to quickly verify the correctness of pre"
2020.findings-emnlp.353,D16-1011,0,0.299935,"few as hundred evidence annotations.1 1 Introduction Despite the success of deep learning for countless prediction tasks, practitioners often desire that these models not only be accurate but also provide interpretations or explanations (Caruana et al., 2015; Weld and Bansal, 2019). Unfortunately, these terms lack precise meaning, and across papers, such explanations purport to address a wide spectrum of desiderata, and it seems unlikely any one method could address them all (Lipton, 2018). In both computer vision (Ribeiro et al., 2016; Simonyan et al., 2013) and natural language processing (Lei et al., 2016; Lehman et al., 2019), proposed explanation methods often take the form of highlighting salient features of the input. These socalled local explanations are intended to highlight features that elucidate “the reasons behind predictions” (Ribeiro et al., 2016). However, this characterization of the problem remains under-specified. 1 Code and datasets to reproduce our work are available at: https://github.com/danishpruthi/ evidence-extraction. In this paper, we instead focus on supplementing predictions with evidence, which we define as information that gives users the ability to quickly verify"
2020.findings-emnlp.353,P11-1015,0,0.0144095,"ens with the highest attention scores (value of k is set to match the fraction of evidence tokens in the development set);5 and (iv) Supervised attention, where attention is supervised to be uniformly high for tokens marked as evidence, and low otherwise (Zhong et al., 2019). Setup We evaluate the different evidence extraction approaches on two text classification tasks: analyzing sentiment of movie reviews (Pang et al., 2002), and detecting propaganda techniques in news articles (Da San Martino et al., 2019). For the sentiment analysis task, we use the IMDb movie reviews dataset collected by Maas et al. (2011) comprising 25K movie reviews available for training, and 25K for development and testing. The dataset has disjoint sets of movies for training and testing. Additionally, we use 1.8K movie reviews with marked evidence spans collected by Zaidan et al. (2007). Of these 1.8K spans, we use 1.2K for 4 There exist trivial solutions to the Information Bottleneck objective when subset granularity is tokens instead of sentences. One such solution is when the extraction model extracts “.” for the positive class, a “,” for the negative class. 5 Interestingly, Jain et al. (2020) find this simple threshold"
2020.findings-emnlp.353,2020.emnlp-demos.6,0,0.0576766,"Missing"
2020.findings-emnlp.353,D08-1004,0,0.427891,"manual annotations of evidence segments, whereas the latter is provided by documents and their class labels which we assume are relatively abundant.2 In the extreme case where evidence annotations are available for all examples, our task reduces to a standard multitask learning problem. In the opposite extreme, where only weak supervision is available, we find ourselves back in the under-specified realm addressed by local explanations. While predictive tokens may be extracted using only weak supervision, evidence extraction requires some amount of strong supervision. We draw inspiration from Zaidan and Eisner (2008), who study the reverse problem—how to leverage marked evidence spans to improve classification performance. We optimize the joint likelihood of class labels and evidence spans, given the input examples. We factorize our objective such that we first classify, and then extract the 2 While the task formulation is broadly applicable, we limit to text classification tasks for the scope of this work. 3965 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3965–3970 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Explanations Evidence Objective Eluc"
2020.findings-emnlp.353,W02-1011,0,0.0252689,"ch that they have maximal mutual information (MI) with the output label, and minimal MI with the original input;4 (iii) the FRESH approach (Jain et al., 2020), which extracts the top-k tokens with the highest attention scores (value of k is set to match the fraction of evidence tokens in the development set);5 and (iv) Supervised attention, where attention is supervised to be uniformly high for tokens marked as evidence, and low otherwise (Zhong et al., 2019). Setup We evaluate the different evidence extraction approaches on two text classification tasks: analyzing sentiment of movie reviews (Pang et al., 2002), and detecting propaganda techniques in news articles (Da San Martino et al., 2019). For the sentiment analysis task, we use the IMDb movie reviews dataset collected by Maas et al. (2011) comprising 25K movie reviews available for training, and 25K for development and testing. The dataset has disjoint sets of movies for training and testing. Additionally, we use 1.8K movie reviews with marked evidence spans collected by Zaidan et al. (2007). Of these 1.8K spans, we use 1.2K for 4 There exist trivial solutions to the Information Bottleneck objective when subset granularity is tokens instead of"
2020.findings-emnlp.353,2020.emnlp-main.153,0,0.454525,"y et al., 2001). The CRF uses representations and attention scores from BERT as emission features, allowing the two tasks (i.e. classification and extraction) to benefit from shared parameters. Further, the evidence extraction module is conditioned on the class label, enabling the CRF to output different evidence spans tailored to each class label. This is illustrated in Table 2. For baselines, we repurpose input attribution methods from the interpretability literature. Many approaches in this category first extract, and then classify (Lei et al., 2016; Lehman et al., 2019; Jain et al., 2020; Paranjape et al., 2020). Across two text sequence classification and evidence extraction tasks, we find our methods to outperform baselines. Encouragingly, we observe gains by using our approach with as few as 100 evidence annotations. 2 mutual information with the output variable and low mutual information with the input variable. 3 Extracting Evidence Formally, let the training data consist of n points {(x1 , y1 )...(xn , yn )}, where xi is a document, yi is the associated label. We assume that for m points (m  n) we also have evidence annotations ei , a binary vector such that eij = 1 if token xij is a part of t"
2020.findings-emnlp.353,N16-3020,0,0.266942,"the interpretability literature to our task. Our approach yields gains with as few as hundred evidence annotations.1 1 Introduction Despite the success of deep learning for countless prediction tasks, practitioners often desire that these models not only be accurate but also provide interpretations or explanations (Caruana et al., 2015; Weld and Bansal, 2019). Unfortunately, these terms lack precise meaning, and across papers, such explanations purport to address a wide spectrum of desiderata, and it seems unlikely any one method could address them all (Lipton, 2018). In both computer vision (Ribeiro et al., 2016; Simonyan et al., 2013) and natural language processing (Lei et al., 2016; Lehman et al., 2019), proposed explanation methods often take the form of highlighting salient features of the input. These socalled local explanations are intended to highlight features that elucidate “the reasons behind predictions” (Ribeiro et al., 2016). However, this characterization of the problem remains under-specified. 1 Code and datasets to reproduce our work are available at: https://github.com/danishpruthi/ evidence-extraction. In this paper, we instead focus on supplementing predictions with evidence, whic"
2020.lrec-1.656,L18-1530,1,0.837929,"t al., 2017; Littell et al., 2018). In our experience, the most requested speech technology for very-low-resource languages is transcription acceleration, an application of speech recognition for decreasing the workload of transcribers. Many low-resource and endangered languages do already have extensive untranscribed speech collections, in the form of recorded radio broadcasts, linguists’ field recordings, or other personal recordings. Transcribing these collections is a high priority for many speech communities, as an untranscribed corpus is difficult to use in either research or education (Adams et al., 2018; Foley et al., 2019). AlloVera and Allosaurus were originally and primarily intended for use in transcription acceleration, although we will also be exploring other practical applications. Another priority technology is approximate search of speech databases. While the aforementioned untranscribed speech collections can straightforwardly be made available online, they are not especially accessible as such. A researcher, teacher, or student cannot in practice listen to years’ worth of radio recordings in search of a particular word or topic. AlloVera and Allosaurus, by making an approximate te"
2020.lrec-1.656,W17-4607,1,0.839092,"is approximate search of speech databases. While the aforementioned untranscribed speech collections can straightforwardly be made available online, they are not especially accessible as such. A researcher, teacher, or student cannot in practice listen to years’ worth of radio recordings in search of a particular word or topic. AlloVera and Allosaurus, by making an approximate text representation of the corpus, open up the possibility for efficient approximate phonetic search through otherwise-untranscribed speech databases. Previous work has demonstrated the feasibility of such an approach (Anastasopoulos et al., 2017; Boito et al., 2017), but the quality of the search results can be significantly boosted by improvements in a first-pass phonetic transcription (Ondel et al., 2018). We are also planning on integrating AlloVera and Allosaurus into a language-neutral forced-alignment pipeline. While forced-alignment is a task that is already commonly done in a zero-shot scenario (by manually mapping target-language phones to the vocabulary of a pretrained acoustic model, often an English one), the extensive phonetic vocabulary of AlloVera means that many phones are already covered. This greatly expands the num"
2020.lrec-1.656,2021.eacl-main.58,0,0.114164,"Missing"
2020.lrec-1.656,W17-0106,1,0.837055,"ance over both the shared phoneme model and the private phoneme model substantially. 4. Applications Currently, we intend to integrate AlloVera and Allosaurus (or other future systems trained using AlloVera) into three practical downstream systems for very-low-resource languages, addressing tasks identified as development priori5333 Shared Phoneme PER Private Phoneme PER Allosaurus PER Inuktitut Tusom 94.1 86.2 73.1 93.5 85.8 64.2 Table 5: Comparisons of phone error rates in two unseen languages ties in recent surveys of indigenous and other low-resource language technology (Thieberger, 2016; Levow et al., 2017; Littell et al., 2018). In our experience, the most requested speech technology for very-low-resource languages is transcription acceleration, an application of speech recognition for decreasing the workload of transcribers. Many low-resource and endangered languages do already have extensive untranscribed speech collections, in the form of recorded radio broadcasts, linguists’ field recordings, or other personal recordings. Transcribing these collections is a high priority for many speech communities, as an untranscribed corpus is difficult to use in either research or education (Adams et al"
2020.lrec-1.656,C18-1222,1,0.85747,"hared phoneme model and the private phoneme model substantially. 4. Applications Currently, we intend to integrate AlloVera and Allosaurus (or other future systems trained using AlloVera) into three practical downstream systems for very-low-resource languages, addressing tasks identified as development priori5333 Shared Phoneme PER Private Phoneme PER Allosaurus PER Inuktitut Tusom 94.1 86.2 73.1 93.5 85.8 64.2 Table 5: Comparisons of phone error rates in two unseen languages ties in recent surveys of indigenous and other low-resource language technology (Thieberger, 2016; Levow et al., 2017; Littell et al., 2018). In our experience, the most requested speech technology for very-low-resource languages is transcription acceleration, an application of speech recognition for decreasing the workload of transcribers. Many low-resource and endangered languages do already have extensive untranscribed speech collections, in the form of recorded radio broadcasts, linguists’ field recordings, or other personal recordings. Transcribing these collections is a high priority for many speech communities, as an untranscribed corpus is difficult to use in either research or education (Adams et al., 2018; Foley et al.,"
2020.lrec-1.656,L18-1429,1,0.90023,"For language-specific models and questions, such representations are often adequate and may even be preferable to the alternatives. However, in multilingual models, the language-specific nature of phonemic abstractions can be a liability. The added phonetic realism of even a broad phonetic representation moves transcriptions closer to a universal space where categories transcend the bounds of a particular language. This paper describes AlloVera1 , a resource that maps between the phonemic representations produced by many NLP tools—including grapheme-to-phoneme (G2P) transducers like our own (Mortensen et al., 2018)—and broad phonetic representations. Specifically, it is a database of phonemeallophone pairs (where an allophone is a phonetic realization of a phoneme—see § 1.1. below) for 14 languages. It is designed for notational compatibility with existing G2P systems. The phonetic representations are relatively broad, a consequence of our sources, but they are phonetically realistic enough to improve performance on a speech-to-phone recognition task, as shown in § 3. 1 https://github.com/dmort27/allovera This resource has applications beyond universal speechto-phone recognition, including approximate s"
2020.lrec-1.656,rousseau-etal-2012-ted,0,0.0854531,"Missing"
2020.lrec-1.656,1996.amta-1.36,0,0.808006,"y manually mapping target-language phones to the vocabulary of a pretrained acoustic model, often an English one), the extensive phonetic vocabulary of AlloVera means that many phones are already covered. This greatly expands the number of languages that can be aligned without the need for an extensive transcribed corpus or manual system configuration. 5. Related Work AlloVera builds on work in three major areas: phonetics and theoretical phonology, phonological ontologies, and human language technologies. The term allophone was coined by Benjamin Lee Whorf in the 1920s and was popularized by Trager and Block (1941). However, the idea goes back much further, to Baudoin de Courtenay (1894). The idea of allophony most relevant to our work here comes from American Structuralist linguists like Harris (1951), but we also invoke the concept of the archiphoneme, associated with the Prague Circle (Trubetskoy, 1939). In the 1950s and 1960s, the structuralist notions of the “taxonomic” phoneme and of allophones came under attack by generative linguists (Chomsky and Halle, 1968; Halle, 1962; Halle, 1959), but they have retained their importance both in linguistic practice and linguistic theory. Various resources co"
2020.ngt-1.1,W19-5301,0,0.0795298,"Missing"
2020.ngt-1.1,N19-1423,0,0.00776584,"o 15 system submission papers. We elicted two double-blind reviews for each submission, avoiding conflicts of interest. With regards to thematology there were 8 papers with a focus on Natural Language Generation and 8 with the application of Machine Translation 1 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 1–9 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d in mind. The underlying emphasis across submissions was placed this year on capitalizing on the use of pre-training models (e.g., BERT; (Devlin et al., 2019) especially for low-resource datasets. The quality of the accepted publications was very high; there was a significant drop in numbers though in comparison to last year (36 accepted papers from 68 submissions) which is most likely due to the extra overhead on conducting research under lockdown policies sanctioned globally due to COVID19 pandemic. 3 GPU is relatively small compared to the NVIDIA V100 GPU, but the newer Turing architecture introduces support for 4-bit and 8-bit integer operations in Tensor Cores. In practice, however, participants used floating-point operations on the GPU even t"
2020.ngt-1.1,D13-1176,0,0.0312763,"Second, we describe the results of the three shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document-level generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language and 3) STAPLE task: creation of as many possible translations of a given input text. This last shared task was organised by Duolingo. 1 Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are the workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 4th Workshop on Neural Machine Translation and Generation (WNGT 2020) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization, NLG from structured data, dialog response generation, among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of"
2020.ngt-1.1,W04-1013,0,0.0806851,"of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differences. In particular, we"
2020.ngt-1.1,2020.ngt-1.28,0,0.0265263,"tput, but in certain cases, it is desirable to have many possible translations of a given input text. At Duolingo, the world’s largest online language-learning platform,7 we grade translationbased challenges with sets of human-curated acceptable translation options. Given the many ways of expressing a piece of text, these sets are slow to create, and may be incomplete. This process is ripe for improvement with the aid of rich multi-output translation and paraphrase systems. To this end, we introduce a shared task called STAPLE: Simultaneous Translation and Paraphrasing for Language Education (Mayhew et al., 2020). 4.4 5.1 4.3 Baselines We prepared two baselines for different tracks: FairSeq-19 We use FairSeq (Ng et al., 2019) (WMT’19 single model6 ) for MT and MT+NLG tracks. Submitted Systems One team participated in the task, who focused on the German-English MT track of the task. In this shared task, participants are given a training set consisting of 2500 to 4000 English sentences (or prompts), each of which is paired with a list of comprehensive translations in the target language, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English promp"
2020.ngt-1.1,D18-1325,0,0.021768,"guage, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English prompts, and are required to produce the set of comprehensive translations for each prompt. We also provide a high-quality automatic reference translation for each prompt, in the event that a participant wants to work on paraphrase-only approaches. The target languages were Hungarian, Japanese, Korean, Portuguese, and Vietnamese. Team FJWU developed a system around Transformer-based sequence-to-sequence model. Additionally, the model employed hierarchical attention following (Miculicich et al., 2018) for both encoder and decoder to account for the documentlevel context. The system was trained in a twostage process, where a base (sentence-level) NMT model was trained followed by the training of hierarchcal attention networks component. To handle the scarcity of in-domain translation data, they experimented with upsizing the in-domain data up to three times to construct training data. Their ablation experiments showed that this upsizing of in-domain data is effective at increasing the BLEU score. 4.5 Task Description 5.2 Submitted Systems There were 20 participants who submitted to the deve"
2020.ngt-1.1,W19-5333,0,0.0306894,"Missing"
2020.ngt-1.1,P02-1040,0,0.107839,"guage. mostly driven by the number of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differen"
2020.ngt-1.1,W18-6319,0,0.0283845,"Missing"
2020.nlpbt-1.4,2020.acl-main.231,0,0.0319372,"core of the best matching substring with the length of the shorter string in comparison. Note that this third metric will bias towards shorter, correct phrases and thus we should have a holistic view of all 3 metrics during the evaluation. Details of two fuzzy metrics are described in Appendix D. 36 should be more than mere object detection. et al., 2018c; Tang et al., 2019; Alayrac et al., 2016; Song et al., 2015; Sener et al., 2015; Huang et al., 2016; Sun et al., 2019b,a; Plummer et al., 2017; Palaskar et al., 2019), combining video & text information in procedures (Yagcioglu et al., 2018; Fried et al., 2020), visual-linguistic reference resolution (Huang et al., 2018, 2017), visual planning (Chang et al., 2019), joint learning of object and actions (Zhukov et al., 2019; Richard et al., 2018; Gao et al., 2017; Damen et al., 2018b), pretraining joint embedding of high level sentence with video clips (Sun et al., 2019b; Miech et al., 2019), our task proposal requires explicit structured knowledge tuple extraction. In addition to closely related work (§3) there is a wide literature (Malmaud et al., 2015; Zhou et al., 2018b; Ushiku et al., 2017; Nishimura et al., 2019; Tang et al., 2019; Huang et al.,"
2020.nlpbt-1.4,Q13-1005,0,0.0340971,"traction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts directly, not from imperative recipe texts. Instructional video understanding. Beyond image se"
2020.nlpbt-1.4,W18-2501,0,0.0230217,"Missing"
2020.nlpbt-1.4,W04-2412,0,0.229122,"Missing"
2020.nlpbt-1.4,J02-3001,0,0.293495,"Missing"
2020.nlpbt-1.4,D15-1090,0,0.401954,"being recognized as “add flower” and “sriracha sauce” being recognized as “sarrah cha sauce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals an"
2020.nlpbt-1.4,D15-1114,0,0.576155,"Missing"
2020.nlpbt-1.4,D11-1142,0,0.0172339,"task, given a narrative video, say a cooking video on YouTube about making clam chowder as shown in Figure 1, our goal is to extract a series of tuples representing the procedure, e.g. (heat, cast iron skillet), (fry, bacon, with heated skillet), etc. We created a manually annotated, large test dataset for evaluation of the task, including over 350 instructional cooking videos along with over 15,000 English sentences in the transcripts spanning over 89 recipe types. This verb-argument structure using arbitrary textual phrases is motivated by open information extraction (Schmitz et al., 2012; Fader et al., 2011), but focuses on procedures rather than entity-entity relations. This task is challenging with respect to both video and language understanding. For video, it requires understanding of video contents, with a speIntroduction Instructional videos are a convenient way to learn a new skill. Although learning from video seems natural to humans, it requires identifying and understanding procedures and grounding them to the real world. In this paper, we propose a new task and dataset for extracting procedural knowledge into a fine-grained structured representation from multimodal information containe"
2020.nlpbt-1.4,P19-1659,0,0.0522706,"Missing"
2020.nlpbt-1.4,D16-1155,0,0.0259752,"r” and “sriracha sauce” being recognized as “sarrah cha sauce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts dire"
2020.nlpbt-1.4,D16-1264,0,0.0333159,"fuzzy matching. The first is straight forward, we count true positives if and only if the predicted phrase is an exact string match in the gold phrases. However, because our task lies in the realm of open phrase extraction without predefined labels, it is unfairly strict to count only the exact string matches as T P . Also by design, the gold extraction results cannot always be found in the original transcript sentence (refer to §3.2), so we are also unable to use token-based metrics as in sequence tagging (Sang and De Meulder, 2003), or span-based metrics as in some question answering tasks (Rajpurkar et al., 2016). Thus for the second metric we call “fuzzy”, we leverage edit distance to enable fuzzy matching and assign a “soft” score for T P . In some cases, the two strings of quite different lengths will hurt the fuzzy score due to the nature of edit distance, even though one string is a substring of another. To get around this, we propose a third metric, “partial fuzzy” to get the score of the best matching substring with the length of the shorter string in comparison. Note that this third metric will bias towards shorter, correct phrases and thus we should have a holistic view of all 3 metrics durin"
2020.nlpbt-1.4,P16-1138,0,0.026742,"auce” being recognized as “sarrah cha sauce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts directly, not from impe"
2020.nlpbt-1.4,Q13-1003,0,0.0385252,"unstructured instructional video (clip and utterances) to structured procedures, defining what actions should be performed on which objects, with what arguments and in what order. We define the input to such an extraction system: Dataset & Analysis While others have created related datasets, they fall short on key dimensions which we remedy in our work. Specifically, In Table 1 we compare to AllRecipes (Kiddon et al., 2015) (AR), YouCook2 (Zhou et al., 2018b) (YC2), CrossTask (Zhukov et al., 2019) (CT), COIN (Tang et al., 2019), How2 (Sanabria et al., 2018), HAKE (Li et al., 2019) and TACOS (Regneri et al., 2013). Additional details about datasets are included in the Appendix A.2 In summary, none have both structured and open extraction annotations for the procedural knowledge extraction task, since most focus on either video summarization/captioning or action localization/classification. • Task R, e.g. “Create Chicken Parmesan” and instructional video VR describing the procedure to achieve task R, e.g. a video titled “Chicken Parmesan - Let’s Cook with ModernMom”.1 • A sequence of n sentences TR = {t0 , t1 , ..., tn } representing video VR ’s corresponding transcript. According to the time stamps of"
2020.nlpbt-1.4,W15-2206,0,0.3589,"zed as “sarrah cha sauce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts directly, not from imperative recipe texts."
2020.nlpbt-1.4,N15-1015,0,0.0311496,"; Palaskar et al., 2019), combining video & text information in procedures (Yagcioglu et al., 2018; Fried et al., 2020), visual-linguistic reference resolution (Huang et al., 2018, 2017), visual planning (Chang et al., 2019), joint learning of object and actions (Zhukov et al., 2019; Richard et al., 2018; Gao et al., 2017; Damen et al., 2018b), pretraining joint embedding of high level sentence with video clips (Sun et al., 2019b; Miech et al., 2019), our task proposal requires explicit structured knowledge tuple extraction. In addition to closely related work (§3) there is a wide literature (Malmaud et al., 2015; Zhou et al., 2018b; Ushiku et al., 2017; Nishimura et al., 2019; Tang et al., 2019; Huang et al., 2016; Shi et al., 2019; Ushiku et al., 2017) that aims to predict/align dense procedural captions given the video, which are the most similar works to ours. Zhou et al. (2018c) extracted temporal procedures and then generated captioning for each procedure. Sanabria et al. (2018) proposes a multimodal abstractive summarization for how-to videos with either human labeled or speech-to-text transcript. Alayrac et al. (2016) also introduces an unsupervised step learning method from instructional vide"
2020.nlpbt-1.4,W14-2407,0,0.433384,"auce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts directly, not from imperative recipe texts. Instructional video u"
2020.nlpbt-1.4,W03-0419,0,0.27491,"Missing"
2020.nlpbt-1.4,D12-1048,0,0.0253518,"et al., 2018a). In our task, given a narrative video, say a cooking video on YouTube about making clam chowder as shown in Figure 1, our goal is to extract a series of tuples representing the procedure, e.g. (heat, cast iron skillet), (fry, bacon, with heated skillet), etc. We created a manually annotated, large test dataset for evaluation of the task, including over 350 instructional cooking videos along with over 15,000 English sentences in the transcripts spanning over 89 recipe types. This verb-argument structure using arbitrary textual phrases is motivated by open information extraction (Schmitz et al., 2012; Fader et al., 2011), but focuses on procedures rather than entity-entity relations. This task is challenging with respect to both video and language understanding. For video, it requires understanding of video contents, with a speIntroduction Instructional videos are a convenient way to learn a new skill. Although learning from video seems natural to humans, it requires identifying and understanding procedures and grounding them to the real world. In this paper, we propose a new task and dataset for extracting procedural knowledge into a fine-grained structured representation from multimodal"
2020.nlpbt-1.4,P19-1641,1,0.798391,"l-linguistic reference resolution (Huang et al., 2018, 2017), visual planning (Chang et al., 2019), joint learning of object and actions (Zhukov et al., 2019; Richard et al., 2018; Gao et al., 2017; Damen et al., 2018b), pretraining joint embedding of high level sentence with video clips (Sun et al., 2019b; Miech et al., 2019), our task proposal requires explicit structured knowledge tuple extraction. In addition to closely related work (§3) there is a wide literature (Malmaud et al., 2015; Zhou et al., 2018b; Ushiku et al., 2017; Nishimura et al., 2019; Tang et al., 2019; Huang et al., 2016; Shi et al., 2019; Ushiku et al., 2017) that aims to predict/align dense procedural captions given the video, which are the most similar works to ours. Zhou et al. (2018c) extracted temporal procedures and then generated captioning for each procedure. Sanabria et al. (2018) proposes a multimodal abstractive summarization for how-to videos with either human labeled or speech-to-text transcript. Alayrac et al. (2016) also introduces an unsupervised step learning method from instructional videos. Inspired by cross-task sharing (Zhukov et al., 2019), which is a weakly supervised method to learn shared actions betw"
2020.nlpbt-1.4,I17-1033,0,0.0296986,"& text information in procedures (Yagcioglu et al., 2018; Fried et al., 2020), visual-linguistic reference resolution (Huang et al., 2018, 2017), visual planning (Chang et al., 2019), joint learning of object and actions (Zhukov et al., 2019; Richard et al., 2018; Gao et al., 2017; Damen et al., 2018b), pretraining joint embedding of high level sentence with video clips (Sun et al., 2019b; Miech et al., 2019), our task proposal requires explicit structured knowledge tuple extraction. In addition to closely related work (§3) there is a wide literature (Malmaud et al., 2015; Zhou et al., 2018b; Ushiku et al., 2017; Nishimura et al., 2019; Tang et al., 2019; Huang et al., 2016; Shi et al., 2019; Ushiku et al., 2017) that aims to predict/align dense procedural captions given the video, which are the most similar works to ours. Zhou et al. (2018c) extracted temporal procedures and then generated captioning for each procedure. Sanabria et al. (2018) proposes a multimodal abstractive summarization for how-to videos with either human labeled or speech-to-text transcript. Alayrac et al. (2016) also introduces an unsupervised step learning method from instructional videos. Inspired by cross-task sharing (Zhuko"
2020.nlpbt-1.4,D18-1166,0,\N,Missing
2020.sigmorphon-1.22,D16-1153,0,0.0172123,"nted with data generated from a related highresource language. Among many, for instance, De Gispert and Marino (2006) built a CatalanEnglish MT by bridging through Spanish, while Xia et al. (2019) show that word-level substitutions can convert a high-resource (related) language corpus into a pseudo low-resource one leading to large improvements in MT quality. Such approaches typically operate at the word level, hence they do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflect"
2020.sigmorphon-1.22,D18-1366,1,0.786098,"from a related highresource language. Among many, for instance, De Gispert and Marino (2006) built a CatalanEnglish MT by bridging through Spanish, while Xia et al. (2019) show that word-level substitutions can convert a high-resource (related) language corpus into a pseudo low-resource one leading to large improvements in MT quality. Such approaches typically operate at the word level, hence they do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al.,"
2020.sigmorphon-1.22,K18-3001,0,0.0552652,"y do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al., 2019) use cognate projection to augment low-resource data, while (Wiemerslage et al., 2018) explore the inflection task using inputs in phonological space as well as bundles of phonological features from PanPhon (Mortensen et al., 2016), showing improvements for both settings. Our work, in contrast, focuses on better cross-lingual transfer, attempting to combine the phonological a"
2020.sigmorphon-1.22,K17-2001,0,0.0758685,"Missing"
2020.sigmorphon-1.22,W18-2413,0,0.0607071,"Missing"
2020.sigmorphon-1.22,C16-1328,0,0.0583672,"Missing"
2020.sigmorphon-1.22,2020.sigmorphon-1.6,1,0.786101,"nto the test languages, G2P conversion, and romanization), we compute the percentage of times that an inflection with each morphological tag failed. Table 4 reports the tags with the highest difference in these ratios, between the baseline and our models for each method. The higher the number, the larger the improvements for this particular tag. For inflecting Portuguese (top and bottom sets of results), we find it hard to make any conclusions: both noun, adjective, and verb tags appear in the top lists. For inflecting Russian 194 10 In fact, our submission to the SIGMORPHON 2020 Shared Task (Murikinati and Anastasopoulos, 2020) following this approach tied for first for Telugu (Vylomova et al., 2020). (middle set), it is mostly noun/adjective tags pertaining to animacy (ANIM, INAN), gender (MASC) and case (GEN, DAT) that show the largest improvements. We still cannot explain the improvements we see in these language pairs, except for vague hypotheses that either the languages do share some similar inflection processes (besides, they are both Indo-European) or that the harder multi-task training setting regularizes the model leading to better accuracy overall. 5 Conclusion With this work we study whether using transl"
2020.sigmorphon-1.22,W19-4202,0,0.130691,"et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al., 2019) use cognate projection to augment low-resource data, while (Wiemerslage et al., 2018) explore the inflection task using inputs in phonological space as well as bundles of phonological features from PanPhon (Mortensen et al., 2016), showing improvements for both settings. Our work, in contrast, focuses on better cross-lingual transfer, attempting to combine the phonological and the orthographic space. 2 Quantifying the Issue In Table 1 we offered a few examples from the literature to indicate that differences in script between the transfer and test language in a cross-lingual learning setting"
2020.sigmorphon-1.22,W18-5805,0,0.0362923,"Missing"
2020.sigmorphon-1.22,P18-4003,0,0.131876,"Missing"
2020.sigmorphon-1.22,P19-1493,0,0.0296544,"e offered a few examples from the literature to indicate that differences in script between the transfer and test language in a cross-lingual learning setting can be a potential issue. In this section, we provide additional evidence that this is indeed the case. The intuition behind our analysis is that a model trained cross-lingually can only claim to indeed learn cross-lingually if it ends up sharing the representations of the different inputs, at least to some extent. This observation of a learned shared space has also been noted in massively multilingual models like the multilingual BERT (Pires et al., 2019), or for cross-lingual learning of word-level representations (Wang et al., 2020). For a character-level model, such as the ones typically used for neural morphological inflection, this implies a learned mapping between the characters of the two inputs. Our hypothesis is that such a learned character mapping, and in particular between related languages, should resemble a transliteration mapping, assuming that both languages use a phonographic writing system (such as the Latin or the Cyrillic alphabet and their variations), to use the notation of Faber (1992).2 To verify whether this intuition"
2020.sigmorphon-1.22,W04-3250,0,0.281827,"ed task, but limit ourselves to the language pairs where (1) the two languages use different writing scripts, and (2) we have access to a transliteration model from the transfer to the test language. As a result, we evaluate our approach on the following language pairs: {Hindi,Sanskrit}–Bengali, Kannada–Telugu, {Arabic,Hebrew}–Maltese, Bashkir–Tatar, Bashkir– Crimean Tatar, Armenian–Kabardian, and Russian– Portuguese. We compare our systems’ performance with the baselines using exact match accuracy over the test set. We also perform statistical significance testing using bootstrap resampling (Koehn, 2004).9 4 Experiments and Results We perform experiments both with single-language transfer as well as transfer from multiple related languages, if available. We also perform ablations in two settings, with and without hallucinated data. Transliterating the Transfer into the Test language We first focus on the setting where a 8 We direct the reader to (Anastasopoulos and Neubig, 2019) for further details on the model. 9 We use 10,000 bootstrap samples and a 12 ratio of samples in each iteration. transliteration tool between the transfer and the target language is available (in all cases, the target"
2020.sigmorphon-1.22,P19-1015,0,0.0240897,"urce language. Among many, for instance, De Gispert and Marino (2006) built a CatalanEnglish MT by bridging through Spanish, while Xia et al. (2019) show that word-level substitutions can convert a high-resource (related) language corpus into a pseudo low-resource one leading to large improvements in MT quality. Such approaches typically operate at the word level, hence they do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al., 2019) use cognate pro"
2020.sigmorphon-1.22,W18-2411,0,0.0443944,"Missing"
2020.sigmorphon-1.22,W16-2701,0,0.0269806,"Missing"
2020.sigmorphon-1.22,K17-2010,0,0.207307,"Missing"
2020.sigmorphon-1.22,W19-4226,0,0.142719,"s languages are synthetic, meaning they have rich morphology. As a result, modeling morphological inflection computationally can have a significant impact on downstream quality, not only in analysis tasks such as named entity recognition and morphological analysis (Zhu et al., 2019), but also for language generation systems for morphologically-rich languages. In recent years, morphological inflection has been extensively studied in monolingual high resource settings, especially through the recent SIGMORPHON challenges (Cotterell et al., 2016, 2017, 2018). The latest SIGMOPRHON 2019 challenge (McCarthy et al., 2019) focused on lowresource settings and encouraged cross-lingual training, an approach that has been successfully applied in other low-resource tasks such as Machine 1 Our code and data are available at https://github. com/nikim99/Inflection-Transliteration. Translation (MT) or parsing. Cross-lingual learning is a particularly promising direction, due to its potential to utilize similarities across languages (often languages from the same linguistic family, which we will refer to as “related&quot;) in order to overcome the lack of training data. In fact, leveraging data from several related languages"
2020.sigmorphon-1.22,L18-1429,0,0.0392696,"Missing"
2020.sigmorphon-1.22,W18-5818,0,0.0215694,"hereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al., 2019) use cognate projection to augment low-resource data, while (Wiemerslage et al., 2018) explore the inflection task using inputs in phonological space as well as bundles of phonological features from PanPhon (Mortensen et al., 2016), showing improvements for both settings. Our work, in contrast, focuses on better cross-lingual transfer, attempting to combine the phonological and the orthographic space. 2 Quantifying the Issue In Table 1 we offered a few examples from the literature to indicate that differences in script between the transfer and test language in a cross-lingual learning setting can be a potential issue. In this section, we provide additional evidence that this is"
2020.sigmorphon-1.22,P19-1579,1,0.825508,"ting, we convert both languages into a shared space, using grapheme-to-phoneme (G2P) conversion into the International Phonetic Alphabet (IPA) as well as romanization. We discuss both settings and their effects on morphological inflection in low-resource settings (§3). Our approach bears similarities to pseudo-corpus approaches that have been used in machine translation (MT), where low-resource language data are augmented with data generated from a related highresource language. Among many, for instance, De Gispert and Marino (2006) built a CatalanEnglish MT by bridging through Spanish, while Xia et al. (2019) show that word-level substitutions can convert a high-resource (related) language corpus into a pseudo low-resource one leading to large improvements in MT quality. Such approaches typically operate at the word level, hence they do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically"
2020.sigmorphon-1.22,K19-1021,0,0.049411,"Missing"
2020.sltu-1.48,apresjan-etal-2006-syntactically,0,0.0146223,"Missing"
2020.sltu-1.48,Q17-1010,0,0.0921251,"s appropriate when users are looking for orthographically similar, but not necessarily exactly matched strings. Word embeddings have also been successful in approximate search, finding semantic similarities between words even across languages. A word embedding is typically a vector representation, trained on large amounts (at least 1M tokens) of monolingual text, whose values reflect syntactic and semantic characteristics of the word, based on the contexts in which the word appears. These embeddings can be obtained using different algorithms, such as GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017), or BERT (Devlin et al., 2019), among many others. 4.2. Graphical User Interface (GUI) A graphical user interface is essential to the system because it makes it easy for our target users (teachers) to query the corpus. Our GUI’s design is simple and languageindependent. From the user’s perspective, the GUI workflow consists of the following steps: 1. An initial query sentence is entered into the interface 347 From the system’s perspective, this interaction requires the following steps: 1. We compute an embedding for each sentence in the entire corpus and store it along with the sentence’s tex"
2020.sltu-1.48,N19-1423,0,0.0206181,"g for orthographically similar, but not necessarily exactly matched strings. Word embeddings have also been successful in approximate search, finding semantic similarities between words even across languages. A word embedding is typically a vector representation, trained on large amounts (at least 1M tokens) of monolingual text, whose values reflect syntactic and semantic characteristics of the word, based on the contexts in which the word appears. These embeddings can be obtained using different algorithms, such as GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017), or BERT (Devlin et al., 2019), among many others. 4.2. Graphical User Interface (GUI) A graphical user interface is essential to the system because it makes it easy for our target users (teachers) to query the corpus. Our GUI’s design is simple and languageindependent. From the user’s perspective, the GUI workflow consists of the following steps: 1. An initial query sentence is entered into the interface 347 From the system’s perspective, this interaction requires the following steps: 1. We compute an embedding for each sentence in the entire corpus and store it along with the sentence’s text. 2. On receiving the user’s f"
2020.sltu-1.48,W17-0102,0,0.0708481,"Missing"
2020.sltu-1.48,P14-2050,0,0.00985133,"-ui.com). Embedding algorithms typically assume that a lot of training data is available, and getting good embeddings with a small corpus has been a challenge. Another challenge is that embeddings tend to capture primarily semantic information with some syntactic information, while we want the reverse. Our team has continued to investigate variations on algorithms that might produce the best results. Approaches under consideration include transfer learning with BERT (Devlin et al., 2019), as well as training using skip-grams over part of speech tags or dependency parses (Mikolov et al., 2013; Levy and Goldberg, 2014). We plan to continue developing embedding strategies that are performant and syntactically rich even when trained with little data, to incorporate fuzzy string matching (possibly augmented with regular expression capabilities) into our system, and to conduct human evaluations that will assess the system’s success as a search interface. 5. Social Media Recently, it has become prevalent for speakers or learners of endangered languages to interact with language on social media (Huaman and Stokes, 2011; Jones and UribeJongbloed, 2012). Previous works on developing extensions for social media incl"
2020.sltu-1.48,D15-1166,0,0.0123109,"f this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw’ida, Kwak’wala, Ojibwe, San Juan Quiahije Chatino, and Seneca. Keywords: Low-resource languages, language documentation, language revitalization 1. Introduction Recently there have been large advances in natural language processing and language technology, leading to usable systems for speech recognition (Hinton et al., 2012; Graves et al., 2013; Hannun et al., 2014; Amodei et al., 2016), machine translation (Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016), text-to-speech (van den Oord et al., 2016), and question answering (Seo et al., 2017) for a few of the world’s most-spoken languages, such as English, German, and Chinese. However, there is an urgent need for similar technology for the rest of the world’s languages, particularly those that are threatened or endangered. The rapid documentation and revitalization of these languages is of paramount importance, but all too often language technology plays little role in this process. In August 2019, the first edition of a ‘Workshop on Language Technology for Language Documentati"
2020.sltu-1.48,C16-1328,1,0.858998,"er. The universal speech recognition method has already been deployed as an app that can be used for documentation at https://www.dictate.app, but quantitative testing of its utility in an actual documentation scenario (as in Michaud et al. (2018)) is yet to be performed. Currently, Allosaurus can currently only output a phone that it has been trained to recognize, but as confused sounds frequently occur at the same place of articulation (Ng, 1998), it should be possible to find links between Allosaurus’s inventory and the provided inventory. Future work and possible integration with PanPhon (Mortensen et al., 2016) could allow the tool to adapt the output to the 343 nearest available sound (considering phonological distance) in the language’s inventory. 2.2. Phone to Orthography Decoder Ideally, one would like to convert the phones recognized by Allosaurus to native orthography. If successful, this would provide a speech recognition system that can directly recognize to the native orthography for low-resource languages, with minimal expertise and effort. Many low-resource languages have fairly new orthographies that are adapted from standard scripts. Because the orthographies are new, the phonetic value"
2020.sltu-1.48,D14-1162,0,0.0843609,"Missing"
2020.sltu-1.48,P16-1162,0,0.0098494,"our website. While some dictionaries have morphological information available that will allow for matching queries to stems or suffixes, not all do. One approach to producing automatic alternatives is to use morphological segmentation tools, including unsupervised tools that try to infer morphological boundaries from data. For Kwak’wala, we experimented with two approaches: Morfessor, a probabilistic model for doing unsupervised or semi-supervised morphological segmentation (Virpioja et al., 2013) and byte pair encoding, a compression model that can be applied to automatically segment words (Sennrich et al., 2016). We use these models to split words up into approximate roots and affixes, which are then used for search or to cluster similar words to show to users. Note that there is scope for improvement on this front, including having language speakers determine which automatic systems produce the best segmentations, exploring different ways of visualizing related words, and using these to improve dictionary search. 3.2. Inuktitut Inuktitut is a polysynthetic language spoken in Northern Canada and is an official language of the Territory of Nunavut. Inuktitut words contain many morphemes, which 345 are"
2020.tacl-1.28,P19-1279,0,0.068789,"Missing"
2020.tacl-1.28,P17-1080,0,0.0179717,"y for the most probable B objects given by the forward probability at both training and test time. As shown in Table 15, the improvement resulting from backward probability is small, indicating that a diversity-promoting scoring function might not be necessary for knowledge retrieval from LMs. 7 Related Work Much work has focused on understanding the internal representations in neural NLP models (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017). For contextualized representations in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a; Tenney et al., 2019b; Jawahar et al., 2019; Goldberg, 2019). Different from analyses probing the representations themselves, our work follows Petroni et al. (2019); P¨orner et al. (2019) in probing for fa"
2020.tacl-1.28,Q19-1004,0,0.0225869,", tr,i ) of each prompt to our optimization-based scoring function in Equation 3. Due to the large search space for objects, we turn to an approximation approach that only computes backward probability for the most probable B objects given by the forward probability at both training and test time. As shown in Table 15, the improvement resulting from backward probability is small, indicating that a diversity-promoting scoring function might not be necessary for knowledge retrieval from LMs. 7 Related Work Much work has focused on understanding the internal representations in neural NLP models (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017). For contextualized representations in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a; Tenney"
2020.tacl-1.28,2005.eamt-1.15,0,0.138024,"Missing"
2020.tacl-1.28,D19-1633,0,0.0251135,"ext of making small changes to an existing input for adversarial attacks (Ebrahimi et al., 2018; Wallace et al., 2019). However, we found that directly optimizing prompts guided by gradients was unstable and often yielded prompts in unnatural English in our preliminary experiments. Thus, we instead resorted to a more straightforward hillclimbing method that starts with an initial prompt, then masks out one token at a time and replaces it with the most probable token conditioned on the other tokens, inspired by the mask-predict decoding algorithm used in non-autoregressive machine translation (Ghazvininejad et al., 2019):9 PLM (wi |tr  i) = P hx,y i∈R PLM (wi |x, tr |R|  i, y ) , where wi is the i-th token in the prompt and tr  i is the prompt with the i-th token masked out. We followed a simple rule that modifies a prompt from left to right, and this is repeated until convergence. We used this method to refine all the mined and manual prompts on the T-REx-train dataset, and display theirperformance on the T-REx dataset in Table 14. After fine-tuning, the oracle performance increased significantly, while the ensemble performances (both rank-based and optimizationbased) dropped slightly. This indicates that"
2020.tacl-1.28,N19-1423,0,0.0730632,"hive (LPAQA) at https://github.com/jzbjyb/LPAQA. 1 Introduction Recent years have seen the primary role of language models (LMs) transition from generating or evaluating the fluency of natural text (Mikolov and Zweig, 2012; Merity et al., 2018; Melis et al., 2018; Gamon et al., 2005) to being a powerful tool for text understanding. This understanding has mainly been achieved through the use of language modeling as a pre-training task for feature extractors, where the hidden vectors learned through a language modeling objective are then used in ∗ 1 Some models we use in this paper, e.g., BERT (Devlin et al., 2019), are bi-directional, and do not directly define probability distribution over text, which is the underlying definition of an LM. Nonetheless, we call them LMs for simplicity. The first two authors contributed equally. 423 Transactions of the Association for Computational Linguistics, vol. 8, pp. 423–438, 2020. https://doi.org/10.1162/tacl a 00324 Action Editor: Timothy Baldwin. Submission batch: 12/2019; Revision batch: 3/2020; Published 7/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. BERT-large as well. We further demonstrate that using a dive"
2020.tacl-1.28,P18-2006,0,0.0231011,"Missing"
2020.tacl-1.28,N19-1419,0,0.179632,"query BERT. Correct answer is underlined. 2 Knowledge Retrieval from LMs in LMs, and in fact, LMs may be even more knowledgeable than these initial results indicate. In this paper we ask the question: ‘‘How can we tighten this lower bound and get a more accurate estimate of the knowledge contained in state-of-the-art LMs?’’ This is interesting both scientifically, as a probe of the knowledge that LMs contain, and from an engineering perspective, as it will result in higher recall when using LMs as part of a knowledge extraction system. In particular, we focus on the setting of Petroni et al. (2019) who examine extracting knowledge regarding the relations between entities (definitions in § 2). We propose two automatic methods to systematically improve the breadth and quality of the prompts used to query the existence of a relation (§ 3). Specifically, as shown in Figure 1, these are mining-based methods inspired by previous relation extraction methods (Ravichandran and Hovy, 2002), and paraphrasing-based methods that take a seed prompt (either manually created or automatically mined), and paraphrase it into several other semantically similar expressions. Further, because different prompt"
2020.tacl-1.28,L18-1544,0,0.0643848,"Missing"
2020.tacl-1.28,D17-1014,0,0.0322168,"Missing"
2020.tacl-1.28,P19-1598,0,0.0770776,"Missing"
2020.tacl-1.28,K16-1006,0,0.0401401,"Missing"
2020.tacl-1.28,P19-1356,0,0.0152566,"ng extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017). For contextualized representations in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a; Tenney et al., 2019b; Jawahar et al., 2019; Goldberg, 2019). Different from analyses probing the representations themselves, our work follows Petroni et al. (2019); P¨orner et al. (2019) in probing for factual knowledge. They use manually defined prompts, which may be under-estimating the true performance obtainable by LMs. Concurrently to this work, Bouraoui et al. (2020) made a similar observation that using different prompts can help better extract relational knowledge from LMs, but they use models explicitly trained for relation extraction whereas our methods examine the knowledge included in LMs without any additional training. O"
2020.tacl-1.28,N16-1014,0,0.104057,"Missing"
2020.tacl-1.28,Q16-1037,0,0.0282736,"s backward probability for the most probable B objects given by the forward probability at both training and test time. As shown in Table 15, the improvement resulting from backward probability is small, indicating that a diversity-promoting scoring function might not be necessary for knowledge retrieval from LMs. 7 Related Work Much work has focused on understanding the internal representations in neural NLP models (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017). For contextualized representations in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a; Tenney et al., 2019b; Jawahar et al., 2019; Goldberg, 2019). Different from analyses probing the representations themselves, our work follows Petroni et al. (2019); P¨orner et al."
2020.tacl-1.28,E17-1083,0,0.0159832,"ould be used for paraphrasing (Romano et al., 2006; Bhagat and Ravichandran, 2008), we follow the simple Middle-word Prompts Following the observation that words in the middle of the subject and object are often indicative of the relation, we 3 We restrict to masked LMs in this paper because the missing slot might not be the last token in the sentence and computing this probability in traditional left-to-right LMs using Bayes’ theorem is not tractable. 425 probabilities4 from the top K prompts to calculate the probability of the object: method of using back-translation (Sennrich et al., 2016; Mallinson et al., 2017) to first translate the initial prompt into B candidates in another language, each of which is then back-translated into B candidates in the original language. We then rank B 2 candidates based on their roundtrip probability (i.e., Pforward (t¯|tˆ) · Pbackward (t|t¯), where tˆ is the initial prompt, t¯ is the translated prompt in the other language, and t is the final prompt), and keep the top T prompts. K X 1 log PLM (y |x, tr,i ), s(y |x, r ) = K (1) i=1 P (y |x, r ) = softmax(s(·|x, r ))y , (2) where tr,i is the prompt ranked at the i-th position. Here, K is a hyper-parameter, where a small"
2020.tacl-1.28,W19-5333,0,0.228215,"query BERT. Correct answer is underlined. 2 Knowledge Retrieval from LMs in LMs, and in fact, LMs may be even more knowledgeable than these initial results indicate. In this paper we ask the question: ‘‘How can we tighten this lower bound and get a more accurate estimate of the knowledge contained in state-of-the-art LMs?’’ This is interesting both scientifically, as a probe of the knowledge that LMs contain, and from an engineering perspective, as it will result in higher recall when using LMs as part of a knowledge extraction system. In particular, we focus on the setting of Petroni et al. (2019) who examine extracting knowledge regarding the relations between entities (definitions in § 2). We propose two automatic methods to systematically improve the breadth and quality of the prompts used to query the existence of a relation (§ 3). Specifically, as shown in Figure 1, these are mining-based methods inspired by previous relation extraction methods (Ravichandran and Hovy, 2002), and paraphrasing-based methods that take a seed prompt (either manually created or automatically mined), and paraphrase it into several other semantically similar expressions. Further, because different prompt"
2020.tacl-1.28,N18-1202,0,0.0488087,"Missing"
2020.tacl-1.28,D19-1005,0,0.05589,"Missing"
2020.tacl-1.28,D19-1250,0,0.126407,"Missing"
2020.tacl-1.28,P16-1009,0,0.0453497,"Although many methods could be used for paraphrasing (Romano et al., 2006; Bhagat and Ravichandran, 2008), we follow the simple Middle-word Prompts Following the observation that words in the middle of the subject and object are often indicative of the relation, we 3 We restrict to masked LMs in this paper because the missing slot might not be the last token in the sentence and computing this probability in traditional left-to-right LMs using Bayes’ theorem is not tractable. 425 probabilities4 from the top K prompts to calculate the probability of the object: method of using back-translation (Sennrich et al., 2016; Mallinson et al., 2017) to first translate the initial prompt into B candidates in another language, each of which is then back-translated into B candidates in the original language. We then rank B 2 candidates based on their roundtrip probability (i.e., Pforward (t¯|tˆ) · Pbackward (t|t¯), where tˆ is the initial prompt, t¯ is the translated prompt in the other language, and t is the final prompt), and keep the top T prompts. K X 1 log PLM (y |x, tr,i ), s(y |x, r ) = K (1) i=1 P (y |x, r ) = softmax(s(·|x, r ))y , (2) where tr,i is the prompt ranked at the i-th position. Here, K is a hyper"
2020.tacl-1.28,D16-1159,0,0.0468737,"Missing"
2020.tacl-1.28,E17-1117,1,0.840261,"in Table 15, the improvement resulting from backward probability is small, indicating that a diversity-promoting scoring function might not be necessary for knowledge retrieval from LMs. 7 Related Work Much work has focused on understanding the internal representations in neural NLP models (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017). For contextualized representations in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a; Tenney et al., 2019b; Jawahar et al., 2019; Goldberg, 2019). Different from analyses probing the representations themselves, our work follows Petroni et al. (2019); P¨orner et al. (2019) in probing for factual knowledge. They use manually defined prompts, which may be under-estimating the true performance ob"
2020.tacl-1.28,P19-1487,0,0.0467438,"Missing"
2020.tacl-1.28,P19-1452,0,0.0158808,"ls (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017). For contextualized representations in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a; Tenney et al., 2019b; Jawahar et al., 2019; Goldberg, 2019). Different from analyses probing the representations themselves, our work follows Petroni et al. (2019); P¨orner et al. (2019) in probing for factual knowledge. They use manually defined prompts, which may be under-estimating the true performance obtainable by LMs. Concurrently to this work, Bouraoui et al. (2020) made a similar observation that using different prompts can help better extract relational knowledge from LMs, but they use models explicitly trained for relation extraction whereas our methods examine the knowledge inclu"
2020.tacl-1.28,P02-1006,0,0.26312,"as a probe of the knowledge that LMs contain, and from an engineering perspective, as it will result in higher recall when using LMs as part of a knowledge extraction system. In particular, we focus on the setting of Petroni et al. (2019) who examine extracting knowledge regarding the relations between entities (definitions in § 2). We propose two automatic methods to systematically improve the breadth and quality of the prompts used to query the existence of a relation (§ 3). Specifically, as shown in Figure 1, these are mining-based methods inspired by previous relation extraction methods (Ravichandran and Hovy, 2002), and paraphrasing-based methods that take a seed prompt (either manually created or automatically mined), and paraphrase it into several other semantically similar expressions. Further, because different prompts may work better when querying for different subjectobject pairs, we also investigate lightweight ensemble methods to combine the answers from different prompts together (§ 4). We experiment on the LAMA benchmark (Petroniet al., 2019), which is an English-language benchmark devised to test the ability of LMs to retrieve relations between entities (§ 5). We first demonstrate that improv"
2020.tacl-1.28,E06-1052,0,0.0352694,"exical diversity while remaining relatively faithful to the original prompt. Specifically, we do so by performing paraphrasing over the original prompt into other semantically similar or identical expressions. For example, if our original prompt is ‘‘x shares a border with y ’’, it may be paraphrased into ‘‘x has a common border with y ’’ and ‘‘x adjoins y ’’. This is conceptually similar to query expansion techniques used in information retrieval that reformulate a given query to improve retrieval performance (Carpineto and Romano, 2012). Although many methods could be used for paraphrasing (Romano et al., 2006; Bhagat and Ravichandran, 2008), we follow the simple Middle-word Prompts Following the observation that words in the middle of the subject and object are often indicative of the relation, we 3 We restrict to masked LMs in this paper because the missing slot might not be the last token in the sentence and computing this probability in traditional left-to-right LMs using Bayes’ theorem is not tractable. 425 probabilities4 from the top K prompts to calculate the probability of the object: method of using back-translation (Sennrich et al., 2016; Mallinson et al., 2017) to first translate the ini"
2020.tacl-1.28,D17-1197,0,0.077228,"Missing"
2020.tacl-1.28,D15-1174,0,0.0422928,"Missing"
2020.tacl-1.28,P19-1139,0,0.0691191,"Missing"
2020.tacl-1.28,D19-1221,0,0.0446368,"Missing"
2020.tacl-1.42,P12-2004,0,0.0509864,"Missing"
2020.tacl-1.42,W98-1505,0,0.155923,"Missing"
2020.tacl-1.42,W12-0503,0,0.0323916,"Missing"
2020.tacl-1.42,P05-1022,0,0.123771,"Missing"
2020.tacl-1.42,D18-1160,1,0.594563,"Technologies Institute, Carnegie Mellon University zhuhao@cmu.edu, {ybisk, gneubig}@cs.cmu.edu Abstract syntactic formulation (Yuret, 1998; Carroll and Charniak, 1992; Paskin, 2002). Specifically, the dependency model with valence (DMV) (Klein and Manning, 2004) forms the basis for many modern approaches in dependency induction. Most recent models for grammar induction, be they for PCFGs, DMVs, or other formulations, have generally coupled these models with some variety of neural model to use embeddings to capture word similarities, improve the flexibility of model parameterization, or both (He et al., 2018; Jin et al., 2019; Kim et al., 2019; Han et al., 2019). Notably, the two different syntactic formalisms capture very different views of syntax. Phrase structure takes advantage of an abstracted recursive view of language, while the dependency structure more concretely focuses on the propensity of particular words in a sentence to relate to each other syntactically. However, few attempts at unsupervised grammar induction have been made to marry the two and let both benefit each other. This is precisely the issue we attempt to tackle in this paper. As a specific formalism that allows us to mode"
2020.tacl-1.42,J03-4003,0,0.554994,"rmalisms capture very different views of syntax. Phrase structure takes advantage of an abstracted recursive view of language, while the dependency structure more concretely focuses on the propensity of particular words in a sentence to relate to each other syntactically. However, few attempts at unsupervised grammar induction have been made to marry the two and let both benefit each other. This is precisely the issue we attempt to tackle in this paper. As a specific formalism that allows us to model both formalisms at once, we turn to lexicalized probabilistic context-free grammars (L-PCFGs; Collins, 2003). L-PCFGs borrow the underlying machinery from PCFGs but expand the grammar by allowing rules to include information about the lexical heads of each phrase, an example of which is shown in Figure 1. The head annotation in the L-PCFG provides lexical dependencies that can be informative in estimating the probabilities of generation rules. For example, the probability of VP[CHASING] → VBZ[IS] VP[CHASING] is much higher than VP → VBZ VP, because ‘‘chasing’’ is a present participle. Historically, these grammars have been mostly used for supervised parsing, combined with traditional count-based est"
2020.tacl-1.42,P02-1043,0,0.304627,"Missing"
2020.tacl-1.42,P19-1228,0,0.104959,"ellon University zhuhao@cmu.edu, {ybisk, gneubig}@cs.cmu.edu Abstract syntactic formulation (Yuret, 1998; Carroll and Charniak, 1992; Paskin, 2002). Specifically, the dependency model with valence (DMV) (Klein and Manning, 2004) forms the basis for many modern approaches in dependency induction. Most recent models for grammar induction, be they for PCFGs, DMVs, or other formulations, have generally coupled these models with some variety of neural model to use embeddings to capture word similarities, improve the flexibility of model parameterization, or both (He et al., 2018; Jin et al., 2019; Kim et al., 2019; Han et al., 2019). Notably, the two different syntactic formalisms capture very different views of syntax. Phrase structure takes advantage of an abstracted recursive view of language, while the dependency structure more concretely focuses on the propensity of particular words in a sentence to relate to each other syntactically. However, few attempts at unsupervised grammar induction have been made to marry the two and let both benefit each other. This is precisely the issue we attempt to tackle in this paper. As a specific formalism that allows us to model both formalisms at once, we turn t"
2020.tacl-1.42,J93-2004,0,0.0699426,"Missing"
2020.tacl-1.42,P13-1028,0,0.0615925,"Missing"
2020.tacl-1.42,P02-1017,0,0.804761,"Missing"
2020.tacl-1.42,P11-1108,0,0.227824,"Missing"
2020.tacl-1.42,D11-1118,0,0.325539,"Missing"
2020.tacl-1.42,W05-1512,0,0.174511,"Missing"
2020.tacl-1.42,N10-1116,0,0.0308048,"Missing"
2020.tacl-1.42,D13-1204,0,0.287495,"Missing"
2020.tacl-1.42,W18-2704,1,0.850082,"Missing"
2020.tacl-1.42,P17-1026,0,0.0487593,"Missing"
2020.tacl-1.42,C88-2121,0,0.622384,"Missing"
2020.tacl-1.42,P07-1049,0,0.346474,"Missing"
2020.tacl-1.8,P18-1073,0,0.040963,"Missing"
2020.tacl-1.8,W19-2804,0,0.221558,"Missing"
2020.tacl-1.8,E06-1002,0,0.0799483,"Missing"
2020.tacl-1.8,D18-1024,0,0.0613179,"Missing"
2020.tacl-1.8,D11-1072,0,0.317046,"Missing"
2020.tacl-1.8,D17-1078,0,0.032933,"Missing"
2020.tacl-1.8,D07-1074,0,0.307549,"Missing"
2020.tacl-1.8,P04-1021,0,0.305166,"Missing"
2020.tacl-1.8,C10-1032,0,0.17316,"Missing"
2020.tacl-1.8,D17-1277,0,0.0432194,"erformance of candidate generation is measured by the gold candidate recall, which is the proportion of retrieved candidate lists that contains the correct entity. It is critical that this number is high, as any time the correct entity is excluded, the disambiguation model will be unable to recover it. Formally, if we denote the correct entity of each mention m as eˆ, the gold candidate recall r is defined as: r= PN ei i=1 δ (ˆ ∈ ei ) N where δ(·) is the indicator function, which is 1 if true else 0, and N is the total number of mentions among all documents. We follow Yamada et al. (2017) and Ganea and Hofmann (2017) to ignore 111 mentions whose linked entity does not exist in the KB in this work.3 We use ‘‘EN’’ to denote the target language English, ‘‘HRL’’ to denote any high-resource language and ‘‘LRL’’ to denote any low-resource language. For example, KHRL is a KB in an HRL (e.g., Spanish Wikipedia), eHRL is an entity in KHRL . Because our focus is on low-resource XEL, the source language is always an LRL. We also refer to the HRL as the ‘‘pivoting’’ language below. 2.2 Baseline Candidate Generation Models In this section, we introduce two existing categories of techniques for candidate generation. Di"
2020.tacl-1.8,P16-1059,0,0.0488408,"Missing"
2020.tacl-1.8,P08-1088,0,0.0263698,"Missing"
2020.tacl-1.8,I11-1029,0,0.243793,"Missing"
2020.tacl-1.8,D19-1680,0,0.0329934,"Missing"
2020.tacl-1.8,L18-1429,0,0.0619928,"pout is set to 0.5.9 For each training language, we set aside a small subset of training data (mHRL − eEN ) as our development set. For all models, we stop training if top-30 gold candidate recall on the development set does not increase for 50 epochs, and the maximum number of training epochs is set to 200. We select the HRL that has the highest character n-gram overlap with the source LRL, a decision we discuss more in §5.4. Rijhwani et al. (2019) used phoneme-based representations to help deal with the fact that different languages use different scripts, and we do so as well using Epitran (Mortensen et al., 2018) to convert strings to international phonetic alphabet (IPA) symbols. The selection of the HRL and the representation of each LRL is shown in Table 2. Epitran has relatively wide and growing coverage (55 languages at the time of this writing). Our method could also potentially be used with other tools such as the Romanizer uroman,10 which is a less accurate phonetic representation than Epitran but covers most languages in the world. However, testing different romanizers is somewhat orthogonal to the main claims of this paper, and thus we have not explicitly performed experiments on this. Our H"
2020.tacl-1.8,P17-1178,0,0.0739865,"Missing"
2020.tacl-1.8,L16-1521,0,0.199314,"Missing"
2020.tacl-1.8,N18-1167,0,0.0495691,"Missing"
2020.tacl-1.8,N12-1052,0,0.103568,"Missing"
2020.tacl-1.8,P19-1015,0,0.0611527,"Missing"
2020.tacl-1.8,N16-1072,0,0.285922,"Missing"
2020.tacl-1.8,D18-1270,0,0.689637,"n batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. et al., 2018). String similarity (e.g., edit distance) cannot easily extend to XEL because surface forms of entities often differ significantly across the source and target language, particularly when the languages are in different scripts. Wikipedia link methods can be extended to XEL by using inter-language links between the two languages to redirect entities to the English KB (Spitkovsky and Chang, 2012; Sil and Florian, 2016; Sil et al., 2018; Upadhyay et al., 2018a). This method works to some extent, but often under-performs on lowresource languages due to the lack of source language Wikipedia resources. Although scarce, there are some methods that propose to improve entity candidate generation by training translation models with low resourcelanguage (LRL)-English entity gazetteers (Pan et al., 2017), or learning neural string matching models based on an entity gazetteer in a related high-resource language (HRL) which is then applied to the LRL (Rijhwani et al., 2019) (more in §2). However, even with these relatively sophisticated methods, top-30 candi"
2020.tacl-1.8,P16-1213,0,0.384956,"03 Action Editor: Radu Florian. Submission batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. et al., 2018). String similarity (e.g., edit distance) cannot easily extend to XEL because surface forms of entities often differ significantly across the source and target language, particularly when the languages are in different scripts. Wikipedia link methods can be extended to XEL by using inter-language links between the two languages to redirect entities to the English KB (Spitkovsky and Chang, 2012; Sil and Florian, 2016; Sil et al., 2018; Upadhyay et al., 2018a). This method works to some extent, but often under-performs on lowresource languages due to the lack of source language Wikipedia resources. Although scarce, there are some methods that propose to improve entity candidate generation by training translation models with low resourcelanguage (LRL)-English entity gazetteers (Pan et al., 2017), or learning neural string matching models based on an entity gazetteer in a related high-resource language (HRL) which is then applied to the LRL (Rijhwani et al., 2019) (more in §2). However, even with these relat"
2020.tacl-1.8,D18-1046,0,0.540546,"n batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. et al., 2018). String similarity (e.g., edit distance) cannot easily extend to XEL because surface forms of entities often differ significantly across the source and target language, particularly when the languages are in different scripts. Wikipedia link methods can be extended to XEL by using inter-language links between the two languages to redirect entities to the English KB (Spitkovsky and Chang, 2012; Sil and Florian, 2016; Sil et al., 2018; Upadhyay et al., 2018a). This method works to some extent, but often under-performs on lowresource languages due to the lack of source language Wikipedia resources. Although scarce, there are some methods that propose to improve entity candidate generation by training translation models with low resourcelanguage (LRL)-English entity gazetteers (Pan et al., 2017), or learning neural string matching models based on an entity gazetteer in a related high-resource language (HRL) which is then applied to the LRL (Rijhwani et al., 2019) (more in §2). However, even with these relatively sophisticated methods, top-30 candi"
2020.tacl-1.8,spitkovsky-chang-2012-cross,0,0.0848577,"Missing"
2020.tacl-1.8,W16-1403,0,0.0635414,"Missing"
2020.tacl-1.8,N16-1156,0,0.059149,"Missing"
2020.tacl-1.8,D16-1157,1,0.8678,"arce, so we did not attempt to expand entity names on the HRL side. 4.3 More Explicit String Encoding As mentioned previously, while BI-LSTMS have proven powerful in modeling sequential data in the literature, we argue that they are not an ideal string encoder for this setting. This is because our training data contain a nontrival number of pairs that contains less predictable word mappings (e.g., translations). With such large freedom in the face of insufficient and noisy training data, this encoder seemingly overfits, resulting in poor generalization. Previous researchers (Dai and Le, 2015; Wieting et al., 2016a) have noticed similar problems when using LSTMs for representation learning. As an alternative, we propose the use of the CHARAGRAM model (Wieting et al., 2016) as the string encoder. This model scans the string with various window sizes and produces a bag of character n-grams. It then maps these n-grams to their corresponding embeddings through a lookup table. The final embedding of the string is the sum of all the n-gram embeddings followed by a nonlinear activation function. Figure 3 shows an illustration of the model. Formally, we denote a string as a sequence of characters x = [x1 , x2"
2020.tacl-1.8,D19-6127,1,0.694919,"fferent words. One thing that is worth mentioning is that CHARAGRAM is able to correctly recognize some mappings of nontransliterated words. For instance, ‘‘Jaamacadda’’ in so is the parallel of ‘‘University’’ in English, and the model was able to correctly align n-grams corresponding to these words. This result demonstrates one way how CHARAGRAM alleviates the TRANS error that BI-LSTM suffers from. 5.6 Improving End-to-end XEL Systems To investigate how our candidate generation model influences the end-to-end XEL system, we use its candidate lists in the disambiguation model BURN proposed by Zhou et al. (2019). BURN creates a fully connected graph for each document and performs joint inference on all mentions in the document. To the best of our knowledge, it is currently the disambiguation model that has demonstrated the strongest empirical results for XEL without any targeted LRL resources. Therefore, we believe it is the most reasonable choice in our low-resource scenario. For details, 5.5 Properties of Learned n-grams As discussed in the previous sections, the objective of CHARAGRAM is to learn n-gram mappings 119 we encourage readers to refer to the original paper.14 To make the best use of sca"
2020.tacl-1.8,Q17-1028,0,\N,Missing
2020.tacl-1.8,J98-4003,0,\N,Missing
2020.wmt-1.4,abdelali-etal-2014-amara,1,0.827823,"ne translated all comments using an in-house transformer-based model into Japanese and German. The goal of that was to be able to examine potential differences in source and (one example of) translation segments.3 We then pre-processed and automatically annotated all 17K segments with the following soft labels for catastrophic errors: Development Data The task specified the following data to help participants evaluate their system’s performance on unseen and multiple domains. • English-German: participants can use the development data from the News translation task, development data from QED (Abdelali et al., 2014) corpus, development data from WMT19 Medical translation task, and development data from the WMT16 IT translation task. 1. Introduction of toxicity: we checked both source and machine translation for toxic words (using in-house lists) and labelled as positive (i.e. potentially containing errors) cases where the source does not contain such words, but the translation does (at least one). • English-Japanese: participants can use the development data from the News translation task, and development data from the MTNT dataset, which contains noisy social media texts and their clean translations. 3."
2020.wmt-1.4,D17-1158,0,0.0130333,"Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The"
2020.wmt-1.4,N19-1311,0,0.0437239,"Missing"
2020.wmt-1.4,P18-1128,0,0.014933,"Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the constrained, zero-shot submissions. To get insight on the proportion of"
2020.wmt-1.4,2020.lrec-1.520,0,0.0937737,"raped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient information. 2 Poor: the target text contains translation errors,"
2020.wmt-1.4,N19-1154,1,0.82504,"luation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Do"
2020.wmt-1.4,D19-1165,0,0.282315,"n a critical way. Critical errors are those that lead to misleading translations which may carry religious, health, safety, legal or financial implications, or introduce toxicity. The set of critical errors used for the guidelines (which also included examples of these errors) includes – but is not limited to – the cases below: Naver Labs (NLE): They participated in Chat and Biomedical tasks along with the Robustness task. They trained a general big-transformer model using FairSeq toolkit (Ott et al., 2019) and adapted it towards different tasks using lightweight adapter layers for each task (Bapna and Firat, 2019). They compared results against the more traditional finetuning method (Luong and Manning, 2015) to show that the former provides a viable alternative, while significantly reducing the amount of parameters per task. They also explored using embedding from pre-trained language models in their NMT system of which they tried two MLM variants: i) using NMT encoder’s setting, using Roberta (Liu et al., 2019). The latter was found more robust to novel domains and noise. The authors found that initializing with first 8 layers instead of the entire model to 80 OPPO: Team OPPO also trained their system"
2020.wmt-1.4,D19-1632,1,0.894086,"Missing"
2020.wmt-1.4,W17-4712,0,0.0183942,"ne-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively u"
2020.wmt-1.4,W17-3205,0,0.0203483,"to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task pr"
2020.wmt-1.4,P17-2061,0,0.0561223,"Missing"
2020.wmt-1.4,C18-1111,0,0.0137039,"6). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopo"
2020.wmt-1.4,W18-6478,0,0.0263186,"different models to obtain further improvements. Only two teams, namely Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the cons"
2020.wmt-1.4,P17-4012,0,0.0234194,"the decoder. This allows the test sets from known domains to use adapter layers and for novel domains to use the generic system. They created a noisy domain by adding synthetic noise to source data. The idea is that residual adapter layer learned from such data learns how to deal with noisy domain and is also able to preserve performance on the cleaner domains. However this did not work as well. The residual adapter fine-tuned using the ParaCrawl corpus gave better performance. PROMPT: Team PROMPT also participated mainly in the News translation task. Their systems were trained using OpenNMT (Klein et al., 2017) toolkit. They applied several stages of data preprocessing including length-based filtering, removing duplications, and using in-house classifier based on Hunalign aligner to identify and discard non-parallel sentences. They used two types of synthetic data to improve their systems: i) randomly selecting subset of Wikipedia equal to the size of news data and generating parallel corpus through back-translation, ii) creating synthetic data with unknown words using the procedure described in (Pinnis et al., 2017). Systems were trained with tags to differentiate between original data and syntheti"
2020.wmt-1.4,P02-1040,0,0.114721,"l can bias the selection to cases that are challenging for this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professi"
2020.wmt-1.4,W17-3204,1,0.837046,"e evaluated both automatically and via a human evaluation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in"
2020.wmt-1.4,W18-6459,0,0.0133196,"rvey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on on"
2020.wmt-1.4,W19-5303,1,0.901081,"e 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predomin"
2020.wmt-1.4,W18-6319,0,0.0156248,"or this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professional translators. The translators were presen"
2020.wmt-1.4,E17-2045,0,0.0383395,"o domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of mono"
2020.wmt-1.4,2021.ccl-1.108,0,0.105338,"Missing"
2020.wmt-1.4,P16-1162,0,0.0120732,"ns at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously"
2020.wmt-1.4,2015.iwslt-evaluation.11,0,0.64331,"ims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine translation (Koehn and Knowles, 2017). Most approaches for improving robustness of MT systems to domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approac"
2020.wmt-1.4,N19-1314,1,0.844424,"hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine transla"
2020.wmt-1.4,D18-1050,1,0.940705,"tation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on one or both these language p"
2020.wmt-1.4,2020.lrec-1.517,1,0.814412,"comments from the social media website reddit.com were scraped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient inform"
2020.wmt-1.4,N19-4007,1,0.808507,"as well as an analysis of catastrophic errors (Section 5.2). 5.1 General Quality Overall, the correlation between human judgments and BLEU is not strong. For En→De (set1), the Pearson’s correlation coefficient is 0.97, while for the other four tasks the coefficients are lower, with 0.78, 0.65, 0.52, 0.79 for En→De (set1), Ja→En (set2), En→Ja (set2), and De→En (set3) respectively. Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in Table 2, where we also include the three online translation systems. We performed significance test using compare-mt (Neubig et al., 2019) where systems are considered as significantly different at p <0.05. The result of significance test is used for the automatic evaluation ranking. Overall, the unconstrained online-B system provides strong results and outperforms most systems in the five language pairs, except the De→En (set3) and En→Ja (set1). Among the participating teams, the best zeroshot systems were OPPO, which outperforms other zero-shot systems in En→De (set1), Ja→En (set2), and En→Ja (set2) tasks, and NLE, which outperforms other systems in the other two tasks. Only Naver Labs participated in the few-shot stage (NLE-f"
2020.wmt-1.4,P17-2089,0,0.0494679,"Missing"
2020.wmt-1.4,D17-1155,0,0.0161114,"Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at traini"
2020.wmt-1.4,D17-1147,0,0.0351203,"Missing"
2020.wmt-1.4,D16-1160,0,0.025774,"ples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et"
2021.acl-demo.34,N19-1078,0,0.0266042,"s. A B OARD 4.1 Experimental Setup Attribute Definition We define attributes following Fu et al. (2020a) and three of them are used below: entity length, sentence length and label of entity. Collection of Systems Outputs Currently, we collect system outputs by either implementing them by ourselves or collecting from other researchers 12 http://www.statmt.org/wmt20/ metrics-task.html Analysis using ExplainaBoard Fig. 2 illustrates different types of results driven by four functionality buttons13 over the top-3 NER systems: LUKE (Yamada et al., 2020), FLERT (Schweter and Akbik, 2020) and FLAIR (Akbik et al., 2019). Box A breaks down the performance of the top-1 system over different attributes.14 We can intuitively observe that even the state-of-the-art system does worse on longer entities. Users can further print error cases in the longer entity bucket by clicking the corresponding bin. Box B shows the 1st system’s (LUKE) performance minus the 2nd system’s (FLERT) performance. We can see that although LUKE surpasses FLERT holistically, it performs worse when dealing with PERSON entities. Box C identifies samples that all systems mispredict. Further analysis of these samples uncovers challenging patter"
2021.acl-demo.34,P05-1001,0,0.0441952,"andard in NLP to release the system outputs that E XPLAINA B OARD needs. 283 classification (Pang et al., 2002), topic identification (Wang and Manning, 2012), and intention detection (Chen et al., 2013). Task Text Pair Classification Prediction of a class given two texts, such as the natural language inference task (Bowman et al., 2015). Sequence Labeling Prediction of a label for each token in a sequence. The E XPLAINA B OARD currently includes four concrete tasks: named entity recognition (Tjong Kim Sang and De Meulder, 2003), part-of-speech tagging (Toutanova et al., 2003), text chunking (Ando and Zhang, 2005), and Chinese word segmentation (Chen et al., 2015). 8 40 2 Topic 4 18 2 Intention 1 3 2 Text-Span Classification Aspect Sentiment 4 20 4 Text Pair Classification NLI 2 6 7 NER 3 74 9 Text Classification Text-Span Classification Prediction of a predefined class from the input of a text and a span, such as aspect-based sentiment classification task (Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2"
2021.acl-demo.34,D13-1160,0,0.0107544,"t classification task (Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2 36 7 Translation 4 60 9 Table 2: Brief descriptions of tasks, datasets and systems that E XPLAINA B OARD currently supports. “Attr.” denotes Attribute. “Pred.” denotes “Prediction”. Structure Prediction Prediction of a syntactic or semantic structure from text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) wh"
2021.acl-demo.34,2020.emnlp-main.751,1,0.728601,"text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from machine translation are collected from the WMT20.12 4.2 4 Case Study Here, we briefly showcase the actual E XPLAIN interface through a case study on analyzing state-of-the-art NER systems. A B OARD 4.1 Experimental Setup Attribute Definition We define attributes following Fu et al. (2020a) and three of them are used below: entity length, sentence length and label of entity. Collection of Systems Outputs Currently, we collect system outputs by either implementing them by ourselves or collecting from other resea"
2021.acl-demo.34,D15-1075,0,0.0257375,"low, and show high-level statistics in Tab. 2. Text Classification Prediction of one or multiple pre-defined label(s) for a given input text. The current interface includes datasets for sentiment 11 265 of these models are implemented by us, as unfortunately it is currently not standard in NLP to release the system outputs that E XPLAINA B OARD needs. 283 classification (Pang et al., 2002), topic identification (Wang and Manning, 2012), and intention detection (Chen et al., 2013). Task Text Pair Classification Prediction of a class given two texts, such as the natural language inference task (Bowman et al., 2015). Sequence Labeling Prediction of a label for each token in a sequence. The E XPLAINA B OARD currently includes four concrete tasks: named entity recognition (Tjong Kim Sang and De Meulder, 2003), part-of-speech tagging (Toutanova et al., 2003), text chunking (Ando and Zhang, 2005), and Chinese word segmentation (Chen et al., 2015). 8 40 2 Topic 4 18 2 Intention 1 3 2 Text-Span Classification Aspect Sentiment 4 20 4 Text Pair Classification NLI 2 6 7 NER 3 74 9 Text Classification Text-Span Classification Prediction of a predefined class from the input of a text and a span, such as aspect-base"
2021.acl-demo.34,D15-1141,1,0.793763,"AINA B OARD needs. 283 classification (Pang et al., 2002), topic identification (Wang and Manning, 2012), and intention detection (Chen et al., 2013). Task Text Pair Classification Prediction of a class given two texts, such as the natural language inference task (Bowman et al., 2015). Sequence Labeling Prediction of a label for each token in a sequence. The E XPLAINA B OARD currently includes four concrete tasks: named entity recognition (Tjong Kim Sang and De Meulder, 2003), part-of-speech tagging (Toutanova et al., 2003), text chunking (Ando and Zhang, 2005), and Chinese word segmentation (Chen et al., 2015). 8 40 2 Topic 4 18 2 Intention 1 3 2 Text-Span Classification Aspect Sentiment 4 20 4 Text Pair Classification NLI 2 6 7 NER 3 74 9 Text Classification Text-Span Classification Prediction of a predefined class from the input of a text and a span, such as aspect-based sentiment classification task (Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2 36 7 Translation 4 60 9 Table 2: Brief description"
2021.acl-demo.34,N13-1124,0,0.0164703,"Missing"
2021.acl-demo.34,2021.naacl-main.146,1,0.797971,"Missing"
2021.acl-demo.34,I11-1153,0,0.0410927,"be useful to identify challenging samples or even annotation errors. 2. In single-system analysis, users can choose particular buckets in the performance histogram9 and see corresponding error samples in that bucket (e.g. which long entities does the current system mispredict?). 3. In pairwise analysis, users can select a bucket, and the unique errors (e.g. system A succeeds while B fails and vice versa) of two models will be displayed. F5: System Combination: Is there potential complementarity between different systems? System combination (Ting and Witten, 1997; González-Rubio et al., 2011; Duh et al., 2011) is a technique to improve performance by combining the output from multiple existing systems. In E X PLAINA B OARD, users can choose multiple systems and obtain combined results calculated by voting over multiple base systems.10 In practice, for NER task, we use the recently proposed S PAN N ER (Fu 9 Each bin of the performance histogram is clickable, returning an error case table. 10 With the system combination button of Explainaboard, we observed the-state-of-the art performance of some tasks (e.g., NER, Chunking) can be further improved. et al., 2021) as a combiner, and for text summarizat"
2021.acl-demo.34,2020.emnlp-main.393,0,0.283313,"ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 280–289, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics they provide a standardized evaluation setup, often on multiple tasks, that eases reproducible model comparison across organizations. However, at the same time, due to the prestige imbued by a top, or high, place on a leaderboard they also can result in a singular focus on raising evaluation numbers at the cost of deeper scientific understanding of model properties (Ethayarajh and Jurafsky, 2020). In particular, we argue that, among others, the following are three major limitations of the existing leaderboard paradigm: • Interpretability: Most existing leaderboards commonly use a single number to summarize system performance holistically. This is conducive to system ranking but at the same time, the results are opaque, making the strengths and weaknesses of systems less interpretable. • Interactivity: Existing leaderboards are static and non-interactive, which limits the ability of users to dig deeper into the results. Thus, (1) they usually do not flexibly support more complex evalua"
2021.acl-demo.34,2021.acl-long.558,1,0.819882,"Missing"
2021.acl-demo.34,2020.emnlp-main.489,1,0.776427,"iety of NLP tasks (as illustrated in Tab. 1). Many of these functionalities are grounded in existing research on evaluation and fine-grained diagnostics. In this paper, we describe E XPLAINA B OARD (see Fig.1), a software package and hosted leaderboard that satisfies all of the above desiderata. It also serves as a prototype implementation of 2.1 Interpretability some desirable features that may be included in future leaderboards, even independent of the pro- Interpretable evaluation (Popovi´c and Ney, 2011; vided software itself. We have deployed E XPLAIN - Stymne, 2011; Neubig et al., 2019; Fu et al., 2020a), is a research area that considers methods that break A B OARD for 9 different tasks and 41 different down the holistic performance of each system into datasets, and demonstrate how it can be easily different interpretable groups. For example, in a adapted to new tasks of interest. We expect that E XPLAINA B OARD will benefit 5 http://explainaboard.nlpedia.ai/ different steps of the research process: leaderboard/xtreme/ 6 (i) System Developement: E XPLAINA B OARD https://sites.research.google/xtreme/ 7 Since the first release of E XPLAINA B OARD, we have provides more detailed information r"
2021.acl-demo.34,2020.emnlp-main.457,1,0.724348,"iety of NLP tasks (as illustrated in Tab. 1). Many of these functionalities are grounded in existing research on evaluation and fine-grained diagnostics. In this paper, we describe E XPLAINA B OARD (see Fig.1), a software package and hosted leaderboard that satisfies all of the above desiderata. It also serves as a prototype implementation of 2.1 Interpretability some desirable features that may be included in future leaderboards, even independent of the pro- Interpretable evaluation (Popovi´c and Ney, 2011; vided software itself. We have deployed E XPLAIN - Stymne, 2011; Neubig et al., 2019; Fu et al., 2020a), is a research area that considers methods that break A B OARD for 9 different tasks and 41 different down the holistic performance of each system into datasets, and demonstrate how it can be easily different interpretable groups. For example, in a adapted to new tasks of interest. We expect that E XPLAINA B OARD will benefit 5 http://explainaboard.nlpedia.ai/ different steps of the research process: leaderboard/xtreme/ 6 (i) System Developement: E XPLAINA B OARD https://sites.research.google/xtreme/ 7 Since the first release of E XPLAINA B OARD, we have provides more detailed information r"
2021.acl-demo.34,P11-1127,0,0.320367,"common error cases, which can be useful to identify challenging samples or even annotation errors. 2. In single-system analysis, users can choose particular buckets in the performance histogram9 and see corresponding error samples in that bucket (e.g. which long entities does the current system mispredict?). 3. In pairwise analysis, users can select a bucket, and the unique errors (e.g. system A succeeds while B fails and vice versa) of two models will be displayed. F5: System Combination: Is there potential complementarity between different systems? System combination (Ting and Witten, 1997; González-Rubio et al., 2011; Duh et al., 2011) is a technique to improve performance by combining the output from multiple existing systems. In E X PLAINA B OARD, users can choose multiple systems and obtain combined results calculated by voting over multiple base systems.10 In practice, for NER task, we use the recently proposed S PAN N ER (Fu 9 Each bin of the performance histogram is clickable, returning an error case table. 10 With the system combination button of Explainaboard, we observed the-state-of-the art performance of some tasks (e.g., NER, Chunking) can be further improved. et al., 2021) as a combiner, and"
2021.acl-demo.34,2021.acl-short.135,1,0.72831,"(Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from machine translation are collected from the WMT20.12 4.2 4 Case Study Here, we briefly showcase the actual E XPLAIN interface through a case study on analyzing state-of-the-art NER systems. A B OARD 4.1 Experimental Setup Attribute Definition We define attributes following Fu et al. (2020a) and three of them are used below: entity length, sentence length and label of entity. Collection of Systems Outputs Currently, we collect system outputs by either implementing them by ourselves or collecting from other researchers 12 http://www.statmt.org/wmt20/ metrics-task.html Analysis"
2021.acl-demo.34,2021.naacl-main.113,1,0.869877,"om multiple existing systems. In E X PLAINA B OARD, users can choose multiple systems and obtain combined results calculated by voting over multiple base systems.10 In practice, for NER task, we use the recently proposed S PAN N ER (Fu 9 Each bin of the performance histogram is clickable, returning an error case table. 10 With the system combination button of Explainaboard, we observed the-state-of-the art performance of some tasks (e.g., NER, Chunking) can be further improved. et al., 2021) as a combiner, and for text summarization we employed R EFACTOR, a state-of-the-art ensemble approach (Liu et al., 2021). Regarding the other tasks, we adopt the majority voting method for system combination. 2.3 Reliability The experimental conclusions obtained from the evaluation metrics are not necessarily statistically reliable, especially when the experimental results can be affected by many factors. E XPLAIN A B OARD also makes a step towards more reliable interpretable evaluation. F6: Confidence Analysis: To what extent can we trust the results of our system? E XPLAIN A B OARD can perform confidence analysis over both holistic and fine-grained performance metrics. As shown in Tab. 1, for each bucket, the"
2021.acl-demo.34,N19-4007,1,0.846656,"licable to a wide variety of NLP tasks (as illustrated in Tab. 1). Many of these functionalities are grounded in existing research on evaluation and fine-grained diagnostics. In this paper, we describe E XPLAINA B OARD (see Fig.1), a software package and hosted leaderboard that satisfies all of the above desiderata. It also serves as a prototype implementation of 2.1 Interpretability some desirable features that may be included in future leaderboards, even independent of the pro- Interpretable evaluation (Popovi´c and Ney, 2011; vided software itself. We have deployed E XPLAIN - Stymne, 2011; Neubig et al., 2019; Fu et al., 2020a), is a research area that considers methods that break A B OARD for 9 different tasks and 41 different down the holistic performance of each system into datasets, and demonstrate how it can be easily different interpretable groups. For example, in a adapted to new tasks of interest. We expect that E XPLAINA B OARD will benefit 5 http://explainaboard.nlpedia.ai/ different steps of the research process: leaderboard/xtreme/ 6 (i) System Developement: E XPLAINA B OARD https://sites.research.google/xtreme/ 7 Since the first release of E XPLAINA B OARD, we have provides more detai"
2021.acl-demo.34,D15-1044,0,0.00960657,"B OARD currently supports. “Attr.” denotes Attribute. “Pred.” denotes “Prediction”. Structure Prediction Prediction of a syntactic or semantic structure from text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from machine translation are collected from the WMT20.12 4.2 4 Case Study Here, we briefly showcase the actual E XPLAIN interface through a case study on analyzing state-of-the-art NER systems. A B OARD 4.1 Experimental Setup Attribute Definition We define attributes following Fu et al. (2020a) and three of them are used below: entity length, sentence len"
2021.acl-demo.34,2021.ccl-1.54,0,0.0296761,"Missing"
2021.acl-demo.34,P11-4010,0,0.0375117,"s that are applicable to a wide variety of NLP tasks (as illustrated in Tab. 1). Many of these functionalities are grounded in existing research on evaluation and fine-grained diagnostics. In this paper, we describe E XPLAINA B OARD (see Fig.1), a software package and hosted leaderboard that satisfies all of the above desiderata. It also serves as a prototype implementation of 2.1 Interpretability some desirable features that may be included in future leaderboards, even independent of the pro- Interpretable evaluation (Popovi´c and Ney, 2011; vided software itself. We have deployed E XPLAIN - Stymne, 2011; Neubig et al., 2019; Fu et al., 2020a), is a research area that considers methods that break A B OARD for 9 different tasks and 41 different down the holistic performance of each system into datasets, and demonstrate how it can be easily different interpretable groups. For example, in a adapted to new tasks of interest. We expect that E XPLAINA B OARD will benefit 5 http://explainaboard.nlpedia.ai/ different steps of the research process: leaderboard/xtreme/ 6 (i) System Developement: E XPLAINA B OARD https://sites.research.google/xtreme/ 7 Since the first release of E XPLAINA B OARD, we hav"
2021.acl-demo.34,2020.emnlp-demos.15,0,0.0830454,"Missing"
2021.acl-demo.34,W02-1011,0,0.0280969,"Missing"
2021.acl-demo.34,D14-1052,0,0.0634385,"Missing"
2021.acl-demo.34,N03-1033,0,0.0247044,"as unfortunately it is currently not standard in NLP to release the system outputs that E XPLAINA B OARD needs. 283 classification (Pang et al., 2002), topic identification (Wang and Manning, 2012), and intention detection (Chen et al., 2013). Task Text Pair Classification Prediction of a class given two texts, such as the natural language inference task (Bowman et al., 2015). Sequence Labeling Prediction of a label for each token in a sequence. The E XPLAINA B OARD currently includes four concrete tasks: named entity recognition (Tjong Kim Sang and De Meulder, 2003), part-of-speech tagging (Toutanova et al., 2003), text chunking (Ando and Zhang, 2005), and Chinese word segmentation (Chen et al., 2015). 8 40 2 Topic 4 18 2 Intention 1 3 2 Text-Span Classification Aspect Sentiment 4 20 4 Text Pair Classification NLI 2 6 7 NER 3 74 9 Text Classification Text-Span Classification Prediction of a predefined class from the input of a text and a span, such as aspect-based sentiment classification task (Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 S"
2021.acl-demo.34,J11-4002,0,0.102992,"Missing"
2021.acl-demo.34,D19-3002,0,0.055649,"Missing"
2021.acl-demo.34,D16-1264,0,0.0175544,"tion of the E XPLAINA B OARD concept. Compared to vanilla leaderboards, E XPLAINA B OARD allows users to perform interpretable (single-system , pairwise analysis, data bias), interactive (system combination, fine-grained/common error analysis), and reliable analysis (confidence interval, calibration) on systems in which they are interested. “Comb.” denotes “combination” and “Errs” represents “errors”. “PER, LOC, ORG” refer to different labels. This is true both for classical tasks such as machine translation (Sutskever et al., 2014; Wu et al., 2016), as well as for new tasks (Lu et al., 2016; Rajpurkar et al., 2016), domains (Beltagy et al., 2019), and languages (Conneau and Lample, 2019). One way this progress is quantified is through leaderboards, which report and update performance numbers 1 Introduction of state-of-the-art systems on one or more tasks. Some prototypical leaderboards include GLUE and Natural language processing (NLP) research has been and is making astounding strides forward. SuperGLUE for natural language understanding (Wang et al., 2018, 2019), XTREME and XGLUE 1 E XPLAINA B OARD keeps updated and is recently up(Hu et al., 2020; Liang et al., 2020) for multilingual graded by support"
2021.acl-demo.34,W18-5446,0,0.0728221,"Missing"
2021.acl-demo.34,P12-2018,0,0.141691,"Missing"
2021.acl-demo.34,2020.emnlp-main.523,0,0.187757,"outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2 36 7 Translation 4 60 9 Table 2: Brief descriptions of tasks, datasets and systems that E XPLAINA B OARD currently supports. “Attr.” denotes Attribute. “Pred.” denotes “Prediction”. Structure Prediction Prediction of a syntactic or semantic structure from text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from machine translation are collected from the WMT20.12 4.2 4 Case Study"
2021.acl-demo.34,2021.eacl-main.324,1,0.883418,"ble interpretable evaluation. F6: Confidence Analysis: To what extent can we trust the results of our system? E XPLAIN A B OARD can perform confidence analysis over both holistic and fine-grained performance metrics. As shown in Tab. 1, for each bucket, there is an error bar whose width reflects how reliable the performance value is. We claim this is an important feature for fine-grained analysis since the numbers of test samples in each bucket are imbalanced, and with the confidence interval, one could know how much uncertainty there is. In practice, we use bootstrapping method (Efron, 1992; Ye et al., 2021) to calculate the confidence interval. F7: Calibration Analysis: How well is the confidence of prediction calibrated with its correctness? One commonly-cited issue with modern neural predictors is that their probability estimates are not accurate (i.e. they are poorly calibrated), often being over-confident in the correctness of their predictions (Guo et al., 2017). We also incorporate this feature into E XPLAINA B OARD, allowing users to evaluate how well-calibrated their systems of interest are. 3 Tasks, Datasets and Systems We have already added to E XPLAINA B OARD 12 NLP tasks, 50 datasets"
2021.acl-demo.34,D18-1425,0,0.0226747,"(Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2 36 7 Translation 4 60 9 Table 2: Brief descriptions of tasks, datasets and systems that E XPLAINA B OARD currently supports. “Attr.” denotes Attribute. “Pred.” denotes “Prediction”. Structure Prediction Prediction of a syntactic or semantic structure from text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from m"
2021.acl-demo.34,D19-5410,1,0.812592,"archers to interact with it, interpreting systems and datasets that they are interested in from different perspectives. (ii) We also release our back-end code for different NLP tasks so that researchers could flexibly use them to process their own system outputs, which can assist their research projects. Example Use-cases To show the practical utility of E XPLAINA B OARD, we first present examples of how it has been used as an analysis tool in existing published research papers. Fu et al. (2020b) (Tab.4) utilize single-system analysis with the attribute of label consistency for NER task while Zhong et al. (2019) (Tab.4-5) use it for Contributing to ExplainaBoard The commutext summarization with attributes of density nity can contribute to E XPLAINA B OARD in sevand compression. Fig.4 and Tab.3 in Fu et al. eral ways: (i) Submit system outputs of their im(2020a) leverage the data bias analysis and pairplemented models. (ii) Add more informative atwise system diagnostics to interpret top-performing tributes for different NLP tasks. (iii) Add new NER systems while Tab.4 in Fu et al. (2020c) use datasets or benchmarks for existing or new tasks. single and pairwise system analysis to investigate what’s ne"
2021.acl-long.505,N18-1118,0,0.321312,"ms fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural capacity to model cross-sentential context do not necessarily learn to do so when trained with existing training paradigms. However, even quantifying model usage of context is an ongoing challenge; while contrastive evaluation has been proposed to measure performance on inter-sentential discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018), this approach is confined to a narrow set of phenomena, such as pronoun translation and lexical cohesion. A toolbox to measure the impact of context in broader settings is still missing. 6467 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6467–6478 August 1–6, 2021. ©2021 Association for Computational Linguistics Source: The Church is merciful. . . It always welcomes the misguided lamb. Target: Baseline Die Kirche ist barmherzig. . . Es heisst die fehlgeleiteten Sch¨afle"
2021.acl-long.505,2020.acl-main.149,0,0.250692,"ine Die Kirche ist barmherzig. . . Es heisst die fehlgeleiteten Sch¨aflein immer willkommen. Context-Aware Es heisst die fehlgeleiteten Sch¨aflein immer willkommen. Context-Aware Sie heisst die fehlgeleiteten Sch¨aflein immer w/ our method willkommen. Table 1: Example where context (italic) is needed to correctly translate the pronoun “it”. Both the sentencelevel baseline and context-aware model fail to correctly translate it while the context-aware model trained with C OW ORD dropout correctly captures the context. To address the limitations above, we take inspiration from the recent work of Bugliarello et al. (2020) and propose a new metric, conditional cross-mutual information (CXMI, §3), to measure quantitatively how much context-aware models actually use the provided context by comparing the model distributions over a dataset with and without context. Figure 1 illustrates how it measures context usage. This metric applies to any probabilistic context-aware machine translation model, not only the ones used in this paper. We release a software package to encourage the use of this metric in future context-aware machine translation research. We then perform a rigorous empirical analysis of the CXMI betwee"
2021.acl-long.505,2012.eamt-1.60,0,0.0344786,"s, training regimens, or random seeds. To address this we consider a single model, qM T , that is able to translate with and without context (more on how this achieved in §3.2). We can then set the context-agnostic model and the contextual model to be the same model qM TA = qM TC = qM T . This way we attribute the information gain to the introduction of context. Throughout the rest of this work, when we reference “context usage” we will precisely mean this information gain (or loss). 3.2 Experiments Data We experiment with a document-level translation task by training models on the IWSLT2017 (Cettolo et al., 2012) dataset for language pairs EN → DE and EN → FR (with approximately 200K sentences for both pairs). We use the test sets 2011-2014 as validation sets and the 2015 as test sets. To address the concerns pointed out by Lopes et al. (2020) that gains in performance are due to the use of small training corpora and weak baselines, we use Paracrawl (Espl`a et al., 2019) and perform some data cleaning based on language identification tools, creating a pretraining dataset of around 6469 82M and 104M sentence pairs for EN → DE and EN → FR respectively. All data is encoded/vectorized with byte-pair encod"
2021.acl-long.505,2020.autosimtrans-1.5,0,0.0177006,"proposed the simple approach of concatenating the previous sentences in both the source and target side to the input to the system; Jean et al. (2017), Bawden et al. (2018), and Zhang et al. (2018) used an additional contextspecific encoder to extract contextual features from the previous sentences; Maruf and Haffari (2018) and Tu et al. (2018b) used cache-based memories to encode context; Wang et al. (2017) used a hierarchical RNN to encode the global context from all previous sentences; Miculicich et al. (2018) and Maruf et al. (2019a) used hierarchical attention networks to encode context; Chen et al. (2020) added document-level discourse structure information to the input; Sun et al. (2020) trained a simple concatenation-based model with varying context size during training to have a model that is able to translate with any context size, similar to what is done in this work. Similarly to what we do with C OW ORD dropout, Jean and Cho (2019) attempted to maximise sensitivity to context by introducing a margin-based regularization term to explicitly encourage context usage. For a more detailed overview, Maruf et al. (2019b) extensively describe the different approaches and how they leverage contex"
2021.acl-long.505,W19-6721,0,0.0415648,"Missing"
2021.acl-long.505,D19-6503,0,0.0321931,"Missing"
2021.acl-long.505,D18-2012,0,0.0306719,"Missing"
2021.acl-long.505,D18-1512,0,0.0481132,"Missing"
2021.acl-long.505,2020.eamt-1.24,1,0.809112,"t models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural capacity to model cross-sentential context do not necessarily learn to do so when trained with existing training paradigms. However, even quantifying model usage of context is an ongoing challenge; while contrastive evaluation has been proposed to measure performance on inter-sentential discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018), this approach is confined to a narrow set of phenomena, such as pronoun translation and lexical cohesion. A toolbox to measur"
2021.acl-long.505,P18-1118,0,0.0182091,"similar to the sentence-level baseline, while when dropout is applied, they are able to effectively start using context. Related Work Context-aware Machine Translation There have been many works in the literature that try to incorporate context into NMT systems. Tiedemann and Scherrer (2017) first proposed the simple approach of concatenating the previous sentences in both the source and target side to the input to the system; Jean et al. (2017), Bawden et al. (2018), and Zhang et al. (2018) used an additional contextspecific encoder to extract contextual features from the previous sentences; Maruf and Haffari (2018) and Tu et al. (2018b) used cache-based memories to encode context; Wang et al. (2017) used a hierarchical RNN to encode the global context from all previous sentences; Miculicich et al. (2018) and Maruf et al. (2019a) used hierarchical attention networks to encode context; Chen et al. (2020) added document-level discourse structure information to the input; Sun et al. (2020) trained a simple concatenation-based model with varying context size during training to have a model that is able to translate with any context size, similar to what is done in this work. Similarly to what we do with C OW"
2021.acl-long.505,N19-1313,1,0.873371,"el evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural capacity to model cross-sentential context do not necessarily learn to do so when trained with existing tr"
2021.acl-long.505,D18-1325,0,0.087652,"er hold with document-level evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural capacity to model cross-sentential context do not necessarily learn to do so when trai"
2021.acl-long.505,W18-6307,0,0.0348686,"Missing"
2021.acl-long.505,N19-4009,0,0.0603517,"Missing"
2021.acl-long.505,P02-1040,0,0.113275,"017) transformer small model (more details in Appendix §C). For all models with target context, when decoding, we use the previous decoded sentences as target context. Table 4 shows the performance across three different seeds of the baseline and contextual models for both the non-pretrained and pretrained setting, with increasing values of C OW ORD dropout p. We also run the baseline with C OW ORD dropout (which, as said previously, reduces to word dropout) to ensure that improvements were not only due to regularization effects on the current source/target. We report the standard BLEU score (Papineni et al., 2002) calculated using sacreBLEU (Post, 2018) and COMET, a more accurate evaluation method using multilingual embeddings (Rei et al., 2020). For the non-pretrained case, we can see that a C OW ORD dropout value p &gt; 0 consistently improves the performance of the contextual models when compared to models running with p = 0 and with the sentence-level baseline with the same values for word dropout. For the pretrained case, the improvements are not as noticeable, although models trained with C OW ORD dropout still always outperform models trained without it. This is perhaps a reflection of the general"
2021.acl-long.505,W18-6319,0,0.0146151,"dix §C). For all models with target context, when decoding, we use the previous decoded sentences as target context. Table 4 shows the performance across three different seeds of the baseline and contextual models for both the non-pretrained and pretrained setting, with increasing values of C OW ORD dropout p. We also run the baseline with C OW ORD dropout (which, as said previously, reduces to word dropout) to ensure that improvements were not only due to regularization effects on the current source/target. We report the standard BLEU score (Papineni et al., 2002) calculated using sacreBLEU (Post, 2018) and COMET, a more accurate evaluation method using multilingual embeddings (Rei et al., 2020). For the non-pretrained case, we can see that a C OW ORD dropout value p &gt; 0 consistently improves the performance of the contextual models when compared to models running with p = 0 and with the sentence-level baseline with the same values for word dropout. For the pretrained case, the improvements are not as noticeable, although models trained with C OW ORD dropout still always outperform models trained without it. This is perhaps a reflection of the general trend that better models are harder to i"
2021.acl-long.505,2020.emnlp-main.213,0,0.0373237,"sentences as target context. Table 4 shows the performance across three different seeds of the baseline and contextual models for both the non-pretrained and pretrained setting, with increasing values of C OW ORD dropout p. We also run the baseline with C OW ORD dropout (which, as said previously, reduces to word dropout) to ensure that improvements were not only due to regularization effects on the current source/target. We report the standard BLEU score (Papineni et al., 2002) calculated using sacreBLEU (Post, 2018) and COMET, a more accurate evaluation method using multilingual embeddings (Rei et al., 2020). For the non-pretrained case, we can see that a C OW ORD dropout value p &gt; 0 consistently improves the performance of the contextual models when compared to models running with p = 0 and with the sentence-level baseline with the same values for word dropout. For the pretrained case, the improvements are not as noticeable, although models trained with C OW ORD dropout still always outperform models trained without it. This is perhaps a reflection of the general trend that better models are harder to improve. 6472 Source Context Source More people watched games because it was faster. The ball c"
2021.acl-long.505,W16-2323,0,0.26832,"then perform a rigorous empirical analysis of the CXMI between the context and target for different context sizes, and between source and target context. We find that: (1) context-aware models use some information from the context, but the amount of information used does not increase uniformly with the context size, and can even lead to a reduction in context usage; (2) target context seems to be used more by models than source context. Given the findings, we next consider how to encourage models to use more context. Specifically, we introduce a simple but effective variation of word dropout (Sennrich et al., 2016a) for context-aware machine translation, dubbed C OW ORD dropout (§4). Put simply, we randomly drop words from the current source sentence by replacing them with a placeholder token. Intuitively, this encourages the model to use extra-sentential information to compensate for the missing information in the current source sentence. We show that models trained with C OW ORD dropout not only increase context usage compared to models trained without it but also improve the quality of translation, both according to standard evaluation metrics (BLEU and COMET) and according to contrastive evaluation"
2021.acl-long.505,P16-1162,0,0.392074,"then perform a rigorous empirical analysis of the CXMI between the context and target for different context sizes, and between source and target context. We find that: (1) context-aware models use some information from the context, but the amount of information used does not increase uniformly with the context size, and can even lead to a reduction in context usage; (2) target context seems to be used more by models than source context. Given the findings, we next consider how to encourage models to use more context. Specifically, we introduce a simple but effective variation of word dropout (Sennrich et al., 2016a) for context-aware machine translation, dubbed C OW ORD dropout (§4). Put simply, we randomly drop words from the current source sentence by replacing them with a placeholder token. Intuitively, this encourages the model to use extra-sentential information to compensate for the missing information in the current source sentence. We show that models trained with C OW ORD dropout not only increase context usage compared to models trained without it but also improve the quality of translation, both according to standard evaluation metrics (BLEU and COMET) and according to contrastive evaluation"
2021.acl-long.505,2020.coling-main.417,0,0.018091,"r measuring context usage and the proposed regularization method of C OW ORD dropout, can theoretically be applied to any of the above-mentioned methods. Evaluation In terms of evaluation, most previous work focuses on targeting a system’s performance on contrastive datasets for specific inter-sentential discourse phenomena. M¨uller et al. (2018) built a large-scale dataset for anaphoric pronoun resolution, Bawden et al. (2018) manually created a dataset for both pronoun resolution and lexical choice and Voita et al. (2019) created a dataset that targets deixis, ellipsis and lexical cohesion. Stojanovski et al. (2020) showed through adversarial attacks that models that do well on other contrastive datasets rely on surface heuristics and create a contrastive dataset to address this. In contrast, our CXMI metric is phenomenon-agnostic and can be measured with respect to all phenomena that require context in translation. Information-Theoretic Analysis Bugliarello et al. (2020) first proposed cross-mutual information (XMI) in the context of measuring the difficulty of translating between languages. Our work differs in that we propose a conditional version of XMI, where S is always observed, and we use it to as"
2021.acl-long.505,W17-4811,0,0.300471,"duction While neural machine translation (NMT) is reported to have achieved human parity in some domains and language pairs (Hassan et al., 2018), these claims seem overly optimistic and no longer hold with document-level evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize tha"
2021.acl-long.505,W18-6312,0,0.0157198,"slation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.1 1 Figure 1: Illustration of how we can measure context usage by a model qM T as the amount of information gained when a model is given the context C and source X vs when the model is only given the X. Introduction While neural machine translation (NMT) is reported to have achieved human parity in some domains and language pairs (Hassan et al., 2018), these claims seem overly optimistic and no longer hold with document-level evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more detai"
2021.acl-long.505,Q18-1029,0,0.2931,"airs (Hassan et al., 2018), these claims seem overly optimistic and no longer hold with document-level evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural cap"
2021.acl-long.505,P19-1116,0,0.0310266,"Missing"
2021.acl-long.505,D17-1301,0,0.0440336,"ctively start using context. Related Work Context-aware Machine Translation There have been many works in the literature that try to incorporate context into NMT systems. Tiedemann and Scherrer (2017) first proposed the simple approach of concatenating the previous sentences in both the source and target side to the input to the system; Jean et al. (2017), Bawden et al. (2018), and Zhang et al. (2018) used an additional contextspecific encoder to extract contextual features from the previous sentences; Maruf and Haffari (2018) and Tu et al. (2018b) used cache-based memories to encode context; Wang et al. (2017) used a hierarchical RNN to encode the global context from all previous sentences; Miculicich et al. (2018) and Maruf et al. (2019a) used hierarchical attention networks to encode context; Chen et al. (2020) added document-level discourse structure information to the input; Sun et al. (2020) trained a simple concatenation-based model with varying context size during training to have a model that is able to translate with any context size, similar to what is done in this work. Similarly to what we do with C OW ORD dropout, Jean and Cho (2019) attempted to maximise sensitivity to context by intr"
2021.acl-long.505,D18-1049,0,0.0701837,"Missing"
2021.acl-long.59,S17-2091,0,0.0524295,"Missing"
2021.acl-long.59,D19-1371,0,0.022721,"n identification, saliency classification, and relation extraction) in a sequence, treating coreference resolution as an external black box. While word and span representations are shared across all tasks and updated to minimize multi-task loss, the model trains each task on gold input. Figure 2 summarizes the baseline model’s end-to-end architecture, and highlights the places where we propose improvements for our CitationIE model. Feature Extraction The model extracts features from raw text in two stages. First, contextualized word embeddings are obtained for each section by running SciBERT (Beltagy et al., 2019) on that section of text (up to 512 tokens). Then, the embeddings from all words over all sections are passed through a bidirectional LSTM (Graves et al., 2005) to contextualize each word’s representation with those from other sections. Mention Identification The baseline model treats this named entity recognition task as an IOBES sequence tagging problem (Reimers and Gurevych, 2017). The tagger takes the SciBERTBiLSTM (Beltagy et al., 2019; Graves et al., 2005) word embeddings (as shown in the Figure 2), feeds them through two feedforward networks (not shown in Figure 2), and produces tag pot"
2021.acl-long.59,D14-1150,0,0.180367,"and relation extraction. For each task, we consider two types of citation graph information, either separately or together: (1) structural information from the graph network topology and (2) textual information from the content of citing and cited documents. 4.1 Structural Information The structure of the citation graph can contextualize a document within the greater body of work. Prior works in scientific information extraction have predominantly used the citation graph only to analyze the content of citing papers, such as CiteTextRank (Das Gollapalli and Caragea, 2014) and Citation TF-IDF (Caragea et al., 2014), which is described in detail in §4.2.2. However, the citation graph can be used to discover relationships between non-adjacent documents in the citation graph; prior works struggle to capture these relationships. 722 Stage 1 Stage 2 Output Figure 4: Feedforward architecture in each task (with CitationIE-specific parameters shown in light blue). Arnold and Cohen (2009) are the only prior work, to our knowledge, to explicitly use the citation graph’s structure for scientific IE. They predict key entities related to a paper via random walks on a combined knowledge-and-citation-graph consisting"
2021.acl-long.59,W18-2501,0,0.0186975,"g end-to-end and per-task metrics. All metrics, except where stated otherwise, are the same as described by Jain et al. (2020). Mention Identification We evaluate mention identification with the average F1 score of classifying entities of each span type. Salient Entity Classification Similar to Jain et al. (2020) we evaluate this task at the mention level and cluster level. We evaluate both metrics on gold standard entity recognition inputs. Baselines 5.1.3 Training Details We build our proposed CitationIE methods on top of the SciREX repository7 (Jain et al., 2020) in the AllenNLP framework (Gardner et al., 2018). For each task, we first train that component in isolation from the rest of the system to minimize 7 724 https://github.com/allenai/SciREX Model F1 P size10 (66 samples) and inter-model variation. (3) Incorporating graph embeddings and citances simultaneously is no better than using either. (4) Our reimplemented baseline differs from the results reported by Jain et al. (2020) despite using their published code to train their model. This may be because we use a batch size of 4 (due to compute limits) while they reported a batch size of 50. R Salient Mention Evaluation Baseline (reported) Basel"
2021.acl-long.59,I11-1001,0,0.02721,"have experimented with such models on the Switchboard task […] ” for the scientific community to read this many papers in a time-critical situation, and make accurate judgements to help separate signal from the noise. To this end, how can machines help researchers quickly identify relevant papers? One step in this direction is to automatically extract and organize scientific information (e.g. important concepts and their relations) from a collection of research articles, which could help researchers identify new methods or materials for a given task. Scientific information extraction (SciIE) (Gupta and Manning, 2011; Yogatama et al., 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; G´abor et al., 2018; Jain et al., 2020). Existing works on SciIE revolve around extraction solely based on the content of different parts of an individual paper, such as the abstract or conclusion (Augenstein et al., 2017; Luan et al., 2019). However, scientific papers do not exist in a vacuum — they are part of a larger ecosystem of papers, related to each other through different con"
2021.acl-long.59,P19-1513,0,0.105876,"Most work on scientific information extraction has used annotated datasets of scientific abstracts, such as those provided for SemEval 2017 and SemEval 2018 shared tasks (Augenstein et al., 2017; G´abor et al., 2018), the SciERC dataset (Luan et al., 2018), and the BioCreative V Chemical Disease Relation dataset (Wei et al., 2016). We focus on the task of open-domain documentlevel relation extraction from long, full-text documents. This is in contrast to the above methods that only use paper abstracts. Our setting is also different from works that consider a fixed set of candidate relations (Hou et al., 2019; Kardas et al., 2020) or those that only consider IE tasks other than relation extraction, such as entity recognition (Verspoor et al., 2011). We base our task definition and baseline models on the recently released SciREX dataset (Jain et al., 2020), which contains 438 annotated papers,3 all related to machine learning research. Each document consists of sections D = {S1 , . . . , SN }, where each section contains a sequence of words Si = {wi,1 , . . . , wi,Ni }. Each document comes with annotations of entities, coreference clusters, cluster-level saliency labels, and 4-ary document-level re"
2021.acl-long.59,2020.acl-main.670,0,0.0286505,"Missing"
2021.acl-long.59,2020.emnlp-main.692,0,0.0162033,"ntific information extraction has used annotated datasets of scientific abstracts, such as those provided for SemEval 2017 and SemEval 2018 shared tasks (Augenstein et al., 2017; G´abor et al., 2018), the SciERC dataset (Luan et al., 2018), and the BioCreative V Chemical Disease Relation dataset (Wei et al., 2016). We focus on the task of open-domain documentlevel relation extraction from long, full-text documents. This is in contrast to the above methods that only use paper abstracts. Our setting is also different from works that consider a fixed set of candidate relations (Hou et al., 2019; Kardas et al., 2020) or those that only consider IE tasks other than relation extraction, such as entity recognition (Verspoor et al., 2011). We base our task definition and baseline models on the recently released SciREX dataset (Jain et al., 2020), which contains 438 annotated papers,3 all related to machine learning research. Each document consists of sections D = {S1 , . . . , SN }, where each section contains a sequence of words Si = {wi,1 , . . . , wi,Ni }. Each document comes with annotations of entities, coreference clusters, cluster-level saliency labels, and 4-ary document-level relations. We break down"
2021.acl-long.59,W04-3250,0,0.057712,"metric. 5.1.2 For each task, we compare against Jain et al. (2020), whose architecture our system is built on. No other model to our knowledge performs all the tasks we consider on full documents. For the 4-ary relation extraction task, we also compare against the DocTAET model (Hou et al., 2019), which is considered as state-of-the-art for full-text scientific relation extraction (Jain et al., 2020; Hou et al., 2019). Significance To improve the rigor of our evaluation, we run significance tests for each of our proposed methods against its associated baseline, via paired bootstrap sampling (Koehn, 2004). In experiments where we trained multiple models with different seeds, we perform a hierarchical bootstrap procedure where we first sample a seed for each model and then sample a randomized test set. Metrics The ultimate product of our work is an end-to-end document-level relation extraction system, but we also measure each component of our system in isolation, giving end-to-end and per-task metrics. All metrics, except where stated otherwise, are the same as described by Jain et al. (2020). Mention Identification We evaluate mention identification with the average F1 score of classifying ent"
2021.acl-long.59,2020.acl-main.447,0,0.152798,"eraged over all sections and passed through another feedforward network which returns a binary prediction. 3 Citation-aware SciIE Dataset Although citation network information has been shown to be effective in other tasks, few works have recently tried using it in SciIE systems. One potential reason is the lack of a suitable dataset. Thus, as a first contribution of this paper, we address this bottleneck by constructing a SciIE dataset that is annotated with citation graph information.4 Specifically, we combine the rich annotations of SciREX with a source of citation graph information, S2ORC (Lo et al., 2020). For each paper, S2ORC includes parsed metadata about which other papers cite this paper, which other papers are 4 We have released code to construct this dataset: https: //github.com/viswavi/ScigraphIE 721 Input Document Title/Abstract Feature Extraction SciBERT BiLSTM Mention Identification Salient Entity Classification Span Embedding CRF Intro. (Part 1) FF “ASR"" FF FF Salient FF FF ✓ Metric: “WER” Relation: { Task: Method: Dataset: Metric: } ASR, RNN, Switchboard, WER, Task: Span Embedding Intro. (Part 2) “ASR"" FF FF B-METHOD L -METHOD O Recurrent nets with ... sigmoid activations “RNN” “S"
2021.acl-long.59,D18-1360,0,0.131786,"To this end, how can machines help researchers quickly identify relevant papers? One step in this direction is to automatically extract and organize scientific information (e.g. important concepts and their relations) from a collection of research articles, which could help researchers identify new methods or materials for a given task. Scientific information extraction (SciIE) (Gupta and Manning, 2011; Yogatama et al., 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; G´abor et al., 2018; Jain et al., 2020). Existing works on SciIE revolve around extraction solely based on the content of different parts of an individual paper, such as the abstract or conclusion (Augenstein et al., 2017; Luan et al., 2019). However, scientific papers do not exist in a vacuum — they are part of a larger ecosystem of papers, related to each other through different conceptual relations. In this paper, we claim a better under719 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Languag"
2021.acl-long.59,N19-1308,0,0.0245759,"search articles, which could help researchers identify new methods or materials for a given task. Scientific information extraction (SciIE) (Gupta and Manning, 2011; Yogatama et al., 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; G´abor et al., 2018; Jain et al., 2020). Existing works on SciIE revolve around extraction solely based on the content of different parts of an individual paper, such as the abstract or conclusion (Augenstein et al., 2017; Luan et al., 2019). However, scientific papers do not exist in a vacuum — they are part of a larger ecosystem of papers, related to each other through different conceptual relations. In this paper, we claim a better under719 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 719–731 August 1–6, 2021. ©2021 Association for Computational Linguistics standing of a research article relies not only on its content but also on its relations with associated works, using both the content of related pape"
2021.acl-long.59,2020.emnlp-main.687,0,0.0349169,"es with annotations of entities, coreference clusters, cluster-level saliency labels, and 4-ary document-level relations. We break down the end-to-end information extraction process as a sequence of these four related tasks, with each task taking the output of the preceding tasks as input. Mention Identification For each span of text within a section, this task aims to recognize if the span describes a Task, Dataset, Method, or Metric entity, if any. Coreference This task requires clustering all entity mentions in a document such that, in each cluster, every mention refers to the same entity (Varkel and Globerson, 2020). The SciREX dataset 2 Our proposed method actually makes correct predictions on both these samples, where the baseline model fails on both. 3 The dataset contains 306 documents for training, 66 for validation, and 66 for testing. 720 includes coreference annotations for each Task, Dataset, Method, and Metric mention. Salient Entity Classification Given a cluster of mentions corresponding to the same entity, the model must predict whether the entity is key to the work described in a paper. We follow the definition from the SciREX dataset (Jain et al., 2020), where an entity in a paper is deeme"
2021.acl-long.59,D11-1055,0,0.0748359,"Missing"
2021.acl-long.65,D18-1216,0,0.0285654,"e above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009). Implications and Future Work In this work, we collected a corpus of supporting context for translating ambiguous words. We examined how baseline context-aware translation models use context, and dem"
2021.acl-long.65,K18-1030,0,0.0269805,"Our work addresses the above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009). Implications and Future Work In this work, we collected a corpus of supporting context for translating ambiguous words. We examined how baseline context-aware translation models us"
2021.acl-long.65,N18-1118,0,0.268261,"(2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al., 2020). Some evidence suggests that models attend to uninformative tokens (Voita et al., 2018) and do not use contextual information adequately (Kim et al., 2019). To understand plausibly why current NMT models are unable to fully leverage the disambiguating context they are provided, and how we can develop models that use context more effectively, we pose the following research questions: (i) In context aware translation, what context is intrinsically usef"
2021.acl-long.65,E06-1032,0,0.283281,"Missing"
2021.acl-long.65,2020.autosimtrans-1.5,0,0.0145477,"ent context-aware NMT approaches enhance NMT by including source- and/or targetside surrounding sentences as context to the model. Tiedemann and Scherrer (2017) concatenate the previous sentences to the input; Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use an additional encoder to extract contextual features; Wang et al. (2017) use a hierarchical RNN to encode the global context from all previous sentences; Maruf and Haffari (2018); Tu et al. (2018) use cache-based memories to encode context; Miculicich et al. (2018); Maruf et al. (2019) use hierarchical attention networks; Chen et al. (2020) add document-level discourse structure information to the input. While Maruf et al. (2019); Voita et al. (2018) also find higher attention mass attributed to relevant tokens in selected examples, our work is the first to guide model attention in context-aware NMT using human supervision and analyze its attention distribution in a quantitative manner. However, recent studies suggest that current context-aware NMT models often do not use context meaningfully. Kim et al. (2019) claim that improvements by context-aware models are mostly from regularization by reserving parameters for context inpu"
2021.acl-long.65,2020.emnlp-main.543,0,0.0252693,"human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009). Implications and Future Work In this work, we collected a corpus of supporting context for translating ambiguous words. We examined how baseline context-aware translation models use context, and demonstrated how context annotations can i"
2021.acl-long.65,2020.eamt-1.24,1,0.738327,"odels rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al., 2020). Some evidence suggests that models attend to uninformative tokens (Voita et al., 2018) and do not use contextual information adequately (Kim et al., 2019). To understand plausibly why current NMT models are unable to fully leverage the disambiguating context they are provided, and how we can develop models that use context more effectively, we pose the following research questions: (i) In context aware translation, what context is intrinsically useful to disambiguate hard translation phenomen"
2021.acl-long.65,P18-1118,0,0.112577,", otherwise “elle”. The model with regularized attention translates the pronoun correctly, with the largest attention on the referent “report”. Top 3 words with the highest attention are highlighted. Introduction There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨uller et al., 2018; Bawden"
2021.acl-long.65,N19-1313,1,0.826755,"fRelated Work Context-Aware Machine Translation Most current context-aware NMT approaches enhance NMT by including source- and/or targetside surrounding sentences as context to the model. Tiedemann and Scherrer (2017) concatenate the previous sentences to the input; Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use an additional encoder to extract contextual features; Wang et al. (2017) use a hierarchical RNN to encode the global context from all previous sentences; Maruf and Haffari (2018); Tu et al. (2018) use cache-based memories to encode context; Miculicich et al. (2018); Maruf et al. (2019) use hierarchical attention networks; Chen et al. (2020) add document-level discourse structure information to the input. While Maruf et al. (2019); Voita et al. (2018) also find higher attention mass attributed to relevant tokens in selected examples, our work is the first to guide model attention in context-aware NMT using human supervision and analyze its attention distribution in a quantitative manner. However, recent studies suggest that current context-aware NMT models often do not use context meaningfully. Kim et al. (2019) claim that improvements by context-aware models are mostly from"
2021.acl-long.65,D16-1249,0,0.0275114,"use context meaningfully. Kim et al. (2019) claim that improvements by context-aware models are mostly from regularization by reserving parameters for context inputs, and Li et al. (2020b) show that replacing the context in multi-encoder models with random signals leads to similar accuracy as using the actual context. Our work addresses the above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A"
2021.acl-long.65,D18-1325,0,0.089293,"report) then “il” is used, otherwise “elle”. The model with regularized attention translates the pronoun correctly, with the largest attention on the referent “report”. Top 3 words with the highest attention are highlighted. Introduction There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨"
2021.acl-long.65,W18-6307,0,0.0324485,"Missing"
2021.acl-long.65,2020.lrec-1.457,0,0.0159497,"is dataset, SCAT: Supporting Context for Ambiguous Translations, are provided in Appendix A. Tasks and Data Quality We perform this study for two tasks: pronoun anaphora resolution (PAR), where the translators are tasked with choosing the correct French gendered pronoun associated to a neutral English pronoun, and word sense disambiguation (WSD), where the translators pick the correct translation of a polysemous word. PAR, and WSD to a lesser extent, have been commonly studied to evaluate context-aware NMT models (Voita et al., 2018; Lopes et al., 2020; M¨uller et al., 2018; Huo et al., 2020; Nagata and Morishita, 2020). 2 https://www.upwork.com 790 Word Sense Disambiguation. There are no existing contrastive datasets for WSD with a context window larger than 1 sentence, therefore, we automatically generate contrastive examples with context window of 5 sentences from OpenSubtitles2018 by identifying polysemous English words and possible French translations. We describe our methodology in Appendix B. Quality. For quality control, we asked 8 internal speakers of English and French, with native or bilingual proficiency in both languages, to carefully annotate the same 100 examples given to all professional tran"
2021.acl-long.65,P02-1040,0,0.112704,"ility of a target document Q Y given the source document X: Pθ (Y |X) = Jj=1 Pθ (y j |xj , C j ), where y j and xj are the j-th target and source sentences, and C j is the collection of contextual sentences for the j-th sentence pair. There are many methods for incorporating context (§6), but even simple concatenation (Tiedemann and Scherrer, 2017), which prepends the previous source or target sentences to the current sentence separated by a hBRKi tag, achieves comparable performance to more sophisticated approaches, especially in highresource scenarios (Lopes et al., 2020). Evaluation. BLEU (Papineni et al., 2002) is most widely used to evaluate MT, but it can be poorly correlated with human evaluation (CallisonBurch et al., 2006; Reiter, 2018). Recently, a number of neural evaluation methods, such as COMET (Rei et al., 2020), have shown better correlation with human judgement. Nevertheless, common automatic metrics have limited ability to evaluate discourse in MT (Hardmeier, 2012). As a remedy to this, researchers often use contrastive test sets for a targeted discourse phenomenon (M¨uller et al., 2018), such as pronoun anaphora resolution and word sense disambiguation, to verify if the model ranks th"
2021.acl-long.65,W18-6319,0,0.032967,"Missing"
2021.acl-long.65,J18-3002,0,0.0126862,"rce sentences, and C j is the collection of contextual sentences for the j-th sentence pair. There are many methods for incorporating context (§6), but even simple concatenation (Tiedemann and Scherrer, 2017), which prepends the previous source or target sentences to the current sentence separated by a hBRKi tag, achieves comparable performance to more sophisticated approaches, especially in highresource scenarios (Lopes et al., 2020). Evaluation. BLEU (Papineni et al., 2002) is most widely used to evaluate MT, but it can be poorly correlated with human evaluation (CallisonBurch et al., 2006; Reiter, 2018). Recently, a number of neural evaluation methods, such as COMET (Rei et al., 2020), have shown better correlation with human judgement. Nevertheless, common automatic metrics have limited ability to evaluate discourse in MT (Hardmeier, 2012). As a remedy to this, researchers often use contrastive test sets for a targeted discourse phenomenon (M¨uller et al., 2018), such as pronoun anaphora resolution and word sense disambiguation, to verify if the model ranks the correct translation of an ambiguous sentence higher than the incorrect translation. Document-Level Translation 3 Neural Machine Tra"
2021.acl-long.65,P16-1162,0,0.0343624,"Missing"
2021.acl-long.65,D18-1548,0,0.0429611,"Missing"
2021.acl-long.65,W17-4811,0,0.401074,"ent of “it” is masculine (e.g., report) then “il” is used, otherwise “elle”. The model with regularized attention translates the pronoun correctly, with the largest attention on the referent “report”. Top 3 words with the highest attention are highlighted. Introduction There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simp"
2021.acl-long.65,W18-6312,0,0.0149766,"son rapport ? Oui, il est d´ej`a a` l’hˆopital Table 1: Translation of the ambiguous pronoun “it”. In French, if the referent of “it” is masculine (e.g., report) then “il” is used, otherwise “elle”. The model with regularized attention translates the pronoun correctly, with the largest attention on the referent “report”. Top 3 words with the highest attention are highlighted. Introduction There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions"
2021.acl-long.65,Q18-1029,0,0.0147868,"ng all context, which may indicate that having irrelevant context can have an adverse efRelated Work Context-Aware Machine Translation Most current context-aware NMT approaches enhance NMT by including source- and/or targetside surrounding sentences as context to the model. Tiedemann and Scherrer (2017) concatenate the previous sentences to the input; Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use an additional encoder to extract contextual features; Wang et al. (2017) use a hierarchical RNN to encode the global context from all previous sentences; Maruf and Haffari (2018); Tu et al. (2018) use cache-based memories to encode context; Miculicich et al. (2018); Maruf et al. (2019) use hierarchical attention networks; Chen et al. (2020) add document-level discourse structure information to the input. While Maruf et al. (2019); Voita et al. (2018) also find higher attention mass attributed to relevant tokens in selected examples, our work is the first to guide model attention in context-aware NMT using human supervision and analyze its attention distribution in a quantitative manner. However, recent studies suggest that current context-aware NMT models often do not use context meani"
2021.acl-long.65,D19-1081,0,0.0248863,"Missing"
2021.acl-long.65,P19-1116,0,0.0280655,"Missing"
2021.acl-long.65,P18-1117,0,0.222083,"https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al., 2020). Some evidence suggests that models attend to uninformative tokens (Voita et al., 2018) and do not use contextual information adequately (Kim et al., 2019). To understand plausibly why current NMT models are unable to fully leverage the disambiguating context they are provided, and how we can develop models that use context more effectively, we pose the following research questions: (i) In context aware translation, what context is intrinsically useful to disambiguate hard translation phenomena such as ambiguous pronouns or word senses?; (ii) Are context-aware MT models paying attention to the relevant context or not?; and (iii) If not, can we 788 Proceedings of the 59th Annual"
2021.acl-long.65,D17-1301,0,0.0244694,"rting context. Furthermore, for attnreg-pre, the score after masking supporting context is significantly lower than when masking all context, which may indicate that having irrelevant context can have an adverse efRelated Work Context-Aware Machine Translation Most current context-aware NMT approaches enhance NMT by including source- and/or targetside surrounding sentences as context to the model. Tiedemann and Scherrer (2017) concatenate the previous sentences to the input; Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use an additional encoder to extract contextual features; Wang et al. (2017) use a hierarchical RNN to encode the global context from all previous sentences; Maruf and Haffari (2018); Tu et al. (2018) use cache-based memories to encode context; Miculicich et al. (2018); Maruf et al. (2019) use hierarchical attention networks; Chen et al. (2020) add document-level discourse structure information to the input. While Maruf et al. (2019); Voita et al. (2018) also find higher attention mass attributed to relevant tokens in selected examples, our work is the first to guide model attention in context-aware NMT using human supervision and analyze its attention distribution in"
2021.acl-long.65,D18-1049,0,0.029498,"Missing"
2021.acl-long.65,P18-2066,0,0.0224531,"ls are mostly from regularization by reserving parameters for context inputs, and Li et al. (2020b) show that replacing the context in multi-encoder models with random signals leads to similar accuracy as using the actual context. Our work addresses the above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such"
2021.acl-long.65,C18-1074,0,0.0276775,"et al. (2020b) show that replacing the context in multi-encoder models with random signals leads to similar accuracy as using the actual context. Our work addresses the above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009). Implications and Future Work In"
2021.americasnlp-1.23,2021.americasnlp-1.30,0,0.035558,"ill now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 2017). We employed the hyperparameters proposed by Guzmán et al. (2019) for a low-resource scenario. We implemented the model using Fairseq (Ott et al., 2019). The implementation of the baseline can be found in the official shared task repository.4 4.2 University of British Columbia The team of the University of British Columbia (UBC-NLP; Billah-Nagoudi et al., 2021) participated for all ten language pairs and in both tracks. They used an encoder-decoder transformer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages have more available data than others, this dataset is unbalanced in favor of languages like Nahuatl, Guarani, and Quechua. The team also proposed a two-stage fine-tuning method: first fine-tuning on the entire da"
2021.americasnlp-1.23,2021.americasnlp-1.28,0,0.0389782,"Missing"
2021.americasnlp-1.23,2020.lrec-1.320,1,0.692133,"test datasets were translated to modern Nahuatl. In particular, the translations belong to Nahuatl Central/Nahuatl de la Huasteca (Hidalgo y San Luis Potosí) dialects. In order to be closer to the training corpus, an orthographic normalization was applied. A simple rule based approach was used, which was based on the most predictable orthographic changes between modern varieties and Classical Nahuatl. Spanish—Guarani Guarani is mostly spoken in Paraguay, Bolivia, Argentina and Brazil. It belongs to the Tupian language family (ISO gnw, gun, gug, gui, grn, nhd). The training corpus for Guarani (Chiruzzo et al., 2020) was collected from web sources (blogs and news articles) that contained a mix of dialects, from pure Guarani to more mixed Jopara which combines Guarani with Spanish neologisms. The development and test corpora, on the other hand, are in standard Paraguayan Guarani. Spanish—Bribri Bribri is a Chibchan language spoken in southern Costa Rica (ISO code bzd). The training set for Bribri was extracted from six sources (Feldman and Coto-Solano, 2020; Margery, 2005; Jara Murillo, 2018a; Constenla et al., 2004; Jara Murillo and García Segura, 2013; Jara Murillo, 2018b; Flores Solórzano, 2017), includ"
2021.americasnlp-1.23,D18-1269,0,0.0884352,"uage families: Aymaran, Arawak, Chibchan, Tupi-Guarani, UtoAztecan, Oto-Manguean, Quechuan, and Panoan. The ten language pairs included in the shared task are: Quechua–Spanish, Wixarika–Spanish, Shipibo-Konibo–Spanish, Asháninka–Spanish, Raramuri–Spanish, Nahuatl–Spanish, Otomí– Spanish, Aymara–Spanish, Guarani–Spanish, and Bribri–Spanish. For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018). For a complete description of this dataset we refer the reader to Ebrahimi et al. (2021). Together with the data, we also provided: a simple baseline based on the small transformer architecture (Vaswani et al., 2017) proposed together with the FLORES dataset (Guzmán et al., 2019); and a description of challenges and particular characteristics for all provided resources1 . We established two tracks: one where training models on the development set after hyperparameter tuning is 1 https://github.com/AmericasNLP/americasnlp2021/ blob/main/data/information_datasets.pdf 202 Proceedings of the Fir"
2021.americasnlp-1.23,P18-1128,0,0.020486,"8.4 8.3 ChrF 39.4 38.3 35.8 33.2 32.8 31.8 26.9 10.3 9.8 9.0 6.6 ChrF 39.9 38.0 29.7 28.6 16.3 15.5 12.4 ChrF 25.8 24.8 24.7 23.9 21.6 16.5 15.9 12.2 10.5 10.5 8.4 Table 3: Results of Track 1 (development set used for training) for all systems and language pairs. The results are ranked by the official metric of the shared task: ChrF. One team decided to send a anonymous submission (Anonym). Best results are shown in bold, and they are significantly better than the second place team (in each language-pair) according to the Wilcoxon signed-ranked test and Pitman’s permutation test with p&lt;0.05 (Dror et al., 2018). 208 ious high-resource languages, and then finetuned for each target language using the official provided data. 4.7 NRC-CNRC The team of the National Research Council Canada (NRC-CNRC; Knowles et al., 2021) submitted systems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used"
2021.americasnlp-1.23,2020.coling-main.351,1,0.754898,"oken in Paraguay, Bolivia, Argentina and Brazil. It belongs to the Tupian language family (ISO gnw, gun, gug, gui, grn, nhd). The training corpus for Guarani (Chiruzzo et al., 2020) was collected from web sources (blogs and news articles) that contained a mix of dialects, from pure Guarani to more mixed Jopara which combines Guarani with Spanish neologisms. The development and test corpora, on the other hand, are in standard Paraguayan Guarani. Spanish—Bribri Bribri is a Chibchan language spoken in southern Costa Rica (ISO code bzd). The training set for Bribri was extracted from six sources (Feldman and Coto-Solano, 2020; Margery, 2005; Jara Murillo, 2018a; Constenla et al., 2004; Jara Murillo and García Segura, 2013; Jara Murillo, 2018b; Flores Solórzano, 2017), including a dictionary, a grammar, two language learning textbooks, one storybook and the transcribed sentences from Spanish–Wixarika Wixarika (also known as 2 Huichol) with ISO code hch is spoken in Mexico ISO 639-3 for the Nahutal languages: and belongs to the Yuto-Aztecan linguistic family. nch, ncx, naz, nln, nhe, ngu, nhk, nhx, nhp, ncl, nhm, nhy, The training, development and test sets all belong nlv, ppl, nhz, npl, nhc, nhv, to the same dialec"
2021.americasnlp-1.23,galarreta-etal-2017-corpus,1,0.842998,"training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a bilingual teacher, and follows the most recent guidelines of the Minister of Education in Peru, however, the third source is an extraction of parallel sentences from an old dictionary. The development and test sets were created following the official convention as in the translated training sets. Spanish—Quechua Quechua is a family of languages spoken in Argentina, Bolivia, Colombia, Spanish—Asháninka Asháninka is an Ecuador, Peru, and Chile with many ISO codes for Arawakan language (ISO: cni) spoken"
2021.americasnlp-1.23,W19-6804,1,0.768784,"az jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a bilingual teacher, and follows the most recent guidelines of the Minister of Education in Peru, however, the third source is an extraction of parallel sentences from an old dictionary. The development and test sets were created following the official convention as in the translated training sets. Spanish—Quechua Quechua is a family of languages spoken in Argentina, Bolivia, Colombia, Spanish—Asháninka Asháninka is an Ecu"
2021.americasnlp-1.23,N15-2021,1,0.735842,"eceive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available parallel corpora (§3). Additionally, all participants were allowed to use other existing datasets or create their own resources for training in order to improve their systems. E"
2021.americasnlp-1.23,L16-1666,1,0.700987,"ng similarities and differences between training sets on the one hand and development and test sets on the other. The training data (Mager et al., 2018a) is a translation of the fairy tales of Hans Christian Andersen and contains word acquisitions and code-switching. Spanish–Nahuatl Nahuatl is a Yuto-Aztecan language spoken in Mexico and El Salvador, with a wide dialectal variation (around 30 variants). For each main dialect a specific ISO 639-3 code is available.2 There is a lack of consensus regarding the orthographic standard. This is very noticeable in the training data: the train corpus (Gutierrez-Vasques et al., 2016) has dialectal, domain, orthographic and diachronic variation (Nahuatl side). However, the majority of entries are closer to a Classical Nahuatl orthographic “standard”. The development and test datasets were translated to modern Nahuatl. In particular, the translations belong to Nahuatl Central/Nahuatl de la Huasteca (Hidalgo y San Luis Potosí) dialects. In order to be closer to the training corpus, an orthographic normalization was applied. A simple rule based approach was used, which was based on the most predictable orthographic changes between modern varieties and Classical Nahuatl. Spani"
2021.americasnlp-1.23,D19-1632,1,0.923833,"nish, Aymara–Spanish, Guarani–Spanish, and Bribri–Spanish. For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018). For a complete description of this dataset we refer the reader to Ebrahimi et al. (2021). Together with the data, we also provided: a simple baseline based on the small transformer architecture (Vaswani et al., 2017) proposed together with the FLORES dataset (Guzmán et al., 2019); and a description of challenges and particular characteristics for all provided resources1 . We established two tracks: one where training models on the development set after hyperparameter tuning is 1 https://github.com/AmericasNLP/americasnlp2021/ blob/main/data/information_datasets.pdf 202 Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas, pages 202–217 June 11, 2021. ©2021 Association for Computational Linguistics allowed (Track 1), and one where models cannot be trained directly on the development set (Track 2). Machine translation"
2021.americasnlp-1.23,2021.americasnlp-1.25,0,0.125338,"dded to the existing dataset. Then, a normalization process was done using existing tools, and the aligned data was further filtered. The quality of the data was also considered, and each dataset was assigned a weight depending on a noisiness estimation. The team used a transformer sequenceto-sequence model trained via two steps. For their main submission they first trained on data which 4 Otomí online corpus: https://tsunkua.elotl.mx/about/ 206 https://github.com/AmericasNLP/americasnlp2021 Team Langs. Sub. CoAStaL (Bollmann et al., 2021) 10 20 Helsinki (Vázquez et al., 2021) 10 50 NRC-CNRC (Knowles et al., 2021) 4 17 REPUcs (Moreno, 2021) Tamalli (Parida et al., 2021) 1 2 10 UBC-NLP (BillahNagoudi et al., 2021) UTokyo (Zheng et al., 2021) Anonymous Data Models Multilingual Pretrained PB-SMT, Constrained Random Strings Transformer NMT No No 42 Bible, JW300, OPUS, Wikipedia, New collected data Bible, OPUS, Constitutions, Normalization, Filtering, BackTranslation No external data, preoricessing, BPE Dropout. JW300, New dataset, Europarl - 8 29 Bible, Wikipedia 10 40 8 14 Monolingual from other languages. Data - Yes, all ST No languages + Spanish-English Transofrmer NMT Yes, languages 4- No Transformer N"
2021.americasnlp-1.23,P07-2045,0,0.0229964,"lcoxon signed-ranked test and Pitman’s permutation test with p&lt;0.05 (Dror et al., 2018). 208 ious high-resource languages, and then finetuned for each target language using the official provided data. 4.7 NRC-CNRC The team of the National Research Council Canada (NRC-CNRC; Knowles et al., 2021) submitted systems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used BPE for tokenization. The team experimented with multilingual models pretrained on either 3 or 4 languages, finding that the 4 language model achieved higher performance. Additionally the team trained a Translation Memory (Simard and Fujita, 2012) using half of the examples of the development set. Surprisingly, even given its small amount of training data, this system outperformed the team’s Track 2 submission for Rarámuri. 4.8 Tamalli The team Tamalli7 (Parida et al., 2021) participated in Track 1 for all 10 language pairs. T"
2021.americasnlp-1.23,D18-2012,0,0.0268314,"encoders using UD annotations could not get any meaningful results. 4.5 REPUcs the the Spanish–Quechua language pair in both tracks. The team collected external data from 3 different sources and analyzed the domain disparity between this training data and the development set. To solve the problem of domain mismatch, they decided to collect additional data that could be a better match for the target domain. The used data from a handbook (Iter and Ortiz-Cárdenas, 2019), a lexicon,5 and poems on the web (Duran, 2010).6 Their model is a transformer encoder-decoder architecture with SentencePiece (Kudo and Richardson, 2018) tokenization. Together with the existing parallel corpora, the new paired data was used for finetuning on top of a pretrained Spanish–English translation model. The team submitted two versions of their system: the first was only finetuned on JW300+ data, while the second one additionally leveraged the newly collected dataset. 4.6 UTokyo The team of the University of Tokyo (UTokyo; Zheng et al., 2021) submitted systems for all languages and both tracks. A multilingual pretrained encoder-decoder model (mBART; Liu et al., 2020) was used, implemented with the Fairseq toolkit (Ott et al., 2019). T"
2021.americasnlp-1.23,C18-1006,1,0.909443,"ms achieved 12.97 ChrF higher than baseline, when averaged across languages. 1 Introduction Many of the world’s languages, including languages native to the Americas, receive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available para"
2021.americasnlp-1.23,mayer-cysouw-2014-creating,0,0.0200209,"to the Valle del Mezquital dialect (ote). This was specially challenging for the translation task, since the development and test sets are from the Ñûhmû de Ixtenco, Tlaxcala, variant (otz), which also has its own orthographic system. This variant is especially endangered as less than 100 elders still speak it. 3.3 External Data Used by Participants In addition to the provided datasets, participants also used additional publicly available parallel data, monolingual corpora or newly collected data sets. The most common datasets were JW300 (Agi´c and Vuli´c, 2019) and the Bible’s New Testament (Mayer and Cysouw, 2014; Christodouloupoulos and Steedman, 2015; McCarthy et al., 2020). Besides those, GlobalVoices (Prokopidis et al., 2016) and datasets available at OPUS (Tiedemann, 2012) were added. New datasets were extracted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a"
2021.americasnlp-1.23,2020.lrec-1.352,0,0.0111319,"llenging for the translation task, since the development and test sets are from the Ñûhmû de Ixtenco, Tlaxcala, variant (otz), which also has its own orthographic system. This variant is especially endangered as less than 100 elders still speak it. 3.3 External Data Used by Participants In addition to the provided datasets, participants also used additional publicly available parallel data, monolingual corpora or newly collected data sets. The most common datasets were JW300 (Agi´c and Vuli´c, 2019) and the Bible’s New Testament (Mayer and Cysouw, 2014; Christodouloupoulos and Steedman, 2015; McCarthy et al., 2020). Besides those, GlobalVoices (Prokopidis et al., 2016) and datasets available at OPUS (Tiedemann, 2012) were added. New datasets were extracted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 20"
2021.americasnlp-1.23,nordhoff-hammarstrom-2012-glottolog,0,0.0139496,"manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages. 1 Introduction Many of the world’s languages, including languages native to the Americas, receive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; B"
2021.americasnlp-1.23,2020.loresmt-1.1,1,0.864016,"earchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available parallel corpora (§3). Additionally, all participants were allowed to use other existing datasets or create their own resources for training in order to improve their systems. Each language pair used in the shared task c"
2021.americasnlp-1.23,N19-4009,1,0.919181,"acted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 2017). We employed the hyperparameters proposed by Guzmán et al. (2019) for a low-resource scenario. We implemented the model using Fairseq (Ott et al., 2019). The implementation of the baseline can be found in the official shared task repository.4 4.2 University of British Columbia The team of the University of British Columbia (UBC-NLP; Billah-Nagoudi et al., 2021) participated for all ten language pairs and in both tracks. They used an encoder-decoder transformer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages h"
2021.americasnlp-1.23,P02-1040,0,0.110098,"ee to use any resources they could find. Possible resources could, for instance, include existing or newly created parallel data, dictionaries, tools, or pretrained models. We invited submissions to two different tracks: Systems in Track 1 were allowed to use the development set as part of the training data, since this is a common practice in the machine translation community. Systems in Track 2 were not allowed to be trained directly on the development set, mimicking a more realistic low-resource setting. 2.2 resulting in a small number of words per sentence. We further reported BLEU scores (Papineni et al., 2002) for all systems and languages. Adequacy The output sentence expresses the meaning of the reference. 1. Extremely bad: The original meaning is not contained at all. 2. Bad: Some words or phrases allow to guess the content. 3. Neutral. 4. Sufficiently good: The original meaning is understandable, but some parts are unclear or incorrect. 5. Excellent: The meaning of the output is the same as that of the reference. Fluency The output sentence is easily readable and looks like a human-produced text. Primary Evaluation In order to be able to evaluate a large number of systems on all 10 languages, w"
2021.americasnlp-1.23,2021.americasnlp-1.24,0,0.0516404,"Missing"
2021.americasnlp-1.23,W15-3049,0,0.0738763,"Missing"
2021.americasnlp-1.23,L16-1144,0,0.161871,"2019), which consists of Jehovah’s Witness texts, sentences extracted from the official dictionary of the Minister of Education (MINEDU), and miscellaneous dictionary entries and samples which have been collected and reviewed by Huarcaya Taquiri (2020). Spanish–Aymara Aymara is a Aymaran language spoken in Bolivia, Peru, and Chile (ISO codes aym, ayr, ayc). The development and test sets are translated into the Central Aymara variant (ayr), specifically Aymara La Paz jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al.,"
2021.americasnlp-1.23,2012.amta-papers.26,0,0.0148085,"ems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used BPE for tokenization. The team experimented with multilingual models pretrained on either 3 or 4 languages, finding that the 4 language model achieved higher performance. Additionally the team trained a Translation Memory (Simard and Fujita, 2012) using half of the examples of the development set. Surprisingly, even given its small amount of training data, this system outperformed the team’s Track 2 submission for Rarámuri. 4.8 Tamalli The team Tamalli7 (Parida et al., 2021) participated in Track 1 for all 10 language pairs. The team used an IBM Model 2 for SMT, and a transformer model for NMT. The team’s NMT models were trained in two settings: one-to-one, with one model being trained per target language, and one-to-many, where decoder weights were shared across languages and a language embedding layer was added to the decoder. They s"
2021.americasnlp-1.23,tiedemann-2012-parallel,0,0.274715,"sentences extracted from the official dictionary of the Minister of Education (MINEDU), and miscellaneous dictionary entries and samples which have been collected and reviewed by Huarcaya Taquiri (2020). Spanish–Aymara Aymara is a Aymaran language spoken in Bolivia, Peru, and Chile (ISO codes aym, ayr, ayc). The development and test sets are translated into the Central Aymara variant (ayr), specifically Aymara La Paz jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a"
2021.americasnlp-1.23,2021.americasnlp-1.29,0,0.0275308,"rmer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages have more available data than others, this dataset is unbalanced in favor of languages like Nahuatl, Guarani, and Quechua. The team also proposed a two-stage fine-tuning method: first fine-tuning on the entire dataset, and then only on the target languages. 4.3 Helsinki The University of Helsinki (Helsinki; Vázquez et al., 2021) participated for all ten language pairs in both tracks. This team did an extensive exploration of the existing datasets, and collected additional resources both from commonly used sources such as the Bible and Wikipedia, as well as other minor sources such as constitutions. Monolingual data was used to generate paired sentences through back-translation, and these parallel examples were added to the existing dataset. Then, a normalization process was done using existing tools, and the aligned data was further filtered. The quality of the data was also considered, and each dataset was assigned"
2021.americasnlp-1.23,2020.emnlp-main.43,0,0.0550595,"Missing"
2021.americasnlp-1.23,2021.americasnlp-1.26,0,0.0686019,"d. The quality of the data was also considered, and each dataset was assigned a weight depending on a noisiness estimation. The team used a transformer sequenceto-sequence model trained via two steps. For their main submission they first trained on data which 4 Otomí online corpus: https://tsunkua.elotl.mx/about/ 206 https://github.com/AmericasNLP/americasnlp2021 Team Langs. Sub. CoAStaL (Bollmann et al., 2021) 10 20 Helsinki (Vázquez et al., 2021) 10 50 NRC-CNRC (Knowles et al., 2021) 4 17 REPUcs (Moreno, 2021) Tamalli (Parida et al., 2021) 1 2 10 UBC-NLP (BillahNagoudi et al., 2021) UTokyo (Zheng et al., 2021) Anonymous Data Models Multilingual Pretrained PB-SMT, Constrained Random Strings Transformer NMT No No 42 Bible, JW300, OPUS, Wikipedia, New collected data Bible, OPUS, Constitutions, Normalization, Filtering, BackTranslation No external data, preoricessing, BPE Dropout. JW300, New dataset, Europarl - 8 29 Bible, Wikipedia 10 40 8 14 Monolingual from other languages. Data - Yes, all ST No languages + Spanish-English Transofrmer NMT Yes, languages 4- No Transformer NMT. WB-SMT. Transformer NMT, Transformer T5 Yes, with Spanish-English 10-languages Spanish-English pretraining No 10-Languages Ne"
2021.conll-1.2,2020.emnlp-main.703,1,0.887914,"Missing"
2021.conll-1.2,D17-1171,0,0.0454038,"Missing"
2021.conll-1.2,W01-0713,0,0.157655,"Missing"
2021.conll-1.2,J03-4003,0,0.403075,"syntactic modeling allows for a binary probabilistic treatment between each word and its head. This binary relationship introduces flexibility for modeling languages with rich morphology and relatively free word order languages (Jurafsky and Martin, 2009). A commonly used model for dependency induction is the dependency model with valence (Klein and Manning, 2004, DMV). The two formalisms are complementary and unified models have been shown to achieve higher induction accuracy than models that are trained for either constituency or dependency parsing alone. For instance, the lexicalized PCFG (Collins, 2003) is a supervised model that jointly learns the two formalisms by associating a word and a part-ofspeech tag with each non-terminal in the tree of the PCFG. A more recent variant, the neural LPCFG (Zhu et al., 2020) is an unsupervised model that uses lexical information to jointly learn both constituency-structure and dependency-structure grammars, achieving state-of-the-art performance in both formalisms. Introduction Grammar induction aims to discover the underlying grammatical structure of a language from strings of symbols. Previous work has focused on two grammar formalisms: constituency a"
2021.conll-1.2,D18-1160,1,0.904366,"Missing"
2021.conll-1.2,P19-1180,0,0.357626,"tags are given to the grammar induction models. However, explicitly providing a ruleset can make non-terminals sensitive to their context, which is slightly contradictory to context-free grammar based methods, such as PCFGs. Thus, a more abstract method is needed to provide both guidance and flexibility to grammar induction models. Nevertheless, inducing grammars entirely from pure text is a tall order, and arguably one not even tackled by humans themselves – humans learn language from the surrounding world (Bisk et al., 2020). Recent work, such as the Visually Grounded Neural Syntax Learner (Shi et al., 2019, VGNSL) show improved performance on constituency grammar induction by learning a shared embedding space for text and its corresponding image. However, Kojima et al. (2020) note that the improvement in performance is not a result of learning complex syntactic rules, and is rather of the model indirectly learning word concreteness, a concept that evaluates the degree to which a word refers to a perceptible entity (Brysbaert et al., 2013). However, concisely describing the effect of word concreteness on the constituency structure of a sentence is non-trivial without resorting to languagespecifi"
2021.conll-1.2,P19-1228,0,0.0249851,"Missing"
2021.conll-1.2,P18-1249,0,0.0233228,"ural LPCFG (Zhu et al., 2020), i.e., the base model described in Section 2. ous works on dependency induction, MSCOCO has been used in previous work on visually informed constituency induction (Shi et al., 2019; Zhao and Titov, 2020). Using the same splits as VGNSL, the dataset contains 82,783 images for training, 1,000 for validation and testing, respectively. Each image has five corresponding captions. As a preprocessing step, we tokenize and lemmatize the captions (hyphenated words are preserved as a single token) using SpaCy (Honnibal et al., 2020). We use the Berkeley constituent parser (Kitaev and Klein, 2018) to create the gold trees, and we use the Stanford Parser for generating the gold dependencies (de Marneffe and Manning, 2008) for the validation and test sets. 4.3 To compare with previous work, we use pre-trained FastText (Joulin et al., 2016) embeddings for the MSCOCO dataset on all variants of our model described in Section 3. We initialized the preterminals embeddings with centroids obtained using K-means clustering on the pre-trained word embeddings following Zhu et al. (2020). This initialization allows the model to have a preliminary understanding of word meanings before training. As d"
2021.conll-1.2,2020.emnlp-main.354,0,0.0532282,"Missing"
2021.conll-1.2,P04-1061,0,0.230293,"he neural L-PCFG which uses pure text and our proposed C ONCRETE L-P CFG which uses word concreteness, respectively. neural L-PCFG incorrectly predicts observing as the root of the sentence, while C ONCRETE L-P CFG selects the correct root fans. approach to syntactic modeling allows for a binary probabilistic treatment between each word and its head. This binary relationship introduces flexibility for modeling languages with rich morphology and relatively free word order languages (Jurafsky and Martin, 2009). A commonly used model for dependency induction is the dependency model with valence (Klein and Manning, 2004, DMV). The two formalisms are complementary and unified models have been shown to achieve higher induction accuracy than models that are trained for either constituency or dependency parsing alone. For instance, the lexicalized PCFG (Collins, 2003) is a supervised model that jointly learns the two formalisms by associating a word and a part-ofspeech tag with each non-terminal in the tree of the PCFG. A more recent variant, the neural LPCFG (Zhu et al., 2020) is an unsupervised model that uses lexical information to jointly learn both constituency-structure and dependency-structure grammars, a"
2021.conll-1.2,2020.tacl-1.42,1,0.819787,"order languages (Jurafsky and Martin, 2009). A commonly used model for dependency induction is the dependency model with valence (Klein and Manning, 2004, DMV). The two formalisms are complementary and unified models have been shown to achieve higher induction accuracy than models that are trained for either constituency or dependency parsing alone. For instance, the lexicalized PCFG (Collins, 2003) is a supervised model that jointly learns the two formalisms by associating a word and a part-ofspeech tag with each non-terminal in the tree of the PCFG. A more recent variant, the neural LPCFG (Zhu et al., 2020) is an unsupervised model that uses lexical information to jointly learn both constituency-structure and dependency-structure grammars, achieving state-of-the-art performance in both formalisms. Introduction Grammar induction aims to discover the underlying grammatical structure of a language from strings of symbols. Previous work has focused on two grammar formalisms: constituency and dependency grammars. Probabilistic context-free grammars (Charniak, 1996; Clark, 2001, PCFG) have been used for modeling probabilistic rules in a constituency grammar, including more recent approaches that combi"
2021.conll-1.2,2020.acl-main.234,0,0.145649,"y to context-free grammar based methods, such as PCFGs. Thus, a more abstract method is needed to provide both guidance and flexibility to grammar induction models. Nevertheless, inducing grammars entirely from pure text is a tall order, and arguably one not even tackled by humans themselves – humans learn language from the surrounding world (Bisk et al., 2020). Recent work, such as the Visually Grounded Neural Syntax Learner (Shi et al., 2019, VGNSL) show improved performance on constituency grammar induction by learning a shared embedding space for text and its corresponding image. However, Kojima et al. (2020) note that the improvement in performance is not a result of learning complex syntactic rules, and is rather of the model indirectly learning word concreteness, a concept that evaluates the degree to which a word refers to a perceptible entity (Brysbaert et al., 2013). However, concisely describing the effect of word concreteness on the constituency structure of a sentence is non-trivial without resorting to languagespecific heuristics such as the head-initial bias of the VGNSL (Shi et al., 2019). On the other At the phrase level, we present a vision-based heuristic to exploit concreteness by"
2021.conll-1.2,P97-1063,0,0.598022,"fore the alignment by removing the role labels. The first entity is always the main activity followed by the participants in the order they appear in the semantic labeling predictions. We also removed stop words from the captions to generate more reliable alignment pairs. This is largely because a misalignment, such as aligning determiners with nouns, can potentially be detrimental to use the C OUPLING heuristic on the neural L-PCFG. An example of the alignment inputs and the generated outputs with the corresponding alignment scores are described in Figure 3. We use the Dice alignment method (Melamed, 1997) to find co-occurrences of the caption words and semantic labels and generate alignment pairs between them. Because semantic role labels explain the situation depicted in the image, it is reasonable to couple the words of the corresponding caption that realizes the role labels in a similar way: couple the predicate representative, eat, with the representatives of its arguments, girl and cheesecake. 4.1 Dataset We use the MSCOCO dataset because it has a large number of visually concrete concepts, which provides a good testbed for our model. Although the MSCOCO dataset is not commonly used in pr"
2021.conll-1.2,D10-1120,0,0.0480089,"Missing"
2021.eacl-main.181,2020.acl-main.740,0,0.061853,"Missing"
2021.eacl-main.181,D18-1190,0,0.0142344,"y {zdou,gneubig}@cs.cmu.edu After Fine-tuning Figure 1: Cosine similarities between subword representations in a parallel sentence pair before and after fine-tuning. Red boxes indicate the gold alignments. Introduction Word alignment is a useful tool to tackle a variety of natural language processing (NLP) tasks, including learning translation lexicons (Ammar et al., 2016; Cao et al., 2019), cross-lingual transfer of language processing tools (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Tiedemann, 2014; Agi´c et al., 2016; Mayhew et al., 2017; Nicolai and Yarowsky, 2019), semantic parsing (Herzig and Berant, 2018) and speech recognition (Xu et al., 2019). In particular, word alignment plays a crucial role in many machine translation (MT) related methods, including guiding learned attention (Liu et al., 2016), incorporating lexicons during decoding (Arthur et al., 2016), domain adaptation (Hu et al., 2019), unsupervised MT (Ren et al., 2020) and automatic evaluation or analysis of translation models (Bau et al., 2018; Stanovsky et al., 2019; Neubig et al., 2019; Wang et al., 2020). However, with neural networks advancing the state of the arts in almost every field of NLP, tools developed based on the 30"
2021.eacl-main.181,P19-1286,1,0.813839,"P) tasks, including learning translation lexicons (Ammar et al., 2016; Cao et al., 2019), cross-lingual transfer of language processing tools (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Tiedemann, 2014; Agi´c et al., 2016; Mayhew et al., 2017; Nicolai and Yarowsky, 2019), semantic parsing (Herzig and Berant, 2018) and speech recognition (Xu et al., 2019). In particular, word alignment plays a crucial role in many machine translation (MT) related methods, including guiding learned attention (Liu et al., 2016), incorporating lexicons during decoding (Arthur et al., 2016), domain adaptation (Hu et al., 2019), unsupervised MT (Ren et al., 2020) and automatic evaluation or analysis of translation models (Bau et al., 2018; Stanovsky et al., 2019; Neubig et al., 2019; Wang et al., 2020). However, with neural networks advancing the state of the arts in almost every field of NLP, tools developed based on the 30-yearold IBM word-based translation models (Brown et al., 1993), such as GIZA++ (Och and Ney, 2003) or fast-align (Dyer et al., 2013), remain popular choices for word alignment tasks. One alternative to using statistical word-based translation models to learn alignments would be to instead train"
2021.eacl-main.181,2005.mtsummit-papers.11,0,0.29534,"set to tune the hyper-parameters. 3.1 Baselines. We compare our models with: Setup Datasets. We perform experiments on five different language pairs, namely German-English (DeEn), French-English (Fr-En), Romanian-English (Ro-En), Japanese-English (Ja-En) and ChineseEnglish (Zh-En). For the De-En, Fr-En, Ro-En datasets, we follow the experimental setting of previous work (Zenkel et al., 2019; Garg et al., 2019; Zenkel et al., 2020). The training and test data for Ro-En and Fr-En are provided by Mihalcea and Pedersen (2003). The Ro-En training data are also augmented by the Europarl v8 corpus (Koehn, 2005). For the De-En data, the Europarl v7 corpus is used as training data and the gold alignments are provided by Vilar et al. (2006). The Ja-En dataset is obtained from the Kyoto Free Translation Task (KFTT) word alignment data (Neubig, 2011), and the Japanese sentences are tokenized with the KyTea tokenizer (Neubig et al., 2011). The Zh-En dataset is obtained from the TsinghuaAligner website1 . We treat their evaluation set as the training data and use the test set in Liu and Sun (2015). • fast align (Dyer et al., 2013): a popular statistical word aligner which is a simple, fast reparameterizati"
2021.eacl-main.181,2020.acl-main.146,0,0.116273,"al., 2020). However, with neural networks advancing the state of the arts in almost every field of NLP, tools developed based on the 30-yearold IBM word-based translation models (Brown et al., 1993), such as GIZA++ (Och and Ney, 2003) or fast-align (Dyer et al., 2013), remain popular choices for word alignment tasks. One alternative to using statistical word-based translation models to learn alignments would be to instead train state-of-the-art neural machine translation (NMT) models on parallel corpora, and extract alignments therefrom, as examined by Luong et al. (2015); Garg et al. (2019); Zenkel et al. (2020). However, these methods have two disadvantages (also shared with more traditional alignment methods): (1) they are directional and the source and target side are treated differently and (2) they cannot easily take advantage of large-scale contextualized 2112 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2112–2128 April 19 - 23, 2021. ©2021 Association for Computational Linguistics word embeddings derived from language models (LMs) multilingually trained on monolingual corpora (Devlin et al., 2019; Lample and Conneau, 2019; C"
2021.eacl-main.181,D17-1207,0,0.0192449,"ction methods and combine the best of the two worlds by fine-tuning contextualized embeddings on parallel data. There are also work on supervised neural word alignment (Stengel-Eskin et al., 2019; Nagata et al., 2020). However, supervised data are not always accessible, making their methods inapplicable in many scenarios. In this paper, we demonstrate that our model can incorporate supervised signals if available and perform semi-supervised learning, which is a more realistic and general setting. Some work on bilingual lexicon induction also share similar general ideas with ours. For example, Zhang et al. (2017) minimize the earth mover’s distance to match the embedding distributions from different languages. Similarly, Grave et al. (2019) present an algorithm to align point clouds with Procrustes (Sch¨onemann, 1966) in Wasserstein distance for unsupervised embedding alignment. 5 Discussion and Conclusion We present a neural word aligner that achieves stateof-the-art performance on five diverse language pairs and obtains robust performance in zero-shot settings. We propose to fine-tune multilingual embeddings with objectives suitable for word alignment and develop two alignment extraction methods. We"
2021.eacl-main.181,2006.iwslt-papers.7,0,0.118978,"Missing"
2021.eacl-main.181,C96-2141,0,0.865801,". Our objectives can improve the model cross-lingual transfer ability. “*” denotes significant differences using paired bootstrapping (p&lt;0.05) . Alignment Examples. We also conduct qualitative analyses as shown in Figure 1, 2 and 3. After fine-tuning, the learned contextualized representations are more aligned, as the cosine distances between semantically similar words become closer, and the extracted alignments are more accurate. More examples are shown in Appendix B. 4 Related Work Based on the IBM translation models (Brown et al., 1993), many statistical word aligners have been ¨ proposed (Vogel et al., 1996; Ostling and Tiedemann, 2016), including the current most popular tools GIZA++ (Och and Ney, 2000, 2003; Gao and Vogel, 2008) and fast align (Dyer et al., 2013). Recently, there is a resurgence of interest in neural word alignment (Tamura et al., 2014; Alkhouli et al., 2018). Based on NMT models trained on parallel corpora, researchers have proposed several methods to extract alignments from them (Luong et al., 2015; Zenkel et al., 2019; Garg et al., 2019; Li et al., 2019) and successfully build an end-to-end neural model that can outperform statistical tools (Zenkel et al., 2020). However, t"
2021.eacl-main.181,2020.acl-main.278,0,0.0208467,"Lapata, 2009; Tiedemann, 2014; Agi´c et al., 2016; Mayhew et al., 2017; Nicolai and Yarowsky, 2019), semantic parsing (Herzig and Berant, 2018) and speech recognition (Xu et al., 2019). In particular, word alignment plays a crucial role in many machine translation (MT) related methods, including guiding learned attention (Liu et al., 2016), incorporating lexicons during decoding (Arthur et al., 2016), domain adaptation (Hu et al., 2019), unsupervised MT (Ren et al., 2020) and automatic evaluation or analysis of translation models (Bau et al., 2018; Stanovsky et al., 2019; Neubig et al., 2019; Wang et al., 2020). However, with neural networks advancing the state of the arts in almost every field of NLP, tools developed based on the 30-yearold IBM word-based translation models (Brown et al., 1993), such as GIZA++ (Och and Ney, 2003) or fast-align (Dyer et al., 2013), remain popular choices for word alignment tasks. One alternative to using statistical word-based translation models to learn alignments would be to instead train state-of-the-art neural machine translation (NMT) models on parallel corpora, and extract alignments therefrom, as examined by Luong et al. (2015); Garg et al. (2019); Zenkel et"
2021.eacl-main.181,P13-2123,0,0.0734125,"Missing"
2021.eacl-main.181,D13-1056,0,0.052181,"Missing"
2021.eacl-main.181,H01-1035,0,0.856464,"r rd g e e Abstract Th e ce ssa co ry rre ## ct ion wil l be ma de . Zi-Yi Dou, Graham Neubig Language Technologies Institute, Carnegie Mellon University {zdou,gneubig}@cs.cmu.edu After Fine-tuning Figure 1: Cosine similarities between subword representations in a parallel sentence pair before and after fine-tuning. Red boxes indicate the gold alignments. Introduction Word alignment is a useful tool to tackle a variety of natural language processing (NLP) tasks, including learning translation lexicons (Ammar et al., 2016; Cao et al., 2019), cross-lingual transfer of language processing tools (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Tiedemann, 2014; Agi´c et al., 2016; Mayhew et al., 2017; Nicolai and Yarowsky, 2019), semantic parsing (Herzig and Berant, 2018) and speech recognition (Xu et al., 2019). In particular, word alignment plays a crucial role in many machine translation (MT) related methods, including guiding learned attention (Liu et al., 2016), incorporating lexicons during decoding (Arthur et al., 2016), domain adaptation (Hu et al., 2019), unsupervised MT (Ren et al., 2020) and automatic evaluation or analysis of translation models (Bau et al., 2018; Stanovsky et al., 2019; Neubig et"
2021.eacl-main.324,C18-1139,0,0.0126384,"length), (ii) and calculate performance (e.g., F1 score) for each bucket. Therefore, data-wise (ΦDts ), the input of performance prediction function in Eq. 4 (gf ine (·)) can be featurized as different types of (i) buckets (ii) aspects (iii) datasets. Additionally, we take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this paper (e.g, entity length and"
2021.eacl-main.324,D08-1078,0,0.0310694,"learning system’s performance based on features of the underlying problem, dataset, or learning algorithm. While this topic is still relatively unexplored in the NLP context, there are a few examples of predicting performance as: (i) a function of training or model parameters for determining the number of training iterations (Kolachina et al., 2012) or value of hyperparameters (Rosenfeld et al., 2019) and identifying and terminating bad training runs (Domhan et al., 2015). (ii) a function of dataset characteristics to illustrate which factors are significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However,"
2021.eacl-main.324,P18-1128,0,0.020658,"nsor can be conceived as a superposition of low-rank components and a sparse component, Robust PCA attempts to recover the low-rank and sparse components. The sparse components can be considered as the gross, but sparse noise in the dataset. 4 Statistical Preliminaries Before going into our second contribution to establishing reliability of performance prediction, we describe two relevant concepts from statistics. 4.1 Confidence Interval (CI) The confidence interval (CI) is a range of possible values for an unknown parameter associated with a confidence level of γ (Nakagawa and Cuthill, 2007; Dror et al., 2018) that the actual parameter can fall into the suggested range. Specifically, suppose that we are interested in estimating the underlying true parameter of ω. Given an observed parameter estimate of ω ˆ , obtained from the data, we aim to compute an interval with a confidence level γ that ω lies in an interval CI. Commonly, there are two approaches to calculate confidence intervals, depending on our knowledge about the distribution of the statistics of interest. When an analytical form exists and we have reasonable assumptions on the distribution, we can employ the normal theory or use Student’s"
2021.eacl-main.324,2020.emnlp-main.489,1,0.832238,"Missing"
2021.eacl-main.324,P14-1062,0,0.0564615,"of (i) buckets (ii) aspects (iii) datasets. Additionally, we take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this paper (e.g, entity length and sentence length). 4. Buckets: The test entities (words) of the NER (CWS) task are partitioned into four buckets according to their attribute value. We compute the F1 score for the entities. 3 Parameterized Regression"
2021.eacl-main.324,W04-3250,0,0.279377,"h replacement from a distribution that approximates it, thereby allowing us to make inferences about the statistics of interest and construct confidence intervals. Common methods to construct the CI with bootstrap include the percentile method, where after specifying a confidence level γ, we take the range of points that cover the middle γ proportion of bootstrap sampling distribution Yˆ as the desired confidence interval, represented by (QYˆ ((1−γ)/2) , QYˆ ((1+γ)/2) ), where Q denotes the quantile. Works on establishing confidence for results in NLP tasks using this bootstrap method include Koehn (2004) and Li et al. (2017). acc(Bm ) = Model Calibration (MC) Calibration (Gleser, 1996), also known as reliability, refers to the ability of a model to make good probabilistic predictions. For a discrete distribution over events, a model is said to be wellcalibrated if for those events that the model assigns a probability of p, the long-run proportion that the event actually occurs turns out to be p. For example, if a weather forecast model predicts that there is a 0.1 probability of rain at 7 a.m., then when observed on a large number of random trials at 7 a.m., the model is well-calibrated if 0."
2021.eacl-main.324,P12-1003,0,0.0210698,"prediction model. Gray and red lines represent corresponding confidence intervals. Numbers in each bar indicate the number of test samples in each length bucket. //github.com/neulab/Reliable-NLPPP 1 3206 Introduction Performance prediction (P2 ) aims to predict a machine learning system’s performance based on features of the underlying problem, dataset, or learning algorithm. While this topic is still relatively unexplored in the NLP context, there are a few examples of predicting performance as: (i) a function of training or model parameters for determining the number of training iterations (Kolachina et al., 2012) or value of hyperparameters (Rosenfeld et al., 2019) and identifying and terminating bad training runs (Domhan et al., 2015). (ii) a function of dataset characteristics to illustrate which factors are significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance pr"
2021.eacl-main.324,P02-1040,0,0.112252,"significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine-grained evaluation scores (§2.3), and"
2021.eacl-main.324,D14-1162,0,0.0900421,"h bucket. Therefore, data-wise (ΦDts ), the input of performance prediction function in Eq. 4 (gf ine (·)) can be featurized as different types of (i) buckets (ii) aspects (iii) datasets. Additionally, we take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this paper (e.g, entity length and sentence length). 4. Buckets: The test entities (words) of the NER (CWS"
2021.eacl-main.324,N18-1202,0,0.0350715,"n a certain aspect (e.g., entity length), (ii) and calculate performance (e.g., F1 score) for each bucket. Therefore, data-wise (ΦDts ), the input of performance prediction function in Eq. 4 (gf ine (·)) can be featurized as different types of (i) buckets (ii) aspects (iii) datasets. Additionally, we take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this"
2021.eacl-main.324,D12-1096,0,0.0266754,"questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine-grained evaluation scores (§2.3), and also propose performance prediction methods particularly suited to this fine-grained evaluation setting (§3). Our second contribution is the development of methods for estimating the reliability of performance predictions. While allowing"
2021.eacl-main.324,W09-1119,0,0.0978428,"istics to illustrate which factors are significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine"
2021.eacl-main.324,D13-1027,0,0.0148,"performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine-grained evaluation scores (§2.3), and also propose performance prediction methods particularly suited to this fine-grained evaluation setting (§3). Our second contribution is the development of methods for estimating the reliability of performance predictions. While allowing estimation 3703 Proceedings"
2021.eacl-main.324,N16-1030,0,0.0188411,"take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this paper (e.g, entity length and sentence length). 4. Buckets: The test entities (words) of the NER (CWS) task are partitioned into four buckets according to their attribute value. We compute the F1 score for the entities. 3 Parameterized Regression Functions The performance prediction model takes in a"
2021.eacl-main.324,P17-1064,0,0.0115779,"m a distribution that approximates it, thereby allowing us to make inferences about the statistics of interest and construct confidence intervals. Common methods to construct the CI with bootstrap include the percentile method, where after specifying a confidence level γ, we take the range of points that cover the middle γ proportion of bootstrap sampling distribution Yˆ as the desired confidence interval, represented by (QYˆ ((1−γ)/2) , QYˆ ((1+γ)/2) ), where Q denotes the quantile. Works on establishing confidence for results in NLP tasks using this bootstrap method include Koehn (2004) and Li et al. (2017). acc(Bm ) = Model Calibration (MC) Calibration (Gleser, 1996), also known as reliability, refers to the ability of a model to make good probabilistic predictions. For a discrete distribution over events, a model is said to be wellcalibrated if for those events that the model assigns a probability of p, the long-run proportion that the event actually occurs turns out to be p. For example, if a weather forecast model predicts that there is a 0.1 probability of rain at 7 a.m., then when observed on a large number of random trials at 7 a.m., the model is well-calibrated if 0.1 of them actually do"
2021.eacl-main.324,N19-4007,1,0.8592,"on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine-grained evaluation scores (§2.3), and also propose performance prediction methods particularly suited to this fine-grained evaluation setting (§3). Our second contribution is the development of methods for estimating the reliability of performance predictions. While allowing estimation 3703 Proceedings of the 16th Conference of the European Chapt"
2021.eacl-main.324,2020.emnlp-main.213,0,0.0132722,"for reliability analysis of the predicted confidence interval, which could also be explored on other scenarios, e.g., density forecasting (Diebold et al., 1997). Another potentially valuable research topic is to build connections with the probability integral transform (Angus, 1994), which is a typical method of calibration evaluation in financial risks, and our proposed calibration method. Calibration for automated evaluation metrics: From a broader point of view, the role of existing learnable automatic evaluation metrics for text generation, such as BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), is similar to a performance prediction model (i.e., both take features of input data as input and then output an evaluation score). Reliability analysis of these metrics is also an important topic since they determine the direction of model optimization. Implications and Future Directions In this work, we not1 only widen the applicability of performance prediction, extending it to finegrained evaluation scenarios, but also establish a 1 set of reliability analysis mechanisms to improve its practicality. In closing, we highlight some potential future directions: We sincerely thank all reviewe"
2021.eacl-main.324,2021.eacl-main.115,0,0.0252345,"Missing"
2021.eacl-main.324,2020.acl-main.704,0,0.0333875,"dence: Our work provides an idea for reliability analysis of the predicted confidence interval, which could also be explored on other scenarios, e.g., density forecasting (Diebold et al., 1997). Another potentially valuable research topic is to build connections with the probability integral transform (Angus, 1994), which is a typical method of calibration evaluation in financial risks, and our proposed calibration method. Calibration for automated evaluation metrics: From a broader point of view, the role of existing learnable automatic evaluation metrics for text generation, such as BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), is similar to a performance prediction model (i.e., both take features of input data as input and then output an evaluation score). Reliability analysis of these metrics is also an important topic since they determine the direction of model optimization. Implications and Future Directions In this work, we not1 only widen the applicability of performance prediction, extending it to finegrained evaluation scenarios, but also establish a 1 set of reliability analysis mechanisms to improve its practicality. In closing, we highlight some potential future directions: W"
2021.eacl-main.324,W08-0305,0,0.138819,"Missing"
2021.eacl-main.324,2020.acl-main.278,0,0.023207,"nts, a model is said to be wellcalibrated if for those events that the model assigns a probability of p, the long-run proportion that the event actually occurs turns out to be p. For example, if a weather forecast model predicts that there is a 0.1 probability of rain at 7 a.m., then when observed on a large number of random trials at 7 a.m., the model is well-calibrated if 0.1 of them actually do result in rain. Similarly, for a classification model matching the probability a model assigns to a predicted label (i.e., confidence) and the correctness measure of the prediction (i.e., accuracy) (Wang et al., 2020) is desired. Nonetheless, it is common that a model could have a high predictive accuracy, but poor calibration if the model systematically over- or under-estimates its confidence in the predictions it makes. One way to quantify miscalibration is to use Expected Calibration Error (ECE; Nae (2015)), which aims to quantitatively characterize the difference in expectation between confidence and accuracy. To calculate ECE, the predictions should first be partitioned into M buckets based on the confidence of the predictions, where N represents the total number of prediction samples and |Bm |is the"
2021.eacl-main.324,2020.acl-main.764,1,0.887359,"tively unexplored in the NLP context, there are a few examples of predicting performance as: (i) a function of training or model parameters for determining the number of training iterations (Kolachina et al., 2012) or value of hyperparameters (Rosenfeld et al., 2019) and identifying and terminating bad training runs (Domhan et al., 2015). (ii) a function of dataset characteristics to illustrate which factors are significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained an"
2021.eacl-main.324,P19-1579,1,0.786512,"ettings) so that they can know which training setting can lead to bad performance without running. Dodge et al. (2020) estimate validation performance as a function of computation budget to conduct more robust model comparisons. Why Performance Prediction matters for NLP tasks Firstly, for some NLP tasks with few resources, it is challenging to build and test systems for all languages or domains. For example, the task of Machine Translation (MT) for low resource languages is hard due to the lack of the large parallel corpora, preventing us from measuring system performance in these scenarios (Xia et al., 2019, 2020). Therefore, performance prediction is useful in that it can efficiently and comprehensively give insights about the workings of models over a wide variety of task settings. Secondly, performance prediction can be used to alleviate the data 3704 sparsity problem in fine-grained evaluation, which plays an important role in current NLP task evaluation (Fu et al., 2020a). In this paper, we consider two performance prediction scenarios, a holistic evaluation setting that most previous works have explored, and a novel setting of predicting fine-grained evaluation metrics. Below, we briefly d"
2021.emnlp-main.458,N19-1388,0,0.0235526,"ults in trade-offs or decreased performance on some languages, particularly LRLs (Arivazha1 Introduction gan et al., 2019; Wang et al., 2020b, 2021). To Multilingual methods that process multiple lan- better control this trade-off, a common practice is to balance the training distribution by heuristic guages with one single model have gained favor across a variety of NLP tasks (Firat et al., 2016; oversampling of LRLs (Johnson et al., 2017; NeuHa et al., 2016; Johnson et al., 2017; Devlin et al., big and Hu, 2018; Arivazhagan et al., 2019). Although simple data balancing can improve the 2019; Aharoni et al., 2019; Conneau et al., 2020) because (1) training and deployment of one mul- performance on LRLs significantly, it is far from optimal. First, the sampling hyperparameters need tilingual model is more computationally efficient compared to maintaining one model for each lan- to be adjusted for different datasets. Second, the guage (Arivazhagan et al., 2019), (2) training mul- use of simple heuristics does not consider the inherent level of difficulty in learning each language, the tilingually can improve accuracy, particularly for low-resource languages (LRLs) (Zoph et al., 2016; similarity between"
2021.emnlp-main.458,N19-1423,0,0.0200253,"rage more uniform performance across language pairs. Weighted Risk Minimization and Sampling Notation. Throughout this paper, n denotes the Strategies. The amount of training data can vary training set size and d the number of parameters significantly across language pairs. As a result, 5665 in ERM training—i.e. optimizing for the average loss across sentence pairs—HRLs contribute most of the objective, resulting in poor performance on LRLs. Balancing the objective—or equivalently, the usage of training data—between HRLs and LRLs is important to maintain good performance across all languages (Devlin et al., 2019; Arivazhagan et al., 2019). A commonly adopted approach in multilingual training is temperature-based sampling (Arivazhagan et al., 2019; Conneau et al., 2020) where the probability of sampling data from Di is proportional to its data size exponentiated 1/τ i| by a temperature term τ , i.e. pτ,i = P|D|D (re|1/τ j j ferred to as ERM with τ in §4). This is equivalent to optimizing the re-weighted objective X Lτ (θ; D) = pτ,i L(θ; Di ). i≤N As a result, τ = 1 corresponds to ERM where most of the contribution comes from the HRLs and τ = ∞ corresponds to sampling language pairs uniformly at random"
2021.emnlp-main.458,D18-1103,1,0.848176,"on LRLs significantly, it is far from optimal. First, the sampling hyperparameters need tilingual model is more computationally efficient compared to maintaining one model for each lan- to be adjusted for different datasets. Second, the guage (Arivazhagan et al., 2019), (2) training mul- use of simple heuristics does not consider the inherent level of difficulty in learning each language, the tilingually can improve accuracy, particularly for low-resource languages (LRLs) (Zoph et al., 2016; similarity between languages in the multilingual dataset, and other factors that affect cross-lingual Neubig and Hu, 2018; Pires et al., 2019). transfer. Because of this, previous work has indiHowever, in multilingual training, the amount cated the importance of learning strategies that are and type of training data available varies greatly explicitly tailored for each multilingual learning ∗ Equal contribution. scenario (Wang et al., 2020a). 1 Our code is available at https://github.com/ violet-zct/fairseq-dro-mnmt In this paper, we propose a new learning proce5664 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5664–5674 c November 7–11, 2021. 2021 Association for"
2021.emnlp-main.458,N16-1101,0,0.0280591,"M), which minimizes the average training loss on the training set, high-resource languages (HRLs) with large amounts of data contribute to the majority of the training objective. When model capacity is limited, this results in trade-offs or decreased performance on some languages, particularly LRLs (Arivazha1 Introduction gan et al., 2019; Wang et al., 2020b, 2021). To Multilingual methods that process multiple lan- better control this trade-off, a common practice is to balance the training distribution by heuristic guages with one single model have gained favor across a variety of NLP tasks (Firat et al., 2016; oversampling of LRLs (Johnson et al., 2017; NeuHa et al., 2016; Johnson et al., 2017; Devlin et al., big and Hu, 2018; Arivazhagan et al., 2019). Although simple data balancing can improve the 2019; Aharoni et al., 2019; Conneau et al., 2020) because (1) training and deployment of one mul- performance on LRLs significantly, it is far from optimal. First, the sampling hyperparameters need tilingual model is more computationally efficient compared to maintaining one model for each lan- to be adjusted for different datasets. Second, the guage (Arivazhagan et al., 2019), (2) training mul- use of"
2021.emnlp-main.458,D19-1432,0,0.199792,"nt set. However, due to the necessity to calculate and store extra gradients, their approach comes at an increased computational and memory cost. In contrast, χ-IBR enjoys the same computational complexity as ERM, and as we show in experiments it also largely outperforms MultiDDS. (the uncertainty set). Formally, we wish to minimize sup E(x,y)∼Q [`(x, y; θ)]. θ (1) Q∈Q Originating from operations research (Delage and Ye, 2010; Ben-Tal et al., 2013a,b; Bertsimas et al., 2018), DRO is a promising way to tackle robustness in a variety of machine learning and NLP problems (Hashimoto et al., 2018; Oren et al., 2019; Levy et al., 2020). We present here a recent variant, Group DRO, developed by Sagawa et al. (2020) which incorporates additional information about the data distribution to define more meaningful uncertainty sets. Abstractly, this method assumes a collection of distributions over subpopulations {Pg }g∈G such that the training distribution is a mixture of these subpopulations. Importantly, it assumes that this group structure is observed. The Group DRO objective then minimizes the worst-case loss over these P groups, which is|G|equivalent to (1) with Q = { g∈G qg Pg : q ∈ ∆ }, or equivalently,"
2021.emnlp-main.458,N19-4009,0,0.0242035,"s provided by the WMT shared task. Specifically, the training data of deu-eng, fra-eng is from WMT14, tam-eng is from WMT20 and tur-eng is from WMT18. We use the corresponding test and dev sets from each shared task for evaluation and validation. We evaluate both en-to-any (translate English to a target language) and any-to-en (translate a source language into English) directions for all language sets. We provide dataset statistics in Appendix C. For the translation models, we adopt the encoderdecoder Transformer (Vaswani et al., 2017) architecture with the implementation provided in fairseq (Ott et al., 2019). For both datasets, we use a Transformer-base architectures that also has 6 encoder and decoder layers with hidden dimension size being 512 and 8 attention heads.3 The model is trained for 200K and 300K steps for TED and WMT respectively with the batch size of 65,536 tokens. For both datasets, we learn the sentencepiece (Kudo and Richardson, 2018) vocabulary for the English and the combined corpus of other languages respectively. We use beam search with beam size 5 for decoding and report the SacreBLEU score (Post, 2018; Papineni et al., 2002) on test sets for evaluation. For the TED and WMT"
2021.emnlp-main.458,P02-1040,0,0.113884,"hitecture with the implementation provided in fairseq (Ott et al., 2019). For both datasets, we use a Transformer-base architectures that also has 6 encoder and decoder layers with hidden dimension size being 512 and 8 attention heads.3 The model is trained for 200K and 300K steps for TED and WMT respectively with the batch size of 65,536 tokens. For both datasets, we learn the sentencepiece (Kudo and Richardson, 2018) vocabulary for the English and the combined corpus of other languages respectively. We use beam search with beam size 5 for decoding and report the SacreBLEU score (Post, 2018; Papineni et al., 2002) on test sets for evaluation. For the TED and WMT datasets respectively, the constraint size ρ for the 2 See Wang et al. (2020a) for the interpretation of the language codes. 3 We train for more steps with a larger batch size, which yields much better results than reported in Wang et al. (2020a). 4.2 5669 Experimental setup Method FastDRO GDRO with PD CVaR-GDRO with PD CVaR-GDRO with IBR χ2 -GDRO with PD ERM (τ =5) χ-IBR deu fra any→en tam tur Avg deu fra en→any tam tur Avg 25.14 26.72 28.62 29.14 29.49 29.25 29.67 27.58 29.13 30.70 31.65 31.47 31.60 31.75 12.71 15.78 15.94 16.31 16.07 16.31 1"
2021.emnlp-main.458,D18-2012,0,0.0140966,"source language into English) directions for all language sets. We provide dataset statistics in Appendix C. For the translation models, we adopt the encoderdecoder Transformer (Vaswani et al., 2017) architecture with the implementation provided in fairseq (Ott et al., 2019). For both datasets, we use a Transformer-base architectures that also has 6 encoder and decoder layers with hidden dimension size being 512 and 8 attention heads.3 The model is trained for 200K and 300K steps for TED and WMT respectively with the batch size of 65,536 tokens. For both datasets, we learn the sentencepiece (Kudo and Richardson, 2018) vocabulary for the English and the combined corpus of other languages respectively. We use beam search with beam size 5 for decoding and report the SacreBLEU score (Post, 2018; Papineni et al., 2002) on test sets for evaluation. For the TED and WMT datasets respectively, the constraint size ρ for the 2 See Wang et al. (2020a) for the interpretation of the language codes. 3 We train for more steps with a larger batch size, which yields much better results than reported in Wang et al. (2020a). 4.2 5669 Experimental setup Method FastDRO GDRO with PD CVaR-GDRO with PD CVaR-GDRO with IBR χ2 -GDRO"
2021.emnlp-main.458,P19-1493,0,0.0227675,"y, it is far from optimal. First, the sampling hyperparameters need tilingual model is more computationally efficient compared to maintaining one model for each lan- to be adjusted for different datasets. Second, the guage (Arivazhagan et al., 2019), (2) training mul- use of simple heuristics does not consider the inherent level of difficulty in learning each language, the tilingually can improve accuracy, particularly for low-resource languages (LRLs) (Zoph et al., 2016; similarity between languages in the multilingual dataset, and other factors that affect cross-lingual Neubig and Hu, 2018; Pires et al., 2019). transfer. Because of this, previous work has indiHowever, in multilingual training, the amount cated the importance of learning strategies that are and type of training data available varies greatly explicitly tailored for each multilingual learning ∗ Equal contribution. scenario (Wang et al., 2020a). 1 Our code is available at https://github.com/ violet-zct/fairseq-dro-mnmt In this paper, we propose a new learning proce5664 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5664–5674 c November 7–11, 2021. 2021 Association for Computational Linguis"
2021.emnlp-main.458,W18-6319,0,0.0116936,"., 2017) architecture with the implementation provided in fairseq (Ott et al., 2019). For both datasets, we use a Transformer-base architectures that also has 6 encoder and decoder layers with hidden dimension size being 512 and 8 attention heads.3 The model is trained for 200K and 300K steps for TED and WMT respectively with the batch size of 65,536 tokens. For both datasets, we learn the sentencepiece (Kudo and Richardson, 2018) vocabulary for the English and the combined corpus of other languages respectively. We use beam search with beam size 5 for decoding and report the SacreBLEU score (Post, 2018; Papineni et al., 2002) on test sets for evaluation. For the TED and WMT datasets respectively, the constraint size ρ for the 2 See Wang et al. (2020a) for the interpretation of the language codes. 3 We train for more steps with a larger batch size, which yields much better results than reported in Wang et al. (2020a). 4.2 5669 Experimental setup Method FastDRO GDRO with PD CVaR-GDRO with PD CVaR-GDRO with IBR χ2 -GDRO with PD ERM (τ =5) χ-IBR deu fra any→en tam tur Avg deu fra en→any tam tur Avg 25.14 26.72 28.62 29.14 29.49 29.25 29.67 27.58 29.13 30.70 31.65 31.47 31.60 31.75 12.71 15.78 1"
2021.emnlp-main.458,N18-2084,1,0.849041,"tion cost is negligible compared to computing the gradient of the loss. We implement the θ-update of (6) by running a training epoch on a re-sampling of the training set D according to q t+1 . To compute the loss values {L(θt ; Di )}i≤N , necessary to perform the q-update, one needs to com- 4 Experiments pute the loss `(x, y; θt ) for every single example 4.1 Datasets (x, y) ∈ D. This is prohibitively expensive to compute at every epoch. To avoid this, we keep We evaluate the proposed method on two datasets: track of the (approximate) historical values of the the 58-languages TED talk corpus (Qi et al., 2018) 5668 Method aze 0.004 bel 0.006 glg 0.013 slk 0.081 tur 0.240 rus 0.274 por 0.243 ces 0.136 Avg any→en ERM (τ =1) MultiDDS χ-IBR 14.11 14.97 14.68 20.14 20.60 19.98 31.94 31.70 31.89 32.47 32.54 33.16 27.18 26.56 27.76 25.68 25.40 26.08 45.26 44.67 45.33 30.26 29.79 30.76 28.38 28.28 28.71 en→any ERM (τ =1) MultiDDS χ-IBR 7.61 8.04 8.49 12.68 15.04 13.84 24.79 26.60 25.77 25.24 25.19 26.09 16.79 16.32 16.78 20.80 20.44 21.59 40.84 40.47 41.38 22.93 22.93 23.70 21.46 21.88 22.21 Method bos 0.007 mar 0.017 hin 0.033 mkd 0.045 ell 0.237 bul 0.308 fra 0.340 kor 0.363 Avg any→en ERM (τ =1) MultiDD"
2021.emnlp-main.458,2020.acl-main.754,1,0.790416,"ges fr, zh and en is (0.1, 0.3, 0.6). Contours represent different radii of the χ2 -ball around ptrain . The blue points are the tempered distributions described in §2.1. across languages. Because most models are trained using empirical risk minimization (ERM), which minimizes the average training loss on the training set, high-resource languages (HRLs) with large amounts of data contribute to the majority of the training objective. When model capacity is limited, this results in trade-offs or decreased performance on some languages, particularly LRLs (Arivazha1 Introduction gan et al., 2019; Wang et al., 2020b, 2021). To Multilingual methods that process multiple lan- better control this trade-off, a common practice is to balance the training distribution by heuristic guages with one single model have gained favor across a variety of NLP tasks (Firat et al., 2016; oversampling of LRLs (Johnson et al., 2017; NeuHa et al., 2016; Johnson et al., 2017; Devlin et al., big and Hu, 2018; Arivazhagan et al., 2019). Although simple data balancing can improve the 2019; Aharoni et al., 2019; Conneau et al., 2020) because (1) training and deployment of one mul- performance on LRLs significantly, it is far fro"
2021.emnlp-main.458,D16-1163,0,0.0225494,"improve the 2019; Aharoni et al., 2019; Conneau et al., 2020) because (1) training and deployment of one mul- performance on LRLs significantly, it is far from optimal. First, the sampling hyperparameters need tilingual model is more computationally efficient compared to maintaining one model for each lan- to be adjusted for different datasets. Second, the guage (Arivazhagan et al., 2019), (2) training mul- use of simple heuristics does not consider the inherent level of difficulty in learning each language, the tilingually can improve accuracy, particularly for low-resource languages (LRLs) (Zoph et al., 2016; similarity between languages in the multilingual dataset, and other factors that affect cross-lingual Neubig and Hu, 2018; Pires et al., 2019). transfer. Because of this, previous work has indiHowever, in multilingual training, the amount cated the importance of learning strategies that are and type of training data available varies greatly explicitly tailored for each multilingual learning ∗ Equal contribution. scenario (Wang et al., 2020a). 1 Our code is available at https://github.com/ violet-zct/fairseq-dro-mnmt In this paper, we propose a new learning proce5664 Proceedings of the 2021 C"
2021.emnlp-main.553,D07-1007,0,0.118219,"r understand the lexical selection process, we train a prediction model which allows us to easily extract such descriptions for each lexical choice vy ∈ trans(vx , tx ). In this paper, we use humanreadable descriptions of the features learned by a linear model, where these features are defined over a set of lexical and semantic features extracted from the source sentences in Dhvx ,tx i . For designing features, we take inspiration from prior work which uses extracted contextual information to improve cross-lingual sense disambiguation in machine translation systems (Garcia-Varea et al., 2001; Carpuat and Wu, 2007b,a). 4.1 Model Features For training a lexical selection model θhvx ,tx i for the focus word hvx , tx i, we construct training data from the source-target sentence pairs Dhvx ,tx i . We focus on features extracted only from the current source sentence, although the framework can be easily extended to include features from the target sentence as well. We represent each source sentence xhvx ,tx i ∈ Dhvx ,tx i with a set of features extracted from the neighborhood of the focus word context relevant to the lexical selection process. This neighborhood includes (1) words from the source sentence th"
2021.emnlp-main.553,N19-1423,0,0.00667338,"trained using the same features 6914 Figure 2: Learning Interface. Rules for the correct answer are displayed to the learner after each question. Individual rules that apply to the given example are highlighted for the convenience of the learner. “wall” here refers to an outside wall and the adjective stone serves as a hint in arriving at the correct answer. as LinearSVM, to validate the choice of SVMs as an interpretable model over other alternatives. Further, we check how our interpretable linear SVM model compares with a “performance skyline”; a less interpretable BERT-based neural model (Devlin et al., 2019) that extracts representations of the source sentence from BERT and trains a classifier to predict the correct lexical choice. 5.1 Setup Data: We experiment with two L2 languages: Spanish and Greek. These languages were chosen due to (1) availability of parallel corpora with which to train models, and (2) availability of linguists and annotators to verify and analyze the data used in our experimental setting. For Spanish we use 10 million English-Spanish parallel sentences from OpenSubtitles (Lison and Tiedemann, 2016), Tatoeba, TED (Tiedemann, 2012), and Europarl (Koehn, 2005). 7 For Greek, w"
2021.emnlp-main.553,2021.eacl-main.181,1,0.689503,"5.1 Setup Data: We experiment with two L2 languages: Spanish and Greek. These languages were chosen due to (1) availability of parallel corpora with which to train models, and (2) availability of linguists and annotators to verify and analyze the data used in our experimental setting. For Spanish we use 10 million English-Spanish parallel sentences from OpenSubtitles (Lison and Tiedemann, 2016), Tatoeba, TED (Tiedemann, 2012), and Europarl (Koehn, 2005). 7 For Greek, we use 31 million English-Greek parallel sentences extracted from OpenSubtitles. For word alignment we use the AWESOME aligner (Dou and Neubig, 2021), for lemmatization we use spaCy (Honnibal et al., 2020), for POS tagging and dependency parsing we use Stanza (Qi et al., 2020), and for English WSD we use EWISER (Bevilacqua and Navigli, 2020).8 7 We use only 1 million sentences from Europarl because we found sentences from Europarl to contain fewer semantic subdivisions owing to the very specific domain of the dataset. 8 POS tagging, dependency parsing and WSD is required only for the source language, here English. Using our automatic pipeline (§3), we identify 157 English words which have fine-grained distinctions in Spanish and 707 Englis"
2021.emnlp-main.553,S13-2029,0,0.0517568,"aid the process of learning a new language. We thus plan to extract the rule set Rvx which governs this lexical selection process in a human- and machine-readable format. 3 Identifying Semantic Subdivisions In this section, we describe in detail the procedure for identifying L1 words that have different lexical manifestations in L2 owing to semantic subdivisions. For the purpose of this work, we refer to these different lexical manifestations in L2 as lexical choices and the corresponding L1 words as focus words. Our work is “loosely inspired” by ContraWSD (Rios et al., 2018) and SemEval2013 (Lefever and Hoste, 2013) which construct a dataset for cross-lingual word sense disambiguation, using a semi-automatic approach combining frequency-based heuristics with human supervision. These datasets are restricted to a subset of manually selected nouns (20 for SemEval-2013 and 70-80 for ContraWSD). In contrast, our approach is fully automated going beyond using just frequency-based filters. Furthermore, we do not restrict to any one word class leading to words being identified across different word classes (nouns, verbs, adjectives, adverbs) for both Spanish and Greek.4 We start with a parallel corpus D = {(x1 ,"
2021.emnlp-main.553,L16-1147,0,0.0107863,"pares with a “performance skyline”; a less interpretable BERT-based neural model (Devlin et al., 2019) that extracts representations of the source sentence from BERT and trains a classifier to predict the correct lexical choice. 5.1 Setup Data: We experiment with two L2 languages: Spanish and Greek. These languages were chosen due to (1) availability of parallel corpora with which to train models, and (2) availability of linguists and annotators to verify and analyze the data used in our experimental setting. For Spanish we use 10 million English-Spanish parallel sentences from OpenSubtitles (Lison and Tiedemann, 2016), Tatoeba, TED (Tiedemann, 2012), and Europarl (Koehn, 2005). 7 For Greek, we use 31 million English-Greek parallel sentences extracted from OpenSubtitles. For word alignment we use the AWESOME aligner (Dou and Neubig, 2021), for lemmatization we use spaCy (Honnibal et al., 2020), for POS tagging and dependency parsing we use Stanza (Qi et al., 2020), and for English WSD we use EWISER (Bevilacqua and Navigli, 2020).8 7 We use only 1 million sentences from Europarl because we found sentences from Europarl to contain fewer semantic subdivisions owing to the very specific domain of the dataset. 8"
2021.emnlp-main.553,P01-1027,0,0.243492,"learners to help them better understand the lexical selection process, we train a prediction model which allows us to easily extract such descriptions for each lexical choice vy ∈ trans(vx , tx ). In this paper, we use humanreadable descriptions of the features learned by a linear model, where these features are defined over a set of lexical and semantic features extracted from the source sentences in Dhvx ,tx i . For designing features, we take inspiration from prior work which uses extracted contextual information to improve cross-lingual sense disambiguation in machine translation systems (Garcia-Varea et al., 2001; Carpuat and Wu, 2007b,a). 4.1 Model Features For training a lexical selection model θhvx ,tx i for the focus word hvx , tx i, we construct training data from the source-target sentence pairs Dhvx ,tx i . We focus on features extracted only from the current source sentence, although the framework can be easily extended to include features from the target sentence as well. We represent each source sentence xhvx ,tx i ∈ Dhvx ,tx i with a set of features extracted from the neighborhood of the focus word context relevant to the lexical selection process. This neighborhood includes (1) words from"
2021.emnlp-main.553,W18-6437,0,0.0634725,"interpretable by humans in order to aid the process of learning a new language. We thus plan to extract the rule set Rvx which governs this lexical selection process in a human- and machine-readable format. 3 Identifying Semantic Subdivisions In this section, we describe in detail the procedure for identifying L1 words that have different lexical manifestations in L2 owing to semantic subdivisions. For the purpose of this work, we refer to these different lexical manifestations in L2 as lexical choices and the corresponding L1 words as focus words. Our work is “loosely inspired” by ContraWSD (Rios et al., 2018) and SemEval2013 (Lefever and Hoste, 2013) which construct a dataset for cross-lingual word sense disambiguation, using a semi-automatic approach combining frequency-based heuristics with human supervision. These datasets are restricted to a subset of manually selected nouns (20 for SemEval-2013 and 70-80 for ContraWSD). In contrast, our approach is fully automated going beyond using just frequency-based filters. Furthermore, we do not restrict to any one word class leading to words being identified across different word classes (nouns, verbs, adjectives, adverbs) for both Spanish and Greek.4"
2021.emnlp-main.553,W17-4702,0,0.0199844,"hose formalism can account for only fixed-length ordered contexts restricting their application. Further, these rules use a combination of only lemma and POS tags while our framework uses more features. Cross-lingual word sense disambiguation CLWSD disambiguates a word in-context by providing appropriate translation across languages. Lefever and Hoste (2010) construct a dataset (25 ambiguous English nouns across six languages) semi-automatically from parallel corpora which are then verified by expert translators. Such lexical choice tasks have been created also for evaluating MT systems (Rios Gonzales et al., 2017; Rios et al., 2018). However, these methods cover a limited set of words (mostly nouns) and require some manual intervention during the data creation process. To the best of our knowledge, our proposed pipeline is 14 This is based on explanations collected from native Spanish speakers, which can also be found in Appendix B.4 the only fully automated one that extracts several ambiguous words across multiple POS tags. 8 Future Work While we have demonstrated the efficacy of our extracted rules in teaching new words for two languages, we plan to apply our framework on much less-resourced languag"
2021.emnlp-main.570,W14-3302,0,0.0824889,"Missing"
2021.emnlp-main.570,W18-6111,0,0.0202721,"ne axis in each plot corresponds to the performance on the clean evaluation set. Our models are more robust on both parsing (LAS) and morphological feature prediction. We report results both over the whole treebank and over only the erroneous tokens. task involves identifying and correcting errors relating to spelling, morphosyntax and word choice. For evaluating L’ AMBRE, we only focus on grammar error identification (GEI) and specifically on identification of morphosyntactic errors. We experiment with two morphologically rich languages, Russian and German. We use the FalkoMERLIN GEC corpus (Boyd, 2018) for German and the RULEC-GEC dataset (Rozovskaya and Roth, 2019) for Russian. We focus on error types related to morphology (see A.3). Evaluation: To evaluate the effectiveness of L’ AMBRE , we run it on the training15 splits of the German and Russian GEC datasets. GEC corpora typically annotate single words or phrases as errors (and provide a correction); in contrast, we only identify errors over a dependency link, which can then be mapped over to either the dependent or head token. This difference is not trivial: a subjectverb agreement error, for instance, could be fixed by modifying eithe"
2021.emnlp-main.570,W18-6433,0,0.154861,"s like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating text (e.g., an incorrect word early in the sentence ma"
2021.emnlp-main.570,W17-4705,0,0.101209,"mple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating text (e.g., an incorrect word ea"
2021.emnlp-main.570,D13-1174,0,0.0180435,"ality of L’ AMBRE. In future work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explicitly measures well-forme"
2021.emnlp-main.570,2020.emnlp-main.422,1,0.904164,"ic overview is outlined in Figure 1). Our measure can be used directly on text generated from a black-box NLG system, and allows for decomposing the system performance into individual grammar rules that identify specific areas to improve the model’s grammaticality. L’ AMBRE relies on a grammatical description of the language, similar to those linguists and language educators have been producing for decades when they document a language or create teaching materials. Specifically, we consider rules describing morphosyntax, including agreement, case assignment, and verb form selection. Following Chaudhary et al. (2020), we describe a procedure to automatically extract these rules from existing dependency treebanks (§3) with high precision.3 When evaluating NLG outputs, adherence to these rules can be assessed through dependency parses (Figure 1). However, off-the-shelf dependency parsers are trained on grammatically sound text and are not well-suited for parsing ungrammatical (or noisy) text (Hashemi and Hwa, 2016) such as that generated by NLG systems. We propose a method to train more robust dependency parsers and morphological feature taggers by synthesizing morphosyntactic errors in existing treebanks ("
2021.emnlp-main.570,L16-1102,0,0.0697468,"Missing"
2021.emnlp-main.570,2020.acl-demos.10,0,0.0306615,"n dialogue (Specia et al., 2010; Dušek et al., 2017). With the exception of the grammaticality-based metric of Napoles et al. (GBM; 2016), these metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific do"
2021.emnlp-main.570,W19-7814,0,0.0276629,"gen- rules as ras (PRON, VERB, subj) → CasePRON = Nom and ras (PRON, VERB, obj) → CasePRON = Acc. 7 Many linguists also produce highly formal accounts of Our hypothesis is that certain syntactic construcgrammatical phenomena. However, many of these formalisms are difficult to implement computationally because they are tions require specific morphological feature selecequivalent (in the most egregious cases) to Turing machines. tion from one of their constituents (e.g., pronoun 8 We use the Surface-Syntactic Universal Dependencies subjects need to be in nominative case, but pronoun (SUD) 2.5 (Gerdes et al., 2019). See A.1 for a comparison of UD and SUD. objects only allow for genitive or accusative case in 7133 Greek).9 This implies that the “local” distribution that a specific construction requires will be different from a “global” distribution of morphological feature values computed over the whole treebank. Figure 2 presents an example for German-GSD. We can automatically discover these rules by finding such cases of distortion. First, we obtain a global distribution (G(fx ) = p(fx )) that captures the empirical distribution of the values of a morphological feature f on POS x over the whole treeban"
2021.emnlp-main.570,H05-1085,0,0.160341,"e Russian and German GEC corpora for evaluating the quality of L’ AMBRE. In future work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In c"
2021.emnlp-main.570,D16-1182,0,0.0236581,"arguably would be even more effective if combined with hand-curated descriptions created by linguists. We leave this as an interesting direction for future work. In our code, we provide detailed instructions for adding new rules. 4 Parsing Noisy Text Within our evaluation framework, we rely on parsers to generate the dependency trees of potentially malformed or noisy sentences from NLG systems. However, publicly available parsers are typically trained on clean and grammatical text from UD treebanks, and may not generalize to noisy inputs (Daiber and van der Goot, 2016; Sakaguchi et al., 2017; Hashemi and Hwa, 2016, 2018). Therefore, it is necessary to ensure that parsers are robust to any morphology-related errors in the input 9 This class of rules are also often lexicalized, depending on text. Ideally, the tagger should accurately identify the lexeme of either the head or the dependent. In the example the morphological features of incorrect word forms, S.1 of Figure 1, the object phrase lange Bücher (‘long Book’) while the dependency parser remains robust to such is inflected in the accusative case because of the verb lesen (‘read’). Other constructions might require the object declined noise. To this"
2021.emnlp-main.570,D19-1279,0,0.0232009,"n UniMorph. 4.2 Training Robust Parsers To adapt to the noisy input conditions in practical NLP settings like ours, our proposed solution is to re-train the parsers/taggers directly on noisy UD treebanks. With the procedure described above (§4.1) we also add noise to the train splits of the UD v2.5 treebanks and re-train the lemmatizer, tagger, and dependency parser from scratch.14 To retain the performance on clean inputs, we concatenate the original clean train splits with our noisy ones. We experimented with commonly used multilingual parsers like UDPipe (Straka and Straková, 2017), UDify (Kondratyuk and Straka, 2019), and Stanza (Qi et al., 2020), settling on Stanza for its superior performance in preliminary experiments. We use the standard training procedure that yields stateof-the-art results on most UD languages with the default hyperparameters for each treebank. Given that we are inherently tokenizing the text to add morphology-related noise, we reuse the pre-trained tokenizers instead of retraining them on noisy data. Figure 4 compares the performance of the original and our robust parsers on three treebanks. Overall, we notice significant improvements on both LAS (with similar gains on UAS) and UFe"
2021.emnlp-main.570,2020.acl-main.126,0,0.0166514,"gically-rich languages. 1 1 Introduction A variety of natural language processing (NLP) applications such as machine translation (MT), summarization, and dialogue require natural language generation (NLG). Each of these applications has a different objective and therefore task-specific evaluation metrics are commonly used. For instance, reference-based measures such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) are used to evaluate MT, ROUGE (Lin, 2004) is a metric widely used in summarization, and various task-based metrics are used in dialogue (Liang et al., 2020). Regardless of the downstream application, an important aspect of evaluating language generation systems is measuring the fluency of the generated text. In this paper, we propose a metric that can be used to evaluate the grammatical well-formedness of text produced by NLG systems.2 Our metric number, case, gender agreement number, person agreement case assignment root case assignment comp:aux subj S.1 mod comp:obj PRON AUX ADJ NOUN VERB Ich werde lange Bücher lesen I-NOM .1 SG will-1 SG long-ACC . PL Book-ACC . PL read-PTCP S.2 *Ich werden langen Bücher lesen I-NOM .1 SG will-1 PL long-DAT ."
2021.emnlp-main.570,W04-1013,0,0.0247647,"metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages. 1 1 Introduction A variety of natural language processing (NLP) applications such as machine translation (MT), summarization, and dialogue require natural language generation (NLG). Each of these applications has a different objective and therefore task-specific evaluation metrics are commonly used. For instance, reference-based measures such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) are used to evaluate MT, ROUGE (Lin, 2004) is a metric widely used in summarization, and various task-based metrics are used in dialogue (Liang et al., 2020). Regardless of the downstream application, an important aspect of evaluating language generation systems is measuring the fluency of the generated text. In this paper, we propose a metric that can be used to evaluate the grammatical well-formedness of text produced by NLG systems.2 Our metric number, case, gender agreement number, person agreement case assignment root case assignment comp:aux subj S.1 mod comp:obj PRON AUX ADJ NOUN VERB Ich werde lange Bücher lesen I-NOM .1 SG wi"
2021.emnlp-main.570,2020.acl-main.490,0,0.0197465,"s 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating text (e.g., an incorrect word early in the sentence may trigger a grammatical error later in the sentence). Most of these methods, with the exception of Mueller et al. (2020), focus only on English or translation to/from English. In this paper, we propose L’ AMBRE, a metric that both evaluates the grammatical well-formedness of text in a fine-grained fashion and can be applied to text from multiple languages. We use widely available dependency parsers to tag and parse target text, and then compute our metric by identifying language-specific morphosyntactic errors in text (a schematic overview is outlined in Figure 1). Our measure can be used directly on text generated from a black-box NLG system, and allows for decomposing the system performance into individual gr"
2021.emnlp-main.570,W18-6450,0,0.0246572,"idelyt-BLEU (Ataman et al., 2020) measures BLEU on outputs used rule-based proofreading software to detect tagged using a morphological analyzer. 7137 en→ cs 1 de et fi ru tr WMT’18 0.84 all ragree 0.91 ras 0.78 -0.06 0.07 -0.10 0.68 0.83 0.62 0.86 0.96 0.77 0.86 0.71 0.89 0.58 0.64 -0.31 WMT’19 all 0.80 0.89 ragree ras 0.70 0.16 0.14 0.13 - 0.85 0.87 0.82 0.57 0.70 0.45 - 0.95 0.95 0.95 0.93 0.93 0.95 0.95 0.95 0.9 ’15 ’16 ’17 ’18 ’19 ’14 ’15 ’16 ’17 ’18 ’19 : systems average ◦: system •: reference For evaluating MT systems, we use the data from the Metrics Shared Task in WMT 2018 and 2019 (Ma et al., 2018, 2019). This corpus includes outputs from all participating systems on the test sets from the News Translation Shared Task (Bojar et al., 2018; Barrault et al., 2019). Our study focuses on systems that translate from English to morphologically-rich target languages: Czech, Estonian, Finnish, German, Russian, and Turkish. We used all relevant languages from the WMT shared task except for Lithuanian and Kazakh, which lack reasonable quality parsers. Correlation Analysis The MT system outputs are accompanied with human judgment scores, both at the segment and system level. In contrast to the ref"
2021.emnlp-main.570,W19-5302,0,0.0157703,"Analysis The MT system outputs are accompanied with human judgment scores, both at the segment and system level. In contrast to the reference-free nature of human judgments, our scorer is both reference-free and source-free. Following the standard WMT procedure for evaluating MT metrics, we measure the Pearson’s r correlations between L’ AMBRE and human z-scores for systems from WMT18 and WMT19. We follow Mathur et al. (2020) to remove outlier systems, since they tend to significantly boost the correlation scores, making the correlations unreliable, especially for the best performing systems (Ma et al., 2019). Table 3 presents the correlation results for WMT18 and WMT19.19 We generally observe moderate to high correlation with human judgments using both sets of rules across all languages, apart from German (WMT18,19). This confirms that grammatically sound output is an important factor in human evaluation of NLG outputs. The correlation is lower with case assignment and verb form choice rules, with notable negative correlations for German, and See A.5 for the corresponding scatter plots. German 0.95 0.94 0.95 0.94 0.9 ’14 Table 3: With a few exceptions, our grammar-based metrics correlate well wit"
2021.emnlp-main.570,D18-1151,0,0.025453,"ut, limiting their applicability to specific tasks like MT or spoken dialogue (Specia et al., 2010; Dušek et al., 2017). With the exception of the grammaticality-based metric of Napoles et al. (GBM; 2016), these metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the ot"
2021.emnlp-main.570,2020.acl-main.448,0,0.0251423,"Estonian, Finnish, German, Russian, and Turkish. We used all relevant languages from the WMT shared task except for Lithuanian and Kazakh, which lack reasonable quality parsers. Correlation Analysis The MT system outputs are accompanied with human judgment scores, both at the segment and system level. In contrast to the reference-free nature of human judgments, our scorer is both reference-free and source-free. Following the standard WMT procedure for evaluating MT metrics, we measure the Pearson’s r correlations between L’ AMBRE and human z-scores for systems from WMT18 and WMT19. We follow Mathur et al. (2020) to remove outlier systems, since they tend to significantly boost the correlation scores, making the correlations unreliable, especially for the best performing systems (Ma et al., 2019). Table 3 presents the correlation results for WMT18 and WMT19.19 We generally observe moderate to high correlation with human judgments using both sets of rules across all languages, apart from German (WMT18,19). This confirms that grammatically sound output is an important factor in human evaluation of NLG outputs. The correlation is lower with case assignment and verb form choice rules, with notable negativ"
2021.emnlp-main.570,D19-5545,0,0.0482666,"Missing"
2021.emnlp-main.570,D16-1228,0,0.158847,"gnment and verb form choice, ras ). Analysis: In both languages, we find agreement rules to be of higher quality than case and verb form assignment ones. This phenomenon is more pronounced in German where many case assignment rules are lexeme-dependent, as discussed in §3. Importantly, our proposed robust parsers lead to clear gains in error identification recall, compared to the pre-trained ones (“Original” vs. “Robust” in Table 2). Given the complexity of the errors present in text from non-native learners and the well-known incompleteness of GEC corpora in listing all possible corrections (Napoles et al., 2016), combined with the prevalence of typos and the dataset’s domain difference compared to the 15 We use the train portion due to its large size, therefore parser’s training data, our error identification modgives a better estimate of our L’ AMBRE performance. Note ule performs quite well. that, in this experiment, we do not aim to compare against state-of-the-art GEI tools. To understand where L’ AMBRE fails, we man7136 ually inspected a sample of false positives. First, we notice that tokens with typos are often erroneously tagged and parsed. Our augmentation is only equipped to handle (correct"
2021.emnlp-main.570,2020.acl-demos.14,0,0.0602168,"Missing"
2021.emnlp-main.570,Q19-1001,0,0.0252876,"e on the clean evaluation set. Our models are more robust on both parsing (LAS) and morphological feature prediction. We report results both over the whole treebank and over only the erroneous tokens. task involves identifying and correcting errors relating to spelling, morphosyntax and word choice. For evaluating L’ AMBRE, we only focus on grammar error identification (GEI) and specifically on identification of morphosyntactic errors. We experiment with two morphologically rich languages, Russian and German. We use the FalkoMERLIN GEC corpus (Boyd, 2018) for German and the RULEC-GEC dataset (Rozovskaya and Roth, 2019) for Russian. We focus on error types related to morphology (see A.3). Evaluation: To evaluate the effectiveness of L’ AMBRE , we run it on the training15 splits of the German and Russian GEC datasets. GEC corpora typically annotate single words or phrases as errors (and provide a correction); in contrast, we only identify errors over a dependency link, which can then be mapped over to either the dependent or head token. This difference is not trivial: a subjectverb agreement error, for instance, could be fixed by modifying either the subject or the verb to agree with the other constituent. To"
2021.emnlp-main.570,Q16-1013,0,0.0163878,"ese metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating tex"
2021.emnlp-main.570,P17-2030,0,0.0584137,"Missing"
2021.emnlp-main.570,2020.acl-main.263,0,0.020855,"os and Neubig (2019), but we leave this for future work. For evaluation, we induce noise into the dev portions of the treebanks and test the robustness of off-the-shelf taggers and parsers from Stanza (Qi et al., 2020) (indicative results on Czech, Greek, and Turkish are shown in Figure 4). Along with the overall scores on the dev set, we also report the results only on the altered word forms (“Altered Forms”). Across the three languages, we notice 12 For each token, we first map the morphological feature annotations in the original UD schema to the UniMorph schema (McCarthy et al., 2018). 13 Tan et al. (2020) follows similar methodology using English-only LemmInflect tool, but our approach is scalable to the large number of languages in UniMorph. 4.2 Training Robust Parsers To adapt to the noisy input conditions in practical NLP settings like ours, our proposed solution is to re-train the parsers/taggers directly on noisy UD treebanks. With the procedure described above (§4.1) we also add noise to the train splits of the UD v2.5 treebanks and re-train the lemmatizer, tagger, and dependency parser from scratch.14 To retain the performance on clean inputs, we concatenate the original clean train spl"
2021.emnlp-main.570,P08-1059,0,0.0600684,"ra for evaluating the quality of L’ AMBRE. In future work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explic"
2021.emnlp-main.570,2020.acl-main.660,0,0.0163098,"igure 5: A diachronic study of grammatical wellformedness of WMT English→X systems’ outputs. The systems in general are becoming more fluent. In the last two years the best systems produce as wellformed outputs as the reference translations. Turkish (WMT18). In the case of German, a significant number of case assignment rules are dependent on the lexeme (as noted in §3) and we expect future work on lexicalized rules to partially address this drawback. In Turkish, the low parser quality plays a significant role and highlights the need for further work on parsing morphologically-rich languages (Tsarfaty et al., 2020). Last, we note that human judgments, unlike L’ AMBRE, incorporate both well-formedness and adequacy (with respect to the source). Therefore, we recommend using L’ AMBRE in tandem with standard MT metrics to obtain a good indication of overall performance, both during model training and evaluation. We additionally perform a correlation analysis of L’ AMBRE with perplexity, BLEU and chrF on the WMT system outputs (A.5 in Appendix). As expected, we see a strong negative correlation with perplexity (low perplexity and high L’ AMBRE). For BLEU and chrF, the results are quite similar to the correla"
2021.emnlp-main.570,Q19-1040,0,0.0140367,"ability to specific tasks like MT or spoken dialogue (Specia et al., 2010; Dušek et al., 2017). With the exception of the grammaticality-based metric of Napoles et al. (GBM; 2016), these metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniqu"
2021.emnlp-main.570,E17-2060,0,0.0233458,"rammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explicitly measures well-formedness, without requiring access to trained MT models. Comparison with Other Metrics: We also compare L’ AMBRE to other metrics that capture fluency and/or grammatical well-formedness, namely perplexity as computed by large language mode"
2021.emnlp-main.570,P16-1162,0,0.00537509,"ture work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explicitly measures well-formedness, without requiring"
2021.emnlp-main.570,K17-3009,0,0.0289797,"to the large number of languages in UniMorph. 4.2 Training Robust Parsers To adapt to the noisy input conditions in practical NLP settings like ours, our proposed solution is to re-train the parsers/taggers directly on noisy UD treebanks. With the procedure described above (§4.1) we also add noise to the train splits of the UD v2.5 treebanks and re-train the lemmatizer, tagger, and dependency parser from scratch.14 To retain the performance on clean inputs, we concatenate the original clean train splits with our noisy ones. We experimented with commonly used multilingual parsers like UDPipe (Straka and Straková, 2017), UDify (Kondratyuk and Straka, 2019), and Stanza (Qi et al., 2020), settling on Stanza for its superior performance in preliminary experiments. We use the standard training procedure that yields stateof-the-art results on most UD languages with the default hyperparameters for each treebank. Given that we are inherently tokenizing the text to add morphology-related noise, we reuse the pre-trained tokenizers instead of retraining them on noisy data. Figure 4 compares the performance of the original and our robust parsers on three treebanks. Overall, we notice significant improvements on both LA"
2021.emnlp-main.802,2020.findings-emnlp.195,0,0.0856569,"Missing"
2021.emnlp-main.802,2020.emnlp-main.630,1,0.933863,"l., 2020), i.e. they We retain the XNLI (Conneau et al., 2018), UDplace the semantically most related text pairs (e.g. a POS (Nivre et al., 2018), WikiANN-NER (Pan question and its answer) closest together in repreet al., 2017), XQuAD (Artetxe et al., 2020a), sentation space, regardless of their language idenMLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark tities. The second analogously frames entity linket al., 2020), and Tatoeba (Artetxe and Schwenk, ing as retrieving from a multilingual pool of entity 2019) tasks from XTREME (see Appendix C). descriptions, given an entity mention in context (Botha et al., 2020). For both, we report perfor3.2 New Tasks mance as mean average precision at 20 (mAP@20). 3.2.1 Multilingual Causal Reasoning LAReQA Language Agnostic Retrieval Question XCOPA The Cross-lingual Choice of Plausible Answering (Roy et al., 2020) is a sentence retrieval Alternatives (Ponti et al., 2020) dataset asks models task. Each query has target answers in multiple lanto decide which of two sentences causally follows guages, and models are expected to rank all correct 10218 Table 3: C HECK L IST templates and generated tests for different capabilities in English, Hebrew, Arabic, and Bengali."
2021.emnlp-main.802,2021.naacl-main.280,0,0.0801691,"Missing"
2021.emnlp-main.802,2020.tacl-1.30,1,0.79452,"i.e. RemBERT for retrieval and mT5 for retrieval and tagging. but remain well below performance on English. On POS tagging (Figure 1d), scores remain largely the same; performance is lower for some languages with non-Latin scripts and low-resource languages. We show the scores for the remaining tasks in Appendix B. The remaining gap to English performance on these tasks is partially an artefact of the evaluation setup: zero-shot cross-lingual transfer from English favors English representations whereas models fine-tuned on in-language monolingual data perform more similarly across languages (Clark et al., 2020; Hu et al., 2020). ilar to the downstream setting but does not significantly improve performance on other tasks. Finetuning on automatically translated task-specific data yields strong gains and is used by most recent models to achieve the best performance (Hu et al., 2020; Ouyang et al., 2020; Luo et al., 2020). Nevertheless, key challenges such as how to learn robust cross-lingual syntactic and semantic processing capabilities during pre-training remain. 3 XTREME-R In order to encourage the NLP community to tackle challenging research directions in pursuit of betOverall, representations fro"
2021.emnlp-main.802,2020.acl-main.747,0,0.251756,"aset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models. XTREME XTREME - R Analysis tools 40 9 Classification, structured prediction, QA, retrieval — Leaderboard Static 50 − 2 + 3 = 10 +language-agnostic retrieval M ULTI C HECK L IST, Explainaboard Interactive, +metadata # of languages # of tasks Task categories Table 1: Overview of XTREME and XTREME - R. have been introduced, consolidating existing multilingual tasks and covering tens of languages. When XTREME was released, the gap between the best-performing baseline, XLM-R Large (Conneau et al., 2020), and human-level performance was roughly 25. This has since shrunk to less than 12 points, a much smaller but still substantial gap compared to the difference from human-level performance observed in English transfer learning (Wang et al., 2019a), which has recently been closed entirely on some evaluation suites (He et al., 2021). In order to examine the nature of this progress, we first perform an analysis of state-of-the-art mul1 Introduction tilingual models on XTREME. We observe that progress has not been uniform, but concentrated on Most research in natural language processing cross-ling"
2021.emnlp-main.802,D18-1269,0,0.022892,"ll be used to search over only English candidates. However, practical settings often violate this assumption, e.g. the answer to a question may be available in any number of languages, possibly different from the query language. Models that cannot compare the appropriateness of retrieval results across languages are thus ineffective in such real-world scenarios. XTREME - R includes two new related crosslingual retrieval tasks. The first seeks to measure the extent to which cross-lingual representations 3.1 Retained Tasks are “strongly aligned” (Roy et al., 2020), i.e. they We retain the XNLI (Conneau et al., 2018), UDplace the semantically most related text pairs (e.g. a POS (Nivre et al., 2018), WikiANN-NER (Pan question and its answer) closest together in repreet al., 2017), XQuAD (Artetxe et al., 2020a), sentation space, regardless of their language idenMLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark tities. The second analogously frames entity linket al., 2020), and Tatoeba (Artetxe and Schwenk, ing as retrieving from a multilingual pool of entity 2019) tasks from XTREME (see Appendix C). descriptions, given an entity mention in context (Botha et al., 2020). For both, we report perfor3.2 New Tasks m"
2021.emnlp-main.802,N19-1423,0,0.0292115,"rk (Hu et al., 2020; Lauscher et al., 2020; Hedderich et al., 2020) demonstrates the benefits of fine-tuning on in-language data, we believe the zero-shot scenario remains the most effective way to evaluate the amount of a priori multilingual knowledge a pre-trained model captures. Due to variation in cross-lingual evaluation (Keung et al., 2020), we recommend researchers to use the validation set of a single target language for development (Artetxe et al., 2020b). 4.1 Baselines We employ established pre-trained multilingual and models using translations as baselines. mBERT Multilingual BERT (Devlin et al., 2019) has been pretrained on the Wikipedias of 104 languages using MLM. XLM-R XLM-R Large (Conneau et al., 2020) uses the same MLM objective with a larger model, and was trained on a magnitude more web data from 100 languages. mT5 Multilingual T5 (Xue et al., 2021) is an encoder-decoder transformer that frames NLP tasks in a “text-to-text” format. It was pre-trained with MLM on a large multilingual web corpus covering 101 languages. We employ the largest mT5-XXL variant with 13B parameters. Translate-train To evaluate the impact of MT, we fine-tune mBERT on translations of English training data fro"
2021.emnlp-main.802,2020.emnlp-main.489,1,0.872791,"zing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on language and task-specific attributes, which enables a more nuanced diagnosis of a model’s behaviour. We also make several logistic improvements to improve XTREME - R’s utility as a leaderboard. To make it easier to choose the best model for a use case, each submission is required to provide metadata such as the number of parameters and amount of pre-training data, which we make available via an interactive leaderboard. We also introduce task and language-specific s"
2021.emnlp-main.802,N18-1108,0,0.0260624,"tr, uk, ur, vi, wo, yo, zh.2 XTREME - R is similarly typologically and genealogically diverse as XTREME while covering a larger number of languages (see Appendix D). 3.4 Diagnostic and evaluation suite To increase the language coverage of low-resource languages in XTREME - R and to enable us to systematically evaluate a model’s cross-lingual generalization ability, we augment XTREME - R with a massively multilingual diagnostic and evaluation suite. Challenge sets and diagnostic suites in NLP (Wang et al., 2019a,b; Belinkov and Glass, 2019) are mostly limited to English, with a few exceptions (Gulordava et al., 2018). As challenge sets are generally created with a human in the loop, the main challenge for creating a large multilingual diagnostic suite is to scale the annotation or translation effort to many languages and to deal with each language’s idiosyncrasies. M ULTI C HECK L IST To address this, we build on the C HECK L IST (Ribeiro et al., 2020) framework, which facilitates creating parameterized tests for models. C HECK L IST enables the creation of test cases using templates, which test for specific behavioral capabilities of a model with regard to a downstream task. Importantly, by relying on te"
2021.emnlp-main.802,2020.emnlp-main.204,0,0.0263038,"erent attribute values. We define new taskspecific attributes for the four task types as well as task-independent attributes (see Appendix K). Metadata We additionally would like to enable practitioners to rank submissions based on other information. To this end, we ask each submission to XTREME - R for relevant metadata such as the number of parameters, the amount of pre-training data, etc. We will show this information in an interactive leaderboard (see Appendix H for the metadata of current XTREME submissions). 4 Experiments glish. While recent work (Hu et al., 2020; Lauscher et al., 2020; Hedderich et al., 2020) demonstrates the benefits of fine-tuning on in-language data, we believe the zero-shot scenario remains the most effective way to evaluate the amount of a priori multilingual knowledge a pre-trained model captures. Due to variation in cross-lingual evaluation (Keung et al., 2020), we recommend researchers to use the validation set of a single target language for development (Artetxe et al., 2020b). 4.1 Baselines We employ established pre-trained multilingual and models using translations as baselines. mBERT Multilingual BERT (Devlin et al., 2019) has been pretrained on the Wikipedias of 104 l"
2021.emnlp-main.802,2020.emnlp-main.479,1,0.848683,"Missing"
2021.emnlp-main.802,2020.acl-main.560,0,0.0412131,"been closed entirely on some evaluation suites (He et al., 2021). In order to examine the nature of this progress, we first perform an analysis of state-of-the-art mul1 Introduction tilingual models on XTREME. We observe that progress has not been uniform, but concentrated on Most research in natural language processing cross-lingual retrieval tasks where fine-tuning on (NLP) to date has focused on developing methods other tasks and pre-training with parallel data lead that work well for English and a small set of other to large gains. On other task categories improvehigh-resource languages (Joshi et al., 2020). In ments are more modest. Models still generally contrast, methods for other languages can be vastly perform poorly on languages with limited data and more beneficial as they enable access to language non-Latin scripts. Fine-tuning on additional transtechnology for more than three billion speakers of low-resource languages and prevent the NLP com- lated data generally leads to the best performance. munity from overfitting to English. Motivated by Based on this analysis, we propose XTREME - R these benefits, the area of multilingual NLP has (XTREME Revisited), a new benchmark with the attract"
2021.emnlp-main.802,2020.findings-emnlp.445,0,0.0706199,"Missing"
2021.emnlp-main.802,2020.emnlp-main.40,0,0.031752,"n to XTREME - R for relevant metadata such as the number of parameters, the amount of pre-training data, etc. We will show this information in an interactive leaderboard (see Appendix H for the metadata of current XTREME submissions). 4 Experiments glish. While recent work (Hu et al., 2020; Lauscher et al., 2020; Hedderich et al., 2020) demonstrates the benefits of fine-tuning on in-language data, we believe the zero-shot scenario remains the most effective way to evaluate the amount of a priori multilingual knowledge a pre-trained model captures. Due to variation in cross-lingual evaluation (Keung et al., 2020), we recommend researchers to use the validation set of a single target language for development (Artetxe et al., 2020b). 4.1 Baselines We employ established pre-trained multilingual and models using translations as baselines. mBERT Multilingual BERT (Devlin et al., 2019) has been pretrained on the Wikipedias of 104 languages using MLM. XLM-R XLM-R Large (Conneau et al., 2020) uses the same MLM objective with a larger model, and was trained on a magnitude more web data from 100 languages. mT5 Multilingual T5 (Xue et al., 2021) is an encoder-decoder transformer that frames NLP tasks in a “text-"
2021.emnlp-main.802,2020.acl-main.329,0,0.0428374,"Missing"
2021.emnlp-main.802,2020.emnlp-main.363,0,0.0253632,"Missing"
2021.emnlp-main.802,2020.acl-main.653,0,0.0342518,"of retrieval results across languages are thus ineffective in such real-world scenarios. XTREME - R includes two new related crosslingual retrieval tasks. The first seeks to measure the extent to which cross-lingual representations 3.1 Retained Tasks are “strongly aligned” (Roy et al., 2020), i.e. they We retain the XNLI (Conneau et al., 2018), UDplace the semantically most related text pairs (e.g. a POS (Nivre et al., 2018), WikiANN-NER (Pan question and its answer) closest together in repreet al., 2017), XQuAD (Artetxe et al., 2020a), sentation space, regardless of their language idenMLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark tities. The second analogously frames entity linket al., 2020), and Tatoeba (Artetxe and Schwenk, ing as retrieving from a multilingual pool of entity 2019) tasks from XTREME (see Appendix C). descriptions, given an entity mention in context (Botha et al., 2020). For both, we report perfor3.2 New Tasks mance as mean average precision at 20 (mAP@20). 3.2.1 Multilingual Causal Reasoning LAReQA Language Agnostic Retrieval Question XCOPA The Cross-lingual Choice of Plausible Answering (Roy et al., 2020) is a sentence retrieval Alternatives (Ponti et al., 2020) dataset asks mo"
2021.emnlp-main.802,2020.acl-main.465,0,0.0160657,"s by covering 50 typologically diverse languages and 10 challenging, diverse tasks. To make retrieval more difficult, we introduce two new tasks that focus on “language-agnostic” retrieval (Roy et al., 2020), where targets must be retrieved from a large multilingual candidate pool. We additionally establish new state-of-the-art mT5 (Xue et al., 2021) and translate-train baselines for our tasks. XTREME - R aims to move away from a single aggregate metric summarizing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on lan"
2021.emnlp-main.802,E17-2002,0,0.063781,"Missing"
2021.emnlp-main.802,2021.acl-demo.34,1,0.889858,"rformance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on language and task-specific attributes, which enables a more nuanced diagnosis of a model’s behaviour. We also make several logistic improvements to improve XTREME - R’s utility as a leaderboard. To make it easier to choose the best model for a use case, each submission is required to provide metadata such as the number of parameters and amount of pre-training data, which we make available via an interactive leaderboard. We also introduce task and language-specific sub-leaderboards to"
2021.emnlp-main.802,2021.ccl-1.108,0,0.0472179,"Missing"
2021.emnlp-main.802,2021.emnlp-main.3,0,0.0855395,"Missing"
2021.emnlp-main.802,P17-1178,0,0.0484182,"Missing"
2021.emnlp-main.802,P02-1040,0,0.111792,"Missing"
2021.emnlp-main.802,2020.emnlp-main.617,1,0.881246,"Missing"
2021.emnlp-main.802,2020.aacl-main.56,0,0.0322166,"tic processing capabilities during pre-training remain. 3 XTREME-R In order to encourage the NLP community to tackle challenging research directions in pursuit of betOverall, representations from token-level MLM pre-training are of limited use for cross-lingual sen- ter cross-lingual model generalization, we propose XTREME - R (XTREME Revisited). XTREME - R tence retrieval, as evidenced by the comparatively poor performance of the mBERT and XLM-R mod- shares its predecessor’s core design principles for creating an accessible benchmark to evaluate crossels. Fine-tuning on sentence-level tasks (Phang et al., 2020; Fang et al., 2021) can mitigate this. lingual transfer but makes some key changes. The strong performance of recent models such as First, XTREME - R focuses on the tasks that have VECO and ERNIE-M on the retrieval tasks can proven to be hardest for current multilingual modbe attributed to a combination of parallel data and els. To this end, it drops XTREME’s PAWS-X and new pre-training objectives that make use of it. Pre- BUCC tasks since recent advances have left less training on parallel data improves performance on room for further improvement, and they cover only retrieval by making the"
2021.emnlp-main.802,2020.emnlp-main.185,0,0.0332192,"Missing"
2021.emnlp-main.802,P19-1015,0,0.0348481,"Missing"
2021.emnlp-main.802,D16-1264,0,0.111238,"Missing"
2021.emnlp-main.802,2020.emnlp-main.477,1,0.864987,"Missing"
2021.emnlp-main.802,D19-1454,0,0.0537933,"Missing"
2021.emnlp-main.802,P18-1072,1,0.884249,"Missing"
2021.emnlp-main.802,P19-1355,0,0.0118816,"forts to include training data in multiple languages. Biases in multilingual models 7.4 Environmental concerns XTREME - R aims to enable efficient evaluation of multilingual models. To this end, we created a new dataset, Mewsli-X, that captures the essence of multilingual entity linking against a diverse knowledge base but is computationally cheaper to evaluate than the large-scale Mewsli-9 (Botha et al., 2020). Nevertheless, the models that perform best on benchmarks like XTREME - R are generally large-scale Transformer models pre-trained on large amounts of data, which comes at a high cost (Strubell et al., 2019). We thus particularly encourage the development of efficient methods to adapt existing models to new languages (Pfeiffer et al., 2020) rather than training multilingual models entirely from scratch. Acknowledgements We thank Marco Tulio Ribeiro for advice on C HECK L IST. We are grateful to Laura Rimell and Jon Clark for valuable feedback on drafts of this paper, and to Dan Gillick for feedback on the Mewsli-X dataset design. We thank Hila Gonen, Bidisha Samantha, and Partha Talukdar for advice on Arabic, Bengali, and Hebrew C HECK L IST examples. References Antonios Anastasopoulos and Graham"
2021.emnlp-main.802,N18-1101,0,0.0932773,"Missing"
2021.emnlp-main.802,2020.acl-main.442,0,0.188983,"c” retrieval (Roy et al., 2020), where targets must be retrieved from a large multilingual candidate pool. We additionally establish new state-of-the-art mT5 (Xue et al., 2021) and translate-train baselines for our tasks. XTREME - R aims to move away from a single aggregate metric summarizing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on language and task-specific attributes, which enables a more nuanced diagnosis of a model’s behaviour. We also make several logistic improvements to improve XTREME - R’s utility as a leade"
2021.emnlp-main.802,2021.naacl-main.41,1,0.926532,"and XGLUE (Liang et al., 2020) multilingual, diverse, and accessible. It expands on 10215 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10215–10245 c November 7–11, 2021. 2021 Association for Computational Linguistics by covering 50 typologically diverse languages and 10 challenging, diverse tasks. To make retrieval more difficult, we introduce two new tasks that focus on “language-agnostic” retrieval (Roy et al., 2020), where targets must be retrieved from a large multilingual candidate pool. We additionally establish new state-of-the-art mT5 (Xue et al., 2021) and translate-train baselines for our tasks. XTREME - R aims to move away from a single aggregate metric summarizing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabil"
2021.emnlp-main.99,2020.acl-main.747,0,0.248096,"te that while this data generation process results in pseudo-parallel data, we do not experiment with using it in a supervised training scenario due to its noisy properties and in order to keep our benchmark comparable. However, this is an interesting direction which we leave for future work. Pretraining Dataset We pretrain AfroBART on 11 languages: Afrikaans, English, French, Dutch9 , Bemba, Xhosa, Zulu, Rundi, Sesotho, Swahili, and Lingala. To construct the original monolingual corpora, we use a combination of the training sets in A FRO MT and data derived from CC10010 (Wenzek et al., 2020; Conneau et al., 2020). We only perform dictionary augmentation on our English monolingual data. We list monolingual and pseudo-monolingual corpora statistics in Table 1. Balancing data across languages As we are training on different languages with widely varying amounts of text, we use the exponential sampling technique used in Conneau and Lample (2019); Liu et al. (2020), where the text is re-sampled according to smoothing parameter α as shown below: pα qk = PN k (1) Following this, we concatenate the existing low˜ and continue training resource corpus L with L our pretrained sequence-to-sequence model on this n"
2021.emnlp-main.99,2020.coling-tutorials.3,0,0.0207941,"96 18.54 20.07 20.48 20.60 43.04 40.26 43.79 43.74 43.87 33.61 30.55 34.19 34.30 34.36 51.89 50.62 52.87 53.22 53.24 52.62 53.16 54.31 54.40 54.68 65.54 62.95 66.14 66.16 66.30 45.85 46.20 47.50 47.69 48.00 68.11 64.73 68.54 68.75 69.03 62.68 60.65 63.22 63.33 63.42 52.91 54.81 58.09 58.15 58.22 58.56 56.44 59.08 59.08 59.11 Table 2: Results on A FRO MT’s En-XX Machine Translation GPUs for one day. When we continue training using pseudo-monolingual data, we use a learning rate of 7 × 10−5 and warm up over 5K iterations and train for 35K iterations. 4.2 multilingual machine translation system (Dabre et al., 2020) trained on all En-XX directions. • Random As additional baselines, we also provide a comparison with a randomly initialized Transformer-base (Vaswani et al., 2017) models for each translation pair. Finetuning Baselines We use the following baselines for our benchmark: • AfroBART Baseline We pretrain a model using only the original monolingual corpora in a similar fashion to Liu et al. (2020). • AfroBART-Dictionary We pretrain a model using the original data in addition to a dictionary augmented English monolingual corpora in Afrikaans, Bemba, Sesotho, Xhosa, Zulu, Lingala, and Swahili. • Afro"
2021.emnlp-main.99,N19-1423,0,0.0128257,"lasses with top 10 most frequent 3-character prefixes. sources and more languages and encourage the community to do so as well. 6 Related Work Machine Translation Benchmarks Previous work in benchmarking includes the commonly used WMT (Bojar et al., 2017) and IWSLT (Federico et al., 2020) shared tasks. Recent work on MT benchmarks for low-resource languages, such as that of Guzmán et al. (2019), have been used for the purpose of studying current NMT techniques for low-resource languages. Multilingual Pretraining Multilingual encoder Although we believe A FRO MT to be an important pretraining (Devlin et al., 2019; Conneau and Lamstep in the right direction, we acknowledge it is ple, 2019; Conneau et al., 2020) has been demonfar from being the end-all-be-all. Specifically, we strated to be an effective technique for cross-lingual note the following: (1) the lack of domain diversity transfer on a variety of classification tasks (Hu among many languages (being largely from reli- et al., 2020; Artetxe et al., 2020). More recently, gious oriented corpora) and (2) the corpora may sequence-to-sequence pretraining has emerged as a still contain some more fine-grained forms of noise prevalent method for achiev"
2021.emnlp-main.99,N13-1073,0,0.0250601,"e characteristic, our fine-grained analysis tool calculates the translation accuracy of the nouns with the top 10 most frequent prefixes in the test data. To do so, one of the challenges is to identify nouns in a sentence written in the target African language. However, there is no available part-of-speech (POS) tagger for these languages. To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an existing English POS tagger in the spaCy14 library to annotate the English source sentences. We then use the fast_align15 tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment Comparison on convergence speed In contrast to the cross-lingual transfer baseline which involves the usage of more data, and the random initialization baseline which needs to learn from scratch, AfroBART is able to leverage the knowl1312 14 15 https://spacy.io/ https://github.com/clab/fast_align 5.4 Shortcomings of A FRO MT Random AfroBART 70 60 Accuracy 50 40 30 20 10 0 ma wa ku ki mi ya vi mw ka ha Avg Noun class Figure 4: Translation accuracy of the AfroBART and Random baseline systems"
2021.emnlp-main.99,W19-6721,0,0.0252499,"-Samuel and Hartell, 1989). For example: in Zulu, isiZulu refers to the Zulu language, whereas amaZulu refers to the Zulu people. Additionally, these languages also use “verb extensions”, verb-suffixes used to modify the meaning of the verb. These qualities contribute to the morphological richness of these languages — a stark contrast with European languages. 2.2 Data Sources For our benchmark, we leverage existing parallel data for each of our language pairs. This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS3 (Tiedemann, 2012) and (2) ParaCrawl (Esplà et al., 2019). From OPUS, we use the JW300 corpus (Agi´c and Vuli´c, 2019), OpenSubtitles (Lison and Tiedemann, 2016), XhosaNavy, Memat, and QED (Abdelali et al., 2014). Despite the existence of this parallel data, these text datasets were often collected from large, relatively unclean multilingual corpora, e.g. JW300 which was extracted from Jehovah’s Witnesses text, or QED which was extracted from transcribed educational videos. This leads to many sentences with high lexical overlap, inconsistent tokenization, and other undesirable properties for a clean, reproducible benchmark. 2.3 Data Preparation Trai"
2021.emnlp-main.99,D19-1632,0,0.0981408,"2 , Graham Neubig2 , Yutaka Matsuo1 1 The University of Tokyo, 2 Carnegie Mellon University {machelreid,matsuo}@weblab.t.u-tokyo.ac.jp {junjieh,gneubig}@cs.cmu.edu Abstract been demonstrated in settings where very large parallel datasets are available (Meng et al., 2019; Reproducible benchmarks are crucial in drivArivazhagan et al., 2019), and NMT systems ofing progress of machine translation research. ten underperform in low-resource settings when However, existing machine translation benchgiven small amounts of parallel corpora (Koehn marks have been mostly limited to highand Knowles, 2017; Guzmán et al., 2019). One resource or well-represented languages. Desolution to this has been leveraging multilingual spite an increasing interest in low-resource machine translation, there are no standardized pretraining on large sets of monolingual data (Conreproducible benchmarks for many African neau and Lample, 2019; Song et al., 2019; Liu languages, many of which are used by milet al., 2020), leading to improvements even with lions of speakers but have less digitized texsmaller parallel corpora. However, this thread of tual data. To tackle these challenges, we prowork has focused on scenarios with the follo"
2021.emnlp-main.99,tiedemann-2012-parallel,0,0.0555944,"he word, usually as a prefix (Bendor-Samuel and Hartell, 1989). For example: in Zulu, isiZulu refers to the Zulu language, whereas amaZulu refers to the Zulu people. Additionally, these languages also use “verb extensions”, verb-suffixes used to modify the meaning of the verb. These qualities contribute to the morphological richness of these languages — a stark contrast with European languages. 2.2 Data Sources For our benchmark, we leverage existing parallel data for each of our language pairs. This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS3 (Tiedemann, 2012) and (2) ParaCrawl (Esplà et al., 2019). From OPUS, we use the JW300 corpus (Agi´c and Vuli´c, 2019), OpenSubtitles (Lison and Tiedemann, 2016), XhosaNavy, Memat, and QED (Abdelali et al., 2014). Despite the existence of this parallel data, these text datasets were often collected from large, relatively unclean multilingual corpora, e.g. JW300 which was extracted from Jehovah’s Witnesses text, or QED which was extracted from transcribed educational videos. This leads to many sentences with high lexical overlap, inconsistent tokenization, and other undesirable properties for a clean, reproducib"
2021.emnlp-main.99,2020.lrec-1.494,0,0.0147624,"pretraining 8 4.1 Note that while this data generation process results in pseudo-parallel data, we do not experiment with using it in a supervised training scenario due to its noisy properties and in order to keep our benchmark comparable. However, this is an interesting direction which we leave for future work. Pretraining Dataset We pretrain AfroBART on 11 languages: Afrikaans, English, French, Dutch9 , Bemba, Xhosa, Zulu, Rundi, Sesotho, Swahili, and Lingala. To construct the original monolingual corpora, we use a combination of the training sets in A FRO MT and data derived from CC10010 (Wenzek et al., 2020; Conneau et al., 2020). We only perform dictionary augmentation on our English monolingual data. We list monolingual and pseudo-monolingual corpora statistics in Table 1. Balancing data across languages As we are training on different languages with widely varying amounts of text, we use the exponential sampling technique used in Conneau and Lample (2019); Liu et al. (2020), where the text is re-sampled according to smoothing parameter α as shown below: pα qk = PN k (1) Following this, we concatenate the existing low˜ and continue training resource corpus L with L our pretrained sequence-to-s"
2021.emnlp-main.99,D16-1163,0,0.019877,"d 8 African languages — Afrikaans, Xhosa, Zulu, Rundi, Sesotho, Swahili, Bemba, and Lingala — four of which are not included in commercial translation systems such as Google Translate (as of Feb. 2021). In §2, we describe the detailed design of our benchmark, including the language selection criterion and the methodology to collect, clean and normalize the data for training and evaluation purposes. In §3, we provide a set of strong baselines for our benchmark, including denoising sequence-to-sequence pretraining (Lewis et al., 2020; Liu et al., 2020), transfer learning with similar languages (Zoph et al., 2016; Neubig and Hu, 2018), and our proposed data augmentation methods for pretraining on low-resource languages. Our first method leverages bilingual dictionaries to augment data in high-resource languages (HRL), and our second method iteratively creates pseudo-monolingual data in low-resource languages (LRL) for pretraining. Extensive experiments in §4 show that our proposed methods outperform our baselines by up to ∼2 BLEU points over all language pairs and up to ∼15 BLEU points in data-constrained scenarios. 2 A FRO MT benchmark In this section, we detail the construction of our new benchmark,"
2021.findings-acl.120,2020.nlpcovid19-2.5,1,0.623006,"Missing"
2021.findings-acl.120,W19-5435,1,0.897806,"Missing"
2021.findings-acl.120,2020.acl-main.747,1,0.696864,"Missing"
2021.findings-acl.120,N19-1423,0,0.00646772,"on the source sentence in the bi-text corpus as the paraphrased target. Let D denote the paraphrased sentence of T and D0 denote the generation from BART conditioned on the noised D. Then we create pseudo labels of D0 denoted LD0 by computing the edit-distance between the D0 and D and use ((S, T, D0 ), LD0 ) as the training data for finetuning. Since the pseudo labels are created based on D, it can prevent the model from learning the edit-distance between T and D0 easily. We provide ablation studies in Appendix D. Masked LM loss We also add the masked language model loss (MLM) Lmlm following (Devlin et al., 2019). To learn this loss, we create a different batch from the above by concatenating only the source S and target T as the input, since the hallucinated target T 0 could provide erroneous information for predicting masked words in T . We find that such multi-task learning objective helps learn better representations of the input and further improves performance on predicting hallucination labels. The final loss is L = Lpred + α · Lmlm where α is a hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the mo"
2021.findings-acl.120,2020.acl-main.454,1,0.844635,"sed for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393–1404 August 1–6, 2021. ©2021 Association for Computational Linguistics lucinated tokens and only correlate weakly with human judgements. We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinated or faithful to the so"
2021.findings-acl.120,D19-1632,1,0.862838,"Missing"
2021.findings-acl.120,D16-1139,0,0.178743,"risingly find our model can generalize well without references, even when they were present during training. To prevent the model from overly relying on the true target T and learning spurious correlations (e.g. the edit distance), we explored two techniques: (1) dropout – randomly drop out tokens in T to force the dependence on the source input; (2) paraphrase – recall that at synthetic data generation time, we generate T 0 from BART conditioned on the noised T . Instead, we can apply noise functions to the paraphrased sentence of T . We create paraphrased targets via knowledge distillation (Kim and Rush, 2016) where we use the output from pretrained Seq2Seq model conditioned on the source sentence in the bi-text corpus as the paraphrased target. Let D denote the paraphrased sentence of T and D0 denote the generation from BART conditioned on the noised D. Then we create pseudo labels of D0 denoted LD0 by computing the edit-distance between the D0 and D and use ((S, T, D0 ), LD0 ) as the training data for finetuning. Since the pseudo labels are created based on D, it can prevent the model from learning the edit-distance between T and D0 easily. We provide ablation studies in Appendix D. Masked LM los"
2021.findings-acl.120,W19-5404,1,0.902402,"Missing"
2021.findings-acl.120,W17-3204,0,0.022642,"nce summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and additionally consider the low resource case where a modest amount of out of domain data is available at training time. Data We use a multi-domain Chinese-English (Zh-En) translation dataset (Wang et al., 2020b) which consists of four balanced domains: law, news, patent and subtitles. We create a new training data Dtrain with law (1.46M sentences), news (1.54M), subtitles (1.77M) train data and randomly sample 870 parallel sentences from the patent training"
2021.findings-acl.120,D18-1512,0,0.0477721,"Missing"
2021.findings-acl.120,2020.acl-main.703,1,0.885035,"token by computing the edit distance between T 0 and T . Labels of 1 refer to hallucinated words. a supervised model on this synthetic labeled data set of ((S, T 0 ), LT 0 ). The key challenge is that T 0 should be a fluent sentence that does not differ too much from T . Generation of hallucinated sentences To control this synthetic hallucination process, we build on a pre-trained denoising autoencoder, which maps a corrupted sentence back to the original text it was derived from, learning to reconstruct missing words that have been arbitrarily masked out. Specifically, we use the BART model (Lewis et al., 2020), without providing it any access to the source sentence, thereby encouraging it to insert new content as needed to ensure fluency. As shown in Fig. 2, we first apply a noising function that removes words from the original target sentence T 4 and then use a pretrained BART to generate T 0 conditioned on the noised T with beam search. T Mike T&apos; Jerry 1 goes to the bookstore on happily goes to the bookstore with his friend. 1 0 0 1 1 1 0 0 Thursday. Figure 3: An example of label assignment. Label assignments After obtaining the hallucinated sentence T 0 with BART, we need to assign appropriate l"
2021.findings-acl.120,W04-1013,0,0.0302168,"work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tai"
2021.findings-acl.120,W18-6478,0,0.0259819,"l. (2020), which injects two types of noise into the input sentences: (1) paraphrase noise created by round-trip translations, and (2) random noise from dropping, mask5 We also tried removing hallucinated target words before training. This underperformed, likely because it produces too many ungrammatical target sentences. Case Study II: Improving Corpus Filtering for Low-Resource MT High-quality parallel data is critical for training effective neural MT systems, but acquiring it can be expensive and time-consuming. Many systems instead use mined and filtered parallel data to train NMT models (Junczys-Dowmunt, 2018; Zhang et al., 2020; Koehn et al., 2019). Nonetheless, the selected parallel data can still be noisy, containing misaligned segments. In this section, we demonstrate that token-level hallucination labels can allow us to make better use of noisy data to and improve the overall translation quality. We apply the token loss truncation method proposed in §6 to the filtered parallel data and evaluate it on the WMT2019 low-resource parallel corpus filtering shared task. Experimental Setup The WMT19 shared task focuses on two low-resource languages – Nepali and Sinhala. It released a very noisy 40.6"
2021.findings-acl.120,2020.tacl-1.47,1,0.838219,"ock are our results. Bold indicates best results not using references. the patent test data. In addition, we also test the NMT models on the COVID-19 domain, sampling 100 examples from the dataset of Anastasopoulos et al. (2020). We denote this 250-sentence dataset as Deval and ask human annotators to evaluate the level of hallucinations thereof. Models Our data is generated from two models on which we will measure hallucination (see Appendix B for more details): (1) TranS2S (Vaswani et al., 2017) is the standard Transformer Seq2Seq model with 6 encoder layers and 6 decoder layers. (2) MBART (Liu et al., 2020) is a Seq2Seq denoising auto-encoder pretrained on large-scale monolingual corpora in many languages. We finetune the 12 layer model on Dtrain . 5 5.1 Experiments Experimental setup Synthetic Data Generation We use a pretrained 12 layer BART (Lewis et al., 2020) model in the fairseq toolkit (Ott et al., 2019) for synthetic labeled data generation. We uniformly sample the percentage of tokens pm to mask from [0, hm ] for each sentence. We also uniformly sample the probability of replacing a token with a random token from [0, hr ] denoted pr . pm and pr are two important factors that affect the"
2021.findings-acl.120,2020.acl-main.66,0,0.0496903,"Missing"
2021.findings-acl.120,2021.ccl-1.108,0,0.0517646,"Missing"
2021.findings-acl.120,P19-3020,0,0.0349607,"Missing"
2021.findings-acl.120,W19-6623,0,0.0233024,"seline methods. We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 . 1 1 1 0 0 0 0 1 1 1 Jerry happily goes to the bookstore with his friend. Figure 1: An example of token-level hallucination detection from MT. The grey box is an example of MT output and the labels above indicate if each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Machine Translation Introducti"
2021.findings-acl.120,2020.acl-main.173,0,0.114553,"ements over strong baseline methods. We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 . 1 1 1 0 0 0 0 1 1 1 Jerry happily goes to the bookstore with his friend. Figure 1: An example of token-level hallucination detection from MT. The grey box is an example of MT output and the labels above indicate if each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Mach"
2021.findings-acl.120,D18-1206,0,0.0429095,"hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the models and datasets described below. 4.1 Abstractive Text Summarization Maynez et al. (2020) studied hallucination problems in extreme summarization on the XS UM dataset which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their single-sentence summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and additionally consider the low resour"
2021.findings-acl.120,N19-4009,0,0.0150506,"e the level of hallucinations thereof. Models Our data is generated from two models on which we will measure hallucination (see Appendix B for more details): (1) TranS2S (Vaswani et al., 2017) is the standard Transformer Seq2Seq model with 6 encoder layers and 6 decoder layers. (2) MBART (Liu et al., 2020) is a Seq2Seq denoising auto-encoder pretrained on large-scale monolingual corpora in many languages. We finetune the 12 layer model on Dtrain . 5 5.1 Experiments Experimental setup Synthetic Data Generation We use a pretrained 12 layer BART (Lewis et al., 2020) model in the fairseq toolkit (Ott et al., 2019) for synthetic labeled data generation. We uniformly sample the percentage of tokens pm to mask from [0, hm ] for each sentence. We also uniformly sample the probability of replacing a token with a random token from [0, hr ] denoted pr . pm and pr are two important factors that affect the noise level when generating the synthetic data. For MT, we set hm and hr to 0.6 and 0.3 respectively. For abstractive summarization, we use 0.4 and 0.2. We use beam search for decoding from BART with beam size of 4 and length penalty of 3. For MT, we first create paraphrased target sentences D0 through knowle"
2021.findings-acl.120,P02-1040,0,0.111696,"s∗ 。 Machine Translation Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez"
2021.findings-acl.120,W18-6319,0,0.0420902,"n Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020"
2021.findings-acl.120,2020.tacl-1.18,0,0.170716,"each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Machine Translation Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building mode"
2021.findings-acl.120,2020.findings-emnlp.147,0,0.0501994,"Missing"
2021.findings-acl.120,P17-1099,0,0.0248872,"pred + α · Lmlm where α is a hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the models and datasets described below. 4.1 Abstractive Text Summarization Maynez et al. (2020) studied hallucination problems in extreme summarization on the XS UM dataset which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their single-sentence summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and add"
2021.findings-acl.120,2020.acl-main.704,0,0.0160313,"48 12.55 ST + seq loss truncation ST-R + seq loss truncation 19.91 19.37 -0.048 -0.057 8.26 10.06 ST + token loss truncation ST + decoder HS masking ST-R + token loss truncation ST-R + decoder HS masking 20.32 20.57 21.02 20.64 0.00244 -0.0001 0.043 0.0308 6.37 6.38 7.34 8.70 ing and shuffling input tokens. We also compare with the recently proposed loss truncation method (Kang and Hashimoto, 2020) that adaptively removes entire examples with high log loss, which was shown to reduce hallucinations. Results and Analysis We present the tokenized BLEU score (Papineni et al., 2002), BLEURT score (Sellam et al., 2020) and the percentage of hallucinated tokens predicted by our system in Tab. 4. We can see that ST improves over the baseline by around 3 BLEU and our best result further improves ST by 1.7 BLEU. Compared with strong baseline methods, our method not only achieves the best translation quality measured by BLEU and BLEURT but also the largest hallucination reduction. We also observe that: (1) Our method with ST alone can outperform other baseline methods, when combined with perturbed ST (noise), and using fine-grained control over the target tokens can further improve the results. (2) ST with parap"
2021.findings-acl.120,P16-1162,0,0.0953211,"Missing"
2021.findings-acl.120,2020.wmt-1.79,1,0.831521,"Missing"
2021.findings-acl.120,2011.mtsummit-papers.58,0,0.0597625,"lness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinated or faithful to the source input. This task does not use the reference output to assess faithfulness, which offers us the ability to also apply it at run-time. Similar to the spirit of our proposed task, word-level quality estimation (Specia et al., 2018; Fonseca et al., 2019) in the MT community predicts if tokens are correctly translated based on human post-editing. However, these methods generally do not distinguish errors in terms of fluency and adequacy (Specia et al., 2011), with the exception of a subset of the WMT 2020 shared task on quality estimation (Specia et al., 2020), where different types and levels of severity of word-level errors are defined. Our proposed task specifically focuses on hallucination errors, and we define these errors in a simpler way with only binary labels, which we argue makes them simpler to use and more conducive to labeling at large scale. The proposed hallucination detection method (described below) is also applicable to the word-level quality estimation task as demonstrated in §5.4. We measure hallucination for two conditional s"
2021.findings-acl.120,N03-1033,0,0.021265,"odels for Conditional Sequence Generation Recent work (Maynez et al., 2020) has shown that pretrained models are better at generating faithful summaries as evaluated by humans. In Tab. 2, summaries generated from BERTS2S contain significantly fewer hallucinations than other model outputs. We also confirmed this trend in MT that translations from MBART contain less hallucinated content than that from TranS2S. Analysis on Hallucinated Words and their Partof-Speech Tags In Fig. 5, we present the percentage of hallucinated tokens categorized by their part-of-speech tags predicted by a POS tagger (Toutanova et al., 2003). First, we see that for both MT and summarization datasets, nouns are the most hallucinated words. In abstractive summarization, verbs also account for a certain number of hallucinations. Second, our model predicted hallucinated words match well with gold annotations on the distributions of POS tags. We also compare the percentage of hallucinations within each POS tag in Appendix E.2. In addition, we provide more 1398 Normalized Hallucination Ratio Normalized Hallucination Ratio MT 0.6 Gold Our predictions 0.4 0.2 0.0 NN others JJ VB IN POS tag CD RB SYM PRP XSum 0.5 Gold Our predictions 0.4"
2021.findings-acl.120,N03-1000,0,0.110311,"Missing"
2021.findings-acl.120,2020.acl-main.450,0,0.299588,"t standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393–1404 August 1–6, 2021. ©2021 Association for Computational Linguistics lucinated tokens and only correlate weakly with human judgements. We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinate"
2021.findings-acl.120,2020.acl-main.326,0,0.261799,"rce meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational"
2021.findings-acl.120,2020.acl-main.756,0,0.0338433,"Missing"
2021.findings-acl.120,2020.acl-main.552,0,0.0260132,"Missing"
2021.findings-acl.120,N19-1161,1,0.858552,"Missing"
2021.findings-emnlp.63,D19-1165,0,0.027544,"t, there are limited datasets and benchmarks that evaluate NLP models’ ability to generalize to unseen dialect variations. Therefore, we only test our method on NER and POS tagging tasks because they have the best language coverage. It is an important future direction to construct high-quality datasets that consider language and dialect variations. Second, our method has slower inference speed due to test time computation. Future work can aim to reduce the cost by algorithmic or hardware innovations. Our work is related to parameter efficient fine- Acknowledgement tuning of pretrained models (Bapna et al., 2019; Pfeiffer et al., 2020b; Li and Liang, 2021; Guo This material is based on work supported by the et al., 2021). Specifically, (Üstün et al., 2020; National Science Foundation under grants 2040926 Karimi Mahabadi et al., 2021) make adapters more and 2125201. XW is supported by the Apple PhD generalizable by learning a parameter generator, fellowship. The authors would like to thank Laura while our work aims to utilize existing pretrained Rimell, Sachin Kumar and Hieu Pham for helpful adapters without further training. Pfeiffer et al. discussions on the drafts of the paper. (2021) propose to le"
2021.findings-emnlp.63,2020.coling-main.579,0,0.0997332,"Missing"
2021.findings-emnlp.63,2020.acl-main.747,0,0.0305789,"nguages and then a task adapter on annotated data in the source language. One drawback of this framework is that a separate language adapter is required for each target language, which is problematic in cases where the data to train these adapters cannot be easily obtained, such as for languages with diverse regional 1 Introduction or demographic variations. In fact, certain language Massively multilingual pretrained models (Devlin varieties are not included in the standard language et al., 2019; Huang et al., 2019; Conneau and identification tools, which makes it challenging to Lample, 2019; Conneau et al., 2020) combined reliably obtain even unlabeled data (Salameh et al., with cross-lingual transfer now define the state 2018; Caswell et al., 2020; Demszky et al., 2021). of the art on a variety of NLP tasks (Hu et al., To give just one example, the Nordic languages 2020). Within this paradigm, multilingual pre- and dialects form a dialect continuum where the trained models are fine-tuned on annotated data total number of language varieties is difficult to esof a task in a high-resource language, and trans- timate, and language varieties constantly emerge in ferred to other languages. Several recent w"
2021.findings-emnlp.63,2021.naacl-main.184,0,0.084255,"Missing"
2021.findings-emnlp.63,N19-1423,0,0.19552,"e embedding space for task Ti . MAD-X trains the adapters Ti (·) and Lj (·) in two steps. First, for each language Lj , its adapter Adapter Ensembling As a first solution to this Lj is inserted into M to replace the output of each problem, we propose an extremely simple strategy layer h with Lj (h). The resulting model, which we of averaging the transformed outputs of multiple denote as Lj ◦ M, is trained on unlabeled data in language adapters. Specifically, we use both the Lj using an unsupervised objective such as masked source language adapter Lsrc and adapters from language modeling (MLM; Devlin et al., 2019). related languages with similar linguistic properties Second, for each task Ti , its adapter Ti is inserted to the new language. Let R be the set of the source on top of a src language adapter Lsrc . The resulting and related language adapters. To do inference on model Ti ◦ Lsrc ◦ M is trained on the downstream a task T for the new language Lnew , we transform task Ti in language Lsrc . After these two steps, 1 Ti ◦ Lj ◦ M can be used to perform zero-shot crosshttps://adapterhub.ml/ 731 the output h of each layerP in M with the language adapters as Lavg (h) = R1 R i=1 Li (h). Entropy Minimize"
2021.findings-emnlp.63,2021.acl-long.353,0,0.0524787,"Missing"
2021.findings-emnlp.63,P17-1178,0,0.0159875,"..., T-1 do . Calculate entropy H(x, α) ← Entropy(T ◦ Lwavg (h, αt ) ◦ M) . Calculate gradient g t = ∇α H(x; αt ) . Update weighting αt+1 ← Update(αt , g t ) end . Calculate final prediction yˆ ← Predict(T ◦ Lwavg (h, αT ) ◦ M) Related hi is ru Additional Test en,ar en,de en mr,bn,ta,bho fo,no,da be,uk,bg Table 1: Test language groups and their corresponding language adapters. Adapters from languages in the first two columns are applied to the test languages in the third column. conduct experiments on named entity recognition (NER) and part-of-speech tagging (POS). We use the WikiAnn dataset (Pan et al., 2017) for NER and Universial Treebank 2.0 for POS tagging (Nivre et al., 2018). Model We use the mBERT (Devlin et al., 2019) model, which shows good performance for lowresource languages on the structured prediction tasks (Pfeiffer et al., 2020b; Hu et al., 2020). We use the English annotated data to train the task adapter. Each experiment is run with 3 different random seeds and we report the average performance. More details can be found in Appendix A. Languages Due to the scarcity of datasets for dialects, we focus on three groups of closely related languages to simulate the setup of language va"
2021.findings-emnlp.63,2021.eacl-main.39,0,0.0208731,"ining and storing extra parameters. Its performance is also not consistent across languages and tasks, likely because it is only trained on English labeled data. 50 70.0 67.5 Baselines We compare with several baselines: 1) En: the English adapter; 2) Related: the best performing related language adapter; 3) Continual learning (CL): we use the English language adapter and update its parameters using the entropy loss for each test input; 4) Fusion: learn another set of key, value and query parameters in each layer that uses the layer output as a query to mix together the output of each adapter (Pfeiffer et al., 2021). Since we do not use labeled data in the new language, we train the fusion parameters on English labeled data. 4.1 mr no F1 3.0 F1 gain by adding English F1 gain over ensemble Table 2: F1 of the baselines and our methods for each language group. EMEA-s1 updates the adapter weights with a single gradient step while EMEA-s10 updates for 10 steps. New adapter EMEA 1k 10k 50k 100k Monolingual Data 40 1k 10k 50k 100k Monolingual Data Figure 4: Comparison to training adapter on different amount of monolingual data. provements or is comparable to the best baseline on other languages. EMEA delivers f"
2021.findings-emnlp.63,2020.emnlp-demos.7,1,0.859962,"Missing"
2021.findings-emnlp.63,2021.acl-long.378,0,0.0665002,"Missing"
2021.findings-emnlp.63,2020.emnlp-main.617,1,0.857427,"Missing"
2021.findings-emnlp.63,2020.acl-main.244,0,0.0241551,"dapters to support a new language Lnew , which is not in {L1 , L2 , ..., Ln } on a given task T without training a new adapter for Lnew . Related Language Adapters One potential solution is to find the most related language Lrel ∈ {L1 , L2 , ..., Ln } and then use T ◦ Lrel ◦ M to do inference in Lnew . However, this has two disadvantages. First, the task adapter T is only trained in the setting of T ◦ Lsrc ◦ M, so it might not generalize well to the test time setting of T ◦ Lrel ◦ M (as shown in § 4.1). Second, while the pretrained model M may be relatively robust against distribution shifts (Hendrycks et al., 2020), the specialized language adapters might make the model brittle to language variations because they are trained for specific languages. Our experiments in § 4.1 show that this solution indeed leads to poor performance. To facilitate our discussion, we briefly summarize the MAD-X framework (Pfeiffer et al., 2020b) for zero-shot cross-lingual transfer and identify its shortcomings. The goal of MAD-X is to fine-tune a multilingual pretrained model M to m downstream tasks T1 , T2 , ..., Tm , each of which could be in n languages L1 , L2 , ..., Ln . To this end, MAD-X relies on language and task a"
2021.findings-emnlp.63,D19-1252,0,0.025998,"m zero-shot transfer by first training language-level adapters on monolingual data in different languages and then a task adapter on annotated data in the source language. One drawback of this framework is that a separate language adapter is required for each target language, which is problematic in cases where the data to train these adapters cannot be easily obtained, such as for languages with diverse regional 1 Introduction or demographic variations. In fact, certain language Massively multilingual pretrained models (Devlin varieties are not included in the standard language et al., 2019; Huang et al., 2019; Conneau and identification tools, which makes it challenging to Lample, 2019; Conneau et al., 2020) combined reliably obtain even unlabeled data (Salameh et al., with cross-lingual transfer now define the state 2018; Caswell et al., 2020; Demszky et al., 2021). of the art on a variety of NLP tasks (Hu et al., To give just one example, the Nordic languages 2020). Within this paradigm, multilingual pre- and dialects form a dialect continuum where the trained models are fine-tuned on annotated data total number of language varieties is difficult to esof a task in a high-resource language, and t"
2021.findings-emnlp.63,2021.acl-long.47,1,0.82652,"Missing"
2021.findings-emnlp.63,2021.eacl-main.6,0,0.0305732,"lts in Tab. 2 showing en as the best individual adapter. For the hi language group, the ar adapter tends to have the least benefit, probably because it has a different script from the languages we test on. 5 Related Work beled data to combine pretrained multitask adapters whereas our method does not require any training or labeled data. While we focus on language adapters in this work, our method is also applicable to ensembling domain or task adapters. Finally, our method is inspired by the test time adaptation framework proposed for image classification (Sun et al., 2020; Wang et al., 2021; Kedia and Chinthakindi, 2021). Instead of adapting a single model, we focus on efficient utilization of many pre-trained language adapters to improve the model’s robustness to language variations. 6 Discussion and Conclusion Language and dialect cannot be simply categorized into monolithic entities. Thus a truly intelligent NLP system should be able to recognize and adapt to personalized language varieties after it is trained and deployed. However, the standard system evaluation is built on the assumption that an NLP model is fixed once it is trained. In this paper, we focus on a specific case of this general problem—we f"
2021.findings-emnlp.63,C18-1113,0,0.0405423,"Missing"
2021.findings-emnlp.63,2020.emnlp-main.180,0,0.0424408,"Missing"
2021.mtsummit-at4ssl.1,W17-4715,0,0.0606491,"Missing"
2021.mtsummit-at4ssl.1,2020.emnlp-main.475,1,0.841933,"ional Workshop on Automatic Translation for Signed and Spoken Languages Page 3 4.1 Back-translation Back-translation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016a) automatically creates pseudo-parallel sentence pairs from monolingual text to improve MT in low-resource settings. However, back-translation is only effective with sufficient parallel data to train a functional MT model, which is not always the case in extremely low-resource settings (Currey et al., 2017), and particularly when the domain of the parallel training data and monolingual data to be translated are mismatched (Dou et al., 2020). 4.2 Proposed Rule-based Augmentation Strategies Given the limitations of standard back-translation techniques, we next move to the proposed method of using rule-based heuristics to generate SL glosses from spoken language text. General rules The differences in SL glosses from spoken language can be summarized by (1) A lack of word inflection, (2) An omission of punctuation and individual words, and (3) Syntactic diversity. We, therefore, propose the corresponding three heuristics to generate pseudo-glosses from spoken language: (1) Lemmatization of spoken words; (2) POS-dependent and random"
2021.mtsummit-at4ssl.1,D18-1045,0,0.0253435,"reover, on PHOENIX Specific-pre achieves significantly better performance than General-pre, which suggests our hand-crafted syntax transformations effectively expose the model to the divergence between DGS and German during pre-training. Next, turning to the tuned models, we see that Specific and General outperform both the baseline and BT by large margins, demonstrating the effectiveness of our proposed methods. Interestingly, General-tuned performs slightly better, in contrast to the previous result. We posit that, similarly to previously reported results on sampling-based back translation (Edunov et al., 2018), General is benefiting from the diversity provided by sampling multiple reordering candidates, even if each candidate is of lower quality. Looking at Figure 3, we see that the superior performance of our methods holds for all data sizes, but it is particularly pronounced when the parallel-data-only baseline achieves moderate BLEU scores in the range of 5-20. This confirms that BT is not a viable data augmentation method when parallel data is not plentiful enough to train a robust back-translation system. 7 Implications and Future Work Consistent improvements over the baseline across two langu"
2021.mtsummit-at4ssl.1,2020.signlang-1.12,0,0.182749,"Missing"
2021.mtsummit-at4ssl.1,W13-2233,0,0.0303371,"usses methods to improve gloss-to-text translation through data augmentation, specifically those that take monolingual corpora of standard spoken languages and generate pseudo-parallel “gloss” text. We first discuss a standard way of doing so, back-translation, point out its potential failings in the SL setting, then propose a novel rule-based data augmentation algorithm. Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 3 4.1 Back-translation Back-translation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016a) automatically creates pseudo-parallel sentence pairs from monolingual text to improve MT in low-resource settings. However, back-translation is only effective with sufficient parallel data to train a functional MT model, which is not always the case in extremely low-resource settings (Currey et al., 2017), and particularly when the domain of the parallel training data and monolingual data to be translated are mismatched (Dou et al., 2020). 4.2 Proposed Rule-based Augmentation Strategies Given the limitations of standard back-translation techniques, we next move to the"
2021.mtsummit-at4ssl.1,W10-1736,0,0.0151848,"Random word permutation. We use spaCy (Honnibal and Montani, 2017) for (1) lemmatization and (2) POS tagging to only keep nouns, verbs, adjectives, adverbs, and numerals. We also drop the remaining tokens with probability p = 0.2, and (3) randomly reorder tokens with maximum distance d = 4. Language-specific rules While random permutation allows some degree of robustness to word order, it cannot capture all aspects of syntactic divergence between signed and spoken language. Therefore, inspired by previous work on rule-based syntactic transformations for reordering in MT (Collins et al., 2005; Isozaki et al., 2010; Zhou et al., 2019), we manually devise a shortlist of syntax transformation rules based on the grammar of DGS and German. We perform lemmatization and POS filtering as before. In addition, we apply compound splitting (Tuggener, 2016) on nouns and only keep the first noun, reorder German SVO sentences to SOV, move adverbs and location words to the start of the sentence, and move negation words to the end. We provide a detailed list of rules in Appendix A. 5 Experimental Setting 5.1 Datasets DGS & German RWTH-PHOENIX-Weather 2014T (Camgoz et al., 2018) is a parallel corpus of 8,257 DGS interpr"
2021.mtsummit-at4ssl.1,P19-1301,1,0.800392,"xical and syntactic similarity between different language pairs denoted by their ISO639-2 codes. However, we argue that the relationship between glossed SLs and their spoken counterparts is different from the usual relationship between two spoken languages. Specifically, glossed SLs are lexically similar but syntactically different from their spoken counterparts. This contrasts heavily with the relationship among spoken language pairs where lexically similar languages tend also to be syntactically similar the great majority of the time. To demonstrate this empirically, we adopt measures from (Lin et al., 2019) to measure the lexical and syntactic similarity between languages, two features also shown to be positively correlated with the effectiveness of performing cross-lingual transfer in MT. Lexical similarity between two languages is measured using word overlap: ow = |T1 ∩ T2 | |T1 |+ |T2 | where T1 and T2 are the sets of types in a corpus for each language. The word overlap between spoken language pairs is calculated using the TED talks dataset (Qi et al., 2018). The overlap between sign-spoken language pairs is calculated from the corresponding corpora in Table 1. Syntactic similarity between t"
2021.mtsummit-at4ssl.1,P19-1579,1,0.852874,"into two steps: (1) video-to-gloss, or continuous sign language recognition (CSLR) (Cui et al., 2017; Camgoz et al., 2018); (2) gloss-to-text, which is a text-to-text machine translation (MT) task (Camgoz et al., 2018; Yin and Read, 2020b). In this paper, we focus on gloss-to-text translation. SL data and resources are often scarce, or nonexistent (§2; Bragg et al. (2019)). Gloss-to-text translation is, therefore, an example of an extremely low-resource MT task. However, while there is extensive literature on low-resource MT between spoken languages (Sennrich et al., 2016a; Zoph et al., 2016; Xia et al., 2019; Zhou et al., 2019), the dissimilarity between sign and spoken languages calls for novel methods. Specifically, as SL glosses borrow the lexical elements from their ambient spoken language, handling syntax and morphology poses greater challenges than lexeme translation (§3). Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 1 ASL Video: GLOSSING ASL Gloss: fs-JOHN FUTURE FINISH READ BOOK WHEN HOLD TRANSLATION English: When will John ﬁnish reading the book? (a)"
2021.mtsummit-at4ssl.1,2020.coling-main.525,1,0.646995,"ication, or closedcaptioning to interact with others. Sign language translation (SLT) is an important research area that aims to improve communication between signers and non-signers while allowing each party to use their preferred language. SLT consists of translating a sign language (SL) video into a spoken language (SpL) text, and current approaches often decompose this task into two steps: (1) video-to-gloss, or continuous sign language recognition (CSLR) (Cui et al., 2017; Camgoz et al., 2018); (2) gloss-to-text, which is a text-to-text machine translation (MT) task (Camgoz et al., 2018; Yin and Read, 2020b). In this paper, we focus on gloss-to-text translation. SL data and resources are often scarce, or nonexistent (§2; Bragg et al. (2019)). Gloss-to-text translation is, therefore, an example of an extremely low-resource MT task. However, while there is extensive literature on low-resource MT between spoken languages (Sennrich et al., 2016a; Zoph et al., 2016; Xia et al., 2019; Zhou et al., 2019), the dissimilarity between sign and spoken languages calls for novel methods. Specifically, as SL glosses borrow the lexical elements from their ambient spoken language, handling syntax and morphology"
2021.mtsummit-at4ssl.1,D19-1143,1,0.926652,") video-to-gloss, or continuous sign language recognition (CSLR) (Cui et al., 2017; Camgoz et al., 2018); (2) gloss-to-text, which is a text-to-text machine translation (MT) task (Camgoz et al., 2018; Yin and Read, 2020b). In this paper, we focus on gloss-to-text translation. SL data and resources are often scarce, or nonexistent (§2; Bragg et al. (2019)). Gloss-to-text translation is, therefore, an example of an extremely low-resource MT task. However, while there is extensive literature on low-resource MT between spoken languages (Sennrich et al., 2016a; Zoph et al., 2016; Xia et al., 2019; Zhou et al., 2019), the dissimilarity between sign and spoken languages calls for novel methods. Specifically, as SL glosses borrow the lexical elements from their ambient spoken language, handling syntax and morphology poses greater challenges than lexeme translation (§3). Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 1 ASL Video: GLOSSING ASL Gloss: fs-JOHN FUTURE FINISH READ BOOK WHEN HOLD TRANSLATION English: When will John ﬁnish reading the book? (a) ASL video with gloss"
2021.mtsummit-at4ssl.1,D16-1163,0,0.0316609,"ecompose this task into two steps: (1) video-to-gloss, or continuous sign language recognition (CSLR) (Cui et al., 2017; Camgoz et al., 2018); (2) gloss-to-text, which is a text-to-text machine translation (MT) task (Camgoz et al., 2018; Yin and Read, 2020b). In this paper, we focus on gloss-to-text translation. SL data and resources are often scarce, or nonexistent (§2; Bragg et al. (2019)). Gloss-to-text translation is, therefore, an example of an extremely low-resource MT task. However, while there is extensive literature on low-resource MT between spoken languages (Sennrich et al., 2016a; Zoph et al., 2016; Xia et al., 2019; Zhou et al., 2019), the dissimilarity between sign and spoken languages calls for novel methods. Specifically, as SL glosses borrow the lexical elements from their ambient spoken language, handling syntax and morphology poses greater challenges than lexeme translation (§3). Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 1 ASL Video: GLOSSING ASL Gloss: fs-JOHN FUTURE FINISH READ BOOK WHEN HOLD TRANSLATION English: When will John ﬁnish read"
2021.naacl-main.195,P17-1042,0,0.0146021,"s-lingual capability of vision-language models. (4) We demonstrate the effectiveness of our approach, by achieving state-of-the-art multilingual text→video search performance in both the zero-shot (§5.5) and fully supervised setup (§5.6). 2 Related Work Cross-lingual representations. Early work on learning non-contextual cross-lingual representations used either parallel corpora (Gouws and Søgaard, 2015; Luong et al., 2015) or a bilingual dictionary to learn a transformation (Faruqui and Dyer, 2014; Mikolov et al., 2013). Later approaches reduced the amount of supervision using self-training (Artetxe et al., 2017). With the advances in monolingual transfer learning (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019), multilingual extensions of pre-trained encoders have been proven effective in learning deep contextual cross-lingual representations (Eriguchi et al., 2017; Lample and Conneau, 2019; Wu and Dredze, 2019b; Siddhant et al., 2020; Pires et al., 2019; Pfeiffer et al., 2020). We extend prior work to incorporate visual context. Video-text representations. The HowTo100M dataset (Miech et al., 2019) has attracted significant interest in leveraging multimodal pre"
2021.naacl-main.195,2020.acl-main.421,0,0.432358,"plied ∗ Equal contribution. as-is to a different language without any additional annotated training data (T¨ackstr¨om et al., 2012; Klementiev et al., 2012; Cotterell and Heigold, 2017; Chen et al., 2018; Neubig and Hu, 2018). In particular, recent techniques for cross-lingual transfer have demonstrated that by performing unsupervised learning of language or translation models on many languages, followed by downstream task fine-tuning using only English annotation, models can nonetheless generalize to a non-English language (Wu and Dredze, 2019a; Lample and Conneau, 2019; Huang et al., 2019a; Artetxe et al., 2020; Hu et al., 2020). This success is attributed to the fact that many languages share a considerable amount of underlying vocabulary or structure. At the vocabulary level, languages often have words that stem from the same origin, for instance, “desk” in English and “Tisch” in German both come from the Latin “discus”. At the structural level, all languages have a recursive structure, and many share traits of morphology or word order. For cross-lingual transfer of vision-language models, the visual information is clearly an essential element. To this end, we make an important yet under-explored"
2021.naacl-main.195,W19-4802,0,0.0265602,"Missing"
2021.naacl-main.195,P17-2012,0,0.0126932,"n learning non-contextual cross-lingual representations used either parallel corpora (Gouws and Søgaard, 2015; Luong et al., 2015) or a bilingual dictionary to learn a transformation (Faruqui and Dyer, 2014; Mikolov et al., 2013). Later approaches reduced the amount of supervision using self-training (Artetxe et al., 2017). With the advances in monolingual transfer learning (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019), multilingual extensions of pre-trained encoders have been proven effective in learning deep contextual cross-lingual representations (Eriguchi et al., 2017; Lample and Conneau, 2019; Wu and Dredze, 2019b; Siddhant et al., 2020; Pires et al., 2019; Pfeiffer et al., 2020). We extend prior work to incorporate visual context. Video-text representations. The HowTo100M dataset (Miech et al., 2019) has attracted significant interest in leveraging multimodal pre-training for text→video search (Korbar et al., 2020), captioning (Iashin and Rahtu, 2020), and unsupervised translation via image-based (Sur´ıs et al., 2020; Huang et al., 2020b) and video-based (Sigurdsson et al., 2020) alignment. This work studies a challenging and unexplored task: Zero-shot c"
2021.naacl-main.195,E14-1049,0,0.036224,"modal pre-training strategy and construct a new Multi-HowTo100M dataset (§4) for pre-training to improve zero-shot cross-lingual capability of vision-language models. (4) We demonstrate the effectiveness of our approach, by achieving state-of-the-art multilingual text→video search performance in both the zero-shot (§5.5) and fully supervised setup (§5.6). 2 Related Work Cross-lingual representations. Early work on learning non-contextual cross-lingual representations used either parallel corpora (Gouws and Søgaard, 2015; Luong et al., 2015) or a bilingual dictionary to learn a transformation (Faruqui and Dyer, 2014; Mikolov et al., 2013). Later approaches reduced the amount of supervision using self-training (Artetxe et al., 2017). With the advances in monolingual transfer learning (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019), multilingual extensions of pre-trained encoders have been proven effective in learning deep contextual cross-lingual representations (Eriguchi et al., 2017; Lample and Conneau, 2019; Wu and Dredze, 2019b; Siddhant et al., 2020; Pires et al., 2019; Pfeiffer et al., 2020). We extend prior work to incorporate visual context. Video-text repre"
2021.naacl-main.195,D17-1303,0,0.145199,"a batch of multilingual text-video samples as B = {(xi , vi )}B i=1 } where B ⊂ C. 3.1 Multilingual Multimodal Transformers Figure 1 gives an overview of the proposed method. Our text encoder consists of a multilingual Transformer (e.g. multilingual BERT (Devlin et al., 2019)) and a text Transformer pooling head (explained below). Similarly, our video encoder consists of a 3D-CNN (e.g. R(2+1)D network (Tran et al., 2018)) and a video Transformer pooling head. We use these multilingual multimodal Transformers to encode text and video for alignment. Unlike prior multilingual text-image models (Gella et al., 2017; Kim et al., 2020; Huang et al., 2019b) that utilize word embeddings and RNNs, our multilingual text encoder is built on a multilingual Transformer that generates contextual multilingual representations ex ∈ RN ×D to encode a sentence x containing N words. We employ an additional 2-layer Transformer which we will call a “Transformer pooling head (TP)” as it serves as a pooling function to selectively encode variablelength sentences and aligns them with the corresponding visual content. We use the first output token of the second Transformer layer as the final sentence representation. Precisel"
2021.naacl-main.195,C12-1089,0,0.0220555,"only work in English, but in all of the world’s approximately 7,000 languages. Since collecting and annotating task-specific parallel multimodal data in all languages is impractical, a framework that makes vision-language models generalize across languages is highly desirable. One technique that has shown promise to greatly improve the applicability of NLP models to new languages is zero-shot cross-lingual transfer, where models trained on a source language are applied ∗ Equal contribution. as-is to a different language without any additional annotated training data (T¨ackstr¨om et al., 2012; Klementiev et al., 2012; Cotterell and Heigold, 2017; Chen et al., 2018; Neubig and Hu, 2018). In particular, recent techniques for cross-lingual transfer have demonstrated that by performing unsupervised learning of language or translation models on many languages, followed by downstream task fine-tuning using only English annotation, models can nonetheless generalize to a non-English language (Wu and Dredze, 2019a; Lample and Conneau, 2019; Huang et al., 2019a; Artetxe et al., 2020; Hu et al., 2020). This success is attributed to the fact that many languages share a considerable amount of underlying vocabulary or"
2021.naacl-main.195,2020.acl-main.730,0,0.0417023,"al multimodal pretraining (MMP) strategy, the Multi-HowTo100M dataset, and a Transformer-based text-video model for learning contextual multilingual multimodal representations. The results in this paper have convincingly demonstrated that MMP is an essential ingredient for zero-shot cross-lingual transfer of vision-language models. Meanwhile, there are many remaining challenges, such as resolving the performance gap between zero-shot and training with in-domain non-English annotations; as well as techniques to transfer varieties of vision-language models (e.g., VQA (Goyal et al., 2017), TVQA (Lei et al., 2020)) or visually-enhanced NLP models such as unsupervised multimodal machine translation (Huang et al., 2020b). We believe the proposed methodology, and the corresponding resources we release, will be an important first step towards spurring more research in this direction. Acknowledgments This work is supported by the DARPA grants funded under the AIDA program (FA875018-2-0018) and the GAILA program (award HR00111990063) (P.Y.). This work is also supported by EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines & Systems [EP/L015897/1] (M.P.). The authors appreciate Prahal Arora"
2021.naacl-main.195,2020.acl-main.653,0,0.0383838,"Attention Attention Attention Attention Attention Attention Attention Attention Attention Attention Attention Attention 1D-CNN 1D-CNN 1D-CNN … a Attention Attention Attention Attention man shotGRU put a ?!|# ?# Intra-modal ?# 3D ?CNN time Time CNN Figure GRU 1: The proposed video-text model for learning contextual multilingual multimodal representations. We utilize intra-modal, inter-modal, and conditional cross-lingual contrastive objectives to align (x, v, y) where x GRU and y are the captions or transcriptions in different languages of a video v. TP: Transformer pooling head. Time swering (Lewis et al., 2020; Artetxe et al., 2020). Recently, XTREME (Hu et al., 2020) was proposed to evaluate the cross-lingual transfer capabilities of multilingual representations across a diverse set of NLP tasks and languages. However, a comprehensive evaluation of multilingual multimodal models on zero-shot cross-lingual transfer capabilities is still missing. To our best knowledge, we are the first work that investigates and improves zero-shot cross-lingual transfer of vision-language models. 3 Method We consider the problem of learning multilingual multimodal representations from a corpus C of video-text pairs"
2021.naacl-main.195,D15-1166,0,0.0185562,"lingual transferrability. (§5.5). (3) We introduce the multilingual multimodal pre-training strategy and construct a new Multi-HowTo100M dataset (§4) for pre-training to improve zero-shot cross-lingual capability of vision-language models. (4) We demonstrate the effectiveness of our approach, by achieving state-of-the-art multilingual text→video search performance in both the zero-shot (§5.5) and fully supervised setup (§5.6). 2 Related Work Cross-lingual representations. Early work on learning non-contextual cross-lingual representations used either parallel corpora (Gouws and Søgaard, 2015; Luong et al., 2015) or a bilingual dictionary to learn a transformation (Faruqui and Dyer, 2014; Mikolov et al., 2013). Later approaches reduced the amount of supervision using self-training (Artetxe et al., 2017). With the advances in monolingual transfer learning (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019), multilingual extensions of pre-trained encoders have been proven effective in learning deep contextual cross-lingual representations (Eriguchi et al., 2017; Lample and Conneau, 2019; Wu and Dredze, 2019b; Siddhant et al., 2020; Pires et al., 2019; Pfeiffer et al.,"
2021.naacl-main.195,N15-1015,0,0.0644859,"Missing"
2021.naacl-main.195,N18-1202,0,0.0526834,"-of-the-art multilingual text→video search performance in both the zero-shot (§5.5) and fully supervised setup (§5.6). 2 Related Work Cross-lingual representations. Early work on learning non-contextual cross-lingual representations used either parallel corpora (Gouws and Søgaard, 2015; Luong et al., 2015) or a bilingual dictionary to learn a transformation (Faruqui and Dyer, 2014; Mikolov et al., 2013). Later approaches reduced the amount of supervision using self-training (Artetxe et al., 2017). With the advances in monolingual transfer learning (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019), multilingual extensions of pre-trained encoders have been proven effective in learning deep contextual cross-lingual representations (Eriguchi et al., 2017; Lample and Conneau, 2019; Wu and Dredze, 2019b; Siddhant et al., 2020; Pires et al., 2019; Pfeiffer et al., 2020). We extend prior work to incorporate visual context. Video-text representations. The HowTo100M dataset (Miech et al., 2019) has attracted significant interest in leveraging multimodal pre-training for text→video search (Korbar et al., 2020), captioning (Iashin and Rahtu, 2020), and unsupervised translati"
2021.naacl-main.195,2020.emnlp-main.617,0,0.0612243,"Missing"
2021.naacl-main.195,P19-1493,0,0.0233629,"d Søgaard, 2015; Luong et al., 2015) or a bilingual dictionary to learn a transformation (Faruqui and Dyer, 2014; Mikolov et al., 2013). Later approaches reduced the amount of supervision using self-training (Artetxe et al., 2017). With the advances in monolingual transfer learning (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019), multilingual extensions of pre-trained encoders have been proven effective in learning deep contextual cross-lingual representations (Eriguchi et al., 2017; Lample and Conneau, 2019; Wu and Dredze, 2019b; Siddhant et al., 2020; Pires et al., 2019; Pfeiffer et al., 2020). We extend prior work to incorporate visual context. Video-text representations. The HowTo100M dataset (Miech et al., 2019) has attracted significant interest in leveraging multimodal pre-training for text→video search (Korbar et al., 2020), captioning (Iashin and Rahtu, 2020), and unsupervised translation via image-based (Sur´ıs et al., 2020; Huang et al., 2020b) and video-based (Sigurdsson et al., 2020) alignment. This work studies a challenging and unexplored task: Zero-shot cross-lingual transfer of vision-language models. Unlike prior image/video-text work that ut"
2021.naacl-main.195,P19-1015,0,0.0251398,"al., 2019; Chen et al., 2020a; Burns et al., 2020; Kim et al., 2020) and inter-modal contrastive objectives (Sigurdsson et al., 2020; Liu et al., 2019; Huang et al., 2019b; Patrick et al., 2021), we employ Transformers to learn contextual multilingual multimodal representations and uniquely models cross-lingual instances. Moreover, we build Multi-HowTo100M, the largest text-video dataset for multilingual multimodal pre-training. Cross-lingual Transfer. Cross-lingual transfer has proven effective in many NLP tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2019), document classification (Schwenk and Li, 2018), and question an2444 a GRU a man performs TP 3D-CNN ?# 3D Contrastive repulsion Transformer Pooling (TP) 3D CNN Attention Attention Attention Attention Attention Attention Attention Attention Linear 1D-CNN 1D-CNN 1D-CNN Attention Attention Attention Attention put Inter-modal Linear 1D-CNN 1D-CNN 1D-CNN … … shot GRU ?&quot; Ψ ? Transformer Pooling (TP) 1D-CNN 1D-CNN 1D-CNN Attention Attention Attention Attention Cross-lingual mBERT TP lanzamiento de bala Φ ? man GRU ?&quot; ? put GRU shot GRU Contrastive attraction"
2021.naacl-main.195,D19-1077,0,0.0425277,"Missing"
2021.naacl-main.195,Q14-1006,0,0.0143586,"col in Miech et al. (2019); Liu et al. (2019) which evaluates on text→video search with the 1K testing set defined by Yu et al. (2018). VATEX (Wang et al., 2019) is a multilingual (Chinese and English) video-text dataset with 35K videos. Five (en,zh) translation pairs and five nonpaired en and zh descriptions are available for each video. We use the official training split (26K videos) and follow the testing protocol in Chen et al. (2020a) to split the validation set equally into 1.5K validation and 1.5K testing videos. Multi30K (Elliott et al., 2016) is a multilingual extension of Flickr30K (Young et al., 2014). For each image, there are two types of annotations available: (1) One parallel (English,German,French,Czech) translation pair and (2) five English and five Ger2447 1 https://marian-nmt.github.io/ Text-B Video-B R@1↑ XLM-R S3D 19.5 XLM-R R(2+1)D 19.0 XLM-R R+S 21.0 mBERT R+S 19.9 man descriptions collected independently. The training, validation, and testing splits contain 29K, 1K, and 1K images respectively. 5.2 Implementation Details For the video backbone, we use a 34-layer, R(2+1)-D (Tran et al., 2018) network pre-trained on IG65M (Ghadiyaram et al., 2019) and a S3D (Miech et al., 2020) n"
2021.naacl-main.225,P13-2009,1,0.790848,"the aligned utterance tokens in Tzs (e.g., Block 1, Fig. 1a). In the second variant (lines 8-9), we find internally contiguous utterance spans (subsequences) in Tzs and align them to z s . For instance, the sub-program (?x1 art_directed M1) in Block 2 of Fig. 1b aligns to two utterance spans: M1 ’s and art director. While this case does not have an exact analog in MT, it is reminiscent of the model of Chiang (2005) which extracts translation rules with discontinuous phrase segments, and could be useful in capturing long-range alignments of utterance subsequences to sub-programs as in Block 2 (Andreas et al., 2013). Span-level alignments for a sub-program are then generated by pairing its program spans zp:q (spans with consecutive program tokens) with all its aligned utterance spans (lines 11-12). Finally, we generate alignments for sketch spans in z by pairing them with any utterance tokens that have not yet been aligned to a sub-program (lines 13-14). Algo. 1 leverages the explicit hierarchical structures of programs to generate alignments between sub-programs and utterance spans. Such an idea of using structural information for alignment extraction has deep roots in statistical syntax-based MT, which"
2021.naacl-main.225,D16-1162,1,0.824538,"n data transformations and model architectures, the de1 Introduction sign of loss functions to encourage compositional Semantic parsers translate natural language ut- generalization has been under-explored. This paterances (e.g., Schedule a meeting with Jean) per investigates attention supervision losses that encourage attention matrices in neural sequence into executable programs (e.g., CreateEvent( attendees=Jean)), and play a crucial role in ap- models to resemble the output of word alignment algorithms (Liu et al. (2016); Mi et al. (2016); plications such as question answering systems and Arthur et al. (2016); Lyu and Titov (2018), inter conversational agents (Liang, 2016; Gupta et al., alia) as a source of inductive bias for composi2018; Wen et al., 2017). As in many language understanding problems, a central challenge in se- tional tasks. Previous work has found that aligning program tokens (e.g., FindManager in Fig. 1) mantic parsing is compositional generalization (Finegan-Dollak et al., 2018; Keysers et al., 2020). to natural language tokens (manager) improves Consider a personal digital assistant for which de- model performance (Misra et al., 2018; Rabinovich et al., 2017; Goldman et al., 20"
2021.naacl-main.225,J93-2003,0,0.157172,"Missing"
2021.naacl-main.225,P05-1033,0,0.318552,"aset (§3). In the first case (lines 5-6), similar to bilingual phrase extraction in machine translation (MT; Och, 2002), we create a single consecutive utterance span um:n via the outer bound of the aligned utterance tokens in Tzs (e.g., Block 1, Fig. 1a). In the second variant (lines 8-9), we find internally contiguous utterance spans (subsequences) in Tzs and align them to z s . For instance, the sub-program (?x1 art_directed M1) in Block 2 of Fig. 1b aligns to two utterance spans: M1 ’s and art director. While this case does not have an exact analog in MT, it is reminiscent of the model of Chiang (2005) which extracts translation rules with discontinuous phrase segments, and could be useful in capturing long-range alignments of utterance subsequences to sub-programs as in Block 2 (Andreas et al., 2013). Span-level alignments for a sub-program are then generated by pairing its program spans zp:q (spans with consecutive program tokens) with all its aligned utterance spans (lines 11-12). Finally, we generate alignments for sketch spans in z by pairing them with any utterance tokens that have not yet been aligned to a sub-program (lines 13-14). Algo. 1 leverages the explicit hierarchical structu"
2021.naacl-main.225,P16-1004,0,0.0455513,"zation in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization. parsers (Dong and Lapata, 2016; Yin and Neubig, 2017), tend to perform poorly at out-of-distribution generalization of this kind (Lake and Baroni, 2018; Furrer et al., 2020; Suhr et al., 2020). Methods have been proposed to bridge the generalization gap using meta-learning (Lake, 2019; Wang et al., 2020) or specialized model architectures (Russin et al., 2019; Li et al., 2019; Liu et al., 2020; Chen et al., 2020). These have registered impressive performance on small synthetic benchmark datasets, but it has proven difficult to effectively combine them with large-scale pre-training (Lewis et al., 2020; Raffel et al., 2020)"
2021.naacl-main.225,P18-1068,0,0.0166598,"pervised Attention Neural Semantic Parsers A semantic parser maps a natural language (NL) utterance u to an executable program z. In this paper, we consider neural parsers using token-based attentive decoders, in which z is predicted as a sequence of consecu|z| tive tokens {zj=1 } by attending to tokens in u = |u| {ui=1 }. Examples include sequence-to-sequence models based on recurrent networks (Dong and Lapata, 2016; Jia and Liang, 2016) or transformers (Vaswani et al., 2017; Raffel et al., 2020), as well as structured parsing methods that predict a program following its syntactic structure (Dong and Lapata (2018), see §3 for more details). Bangkok 1st is aligned to the j-th target (program) token zj . A|u|×|z |can be inferred using latent variable models (Brown et al., 1993; Och and Ney, 2003; Dyer et al., 2013). During training, when the decoder predicts a target token zj , supervised attention encourages the target-to-source attention distribution patt (ui |zj ) to match the prior alignment distribua tion pprior (ui |zj ) = P i,j , which is normalized k ak,j by the number of source tokens aligned to zj . We use a squared error loss (Liu et al., 2016): Lsup_att = |z ||u| 2 1 XX patt (ui |zj )−pprior"
2021.naacl-main.225,N13-1073,0,0.0448885,"s, in which z is predicted as a sequence of consecu|z| tive tokens {zj=1 } by attending to tokens in u = |u| {ui=1 }. Examples include sequence-to-sequence models based on recurrent networks (Dong and Lapata, 2016; Jia and Liang, 2016) or transformers (Vaswani et al., 2017; Raffel et al., 2020), as well as structured parsing methods that predict a program following its syntactic structure (Dong and Lapata (2018), see §3 for more details). Bangkok 1st is aligned to the j-th target (program) token zj . A|u|×|z |can be inferred using latent variable models (Brown et al., 1993; Och and Ney, 2003; Dyer et al., 2013). During training, when the decoder predicts a target token zj , supervised attention encourages the target-to-source attention distribution patt (ui |zj ) to match the prior alignment distribua tion pprior (ui |zj ) = P i,j , which is normalized k ak,j by the number of source tokens aligned to zj . We use a squared error loss (Liu et al., 2016): Lsup_att = |z ||u| 2 1 XX patt (ui |zj )−pprior (ui |zj ) . (1) |z |j=1 i=1 Previous work has also used a cross entropy loss (Rabinovich et al., 2017; Oren et al., 2020). Sub-program-to-Span Alignment We present a simple heuristic algorithm to extrac"
2021.naacl-main.225,P18-1033,0,0.0116855,"., CreateEvent( attendees=Jean)), and play a crucial role in ap- models to resemble the output of word alignment algorithms (Liu et al. (2016); Mi et al. (2016); plications such as question answering systems and Arthur et al. (2016); Lyu and Titov (2018), inter conversational agents (Liang, 2016; Gupta et al., alia) as a source of inductive bias for composi2018; Wen et al., 2017). As in many language understanding problems, a central challenge in se- tional tasks. Previous work has found that aligning program tokens (e.g., FindManager in Fig. 1) mantic parsing is compositional generalization (Finegan-Dollak et al., 2018; Keysers et al., 2020). to natural language tokens (manager) improves Consider a personal digital assistant for which de- model performance (Misra et al., 2018; Rabinovich et al., 2017; Goldman et al., 2018; Richardson et al., velopers have assembled separate collections of 2018; Herzig and Berant, 2020; Oren et al., 2020). annotated utterances for user requests involving their calendars (e.g., Schedule a meeting with Jean) However, the token-level alignments derived from and their contact books (e.g., Who is Jean’s man- off-the-shelf aligners are often noisy, and the correspondence between n"
2021.naacl-main.225,N04-1035,0,0.271193,"cutive program tokens) with all its aligned utterance spans (lines 11-12). Finally, we generate alignments for sketch spans in z by pairing them with any utterance tokens that have not yet been aligned to a sub-program (lines 13-14). Algo. 1 leverages the explicit hierarchical structures of programs to generate alignments between sub-programs and utterance spans. Such an idea of using structural information for alignment extraction has deep roots in statistical syntax-based MT, which leverages the syntactic structure of sentences to generate alignments between parse trees and NL constituents (Galley et al., 2004; Chiang, 2005; Liu et al., 2006). Our approach is also broadly related to lexicon induction models in semantic parsers based on probabilistic CCG grammars (Kwiatkowski et al., 2011) or other formalisms (Jones et al., 2012), which learn mapping rules between logical form templates and utterance tokens. 3 Experiments We evaluate span-level supervised attention on three benchmarks of compositional generalization. SMC AL F LOW Compositional Skills (SMC AL F LOW-CS) is a new dataset created in this study based on the task-oriented dialogue corpus SMC AL F LOW (Semantic Machines et al., 2020), feat"
2021.naacl-main.225,P18-1168,0,0.0119956,"thur et al. (2016); Lyu and Titov (2018), inter conversational agents (Liang, 2016; Gupta et al., alia) as a source of inductive bias for composi2018; Wen et al., 2017). As in many language understanding problems, a central challenge in se- tional tasks. Previous work has found that aligning program tokens (e.g., FindManager in Fig. 1) mantic parsing is compositional generalization (Finegan-Dollak et al., 2018; Keysers et al., 2020). to natural language tokens (manager) improves Consider a personal digital assistant for which de- model performance (Misra et al., 2018; Rabinovich et al., 2017; Goldman et al., 2018; Richardson et al., velopers have assembled separate collections of 2018; Herzig and Berant, 2020; Oren et al., 2020). annotated utterances for user requests involving their calendars (e.g., Schedule a meeting with Jean) However, the token-level alignments derived from and their contact books (e.g., Who is Jean’s man- off-the-shelf aligners are often noisy, and the correspondence between natural language and program ager?). An effective model should learn from this data how to additionally handle requests like Sched- tokens is not always a many-to-one map of the kind returned by standard alig"
2021.naacl-main.225,D18-1300,0,0.0646605,"Missing"
2021.naacl-main.225,P16-1002,0,0.0163993,"tently improves over token-level objectives, achieving strong results on three semantic parsing datasets featuring diverse formalisms and tests of generalization. 2 Span-level Supervised Attention Neural Semantic Parsers A semantic parser maps a natural language (NL) utterance u to an executable program z. In this paper, we consider neural parsers using token-based attentive decoders, in which z is predicted as a sequence of consecu|z| tive tokens {zj=1 } by attending to tokens in u = |u| {ui=1 }. Examples include sequence-to-sequence models based on recurrent networks (Dong and Lapata, 2016; Jia and Liang, 2016) or transformers (Vaswani et al., 2017; Raffel et al., 2020), as well as structured parsing methods that predict a program following its syntactic structure (Dong and Lapata (2018), see §3 for more details). Bangkok 1st is aligned to the j-th target (program) token zj . A|u|×|z |can be inferred using latent variable models (Brown et al., 1993; Och and Ney, 2003; Dyer et al., 2013). During training, when the decoder predicts a target token zj , supervised attention encourages the target-to-source attention distribution patt (ui |zj ) to match the prior alignment distribua tion pprior (ui |zj )"
2021.naacl-main.225,P12-1051,0,0.0348018,"es 13-14). Algo. 1 leverages the explicit hierarchical structures of programs to generate alignments between sub-programs and utterance spans. Such an idea of using structural information for alignment extraction has deep roots in statistical syntax-based MT, which leverages the syntactic structure of sentences to generate alignments between parse trees and NL constituents (Galley et al., 2004; Chiang, 2005; Liu et al., 2006). Our approach is also broadly related to lexicon induction models in semantic parsers based on probabilistic CCG grammars (Kwiatkowski et al., 2011) or other formalisms (Jones et al., 2012), which learn mapping rules between logical form templates and utterance tokens. 3 Experiments We evaluate span-level supervised attention on three benchmarks of compositional generalization. SMC AL F LOW Compositional Skills (SMC AL F LOW-CS) is a new dataset created in this study based on the task-oriented dialogue corpus SMC AL F LOW (Semantic Machines et al., 2020), featuring real-world human-generated utterances about calendar management. Like the motivating story in §1, we create training data for skills S involving event creation (e.g., Schedule a meeting with Adam) and organization str"
2021.naacl-main.225,D11-1140,0,0.0454445,"have not yet been aligned to a sub-program (lines 13-14). Algo. 1 leverages the explicit hierarchical structures of programs to generate alignments between sub-programs and utterance spans. Such an idea of using structural information for alignment extraction has deep roots in statistical syntax-based MT, which leverages the syntactic structure of sentences to generate alignments between parse trees and NL constituents (Galley et al., 2004; Chiang, 2005; Liu et al., 2006). Our approach is also broadly related to lexicon induction models in semantic parsers based on probabilistic CCG grammars (Kwiatkowski et al., 2011) or other formalisms (Jones et al., 2012), which learn mapping rules between logical form templates and utterance tokens. 3 Experiments We evaluate span-level supervised attention on three benchmarks of compositional generalization. SMC AL F LOW Compositional Skills (SMC AL F LOW-CS) is a new dataset created in this study based on the task-oriented dialogue corpus SMC AL F LOW (Semantic Machines et al., 2020), featuring real-world human-generated utterances about calendar management. Like the motivating story in §1, we create training data for skills S involving event creation (e.g., Schedule"
2021.naacl-main.225,2020.acl-main.703,0,0.0287333,"alization. parsers (Dong and Lapata, 2016; Yin and Neubig, 2017), tend to perform poorly at out-of-distribution generalization of this kind (Lake and Baroni, 2018; Furrer et al., 2020; Suhr et al., 2020). Methods have been proposed to bridge the generalization gap using meta-learning (Lake, 2019; Wang et al., 2020) or specialized model architectures (Russin et al., 2019; Li et al., 2019; Liu et al., 2020; Chen et al., 2020). These have registered impressive performance on small synthetic benchmark datasets, but it has proven difficult to effectively combine them with large-scale pre-training (Lewis et al., 2020; Raffel et al., 2020) and natural data (Furrer et al., 2020). In contrast to this extensive literature on data transformations and model architectures, the de1 Introduction sign of loss functions to encourage compositional Semantic parsers translate natural language ut- generalization has been under-explored. This paterances (e.g., Schedule a meeting with Jean) per investigates attention supervision losses that encourage attention matrices in neural sequence into executable programs (e.g., CreateEvent( attendees=Jean)), and play a crucial role in ap- models to resemble the output of word alig"
2021.naacl-main.225,D19-1438,0,0.0280134,"s to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization. parsers (Dong and Lapata, 2016; Yin and Neubig, 2017), tend to perform poorly at out-of-distribution generalization of this kind (Lake and Baroni, 2018; Furrer et al., 2020; Suhr et al., 2020). Methods have been proposed to bridge the generalization gap using meta-learning (Lake, 2019; Wang et al., 2020) or specialized model architectures (Russin et al., 2019; Li et al., 2019; Liu et al., 2020; Chen et al., 2020). These have registered impressive performance on small synthetic benchmark datasets, but it has proven difficult to effectively combine them with large-scale pre-training (Lewis et al., 2020; Raffel et al., 2020) and natural data (Furrer et al., 2020). In contrast to this extensive literature on data transformations and model architectures, the de1 Introduction sign of loss functions to encourage compositional Semantic parsers translate natural language ut- generalization has been under-explored. This paterances (e.g., Schedule a meeting with Jean) per in"
2021.naacl-main.225,C16-1291,0,0.0540971,"020) and natural data (Furrer et al., 2020). In contrast to this extensive literature on data transformations and model architectures, the de1 Introduction sign of loss functions to encourage compositional Semantic parsers translate natural language ut- generalization has been under-explored. This paterances (e.g., Schedule a meeting with Jean) per investigates attention supervision losses that encourage attention matrices in neural sequence into executable programs (e.g., CreateEvent( attendees=Jean)), and play a crucial role in ap- models to resemble the output of word alignment algorithms (Liu et al. (2016); Mi et al. (2016); plications such as question answering systems and Arthur et al. (2016); Lyu and Titov (2018), inter conversational agents (Liang, 2016; Gupta et al., alia) as a source of inductive bias for composi2018; Wen et al., 2017). As in many language understanding problems, a central challenge in se- tional tasks. Previous work has found that aligning program tokens (e.g., FindManager in Fig. 1) mantic parsing is compositional generalization (Finegan-Dollak et al., 2018; Keysers et al., 2020). to natural language tokens (manager) improves Consider a personal digital assistant for wh"
2021.naacl-main.225,P06-1077,0,0.12014,"aligned utterance spans (lines 11-12). Finally, we generate alignments for sketch spans in z by pairing them with any utterance tokens that have not yet been aligned to a sub-program (lines 13-14). Algo. 1 leverages the explicit hierarchical structures of programs to generate alignments between sub-programs and utterance spans. Such an idea of using structural information for alignment extraction has deep roots in statistical syntax-based MT, which leverages the syntactic structure of sentences to generate alignments between parse trees and NL constituents (Galley et al., 2004; Chiang, 2005; Liu et al., 2006). Our approach is also broadly related to lexicon induction models in semantic parsers based on probabilistic CCG grammars (Kwiatkowski et al., 2011) or other formalisms (Jones et al., 2012), which learn mapping rules between logical form templates and utterance tokens. 3 Experiments We evaluate span-level supervised attention on three benchmarks of compositional generalization. SMC AL F LOW Compositional Skills (SMC AL F LOW-CS) is a new dataset created in this study based on the task-oriented dialogue corpus SMC AL F LOW (Semantic Machines et al., 2020), featuring real-world human-generated"
2021.naacl-main.225,D15-1166,0,0.0715009,"Missing"
2021.naacl-main.225,P18-1037,0,0.0192076,"and model architectures, the de1 Introduction sign of loss functions to encourage compositional Semantic parsers translate natural language ut- generalization has been under-explored. This paterances (e.g., Schedule a meeting with Jean) per investigates attention supervision losses that encourage attention matrices in neural sequence into executable programs (e.g., CreateEvent( attendees=Jean)), and play a crucial role in ap- models to resemble the output of word alignment algorithms (Liu et al. (2016); Mi et al. (2016); plications such as question answering systems and Arthur et al. (2016); Lyu and Titov (2018), inter conversational agents (Liang, 2016; Gupta et al., alia) as a source of inductive bias for composi2018; Wen et al., 2017). As in many language understanding problems, a central challenge in se- tional tasks. Previous work has found that aligning program tokens (e.g., FindManager in Fig. 1) mantic parsing is compositional generalization (Finegan-Dollak et al., 2018; Keysers et al., 2020). to natural language tokens (manager) improves Consider a personal digital assistant for which de- model performance (Misra et al., 2018; Rabinovich et al., 2017; Goldman et al., 2018; Richardson et al.,"
2021.naacl-main.225,E17-1042,0,0.0203363,"Missing"
2021.naacl-main.225,P17-1105,0,0.161585,"answering systems and Arthur et al. (2016); Lyu and Titov (2018), inter conversational agents (Liang, 2016; Gupta et al., alia) as a source of inductive bias for composi2018; Wen et al., 2017). As in many language understanding problems, a central challenge in se- tional tasks. Previous work has found that aligning program tokens (e.g., FindManager in Fig. 1) mantic parsing is compositional generalization (Finegan-Dollak et al., 2018; Keysers et al., 2020). to natural language tokens (manager) improves Consider a personal digital assistant for which de- model performance (Misra et al., 2018; Rabinovich et al., 2017; Goldman et al., 2018; Richardson et al., velopers have assembled separate collections of 2018; Herzig and Berant, 2020; Oren et al., 2020). annotated utterances for user requests involving their calendars (e.g., Schedule a meeting with Jean) However, the token-level alignments derived from and their contact books (e.g., Who is Jean’s man- off-the-shelf aligners are often noisy, and the correspondence between natural language and program ager?). An effective model should learn from this data how to additionally handle requests like Sched- tokens is not always a many-to-one map of the kind ret"
2021.naacl-main.225,P17-1041,1,0.835763,"ers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization. parsers (Dong and Lapata, 2016; Yin and Neubig, 2017), tend to perform poorly at out-of-distribution generalization of this kind (Lake and Baroni, 2018; Furrer et al., 2020; Suhr et al., 2020). Methods have been proposed to bridge the generalization gap using meta-learning (Lake, 2019; Wang et al., 2020) or specialized model architectures (Russin et al., 2019; Li et al., 2019; Liu et al., 2020; Chen et al., 2020). These have registered impressive performance on small synthetic benchmark datasets, but it has proven difficult to effectively combine them with large-scale pre-training (Lewis et al., 2020; Raffel et al., 2020) and natural data (Furre"
2021.naacl-main.225,N18-1066,0,0.0326748,"Missing"
2021.naacl-main.225,P17-1099,0,0.107671,"Missing"
2021.naacl-main.284,W18-6318,0,0.0218824,"objectives in learning better cross-lingual representations for downstream tasks. Nonetheless, several challenging and promising directions can be considered in the future. First, most existing multilingual models tokenize words into subword units, which makes the alignment less interpretable. How to align a span of subword units with meaningful semantics at the phrase level deserves further investigation. Second, several studies (Ghader and Monz, 2017; Li et al., 2019) have shown that attention may fail to capture word alignment for some language pairs, and a few works (Legrand et al., 2016; Alkhouli et al., 2018) proposed neural word alignment to improve the word alignment quality. Incorporating such recent advances into the alignment objective is one future direction. Third, how to fine-tune a well-aligned multilingual model on English annotations without catastrophic forgetting of the alignment information is a potential way to improve cross-lingual generalization on the downstream applications. 4 Acknowledgements Related Work While cross-lingual alignment is a long-standing We’d like to thank Yinfei Yang and Wei-Cheng challenge dating back to the early stage of re- Chang for answering our questions"
2021.naacl-main.284,2020.acl-main.421,0,0.0622819,"Missing"
2021.naacl-main.284,Q19-1038,0,0.0199302,"t 1 week. 3.2 Datasets Cross-lingual Part-Of-Speech (POS) contains data in 13 languages from the Universal Dependencies v2.3 (Nivre et al., 2018). 3 https://github.com/google-research/ bert 3635 PAWS-X (Yang et al., 2019b) is a paraphrase detection dataset. We train on the English data (Zhang et al., 2019), and evaluate the prediction accuracy on the test set translated into 4 other languages. XNLI (Conneau et al., 2018) is a natural language inference dataset in 15 languages. We train models on the English MultiNLI training data (Williams et al., 2018), and evaluate on the other 14. Tatoeba (Artetxe and Schwenk, 2019) is a testbed for parallel sentence identification. We select the 14 non-English languages covered by our parallel data, and follow the setup in Hu et al. (2020) finding the English translation for a given a non-English sentence with maximum cosine similarity. 3.3 Result Analysis Model POS PAWS-X XNLI Tatoeba mBERT (public) XLM-15 XLM-100 XLM-R-base XLM-R-large Unicoder 68.5 68.8 69.5 68.8 70.0 71.7 86.2 88.0 86.4 87.4 89.4 88.1 65.4 72.6 69.1 73.4 79.2 74.8 45.6 77.2 36.6 57.6 60.6 72.2 (MLM) (MLM+TLM) AMBER (MLM+TLM+WA) AMBER (MLM+TLM+WA+SA) 69.8 70.5 71.1 70.5 87.1 87.7 89.0 89.2 67.7 70.9"
2021.naacl-main.284,J93-2003,0,0.244134,"Missing"
2021.naacl-main.284,N16-1102,0,0.0630932,"training regimen for learning contextualized word representations 1 Introduction that encourages symmetry at both the word and Cross-lingual embeddings, both traditional non- sentence levels at training time. Our word-level contextualized word embeddings (Faruqui and alignment objective is inspired by work in machine Dyer, 2014) and the more recent contextualized translation that defines objectives encouraging conword embeddings (Devlin et al., 2019), are an es- sistency between the source-to-target and targetsential tool for cross-lingual transfer in downstream to-source attention matrices (Cohn et al., 2016). applications. In particular, multilingual contextu- Our sentence-level alignment objective encourages alized word representations have proven effective prediction of the correct translations within a miniin reducing the amount of supervision needed in batch for a given source sentence, which is inspired a variety of cross-lingual NLP tasks such as se- by work on learning multilingual sentence reprequence labeling (Pires et al., 2019), question an- sentations (Yang et al., 2019a; Wieting et al., 2019). swering (Artetxe et al., 2020), parsing (Wang et al., In experiments, we evaluate the zero-"
2021.naacl-main.284,2020.acl-main.747,0,0.0415604,"71.4 75.5 74.5 74.7 84.7 74.3 74.2 72.5 76.9 76.6 76.5 AMBER (full) Table 3: F1 scores of AMBER trained with all objectives and Cao et al. (2020) on 6 languages on XNLI. 5 10 4 8 3 6 ΔAccuracy ΔF1 2 1 0 4 2 -1 0 .4) .7) .7) 1.6) .2) .5) tr(0 ur(0 hi( es(11 ru(11 fr(13 (a) Part-of-Speech (0. sw 2) .4) .6) .7) .2) .5) tr(0 bg(0 es(11 ru(11 fr(13 (b) XNLI Figure 2: Performance difference between AMBER trained with alignments on parallel data and AMBER (MLM). Languages are sorted by no. of parallel data (Million) used for training AMBER with alignments. 3.5 Xing et al., 2015; Devlin et al., 2019; Conneau et al., 2020a) are highly promising in their easy integration into neural network models for a variety of cross-lingual applications. Analysis studies on recent cross-lingual contextualized representations (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Siddhant et al., 2020) further demonstrates this advantage for zero-shot cross-lingual transfer in a representative set of languages and tasks. In particular to improve cross-lingual transfer, some attempts directly leverage multilingual parallel corpus to train contextualized representations (McCann et al., 2017; Eriguchi et al., 2018; Conneau"
2021.naacl-main.284,D18-1269,0,0.0248366,"of subwords in the concatenation of each sentence pair to 256 and use 10k warmup steps with the peak learning rate of 1e-4 and a linear decay of the learning rate. We train AMBER on TPU v3 for about 1 week. 3.2 Datasets Cross-lingual Part-Of-Speech (POS) contains data in 13 languages from the Universal Dependencies v2.3 (Nivre et al., 2018). 3 https://github.com/google-research/ bert 3635 PAWS-X (Yang et al., 2019b) is a paraphrase detection dataset. We train on the English data (Zhang et al., 2019), and evaluate the prediction accuracy on the test set translated into 4 other languages. XNLI (Conneau et al., 2018) is a natural language inference dataset in 15 languages. We train models on the English MultiNLI training data (Williams et al., 2018), and evaluate on the other 14. Tatoeba (Artetxe and Schwenk, 2019) is a testbed for parallel sentence identification. We select the 14 non-English languages covered by our parallel data, and follow the setup in Hu et al. (2020) finding the English translation for a given a non-English sentence with maximum cosine similarity. 3.3 Result Analysis Model POS PAWS-X XNLI Tatoeba mBERT (public) XLM-15 XLM-100 XLM-R-base XLM-R-large Unicoder 68.5 68.8 69.5 68.8 70.0"
2021.naacl-main.284,2020.acl-main.536,0,0.0323794,"71.4 75.5 74.5 74.7 84.7 74.3 74.2 72.5 76.9 76.6 76.5 AMBER (full) Table 3: F1 scores of AMBER trained with all objectives and Cao et al. (2020) on 6 languages on XNLI. 5 10 4 8 3 6 ΔAccuracy ΔF1 2 1 0 4 2 -1 0 .4) .7) .7) 1.6) .2) .5) tr(0 ur(0 hi( es(11 ru(11 fr(13 (a) Part-of-Speech (0. sw 2) .4) .6) .7) .2) .5) tr(0 bg(0 es(11 ru(11 fr(13 (b) XNLI Figure 2: Performance difference between AMBER trained with alignments on parallel data and AMBER (MLM). Languages are sorted by no. of parallel data (Million) used for training AMBER with alignments. 3.5 Xing et al., 2015; Devlin et al., 2019; Conneau et al., 2020a) are highly promising in their easy integration into neural network models for a variety of cross-lingual applications. Analysis studies on recent cross-lingual contextualized representations (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Siddhant et al., 2020) further demonstrates this advantage for zero-shot cross-lingual transfer in a representative set of languages and tasks. In particular to improve cross-lingual transfer, some attempts directly leverage multilingual parallel corpus to train contextualized representations (McCann et al., 2017; Eriguchi et al., 2018; Conneau"
2021.naacl-main.284,N19-1423,0,0.194875,"contextualized representations, albeit in a BER . Our code and models are available at post-hoc fashion. http://github.com/junjiehu/amber. In this work, we propose a training regimen for learning contextualized word representations 1 Introduction that encourages symmetry at both the word and Cross-lingual embeddings, both traditional non- sentence levels at training time. Our word-level contextualized word embeddings (Faruqui and alignment objective is inspired by work in machine Dyer, 2014) and the more recent contextualized translation that defines objectives encouraging conword embeddings (Devlin et al., 2019), are an es- sistency between the source-to-target and targetsential tool for cross-lingual transfer in downstream to-source attention matrices (Cohn et al., 2016). applications. In particular, multilingual contextu- Our sentence-level alignment objective encourages alized word representations have proven effective prediction of the correct translations within a miniin reducing the amount of supervision needed in batch for a given source sentence, which is inspired a variety of cross-lingual NLP tasks such as se- by work on learning multilingual sentence reprequence labeling (Pires et al., 201"
2021.naacl-main.284,E14-1049,0,0.111585,"Missing"
2021.naacl-main.284,D19-1382,0,0.0168817,"onneau and Lample, 2019). We use the same monolingual data as mBERT and follow Conneau and Lample (2019) to prepare the parallel data with one change to maintain truecasing. We set the maximum number of subwords in the concatenation of each sentence pair to 256 and use 10k warmup steps with the peak learning rate of 1e-4 and a linear decay of the learning rate. We train AMBER on TPU v3 for about 1 week. 3.2 Datasets Cross-lingual Part-Of-Speech (POS) contains data in 13 languages from the Universal Dependencies v2.3 (Nivre et al., 2018). 3 https://github.com/google-research/ bert 3635 PAWS-X (Yang et al., 2019b) is a paraphrase detection dataset. We train on the English data (Zhang et al., 2019), and evaluate the prediction accuracy on the test set translated into 4 other languages. XNLI (Conneau et al., 2018) is a natural language inference dataset in 15 languages. We train models on the English MultiNLI training data (Williams et al., 2018), and evaluate on the other 14. Tatoeba (Artetxe and Schwenk, 2019) is a testbed for parallel sentence identification. We select the 14 non-English languages covered by our parallel data, and follow the setup in Hu et al. (2020) finding the English translation"
2021.naacl-main.284,N19-1131,0,0.0642282,"Missing"
2021.naacl-main.337,2020.acl-main.124,0,0.0489445,"Missing"
2021.naacl-main.337,P19-1041,0,0.0135674,"kelihood of the references. As a result, commonly measured by the BLEU score between the system outputs and source texts. Since these some of the earliest work (Shen et al., 2017; Hu automatic metrics only require the system outputs et al., 2017; Fu et al., 2018) on unsupervised text and source texts, they can be used as rewards for style transfer proposed training algorithms that are training. Moreover, the two lines of approaches can still based on MLE by formulating the style transfer models as auto-encoders optimized with reconstruc- be used together, and previous work (Yang et al., 2018; John et al., 2019; Madaan et al., 2020) protion loss. Specifically, during training the model posed methods which use the auto-encoders as the is tasked to generate a style-agnostic encoding and backbone augmented with task-specific rewards. reconstruct the input text based on this encoding with style-specific embeddings or decoders. Dur- In particular, the style transfer accuracy reward is used by most of the recent work. ing inference, the model aims to transfer the source 1 However, reward-based training algorithms still Code and data are available at: https://github. com/yixinL7/Direct-Style-Transfer have"
2021.naacl-main.337,E17-2068,0,0.0137434,"00 Table 1: Number of samples in the Train, Dev, and Test splits for each dataset in our experiments. λcls λadv λsim λlang λrec λcyc Dataset Eq. Yelp (10) 2 (12) 1 0.5 - 20 - 2 - 0.1 1 1.5 Amazon (10) 2 (12) 5 0.5 - 20 - 2 - 1 1 0.5 IMDb (10) 1 (12) 1 0.5 - 20 - 2 - 1 1 1 GYAFC (10) 2 (12) 1 0.5 - 20 - 2 - 1 1 1 Table 2: Hyperparameter setting of Eq. 10 and Eq. 12 on each dataset. we chose the Family & Relationships category for our experiments. Datasets statistics are shown in Table 1. 3.2 Experimental Details Following previous work, we measure the style transfer accuracy using a FastText5 (Joulin et al., 2017) style classifier trained on the respective training set of each dataset. To measure content preservation, we use SIM and BLEU as metrics where self-SIM and self-BLEU are computed between the source sentences and system outputs, while ref-SIM and ref-BLEU are computed between the system outputs and human references when available. To measure the fluency we use a pre-trained GPT-2 model to compute the perplexity.6 Our generator, GPT-2, has 1.5 billion parameters, and we train on a GTX 1080 Ti GPU for about 12 hours. The weights of the loss terms in Eq. 10 and Eq. 12 are detailed in Table 2. Whi"
2021.naacl-main.337,D14-1181,0,0.00303821,"i.e., x ˜s = g(xs , 1 − s). Rewards for Style Transfer Accuracy We use a style classifier to provide the supervision signal to the generator with respect to the style transfer accuracy. The min-max game between the generator g and the classifier fcls is: min max Exs [log(1 − fcls (g(xs , 1 − s), 1 − s))] θg θfcls + Exs [log fcls (xs , s) + log(1 − fcls (xs , 1 − s))]. (1) The style transfer accuracy reward for the generator is the log-likelihood of the output being labeled as the target style: rcls (˜ xs ) = log(fcls (˜ xs , 1 − s)). (2) Following prior work, we use the CNN-based classifier (Kim, 2014) fcls , which takes both the sentence and the style label as input and its objective is to predict the likelihood of the sentence being coherent to the given style. (3) min(|r|,|h|) 1− max(|r|,|h|) LP(r, h) = e , (4) and α is an exponential term to control the weight of the length penalty, which is set to 0.25. We also use the cycle-consistency loss Lcyc to bootstrap the training: Lcyc (θg ) = Exs [− log(pg (xs |g(xs , 1 − s), s))]. (5) Here, pg is the likelihood assigned by the generator g. This introduces two generation passes, i.e., x ˜s = g(x, 1 − s) and x ¯s = g(˜ xs , s) while SIM reward"
2021.naacl-main.337,D19-1366,0,0.0310122,"Missing"
2021.naacl-main.337,N18-1169,0,0.0685559,"Missing"
2021.naacl-main.337,D19-1623,0,0.0200814,"performance more directly during training. the existing automatic metrics and propose effiThis approach is conceptually similar to training cient strategies of using these metrics for trainalgorithms that optimize models using rewards reing. The experimental results show that our lated to the corresponding evaluation metrics for model provides significant gains in both autoother NLP tasks, such as machine translation (Shen matic and human evaluation over strong baseet al., 2016; Wieting et al., 2019a) or text summalines, indicating the effectiveness of our pro1 rization (Paulus et al., 2018; Li et al., 2019). As posed methods and training strategies. for unsupervised style transfer, the widely used au1 Introduction tomatic metrics mainly attend to three desiderata: (1) style transfer accuracy – the generated sentence Text style transfer aims to convert an input text into must be in the target style, commonly measured another generated text with a different style but by the accuracy of a style classifier applied to the the same basic semantics as the input. One major transferred text, (2) fluency – the generated text challenge in this setting is that many style transfer tasks lack parallel corpora"
2021.naacl-main.337,2020.acl-main.169,1,0.750556,"erences. As a result, commonly measured by the BLEU score between the system outputs and source texts. Since these some of the earliest work (Shen et al., 2017; Hu automatic metrics only require the system outputs et al., 2017; Fu et al., 2018) on unsupervised text and source texts, they can be used as rewards for style transfer proposed training algorithms that are training. Moreover, the two lines of approaches can still based on MLE by formulating the style transfer models as auto-encoders optimized with reconstruc- be used together, and previous work (Yang et al., 2018; John et al., 2019; Madaan et al., 2020) protion loss. Specifically, during training the model posed methods which use the auto-encoders as the is tasked to generate a style-agnostic encoding and backbone augmented with task-specific rewards. reconstruct the input text based on this encoding with style-specific embeddings or decoders. Dur- In particular, the style transfer accuracy reward is used by most of the recent work. ing inference, the model aims to transfer the source 1 However, reward-based training algorithms still Code and data are available at: https://github. com/yixinL7/Direct-Style-Transfer have their limitations, and"
2021.naacl-main.337,N18-1049,0,0.0290982,", where the model generates an output in the target style, and the ability of a reconstruction model to re-generate the original text is used as a proxy for content preservation. While this method is more tolerant to lexical differences, the correlation between the reconstruction loss and content preservation can be weak. Therefore, we aim to design a reward for content preservation which can directly assess the semantic similarity between the system outputs and input texts. Specifically, we note that models of semantic similarity are widely studied (Wieting et al., 2016; Sharma et al., 2017; Pagliardini et al., 2018; Zhang* et al., 2020), and we can leverage these methods to directly calculate the similarity between the system outputs and input texts. This renders our method applicable for even unsupervised settings where no human references are available. lexical similarity with the input texts. Upon identifying this risk, we re-visit and propose several strategies that serve as auxiliary regularization on the style transfer models, effectively mitigating the problem discussed above. We empirically show that our proposed reward functions can provide significant gains in both automatic and human evaluati"
2021.naacl-main.337,D19-5614,0,0.0772983,"e it needs to consider the overlap in the semantics between the source text and system outputs. While using BLEU score between the source text and system output would be a direct solution (Xu et al., 2018), this approach has an inherent limitation in that n-gram based metrics such as BLEU are sensitive to lexical differences and will penalize modifications that are necessary for transferring text style. In fact, previous work has proposed various different proxy rewards for content preservation. One of the most popular methods is the cycle-consistency loss (Luo et al., 2019; Dai et al., 2019; Pang and Gimpel, 2019), which introduces a round-trip generation process, where the model generates an output in the target style, and the ability of a reconstruction model to re-generate the original text is used as a proxy for content preservation. While this method is more tolerant to lexical differences, the correlation between the reconstruction loss and content preservation can be weak. Therefore, we aim to design a reward for content preservation which can directly assess the semantic similarity between the system outputs and input texts. Specifically, we note that models of semantic similarity are widely st"
2021.naacl-main.337,P02-1040,0,0.11868,"ard functions to control the quality this by making minimal changes to the input texts of the system outputs. The quality of the outputs which are enough to trick the classifier used for is assessed in three ways: style transfer accuracy, style transfer accuracy while achieving high con- content preservation, and fluency. We attend to tent preservation and fluency scores due to the high each of these factors with their respective rewards. 4263 Gimpel, 2019), our method is more direct since it doesn’t require a second-pass generation. It also has advantages over n-gram based metrics like BLEU (Papineni et al., 2002) since it is more robust to lexical changes and can provide smoother rewards. In Wieting et al. (2019a), SIM is augmented with a length penalty to help control the length of the generated text. We use their entire model, S IMI L E, as the content preservation reward, rsim (˜ xs ) = LP(xs , x ˜s )α SIM(xs , x ˜s ), where Figure 1: SIM Loss v.s. Cycle-Consistency Loss Here we denote the input text x having style s by xs , and denote the output by x ˜s , i.e., x ˜s = g(xs , 1 − s). Rewards for Style Transfer Accuracy We use a style classifier to provide the supervision signal to the generator wit"
2021.naacl-main.337,P18-1080,0,0.0561968,"Missing"
2021.naacl-main.337,N18-1012,0,0.0605699,"takes the sequence of w ˆi as its input. We chose this method because it provides a token-level supervision signal to the generator, while the REINFORCE algorithm provides sentence-level signals. 3 3.1 Experiments Datasets We evaluate our approach on three datasets for sentiment transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset provided by Li et al. (2018),2 and the IMDb movie review dataset provided by Dai et al. (2019).3 We also evaluate our methods on a formality style transfer dataset, Grammarly’s Yahoo Answers Formality Corpus (GYAFC),4 introduced in Rao and Tetreault (2018). Although it is a parallel corpus, we treat it as an unaligned corpus in our experiments. In order to compare to previous work, We select the checkpoint with the highest mean of the style transfer accuracy and BLEU on the development set as the starting point for the second training stage. In the second stage, the generator is optimized with Eq. 10. The classifier fcls for Lcls is pretrained and the language model for Llang is finetuned on the training set. During training, the discriminator fadv for Ladv is trained against the generator. fcls is fixed when trained on some datasets, 2 while i"
2021.naacl-main.337,2020.acl-main.704,0,0.0329874,"Missing"
2021.naacl-main.337,P16-1159,0,0.0322826,"Missing"
2021.naacl-main.337,D19-1322,0,0.164871,"rial Results. D IR R-Y ELP-A DV and D IR R-A MAZON-A DV denote the models which generate adversarial examples. Acc denotes the style transfer accuracy, PPL denotes the perplexity, BLEU is computed between the human references and system outputs. better performance in automatic evaluation doesn’t always entail better performance in human evaluation. Therefore, we also manually checked the quality of the transferred texts on development set when we chose the value of the hyperparameters. We compare our model with several state-ofthe-art methods: DeleteAndRetrieve (D&R) (Li et al., 2018), B-GST (Sudhakar et al., 2019), CycleMulti (Dai et al., 2019), Deep-Latent (He et al., 2020), Tag&Gen (Madaan et al., 2020), and DualRL (Luo et al., 2019). We also compare our final model, D IR R(Direct-Reward), with the model only trained with the first stage (D IR R-C YCLE) as mentioned in Section 2.4. 3.3 Adversarial Examples Yelp and Amazon are arguably the most frequently used datasets for the sentiment transfer task. In our experiments, we found that the automatic evaluation metrics can be tricked on these datasets. Table 3 shows the performance of the models which generate adversarial examples. Upon identifying thes"
2021.naacl-main.337,P19-1427,1,0.938563,"which can assess the between system outputs and input texts. We also investigate the potential weaknesses of model performance more directly during training. the existing automatic metrics and propose effiThis approach is conceptually similar to training cient strategies of using these metrics for trainalgorithms that optimize models using rewards reing. The experimental results show that our lated to the corresponding evaluation metrics for model provides significant gains in both autoother NLP tasks, such as machine translation (Shen matic and human evaluation over strong baseet al., 2016; Wieting et al., 2019a) or text summalines, indicating the effectiveness of our pro1 rization (Paulus et al., 2018; Li et al., 2019). As posed methods and training strategies. for unsupervised style transfer, the widely used au1 Introduction tomatic metrics mainly attend to three desiderata: (1) style transfer accuracy – the generated sentence Text style transfer aims to convert an input text into must be in the target style, commonly measured another generated text with a different style but by the accuracy of a style classifier applied to the the same basic semantics as the input. One major transferred text, (2)"
2021.naacl-main.337,P19-1453,1,0.928599,"which can assess the between system outputs and input texts. We also investigate the potential weaknesses of model performance more directly during training. the existing automatic metrics and propose effiThis approach is conceptually similar to training cient strategies of using these metrics for trainalgorithms that optimize models using rewards reing. The experimental results show that our lated to the corresponding evaluation metrics for model provides significant gains in both autoother NLP tasks, such as machine translation (Shen matic and human evaluation over strong baseet al., 2016; Wieting et al., 2019a) or text summalines, indicating the effectiveness of our pro1 rization (Paulus et al., 2018; Li et al., 2019). As posed methods and training strategies. for unsupervised style transfer, the widely used au1 Introduction tomatic metrics mainly attend to three desiderata: (1) style transfer accuracy – the generated sentence Text style transfer aims to convert an input text into must be in the target style, commonly measured another generated text with a different style but by the accuracy of a style classifier applied to the the same basic semantics as the input. One major transferred text, (2)"
2021.naacl-main.337,P19-1482,0,0.019092,"have been shown effective for improving the model performance. Li et al. (2018) proposes a retrievebased pipeline, which contains three stages, namely, delete, retrieve and generate. Sudhakar et al. (2019) extends this pipeline by using GPT (Radford et al., 2018) as the generator. Compared to these methods, we propose a more direct and effective approach to encourage semantic-preserving transfer by directly measuring the semantic similarity of the source texts and system outputs. Recently, other works have been proposed for unsupervised text style transfer (Jin et al., 2019; Lai et al., 2019; Wu et al., 2019; Li et al., 2020). He et al. (2020) proposes a probabilistic view which models the non-parallel data from two domains as a partially observed parallel corpus. Madaan et al. (2020) proposes a tag-and-generate pipeline, which firstly identifies style attribute markers from the source texts, then replaces them with a special token, and generates the outputs based on the tagged sentences. Zhou et al. (2020) focuses on exploring the word-level style relevance which is assigned by a pre-trained style classifier. They propose a reward for content preservation which is based on the weighted combinati"
2021.naacl-main.337,P18-1090,0,0.325433,"l success, they In most cases, the lack of parallel corpora face the inherent difficulty of coming up with a makes it impossible to directly train superstyle-agnostic but content-preserving encoding – vised models for the text style transfer task. this is a non-trivial task and failure at this first step In this paper, we explore training algorithms that instead optimize reward functions that exwill diminish style transfer accuracy and content plicitly consider different aspects of the stylepreservation of the final output. transferred outputs. In particular, we leverage Another line of work (Xu et al., 2018; Pang and semantic similarity metrics originally used for Gimpel, 2019; Luo et al., 2019) proposes trainfine-tuning neural machine translation models ing algorithms based on rewards related to the auto explicitly assess the preservation of content tomatic evaluation metrics, which can assess the between system outputs and input texts. We also investigate the potential weaknesses of model performance more directly during training. the existing automatic metrics and propose effiThis approach is conceptually similar to training cient strategies of using these metrics for trainalgorithms that opt"
2021.naacl-main.337,D19-1053,0,0.0395721,"Missing"
2021.naacl-main.337,2020.acl-main.639,0,0.0688279,"rectly measuring the semantic similarity of the source texts and system outputs. Recently, other works have been proposed for unsupervised text style transfer (Jin et al., 2019; Lai et al., 2019; Wu et al., 2019; Li et al., 2020). He et al. (2020) proposes a probabilistic view which models the non-parallel data from two domains as a partially observed parallel corpus. Madaan et al. (2020) proposes a tag-and-generate pipeline, which firstly identifies style attribute markers from the source texts, then replaces them with a special token, and generates the outputs based on the tagged sentences. Zhou et al. (2020) focuses on exploring the word-level style relevance which is assigned by a pre-trained style classifier. They propose a reward for content preservation which is based on the weighted combination of the word embeddings of the source texts and system outputs. Compared to this reward, our proposed content reward is specifically designed for semantic similarity and pre-trained on large corpora, which makes it more robust across different datasets. rics more effective rewards for model training. Considering the weaknesses of the automatic metrics presented in this work, we believe that more rigoro"
2021.naacl-main.384,N18-2097,0,0.0172867,"18a) is an abstractive dataset that contains one-sentence summaries of online articles from BBC. CNN/DM (Hermann et al., 2015; Nallapati et al., 2016) is a widely-used summarization dataset consisting of news articles and associated highlights as summaries. We use its non-anonymized version. WikiHow (Koupaee and Wang, 2018) is extracted from an online knowledge base and requires high level of abstraction. New York Times (NYT) (Sandhaus, 2008) is a dataset that consists of news articles and their associated summaries.3 We follow Kedzie et al. (2018) to preprocess and split the dataset. PubMed (Cohan et al., 2018) is relatively extractive and is collected from scientific papers. 4.2 Guide R-1 R-2 R-L - 43.25 41.72 41.58 20.24 19.39 18.99 39.63 38.76 38.56 BertAbs + Sentence Auto. Oracle 43.78 55.18 20.66 32.54 40.66 52.06 BertAbs + Keyword Auto. Oracle 42.21 45.08 19.36 22.22 39.23 42.07 BertAbs + Relation Auto. Oracle 41.40 45.96 18.66 23.09 38.40 42.92 BertAbs + Retrieve Auto. Oracle 40.88 43.69 18.24 20.53 37.99 40.71 ∗ BertExt (Base) BertAbs∗ BertAbs (Ours) Ours Table 3: Results (ROUGE; Lin (2004)) on CNN/DM. “Auto” and “oracle” denote using automatically predicted and oracle-extracted guidance at"
2021.naacl-main.384,D18-1208,0,0.0280179,"we extract for extractive summarization. XSum (Narayan et al., 2018a) is an abstractive dataset that contains one-sentence summaries of online articles from BBC. CNN/DM (Hermann et al., 2015; Nallapati et al., 2016) is a widely-used summarization dataset consisting of news articles and associated highlights as summaries. We use its non-anonymized version. WikiHow (Koupaee and Wang, 2018) is extracted from an online knowledge base and requires high level of abstraction. New York Times (NYT) (Sandhaus, 2008) is a dataset that consists of news articles and their associated summaries.3 We follow Kedzie et al. (2018) to preprocess and split the dataset. PubMed (Cohan et al., 2018) is relatively extractive and is collected from scientific papers. 4.2 Guide R-1 R-2 R-L - 43.25 41.72 41.58 20.24 19.39 18.99 39.63 38.76 38.56 BertAbs + Sentence Auto. Oracle 43.78 55.18 20.66 32.54 40.66 52.06 BertAbs + Keyword Auto. Oracle 42.21 45.08 19.36 22.22 39.23 42.07 BertAbs + Relation Auto. Oracle 41.40 45.96 18.66 23.09 38.40 42.92 BertAbs + Retrieve Auto. Oracle 40.88 43.69 18.24 20.53 37.99 40.71 ∗ BertExt (Base) BertAbs∗ BertAbs (Ours) Ours Table 3: Results (ROUGE; Lin (2004)) on CNN/DM. “Auto” and “oracle” denot"
2021.naacl-main.384,D16-1140,1,0.776214,"methods for guided neural abstractive summarization: methods that provide various types of guidance signals that 1) constrain the summary so that the output content will deviate less from 1 Introduction the source document; 2) allow for controllability Modern techniques for text summarization gener- through provision of user-specified inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant summaries from the training set. While produce novel words and sentences. Compared these methods have demonstrated improvements with ex"
2021.naacl-main.384,D15-1044,0,0.0697521,"lity Modern techniques for text summarization gener- through provision of user-specified inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant summaries from the training set. While produce novel words and sentences. Compared these methods have demonstrated improvements with extractive algorithms, abstractive algorithms in summarization quality and controllability, each are more flexible, making them more likely to pro- focuses on one particular type of guidance – it reduce fluent and coherent summaries. However, the"
2021.naacl-main.384,D19-1387,0,0.410408,"attention to them at test time. With this in mind, we describe the four varieties of guidance signal we experiment with, along with their automatic and oracle extraction methods. Highlighted Sentences. The success of extractive approaches have demonstrated that we can extract a subset of sentences {xi1 , · · · , xin } from the source document and concatenate them to form a summary. Inspired by this, we explicitly inform our model which subset of source sentences should be highlighted using extractive models. We perform oracle extraction using a greedy search algorithm (Nallapati et al., 2017; Liu and Lapata, 2019) to find a set of sentences in the source document that have the highest ROUGE scores with the reference (detailed in Appendix) and treat these as our guidance g. At test time, we use pretrained extractive summarization models (BertExt (Liu and Lapata, 2019) or MatchSum (Zhong et al., 2020) in our experiments) to perform automatic prediction. cur in an actual summary, which could distract the model from focusing on the desired aspects of the input. Therefore, we also try to feed our model with a set of individual keywords {w1 , . . . , wn } from the source document. For oracle extraction, we f"
2021.naacl-main.384,P17-1099,0,0.123549,"t summaries. 2 Background and Related Work Neural abstractive summarization typically takes a source document x consisting of multiple sentences x1 , · · · , x|x |, runs them through an encoder to generate representations, and passes them to a decoder that outputs the summary y one target word at a time. Model parameters θ are trained to maximize the conditional likelihood of the outputs in a parallel training corpus hX , Yi: X arg max log p(yi |xi ; θ). θ hxi ,yi i∈hX ,Yi Several techniques have been proposed to improve the model architecture. For example, models of copying (Gu et al., 2016; See et al., 2017; Gehrmann et al., 2018) allow words to be copied directly from the input to the output, and models of coverage discourage the model from generating repetitive words (See et al., 2017). We evaluate our methods on 6 popular summarization benchmarks. Our best model, using highlighted sentences as guidance, can achieve stateof-the-art performance on 4 out of the 6 datasets, Guidance can be defined as some variety of sigincluding 1.28/0.79/1.13 ROUGE-1/2/L improve- nal g that is fed into the model in addition to the source document x: ments over previous state-of-the-art model on the X widely-used"
2021.naacl-main.384,D18-1444,0,0.0619156,"an also result in problems. First, it can result In this paper, we propose a general and exten1 sible guided summarization framework that can Code is available at https://github.com/ neulab/guided_summarization. take different kinds of external guidance as in4830 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4830–4842 June 6–11, 2021. ©2021 Association for Computational Linguistics Guidance Form Work Tokens Triples Sentences Summaries Kikuchi et al. (2016) Cao et al. (2018) Li et al. (2018) Liu et al. (2018a) Liu et al. (2018b) Fan et al. (2018) Zhu et al. (2020) Jin et al. (2020) Saito et al. (2020) 3 (length tokens) 7 3 (keywords) 7 3 (length tokens) 3 (length, entity, style tokens) 7 7 3 (keywords) 7 7 7 7 7 7 3 (relations) 3 (relations) 7 7 7 7 3 (highlighted sents.) 7 7 7 7 3 (highlighted sents.) 7 3 (retrieved sums.) 7 7 7 7 7 7 7 Ours 3 (keywords) 3 (relations) 3 (highlighted sents.) 3 (retrieved sums.) Table 1: A comparison of different guided neural abstractive summarization models. Previous works have tried to provide guidance in different forms, including tokens, triples, sentences an"
2021.naacl-main.384,W04-3252,0,0.124575,"led in Appendix) and treat these as our guidance g. At test time, we use pretrained extractive summarization models (BertExt (Liu and Lapata, 2019) or MatchSum (Zhong et al., 2020) in our experiments) to perform automatic prediction. cur in an actual summary, which could distract the model from focusing on the desired aspects of the input. Therefore, we also try to feed our model with a set of individual keywords {w1 , . . . , wn } from the source document. For oracle extraction, we first use the greedy search algorithm mentioned above to select a subset of input sentences, then use TextRank (Mihalcea and Tarau, 2004) to extract keywords from these sentences. We also filter the keywords that are not in the target summary. The remaining keywords are then fed to our models. For automatic prediction, we use another neural model (BertAbs (Liu and Lapata, 2019) in the experiments) to predict the keywords in the target summary. Relations. Relations are typically represented in the form of relational triples, with each triple containing a subject, a relation, and an object. For example, Barack Obama was born in Hawaii will create a triple (Barack Obama, was born in, Hawaii). For oracle extraction, we first use St"
2021.naacl-main.384,2020.acl-main.552,1,0.945898,"ences {xi1 , · · · , xin } from the source document and concatenate them to form a summary. Inspired by this, we explicitly inform our model which subset of source sentences should be highlighted using extractive models. We perform oracle extraction using a greedy search algorithm (Nallapati et al., 2017; Liu and Lapata, 2019) to find a set of sentences in the source document that have the highest ROUGE scores with the reference (detailed in Appendix) and treat these as our guidance g. At test time, we use pretrained extractive summarization models (BertExt (Liu and Lapata, 2019) or MatchSum (Zhong et al., 2020) in our experiments) to perform automatic prediction. cur in an actual summary, which could distract the model from focusing on the desired aspects of the input. Therefore, we also try to feed our model with a set of individual keywords {w1 , . . . , wn } from the source document. For oracle extraction, we first use the greedy search algorithm mentioned above to select a subset of input sentences, then use TextRank (Mihalcea and Tarau, 2004) to extract keywords from these sentences. We also filter the keywords that are not in the target summary. The remaining keywords are then fed to our model"
2021.naacl-main.384,K16-1028,0,0.432985,"ied inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant summaries from the training set. While produce novel words and sentences. Compared these methods have demonstrated improvements with extractive algorithms, abstractive algorithms in summarization quality and controllability, each are more flexible, making them more likely to pro- focuses on one particular type of guidance – it reduce fluent and coherent summaries. However, the mains unclear which is better and whether they are unconstrained nature of abstractive su"
2021.naacl-main.384,D18-1206,0,0.236389,"hard to pick in advance which aspects of the original content an abstractive system may touch upon. To address the issues, we propose methods for guided neural abstractive summarization: methods that provide various types of guidance signals that 1) constrain the summary so that the output content will deviate less from 1 Introduction the source document; 2) allow for controllability Modern techniques for text summarization gener- through provision of user-specified inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant"
2021.naacl-main.384,P18-1061,0,0.020999,"ctive system may touch upon. To address the issues, we propose methods for guided neural abstractive summarization: methods that provide various types of guidance signals that 1) constrain the summary so that the output content will deviate less from 1 Introduction the source document; 2) allow for controllability Modern techniques for text summarization gener- through provision of user-specified inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant summaries from the training set. While produce novel words and senten"
2021.naacl-main.40,D18-1269,0,0.0411757,"n the input tokenized by deterministic segmentation only. Therefore, our method does not add additional decoding latency. MVR needs about twice the fine-tuning cost compared to the baseline. However, compared to pretraining and inference usage of a model, fine-tuning is generally the least expensive component. 5 5.1 Experiments Training and evaluation We evaluate the multilingual representations using tasks from the XTREME benchmark (Hu et al., 2020), focusing on the zero-shot cross-lingual transfer with English as the source language. We consider sentence classification tasks including XNLI (Conneau et al., 2018) and PAWS-X (Yang et al., 2019), a structured prediction task of multilingual NER (Pan et al., 2017), and questionanswering tasks including XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020). 5.2 Experiment setup We evaluate on both the mBERT model which utilizes BPE to tokenize the inputs, and the XLM-R models which uses ULM segmentation. To replicate the baseline, we follow the hyperparameters provided in the XTREME codebase4 . Models are fine-tuned on English training data and zero-shot transferred to other languages. We run each experiment with 5 random seeds and record the average"
2021.naacl-main.40,2020.acl-main.275,0,0.0119164,"that optimize word representations for NMT and language modeling. Character level embeddings have been utilized instead of subword segmentation for NMT (Cherry et al., 2018; Lee et al., 2017; Ataman and Federico, 2018) and language modeling (Kim et al., 2016; Józefowicz et al., 2016). Wang et al. (2019) propose a multilingual word embedding method for NMT that relies on character n-gram embedding and a latent semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model consistency on unlabeled data. Several self-training methods utilize unlabeled examples to minimize the distance between the model predictions based on the unlabeled example and a noised version of the same input (Miyato et al., 2017b,a; Xu and Yang, 2017; Clark et al., 2018; Xie et al., 2018). Xu and Yang (2017) use knowledge distillation on unlabeled data to adapt models to a new language. Clark et al. (2018) propose to mask out differen"
2021.naacl-main.40,2020.acl-main.421,1,0.811039,"pared to the baseline. However, compared to pretraining and inference usage of a model, fine-tuning is generally the least expensive component. 5 5.1 Experiments Training and evaluation We evaluate the multilingual representations using tasks from the XTREME benchmark (Hu et al., 2020), focusing on the zero-shot cross-lingual transfer with English as the source language. We consider sentence classification tasks including XNLI (Conneau et al., 2018) and PAWS-X (Yang et al., 2019), a structured prediction task of multilingual NER (Pan et al., 2017), and questionanswering tasks including XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020). 5.2 Experiment setup We evaluate on both the mBERT model which utilizes BPE to tokenize the inputs, and the XLM-R models which uses ULM segmentation. To replicate the baseline, we follow the hyperparameters provided in the XTREME codebase4 . Models are fine-tuned on English training data and zero-shot transferred to other languages. We run each experiment with 5 random seeds and record the average results and the standard deviation. SR We use BPE-dropout (Provilkov et al., 2020) for mBERT and ULM-sample (Kudo, 2018) for XLM-R models to do probabilistic segmentat"
2021.naacl-main.40,P18-2049,0,0.0209554,"and they require modification to the model pretraining stage. Chung et al. (2020) propose to cluster related languages together and run subword vocabulary construction on each language cluster when constructing vocabularies for mBERT. Their method is also applied at the pretraining stage and could be combined with our method for potential additional improvements. Our method is also related to prior work that optimize word representations for NMT and language modeling. Character level embeddings have been utilized instead of subword segmentation for NMT (Cherry et al., 2018; Lee et al., 2017; Ataman and Federico, 2018) and language modeling (Kim et al., 2016; Józefowicz et al., 2016). Wang et al. (2019) propose a multilingual word embedding method for NMT that relies on character n-gram embedding and a latent semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model consistency on unlabeled data. Several self-tr"
2021.naacl-main.40,D19-1252,0,0.0811424,"tation that is inconsistent across languages, harming cross-lingual transfer performance, particularly on under-represented languages. As one example, consider the segmentation of the word “excitement” in different languages in Tab. 1. The English word is not segmented, but its translations in the other languages, including the relatively high-resourced French and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to a unique embedding vector, the segmentation discrepancy—which generally does Multilingual pre-trained representations (Devlin et al., 2019; Huang et al., 2019; Conneau and Lam- not agree with a language’s morphology—could ple, 2019; Conneau et al., 2020) are now an es- map words from different languages to very distant representations, hurting cross-lingual transfer. In sential component of state-of-the-art methods for fact, previous work (Conneau et al., 2020; Artetxe cross-lingual transfer (Wu and Dredze, 2019; Pires et al., 2020) has shown that heuristic fixes such as et al., 2019). These methods pretrain an encoder by increasing the subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource language"
2021.naacl-main.40,2020.findings-emnlp.414,0,0.11505,"er by increasing the subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource languages during learning data in up to hundreds of languages which can then of the subword segmentation can lead to significant be fine-tuned on annotated data of a downstream performance improvements. task in a high-resource language, often English, and Despite this, there is not much work studying transferred to another language. In order to encode or improving subword segmentation methods for hundreds of languages with diverse vocabulary, it cross-lingual transfer. Bostrom and Durrett (2020) is standard for such multilingual models to employ empirically compare several popular word segmena shared subword vocabulary jointly learned on the tation algorithms for pretrained language models 1 Code for the method is released here: of a single language. Several works propose to https://github.com/cindyxinyiwang/ multiview-subword-regularization use different representation granularities, such as 473 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 473–482 June 6–11, 2021. ©2021 Associati"
2021.naacl-main.40,2020.findings-emnlp.118,0,0.125653,"subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource languages during learning data in up to hundreds of languages which can then of the subword segmentation can lead to significant be fine-tuned on annotated data of a downstream performance improvements. task in a high-resource language, often English, and Despite this, there is not much work studying transferred to another language. In order to encode or improving subword segmentation methods for hundreds of languages with diverse vocabulary, it cross-lingual transfer. Bostrom and Durrett (2020) is standard for such multilingual models to employ empirically compare several popular word segmena shared subword vocabulary jointly learned on the tation algorithms for pretrained language models 1 Code for the method is released here: of a single language. Several works propose to https://github.com/cindyxinyiwang/ multiview-subword-regularization use different representation granularities, such as 473 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 473–482 June 6–11, 2021. ©2021 Associati"
2021.naacl-main.40,D14-1181,0,0.00330276,"l. (2018) propose to mask out different parts of the unlabeled input and encourage the model to make consistent prediction given these different inputs. These methods all focus on semi-supervised learning, while our method regulates model consistency to mitigate the subword segmentation discrepancy between different languages. 8 Conclusion Several works propose to optimize subwordsensitive word encoding methods for pretrained We believe that the results in this paper convinclanguage models. Ma et al. (2020) uses convolu- ingly demonstrate that standard deterministic subtional neural networks (Kim, 2014) on characters word segmentation is sub-optimal for multilingual to calculate word representations. Zhang and Li pretrained representations. Even incorporating sim(2020) propose to add phrases into the vocabulary ple methods for subword regularization such as 480 BPE-dropout at fine-tuning can improve the crosslingual transfer of pretrained models, and our proposed Multi-view Subword Regularization method further shows consistent and strong improvements over a variety of tasks for models built upon different subword segmentation algorithms. Going forward, we suggest that some variety of subwor"
2021.naacl-main.40,D18-1461,0,0.01798,"d representations of a single language, and they require modification to the model pretraining stage. Chung et al. (2020) propose to cluster related languages together and run subword vocabulary construction on each language cluster when constructing vocabularies for mBERT. Their method is also applied at the pretraining stage and could be combined with our method for potential additional improvements. Our method is also related to prior work that optimize word representations for NMT and language modeling. Character level embeddings have been utilized instead of subword segmentation for NMT (Cherry et al., 2018; Lee et al., 2017; Ataman and Federico, 2018) and language modeling (Kim et al., 2016; Józefowicz et al., 2016). Wang et al. (2019) propose a multilingual word embedding method for NMT that relies on character n-gram embedding and a latent semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model"
2021.naacl-main.40,2020.emnlp-main.367,0,0.61274,"e sample different segmentations x0 for each input sentence x⇤ . Previous works (Kudo, 2018; Provilkov et al., 2020) have demonstrated that subword regularization us2 ing both BPE-dropout and ULM-sampling are efWe use Pan et al. (2017)’s named entity recognition test fective at improving machine translation accuracy, data with mBERT’s tokenizer. 475 propose to adapt a pretrained multilingual model to a new language by augmenting the vocabulary with a new subword vocabulary learned on the target language, but this method might not help for languages other than the target language it adapts to. Chung et al. (2020) propose to separately construct a subword segmentation model for each cluster of related languages for pretraining the multilingual representations. However, directly modifying the word segmentation requires retraining large pretrained models, which is computationally prohibitive in most cases. In this paper, we instead propose a more efficient approach of using probabilistic segmentation during fine-tuning on labeled data of a downstream task. As mismatch in segmentation is one of the factors harming cross-lingual transfer, we expect a model that becomes more robust to different varieties of"
2021.naacl-main.40,D18-1217,0,0.334623,"sequence of characters. be sub-optimal as it creates a discrepancy between It then counts the most frequent character token the segmentations during the pretraining and fine- bigrams in the data, merges them into a new token, tuning stages. To address this problem, we pro- and adds the new token to the vocabulary. This pose Multi-view Subword Regularization (MVR; process is done iteratively until a predefined vocabFig. 1), a novel method—inspired by the usage ulary size is reached. of consistency regularization in semi-supervised To segment a word, BPE simply splits the word learning methods (Clark et al., 2018; Xie et al., into character tokens, and iteratively merges adja2018)—which utilizes both the standard and proba- cent tokens with the highest priority until no merge bilistically segmented inputs, enforcing the model’s operation is possible. That is, for an input x⇤ , it predictions to be consistent across the two views. assigns segmentation probability P (b x) = 1 for the Such consistency regularization further improves sequence x b obtained from the greedy merge operaaccuracy, with MVR finally demonstrating consis- tions, and assigns other possible segmentations a tent gains of up to 2.5 po"
2021.naacl-main.40,2020.acl-main.747,0,0.596672,"ticularly on under-represented languages. As one example, consider the segmentation of the word “excitement” in different languages in Tab. 1. The English word is not segmented, but its translations in the other languages, including the relatively high-resourced French and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to a unique embedding vector, the segmentation discrepancy—which generally does Multilingual pre-trained representations (Devlin et al., 2019; Huang et al., 2019; Conneau and Lam- not agree with a language’s morphology—could ple, 2019; Conneau et al., 2020) are now an es- map words from different languages to very distant representations, hurting cross-lingual transfer. In sential component of state-of-the-art methods for fact, previous work (Conneau et al., 2020; Artetxe cross-lingual transfer (Wu and Dredze, 2019; Pires et al., 2020) has shown that heuristic fixes such as et al., 2019). These methods pretrain an encoder by increasing the subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource languages during learning data in up to hundreds of languages which can then of the subword segmentation"
2021.naacl-main.40,P18-1007,0,0.317836,"g1 Sebastian Ruder2 Graham Neubig1 1 Language Technology Institute, Carnegie Mellon University 2 DeepMind xinyiw1@cs.cmu.edu,ruder@google.com,gneubig@cs.cmu.edu Abstract Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency between predictions of using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.1 en de el excitement Auf"
2021.naacl-main.40,D18-2012,0,0.165334,"brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.1 en de el excitement Auf/re/gung en/j/ousi/asmÏc fr pt ru excita/tion excita/ção волн/ение Table 1: XLM-R segmentation of “excitement” in different languages. The English word is not segmented while the same word in other languages is over-segmented. A better segmentation would allow the model to match the verb stem and derivational affix across languages. multilingual data using heuristic word segmentation methods based on byte-pair-encoding (BPE; Sennrich et al., 2016) or unigram language models (Kudo and Richardson, 2018) (details in §2). However, subword-based preprocessing can lead to sub-optimal segmentation that is inconsistent across languages, harming cross-lingual transfer performance, particularly on under-represented languages. As one example, consider the segmentation of the word “excitement” in different languages in Tab. 1. The English word is not segmented, but its translations in the other languages, including the relatively high-resourced French and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to a unique embedding vector, the segmentation discrepancy"
2021.naacl-main.40,Q17-1026,0,0.0193419,"a single language, and they require modification to the model pretraining stage. Chung et al. (2020) propose to cluster related languages together and run subword vocabulary construction on each language cluster when constructing vocabularies for mBERT. Their method is also applied at the pretraining stage and could be combined with our method for potential additional improvements. Our method is also related to prior work that optimize word representations for NMT and language modeling. Character level embeddings have been utilized instead of subword segmentation for NMT (Cherry et al., 2018; Lee et al., 2017; Ataman and Federico, 2018) and language modeling (Kim et al., 2016; Józefowicz et al., 2016). Wang et al. (2019) propose a multilingual word embedding method for NMT that relies on character n-gram embedding and a latent semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model consistency on unl"
2021.naacl-main.40,2020.acl-main.653,0,0.0907825,"Missing"
2021.naacl-main.40,2020.coling-main.4,0,0.611782,"vocabulary jointly learned on the tation algorithms for pretrained language models 1 Code for the method is released here: of a single language. Several works propose to https://github.com/cindyxinyiwang/ multiview-subword-regularization use different representation granularities, such as 473 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 473–482 June 6–11, 2021. ©2021 Association for Computational Linguistics phrase-level segmentation (Zhang and Li, 2020) or character-aware representations (Ma et al., 2020) for pretrained language models of a single highresource language, such as English or Chinese only. However, it is not a foregone conclusion that methods designed and tested on monolingual models will be immediately applicable to multilingual representations. Furthermore, they add significant computation cost to the pretraining stage, which is especially problematic for multilingual pretraining on hundreds of languages. The problem of suboptimal subword segmentation has drawn more attention in the context of neural machine translation (NMT). Specifically, subword regularization methods have be"
2021.naacl-main.40,P17-1178,0,0.143433,"ual alignment. Despite these issues, few methods have tried to address this subword segmentation problem for multilingual pretrained models. Chau et al. (2020) Subword regularization (Kudo, 2018) is a method that incorporates probabilistic segmentation at training time to improve the robustness of models to different segmentations. The idea is conceptually simple: at training time sample different segmentations x0 for each input sentence x⇤ . Previous works (Kudo, 2018; Provilkov et al., 2020) have demonstrated that subword regularization us2 ing both BPE-dropout and ULM-sampling are efWe use Pan et al. (2017)’s named entity recognition test fective at improving machine translation accuracy, data with mBERT’s tokenizer. 475 propose to adapt a pretrained multilingual model to a new language by augmenting the vocabulary with a new subword vocabulary learned on the target language, but this method might not help for languages other than the target language it adapts to. Chung et al. (2020) propose to separately construct a subword segmentation model for each cluster of related languages for pretraining the multilingual representations. However, directly modifying the word segmentation requires retrain"
2021.naacl-main.40,2020.emnlp-main.617,1,0.871264,"Missing"
2021.naacl-main.40,P19-1493,0,0.169022,"Missing"
2021.naacl-main.40,2020.acl-main.170,0,0.564093,"Ruder2 Graham Neubig1 1 Language Technology Institute, Carnegie Mellon University 2 DeepMind xinyiw1@cs.cmu.edu,ruder@google.com,gneubig@cs.cmu.edu Abstract Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency between predictions of using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.1 en de el excitement Auf/re/gung en/j/ousi/asmÏc"
2021.naacl-main.40,P16-1162,0,0.334748,"ilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.1 en de el excitement Auf/re/gung en/j/ousi/asmÏc fr pt ru excita/tion excita/ção волн/ение Table 1: XLM-R segmentation of “excitement” in different languages. The English word is not segmented while the same word in other languages is over-segmented. A better segmentation would allow the model to match the verb stem and derivational affix across languages. multilingual data using heuristic word segmentation methods based on byte-pair-encoding (BPE; Sennrich et al., 2016) or unigram language models (Kudo and Richardson, 2018) (details in §2). However, subword-based preprocessing can lead to sub-optimal segmentation that is inconsistent across languages, harming cross-lingual transfer performance, particularly on under-represented languages. As one example, consider the segmentation of the word “excitement” in different languages in Tab. 1. The English word is not segmented, but its translations in the other languages, including the relatively high-resourced French and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to"
2021.naacl-main.40,D19-1077,0,0.0632026,"ench and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to a unique embedding vector, the segmentation discrepancy—which generally does Multilingual pre-trained representations (Devlin et al., 2019; Huang et al., 2019; Conneau and Lam- not agree with a language’s morphology—could ple, 2019; Conneau et al., 2020) are now an es- map words from different languages to very distant representations, hurting cross-lingual transfer. In sential component of state-of-the-art methods for fact, previous work (Conneau et al., 2020; Artetxe cross-lingual transfer (Wu and Dredze, 2019; Pires et al., 2020) has shown that heuristic fixes such as et al., 2019). These methods pretrain an encoder by increasing the subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource languages during learning data in up to hundreds of languages which can then of the subword segmentation can lead to significant be fine-tuned on annotated data of a downstream performance improvements. task in a high-resource language, often English, and Despite this, there is not much work studying transferred to another language. In order to encode or improving"
2021.naacl-main.40,P17-1130,0,0.0191392,"nt semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model consistency on unlabeled data. Several self-training methods utilize unlabeled examples to minimize the distance between the model predictions based on the unlabeled example and a noised version of the same input (Miyato et al., 2017b,a; Xu and Yang, 2017; Clark et al., 2018; Xie et al., 2018). Xu and Yang (2017) use knowledge distillation on unlabeled data to adapt models to a new language. Clark et al. (2018) propose to mask out different parts of the unlabeled input and encourage the model to make consistent prediction given these different inputs. These methods all focus on semi-supervised learning, while our method regulates model consistency to mitigate the subword segmentation discrepancy between different languages. 8 Conclusion Several works propose to optimize subwordsensitive word encoding methods for pretrained We believe that the"
2021.naacl-main.40,D19-1382,0,0.0414781,"stic segmentation only. Therefore, our method does not add additional decoding latency. MVR needs about twice the fine-tuning cost compared to the baseline. However, compared to pretraining and inference usage of a model, fine-tuning is generally the least expensive component. 5 5.1 Experiments Training and evaluation We evaluate the multilingual representations using tasks from the XTREME benchmark (Hu et al., 2020), focusing on the zero-shot cross-lingual transfer with English as the source language. We consider sentence classification tasks including XNLI (Conneau et al., 2018) and PAWS-X (Yang et al., 2019), a structured prediction task of multilingual NER (Pan et al., 2017), and questionanswering tasks including XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020). 5.2 Experiment setup We evaluate on both the mBERT model which utilizes BPE to tokenize the inputs, and the XLM-R models which uses ULM segmentation. To replicate the baseline, we follow the hyperparameters provided in the XTREME codebase4 . Models are fine-tuned on English training data and zero-shot transferred to other languages. We run each experiment with 5 random seeds and record the average results and the standard devia"
2021.naacl-main.42,P19-1015,0,0.0348584,"nesian Kartvelian Uralic Austronesian Turkic Tupian Spanish Chinese Indonesian Georgian Russian Indonesian Turkish Spanish Table 1: Target language information on the NER task. The data set size of the these languages is 100. Corpus (MARC) (Keung et al., 2020) as the highresource language and product review datasets in two low-resource languages, Telugu and Persian (Gangula and Mamidi, 2018; Hosseini et al., 2018). WikiAnn WikiAnn (Pan et al., 2017) is a multilingual NER dataset constructed with Wikipedia articles and anchor links. We use the train, development and test partitions provided in Rahimi et al. (2019). The dataset size ranges from 100 to 20k for different languages. MARC The Multilingual Amazon Reviews Corpus (Keung et al., 2020) is a collection of Amazon product reviews for multilingual text classification. The dataset contains reviews in English, Japanese, German, French, Spanish, and Chinese with fivestar ratings. Each language has 200k examples for training. Note that we only use its English dataset. SentiPers SentiPers (Hosseini et al., 2018) is a sentiment corpus in Persian (fa) consisting of around 26k sentences of users’ opinions for digital products. Each sentence has an assigned"
2021.naacl-main.42,D19-6127,1,0.825109,"that are already covered languages. However, these techniques may not by existing representations (Wu and Dredze, 2019). readily transfer onto extremely low-resource lan- In contrast, existing work on transferring to languages, where: (1) large-scale monolingual cor- guages without significant monolingual resources pora are not available for pre-training and (2) suf- tends to be more sparse and typically focuses on ficient labeled data is lacking for effective fine- specific tasks such as language modeling (Adams tuning for downstream tasks. For example, mul- et al., 2017) or entity linking (Zhou et al., 2019). tilingual BERT (mBERT) (Devlin et al., 2018) is Building NLP systems in these settings is chalpre-trained on 104 languages with many articles on lenging for several reasons. First, a lack of suf∗ ficient annotated data in the target language preMost of the work was done while the first author was an intern at Microsoft Research. vents effective fine-tuning. Second, multilingual 499 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 499–511 June 6–11, 2021. ©2021 Association for Computational Li"
2021.tacl-1.1,C18-1214,1,0.92547,"results both in higher accuracy as well as lower calibration error (SCE). This effect of CVT is much more pronounced in the second experiment, which presents a low-resource scenario and is common in an AL framework. 6 Human Annotation Experiment In this section, we apply our proposed approach on Griko, an endangered language spoken by around 20,000 people in southern Italy, in the Grec`ıa Salentina area southeast of Lecce. The only available online Griko corpus, referred to as UoI (Lekakou et al., 2013),8 consists of 330 utterances by nine native speakers having POS annotations. Additionally, Anastasopoulos et al. (2018) collected, processed, and released 114 stories, of which only the first 10 stories were annotated by experts and have gold-standard annotations. We conduct human annotation experiments on the remaining unannotated stories in order to compare the different active learning methods. 5.4 Effect of Cross-View Training As mentioned in Section § 4.2, we use CVT to not only improve our model overall but also to have a well-calibrated model that can be important for active learning. A model is well-calibrated when a model’s predicted probabilities over the outcomes reflects the true probabilities over"
2021.tacl-1.1,P18-1246,0,0.0562382,"Missing"
2021.tacl-1.1,D19-1520,1,0.892816,"Missing"
2021.tacl-1.1,D18-1217,0,0.0535786,"Missing"
2021.tacl-1.1,D17-1078,0,0.0488873,"Missing"
2021.tacl-1.1,P11-1061,0,0.0734369,"Missing"
2021.tacl-1.1,L18-1293,0,0.0415965,"Missing"
2021.tacl-1.1,P17-2093,0,0.192463,"world’s 7000 languages (Hammarstr¨om et al., 2018). Furthermore, manually annotating large amounts of text with trained experts is an expensive and time-consuming task, even more so when linguists/annotators might not be native speakers of the language. Active Learning (Lewis, 1995; Settles, 2009, AL) is a family of methods that aim to train effective models with less human effort and cost by selecting such a subset of data that maximizes the end model performance. Although many methods have been proposed for AL in sequence labeling (Settles and Craven, 2008; Marcheggiani and Arti`eres, 2014; Fang and Cohn, 2017), through an empirical study across six typologically diverse languages we show that within the same task setup these methods perform inconsistently. Furthermore, even in an oracle scenario, where we have access to the true labels during data selection, existing methods are far from optimal. We posit that the primary reason for this inconsistent performance is that while existing methods consider uncertainty in predictions, they do not consider the direction of the uncertainty with respect to the output labels. For instance, in Figure 1 we consider the German token ‘‘die,’’ which may be either"
2021.tacl-1.1,N13-1014,0,0.0317149,"data which however, is from a slightly different domain, which affects the results somewhat. We observe that QBC has lower WD scores for Linguist-1 and Linguist-2 and UNS for Linguist-3. On further analysis, we find that even though QBC has lower WD, it also has the least coverage of the test data—that is, it has the fewest number of annotated tokens which are present in the test data, as shown in Table 7. We would like to note that a lower WD score does not necessarily translate to 7 Related Work Active Learning for POS Tagging: AL has been widely used for POS tagging. Garrette and Baldridge (2013) use a graph-based label propagation to generalize initial POS annotations to the unlabeled corpus. Further, they find that under a constrained time setting, typelevel annotations prove to be more useful than token-level annotations. In line with this, Fang and Cohn (2017) also select informative word types based on uncertainty sampling for lowresource POS tagging. They also construct a tag dictionary from these type-level annotations and then propagate the labels across the entire unlabeled corpus. However, in our initial analysis on uncertainty sampling, we found that adding label-propagatio"
2021.tacl-1.1,P19-1172,0,0.0440182,"Missing"
2021.tacl-1.1,L16-1262,0,0.058029,"Missing"
2021.tacl-1.1,P16-1101,0,0.0150373,"SCRAL (z )+(1 − pi,t,yˆi, t ) ˆj ← arg maxj ∈ J  {yˆ } pi, t, j 11: i, t 12: OCRAL (z, ˆj ) ← OCRAL (z, ˆj )+ 1 13: XINIT ← b- arg maxz ∈Z SCRAL (z ) 14: for zk ∈ XINIT do 15: jk ← arg maxj ∈ J OCRAL (zk , j ) 16: for xi, t ∈ D s.t. xi, t = zk do 17: cxi, t ← enc(xi, t ) 18: Wxi, t = pi, t, jk ∗ cxi, t 19: 4 Model and Training Regimen Now that we have a method to select data for annotation, we present our POS tagger in Section §4.1, followed by the training algorithm in Section §4.2. 4.1 Model Architecture Our POS tagging model is a hierarchical neural conditional random field (CRF) tagger (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017). Each token (x, t) from the input sequence x is first passed through a character-level Bi-LSTM, followed by a self-attention layer (Vaswani et al., 2017), followed by another Bi-LSTM to capture information about subword structure of the words. Finally, these character-level representations are fed into a token-level Bi-LSTM in order to − → ← − create contextual representations ct = ht : ht , ← − − → where ht and ht are the representations from the forward and backward LSTMs, and ‘‘:’’ denotes the concatenation operation. The encoded representations are"
2021.tacl-1.1,P18-1247,1,0.905155,"Missing"
2021.tacl-1.1,D08-1112,0,0.675088,"resources are not readily available for the majority of the world’s 7000 languages (Hammarstr¨om et al., 2018). Furthermore, manually annotating large amounts of text with trained experts is an expensive and time-consuming task, even more so when linguists/annotators might not be native speakers of the language. Active Learning (Lewis, 1995; Settles, 2009, AL) is a family of methods that aim to train effective models with less human effort and cost by selecting such a subset of data that maximizes the end model performance. Although many methods have been proposed for AL in sequence labeling (Settles and Craven, 2008; Marcheggiani and Arti`eres, 2014; Fang and Cohn, 2017), through an empirical study across six typologically diverse languages we show that within the same task setup these methods perform inconsistently. Furthermore, even in an oracle scenario, where we have access to the true labels during data selection, existing methods are far from optimal. We posit that the primary reason for this inconsistent performance is that while existing methods consider uncertainty in predictions, they do not consider the direction of the uncertainty with respect to the output labels. For instance, in Figure 1 w"
2021.tacl-1.1,H01-1035,0,0.191632,"Missing"
2021.tacl-1.1,D08-1063,0,0.0194508,"θpst that does not look at either the left context or the current token. The token representations ct for each module can be seen as follows: Using the architecture described above, for any given target language we first train a POS model on a group of related high-resource languages. We then fine-tune this pre-trained model on the newly acquired annotations on the target language, as obtained from an AL method. The objective of cross-lingual transfer learning is to warm-start the POS model on the target language. Several methods have been proposed in the past including annotation projection (Zitouni and Florian, 2008), and model transfer using pre-trained models such as m-BERT (Devlin et al., 2019). In this work our primary focus is on designing an active learning method, so we simply pre-train a POS model on a group of related high-resource languages (Cotterell and Heigold, 2017), which is a computationally cheap solution, a crucial requirement for running multiple AL iterations. Furthermore, recent work (Siddhant et al., 2020) has shown the advantage of pre-training using a selected set of related languages over a model pre-trained over all available languages. Following this, for a given target language"
C14-1106,D10-1092,0,0.0603452,"big, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-bests from each MT system and features described in Section 2.3. We use 1-grams to 3-grams as n-gram features. 2 3 http://code.google.com/p/nile/ http://code.google.com/p/egret-parser/ 1128 System pbmt hiero f2s BLEU(dev) Original LM applied 0"
C14-1106,P07-2045,0,0.0192459,"effectiveness of our method by performing a manual evaluation over three translation systems, two translation directions, and two evaluation measures. 4.1 Experiment Setup For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as descri"
C14-1106,C04-1072,0,0.0264719,"2 Discriminative Language Models In this section, we first introduce the discriminative LM used in our method. As a target for our analysis, ˆ1, . . . , E ˆ K } of an MT system, and we have input sentences F = {F1 , . . . , FK }, n-best outputs Eˆ = {E reference translations R = {R1 , . . . , RK }. Discriminative LMs define feature vectors ϕ(Ei ) for each ˆ k = {E1 , E2 , . . . , EI }, and calculate inner products w · ϕ(Ei ) as scores. candidate in E To train the weight vector w, we first calculate evaluation scores of all candidates using a sentencelevel evaluation measure EV such as BLEU+1 (Lin and Och, 2004) given the reference sentence Rk . 1125 We choose the sentence with the highest evaluation EV as an oracle Ek∗ . Oracles are chosen for each n-best, and we train w so that the oracle’s score becomes higher than the other candidates. 2.1 Structured Perceptron While there are a number of methods for training discriminative LMs, we follow Roark et al. (2007) in using the structured perceptron as a simple and effective method for LM training. The structured perceptron is a widely used on-line learning method that examines one training instance and updates the weight vector using the difference bet"
C14-1106,P13-4016,1,0.814008,"tion systems, and choose representative n-grams using the proposed method. Then we examine the selected n-grams in context and then compare the result of this analysis. 4 Experiments We evaluate the effectiveness of our method by performing a manual evaluation over three translation systems, two translation directions, and two evaluation measures. 4.1 Experiment Setup For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et"
C14-1106,J03-1002,0,0.00792232,"Setup For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-b"
C14-1106,P03-1021,0,0.0624679,", 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-bests from each MT system and features described in Section 2.3. We us"
C14-1106,P02-1040,0,0.091993,"he size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-bests from each MT system and features described in Section 2.3. We use 1-grams to 3-grams as n-gram fea"
C14-1106,J11-4002,0,0.124693,"analyze a large number of translations to get an overall grasp of the system’s error trends. In addition, many sentences will contain no errors, or only errors from the long tail that are not representative of the system as a whole. On the other hand, if we are able to detect and rank important errors automatically, we will likely be able to find representative errors of the SMT system more efficiently. Previous work has proposed methods for automatic error analysis of MT systems based on automatically separating errors into classes and sorting these classes by frequency (Vilar et al., 2006; Popovic and Ney, 2011). These classes cover common mistakes of MT systems, e.g. conjugation, reordering, word deletion, and insertion. This makes it possible to view overall error trends, but when the goal of analysis is to identify errors to make some concrete improvement to the system, it is often necessary to perform a more focused analysis, looking at actual errors made by a particular language pair or system. We show examples of errors types that are informative, but are language- or task-specific, and not covered by previous methods in Figure 1. In this example, the type given by more standard error typologie"
C14-1106,P09-1054,0,0.0258248,"ge or we reach a fixed iteration limit N . We show the above procedure in Algorithm 1. Algorithm 1 Structured perceptron training of the discriminative LM for n = 1 to N do ˆ ∈ Eˆ do for all E ∗ E ← arg max EV (E) ˆ E∈E ˆ ← arg max w · ϕ(E) E ˆ E∈E ˆ w ← w + ϕ(E ∗ ) − ϕ(E) end for end for 2.2 Learning Sparse Discriminative LMs While the structured perceptron is a simple and effective method for learning discriminative LMs, it also has no bias towards reducing the number of features used in the model. However, if we add a bias towards learning smaller models, we can keep only salient features (Tsuruoka et al., 2009). In our work, we use L1 regularization to add this bias. L1 regularization gives a penalty to w pro∑ portional to the L1 norm ∥w∥1 = i |wi |, pushing a large number of elements in w to 0, so ineffective features are removed from the model. To train L1 regularized discriminative LMs, we use the forward-backward splitting (FOBOS) algorithm proposed by Duchi and Singer (2009). FOBOS splits update and regularization, and lazily calculates the regularization upon using the weight to improve efficiency. 2.3 Features of Discriminative LMs In the LM, we used the following three features: 1. System sc"
C14-1106,vilar-etal-2006-error,0,0.0742926,"Missing"
C14-1161,W13-4016,0,0.324973,"f framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that applying reinforcement learning is effective for construction of cooperative persuasive dialogue systems which use framing. 1 Introduction With the basic technology supporting dialogue systems maturing, there has been more interest in recent years about dialogue systems that move beyond the traditional task-based or chatter bot frameworks. In particular there has been increasing interest in dialogue systems that engage in persuasion or negotiation (Georgila and Traum, 2011; Georgila, 2013; Paruchuri et al., 2009; Heeman, 2009; Mazzotta and de Rosis, 2006; Mazzotta et al., 2007; Nguyen et al., 2007; Guerini et al., 2003). We concern ourselves with cooperative persuasive dialogue systems (Hiraoka et al., 2013), which try to satisfy both the user and system goals. For these types of systems, creating a system policy that both has persuasive power and is able to ensure that the user is satisfied is the key to the system’s success. In recent years, reinforcement learning has gained much attention in the dialogue research community as an approach for automatically learning optimal d"
C14-1161,C10-1086,0,0.0268802,"a et al., 2014). In this paper, dialogue features for the predictive model are calculated at each turn. In addition, persuasion success and user satisfaction are successively calculated at each turn. In contrast, in previous research, the predictive model was constructed with dialogue features calculated at end of the dialogue. Therefore, it is not guaranteed that the predictive model estimates appropriate persuasion success and user satisfaction at each turn. Another reason is that the simulator is not sufficiently accurate to use for reflecting real user’s behavior. Compared to other works (Meguro et al., 2010; Misu et al., 2012), we are using a relatively small sized corpus for training the user simulator. Therefore, the user simulator cannot be trained to accurately imitate real user behavior. Improving the user simulator is an important challenge for future work. 7 Related work There are a number of related works that apply reinforcement learning to persuasion and negotiation dialogue. Georgila and Traum (2011) apply reinforcement learning to negotiation dialogue using user simulators divided into three types representing individualist, collectivist, and altruist. Dialogue between a florist and"
C14-1161,W12-1611,0,0.0952795,"his paper, dialogue features for the predictive model are calculated at each turn. In addition, persuasion success and user satisfaction are successively calculated at each turn. In contrast, in previous research, the predictive model was constructed with dialogue features calculated at end of the dialogue. Therefore, it is not guaranteed that the predictive model estimates appropriate persuasion success and user satisfaction at each turn. Another reason is that the simulator is not sufficiently accurate to use for reflecting real user’s behavior. Compared to other works (Meguro et al., 2010; Misu et al., 2012), we are using a relatively small sized corpus for training the user simulator. Therefore, the user simulator cannot be trained to accurately imitate real user behavior. Improving the user simulator is an important challenge for future work. 7 Related work There are a number of related works that apply reinforcement learning to persuasion and negotiation dialogue. Georgila and Traum (2011) apply reinforcement learning to negotiation dialogue using user simulators divided into three types representing individualist, collectivist, and altruist. Dialogue between a florist and a grocer are assumed"
C14-1161,W12-1639,0,0.024996,"dialogue acts is added at each turn. 3 Preliminary experiments indicated that the user behaved differently depending on the first selection of the camera, thus we introduce this variable to the user simulator. 4 We also optimized the policy in the case where the reward (Equation (3)) is given only when dialogue is closed. However, the convergence of the learning was much longer, and the performance was relatively bad. 5 Originally, there are more dialogue features for the predictive model. However as in previous research, we choose significant dialogue features by step-wise feature selection (Terrell and Bilge, 2012). 1710 Table 4: Features for calculating reward. These features are also used as the system belief state. Satuser P Ssys N Table 5: System framing. Pos represents positive framing and Neg represents negative framing. A, B, C, D, E represent camera names. Pos A Neg A Frequency of system commisive Frequency of system question Total time Calt (for each 6 cameras) Salt (for each 6 cameras) System and user current GPF System and user previous GPF System framing Pos B Neg B Pos C Neg C Pos D Neg D Pos E Neg E None Table 6: System action. <None, ReleaseTurn&gt; <Pos A, Inform&gt; <Neg A, Inform&gt; <Pos B, An"
C16-1292,W05-0909,0,0.0342364,"Q(true) := wi yi (1) i∈ALL Note that this definition does not aim to handle document-level discourse phenomena such as coherence, cohesion, and and consistency, but estimates the average sentence-level quality for the document in order to evaluate the overall quality of the whole output hypothesis. In this paper, we weigh segments proportionally to their length, although future work may investigate more sophisticated notions of segment importance. Our definition of document quality is simplistic, but widely used in both MT and ASR communities, e.g. document- or corpus-level WER, TER, METEOR (Banerjee and Lavie, 2005), and human rankings are usually computed this way. BLEU is computed on the corpus level and thus not directly usable with our approach, but we can instead resort to computing average sentence-level BLEU variants such as BLEU+1 (Lin and Och, 2004) that essentially differ only in the smoothing details. Our lightly supervised estimation framework determines document quality in several steps. The first step is to automatically estimate the quality for each segment in the hypothesis (§4). This can be achieved by training a regressor with the desired target measure, as discussed in numerous previou"
C16-1292,P13-2097,0,0.027685,"n found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). The WMT 2015 shared task on QE considered only the scenario of training and testing on output produced by the same system (Bojar et al., 2015). The usual way to address domain mismatch when training data for all domains is available is via adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold standard “testing” labels occasionally (Joglekar and Garcia-Molina, 2013), and/or automatically predicting quality (Roy et al., 2010; Gao et al., 2015). Our work can be seen as a generalization of the latter two, with the gold labels corresponding"
C16-1292,W14-3338,0,0.0219279,"information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). The WMT 2015 shared task on QE considered only the scenario of training and testing on output produced by the same system (Bojar et al., 2015). The usual way to address domain mismatch when training data for all domains is available is via adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold stan"
C16-1292,P07-1033,0,0.0452183,"Missing"
C16-1292,P15-1022,0,0.0463442,"Missing"
C16-1292,N15-1073,0,0.0557124,"Missing"
C16-1292,P15-1174,0,0.0243,"lly automatic baseline (§3.1). For comparison, we evaluate a mean-predictor baseline that always predicts the training mean, regardless of the input features. This baseline has been found surprisingly strong previously (Negri et al., 2014; Specia et al., 2015), which we confirm in Table 2. On segment-level, gains over the mean-predictor baseline are clearly visible only for the ASR setting. As expected, the out-of-domain tasks appear much more difficult than the in-domain setting. Note that even though the mean baseline sometimes achieves lower MAE, the XT regressor maintains the advantages 3 Graham (2015) argues that correlation is better for evaluating sentence-level QE, because MAE can be improved by transformation to match estimated global mean and variance. However, we find MAE more indicative for our purpose as it measures not only how well systems are compared against one another, but also how well overall quality is judged in absolute terms. Moreover, collecting global statistics for transformation seems problematic when flexibility for domain changes is required. 4 Tuning directly for MAE yielded similar results. 3108 MT.in-domain MT.out-of-domain ASR.in-domain ASR.out-of-domain ↓MAE m"
C16-1292,W04-3250,0,0.323366,"Missing"
C16-1292,C04-1072,0,0.326451,"verall quality of the whole output hypothesis. In this paper, we weigh segments proportionally to their length, although future work may investigate more sophisticated notions of segment importance. Our definition of document quality is simplistic, but widely used in both MT and ASR communities, e.g. document- or corpus-level WER, TER, METEOR (Banerjee and Lavie, 2005), and human rankings are usually computed this way. BLEU is computed on the corpus level and thus not directly usable with our approach, but we can instead resort to computing average sentence-level BLEU variants such as BLEU+1 (Lin and Och, 2004) that essentially differ only in the smoothing details. Our lightly supervised estimation framework determines document quality in several steps. The first step is to automatically estimate the quality for each segment in the hypothesis (§4). This can be achieved by training a regressor with the desired target measure, as discussed in numerous previous works. The 3104 second step is then to manually annotate the quality score for a certain number of segments. In our evaluation (§5), we experiment with typical amounts of tens to hundreds of annotated words. The final step is to aggregate manual"
C16-1292,C14-1171,0,0.0494527,"Missing"
C16-1292,P02-1040,0,0.103668,"chnology, enabling users and engineers to judge overall quality of the output, detect key problems, improve systems, and choose among competing systems. Although most users and engineers share these goals, the chosen evaluation approaches can differ strongly, with some people resorting to automatic, reference-based evaluation, while others rely on manual evaluation for their purposes. This is especially pronounced in the case of machine translation (MT), as pointed out by Harris et al. (2016). On one hand, much research effort has been devoted to devising reference-based methods such as BLEU (Papineni et al., 2002) that are well-correlated with human judgment. On the other hand, practitioners need to react to changing domains from customer to customer, and reflect multi-faceted quality requirements that are difficult to measure in a single, generic score, often leaving manual evaluation as the only choice. In recent years, automatic quality estimation (QE) has emerged as a method that could potentially address the lack of flexibility of reference-based evaluation to deal with changing requirements, and the high effort of manual evaluation. Automatic QE uses machine learning techniques that are trained o"
C16-1292,Q14-1025,0,0.0302706,"ia adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold standard “testing” labels occasionally (Joglekar and Garcia-Molina, 2013), and/or automatically predicting quality (Roy et al., 2010; Gao et al., 2015). Our work can be seen as a generalization of the latter two, with the gold labels corresponding to our fully manual baseline, the automatic estimation corresponding to our fully automatic baseline, and the workers being our systems. 7 Conclusion We proposed lightly supervised quality estimation at the document level, a framework that allows flexible quality estimation across changing domains and quality requirements, while requi"
C16-1292,2014.eamt-1.21,0,0.0283169,"ther or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). T"
C16-1292,W15-4916,0,0.0378992,"Missing"
C16-1292,P10-1063,0,0.019873,"ar observations for MT, and we expect these findings to hold for other datasets. We also confirmed that results are similar when using BLEU+1 (Lin and Och, 2004) as the metric, instead of TER. Whether or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-doma"
C16-1292,W12-3121,0,0.0227726,"n using BLEU+1 (Lin and Och, 2004) as the metric, instead of TER. Whether or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challengin"
C16-1292,P13-4014,0,0.0470002,"Missing"
C16-1292,P15-4020,0,0.102177,"tor who replicates these exactly. Our main evaluation measure is mean absolute error (MAE)3 between the predicted and true document-level TER/WER. We use 5 datasets as indicated in Table 1, with testing data varying over all datasets, but only the ones labeled “in-domain” used for regressor training. Moreover, for each evaluated document the QE regressor was retrained with training data excluding that which corresponded to the same system or document currently tested (for in-domain tests). This makes even our in-domain scenario more challenging than some of the previous works on automatic QE (Specia et al., 2015). We use scikit-learn (Pedregosa et al., 2011) for regressor training. We assign weights to training samples proportional to their segment length, because longer segments are weighted more strongly in our aggregation strategies (§3) and are thus more important to be accurately predicted. We perform random search with 20 iterations to optimize hyper-parameters (namely, the max-depth and min-samplessplit parameters of XTs) in terms of mean squared error.4 Tuning is conducted separately for every test document, using 10-fold cross validation on the respective training data. For regressor adaptati"
C16-1292,2015.eamt-1.17,0,\N,Missing
C18-1198,D17-1263,0,0.0599234,"Missing"
C18-1198,D17-1215,0,0.145006,"Missing"
C18-1198,P17-1015,0,0.0422534,"Missing"
C18-1198,P11-2057,0,0.0357943,"Missing"
C18-1198,W17-5405,0,0.0695799,"Missing"
C18-1198,W07-1407,0,0.140553,"Missing"
C18-1198,S14-2001,0,0.0573409,"Missing"
C18-1198,marelli-etal-2014-sick,0,0.137032,"Missing"
C18-1198,W17-5301,0,0.0529109,"Missing"
C18-1198,W17-5308,0,0.0824878,"Missing"
C18-1198,S18-2023,0,0.15601,"Missing"
C18-1198,D09-1085,0,0.13395,"Missing"
C18-1198,E06-1052,0,0.0638059,"Missing"
C18-1198,W17-5402,0,0.0661211,"Missing"
C18-1198,W17-5410,0,0.0813283,"Missing"
C18-1198,H05-1049,0,\N,Missing
C18-1198,P06-1114,0,\N,Missing
C18-1198,W07-1401,0,\N,Missing
C18-1198,W17-5307,0,\N,Missing
C18-1198,W17-4705,0,\N,Missing
C18-1198,N18-2017,0,\N,Missing
D12-1077,W05-0909,0,0.0281214,"dition is true, and 0 otherwise. An example of this is given in Figure 2 (b). To calculate an accuracy measure for ordering F 0 , we ﬁrst calculate the maximum loss for the sentence, which is equal to the total number of non-equal rank comparisons in the sentence5 F J−1 ∑ Lt (F 0 ) , max Lt (F˜ 0 ) F˜ 0 which will take a value between 0 (when F 0 has maximal loss), and 1 (when F 0 matches one of the oracle orderings). In Figure 2 (b), Lt (F 0 ) = 2 and max Lt (F˜ 0 ) = 8, so At (F 0 ) = 0.75. J ∑ 4.3 Chunk Fragmentation Another measure that has been used in evaluation of translation accuracy (Banerjee and Lavie, 2005) and pre-ordering accuracy (Talbot et al., 2011) is chunk fragmentation. This measure is based on the number of chunks that the sentence needs to be broken into to reproduce the correct ordering, with a motivation that the number of continuous chunks is equal to the number of times the reader will have to jump to a diﬀerent position in the reordered sentence to read it in the target order. One way to measure the number of continuous chunks is considering 0 whether each word pair fj0 and fj+1 is discon0 tinuous (the rank of fj+1 is not equal to or one greater than fj0 ) 0 discont(fj0 , fj+1 )="
D12-1077,P10-2033,0,0.013484,"ansforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual p"
D12-1077,J07-2003,0,0.86214,"ng framework results in signiﬁcant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most diﬃcult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ord"
D12-1077,P11-2031,0,0.00445512,"xperiments Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F 0 and oracle F 0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ , but evaluated on the target sentence E instead of the reordered sentence F 0 . All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types orig 3-step 3-step+φpos 3-step+φcf g lader lader+φpos lader+φcf g Chunk 61.22 63.51 64.28 65.76 73.19 73.97 75.06 en-ja τ BLEU 73.46 21.87 72.55 21.45 72.11 21.45 75.32 21.67 78.44 23.11 79.24 23.32 80.53 23.36 RIBES 68.25 67.66 67.44 68.47 69.86 69.78 70.89 Chunk 66.42 67.17 67.56 67.23 75.14 75.49 75.14 ja-en τ BLEU 72.99 18.34 73.01 17.78 74.21 18.18 74.06 18.18 79.14 19.54 78.79 19.89 77.80 19.35 RIBES 65.36 64.42 64.65 64.93 66.93 67.24 66.12 Table"
D12-1077,P05-1066,0,0.768146,"in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual gram"
D12-1077,W02-1001,0,0.0148034,"ions by selecting the derivation with the largest model score. From an implementation point of view, this can be done by ﬁnding the derivation that minimizes L(Dk |Fk , Ak ) − αS(Dk |Fk , w), where α is a constant small enough to ensure that the eﬀect of the loss will always be greater than the eﬀect of the score. Finally, if the model parse D˙ k has a loss that ˆ k , we is greater than that of the oracle parse D update the weights to increase the score of the oracle parse and decrease the score of the model parse. Any criterion for weight updates may be used, such as the averaged perceptron (Collins, 2002) and MIRA (Crammer et al., 2006), but we opted to use Pegasos (Shalev-Shwartz et al., 2007) as it allows for the introduction of regularization and relatively stable learning. To perform this full process, given a source sentence Fk , alignment Ak , and model weights w we need to be able to eﬃciently calculate scores, calculate losses, and create parse forests for derivations Dk , the details of which will be explained in the following sections. 5.2 Scoring Derivation Trees First, we must consider how to eﬃciently assign scores S(D|F, w) to a derivation or forest during parsing. The most stand"
D12-1077,D11-1018,0,0.622387,"les the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system. In this work, we present a method for inducing a parser for SMT by training a discriminative model to maximize reordering accuracy while treating the parse tree as a latent variable. As a learning framework, we use online large-margin methods to train the model to directly minimize two measures of reordering accuracy. We propose a variety of features, and demonstrate that learning can succeed when no linguistic informati"
D12-1077,N10-1128,0,0.0575155,"ring ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero"
D12-1077,I11-1087,1,0.815752,"n, 2004). RM-train RM-test TM/LM Tune Test sent. 602 555 329k 1166 1160 word (ja) 14.5k 11.2k 6.08M 26.8k 28.5k word (en) 14.3k 10.4k 5.91M 24.3k 26.7k Table 1: The number of sentences and words for training and testing the reordering model (RM), translation model (TM), and language model (LM). except φpos and φcf g . In addition, we test systems with φpos and φcf g added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse. 6.1 Eﬀect of Pre-ordering of pre-ordering: original order with F 0 ← F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3step), and the proposed model with latent derivations (lader).7 Except when stated otherwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10−3 (chosen through cross-validation). We test our systems on Japanese-English and English-Japanese translation using data from the Kyot"
D12-1077,C10-1043,0,0.357752,"ure 1). Reordering ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our a"
D12-1077,P09-1104,0,0.0189689,"urthermore, we assume that the score S(D|F ) is the weighted sum of a number of feature functions deﬁned over D and F ∑ S(D|F, w) = wi φi (D, F ) i where φi is the ith feature function, and wi is its corresponding weight in weight vector w. Given this model, we must next consider how to learn the weights w. As the ﬁnal goal of our model is to produce good reorderings F 0 , it is natural to attempt to learn weights that will allow us to produce these high-quality reorderings. 3 BTGs cannot reproduce all possible reorderings, but can handle most reorderings occurring in natural translated text (Haghighi et al., 2009). 845 Figure 2: An example of (a) the ranking function r(fj ), (b) loss according to Kendall’s τ , (c) loss according to chunk fragmentation. 4 Evaluating Reorderings Before we explain the learning algorithm, we must know how to distinguish whether the F 0 produced by the model is good or bad. This section explains how to calculate oracle reorderings, and assign each F 0 a loss and an accuracy according to how well it reproduces the oracle. 4.1 Calculating Oracle Orderings In order to calculate reordering quality, we ﬁrst deﬁne a ranking function r(fj |F, A), which indicates the relative posit"
D12-1077,D10-1092,0,0.110239,"Missing"
D12-1077,W10-1736,0,0.679774,"ing a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the"
D12-1077,D11-1017,0,0.0374907,"cketing transduction grammar (BTG, Wu (1997)) framework. BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1. Each non-terminal node can either be a straight (str) or inverted (inv) production, and terminals (term) span a nonempty substring f .2 The ordering of the sentence is determined by the tree structure and the non-terminal labels str and inv, and can be built bottom-up. Each subtree represents a source substring f and its reordered counterpart f 0 . For each terminal node, no reordering occurs and f is equal to f 0 . 1 The semi-supervised method of Katz-Brown et al. (2011) also optimizes reordering accuracy, but requires manually annotated parses as seed data. 2 In the original BTG framework used in translation, terminals produce a bilingual substring pair f /e, but as we are only interested in reordering the source F , we simplify the model by removing the target substring e. For each non-terminal node spanning f with its left child spanning f 1 and its right child spanning f 2 , if the non-terminal symbol is str, the reordered strings will be concatenated in order as f 0 = f 01 f 02 , and if the non-terminal symbol is inv, the reordered strings will be concat"
D12-1077,I11-1005,0,0.255321,"Missing"
D12-1077,P03-1054,0,0.00645291,"dering (chunk, τ ) and translation (BLEU, RIBES) results for each system. Bold numbers indicate no signiﬁcant diﬀerence from the best system (bootstrap resampling with p > 0.05) (Koehn, 2004). RM-train RM-test TM/LM Tune Test sent. 602 555 329k 1166 1160 word (ja) 14.5k 11.2k 6.08M 26.8k 28.5k word (en) 14.3k 10.4k 5.91M 24.3k 26.7k Table 1: The number of sentences and words for training and testing the reordering model (RM), translation model (TM), and language model (LM). except φpos and φcf g . In addition, we test systems with φpos and φcf g added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse. 6.1 Eﬀect of Pre-ordering of pre-ordering: original order with F 0 ← F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3step), and the proposed model with latent derivations (lader).7 Except when stated otherwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limi"
D12-1077,N03-1017,0,0.0461607,"012. 2012 Association for Computational Linguistics Figure 1: An example with a source sentence F reordered into target order F 0 , and its corresponding target sentence E. D is one of the BTG derivations that can produce this ordering. the pre-ordering approach to machine translation (Xia and McCord, 2004), which performs translation as a two step process of reordering and translation (Figure 1). Reordering ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for"
D12-1077,2005.iwslt-1.8,0,0.165383,"ored over the parse tree. Using this model in the pre-ordering framework results in signiﬁcant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most diﬃcult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse s"
D12-1077,P07-2045,0,0.00429164,"ranslation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F 0 and oracle F 0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ , but evaluated on the target sentence E instead of the reordered sentence F 0 . All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types orig 3-step 3-step+φpos 3-step+φcf g lader lader+φpos lader+φcf g Chunk 61.22 63.51 64.28 65.76 73.19 73.97 75.06 en-ja τ BLEU 73.46 21.87 72.55 21.45 72.11 21.45 75.32 21.67 78.44 23.11 79.24 23.32 80.53 23.36 RIBES 68.25 67.66 67.44 68.47 69.86 69.78 70.89 Chunk 66.42 67.17 67.56 67.23 75.14 75.49 75.14 ja-en τ BLEU 72.99 18.34 73.01 17.78 74.21 18.18 74.06 18.18 79.14 19.54 78.79 19.89 77.80 19.35 RIBES 65.36 64.42 64.65 64.93 66.93 67.24 66.12 Table 2: Reordering (chunk, τ ) and translation (BLEU, RIB"
D12-1077,W04-3250,0,0.117398,"Missing"
D12-1077,P07-1091,0,0.20119,"translation (Figure 1). Reordering ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being si"
D12-1077,P06-1096,0,0.0852397,"Missing"
D12-1077,P11-2093,1,0.714814,"niﬁcant diﬀerence from the best system (bootstrap resampling with p > 0.05) (Koehn, 2004). RM-train RM-test TM/LM Tune Test sent. 602 555 329k 1166 1160 word (ja) 14.5k 11.2k 6.08M 26.8k 28.5k word (en) 14.3k 10.4k 5.91M 24.3k 26.7k Table 1: The number of sentences and words for training and testing the reordering model (RM), translation model (TM), and language model (LM). except φpos and φcf g . In addition, we test systems with φpos and φcf g added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse. 6.1 Eﬀect of Pre-ordering of pre-ordering: original order with F 0 ← F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3step), and the proposed model with latent derivations (lader).7 Except when stated otherwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10−3 (chosen through cross-validation). We test our sy"
D12-1077,E99-1010,0,0.0604002,"term), while l and r are the leftmost and rightmost indices of the span that d covers. c and c + 1 are the rightmost index of the left child and leftmost index of the right child for non-terminal nodes. All features are intersected with the node label s, so each feature described below corresponds to three diﬀerent features (or two for features applicable to only non-terminal nodes). • φlex : Identities of words in positions fl , fr , fc , fc+1 , fl−1 , fr+1 , fl fr , and fc fc+1 . • φclass : Same as φlex , but with words abstracted to classes. We use the 50 classes automatically generated by Och (1999)’s method that are calculated during alignment in standard SMT systems. • φbalance : For non-terminals, features indicating whether the length of the left span 848 (c − l + 1) is lesser than, equal to, or greater than the length of the right span (r − c). • φtable : Features, bucketed by length, that indicate whether “fl . . . fr ” appears as a contiguous phrase in the SMT training data, as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011). Phrase length is limited to 8, and phrases o"
D12-1077,P02-1040,0,0.101341,"easonable burden in terms of time and memory. To overcome this problem, we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM, and thus we can employ cube-pruning to reduce our search space (Chiang, 2007). 6 Experiments Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F 0 and oracle F 0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ , but evaluated on the target sentence E instead of the reordered sentence F 0 . All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types orig 3-step 3-step+φpos 3-step+φcf g lader lader+φpos lader+φcf g Chunk 61.22 63.51 64.28 65.76 73.19 73.97 75.06 en-ja τ BLEU 73.46 21.87 72.55 21.45 72.11 21.45 75.32 21.67 78.44 23.11 79.24"
D12-1077,2007.tmi-papers.21,0,0.0861613,"works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training 844 a reordering model that selects a reordering based on this parse structure. In contrast, our method trains the model in a single step, treating the parse structure as a latent vari"
D12-1077,2011.mtsummit-papers.36,0,0.0181648,"s for systems trained to optimize chunk fragmentation (Lc ) or Kendall’s τ (Lt ). that spanned constituent boundaries (as long as the phrase frequency was high). Finally, as Section 6.2 shows in detail, the ability of lader to maximize reordering accuracy directly allows for improved reordering and translation results. It can also be seen that incorporating POS tags or parse trees improves accuracy of both lader and 3-step, particularly for EnglishJapanese, where syntax has proven useful for pre-ordering, and less so for Japanese-English, where syntactic pre-ordering has been less successful (Sudoh et al., 2011b). We also tested Moses’s implementation of hierarchical phrase-based SMT (Chiang, 2007), which achieved BLEU scores of 23.21 and 19.30 for English-Japanese and Japanese-English respectively, approximately matching lader in accuracy, but with a signiﬁcant decrease in decoding speed. Further, when pre-ordering with lader and hierarchical phrase-based SMT were combined, BLEU scores rose to 23.29 and 19.69, indicating that the two techniques can be combined for further accuracy improvements. 6.2 Eﬀect of Training Loss Table 3 shows results when one of three losses is optimized during training: c"
D12-1077,W11-2102,0,0.294296,"d fj2 are assigned the same rank. We can now deﬁne measures of reordering accuracy for F 0 by how well it arranges the words in order of ascending rank. It should be noted that as we allow ties in rank, there are multiple possible F 0 where all words are in strictly ascending order, which we will call oracle orderings. 4.2 The ﬁrst measure of reordering accuracy that we will consider is Kendall’s τ (Kendall, 1938), a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy (Isozaki et al., 2010a; Birch et al., 2010) and pre-ordering accuracy (Talbot et al., 2011). The fundamental idea behind the measure lies in comparisons between each pair of elements fj01 and fj02 of the reordered sentence, where j1 < j2 . Because j1 < j2 , fj01 comes before fj02 in the reordered sentence, the ranks should be r(fj01 ) ≤ r(fj02 ) in order to produce the correct ordering. Based on this criterion, we ﬁrst deﬁne a loss Lt (F 0 ) that will be higher for orderings that are further from the oracle. Speciﬁcally, we take the sum of all pairwise orderings that do not follow the expected order Lt (F ) = J−1 ∑ J ∑ where δ(·) is an indicator function that is 1 when its condition"
D12-1077,D09-1105,0,0.607314,"ic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training 844 a reordering model that selects a reordering based on this parse structure. In contrast, our method trains the model in a single step, treating the parse structure as a latent variable in a discriminative r"
D12-1077,D07-1080,1,0.843185,"1, which corresponds to the total number of comparisons made in calculating the loss6 Ac (F 0 ) = 1 − Lc (F 0 ) . J +1 In Figure 2 (c), Lc (F 0 ) = 3 and J + 1 = 6, so Ac (F 0 ) = 0.5. 5 Learning a BTG Parser for Reordering Now that we have a deﬁnition of loss over reorderings produced by the model, we have a clear learning objective: we would like to ﬁnd reorderings F 0 with low loss. The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al., 2006), and extended to use large-margin training in an online framework (Watanabe et al., 2007). 5.1 Learning Algorithm Learning uses the general framework of largemargin online structured prediction (Crammer et al., 2006), which makes several passes through the data, ﬁnding a derivation with high model score (the model parse) and a derivation with 6 It should be noted that for sentences of length one or sentences with tied ranks, the maximum loss may be less than J + 1, but for simplicity we use this approximation. 847 minimal loss (the oracle parse), and updating w if these two parses diverge (Figure 3). In order to create both of these parses eﬃciently, we ﬁrst create a parse forest"
D12-1077,J97-3002,0,0.73298,"ization, including features such as those that utilize the existence of a span in the phrase table. Our work is also unique in that we show that it is possible to directly optimize several measures of reordering accuracy, which proves important for achieving good translations.1 3 Training a Reordering Model with Latent Derivations In this section, we provide a basic overview of the proposed method for learning a reordering model with latent derivations using online discriminative learning. 3.1 Space of Reorderings The model we present here is based on the bracketing transduction grammar (BTG, Wu (1997)) framework. BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1. Each non-terminal node can either be a straight (str) or inverted (inv) production, and terminals (term) span a nonempty substring f .2 The ordering of the sentence is determined by the tree structure and the non-terminal labels str and inv, and can be built bottom-up. Each subtree represents a source substring f and its reordered counterpart f 0 . For each terminal node, no reordering occurs and f is equal to f 0 . 1 The semi-supervised method of Katz-Brown et al. (2011) also optimizes reor"
D12-1077,C04-1073,0,0.932492,"sebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most diﬃcult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system. In this work, we present a method for inducing a parser for"
D12-1077,N09-1028,0,0.450117,"anslation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, tr"
D12-1077,P01-1067,0,0.229152,"in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most diﬃcult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system. In this work, we pre"
D12-1077,W07-0401,0,0.0103136,"ss of reordering and translation (Figure 1). Reordering ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve men"
D12-1077,W06-3119,0,0.0728907,"as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011). Phrase length is limited to 8, and phrases of frequency one are removed. • φpos : Same as φlex , but with words abstracted to language-dependent POS tags. • φcf g : Features indicating the label of the spans fl . . . fr , fl . . . fc , and fc+1 . . . fr in a supervised parse tree, and the intersection of the three labels. When spans do not correspond to a span in the supervised parse tree, we indicate “no span” with the label “X” (Zollmann and Venugopal, 2006). Most of these features can be calculated from only a parallel corpus, but φpos requires a POS tagger and φcf g requires a full syntactic parser in the source language. As it is preferable to have a method that is applicable in languages where these tools are not available, we perform experiments both with and without the features that require linguistic analysis tools. 5.3 Finding Losses for Derivation Trees The above features φ and their corresponding weights w are all that are needed to calculate scores of derivation trees at test time. However, during training, it is also necessary to ﬁnd"
D12-1077,D11-1045,0,\N,Missing
D15-1250,N15-1027,0,0.0153685,"sh training data from noise by maximize the conditional likelihood, L = log P (v = 1|C, ti ) + k P j=1 log P (v = 0|C, tik ). The normalization cost can be avoided by using p (ti |C) as an approximation of P (ti |C).2 1 If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word. 2 The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)’s method, are investigated by Andreas and Klein (2015). 3 Binarized NNJM In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classification problem by adding a variable v ∈ {0, 1} that stands for whether the current target word ti is correctly/wrongly produced in terms of source cona +(m−1)/2 text words saii −(m−1)/2 and target history words i−1 ti−n+1 ,   a +(m−1)/2 P v|saii −(m−1)/2 , ti−1 i−n+1 , ti . The BNNJM is learned by a feedforward neural network o with m + n inputs n ai +(m−1)/2 i−1 sai −(m−1)/2 , ti−n+1 ,"
D15-1250,D13-1106,0,0.0273384,"classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost dur"
D15-1250,P14-1129,0,0.0837573,"Missing"
D15-1250,E14-1003,0,0.0168384,"native to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to"
D15-1250,P07-2045,0,0.00974293,"9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples"
D15-1250,P02-1040,0,0.0928294,"tion task, as it did for the other translation tasks. We found that using the BNNJM instead of the NNJM on the JE task did improve translation quality significantly for infrequent words, but not for frequent words. First, we describe how we estimate translation quality for infrequent words. Suppose we have a test set S, a reference set R and a translation set T with I sentences, Si (1 ≤ i ≤ I) , Ri (1 ≤ i ≤ I) , Ti (1 ≤ i ≤ I) Ti contains J individual words, To (Wij ) is how many times Wij occurs in Ti and Ro (Wij ) is how many times Wij occurs in Ri . The general 1-gram translation accuracy (Papineni et al., 2002) is calculated as, I P J P Pg = i=1 j=1 min(To (Wij ),Ro (Wij )) I P J P To (Wij ) i=1 j=1 This general 1-gram translation accuracy does not distinguish word frequency. We use a modified 1-gram translation accuracy that weights infrequent words more heavily, I P J P Pc = i=1 j=1 min(To (Wij ),Ro (Wij ))· I P J P i=1 j=1 1 Occur Wij ( ) To (Wij ) where Occur (Wij ) is how many times Wij occurs in the whole reference set. Note Pc will not be 1 even in the case of completely accurate translations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word t"
D15-1250,C12-2104,0,0.0869208,"kes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. H"
D15-1250,D14-1003,0,0.0152779,"JM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive nor"
D15-1250,D13-1140,0,0.562022,"t of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. However, they also note that this self-normalization technique sacrifices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE). To remedy the problem of long training times in the context of NNLMs, Vaswani et al. (2013) used a method called noise contrastive estimation (NCE). Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise. This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is correct or not, as shown in Figure 1b. Because the BNNJM uses the current target word"
D15-1250,D11-1104,0,0.0257476,"g 68.2 68.4 0.29 JE Pc 4.15 4.30 3.6 Pg 61.2 61.7 0.81 FE Pc 6.70 6.86 2.4 Table 5: 1-gram precisions and improvements. grammatical features of Japanese and English are quite different.8 Wrong function word alignments will make noise sampling less effective and therefore lower the BNNJM performance for function word translations. Although wrong word alignments will also make noise sampling less effective for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM. 6 Wij ∈ W ords (Ti ) Pg 70.3 70.9 0.85 Related Work Xu et al. (2011) proposed a method to use binary classifiers to learn NNLMs. But they also used the current target word in the output, similarly to NCE. The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word information and processed in hidden layers. Mauser et al. (2009) presented discriminative lexicon models to predict target words. They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share features. In contrast, the BNNJM uses rea"
D15-1250,W06-0127,0,0.144311,"P align(sai ,ti 0 ) ti 00 ∈U (sai ) align(sai ,ti Experiments We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE), Japanese-toEnglish (JE) and French-to-English (FE) translation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The wo"
D15-1250,W04-3250,0,0.0992848,"(E) and time (T) in minutes per epoch for each task. Base NNJM BNNJM UPD TPD UPD TPD CE 32.95 34.36+ 34.60+ 32.89 35.05+* JE 30.13 31.30+ 31.50+ 30.04 31.42+ FE 24.56 24.68 24.80 24.50 25.84+* Table 3: Translation examples. Here, S: source; R: reference; T1 uses NNJM; T2 uses BNNJM. 该− &gt;the 移动− &gt;mobile 持续− &gt;continues 到− &gt;to SUM 该− &gt;this 移动− &gt;movement null− &gt;is 持续− &gt;continued 到− &gt;until SUM Table 2: Translation results. The symbol + and * represent significant differences at the p &lt; 0.01 level against Base and NNJM+UPD, respectively. Significance tests were conducted using bootstrap resampling (Koehn, 2004). the whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive. However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples. 5.2 Results and Discussion Table 1 shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine.7 Translation results are shown in Table 2. We can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the t"
D15-1250,D09-1022,0,0.0791321,"Missing"
D15-1250,P03-1021,0,0.105131,"Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples for NCE was set to be 100. For the BNNJM, we used only one negative example for each positive example in each training epoch, as the BNNJM needs to calculate 3 00 ) where align (sai , ti 0 ) is how many times ti 0 is aligned to sai in the parallel corpus. Note that ti could be unaligned, in"
D16-1124,P14-2023,0,0.0264448,"ons of a standard neural model (or δ distributions) trained on PTB, and count based distributions trained on PTB, WSJ, and GW are added one-by-one using the standard static and proposed LSTM interpolation methods. From the results, we can see that when only PTB data is used, the methods have similar results, but with the more diverse data sets the proposed method edges out its static counterpart.7 7 In addition to better perplexities, neural/n-gram hybrids are trained in a single pass instead of performing post-facto interpolation, which may give advantages when training for other objectives (Auli and Gao, 2014; Li et al., 2015). Interp Lin. LSTM δ+PTB 95.1 95.3 +WSJ 70.5 68.3 +GW 65.8 63.5 Table 4: PTB perplexity for interpolation between neural (δ) LMs and count-based models. 7 Related Work Acknowledgements A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G¨ulc¸ehre et al., 2015). Perhaps most relevant is Hsu (2007)’s work on learning to interpolate multiple LMs using log-linear models. This differs from our work in that it learns functions to estimate the fallback probabilities αn"
D16-1124,N15-1083,0,0.0194106,"999) and adapt them based on the distribution of the current document, albeit in a linear model. There has also been work incorporating binary n-gram features into neural language models, which allows for more direct learning of ngram weights (Mikolov et al., 2011), but does not afford many of the advantages of the proposed model such as the incorporation of count-based probability estimates. Finally, recent works have compared ngram and neural models, finding that neural models often perform better in perplexity, but n-grams have their own advantages such as effectiveness in extrinsic tasks (Baltescu and Blunsom, 2015) and better modeling of rare words (Chen et al., 2015). 8 neural machine translation (Sutskever et al., 2014) or dialog response generation (Sordoni et al., 2015). In addition, given the positive results using block dropout for hybrid models, we plan to develop more effective learning methods for mixtures of sparse and dense distributions. Conclusion and Future Work In this paper, we proposed a framework for language modeling that generalizes both neural network and count-based n-gram LMs. This allowed us to learn more effective interpolation functions for count-based n-grams, and to create ne"
D16-1124,D07-1090,0,0.0149178,"Dyer‡ † Carnegie Mellon University, USA ‡ Google DeepMind, United Kingdom Abstract and Goodman, 1996). Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs. On the other hand, these neural LMs also come at the cost of increased computational complexity at both training and test time, and even the largest reported neural LMs (Chen et al., 2015; Williams et al., 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al., 2007). In this paper we focus on a class of LMs, which we will call mixture of distributions LMs (MODLMs; §2). Specifically, we define MODLMs as all LMs that take the following form, calculating the probabilities of the next word in a sentence wi given preceding context c according to a mixture of several component probability distributions Pk (wi |c): Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability an"
D16-1124,N03-2003,0,0.0524999,"ts the proposed method edges out its static counterpart.7 7 In addition to better perplexities, neural/n-gram hybrids are trained in a single pass instead of performing post-facto interpolation, which may give advantages when training for other objectives (Auli and Gao, 2014; Li et al., 2015). Interp Lin. LSTM δ+PTB 95.1 95.3 +WSJ 70.5 68.3 +GW 65.8 63.5 Table 4: PTB perplexity for interpolation between neural (δ) LMs and count-based models. 7 Related Work Acknowledgements A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G¨ulc¸ehre et al., 2015). Perhaps most relevant is Hsu (2007)’s work on learning to interpolate multiple LMs using log-linear models. This differs from our work in that it learns functions to estimate the fallback probabilities αn (c) in Eq. 3 instead of λ(c), and does not cover interpolation of n-gram components, non-linearities, or the connection with neural network LMs. Also conceptually similar is work on adaptation of n-gram LMs, which start with n-gram probabilities (Della Pietra et al., 1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996; Iyer and Ostendorf, 1999"
D16-1124,P96-1041,0,0.606559,"dly used language modeling paradigm is that of count-based LMs, usually smoothed n-grams (Witten and Bell, 1991; Chen 1 Work was performed while GN was at the Nara Institute of Science and Technology and CD was at Carnegie Mellon University. Code and data to reproduce experiments is available at http://github.com/neubig/modlm P (wi |c) = K X k=1 λk (c)Pk (wi |c). (1) Here, λk (c) is a function that defines P the mixture weights, with the constraint that K k=1 λk (c) = 1 for all c. This form is not new in itself, and widely used both in the calculation of smoothing coefficients for n-gram LMs (Chen and Goodman, 1996), and interpolation of LMs of various varieties (Jelinek and Mercer, 1980). The main contribution of this paper is to demonstrate that depending on our definition of c, λk (c), and Pk (wi |c), Eq. 1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and 1163 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1163–1172, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network"
D16-1124,H92-1020,0,0.479706,"ing LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G¨ulc¸ehre et al., 2015). Perhaps most relevant is Hsu (2007)’s work on learning to interpolate multiple LMs using log-linear models. This differs from our work in that it learns functions to estimate the fallback probabilities αn (c) in Eq. 3 instead of λ(c), and does not cover interpolation of n-gram components, non-linearities, or the connection with neural network LMs. Also conceptually similar is work on adaptation of n-gram LMs, which start with n-gram probabilities (Della Pietra et al., 1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996; Iyer and Ostendorf, 1999) and adapt them based on the distribution of the current document, albeit in a linear model. There has also been work incorporating binary n-gram features into neural language models, which allows for more direct learning of ngram weights (Mikolov et al., 2011), but does not afford many of the advantages of the proposed model such as the incorporation of count-based probability estimates. Finally, recent works have compared ngram and neural models, finding that neural models often perform better in perplexity, but n-grams"
D16-1124,P11-2005,0,0.0174869,"rpolated LMs by normalizing the discounted distribution: i−1 PN D (wi |wi−n+1 ) = PJ i−1 PD (wi |wi−n+1 ) j=1 PD (wi i−1 = j|wi−n+1 ) , which allows us to replace β(·) for α(·) and PN D (·) for PM L (·) in Eq. 3, and proceed as normal. Kneser–Ney (KN; Kneser and Ney (1995)) and Modified KN (Chen and Goodman, 1996) smoothing further improve discounted LMs by adjusting the counts of lower-order distributions to more closely match their expectations as fallbacks for higher order distributions. Modified KN is currently the defacto standard in n-gram LMs despite occasional improvements (Teh, 2006; Durrett and Klein, 2011), and we will express it as PKN (·). 3.2 Neural LMs as Mixtures of Distributions In this section we demonstrate how neural network LMs can also be viewed as an instantiation of the MODLM framework. Feed-forward neural network LMs: Feedforward LMs (Bengio et al., 2006; Schwenk, 2007) are LMs that, like n-grams, calculate the probability of the next word based on the previous i−1 words. Given context wi−N +1 , these words are converted into real-valued word representation veci−1 tors ri−N +1 , which are concatenated into an overi−1 all representation vector q = ⊕(ri−N +1 ), where ⊕(·) is the vec"
D16-1124,C90-3038,0,0.318275,"at http://github.com/neubig/modlm P (wi |c) = K X k=1 λk (c)Pk (wi |c). (1) Here, λk (c) is a function that defines P the mixture weights, with the constraint that K k=1 λk (c) = 1 for all c. This form is not new in itself, and widely used both in the calculation of smoothing coefficients for n-gram LMs (Chen and Goodman, 1996), and interpolation of LMs of various varieties (Jelinek and Mercer, 1980). The main contribution of this paper is to demonstrate that depending on our definition of c, λk (c), and Pk (wi |c), Eq. 1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and 1163 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1163–1172, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network LMs (§3). This observation is useful theoretically, as it provides a single mathematical framework that encompasses several widely used classes of LMs. It is also useful practically, in that this new view of these traditional models allows us to create new models that combine the desirable"
D16-1124,N15-1020,0,0.0319479,"al language models, which allows for more direct learning of ngram weights (Mikolov et al., 2011), but does not afford many of the advantages of the proposed model such as the incorporation of count-based probability estimates. Finally, recent works have compared ngram and neural models, finding that neural models often perform better in perplexity, but n-grams have their own advantages such as effectiveness in extrinsic tasks (Baltescu and Blunsom, 2015) and better modeling of rare words (Chen et al., 2015). 8 neural machine translation (Sutskever et al., 2014) or dialog response generation (Sordoni et al., 2015). In addition, given the positive results using block dropout for hybrid models, we plan to develop more effective learning methods for mixtures of sparse and dense distributions. Conclusion and Future Work In this paper, we proposed a framework for language modeling that generalizes both neural network and count-based n-gram LMs. This allowed us to learn more effective interpolation functions for count-based n-grams, and to create neural LMs that incorporate information from count-based models. As the framework discussed here is general, it is also possible that they could be used in other ta"
D16-1140,D13-1176,0,\N,Missing
D16-1140,D15-1044,0,\N,Missing
D16-1140,P00-1041,0,\N,Missing
D16-1140,W03-0501,0,\N,Missing
D16-1140,W08-1105,0,\N,Missing
D16-1140,C08-1018,0,\N,Missing
D16-1140,N10-1131,0,\N,Missing
D16-1140,W04-1013,0,\N,Missing
D16-1140,W11-1610,0,\N,Missing
D16-1140,D15-1042,0,\N,Missing
D16-1140,D13-1155,0,\N,Missing
D16-1140,W01-0100,0,\N,Missing
D16-1140,W12-3018,0,\N,Missing
D16-1140,P16-1154,0,\N,Missing
D16-1140,P16-1094,0,\N,Missing
D16-1140,D10-1050,0,\N,Missing
D16-1140,P16-1014,0,\N,Missing
D16-1140,N16-1012,0,\N,Missing
D16-1140,N16-1005,0,\N,Missing
D16-1162,2011.iwslt-evaluation.18,0,0.0073927,"4.3 Hybrid Lexicons Handmade lexicons have broad coverage of words but their probabilities might not be as accurate as the Data Train Dev Test Corpus Sentence BTEC KFTT BTEC KFTT BTEC KFTT 464K 377K 510 1160 508 1169 Tokens En Ja 3.60M 4.97M 7.77M 8.04M 3.8K 5.3K 24.3K 26.8K 3.8K 5.5K 26.0K 28.4K Table 1: Corpus details. learned ones, particularly if the automatic lexicon is constructed on in-domain data. Thus, we also test a hybrid method where we use the handmade lexicons to complement the automatically learned lexicon.2 3 Specifically, inspired by phrase table fill-up used in PBMT systems (Bisazza et al., 2011), we use the probability of the automatically learned lexicons pl,a by default, and fall back to the handmade lexicons pl,m only for uncovered words: { pl,a (e|f ) if f is covered (6) pl,h (e|f ) = pl,m (e|f ) otherwise 5 Experiment & Result In this section, we describe experiments we use to evaluate our proposed methods. 5.1 Settings Dataset: We perform experiments on two widelyused tasks for the English-to-Japanese language pair: KFTT (Neubig, 2011) and BTEC (Kikui et al., 2003). KFTT is a collection of Wikipedia article about city of Kyoto and BTEC is a travel conversation corpus. BTEC is a"
D16-1162,J93-2003,0,0.140989,"ty with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.1 1 I come from Tunisia. チュニジア の 出身です。 Chunisia no shusshindesu. System: (I’m from Tunisia.) ノルウェー の 出身です。 Noruue- no shusshindesu. (I’m from Norway.) Figure 1: An example of a mistake made by NMT on low-frequency content words. Introduction Neural machine translation (NMT, §2; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016). One feature of NMT systems is that they treat each word in the vocabulary as a vector of 1 Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html continuous-valued numbers. This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; Koehn"
D16-1162,J07-2003,0,0.149615,"Missing"
D16-1162,W14-4012,0,0.085105,"Missing"
D16-1162,P16-1160,0,0.0188031,"the original HMM alignments gathered from the training corpus. Basically, this method is a specific version of our bias method that gives some of the vocabulary a bias of negative infinity and all other vocabulary a uniform distribution. Our method improves over this by considering actual translation probabilities, and also considering the attention vector when deciding how to combine these probabilities. Finally, there have been a number of recent works that improve accuracy of low-frequency words using character-based translation models (Ling et al., 2015; Costa-Juss`a and Fonollosa, 2016; Chung et al., 2016). However, Luong and Manning (2016) have found that even when using character-based models, incorporating information about words allows for gains in translation accuracy, and it is likely that our lexicon-based method could result in improvements in these hybrid systems as well. 8 Conclusion & Future Work In this paper, we have proposed a method to incorporate discrete probabilistic lexicons into NMT systems to solve the difficulties that NMT systems have demonstrated with low-frequency words. As a result, we achieved substantial increases in BLEU (2.0-2.3) and NIST (0.13-0.44) scores, and ob"
D16-1162,P16-2058,0,0.0238357,"Missing"
D16-1162,N13-1073,0,0.110911,"strap the learning of the NMT system, allowing it to approach an appropriate answer in a more timely fashion.8 It is also interesting to examine the alignment vectors produced by the baseline and proposed meth8 Note that these gains are despite the fact that one iteration of the proposed method takes a longer (167 minutes for attn vs. 275 minutes for auto-bias) due to the necessity to calculate and use the lexical probability matrix for each sentence. It also takes an additional 297 minutes to train the lexicon with GIZA++, but this can be greatly reduced with more efficient training methods (Dyer et al., 2013). (a) BTEC Lexicon auto man hyb BLEU bias linear 48.31 49.74∗ 47.97 49.08 51.04† † 50.34 49.27 NIST linear 5.98 6.11 5.90 ∗ 6.03 6.14† ∗ 6.10 5.94 bias (b) KFTT Lexicon auto man hyb BLEU bias linear 20.86 † 23.20 18.19 20.78 20.88 † 22.80 20.33 NIST linear 5.15 5.59† 4.61 5.12 5.11 † 5.55 5.03 bias Table 4: A comparison of the bias and linear lexicon integration methods on the automatic, manual, and hybrid lexicons. The first line without lexicon is the traditional attentional NMT. ods, a visualization of which we show in Figure 3. For this sentence, the outputs of both methods were both ident"
D16-1162,P16-1154,0,0.140543,"(·) and the lexicon probability pl (·). We will call this the linear method, and define it as follows:  po (ei |F, ei−1 1 )=  pl (ei = 1|F, ei−1 pm (e = 1|F, ei−1 [ ] 1 ) 1 ) λ   .. .. ,   . . 1−λ i−1 i−1 pl (ei = |Ve ||F, e1 ) pm (e = |Ve ||F, e1 ) where λ is an interpolation coefficient that is the result of the sigmoid function λ = sig(x) = 1+e1−x . x is a learnable parameter, and the sigmoid function ensures that the final interpolation level falls between 0 and 1. We choose x = 0 (λ = 0.5) at the beginning of training. This notation is partly inspired by Allamanis et al. (2016) and Gu et al. (2016) who use linear interpolation to merge a standard attentional model with a “copy” operator that copies a source word as-is into the target sentence. The main difference is that they use this to copy words into the output while our method uses it to influence the probabilities of all target words. 4 Constructing Lexicon Probabilities In the previous section, we have defined some ways to use predictive probabilities pl (ei |F, ei−1 1 ) based on word-to-word lexical probabilities pl (e|f ). Next, we define three ways to construct these lexical probabilities using automatically learned lexicons, h"
D16-1162,P16-1014,0,0.023798,"Missing"
D16-1162,P15-1001,0,0.192362,"inement, with Model 1 relying solely on these lexical probabilities, and latter IBM models (Models 2, 3, 4, 5) introducing more sophisticated models of fertility and relative alignment. Even though IBM models also occasionally have problems when dealing with the rare words (e.g. “garbage collecting” effects (Liang et al., 2006)), traditional SMT systems generally achieve better translation accuracies of lowfrequency words than NMT systems (Sutskever et al., 2014), indicating that these problems are less prominent than they are in NMT. Note that in many cases, NMT limits the target vocabulary (Jean et al., 2015) for training speed or memory constraints, resulting in rare words not being covered by the NMT vocabulary VE . Accordingly, we allocate the remaining probability assigned by the lexicon to the unknown word symbol ⟨unk⟩: ∑ pl,a (e = ⟨unk⟩|f ) = 1 − pl,a (e = i|f ). (5) i∈Ve 4.2 Manual Lexicons In addition, for many language pairs, broadcoverage handmade dictionaries exist, and it is desirable that we be able to use the information included in them as well. Unlike automatically learned lexicons, however, handmade dictionaries generally do not contain translation probabilities. To construct the"
D16-1162,D13-1176,0,0.0542733,"which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.1 1 I come from Tunisia. チュニジア の 出身です。 Chunisia no shusshindesu. System: (I’m from Tunisia.) ノルウェー の 出身です。 Noruue- no shusshindesu. (I’m from Norway.) Figure 1: An example of a mistake made by NMT on low-frequency content words. Introduction Neural machine translation (NMT, §2; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016). One feature of NMT systems is that they treat each word in the vocabulary as a vector of 1 Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html continuous-valued numbers. This i"
D16-1162,N03-1017,0,0.278912,"Missing"
D16-1162,W04-3250,0,0.0368593,"iro contains 104K distinct word-to-word translation entries. The third lexicon (hyb) is built by combining the first and second lexicon with the hybrid method of §4.3. Evaluation: We use standard single reference BLEU-4 (Papineni et al., 2002) to evaluate the translation performance. Additionally, we also use NIST (Doddington, 2002), which is a measure that puts a particular focus on low-frequency word strings, and thus is sensitive to the low-frequency words we are focusing on in this paper. We measure the statistical significant differences between systems using paired bootstrap resampling (Koehn, 2004) with 10,000 iterations and measure statistical significance at the p &lt; 0.05 and p &lt; 0.10 levels. Additionally, we also calculate the recall of rare words from the references. We define “rare words” as words that appear less than eight times in the target training corpus or references, and measure the percentage of time they are recovered by each translation system. 10 5 attn auto-bias hyb-bias 0 1000 2000 3000 time (minutes) 4000 Figure 2: Training curves for the baseline attn and the proposed bias method. with the auto or hyb lexicons, which empirically gave the best results, and perform a c"
D16-1162,N06-1014,0,0.0168164,"lgorithm. First in the expectation step, the algorithm estimates the expected count c(e|f ). In the maximiza1560 tion step, lexical probabilities are calculated by dividing the expected count by all possible counts: c(f, e) pl,a (e|f ) = ∑ , ˜) e˜ c(f, e The IBM models vary in level of refinement, with Model 1 relying solely on these lexical probabilities, and latter IBM models (Models 2, 3, 4, 5) introducing more sophisticated models of fertility and relative alignment. Even though IBM models also occasionally have problems when dealing with the rare words (e.g. “garbage collecting” effects (Liang et al., 2006)), traditional SMT systems generally achieve better translation accuracies of lowfrequency words than NMT systems (Sutskever et al., 2014), indicating that these problems are less prominent than they are in NMT. Note that in many cases, NMT limits the target vocabulary (Jean et al., 2015) for training speed or memory constraints, resulting in rare words not being covered by the NMT vocabulary VE . Accordingly, we allocate the remaining probability assigned by the lexicon to the unknown word symbol ⟨unk⟩: ∑ pl,a (e = ⟨unk⟩|f ) = 1 − pl,a (e = i|f ). (5) i∈Ve 4.2 Manual Lexicons In addition, for"
D16-1162,P16-1100,0,0.022818,"Missing"
D16-1162,D15-1166,0,0.0679155,"Missing"
D16-1162,P15-1002,0,0.73991,"Chunisia no shusshindesu. System: (I’m from Tunisia.) ノルウェー の 出身です。 Noruue- no shusshindesu. (I’m from Norway.) Figure 1: An example of a mistake made by NMT on low-frequency content words. Introduction Neural machine translation (NMT, §2; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016). One feature of NMT systems is that they treat each word in the vocabulary as a vector of 1 Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html continuous-valued numbers. This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; Koehn et al. (2003)), which represent translations as discrete pairs of word strings in the source and target languages. The use of continuous representations is a major advantage, allowing NMT to share statistical power between similar words (e.g. “do"
D16-1162,J93-2004,0,0.0712609,"Missing"
D16-1162,P16-2021,0,0.0327462,"the linear approach of §3.2.2, but are only applicable to words that can be copied asis into the target language. In fact, these models can be thought of as a subclass of the proposed approach that use a lexicon that assigns a all its probability to target words that are the same as the source. On the other hand, while we are simply using a static interpolation coefficient λ, these works generally have a more sophisticated method for choosing the interpolation between the standard and “copy” models. Incorporating these into our linear method is a promising avenue for future work. In addition Mi et al. (2016) have also recently proposed a similar approach by limiting the number of vocabulary being predicted by each batch or sentence. This vocabulary is made by considering the original HMM alignments gathered from the training corpus. Basically, this method is a specific version of our bias method that gives some of the vocabulary a bias of negative infinity and all other vocabulary a uniform distribution. Our method improves over this by considering actual translation probabilities, and also considering the attention vector when deciding how to combine these probabilities. Finally, there have been"
D16-1162,P11-2093,1,0.34484,"Missing"
D16-1162,P13-4016,1,0.790627,"Missing"
D16-1162,J03-1002,0,0.0763353,"of all target words. 4 Constructing Lexicon Probabilities In the previous section, we have defined some ways to use predictive probabilities pl (ei |F, ei−1 1 ) based on word-to-word lexical probabilities pl (e|f ). Next, we define three ways to construct these lexical probabilities using automatically learned lexicons, handmade lexicons, or a combination of both. 4.1 Automatically Learned Lexicons In traditional SMT systems, lexical translation probabilities are generally learned directly from parallel data in an unsupervised fashion using a model such as the IBM models (Brown et al., 1993; Och and Ney, 2003). These models can be used to estimate the alignments and lexical translation probabilities pl (e|f ) between the tokens of the two languages using the expectation maximization (EM) algorithm. First in the expectation step, the algorithm estimates the expected count c(e|f ). In the maximiza1560 tion step, lexical probabilities are calculated by dividing the expected count by all possible counts: c(f, e) pl,a (e|f ) = ∑ , ˜) e˜ c(f, e The IBM models vary in level of refinement, with Model 1 relying solely on these lexical probabilities, and latter IBM models (Models 2, 3, 4, 5) introducing more"
D16-1162,P02-1040,0,0.096508,"ling 6 experiments. The first lexicon (auto) is built on the training data using the automatically learned lexicon method of §4.1 separately for both the BTEC and KFTT experiments. Automatic alignment is performed using GIZA++ (Och and Ney, 2003). The second lexicon (man) is built using the popular English-Japanese dictionary Eijiro7 with the manual lexicon method of §4.2. Eijiro contains 104K distinct word-to-word translation entries. The third lexicon (hyb) is built by combining the first and second lexicon with the hybrid method of §4.3. Evaluation: We use standard single reference BLEU-4 (Papineni et al., 2002) to evaluate the translation performance. Additionally, we also use NIST (Doddington, 2002), which is a measure that puts a particular focus on low-frequency word strings, and thus is sensitive to the low-frequency words we are focusing on in this paper. We measure the statistical significant differences between systems using paired bootstrap resampling (Koehn, 2004) with 10,000 iterations and measure statistical significance at the p &lt; 0.05 and p &lt; 0.10 levels. Additionally, we also calculate the recall of rare words from the references. We define “rare words” as words that appear less than e"
D16-1162,P16-1009,0,0.0439705,"desu. System: (I’m from Tunisia.) ノルウェー の 出身です。 Noruue- no shusshindesu. (I’m from Norway.) Figure 1: An example of a mistake made by NMT on low-frequency content words. Introduction Neural machine translation (NMT, §2; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016). One feature of NMT systems is that they treat each word in the vocabulary as a vector of 1 Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html continuous-valued numbers. This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; Koehn et al. (2003)), which represent translations as discrete pairs of word strings in the source and target languages. The use of continuous representations is a major advantage, allowing NMT to share statistical power between similar words (e.g. “dog” and “cat”) or contexts"
D16-1162,P07-2045,0,\N,Missing
D16-1263,W08-0336,0,0.0470765,"Missing"
D16-1263,N16-1109,1,0.622951,"pressed with a weighted finite-state transducer (WFST) framework represents the joint distribution of source acoustic features, phonemes and latent source words given the target words. Sampling of alignments is used to learn source words and their target translations, which are then used to improve transcription of the source audio they were learnt from. Importantly, the model assumes no prior lexicon or translation model. This method builds on work on phoneme translation modeling (Besacier et al., 2006; St¨uker et al., 2009; Stahlberg et al., 2012; Stahlberg et al., 2014; Adams et al., 2015; Duong et al., 2016), speech translation (Casacuberta et al., 2004; Matusov et al., 2005), computer-aided translation, (Brown et al., 1994; Vidal et al., 2006; Khadivi and Ney, 2008; Reddy and Rose, 2010; Pelemans et al., 2015), translation modeling from automatically transcribed 1 Code is available at https://github.com/oadams/latticetm. 2377 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2377–2382, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics speech (Paulik and Waibel, 2013), word segmentation and translation modeling (Chang e"
D16-1263,N09-1046,0,0.0684154,"Missing"
D16-1263,W08-0704,0,0.0500543,"Missing"
D16-1263,P11-2093,1,0.626336,"y their ability to improve phoneme recognition, measuring phoneme error rate (PER). Experimental setup We used less than 10 hours of English–Japanese data from the BTEC corpus (Takezawa et al., 2002), comprised of spoken utterances paired with textual translations. This allows us to assess the approach assuming quality acoustic models. We used acoustic models similar to Heck et al. (2015) to obtain source phoneme lattices. Gold phoneme transcriptions were obtained by transforming the text with pronunciation lexicons and, in the Japanese case, first segmenting the text into tokens using KyTea (Neubig et al., 2011). We run experiments in both directions: English– Japanese and Japanese–English (en–ja and ja–en), while comparing against three settings: the ASR 1best path uninformed by the model (ASR); a monolingual version of our model that is identical except without conditioning on the target side (Mono); and the model applied using the source language sentence as the target (Oracle). We tuned on the first 1,000 utterences (about 1 hour) of speech and trained on up to 9 hours of the English (en) Mono –ja Oracle ASR 22.1 Vague 17.7 18.5 17.2 Shifted 17.4 16.9 16.6 Poisson 17.3 17.2 16.8 Japanese (ja) Mon"
D16-1263,C10-1092,0,0.0680955,"Missing"
D16-1263,takezawa-etal-2002-toward,0,0.0482154,"ce constituting one block. To sample from WFSTs, we use forwardfiltering/backward-sampling (Scott, 2002; Neubig et al., 2012), creating forward probabilities using the forward algorithm for hidden Markov models before backward-sampling edges proportionally to the product of the forward probability and the edge weight.3 3 No Metropolis-Hastings rejection step was used. 2379 We evaluate the lexicon and translation model by their ability to improve phoneme recognition, measuring phoneme error rate (PER). Experimental setup We used less than 10 hours of English–Japanese data from the BTEC corpus (Takezawa et al., 2002), comprised of spoken utterances paired with textual translations. This allows us to assess the approach assuming quality acoustic models. We used acoustic models similar to Heck et al. (2015) to obtain source phoneme lattices. Gold phoneme transcriptions were obtained by transforming the text with pronunciation lexicons and, in the Japanese case, first segmenting the text into tokens using KyTea (Neubig et al., 2011). We run experiments in both directions: English– Japanese and Japanese–English (en–ja and ja–en), while comparing against three settings: the ASR 1best path uninformed by the mod"
D16-1263,1983.tc-1.13,0,0.746681,"Missing"
D16-1263,D15-1142,0,\N,Missing
D16-1263,W14-2201,1,\N,Missing
D17-1145,P08-1115,0,0.298669,"ic speech recognition (ASR) system. Previous research on traditional phrase-based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step (Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008). Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works. As a remedy, Su et al. (2017) proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq"
D17-1145,P16-1078,0,0.0159692,"ords we replace (1) by the following:  → − → − h i = LatticeLSTM xi , { h k |k∈C(i)} (4) Similarly, we encode the lattice in backward direction and replace (2) accordingly. Figure 2 illustrates the result. The computational complexity of the encoder is O(|V |+ |E|), i.e. linear in the number of nodes plus number of edges in the graph. The complexity of the attention mechanism is O(|V |M ), where M is the output sequence 2 It is perhaps more common to think of each edge representing a word, but we will motivate why we instead assign word labels to nodes in §3.3. 3 This is similar in spirit to Eriguchi et al. (2016) who used the TreeLSTM in an attentional tree-to-sequence model. 1382 Sequential LSTM recurrence ˜ i = hi−1 h forget gt.   ˜ i + bf f i = σ W f xi + Uf h TreeLSTM ˜i = P h k∈C(i) hk fik = σ(Wf xi + U f hk + bf ) Proposed LatticeLSTM Sh wb/f,k ˜i = P h k∈C(i) Zh,k hk (5)  fik = σ(Wf xi + Uf hk + (6)  ln wb/f,k Sf − Zf,k + bf ) update  ˜ i + bin ii = σ Win xi + Uin h   ˜ i + bo oi = σ Wo xi + Uo h   ˜ i + bu ui = tanh Wu xi + Uu h cell ci = ii ui + fi ci−1 ci = ii ui + P k∈C(i) fik ck as TreeLSTM hidden hi = oi tanh(ci ) as sequential as sequential input gt.  output gt. attention as se"
D17-1145,D13-1176,0,0.373947,"based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step (Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008). Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works. As a remedy, Su et al. (2017) proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq) model. This is achieved by extending the encoder’s Gated Recurrent Units (GRUs) (Cho e"
D17-1145,D15-1166,0,0.0804604,"o hidden states are generated as  → − → − h i = LSTM Efwd (xi ), h i−1 (1) ← − ← −  h i = LSTM Ebwd (xi ), h i+1 , (2) where Efwd and Ebwd are source embedding lookup tables. We opt for long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) recurrent units because of their high performance and in order to later take advantage of the TreeLSTM extension (Tai et al., 2015). We stack multiple LSTM layers and concatenate the final layer → − ← − into the final source hidden state hi = h i |h i , where layer indices are omitted for simplicity. 2.2 Attention We use an attention mechanism (Luong et al., 2015) to summarize the encoder outputs into a 1381 fixed-size representation. At each decoding time step j, a context vector cj is computed as a weighted PNaverage of the source hidden states: cj = i=1 αij hi . The normalized attentional weights αij measure the relative importance of the source words for the current decoding step and are computed as a softmax with normalization factor Z summing over i: αij =  1 exp s sj−1 , hi Z (3) s(·) is a feed-forward neural network with a single layer that estimates the importance of source hidden state hi for producing the next target symbol yj , conditione"
D17-1145,2013.iwslt-papers.14,0,0.531314,"r the baseline. According to our knowledge, this is the first attempt of integrating lattice scores already at the training stage of a machine translation model. • We exploit the fact that our lattice encoder is a strict generalization of a sequential encoder by pre-training on sequential data, obtaining faster and better training convergence on large corpora of parallel sequential data. 1 This is reminiscent of the weighted pooling strategy by Ladhak et al. (2016) for spoken utterance classification. We conduct experiments on the Fisher and Callhome Spanish–English Speech Translation Corpus (Post et al., 2013) and report improvements of 1.4 BLEU points on Fisher and 0.8 BLEU points on Callhome, compared to a strong baseline optimized for translating 1-best ASR outputs. We find that the proposed integration of lattice scores is crucial for achieving these improvements. 2 Background Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section. Given an input sequence x = (x1 , x2 , . . . , xN ), the goal is to generate an appropriate output sequence y = (y1 , y2 , . . . , yM ). T"
D17-1145,P15-1150,0,0.0479141,"Missing"
D17-1145,2005.iwslt-1.2,0,0.377035,"eech translation, where a down-stream translation system must consume the output of an up-stream automatic speech recognition (ASR) system. Previous research on traditional phrase-based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step (Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008). Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works. As a remedy, Su et al. (2017)"
D17-1268,C16-1298,0,0.146317,"Missing"
D17-1268,N09-1067,0,0.0672915,"Missing"
D17-1268,P07-1009,0,0.486746,"Missing"
D17-1268,E17-1117,1,0.881998,"Missing"
D17-1268,I08-2093,0,0.157377,"Missing"
D17-1268,E17-2002,1,0.938759,"(Zhang et al., 2012), and machine translation (Daiber et al., 2016). However, the needs of NLP tasks differ in many ways from the needs of scientific typology, and typological databases are often only sparsely populated, by necessity or by design.2 In NLP, on the other hand, what is important is having a relatively full set of features for the particular group of languages you are working on. This mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daum´e III and Campbell, 2007; Daum´e III, 2009; Coke et al., 2016; Littell et al., 2017). In this study, we examine whether we can 2 For example, each chapter of WALS aims to provide a statistically balanced set of languages over language families and geographical areas, and so many languages are left out in order to maintain balance. 2529 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2529–2535 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tackle the problem of inferring linguistic typology from parallel corpora, specifically by training a massively multi-lingual neural machine translati"
D17-1268,P12-1066,0,0.0542069,"own right since the 19th century (Greenberg, 1963; Comrie, 1989; Nichols, 1992), but recently typology has borne practical fruit within various subfields of NLP, particularly on problems involving lower-resource languages. 1 Code and learned vectors are available at http:// github.com/chaitanyamalaviya/lang-reps Typological information from sources like the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), has proven useful in many NLP tasks (O’Horan et al., 2016), such as multilingual dependency parsing (Ammar et al., 2016), generative parsing in low-resource settings (Naseem et al., 2012; T¨ackstr¨om et al., 2013), phonological language modeling and loanword prediction (Tsvetkov et al., 2016), POStagging (Zhang et al., 2012), and machine translation (Daiber et al., 2016). However, the needs of NLP tasks differ in many ways from the needs of scientific typology, and typological databases are often only sparsely populated, by necessity or by design.2 In NLP, on the other hand, what is important is having a relatively full set of features for the particular group of languages you are working on. This mismatch of needs has motivated various proposals to reconstruct missing entrie"
D17-1268,C16-1123,0,0.0775196,"Missing"
D17-1268,P15-2034,0,0.41932,"g, pages 2529–2535 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tackle the problem of inferring linguistic typology from parallel corpora, specifically by training a massively multi-lingual neural machine translation (NMT) system and using the learned representations to infer typological features for each language. This is motivated both by prior work in linguistics (Bugarski, 1991; Garc´ıa, 2002) demonstrating strong links between translation studies and tools for contrastive linguistic analysis, work ¨ in inferring typology from bilingual data (Ostling, 2015) and English as Second Language texts (Berzak et al., 2014), as well as work in NLP (Shi et al., 2016; Kuncoro et al., 2017; Belinkov et al., 2017) showing that syntactic knowledge can be extracted from neural nets on the word-by-word or sentence-by-sentence level. This work presents a more holistic analysis of whether we can discover what neural networks learn about the linguistic concepts of an entire language by aggregating their representations over a large number of the sentences in the language. We examine several methods for discovering feature vectors for typology prediction, including"
D17-1268,E17-2102,0,0.109708,"t al., 2017; Belinkov et al., 2017) showing that syntactic knowledge can be extracted from neural nets on the word-by-word or sentence-by-sentence level. This work presents a more holistic analysis of whether we can discover what neural networks learn about the linguistic concepts of an entire language by aggregating their representations over a large number of the sentences in the language. We examine several methods for discovering feature vectors for typology prediction, including those learning a language vector specifying the language while training multilingual neural lan¨ guage models (Ostling and Tiedemann, 2017) or neural machine translation (Johnson et al., 2016) systems. We further propose a novel method for aggregating the values of the latent state of the encoder neural network to a single vector representing the entire language. We calculate these feature vectors using an NMT model trained on 1017 languages, and use them for typlogy prediction both on their own and in composite with feature vectors from previous work based on the genetic and geographic distance between languages (Littell et al., 2017). Results show that the extracted representations do in fact allow us to learn about the typolog"
D17-1268,P16-1162,0,0.0128,"men To train a multilingual neural machine translation system, we used a corpus of Bible translations that was obtained by scraping a massive online Bible database at bible.com.5 This corpus contains data for 1017 languages. After preprocessing the corpus, we obtained a training set of 20.6 million sentences over all languages. The implementation of both the LM and NMT models described in §3 was done in the DyNet toolkit (Neubig et al., 2017). In order to obtain a manageable shared vocabulary for all languages, we divided the data into subwords using joint byte-pair encoding of all languages (Sennrich et al., 2016) with 32K merge operations. We 4 NMT Encoder Mean Cell States Finally, we propose a new vector representation of a language (MTC ELL) that has not been investigated in previous work: the average hidden cell state of the encoder LSTM for all sentences in the language. Inspired by previous work that has noted that the hidden cells of LSTMs can automatically capture salient and interpretable information such as syntax (Karpathy et al., 2015; Shi et al., 2016) or Syntax -Aux +Aux 69.91 83.07 71.32 82.94 74.90 83.31 75.91 85.14 77.11 86.33 We also tried using the mean of final hidden cell states of"
D17-1268,D16-1159,0,0.166829,"Linguistics tackle the problem of inferring linguistic typology from parallel corpora, specifically by training a massively multi-lingual neural machine translation (NMT) system and using the learned representations to infer typological features for each language. This is motivated both by prior work in linguistics (Bugarski, 1991; Garc´ıa, 2002) demonstrating strong links between translation studies and tools for contrastive linguistic analysis, work ¨ in inferring typology from bilingual data (Ostling, 2015) and English as Second Language texts (Berzak et al., 2014), as well as work in NLP (Shi et al., 2016; Kuncoro et al., 2017; Belinkov et al., 2017) showing that syntactic knowledge can be extracted from neural nets on the word-by-word or sentence-by-sentence level. This work presents a more holistic analysis of whether we can discover what neural networks learn about the linguistic concepts of an entire language by aggregating their representations over a large number of the sentences in the language. We examine several methods for discovering feature vectors for typology prediction, including those learning a language vector specifying the language while training multilingual neural lan¨ gua"
D17-1268,N13-1126,0,0.0750435,"Missing"
D17-1268,L16-1011,0,0.0280685,"hether a language has prepositions or postpositions), phonology (e.g. whether a language has complex syllabic onset clusters), and phonetic inventory (e.g. whether a language has interdental fricatives). There are 103 syntactical features, 28 phonology features and 158 phonetic inventory features in the database. Baseline Feature Vectors: Several previous methods take advantage of typological implicature, the fact that some typological traits correlate strongly with others, to use known features of a language to help infer other unknown features of the language (Daum´e III and Campbell, 2007; Takamura et al., 2016; Coke et al., 2016). As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand, Littell et al. (2017) have proposed a method for inferring typological features directly from the language’s k nearest neighbors (k-NN) according to geodesic distance (distance on the Earth’s surface) and genetic distance (distance according to a phylogenetic family tree). In our experiments, our baseline uses this method by taking the 3-NN for each language according to normalized geodesic+genetic distance, and calculating an average feature vec"
D17-1268,N16-1161,1,0.829814,"Missing"
D17-1268,D12-1125,0,0.0413095,"ious subfields of NLP, particularly on problems involving lower-resource languages. 1 Code and learned vectors are available at http:// github.com/chaitanyamalaviya/lang-reps Typological information from sources like the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), has proven useful in many NLP tasks (O’Horan et al., 2016), such as multilingual dependency parsing (Ammar et al., 2016), generative parsing in low-resource settings (Naseem et al., 2012; T¨ackstr¨om et al., 2013), phonological language modeling and loanword prediction (Tsvetkov et al., 2016), POStagging (Zhang et al., 2012), and machine translation (Daiber et al., 2016). However, the needs of NLP tasks differ in many ways from the needs of scientific typology, and typological databases are often only sparsely populated, by necessity or by design.2 In NLP, on the other hand, what is important is having a relatively full set of features for the particular group of languages you are working on. This mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daum´e III and Campbell, 2007; Daum´e III, 2009; Coke et al., 2016; Littell et al., 2017"
D17-1268,P17-1080,0,\N,Missing
D17-1268,W14-1603,0,\N,Missing
D17-1315,P16-1160,0,0.0344896,"ur data driven approach. Also, their focus is on brand names. Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT. Ling et al. (2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT. Zoph and Knight (2016) propose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case. 7 Conclusion We have proposed an end-to-end neural system to model portmanteau generation. Our experiments show the efficacy of proposed system in predicting portmanteaus given the root words. We conclude that pre-training character embeddings on the English vocabulary helps the model. Through human evaluation we show that our model’s predictions are superior t"
D17-1315,N15-1021,0,0.486256,"odern Hebrew BatEl (1996); Berman (1989) and Spanish Pi˜neros (2004). Their short length makes them ideal for headlines and brandnames (Gabler, 2015). Unlike better-defined morphological phenomenon such as inflection and derivation, portmanteau generation ∗ * denotes equal contribution is difficult to capture using a set of rules. For instance, Shaw et al. (2014) state that the composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head. An existing work by Deri and Knight (2015) aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches. Their methods rely on a grapheme to phoneme converter, which takes into account the phonetic features of the language, but may not be available or accurate for non-dictionary words, or low resource languages. Prior works, such as Faruqui et al. (2016), have demonstrated the efficacy of neural approaches for morphological tasks such as inflection. We hypothesize that such neural methods can (1) provide a simpler and more integrated end-to-end framework than multiple F"
D17-1315,N16-1077,1,0.680948,"composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head. An existing work by Deri and Knight (2015) aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches. Their methods rely on a grapheme to phoneme converter, which takes into account the phonetic features of the language, but may not be available or accurate for non-dictionary words, or low resource languages. Prior works, such as Faruqui et al. (2016), have demonstrated the efficacy of neural approaches for morphological tasks such as inflection. We hypothesize that such neural methods can (1) provide a simpler and more integrated end-to-end framework than multiple FSTs used in the previous work, and (2) automatically capture features such as phonetic similarity through the use of character embeddings, removing the need for explicit 2917 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2917–2922 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics grapheme-"
D17-1315,W04-3250,0,0.073361,"Missing"
D17-1315,P12-1074,0,0.426566,"Missing"
D17-1315,N16-1004,0,0.0265837,"based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT. Ling et al. (2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT. Zoph and Knight (2016) propose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case. 7 Conclusion We have proposed an end-to-end neural system to model portmanteau generation. Our experiments show the efficacy of proposed system in predicting portmanteaus given the root words. We conclude that pre-training character embeddings on the English vocabulary helps the model. Through human evaluation we show that our model’s predictions are superior to the baseline. We have also released our dataset and code6 to encourage further research on the phenomenon of portmanteaus. We also release"
D18-1034,D16-1250,0,0.390551,"e 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have differ"
D18-1034,P17-1042,0,0.167017,"tional Linguistics art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not availa"
D18-1034,D16-1153,1,0.872625,"g the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our problem setting (§2.1), then present our methods in detail (§2.2), and provide some additional motivati"
D18-1034,Q17-1010,0,0.462655,"is shared space further, we iteratively perform a self-learning refinement step k 2 times by: 4. Train an NER model using the translated words along with the named entity tags from the English corpus (§2.2.4). We consider each in detail. 2.2.1 Learning Monolingual Embeddings Given text in the source and target language, we first independently learn word embedding matrices X and Y in the source and target languages respectively. These embeddings can be learned on monolingual text in both languages with any of the myriad of word embedding methods (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). 2.2.2 Learning Bilingual Embeddings Next, we learn a cross-lingual projection of X and Y into a shared space. Assume we are given a dictionary {xi , yi }D i=1 , where xi and yi denote the embeddings of a word pair. Let XD = [x1 , x2 , · · · , xD ]> and YD = [y1 , y2 , · · · , yD ]> denote two embedding matrices consisting of word pairs from the dictionary. Following previous work (Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017), we optimize the following objective: min W d X 2 kW xi − yi k s.t. W W > 1. Using the aligned embeddings to generate a new dictionary that consists of"
D18-1034,Q16-1026,0,0.0512909,"methods achieve state-of-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 36"
D18-1034,P12-1073,0,0.0773916,"guage with abundant entity labels to a low-resource target language with few or no labels. Specifically, in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017). Since parallel corpora may not be always available, Mayhew et al. (2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can directly exploit"
D18-1034,N16-1030,0,0.891813,"-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Bel"
D18-1034,I17-2016,0,0.0616114,"words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across different languages by sharing parameters to allow for knowledge transfer (Ammar et al., 2016a; Yang et al., 2017; Cotterell and Duh, 2017; Lin et al., 2018). However, such approaches usually require some amounts of training data in the target language for bootstrapping, which is different from our unsupervised approach that requires no labeled resources in the target language. under a cross-lingual setting, with lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through j"
D18-1034,P18-1074,0,0.126914,"es and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across different languages by sharing parameters to allow for knowledge transfer (Ammar et al., 2016a; Yang et al., 2017; Cotterell and Duh, 2017; Lin et al., 2018). However, such approaches usually require some amounts of training data in the target language for bootstrapping, which is different from our unsupervised approach that requires no labeled resources in the target language. under a cross-lingual setting, with lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training. Appr"
D18-1034,P11-1061,0,0.0854468,"some named entities are not covered by the dictionary or the translation is not reliable. We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent"
D18-1034,R11-1017,0,0.479375,"gh-resource source language with abundant entity labels to a low-resource target language with few or no labels. Specifically, in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017). Since parallel corpora may not be always available, Mayhew et al. (2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can"
D18-1034,P16-1101,0,0.605954,"gual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can directly exploit features extracted from the surface forms (e.g. through character-level neural feature extractors), which has proven essential for high accuracy in the monolingual scenario (Ma and Hovy, 2016). However, these methods are largely predicated on the availability of large-scale parallel resources, and thus, their applicability to lowresource languages is limited. In contrast, it is also possible to learn lexical mappings through bilingual word embeddings (BWE). These bilingual embeddings can be obtained by using a small dictionary to project two sets of embeddings into a consistent space (Mikolov et al., 2013a; Faruqui and Dyer, For languages with no annotated resources, unsupervised transfer of natural language processing models such as named-entity recognition (NER) from resource-ric"
D18-1034,K16-1018,0,0.0300548,"onary or the translation is not reliable. We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose"
D18-1034,D17-1269,0,0.420462,"Missing"
D18-1034,P17-2093,0,0.159085,"rpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our problem setting (§2.1), then present our methods in detail (§2.2"
D18-1034,D11-1006,0,0.0876572,"ies is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches uti"
D18-1034,E14-1049,0,0.117455,"Missing"
D18-1034,P15-1119,0,0.192844,"ng the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our p"
D18-1034,P17-1135,0,0.671727,"Missing"
D18-1034,P14-1006,0,0.028839,"m our unsupervised approach that requires no labeled resources in the target language. under a cross-lingual setting, with lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training. Approaches based on parallel corpora usually learn bilingual word embeddings that can produce similar representations for aligned sentences (Hermann and Blunsom, 2014; Chandar et al., 2014). Jointlytrained models combine the common monolingual training objective with a cross-lingual training objective that often comes from parallel corpus (Zou et al., 2013; Gouws et al., 2015). Recently, unsupervised approaches also have been used to align two sets of word embeddings by learning a mapping through adversarial learning or selflearning (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). References 6 Acknowledgments We thank Stephen Mayhew for sharing the data, and Zihang Dai for meaningful discussion. This research was sponsored by Defense Advanc"
D18-1034,D14-1162,0,0.0789903,"efine the alignment in this shared space further, we iteratively perform a self-learning refinement step k 2 times by: 4. Train an NER model using the translated words along with the named entity tags from the English corpus (§2.2.4). We consider each in detail. 2.2.1 Learning Monolingual Embeddings Given text in the source and target language, we first independently learn word embedding matrices X and Y in the source and target languages respectively. These embeddings can be learned on monolingual text in both languages with any of the myriad of word embedding methods (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). 2.2.2 Learning Bilingual Embeddings Next, we learn a cross-lingual projection of X and Y into a shared space. Assume we are given a dictionary {xi , yi }D i=1 , where xi and yi denote the embeddings of a word pair. Let XD = [x1 , x2 , · · · , xD ]> and YD = [y1 , y2 , · · · , yD ]> denote two embedding matrices consisting of word pairs from the dictionary. Following previous work (Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017), we optimize the following objective: min W d X 2 kW xi − yi k s.t. W W > 1. Using the aligned embeddings to generate a new di"
D18-1034,P17-1161,0,0.02521,"languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for C"
D18-1034,N18-1202,0,0.0600342,", with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics art cross-lingu"
D18-1034,N16-1156,0,0.243386,"edia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our problem setting (§2.1), then present our m"
D18-1034,D17-1035,0,0.019569,"tput of attention layer is defined as: Experimental Settings We evaluate our proposed methods on the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain 4 European languages, English, German, Dutch and Spanish. For all experiments, we use English as the source language and translate its training data into the target language. We train a model on the translated data, and test it on the target language. For each experiment, we run our models 5 times using different seeds and report the mean and standard deviation, as suggested by Reimers and Gurevych (2017). Word Embeddings For all languages, we use two different embedding methods, fastText (Bojanowski et al., 2017) and GloVe (Pennington et al., 2014), to perform word-embedding based translations and train the NER model, respectively. For fastText, we use the publicly available embeddings trained on Wikipedia for all languages. For GloVe, we use the publicly available embeddings pre-trained on Gigaword and Wikipedia for English. For Spanish, German and Dutch, we use Spanish Gigaword and Wikipedia, German WMT News Crawl data and Wikipedia, and Dutch H a = softmax(QK > ) (E − I)H = [ha1 , ha2 , .."
D18-1034,P15-2064,0,0.261305,"y using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across differ"
D18-1034,D08-1063,0,0.0600569,"We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space"
D18-1034,D13-1141,0,0.0502521,"ges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training. Approaches based on parallel corpora usually learn bilingual word embeddings that can produce similar representations for aligned sentences (Hermann and Blunsom, 2014; Chandar et al., 2014). Jointlytrained models combine the common monolingual training objective with a cross-lingual training objective that often comes from parallel corpus (Zou et al., 2013; Gouws et al., 2015). Recently, unsupervised approaches also have been used to align two sets of word embeddings by learning a mapping through adversarial learning or selflearning (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). References 6 Acknowledgments We thank Stephen Mayhew for sharing the data, and Zihang Dai for meaningful discussion. This research was sponsored by Defense Advanced Research Projects Agency Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program, issued by DARPA/I2O under Contract No. HR0011-15-C011"
D18-1034,Q13-1001,0,0.337503,"Missing"
D18-1034,N12-1052,0,0.317254,"Missing"
D18-1034,W02-2024,0,0.222579,"rs similar to those seen at training time, which we posit introduces a level of flexibility with respect to the word order, and thus may allow for better generalization. Let H = [h1 , h2 , · · · , hn ]> be a sequence of word-level hidden representations. We apply a single layer MLP on H to obtain the queries Q and keys K = tanh(HW + b), where W ∈ Rd×d is a parameter matrix and b ∈ Rd is a bias term, with d being the hidden state size. The output of attention layer is defined as: Experimental Settings We evaluate our proposed methods on the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain 4 European languages, English, German, Dutch and Spanish. For all experiments, we use English as the source language and translate its training data into the target language. We train a model on the translated data, and test it on the target language. For each experiment, we run our models 5 times using different seeds and report the mean and standard deviation, as suggested by Reimers and Gurevych (2017). Word Embeddings For all languages, we use two different embedding methods, fastText (Bojanowski et al., 2017) and GloVe (Pennington et a"
D18-1034,W03-0419,0,0.350689,"Missing"
D18-1034,K16-1022,0,0.679344,"umber with 0 when used as input to the character level Bi-LSTM. Network Training We use SGD with momentum to train the NER model for 30 epochs, and select the best model on the target language development set. We choose the initial learning rate 4.2.1 Comparison with Dictionary-Based Translation Table 1 also presents results of a comparison between our proposed BWE translation method and the “cheap translation” baseline of (Mayhew et al., 2017). The size of the dictionaries used by both 4 https://github.com/facebookresearch/ MUSE 374 Model ∗ T¨ackstr¨om et al. (2012) ∗ Nothman et al. (2013) ∗ Tsai et al. (2016) ∗ Ni et al. (2017) ∗† Mayhew et al. (2017) ∗ Mayhew et al. (2017) (only Eng. data) Our methods: BWET (id.c.) BWET (id.c.) + self-att. BWET (adv.) BWET (adv.) + self-att. BWET BWET + self-att. ∗ BWET on data from Mayhew et al. (2017) ∗ BWET + self-att. on data from Mayhew et al. (2017) ∗ Our supervised results Spanish 59.30 61.0 60.55 65.10 65.95 51.82 Dutch 58.40 64.00 61.60 65.40 66.50 53.94 German 40.40 55.80 48.10 58.50 59.11 50.96 Extra Resources parallel corpus Wikipedia Wikipedia Wikipedia, parallel corpus, 5K dict. Wikipedia, 1M dict. 1M dict. 71.14 ± 0.60 72.37 ± 0.65 70.54 ± 0.85 71."
D18-1034,Q14-1005,0,0.030546,"t entity labels to a low-resource target language with few or no labels. Specifically, in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017). Since parallel corpora may not be always available, Mayhew et al. (2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can directly exploit features extracted from"
D18-1034,H01-1035,0,0.339379,"esource languages when some named entities are not covered by the dictionary or the translation is not reliable. We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to trai"
D18-1034,P17-1179,0,0.166689,"ociation for Computational Linguistics art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surfac"
D18-1039,P15-1166,0,0.642249,"the mapping of a source language to a target language without any need for training or tuning any component of the system separately. This has led to a rapid progress in NMT and its successful adoption in many large-scale settings (Wu et al., 2016; Crego et al., 2016). The encoder-decoder abstraction makes it conceptually feasible to build a system that maps any source sentence in any language to a vector representation, and then decodes this representation into any target language. Thus, various approaches have been proposed to extend this abstraction for multilingual MT (Luong et al., 2016; Dong et al., 2015; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016a). Prior work in multilingual NMT can be broadly categorized into two paradigms. The first, univer1 In fact, it could likely be applied in other scenarios, such as domain adaptation, as well. 425 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 425–435 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics be applied to most existing NMT systems with some minor modification, and it is able to accommodate attention layers seamlessly. 2. Multilingua"
D18-1039,D18-1549,0,0.0321517,"a model on a many-to-one translation task. Closest to our work are more recent approaches, already described in Section 2 (Firat et al., 2016a; Johnson et al., 2017; Ha et al., 2016), that attempt to enforce different kinds of parameter sharing across languages. Parameter sharing in multilingual NMT naturally enables semi-supervised and zero-shot learning. Unsupervised learning has been previously explored with key ideas such as back-translation (Sennrich et al., 2016a), dual learning (He et al., 2016), common latent space learning (Lample et al., 2018), etc. In the vein of multilingual NMT, Artetxe et al. (2018) proposed a model that uses a shared encoder and multiple decoders with a focus on unsupervised translation. The entire system uses cross-lingual embeddings and is trained to reconstruct its input using only monolingual data. Zero-shot translation was first attempted in (Firat et al., 2016b) who performed zero-zhot translation using their pre-trained multi-way multilingual model, fine-tuning it with pseudo-parallel data generated by the model itself. This was recently extended using a teacher-student framework (Chen et al., 2017). Later, zero-shot translation without any additional steps was a"
D18-1039,N16-1101,0,0.611934,"single model for all languages. Universal NMT lacks any language-specific parameterization, which is an oversimplification and detrimental when we have very different languages and limited training data. As verified by our experiments, the method of Johnson et al. (2017) suffers from high sample complexity and thus underperforms in limited data settings. The universal model proposed by Ha et al. (2016) requires a new coding scheme for the input sentences, which results in large vocabulary sizes that are difficult to scale. The second paradigm, per-language encoder-decoder (Luong et al., 2016; Firat et al., 2016a), uses separate encoders and decoders for each language. This does not allow for sharing of information across languages, which can result in overparameterization and can be detrimental when the languages are similar. In this paper, we strike a balance between these two approaches, proposing a model that has the ability to learn parameters separately for each language, but also share information between similar languages. We propose using a new contextual parameter generator (CPG) which (a) generalizes all of these methods, and (b) mitigates the aforementioned issues of universal and per-lan"
D18-1039,D16-1026,0,0.128128,"Missing"
D18-1039,W16-2358,0,0.0355132,"Missing"
D18-1039,N18-1032,0,0.0438699,", 2016b) who performed zero-zhot translation using their pre-trained multi-way multilingual model, fine-tuning it with pseudo-parallel data generated by the model itself. This was recently extended using a teacher-student framework (Chen et al., 2017). Later, zero-shot translation without any additional steps was attempted in (Johnson et al., 2017) using their shared encoderdecoder network. An iterative training procedure that leverages the duality of translations directly generated by the system for zero-shot learning was proposed by Lakew et al. (2017). For extremely low resource languages, Gu et al. (2018) proposed sharing lexical and sentence-level representations across multiple source languages with a single target language. Closely related is the work of Cheng et al. (2016) who proposed the joint training of source-to-pivot and pivot-to-target NMT models. Ha et al. (2018) are probably the first to introduce a similar idea to that of having one network (called a hypernetwork) generate the parameters of another. However, in that work, the input to the hypernetwork are structural features of the original network (e.g., layer size and index). Al-Shedivat et al. (2017) also propose a related met"
D18-1039,2004.iwslt-evaluation.1,0,0.0530795,"Missing"
D18-1039,P17-1176,0,0.0574597,"(Lample et al., 2018), etc. In the vein of multilingual NMT, Artetxe et al. (2018) proposed a model that uses a shared encoder and multiple decoders with a focus on unsupervised translation. The entire system uses cross-lingual embeddings and is trained to reconstruct its input using only monolingual data. Zero-shot translation was first attempted in (Firat et al., 2016b) who performed zero-zhot translation using their pre-trained multi-way multilingual model, fine-tuning it with pseudo-parallel data generated by the model itself. This was recently extended using a teacher-student framework (Chen et al., 2017). Later, zero-shot translation without any additional steps was attempted in (Johnson et al., 2017) using their shared encoderdecoder network. An iterative training procedure that leverages the duality of translations directly generated by the system for zero-shot learning was proposed by Lakew et al. (2017). For extremely low resource languages, Gu et al. (2018) proposed sharing lexical and sentence-level representations across multiple source languages with a single target language. Closely related is the work of Cheng et al. (2016) who proposed the joint training of source-to-pivot and pivo"
D18-1039,Q17-1024,0,0.161978,"Missing"
D18-1039,D15-1166,0,0.0540913,"ate representation that can later be used by a decoder to generate sentences in a target language. Generally, we can think of the encoder as a function, f (enc) , parameterized by θ(enc) . Similarly, we can think of the decoder as another function, f (dec) , parameterized by θ(dec) . The goal of learning to translate can then be defined as finding the values for θ(enc) and θ(dec) that result in the best translations. A large amount of previous work proposes novel designs for the encoder/decoder module. For example, using attention over the input sequence while decoding (Bahdanau et al., 2015; Luong et al., 2015) provides significant gains in translation performance.2 Parameter Generator. All modules defined so far have previously been used when describing NMT systems and are thus easy to conceptualize. However, in previous work, most models are trained for a given language pair, and it is not trivial to extend them to work for multiple pairs of languages. We introduce here the concept of the parameter generator, which makes it easy to define and describe multilingual NMT systems. This module is responsible for generating θ(enc) and θ(dec) for any given source and target language. Different parameter"
D18-1039,P18-2050,1,0.726171,"= {θj }j=1 and (enc) (enc) Pj θj ∈R , where G denotes the number of groups. Then, we define: Parameter Generator Network (enc) We refer to the functions g (enc) and g (dec) as parameter generator networks. Even though our proposed NMT framework does not rely on a specific choice for g (enc) and g (dec) , here we describe the functional form we used for our experiments. Our goal is to provide a simple form that works, and for which we can reason about. For this reason, we decided to define the parameter generator networks as simple linear transforms, similar to the factored adaptation model of Michel and Neubig (2018), which was only applied to the bias terms of the output softmax: g (enc) (ls ) , W(enc) ls , g (dec) (lt ) , W (dec) lt , θj (enc) (enc) , Wj (enc) (enc) Pj 0 ls , (3) (enc) where Wj ∈ RPj ×M and Pj ∈ 0 ×M M 0 R , with M < M (and similarly for the de(enc) coder parameters). We can see now that Pj is used to extract the relevant information (size M 0 ) for parameter group j, from the larger language embedding (size M ). This allows us to control the parameter sharing across languages in the following way: if we want to increase the number of per-language parameters (i.e., the language embeddin"
D18-1039,N16-1004,0,0.047795,"language pairs. 7 Note that, our results for IWSLT-17 are not comparable to those of the official challenge report (Cettolo et al., 2017), as we use less training data, a smaller baseline model, and our evaluation pipeline potentially differs. However, the numbers presented for all methods in this paper are comparable, as they were all obtained using the same baseline model and evaluation pipeline. 8 https://github.com/eaplatanios/symphony-mt https://github.com/tensorflow/nmt 10 https://github.com/rsennrich/subword-nmt 9 432 (Caglayan et al., 2016) for translation across multiple modalities. Zoph and Knight (2016) flipped this idea with a many-to-one translation model, however requiring the presence of a multi-way parallel corpus between all the languages, which is difficult to obtain. Lee et al. (2017) used a single character-level encoder across multiple languages by training a model on a many-to-one translation task. Closest to our work are more recent approaches, already described in Section 2 (Firat et al., 2016a; Johnson et al., 2017; Ha et al., 2016), that attempt to enforce different kinds of parameter sharing across languages. Parameter sharing in multilingual NMT naturally enables semi-superv"
D18-1039,P16-1009,0,0.513938,"art performance. We first introduce a modular framework that can be used to define and describe most existing NMT systems. Then, in Section 3, we introduce our main contribution, the contextual parameter generator (CPG), in terms of that framework. We also argue that the proposed approach takes us a step closer to a common universal interlingua. 2 two-way mapping from preprocessed sentences to sequences of word indices that will be used for the translation. A commonly used proposal for defining the vocabulary is the byte-pair encoding (BPE) algorithm which generates subword unit vocabularies (Sennrich et al., 2016b). This eliminates the notion of out-of-vocabulary words, often resulting in increased translation quality. Encoder/Decoder. The encoder takes in indexed source language sentences, and produces an intermediate representation that can later be used by a decoder to generate sentences in a target language. Generally, we can think of the encoder as a function, f (enc) , parameterized by θ(enc) . Similarly, we can think of the decoder as another function, f (dec) , parameterized by θ(dec) . The goal of learning to translate can then be defined as finding the values for θ(enc) and θ(dec) that resul"
D18-1050,D11-1033,0,0.0751684,"Missing"
D18-1050,2012.eamt-1.60,0,0.0588849,"h WMT 2015 translation task.6 This amounts to ≈ 40.86 million sentences. English and French, we normalize the punctuation, lowercase and tokenize the comments using the Moses tokenizer. For Japanese, we simply lowercase the alphabetical characters in the comments. Note that this normalization is done for the purpose of noise detection only. The collected comments are released without any kind of preprocessing. We apply the same normalization procedure to the contrast corpora. Japanese: We aggregate three small/medium sized MT datasets: KFTT (Neubig, 2011), JESC (Pryzant et al.) and TED talks (Cettolo et al., 2012), amounting to ≈ 4.19 million sentences. 3.3 Unknown words In the case of French and English, a clear indication of noise is the presence of out-of-vocabulary words (OOV): we record all lowercased words encountered in our reference corpus described in Section 3.2 and only keep comments that contain at least one OOV. Since we did not use word segmentation for the Japanese reference corpus, we found this method not to be very effective to select Japanese comments and therefore skipped this step. Identifying Noisy Comments We now describe the procedure used to identify comments containing noise."
D18-1050,P18-1163,0,0.103298,"University) effect, where state-of-the-art models become brittle while human’s language processing capability is more robust (Sakaguchi et al., 2017; Belinkov and Bisk, 2018). Efforts to address these challenges have been focused on creating in-domain datasets and annotations (Owoputi et al.; Kong et al.; Blodgett et al., 2017), and domain adaptation training (Luong and Manning, 2015). In MT, improvements were obtained for SMT (Formiga and Fonollosa). However, the specific challenges for neural machine translation have not been studied until recently (Belinkov and Bisk, 2018; Sperber et al.; Cheng et al., 2018). The first provides empirical evidence of non-trivial quality degradation when source sentences contain natural noise or synthetic noise within words, and the last two explore data augmentation and adversarial approaches of adding noise efficiently to training data to improve robustness. Our work also contributes to recent advances in evaluating neural machine translation quality with regard to specific linguistic phenomena, such as Conclusion We proposed a new dataset to test MT models for robustness to the types of noise encountered in natural language on the Internet. We contribute paralle"
D18-1050,P16-1185,0,0.0789457,"Missing"
D18-1050,P17-2061,0,0.0911127,"Missing"
D18-1050,C10-1075,0,0.129871,"Missing"
D18-1050,P12-3005,0,0.148496,"Missing"
D18-1050,N13-1037,0,0.238462,"Missing"
D18-1050,2015.iwslt-evaluation.11,0,0.0847387,"f content (Baldwin et al.). To give the reader an idea of the challenges posed to MT and Natural Language Processing (NLP) systems operating on this kind of text, we provide a non-exhaustive list of types of noise and more generally input variations that deviate from standard MT training data we’ve encountered in Reddit comments: masked) Is Translating Noisy Text just another Adaptation Problem? To a certain extent, translating noisy text is a type of adaptation, which has been studied extensively in the context of both Statistical Machine Translation (SMT) and NMT (Axelrod et al.; Li et al.; Luong and Manning, 2015; Chu et al.; Miceli Barone et al.; Wang et al.; Michel and Neubig, 2018). However, it presents many differences with previous domain adaptation problems, where the main goal is to adapt from a particular topic or style. In the case of noisy text, it will not only be the case that a particular word will be translated in a different way than it is in the general domain (e.g. as in the case of “sub”), but also that there will be increased lexical variation (e.g. due to spelling or typographical errors), and also inconsistency in grammar (e.g. due to omissions of critical words or mis-usage). The"
D18-1050,C12-2032,0,0.0739649,"Missing"
D18-1050,D17-1263,0,0.0507482,"Missing"
D18-1050,P18-2050,1,0.854982,"posed to MT and Natural Language Processing (NLP) systems operating on this kind of text, we provide a non-exhaustive list of types of noise and more generally input variations that deviate from standard MT training data we’ve encountered in Reddit comments: masked) Is Translating Noisy Text just another Adaptation Problem? To a certain extent, translating noisy text is a type of adaptation, which has been studied extensively in the context of both Statistical Machine Translation (SMT) and NMT (Axelrod et al.; Li et al.; Luong and Manning, 2015; Chu et al.; Miceli Barone et al.; Wang et al.; Michel and Neubig, 2018). However, it presents many differences with previous domain adaptation problems, where the main goal is to adapt from a particular topic or style. In the case of noisy text, it will not only be the case that a particular word will be translated in a different way than it is in the general domain (e.g. as in the case of “sub”), but also that there will be increased lexical variation (e.g. due to spelling or typographical errors), and also inconsistency in grammar (e.g. due to omissions of critical words or mis-usage). The sum of these differences warrants that noisy MT be treated as a separate"
D18-1050,P10-2041,0,0.118863,"Missing"
D18-1050,D13-1176,0,0.187505,"Missing"
D18-1050,P11-2093,1,0.734456,"ither end of the distribution of normalized LM scores within the set of collected comments. We only keep comments whose normalized score is within the 5-70 percentile for English (resp. 5-60 for French and 10-70 for Japanese). These numbers are chosen by manually inspecting the data. we isolate ≈ 900 samples in each direction to serve as validation data. Information about the size of the data can be found in Table 1, 2 and 3 for the test, training and validation sets respectively. We tokenize the English and French data with the Moses (Koehn et al.) tokenizer and the Japanese data with Kytea (Neubig et al., 2011) before counting the number of tokens in each dataset. • Choose 15, 000 samples at random. 3.5 Monolingual Corpora After the creation of the parallel train and test sets, a large number of unused comments remain in each language, which we provide as monolingual corpora. This additional data has two purposes: first, it serves as a resource for in-domain training using semi-supervised methods relying on monolingual data (e.g. Cheng et al.; Zhang and Zong). Second, it provides a language modeling dataset for noisy text in three languages. We select 3, 000 comments at random in each dataset to for"
D18-1050,D17-1155,0,0.0671191,"Missing"
D18-1050,1983.tc-1.13,0,0.347863,"Missing"
D18-1050,W18-6319,0,0.0515166,"well as dropout (with probability 0.3). We used BPE subwords to handle OOV words. Full configuration details as well as code to reproduce the baselines is available at https://github. com/pmichel31415/mtnt. 5.2 en-fr 33.52 33.03 21.77 29.73 en-ja 14.51 20.82 15.77 9.02 12.45 fr-en 28.93 30.76 23.27 30.29 ja-en 13.25 20.77 18.00 6.65 9.82 Table 6: BLEU scores of NMT models on the various datasets. associated with these corpora to serve as validation data and evaluate on each respective test set separately. 5.3 Results We use sacreBLEU18 , a standardized BLEU score evaluation script proposed by Post (2018), for BLEU evaluation of our benchmark dataset. It takes in detokenized references and hypotheses and performs its own tokenization before computing BLEU score. We specify the intl tokenization option. In the case of Japanese text, we run both hypothesis and reference through KyTea before computing BLEU score. We strongly encourage that evaluation be performed in the same manner in subsequent work, and will provide both scripts and an evaluation web site in order to facilitate reproducibility. Table 6 lists the BLEU scores for our models on the relevant test sets in the two language pairs, inc"
D18-1050,D11-1141,0,0.0896701,"Missing"
D18-1050,W17-4408,0,\N,Missing
D18-1100,J96-1002,0,0.232513,"+ ⌧ H(q(X, (1) where ⌧ controls the strength of the diversity objective. The first term in (1) instantiates the smoothness assumption, which encourages q to draw samples that are similar to (x, y). Meanwhile, the second term in (1) encourages more diverse samples from q. Together, the objective J(q; x, y) extends the information in the “pivotal” empirical sample (x, y) to a diverse set of similar cases. This echoes our particular parameterization of q in Section 2.2. The objective J(q; x, y) in (1) is the canonical maximum entropy problem that one often encounters in deriving a max-ent model (Berger et al., 1996), which has the analytic solution: exp {s(b x, yb; x, y)/⌧ } q⇤ (b x, yb|x, y) = P x0 , yb0 ; x, y)/⌧ } x b0 ,b y 0 exp {s(b (2) Note that (2) is a fairly generic solution which is agnostic to the choice of the similarity measure s. Obviously, not all similarity measures are equally good. Next, we will show that some existing algorithms can be seen as specific instantiations under our framework. Moreover, this leads us to propose a novel and effective data augmentation algorithm. 2.4 Existing and New Algorithms Word Dropout. In the context of machine translation, Sennrich et al. (2016a) propos"
D18-1100,P11-2031,0,0.053036,"as in Section 2.4. Additionally, on the en-de task, we compare SwitchOut against back-translation (Sennrich et al., 2016b). SwitchOut vs. Word Dropout and RAML. We report the BLEU scores of SwitchOut, word dropout, and RAML on the test sets of the tasks in Table 1. To account for variance, we run each experiment multiple times and report the median BLEU. Specifically, each experiment without SwitchOut is run for 4 times, while each experiment with SwitchOut is run for 9 times due to its inherently higher variance. We also conduct pairwise statistical significance tests using paired bootstrap (Clark et al., 2011), and record the results in Table 1. For 4 of the 6 settings, SwitchOut delivers significant improvements over the best baseline without SwitchOut. For the remaining two settings, the differences are not statistically significant. The gains in BLEU with SwitchOut over the best baseline on WMT 15 en-de are all significant (p < 0.0002). Notably, SwitchOut on the source demonstrates as large gains as these obtained by RAML on the target side, and SwitchOut delivers further improvements when combined with RAML. SwitchOut vs. Back Translation. Traditionally, data-augmentation is viewed as a method"
D18-1100,P17-2090,0,0.0977004,"stems brittle (Belinkov and Bisk, 2018). Due to such difficulties, the literature in data augmentation for NMT is relatively scarce. To our knowledge, data augmentation techniques for NMT fall into two categories. The first category is based on back-translation (Sennrich et al., 2016b; Poncelas et al., 2018), which utilizes monolingual data to augment a parallel training corpus. While effective, back-translation is often vulnerable to errors in initial models, a common problem of self-training algorithms (Chapelle et al., 2009). The second category is based on word replacements. For instance, Fadaee et al. (2017) propose to replace words in the target sentences with rare words in the target vocabulary according to a language model, and then modify the aligned source words accordingly. While this method generates augmented data with relatively high quality, it requires several complicated preprocessing steps, and is only shown to be effective for low-resource datasets. Other generic word replacement methods include word dropout (Sennrich et al., 2016a; Gal and Ghahramani, 2016), which uniformly set some word embeddings to 0 at random, and Reward Augmented Maximum Likelihood (RAML; Norouzi et al. (2016)"
D18-1100,2015.iwslt-evaluation.11,0,0.0560053,"scales: 1) IWSLT 2015 English-Vietnamese (en-vi); 2) IWSLT 2016 German-English (de-en); and 3) WMT 2015 English-German (en-de). All translations are wordbased. These tasks and pre-processing steps are standard, used in several previous works. Detailed statistics and pre-processing schemes are in Appendix A.3. Models and Experimental Procedures. Our translation model, i.e. p✓ (y|x), is a Transformer network (Vaswani et al., 2017). For each dataset, we first train a standard Transformer model without SwitchOut and tune the hyper-parameters on the dev set to achieve competitive results. (w.r.t. Luong and Manning (2015); Gu et al. (2018); Vaswani et al. (2017)). Then, fixing all hyper-parameters, and fixing ⌧y = 0, we tune the ⌧x rate, which controls how far we are willing to let x b deviate from x. Our hyper-parameters are listed in Appendix A.4. Baselines. While the Transformer network without SwitchOut is already a strong baseline, we also compare SwitchOut against two other baselines that further use existing varieties of data augmentation: 1) word dropout on the source side with the dropping probability of word = 0.1; and 2) RAML on the target side, as in Section 2.4. Additionally, on the en-de task, we"
D18-1100,D15-1166,1,0.531409,") IWSLT 2015 English-Vietnamese (en-vi); 2) IWSLT 2016 German-English (de-en); and 3) WMT 2015 English-German (en-de). All translations are wordbased. These tasks and pre-processing steps are standard, used in several previous works. Detailed statistics and pre-processing schemes are in Appendix A.3. Models and Experimental Procedures. Our translation model, i.e. p✓ (y|x), is a Transformer network (Vaswani et al., 2017). For each dataset, we first train a standard Transformer model without SwitchOut and tune the hyper-parameters on the dev set to achieve competitive results. (w.r.t. Luong and Manning (2015); Gu et al. (2018); Vaswani et al. (2017)). Then, fixing all hyper-parameters, and fixing ⌧y = 0, we tune the ⌧x rate, which controls how far we are willing to let x b deviate from x. Our hyper-parameters are listed in Appendix A.4. Baselines. While the Transformer network without SwitchOut is already a strong baseline, we also compare SwitchOut against two other baselines that further use existing varieties of data augmentation: 1) word dropout on the source side with the dropping probability of word = 0.1; and 2) RAML on the target side, as in Section 2.4. Additionally, on the en-de task, we"
D18-1100,W16-2323,0,0.448002,"an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix. 1 Introduction and Related Work Data augmentation algorithms generate extra data points from the empirically observed training set to train subsequent machine learning algorithms. While these extra data points may be of lower quality than those in the training set, their quantity and diversity have proven to benefit various learning algorithms (DeVries and Taylor, 2017; Amodei et al., 2016). In image processing, simple augmentation techniques such as flipping, cropping, or increasing and decreasing the contrast of the image are bot"
D18-1100,P16-1009,0,0.540567,"an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix. 1 Introduction and Related Work Data augmentation algorithms generate extra data points from the empirically observed training set to train subsequent machine learning algorithms. While these extra data points may be of lower quality than those in the training set, their quantity and diversity have proven to benefit various learning algorithms (DeVries and Taylor, 2017; Amodei et al., 2016). In image processing, simple augmentation techniques such as flipping, cropping, or increasing and decreasing the contrast of the image are bot"
D18-1103,W17-3203,1,0.758662,"appropriate helper when this is not the case is an interesting problem for future work. 4 In contrast to Gu et al. (2018), who train on 10 languages. Malaviya et al. (2017); Tiedemann (2018) train NMT on over 1,000 languages, but only as a feature extractor for downstream tasks; MT accuracy itself is not evaluated. 876 LRL aze bel glg slk train 5.94k 4.51k 10.0k 61.5k dev 671 248 682 2,271 test 903 664 1,007 2,445 HRL tur rus por ces train 182k 208k 185k 103k word embeddings, 512-dimensional hidden states, and a standard LSTM-based decoder. Following standard practice (Sennrich et al., 2016; Denkowski and Neubig, 2017), we break low-frequency words into subwords using the sentencepiece toolkit.6 There are two alternatives for creating subword units: jointly learning subwords over all source language, or separately learning subwords for each source language, then taking the union of all the subword vocabularies as the vocabulary for the multilingual model. Previous work on multilingual training has preferred the former (Nguyen and Chiang, 2017), but in this paper we use the latter for two reasons: (1) because data in the LRL will not affect the subword units from the other languages, in the cold-start scenar"
D18-1103,N16-1101,0,0.0742726,"tation, adaptation is helpful, particularly (and unsurprisingly) in the cold-start case. When adapting directly to only the target language (“→Sing.”), adapting from the massively multilingual model performs better, indicating that information about all input languages is better than just a single language. Next, comparing with our proposed method of adding similar 4 Related Work While adapting MT systems to new languages is a long-standing challenge (Schultz and Black, 2006; Jabaian et al., 2013), multilingual NMT is highly promising in its ability to abstract across 878 language boundaries (Firat et al., 2016; Ha et al., 2016; Johnson et al., 2016). Results on multilingual training for low-resource translation (Gu et al., 2018; Qi et al., 2018) further demonstrates this potential, although these works do not consider adaptation to new languages, the main focus of our work. Notably, we did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation (Zoph et al., 2016); this is orthogonal to our multi-lingual training approach but the two methods could potentially be combined. Finally, unsupervised NMT approaches (Artetxe et al., 2017; Lample et al., 2018, 2"
D18-1103,N18-1032,0,0.440598,"g data as data becomes available. For the cold-start models, we start with a model that is only trained on the HRL similar to the LRL (Bi− ), or a model trained 2 Translation into LRLs, is a challenging and interesting problem in it’s own right, but beyond the scope of the paper. 3 “Related” could mean different things: typologically related or having high lexical overlap. In our experiments our LRLs are all selected to have an helper that is highly similar in both aspects, but choosing an appropriate helper when this is not the case is an interesting problem for future work. 4 In contrast to Gu et al. (2018), who train on 10 languages. Malaviya et al. (2017); Tiedemann (2018) train NMT on over 1,000 languages, but only as a feature extractor for downstream tasks; MT accuracy itself is not evaluated. 876 LRL aze bel glg slk train 5.94k 4.51k 10.0k 61.5k dev 671 248 682 2,271 test 903 664 1,007 2,445 HRL tur rus por ces train 182k 208k 185k 103k word embeddings, 512-dimensional hidden states, and a standard LSTM-based decoder. Following standard practice (Sennrich et al., 2016; Denkowski and Neubig, 2017), we break low-frequency words into subwords using the sentencepiece toolkit.6 There are two al"
D18-1103,W17-3204,0,0.0605194,"rained models, indicating that adapting from seed models is a good strategy for rapid construction of MT systems in new languages. Comparing the cold-start adaptation strategies, we can see that in general, the higher the density of target language training data, the faster the training converges to a solution, but the worse the final solution is. This suggests that there is a speed/accuracy tradeoff in the amount of similar language regularization we apply during fine-tuning. capacity, training on a highly resourced language is effective. Comparing with the phrase-based baseline, as noted by Koehn and Knowles (2017) NMT tends to underperform on low-resource settings when trained only on the data available for these languages. However, multilingual training of any variety quickly remedies this issue; all outperform phrase-based handily. More interestingly, examining the cold-start results, we can see that even systems with no data in the target language are able to achieve nontrivial accuracies, up to 15.5 BLEU on glg-eng. Interestingly, in the cold-start scenario, the All− model bests the Bi− model, indicating that massively multilingual training is more useful in this setting. In contrast, the unsupervi"
D18-1103,J82-2005,0,0.500507,"Missing"
D18-1103,2010.eamt-1.37,0,0.0300016,"en disaster strikes, news and social media are invaluable sources of information, allowing humanitarian organizations to rapidly mitigate crisis situations and save lives (Vieweg et al., 2010; Neubig et al., 2011; Starbird et al., 2012). However, language barriers looms large over these efforts, especially when disasters occur in parts of the world that use less common languages. In these cases, machine translation (MT) technology can be a valuable tool, with one widely-heralded success story being the deployment of Haitian Creoleto-English translation systems during the earthquakes in Haiti (Lewis, 2010; Munro, 2010). However, data-driven MT systems, particularly neural machine translation (NMT; Kalchbrenner 1 Code to reproduce experiments at https://github. com/neubig/rapid-adaptation 875 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 875–880 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics BLEU points averaged over several language pairs compared to previous methods adapting to only the LRL. 2 model that has wide coverage of vocabulary and syntax of a large number of languages, but also has the"
D18-1103,D17-1268,1,0.82259,"d-start models, we start with a model that is only trained on the HRL similar to the LRL (Bi− ), or a model trained 2 Translation into LRLs, is a challenging and interesting problem in it’s own right, but beyond the scope of the paper. 3 “Related” could mean different things: typologically related or having high lexical overlap. In our experiments our LRLs are all selected to have an helper that is highly similar in both aspects, but choosing an appropriate helper when this is not the case is an interesting problem for future work. 4 In contrast to Gu et al. (2018), who train on 10 languages. Malaviya et al. (2017); Tiedemann (2018) train NMT on over 1,000 languages, but only as a feature extractor for downstream tasks; MT accuracy itself is not evaluated. 876 LRL aze bel glg slk train 5.94k 4.51k 10.0k 61.5k dev 671 248 682 2,271 test 903 664 1,007 2,445 HRL tur rus por ces train 182k 208k 185k 103k word embeddings, 512-dimensional hidden states, and a standard LSTM-based decoder. Following standard practice (Sennrich et al., 2016; Denkowski and Neubig, 2017), we break low-frequency words into subwords using the sentencepiece toolkit.6 There are two alternatives for creating subword units: jointly lear"
D18-1103,2010.amta-workshop.1,0,0.0172353,"trikes, news and social media are invaluable sources of information, allowing humanitarian organizations to rapidly mitigate crisis situations and save lives (Vieweg et al., 2010; Neubig et al., 2011; Starbird et al., 2012). However, language barriers looms large over these efforts, especially when disasters occur in parts of the world that use less common languages. In these cases, machine translation (MT) technology can be a valuable tool, with one widely-heralded success story being the deployment of Haitian Creoleto-English translation systems during the earthquakes in Haiti (Lewis, 2010; Munro, 2010). However, data-driven MT systems, particularly neural machine translation (NMT; Kalchbrenner 1 Code to reproduce experiments at https://github. com/neubig/rapid-adaptation 875 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 875–880 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics BLEU points averaged over several language pairs compared to previous methods adapting to only the LRL. 2 model that has wide coverage of vocabulary and syntax of a large number of languages, but also has the drawback in t"
D18-1103,I11-1108,1,0.784654,"language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similarlanguage regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.1 1 Introduction When disaster strikes, news and social media are invaluable sources of information, allowing humanitarian organizations to rapidly mitigate crisis situations and save lives (Vieweg et al., 2010; Neubig et al., 2011; Starbird et al., 2012). However, language barriers looms large over these efforts, especially when disasters occur in parts of the world that use less common languages. In these cases, machine translation (MT) technology can be a valuable tool, with one widely-heralded success story being the deployment of Haitian Creoleto-English translation systems during the earthquakes in Haiti (Lewis, 2010; Munro, 2010). However, data-driven MT systems, particularly neural machine translation (NMT; Kalchbrenner 1 Code to reproduce experiments at https://github. com/neubig/rapid-adaptation 875 Proceeding"
D18-1103,W18-1818,1,0.824654,"-language-toEnglish TED corpus (Qi et al., 2018), which is ideal for our purposes because it has a wide variety of languages over several language families, some high-resourced and some low-resourced. Like Qi et al. (2018), we experiment with Azerbaijani (aze), Belarusian (bel), and Galician (glg) to English, and also additionally add Slovak (slk), a slightly higher resourced language, for contrast. These languages are all paired with a similar HRL: Turkish (tur), Russian (rus), Portuguese (por), and Czech (ces) respectively. Data sizes are shown in Table 1. Models are implemented using xnmt (Neubig et al., 2018), commit 8173b1f, and start with the recipe for training on IWSLT TED5 . The model consists of an attentional neural machine translation model (Bahdanau et al., 2015), using bi-directional LSTM encoders, 128-dimensional 5 3.2 Experimental Results Table 2 shows our main translation results, with warm-start scenarios in the upper half and coldstart scenarios in the lower half. Does Multilingual Training Help? To answer this question, we can compare the warm-start Sing., Bi, and All settings, and find that the answer is a resounding yes, gains of 7-13 BLEU points are obtained by going from single"
D18-1103,I17-2050,0,0.108058,"es train 182k 208k 185k 103k word embeddings, 512-dimensional hidden states, and a standard LSTM-based decoder. Following standard practice (Sennrich et al., 2016; Denkowski and Neubig, 2017), we break low-frequency words into subwords using the sentencepiece toolkit.6 There are two alternatives for creating subword units: jointly learning subwords over all source language, or separately learning subwords for each source language, then taking the union of all the subword vocabularies as the vocabulary for the multilingual model. Previous work on multilingual training has preferred the former (Nguyen and Chiang, 2017), but in this paper we use the latter for two reasons: (1) because data in the LRL will not affect the subword units from the other languages, in the cold-start scenario we can postpone creation of subword units for the LRL until directly before we start training on the LRL itself, and (2) we need not be concerned with the LRL being “overwhelmed” by the higher-resourced languages when calculating statistics used in the creation of subword units, because all languages get an equal share.7 In the experiments, we use a subword vocabulary of 8,000 for each language. We also compare with two additi"
D18-1103,N18-2084,1,0.881386,"nt overfitting. We do this in two ways: Corpus Concatenation: Simply concatenate the data from the two corpora, so that we have a small amount of data in the LRL, and a large amount of data in the similar HRL. Balanced Sampling: Every time we select a minibatch to do training, we either sample it from the LRL, or from the HRL according to a fixed ratio. We try different sampling strategies, including sampling with a 1-to-1 ratio, 1-to-2 ratio, and 1-to-4 ratio for the LRL and HRL respectively. 3 Experiments 3.1 Experimental Setup We perform experiments on the 58-language-toEnglish TED corpus (Qi et al., 2018), which is ideal for our purposes because it has a wide variety of languages over several language families, some high-resourced and some low-resourced. Like Qi et al. (2018), we experiment with Azerbaijani (aze), Belarusian (bel), and Galician (glg) to English, and also additionally add Slovak (slk), a slightly higher resourced language, for contrast. These languages are all paired with a similar HRL: Turkish (tur), Russian (rus), Portuguese (por), and Czech (ces) respectively. Data sizes are shown in Table 1. Models are implemented using xnmt (Neubig et al., 2018), commit 8173b1f, and start"
D18-1103,P16-1162,0,0.139641,"spects, but choosing an appropriate helper when this is not the case is an interesting problem for future work. 4 In contrast to Gu et al. (2018), who train on 10 languages. Malaviya et al. (2017); Tiedemann (2018) train NMT on over 1,000 languages, but only as a feature extractor for downstream tasks; MT accuracy itself is not evaluated. 876 LRL aze bel glg slk train 5.94k 4.51k 10.0k 61.5k dev 671 248 682 2,271 test 903 664 1,007 2,445 HRL tur rus por ces train 182k 208k 185k 103k word embeddings, 512-dimensional hidden states, and a standard LSTM-based decoder. Following standard practice (Sennrich et al., 2016; Denkowski and Neubig, 2017), we break low-frequency words into subwords using the sentencepiece toolkit.6 There are two alternatives for creating subword units: jointly learning subwords over all source language, or separately learning subwords for each source language, then taking the union of all the subword vocabularies as the vocabulary for the multilingual model. Previous work on multilingual training has preferred the former (Nguyen and Chiang, 2017), but in this paper we use the latter for two reasons: (1) because data in the LRL will not affect the subword units from the other langua"
D18-1103,D16-1163,0,0.22301,"is a difficult challenge where research efforts have just begun (Gu et al., 2018). Another hurdle, which to our knowledge has not been covered in previous research, is the time it takes to create such a system. In a crisis situation, time is of the essence, and systems that require days or weeks of training will not be desirable or even feasible. In this paper we focus on the question: how can we create MT systems for new language pairs as accurately as possible, and as quickly as possible? To examine this question we propose NMT methods at the intersection of cross-lingual transfer learning (Zoph et al., 2016) and multilingual training (Johnson et al., 2016), two paradigms that, to our knowledge, have not been used together in previous work. Our methods, laid out in §2 follow the process of training a seed model on a large number of languages, then fine-tuning the model to improve its performance on the language of interest. We propose a novel method of similar-language regularization (SLR) where training data from a second similar languages is used to help prevent over-fitting to the small LRL dataset. In the experiments in §3, we attempt to answer two questions: (1) Which method of creating multi"
D18-1111,W17-4713,0,0.128748,"d Neubig, 2017; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code. However, representing code as a tree is not a trivial task, as the number of nodes in the tree often greatly exceeds the length of the NL description. As a result, tree-based approaches are often incapable of generating correct code for phrases in the corresponding NL description that have low frequency in the training data. In machine translation (MT) problems (Zhang et al., 2018; Gu et al., 2018; Amin Farajian et al., 2017; Li et al., 2018), hybrid methods combining retrieval of salient examples and neural models have proven successful in dealing with rare words. Following the intuition of these models, we hypothesize that our model can benefit from querying pairs of NL descriptions and AST structures from training data. In this paper, we propose R E C ODE, and adaptation of Zhang et al. (2018)’s retrieval-based approach neural MT method to the code generation problem by expanding it to apply to generation of tree structures. Our main contribution is to introduce the use of retrieval methods in neural code gene"
D18-1111,P16-1004,0,0.0465783,"za et al., 2015; Kushman and Barzilay, 2013). For general purpose code generation, some data-driven work has been 4 More example of HS code is provided in the supplementary material. 928 input on early version writing, and anonymous reviewers for useful feedback. This material is based upon work supported by the National Science Foundation under Grant No. 1815287. done for predicting input parsers (Lei et al., 2013) or a set of relevant methods (Raghothaman et al., 2016). Some attempts using neural networks have used sequence-to-sequence models (Ling et al., 2016) or tree-based architectures (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017). Ling et al. (2016); Jia and Liang (2016); Locascio et al. (2016) treat semantic parsing as a sequence generation task by linearizing trees. The closest work to ours are Yin and Neubig (2017) and Rabinovich et al. (2017) which represent code as an AST. Another close work is Dong and Lapata (2018), which uses a two-staged structure-aware neural architecture. They initially generate a lowlevel sketch and then fill in the missing information using the NL and the sketch. Recent works on retrieval-guided neural machine translation have been presented by Gu et al."
D18-1111,P18-1068,0,0.0469892,"oundation under Grant No. 1815287. done for predicting input parsers (Lei et al., 2013) or a set of relevant methods (Raghothaman et al., 2016). Some attempts using neural networks have used sequence-to-sequence models (Ling et al., 2016) or tree-based architectures (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017). Ling et al. (2016); Jia and Liang (2016); Locascio et al. (2016) treat semantic parsing as a sequence generation task by linearizing trees. The closest work to ours are Yin and Neubig (2017) and Rabinovich et al. (2017) which represent code as an AST. Another close work is Dong and Lapata (2018), which uses a two-staged structure-aware neural architecture. They initially generate a lowlevel sketch and then fill in the missing information using the NL and the sketch. Recent works on retrieval-guided neural machine translation have been presented by Gu et al. (2018); Amin Farajian et al. (2017); Li et al. (2018); Zhang et al. (2018). Gu et al. (2018) use the retrieved sentence pairs as extra inputs to the NMT model. Zhang et al. (2018) employ a simpler and faster retrieval method to guide neural MT where translation pieces are n-grams from retrieved target sentences. We modify Zhang et"
D18-1111,P06-1121,0,0.212633,"Missing"
D18-1111,P16-1154,0,0.027875,"ponding words in the input sentence (§3.3), and • at every decoding step, increase the probability of actions that would lead to having these subtrees in the produced tree (§3.4). eration model by Yin and Neubig (2017), which uses sequences of actions to generate the AST before converting it to surface code. Formally, we want to find the best generated AST a ˆ given by: a ˆ = arg max p(a|q) a p(a|q) = T Y 3.1 p(yt |y&lt;t , q) t=1 For every retrieved NL description qm from training set (or retrieved sentence for short), we compute its similarity with input q, using a sentence similarity formula (Gu et al., 2016; Zhang et al., 2018): where yt is the action taken at time step t and y&lt;t = y1 ...yt−1 and T is the number of total time steps of the whole action sequence resulting in AST a. We have two types of actions to build an AST: A PPLY RULE and G EN T OKEN. A PPLY RULE (r) expands the current node in the tree by applying production rule r from the abstract syntax grammar2 to the current node. G EN T OKEN (v) populates terminal nodes with the variable v which can be generated from vocabulary or by C OPYing variable names or values from the NL description. The generation process follows a preorder tra"
D18-1111,P82-1020,0,0.738087,"Missing"
D18-1111,N18-1120,1,0.694397,"edu Abstract Tree-based approaches (Yin and Neubig, 2017; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code. However, representing code as a tree is not a trivial task, as the number of nodes in the tree often greatly exceeds the length of the NL description. As a result, tree-based approaches are often incapable of generating correct code for phrases in the corresponding NL description that have low frequency in the training data. In machine translation (MT) problems (Zhang et al., 2018; Gu et al., 2018; Amin Farajian et al., 2017; Li et al., 2018), hybrid methods combining retrieval of salient examples and neural models have proven successful in dealing with rare words. Following the intuition of these models, we hypothesize that our model can benefit from querying pairs of NL descriptions and AST structures from training data. In this paper, we propose R E C ODE, and adaptation of Zhang et al. (2018)’s retrieval-based approach neural MT method to the code generation problem by expanding it to apply to generation of tree structures. Our main contribution is to introduce the"
D18-1111,N13-1103,0,0.0174336,"ind this intuition to be true not only for HS but also for Django. Examining the generated output for the Django dataset in Table 3, we can see that in the first example, our retrieval model can successfully generate the correct code when YN17 fails. This difference suggests that our retrieval model benefits from the action subtrees from the retrieved sentences. In the second example, although our generated code does not perfectly match the reference code, it has a higher BLEU score compared 6 Related Work Several works on code generation focus on domain specific languages (Raza et al., 2015; Kushman and Barzilay, 2013). For general purpose code generation, some data-driven work has been 4 More example of HS code is provided in the supplementary material. 928 input on early version writing, and anonymous reviewers for useful feedback. This material is based upon work supported by the National Science Foundation under Grant No. 1815287. done for predicting input parsers (Lei et al., 2013) or a set of relevant methods (Raghothaman et al., 2016). Some attempts using neural networks have used sequence-to-sequence models (Ling et al., 2016) or tree-based architectures (Dong and Lapata, 2016; Alvarez-Melis and Jaa"
D18-1111,P13-1127,0,0.0255443,"e, although our generated code does not perfectly match the reference code, it has a higher BLEU score compared 6 Related Work Several works on code generation focus on domain specific languages (Raza et al., 2015; Kushman and Barzilay, 2013). For general purpose code generation, some data-driven work has been 4 More example of HS code is provided in the supplementary material. 928 input on early version writing, and anonymous reviewers for useful feedback. This material is based upon work supported by the National Science Foundation under Grant No. 1815287. done for predicting input parsers (Lei et al., 2013) or a set of relevant methods (Raghothaman et al., 2016). Some attempts using neural networks have used sequence-to-sequence models (Ling et al., 2016) or tree-based architectures (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017). Ling et al. (2016); Jia and Liang (2016); Locascio et al. (2016) treat semantic parsing as a sequence generation task by linearizing trees. The closest work to ours are Yin and Neubig (2017) and Rabinovich et al. (2017) which represent code as an AST. Another close work is Dong and Lapata (2018), which uses a two-staged structure-aware neural architecture. Th"
D18-1111,L18-1146,0,0.194537,"ich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code. However, representing code as a tree is not a trivial task, as the number of nodes in the tree often greatly exceeds the length of the NL description. As a result, tree-based approaches are often incapable of generating correct code for phrases in the corresponding NL description that have low frequency in the training data. In machine translation (MT) problems (Zhang et al., 2018; Gu et al., 2018; Amin Farajian et al., 2017; Li et al., 2018), hybrid methods combining retrieval of salient examples and neural models have proven successful in dealing with rare words. Following the intuition of these models, we hypothesize that our model can benefit from querying pairs of NL descriptions and AST structures from training data. In this paper, we propose R E C ODE, and adaptation of Zhang et al. (2018)’s retrieval-based approach neural MT method to the code generation problem by expanding it to apply to generation of tree structures. Our main contribution is to introduce the use of retrieval methods in neural code generation models. We"
D18-1111,P16-1057,0,0.124697,"Missing"
D18-1111,D16-1197,0,0.0427342,"Missing"
D18-1111,P17-1105,0,0.394369,"input sentences using a dynamicprogramming-based sentence similarity scoring method. Next, we extract n-grams of action sequences that build the associated abstract syntax tree. Finally, we increase the probability of actions that cause the retrieved n-gram action subtree to be in the predicted code. We show that our approach improves the performance on two code generation tasks by up to +2.6 BLEU.1 1 Introduction Natural language to code generation, a subtask of semantic parsing, is the problem of converting natural language (NL) descriptions to code (Ling et al., 2016; Yin and Neubig, 2017; Rabinovich et al., 2017). This task is challenging because it has a well-defined structured output and the input structure and output structure are in different forms. A number of neural network approaches have been proposed to solve this task. Sequential approaches (Ling et al., 2016; Jia and Liang, 2016; Locascio et al., 2016) convert the target code into a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct. 1 Code available at sweetpeach/ReCode 2 Syntactic Code Generation Given an NL description q, our purpose is to generate co"
D18-1111,P17-1041,1,0.417824,"s that are similar to input sentences using a dynamicprogramming-based sentence similarity scoring method. Next, we extract n-grams of action sequences that build the associated abstract syntax tree. Finally, we increase the probability of actions that cause the retrieved n-gram action subtree to be in the predicted code. We show that our approach improves the performance on two code generation tasks by up to +2.6 BLEU.1 1 Introduction Natural language to code generation, a subtask of semantic parsing, is the problem of converting natural language (NL) descriptions to code (Ling et al., 2016; Yin and Neubig, 2017; Rabinovich et al., 2017). This task is challenging because it has a well-defined structured output and the input structure and output structure are in different forms. A number of neural network approaches have been proposed to solve this task. Sequential approaches (Ling et al., 2016; Jia and Liang, 2016; Locascio et al., 2016) convert the target code into a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct. 1 Code available at sweetpeach/ReCode 2 Syntactic Code Generation Given an NL description q, our"
D18-1160,P14-2131,0,0.202435,"within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the specific properties of language captured by any particular embedding scheme can be difficult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram embeddings (Mikolov et al., 2013) with small context window size are found to capture the syntactic properties of language well (Bansal et al., 2014; Lin et al., 2015). However, if our goal is to separate syntactic categories, this embedding space is not ideal – POS categories correspond to overlap1292 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics zi ⇠ Syntax Model z3 z2 z1 &lt;latexit sha1_base64=&quot;ZNKuRe23lzCJMNesh6cRgweKXJI=&quot;&gt;AAACCHicZVDLSgMxFM3UV62vqks3g0VwUcpMEdRd0Y3Lio4W2qFk0kwbmkyG5I5Qh36BuNXvcCVu/Qs/wz8wM52FbS+Ee3LuueHkBDFnGhznxyqtrK6tb5Q3K1vbO7t71f2DBy0TRahHJJeqE2BNOYuoBww47cSKYh"
D18-1160,N10-1083,1,0.89622,"Missing"
D18-1160,P11-1087,0,0.15266,"nd latent embeddings learned by our approach with a Markov-structured prior. Each node represents a word and is colored according to the most likely gold POS tag from the Penn Treebank (best seen in color). Introduction Data annotation is a major bottleneck for the application of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word"
D18-1160,Q17-1010,0,0.0633438,"with random initialization. This is consistent with previous work that has noted the importance of careful initialization for DMV-based models such as the commonly used harmonic initializer (Klein and Manning, 2004). However, it is not straightforward to apply the harmonic initializer for DMV directly in our model without using some kind of pre-training since we do not observe gold POS. Impact of Observed Embeddings. We investigate the effect of the choice of pre-trained embedding on performance while using our approach. To this end, we additionally include results using fastText embeddings (Bojanowski et al., 2017) – which, in contrast with skip-gram embeddings, include character-level information. We set the context windows size to 1 and the dimension size to 100 as in the skip-gram training, while keeping other parameters set to their defaults. These results are summarized in Table 4 and Table 5. While fastText embeddings lead to reduced performance with our model, our approach still yields an improvement over the Gaussian baseline with the new observed embeddings space. 4.6 Qualitative Analysis of Embeddings We perform qualitative analysis to understand how the latent embeddings help induce syntactic"
D18-1160,D17-1171,0,0.279698,"we do the one-to-one mapping between gold POS tags and induced clusters and plot the normalized confusion matrix of noun subcategories in Figure 4. The Gaussian HMM fails to identify “NN” and “NNS” correctly for most cases, and it often recognizes “NNPS” as “NNP”. In contrast, our approach corrects these errors well. 1297 4.4 Unsupervised Dependency Parsing without gold POS tags 6 10 w/o gold POS tags DMV (Klein and Manning, 2004) 49.6 E-DMV (Headden III et al., 2009) 52.1 UR-A E-DMV (Tu and Honavar, 2012) 58.9 CS∗ (Spitkovsky et al., 2013) 72.0∗ Neural E-DMV (Jiang et al., 2016) 55.3 CRFAE (Cai et al., 2017) 37.2 Gaussian DMV 55.4 (1.3) Ours (4 layers) 58.4 (1.9) Ours (8 layers) 60.2 (1.3) Ours (16 layers) 54.1 (8.5) 35.8 38.2 46.1 64.4∗ 42.7 29.5 43.1 (1.2) 46.2 (2.3) 47.9 (1.2) 43.9 (5.7) w/ gold POS tags (for reference only) DMV (Klein and Manning, 2004) 55.1 UR-A E-DMV (Tu and Honavar, 2012) 71.4 MaxEnc (Le and Zuidema, 2015) 73.2 Neural E-DMV (Jiang et al., 2016) 72.5 CRFAE (Cai et al., 2017) 71.7 L-NDMV (Big training data) (Han et al., 2017) 77.2 39.7 57.0 65.8 57.6 55.7 63.2 System For the task of unsupervised dependency parse induction, we employ the Dependency Model with Valence (DMV) (K"
D18-1160,D10-1056,0,0.130718,"Missing"
D18-1160,D11-1005,0,0.154037,"lly because automatically parsing from words is difficult even when using unsupervised syntactic categories (Spitkovsky et al., 2011a). However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) (Spitkovsky et al., 2011a,b, 2012, 2013), acoustic cues (Pate and Goldwater, 2013), additional training data (Pate and Johnson, 2016), or annotated data from related languages (Cohen et al., 2011). Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies without using gold POS tags or other extra linguistic information. Setup. Like previous work we use sections 0221 of WSJ corpus as training data and evaluate on section 23, we remove punctuations and train the models on sentences of length 6 10, “headpercolation” rules (Collins, 1999) are applied to obtain gold dependencies for evaluation. We train basic DMV, extended DMV (E-DMV) (Headden III et al., 2009) and Gaussian DMV (which treats POS tag as unknown latent variables and gener"
D18-1160,N09-1009,0,0.0601511,"ank (best seen in color). Introduction Data annotation is a major bottleneck for the application of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated cat"
D18-1160,N15-1140,0,0.0473601,"Missing"
D18-1160,D17-1176,0,0.575784,"erence and marginal likelihood computation procedures so long as inference is tractable in the underlying syntax model. In §3.1 we show that this derivation corresponds to an alternate view of our approach whereby we jointly learn a mapping of observed word embeddings to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to prevent information loss. Recent work has sought to take advantage of word embeddings in unsupervised generative models with alternate approaches (Lin et al., 2015; Tran et al., 2016; Jiang et al., 2016; Han et al., 2017). Lin et al. (2015) build an HMM with Gaussian emissions on observed word embeddings, but they do not attempt to learn new embeddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, the DMV. We evaluate on two tasks: part-o"
D18-1160,P17-1044,0,0.0229364,"ortant. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the specific properties of language captured by any particular embedding scheme can be difficult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram embeddings (Mi"
D18-1160,N09-1012,0,0.246964,"Missing"
D18-1160,D16-1073,0,0.602307,"tractable exact inference and marginal likelihood computation procedures so long as inference is tractable in the underlying syntax model. In §3.1 we show that this derivation corresponds to an alternate view of our approach whereby we jointly learn a mapping of observed word embeddings to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to prevent information loss. Recent work has sought to take advantage of word embeddings in unsupervised generative models with alternate approaches (Lin et al., 2015; Tran et al., 2016; Jiang et al., 2016; Han et al., 2017). Lin et al. (2015) build an HMM with Gaussian emissions on observed word embeddings, but they do not attempt to learn new embeddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, the DMV. We evaluate o"
D18-1160,D07-1031,0,0.052933,"and initialize it with the empirical variance of the word vectors. Following Lin et al. (2015), the covariance matrix is fixed during training. The multinomial probabilities are initialized as θkv ∝ exp(ukv ), where ukv ∼ U [0, 1]. For our approach, we initialize the syntax model and Gaussian parameters with the pre-trained Gaussian HMM. The weights of layers in the rectified network are initialized from a uniform distribution p with mean zero and a standard deviation of 1/nin , where nin is the input dimension.4 We evaluate the performance of POS tagging with both Many-to-One (M-1) accuracy (Johnson, 2007) and V-Measure (VM) (Rosenberg and Hirschberg, 2007). Given a model we found that the tagging performance is well-correlated with the training data likelihood, thus we use training data likelihood as a unsupervised criterion to select the trained model over 10 random restarts after training 50 epochs. We repeat this process 5 times and report the mean and standard deviation of performance. 0.01 0.00 0.32 0.30 0.37 Others NNPS 4.3 System 0.01 0.20 0.01 0.47 0.31 0.02 0.00 0.00 0.00 0.98 NN NNS NNP NNPS Others (b) Our approach Figure 4: Normalized Confusion matrix for POS tagging experiments, ro"
D18-1160,P04-1061,0,0.921002,"gs to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to prevent information loss. Recent work has sought to take advantage of word embeddings in unsupervised generative models with alternate approaches (Lin et al., 2015; Tran et al., 2016; Jiang et al., 2016; Han et al., 2017). Lin et al. (2015) build an HMM with Gaussian emissions on observed word embeddings, but they do not attempt to learn new embeddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, the DMV. We evaluate on two tasks: part-of-speech (POS) induction and unsupervised dependency parsing without gold POS tags. Experimental results on the Penn Treebank (Marcus et al., 1993) demonstrate that our approach improves the basic HMM and DMV by a large margin, leading to the state-of-the-art results on PO"
D18-1160,N15-1067,0,0.233499,"Missing"
D18-1160,P14-2050,0,0.0366216,"eighbors, based on skip-gram embeddings and our learned latent embeddings with Markov-structured syntax model. process dreams error 5 agenda plans payments timetable parents furriers smokers aides issuers folks (subj) (subj)aide owner (obj) resident attorney in the overlapping region are typically objects. This demonstrates that the latent embeddings are focusing on aspects of language that are specifically important for modeling dependency without ever having seen examples of dependency parses. Some previous work has deliberately created embeddings to capture different notions of similarity (Levy and Goldberg, 2014; Cotterell and Sch¨utze, 2015), while they use extra morphology or dependency annotations to guide the embedding learning, our approach provides a potential alternative to create new embeddings that are guided by structured syntax model, only using unlabeled text corpora. actress singer Figure 5: Visualization (t-SNE) of learned latent embeddings with DMV-structured syntax model. Each node represents a word and is colored according to the most likely gold POS tag in the Penn Treebank (best seen in color). For our Markov-structured model, we have displayed the embedding space in Figure 1(b), w"
D18-1160,N15-1144,0,0.122378,"models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the specific properties of language captured by any particular embedding scheme can be difficult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram embeddings (Mikolov et al., 2013) with small context window size are found to capture the syntactic properties of language well (Bansal et al., 2014; Lin et al., 2015). However, if our goal is to separate syntactic categories, this embedding space is not ideal – POS categories correspond to overlap1292 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics zi ⇠ Syntax Model z3 z2 z1 &lt;latexit sha1_base64=&quot;ZNKuRe23lzCJMNesh6cRgweKXJI=&quot;&gt;AAACCHicZVDLSgMxFM3UV62vqks3g0VwUcpMEdRd0Y3Lio4W2qFk0kwbmkyG5I5Qh36BuNXvcCVu/Qs/wz8wM52FbS+Ee3LuueHkBDFnGhznxyqtrK6tb5Q3K1vbO7t71f2DBy0TRahHJJeqE2BNOYuoBww47cSKYhFw+hiMr7P54xNVmsnoH"
D18-1160,J93-2004,0,0.0606684,"ddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, the DMV. We evaluate on two tasks: part-of-speech (POS) induction and unsupervised dependency parsing without gold POS tags. Experimental results on the Penn Treebank (Marcus et al., 1993) demonstrate that our approach improves the basic HMM and DMV by a large margin, leading to the state-of-the-art results on POS induction, and state-of-the-art results on unsupervised dependency parsing in the difficult training scenario where neither gold POS annotation nor punctuation-based constraints are available. 2 Model As an illustrative example, we first present a baseline model for Markov syntactic structure (POS induction) that treats a sequence of pre-trained word embeddings as observations. Then, we propose our novel approach, again using Markov structure, that introduces latent w"
D18-1160,D16-1031,0,0.0305231,"rast, the nouns Related Work Our approach is related to flow-based generative models, which are first described in NICE (Dinh et al., 2014) and have recently received more attention (Dinh et al., 2016; Jacobsen et al., 2018; Kingma and Dhariwal, 2018). This relevant work mostly adopts simple (e.g. Gaussian) and fixed priors and does not attempt to learn interpretable latent structures. Another related generative model class is variational auto-encoders (VAEs) (Kingma and Welling, 2013) that optimize a lower bound on the marginal data likelihood, and can be extended to learn latent structures (Miao and Blunsom, 2016; Yin et al., 2018). Against the flow-based models, VAEs remove the invertibility constraint but sacrifice the merits of exact inference and exact log likelihood computation, which potentially results in optimization challenges (Kingma et al., 2016). Our approach can also be viewed in connection with generative adversarial networks (GANs) (Goodfellow et al., 2014) that is a likelihood-free framework to learn implicit generative models. However, it is nontrivial for a gradient-based method like GANs to propagate gradients through discrete structures. 6 Conclusion In this work, we define a novel"
D18-1160,Q13-1006,0,0.0629485,"ot fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically parsing from words is difficult even when using unsupervised syntactic categories (Spitkovsky et al., 2011a). However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) (Spitkovsky et al., 2011a,b, 2012, 2013), acoustic cues (Pate and Goldwater, 2013), additional training data (Pate and Johnson, 2016), or annotated data from related languages (Cohen et al., 2011). Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies without using gold POS tags or other extra linguistic information. Setup. Like previous work we use sections 0221 of WSJ corpus as training data and evaluate on section 23, we remove punctuations and train the models on sentences of length 6 10, “headpercolation” rules (Collins, 1999) are applied to obtain gold dependencies for evaluation. We train basic DMV, extended D"
D18-1160,C16-1003,0,0.470026,"). Introduction Data annotation is a major bottleneck for the application of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the spe"
D18-1160,N18-1202,0,0.0130934,"related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the specific properties of language captured by any particular embedding scheme can be difficult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram embeddings (Mikolov et al., 2013) wi"
D18-1160,Q16-1018,0,0.179395,"rned by our approach with a Markov-structured prior. Each node represents a word and is colored according to the most likely gold POS tag from the Penn Treebank (best seen in color). Introduction Data annotation is a major bottleneck for the application of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massiv"
D18-1160,W16-5907,0,0.500835,"are able to derive tractable exact inference and marginal likelihood computation procedures so long as inference is tractable in the underlying syntax model. In §3.1 we show that this derivation corresponds to an alternate view of our approach whereby we jointly learn a mapping of observed word embeddings to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to prevent information loss. Recent work has sought to take advantage of word embeddings in unsupervised generative models with alternate approaches (Lin et al., 2015; Tran et al., 2016; Jiang et al., 2016; Han et al., 2017). Lin et al. (2015) build an HMM with Gaussian emissions on observed word embeddings, but they do not attempt to learn new embeddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, th"
D18-1160,D12-1121,0,0.536256,".0 points on VM. Confusion Matrix. We found that most tagging errors happen in noun subcategories. Therefore, we do the one-to-one mapping between gold POS tags and induced clusters and plot the normalized confusion matrix of noun subcategories in Figure 4. The Gaussian HMM fails to identify “NN” and “NNS” correctly for most cases, and it often recognizes “NNPS” as “NNP”. In contrast, our approach corrects these errors well. 1297 4.4 Unsupervised Dependency Parsing without gold POS tags 6 10 w/o gold POS tags DMV (Klein and Manning, 2004) 49.6 E-DMV (Headden III et al., 2009) 52.1 UR-A E-DMV (Tu and Honavar, 2012) 58.9 CS∗ (Spitkovsky et al., 2013) 72.0∗ Neural E-DMV (Jiang et al., 2016) 55.3 CRFAE (Cai et al., 2017) 37.2 Gaussian DMV 55.4 (1.3) Ours (4 layers) 58.4 (1.9) Ours (8 layers) 60.2 (1.3) Ours (16 layers) 54.1 (8.5) 35.8 38.2 46.1 64.4∗ 42.7 29.5 43.1 (1.2) 46.2 (2.3) 47.9 (1.2) 43.9 (5.7) w/ gold POS tags (for reference only) DMV (Klein and Manning, 2004) 55.1 UR-A E-DMV (Tu and Honavar, 2012) 71.4 MaxEnc (Le and Zuidema, 2015) 73.2 Neural E-DMV (Jiang et al., 2016) 72.5 CRFAE (Cai et al., 2017) 71.7 L-NDMV (Big training data) (Han et al., 2017) 77.2 39.7 57.0 65.8 57.6 55.7 63.2 System For"
D18-1160,D12-1086,0,0.131675,"re our approach with basic HMM, Gaussian HMM, and several stateof-the-art systems, including sophisticated HMM variants and clustering techniques with handengineered features. The results are presented in Table 1. Through the introduced latent embeddings and additional neural projection, our approach improves over the Gaussian HMM by 5.4 points in M-1 and 5.6 points in VM. Neural HMM 4 This is the default parameter initialization in PyTorch. w/ hand-engineered features Feature HMM (Berg-Kirkpatrick et al., 2010) 75.5 Brown (+ proto) (Christodoulopoulos et al., 2010) 76.1 Cluster (word-based) (Yatbaz et al., 2012) 80.2 Cluster (token-based) (Yatbaz et al., 2014) 79.5 VM 53.8 69.8 54.2 66.1 71.7 68.5 (0.5) 73.0 (0.7) 74.1 (0.7) 70.5 (2.1) – 68.8 72.1 69.1 Table 1: Unsupervised POS tagging results on entire WSJ, 0.01 0.18 0.50 0.19 0.12 0.01 0.00 0.00 0.00 0.98 NN NNS NNP NNPS Others (a) Gaussian HMM NN 0.01 0.00 0.35 0.06 0.59 NNS NNS 0.13 0.48 0.02 0.00 0.37 0.78 0.00 0.00 0.02 0.21 0.03 0.89 0.00 0.02 0.06 NNP NN 0.33 0.00 0.02 0.00 0.65 NNP compared with other baselines and state-of-the-art systems. Standard deviation is given in parentheses when available. Others NNPS Setup. Following existing liter"
D18-1160,C14-1217,0,0.0312024,"Missing"
D18-1160,P18-1070,1,0.760408,"Work Our approach is related to flow-based generative models, which are first described in NICE (Dinh et al., 2014) and have recently received more attention (Dinh et al., 2016; Jacobsen et al., 2018; Kingma and Dhariwal, 2018). This relevant work mostly adopts simple (e.g. Gaussian) and fixed priors and does not attempt to learn interpretable latent structures. Another related generative model class is variational auto-encoders (VAEs) (Kingma and Welling, 2013) that optimize a lower bound on the marginal data likelihood, and can be extended to learn latent structures (Miao and Blunsom, 2016; Yin et al., 2018). Against the flow-based models, VAEs remove the invertibility constraint but sacrifice the merits of exact inference and exact log likelihood computation, which potentially results in optimization challenges (Kingma et al., 2016). Our approach can also be viewed in connection with generative adversarial networks (GANs) (Goodfellow et al., 2014) that is a likelihood-free framework to learn implicit generative models. However, it is nontrivial for a gradient-based method like GANs to propagate gradients through discrete structures. 6 Conclusion In this work, we define a novel generative approac"
D18-1160,D07-1043,0,0.13523,"l variance of the word vectors. Following Lin et al. (2015), the covariance matrix is fixed during training. The multinomial probabilities are initialized as θkv ∝ exp(ukv ), where ukv ∼ U [0, 1]. For our approach, we initialize the syntax model and Gaussian parameters with the pre-trained Gaussian HMM. The weights of layers in the rectified network are initialized from a uniform distribution p with mean zero and a standard deviation of 1/nin , where nin is the input dimension.4 We evaluate the performance of POS tagging with both Many-to-One (M-1) accuracy (Johnson, 2007) and V-Measure (VM) (Rosenberg and Hirschberg, 2007). Given a model we found that the tagging performance is well-correlated with the training data likelihood, thus we use training data likelihood as a unsupervised criterion to select the trained model over 10 random restarts after training 50 epochs. We repeat this process 5 times and report the mean and standard deviation of performance. 0.01 0.00 0.32 0.30 0.37 Others NNPS 4.3 System 0.01 0.20 0.01 0.47 0.31 0.02 0.00 0.00 0.00 0.98 NN NNS NNP NNPS Others (b) Our approach Figure 4: Normalized Confusion matrix for POS tagging experiments, row label represents the gold tag. (NHMM) (Tran et al."
D18-1160,D11-1118,0,0.485595,"l that defines a probability distribution over dependency parse trees and syntactic categories, generating tokens and dependencies in a head-outward fashion. While, traditionally, DMV is trained using gold POS tags as observed syntactic categories, in our approach, we treat each tag as a latent variable, as described in §2.3. Most existing approaches to this task are not fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically parsing from words is difficult even when using unsupervised syntactic categories (Spitkovsky et al., 2011a). However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) (Spitkovsky et al., 2011a,b, 2012, 2013), acoustic cues (Pate and Goldwater, 2013), additional training data (Pate and Johnson, 2016), or annotated data from related languages (Cohen et al., 2011). Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies"
D18-1160,W11-0303,0,0.304323,"l that defines a probability distribution over dependency parse trees and syntactic categories, generating tokens and dependencies in a head-outward fashion. While, traditionally, DMV is trained using gold POS tags as observed syntactic categories, in our approach, we treat each tag as a latent variable, as described in §2.3. Most existing approaches to this task are not fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically parsing from words is difficult even when using unsupervised syntactic categories (Spitkovsky et al., 2011a). However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) (Spitkovsky et al., 2011a,b, 2012, 2013), acoustic cues (Pate and Goldwater, 2013), additional training data (Pate and Johnson, 2016), or annotated data from related languages (Cohen et al., 2011). Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies"
D18-1160,W12-1903,0,0.0451891,"Missing"
D18-1160,D13-1204,0,0.323605,"Missing"
D18-1160,W10-2902,0,0.048632,"a linguistic information. Setup. Like previous work we use sections 0221 of WSJ corpus as training data and evaluate on section 23, we remove punctuations and train the models on sentences of length 6 10, “headpercolation” rules (Collins, 1999) are applied to obtain gold dependencies for evaluation. We train basic DMV, extended DMV (E-DMV) (Headden III et al., 2009) and Gaussian DMV (which treats POS tag as unknown latent variables and generates observed word embeddings directly conditioned on them following Gaussian distribution) as baselines. Basic DMV and E-DMV are trained with Viterbi EM (Spitkovsky et al., 2010) on unsupervised POS tags induced from our Markov-structured model described in §4.3. Multinomial parameters of the syntax model in both Gaussian DMV and our model are initialized with the pre-trained DMV baseline. Other all Table 2: Directed dependency accuracy on section 23 of WSJ, evaluating on sentences of length 6 10 and all lengths. Starred entries (∗) denote that the system benefits from additional punctuation-based constraints. Standard deviation is given in parentheses when available. parameters are initialized in the same way as in the POS tagging experiment. The directed dependency"
D18-1366,E17-1088,1,0.86445,"iment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diﬀerent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, whi"
D18-1366,E17-2067,0,0.445484,"haracter ngrams, denoted 1 ∑ by u?? = |?| ?∈? x? , where ? is the set of character ngrams and x? is the vector representation of ngram ?. Such representations capture morphological information in a brute-force but principled fashion—words that share the same morpheme are more likely to share the same character ngrams than words that do not. Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embeddings are trained on morphological representations (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015). Avraham and Goldberg (2017) explicitly model lemmas (stems or citation forms) and morphological properties (the sets of which are sometimes called “tags”) for training the word embeddings. Lemmas capture information about the lexical identity of a word and are closely correlated with the semantics of a word; tags capture information about the syntactic context of a word. See Figure 1 for an example. We take inspiration from the above work in adapting these subword units for cross-lingual transfer. Phonological units: Subword units other than tags might seem to be of no use in closely-related languages with diﬀerent scri"
D18-1366,D16-1153,1,0.878386,"., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diﬀerent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of limited quantity and uncert"
D18-1366,D16-1136,0,0.0268979,"tations for the downstream task. These two regimes are described below: CT-Joint: This model explicitly maps the word representations of the two languages into the same space by training simultaneously on both. This is achieved simply by combining the corpora of both the high-resource and the low-resource language and training jointly using the skip-gram objective, discussed above. The central intuition is as follows: once two related languages are placed in the same phonological and morphological space, they will share many subword units in common and this will make joint training proﬁtable. Duong et al. (2016) and Gouws et al. (2015) have previously shown the advantages of joint training and we observe this to be true in our case as well. CT-FineTune: This model implicitly maps the word representations of the two languages into the same space. The model attempts this by taking the learned continuous representations of the high resource subword units, referred to by x?? ??? , and uses them to initialize the model for the low resource language. The model is ﬁrst trained using all subword units on the high resource language and the learned representations are then used for initializing the subword uni"
D18-1366,P08-1088,0,0.016977,"et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diﬀerent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of lim"
D18-1366,W17-4110,0,0.0301722,"s. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciﬁc and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources. 7 Conclusion In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword informa"
D18-1366,D17-1302,0,0.026782,"ive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciﬁc and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources. 7 Conclusion In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword information from resource-rich languages, especially through phonological and morphological representations. CT-Joint and CTFineTune do not require morphological analyzers, but we have found that even a morphological analyzer built in 2-3 weeks can boost performance an"
D18-1366,Q15-1016,0,0.0420122,"t tokens, within a speciﬁed window of the focus word ?? and ?(?|?? ) is the probability of observing context word ? given focus word ?? . The skipgram was originally deﬁned using the softmax function: ?(?|?? ) = ??(?,?? ) ?(?? ,?) ∑? ?=1 ? (2) where ? is a scoring function mapping ? and ?? to ℝ. The summation in the denominator is over the entire vocabulary ? which makes this formulation computationally ineﬃcient as cost of gradient computation is proportional to ? which is quite large (∼ 106 ). Mikolov et al. (2013b) hence employ negative sampling to make this computation eﬃcient and robust (Levy et al., 2015) and give better representations for infrequent words4 , which is crucial for the low resource settings. Negative sampling represents the above objective function (Equation 1) using a binary logistic loss as shown below: ? 3. We produce continuous representations for each subword unit, giving researchers the ability to use them in their own tasks as they see ﬁt. The code 1 for training word embeddings and the embeddings 2 which produced the best results are publicly available. We also release morphological analyzers for Hindi and Bengali3 . 2 Skipgram Objective https://github.com/Aditi138/Embe"
D18-1366,D15-1176,0,0.0397207,"g approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of limited quantity and uncertain quality. In this paper, we take a diﬀerent task: focusing instead on the similarity of the surface forms, phonology, or morphology of the two transfer languages. Speciﬁcally, inspired by Ling et al. (2015), who demonstrate the eﬀectiveness of character-level modeling for knowledge sharing in multilingual scenarios, we propose two approaches to transfer word embeddings using diﬀerent types of linguistically-inspired subword-level information. Both approaches focus on mapping the low resource language embeddings closer to those of the high resource language and are executed using two diﬀerent training regimes. We explore the eﬀect of diﬀerent subword units— characters, lemmas, inﬂectional properties, and phonemes— as each one oﬀers a unique linguistic insight, discussed more in Section 3. Our pro"
D18-1366,W13-3512,0,0.534869,"haracter-level modeling by representing the focus word ?? as a set of its character ngrams, denoted 1 ∑ by u?? = |?| ?∈? x? , where ? is the set of character ngrams and x? is the vector representation of ngram ?. Such representations capture morphological information in a brute-force but principled fashion—words that share the same morpheme are more likely to share the same character ngrams than words that do not. Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embeddings are trained on morphological representations (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015). Avraham and Goldberg (2017) explicitly model lemmas (stems or citation forms) and morphological properties (the sets of which are sometimes called “tags”) for training the word embeddings. Lemmas capture information about the lexical identity of a word and are closely correlated with the semantics of a word; tags capture information about the syntactic context of a word. See Figure 1 for an example. We take inspiration from the above work in adapting these subword units for cross-lingual transfer. Phonological units: Subword units other"
D18-1366,P16-1101,0,0.272374,"r approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU. 1 Introduction Continuous word representations have demonstrated utility in state-of-the-art neural models for several NLP tasks, such as named entity recognition (NER; Ma and Hovy (2016)), machine reading (Tan et al., 2017), sentiment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high depen"
D18-1366,I13-1136,0,0.066857,"Missing"
D18-1366,D17-1269,0,0.120208,"Missing"
D18-1366,L18-1429,1,0.883211,"Missing"
D18-1366,W18-1818,1,0.81319,"l trained with charngrams+lemma+morph ngrams+lemma+morph and monolingual: char-ngram+lemma+morph). The diﬀerence is striking—in the monolingual condition, the NEs are widely dispersed, but in the bilingual condition, the NEs cluster together. This suggests that phonologically-mediated transfer through Turkish is resulting in embeddings in which NEs are close to one another, relative to monolingual Uyghur embeddings. 5.4 Machine Translation Task In addition to NER, we test the performance of our proposed approaches on the MT task to test generality of our conclusions. We use XNMT toolkit 3292 (Neubig et al., 2018) to translate sentences from the low-resource language to English. We run similar transfer and monolingual experiments as done for NER. Due to space limitations, we use select subword combinations for the experiments, details of which can be found in Appendix. BLEU is used as the evaluation metric. From Table 6, we observe that the combination of character-ngrams and lemma performs the best for Uyghur (+0.1) and the combination of character-ngrams, lemma and morph gives the best performance for Bengali (+1.7), over the word baseline, which demonstrates the importance of subword units for low-r"
D18-1366,D14-1162,0,0.084187,"that of NER. We hypothesize that this is because the MT models were trained on a training set that did not have translation pairs from the high resource language. As Qi et al. (2018) note, when training MT systems on a single language pair, it is less necessary for the embeddings to be coordinated across the languages. 6 Related Work Word Embedding Models: Most algorithms for learning embeddings take inspiration from language modeling (Bengio et al., 2003), motivated by distributional hypothesis (Harris, 1954), and employ a shallow neural network to map the words into a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiﬁcant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment a"
D18-1366,N18-1202,0,0.0322711,"ge. As Qi et al. (2018) note, when training MT systems on a single language pair, it is less necessary for the embeddings to be coordinated across the languages. 6 Related Work Word Embedding Models: Most algorithms for learning embeddings take inspiration from language modeling (Bengio et al., 2003), motivated by distributional hypothesis (Harris, 1954), and employ a shallow neural network to map the words into a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiﬁcant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word represe"
D18-1366,C16-1018,0,0.0442983,"Missing"
D18-1366,K16-1022,0,0.0265297,"t al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciﬁc and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources. 7 Conclusion In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword information from resource-rich languages, especially through phonological and morphological representations. CT-Joint and CTFineTune do"
D18-1366,D16-1157,0,0.0138983,"⟨⟩قارىيالمايدۇ /qarijalmajdu/ /qari-jal-ma-jdu/ qari+Verb+Pot+Neg+Pres+A3sg ‘s/he can’t care for’ Figure 1: Representations of a word in Uyghur morphologically rich languages such as Turkish, Uyghur, Hindi, and Bengali. Although, given a large enough training corpus, most or all morphological forms of a lexeme (of which there may be many) could theoretically learn to have similar vector representations, it will be vastly more data efﬁcient if we can take into account regularities of their form to model morphology explicitly. We explore the following methods for doing so: Orthographic units: Wieting et al. (2016) and Bojanowski et al. (2016) show the utility of character-level modeling by representing the focus word ?? as a set of its character ngrams, denoted 1 ∑ by u?? = |?| ?∈? x? , where ? is the set of character ngrams and x? is the vector representation of ngram ?. Such representations capture morphological information in a brute-force but principled fashion—words that share the same morpheme are more likely to share the same character ngrams than words that do not. Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embedd"
D18-1366,N16-1119,0,0.0259393,"a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiﬁcant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al."
D18-1366,D16-1163,0,0.0326908,"guage. The model is ﬁrst trained using all subword units on the high resource language and the learned representations are then used for initializing the subword units for the low resource language. To elucidate which pretrained subword helped the most on the low resource language, we use the same model for diﬀerent experiments, which is trained using all subword units—phoneme-ngrams, lemma and morphological properties. The linguistic intuition behind CTFineTune is similar to that behind CT-Joint. This idea of transferring parameters from high resource language has been previously explored by Zoph et al. (2016) for low resource neural machine translation which showed considerable improvement. 5 Evaluation In this section, we ﬁrst describe the model setup for training word embeddings followed by details on NER and MT experiments. 5.1 Implementation details We base our model on the C++ implementation of fasttext5 (Bojanowski et al., 2016) with modiﬁcations as described above. Data: We represent a word in the training corpus using the format presented by Avraham and Goldberg (2017). For instance, the Uyghur word in Figure 1 is represented as follows: phoneme ipa: qarijalmajdu, lemma l:qari, and morphol"
D18-1366,N18-2084,1,0.921313,"ource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU. 1 Introduction Continuous word representations have demonstrated utility in state-of-the-art neural models for several NLP tasks, such as named entity recognition (NER; Ma and Hovy (2016)), machine reading (Tan et al., 2017), sentiment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown"
D18-1366,N15-1140,0,\N,Missing
D18-1509,P17-1177,0,0.0216059,"implicitly discover syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017). However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. generating one target word at a time. One exception is Wu et al. (2017), which utilizes two RNNs for generating target dependency trees. Nevertheless, Wu et al. (2017) is specifically designed for dependency tree structures and is not trivially applicable to other varieties of trees such as phrase-structure trees, which have been used more widely in other w"
D18-1509,N16-1024,0,0.0400287,"structures are not friendly to recurrent neural networks (RNNs). In this paper, we propose TrDec, a method for incorporating tree structures in NMT. TrDec simultaneously generates a target-side tree topology and a translation, using the partially-generated tree to guide the translation process (§ 2). TrDec employs two RNNs: a rule RNN, which tracks the topology of the tree based on rules defined by a Context Free Grammar (CFG), and a word RNN, which tracks words at the leaves of the tree (§ 3). This model is similar to neural models of tree-structured data from syntactic and semantic parsing (Dyer et al., 2016; Alvarez-Melis and Jaakkola, 2017; Yin and Neubig, 2017), but with the addition of the word RNN, which is especially important for MT where fluency of transitions over the words is critical. TrDec can generate any tree structure that can be represented by a CFG. These structures include linguistically-motivated syntactic tree representations, e.g. constituent parse trees, as well as syntaxfree tree representations, e.g. balanced binary trees (§ 4). This flexibility of TrDec allows us to com4772 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4772–"
D18-1509,P16-1078,0,0.0226829,"2014; Bahdanau et al., 2015). While seq2seq models can implicitly discover syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017). However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. generating one target word at a time. One exception is Wu et al. (2017), which utilizes two RNNs for generating target dependency trees. Nevertheless, Wu et al. (2017) is specifically designed for dependency tree structures and is not trivially applicable to other varieties of trees such as phrase-structure"
D18-1509,P17-2012,0,0.122951,"syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017). However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. generating one target word at a time. One exception is Wu et al. (2017), which utilizes two RNNs for generating target dependency trees. Nevertheless, Wu et al. (2017) is specifically designed for dependency tree structures and is not trivially applicable to other varieties of trees such as phrase-structure trees, which have been used more widely in other works on syntax-based mac"
D18-1509,P06-1121,0,0.0603041,"corporating target-side syntax by up to 0.7 BLEU.1 1 Introduction Most NMT methods use sequence-to-sequence (seq2seq) models, taking in a sequence of source words and generating a sequence of target words (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). While seq2seq models can implicitly discover syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017). However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. generating one target word at a time. One exception"
D18-1509,P17-1041,1,0.73084,"rks (RNNs). In this paper, we propose TrDec, a method for incorporating tree structures in NMT. TrDec simultaneously generates a target-side tree topology and a translation, using the partially-generated tree to guide the translation process (§ 2). TrDec employs two RNNs: a rule RNN, which tracks the topology of the tree based on rules defined by a Context Free Grammar (CFG), and a word RNN, which tracks words at the leaves of the tree (§ 3). This model is similar to neural models of tree-structured data from syntactic and semantic parsing (Dyer et al., 2016; Alvarez-Melis and Jaakkola, 2017; Yin and Neubig, 2017), but with the addition of the word RNN, which is especially important for MT where fluency of transitions over the words is critical. TrDec can generate any tree structure that can be represented by a CFG. These structures include linguistically-motivated syntactic tree representations, e.g. constituent parse trees, as well as syntaxfree tree representations, e.g. balanced binary trees (§ 4). This flexibility of TrDec allows us to com4772 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4772–4777 c Brussels, Belgium, October 31 - November 4, 2018."
D18-1509,N04-1035,0,0.0798626,"p to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU.1 1 Introduction Most NMT methods use sequence-to-sequence (seq2seq) models, taking in a sequence of source words and generating a sequence of target words (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). While seq2seq models can implicitly discover syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017). However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. gene"
D18-1509,D13-1176,0,0.0376514,"rally generate the topology of an arbitrary tree structure on the target side, and (2) experiment with various target tree structures. Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU.1 1 Introduction Most NMT methods use sequence-to-sequence (seq2seq) models, taking in a sequence of source words and generating a sequence of target words (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). While seq2seq models can implicitly discover syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders"
D18-1509,P17-1064,0,0.0129085,"Sutskever et al., 2014; Bahdanau et al., 2015). While seq2seq models can implicitly discover syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017). However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. generating one target word at a time. One exception is Wu et al. (2017), which utilizes two RNNs for generating target dependency trees. Nevertheless, Wu et al. (2017) is specifically designed for dependency tree structures and is not trivially applicable to other varieties of trees"
D18-1509,D15-1166,1,0.717811,"Missing"
D18-1509,de-marneffe-etal-2006-generating,0,0.0133597,"Missing"
D18-1509,W07-0701,0,0.0399132,"and other methods for incorporating target-side syntax by up to 0.7 BLEU.1 1 Introduction Most NMT methods use sequence-to-sequence (seq2seq) models, taking in a sequence of source words and generating a sequence of target words (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). While seq2seq models can implicitly discover syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017). However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. generating one target word at"
D18-1509,W17-4707,0,0.114523,"model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017). However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. generating one target word at a time. One exception is Wu et al. (2017), which utilizes two RNNs for generating target dependency trees. Nevertheless, Wu et al. (2017) is specifically designed for dependency tree structures and is not trivially applicable to other varieties of trees such as phrase-structure trees, which have been used more widely in other works on syntax-based machine translation. One potential reason for the dearth of work on syntactic decoders i"
D18-1509,N15-3009,1,0.88949,"Missing"
D18-1509,P06-1055,0,0.0695464,"gure 3: Conversion of a dependency tree for TrDec. Left: original dependency tree. Right: after conversion. Softmax. At any step t, our softmax logits are word ], where W varies depending W · tanh [stree t , st on whether a rule or a subword unit is needed. 4 Tree Structures Unlike prior work on syntactic decoders designed for utilizing a specific type of syntactic information (Wu et al., 2017), TrDec is a flexible NMT model that can utilize any tree structure. Here we consider two categories of tree structures: Syntactic Trees are generated using a thirdparty parser, such as Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). Fig. 2 Top Left illustrates an example constituency parse tree. We also consider a variation of standard constituency parse trees where all of their nonterminal tags are replaced by a null tag, which is visualized in Fig. 2 Top Right. In addition to constituency parse trees, TrDec can also utilize dependency parse trees via a simple procedure that converts a dependency tree into a constituency tree. Specifically, this procedure creates a parent node with null tag for each word, and then attaches each word to the parent node of its head word while preserving the word"
D18-1509,N07-1051,0,0.0215891,"a dependency tree for TrDec. Left: original dependency tree. Right: after conversion. Softmax. At any step t, our softmax logits are word ], where W varies depending W · tanh [stree t , st on whether a rule or a subword unit is needed. 4 Tree Structures Unlike prior work on syntactic decoders designed for utilizing a specific type of syntactic information (Wu et al., 2017), TrDec is a flexible NMT model that can utilize any tree structure. Here we consider two categories of tree structures: Syntactic Trees are generated using a thirdparty parser, such as Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). Fig. 2 Top Left illustrates an example constituency parse tree. We also consider a variation of standard constituency parse trees where all of their nonterminal tags are replaced by a null tag, which is visualized in Fig. 2 Top Right. In addition to constituency parse trees, TrDec can also utilize dependency parse trees via a simple procedure that converts a dependency tree into a constituency tree. Specifically, this procedure creates a parent node with null tag for each word, and then attaches each word to the parent node of its head word while preserving the word order. An example of this"
D18-1509,P16-1162,0,0.387462,"small dataset. Our findings are surprising – TrDec performs well, but it performs the best with balanced binary trees constructed without any linguistic guidance. 2 Generation Process TrDec simultaneously generates the target sequence and its corresponding tree structure. We first discuss the high-level generation process using an example, before describing the prediction model (§ 3) and the types of trees used by TrDec (§ 4). Fig. 1 illustrates the generation process of the sentence “_The _cat _eat s _fi sh _.”, where the sentence is split into subword units, delimited by the underscore “_” (Sennrich et al., 2016). The example uses a syntactic parse tree as the intermediate tree representation, but the process of generating with other tree representations, e.g. syntax-free trees, follows the same procedure. Trees used in TrDec have two types of nodes: terminal nodes, i.e. the leaf nodes that represent subword units; and nonterminal nodes, i.e. the nonleaf nodes that represent a span of subwords. Additionally, we define a preterminal node to be a nonterminal node whose children are all terminal nodes. In Fig. 1 Left, the green squares represent preterminal nodes. TrDec generates a tree in a top-down, le"
D18-1509,D16-1159,0,0.0231803,"result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU.1 1 Introduction Most NMT methods use sequence-to-sequence (seq2seq) models, taking in a sequence of source words and generating a sequence of target words (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). While seq2seq models can implicitly discover syntactic properties of the source language (Shi et al., 2016), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) (Galley et al., 2004; Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens"
D18-1509,P17-1065,0,0.317298,"cent works have established that explicitly leveraging syntactic information can improve NMT quality, ei1 Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch. ther through syntactic encoders (Li et al., 2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al., 2017), or direct addition of syntactic tokens to the target sequence (Nadejde et al., 2017; Aharoni and Goldberg, 2017). However, these syntax-aware models only employ the standard decoding process of seq2seq models, i.e. generating one target word at a time. One exception is Wu et al. (2017), which utilizes two RNNs for generating target dependency trees. Nevertheless, Wu et al. (2017) is specifically designed for dependency tree structures and is not trivially applicable to other varieties of trees such as phrase-structure trees, which have been used more widely in other works on syntax-based machine translation. One potential reason for the dearth of work on syntactic decoders is that such parse tree structures are not friendly to recurrent neural networks (RNNs). In this paper, we propose TrDec, a method for incorporating tree structures in NMT. TrDec simultaneously generates"
D18-2002,P18-1034,0,0.0811327,"Missing"
D18-2002,D14-1135,0,0.310147,"Missing"
D18-2002,Q13-1005,0,0.0286814,"transducing natural language (NL) utterances into formal meaning representations (MRs). The target MRs can be defined according to a wide variety of formalisms. This include linguistically-motivated semantic representations that are designed to capture the meaning of any sentence such as λcalculus (Zettlemoyer and Collins, 2005) or the abstract meaning representations (Banarescu et al., 2013). Alternatively, for more task-driven approaches to semantic parsing, it is common for meaning representations to represent executable programs such as SQL queries (Zhong et al., 2017), robotic commands (Artzi and Zettlemoyer, 2013), smart phone instructions (Quirk et al., 2015), and even general-purpose programming languages like Python (Yin and Neubig, 2017; Rabinovich et al., 2017) and Java (Ling et al., 2016). • Generalization ability T RANX employs ASTs as a general-purpose intermediate meaning representation, and the task-dependent grammar is provided to the system as external knowledge to guide the parsing process, therefore decoupling the semantic parsing procedure with specificities of grammars. • Extensibility T RANX uses a simple transition system to parse NL utterances into tree1 Available at https://github.c"
D18-2002,W13-2322,0,0.0455941,"mantic parsing and code generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers.1 1 Introduction Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs). The target MRs can be defined according to a wide variety of formalisms. This include linguistically-motivated semantic representations that are designed to capture the meaning of any sentence such as λcalculus (Zettlemoyer and Collins, 2005) or the abstract meaning representations (Banarescu et al., 2013). Alternatively, for more task-driven approaches to semantic parsing, it is common for meaning representations to represent executable programs such as SQL queries (Zhong et al., 2017), robotic commands (Artzi and Zettlemoyer, 2013), smart phone instructions (Quirk et al., 2015), and even general-purpose programming languages like Python (Yin and Neubig, 2017; Rabinovich et al., 2017) and Java (Ling et al., 2016). • Generalization ability T RANX employs ASTs as a general-purpose intermediate meaning representation, and the task-dependent grammar is provided to the system as external knowledge"
D18-2002,P16-1127,0,0.180769,"e Technologies Institute Carnegie Mellon University {pcyin,gneubig}@cs.cmu.edu Abstract Because of these varying formalisms for MRs, the design of semantic parsers, particularly neural network-based ones has generally focused on a small subset of tasks — in order to ensure the syntactic well-formedness of generated MRs, a parser is usually specifically designed to reflect the domain-dependent grammar of MRs in the structure of the model (Zhong et al., 2017; Xu et al., 2017). To alleviate this issue, there have been recent efforts in neural semantic parsing with general-purpose grammar models (Xiao et al., 2016; Dong and Lapata, 2018). Yin and Neubig (2017) put forward a neural sequence-to-sequence model that generates tree-structured MRs using a series of tree-construction actions, guided by the task-specific context free grammar provided to the model a priori. Rabinovich et al. (2017) propose the abstract syntax networks (ASNs), where domain-specific MRs are represented by abstract syntax trees (ASTs, Fig. 2 Left) specified under the abstract syntax description language (ASDL) framework (Wang et al., 1997). An ASN employs a modular architecture, generating an AST using specifically designed neural"
D18-2002,P16-1004,0,0.0623254,"intain an embedding vector for each action. ˜ st is the attentional vector defined as in Luong et al. (2015) 3.1.1 Semantic Parsing We evaluate on G EO and ATIS datasets. G EO is a collection of 880 U.S. geographical questions (e.g., “Which states border Texas?”), and ATIS is a set of 5,410 inquiries of flight information (e.g., “Show me flights from Dallas to Baltimore”). The MRs in the two datasets are defined in λ-calculus logical forms (e.g., “lambda x (and (state x) (next to x texas))” and “lambda x (and (flight x dallas) (to x baltimore))”). We use the pre-processed datasets released by Dong and Lapata (2016). We use the ASDL grammar defined in Rabinovich et al. (2017), as listed in Fig. 3. ˜ st = tanh(Wc [ct : st ]). where ct is the context vector retrieved from input encodings {hi }ni=1 using attention. Parent Feeding pt is a vector that encodes the information of the parent frontier field nft on the derivation, which is a concatenation of two vectors: the embedding of the frontier field nft , and spt , the decoder’s state at which the constructor of nft is generated by the A PPLY C ONSTR action. Parent feeding reflects the topology of treestructured ASTs, and gives better performance on generat"
D18-2002,P18-1068,0,0.0682537,"itute Carnegie Mellon University {pcyin,gneubig}@cs.cmu.edu Abstract Because of these varying formalisms for MRs, the design of semantic parsers, particularly neural network-based ones has generally focused on a small subset of tasks — in order to ensure the syntactic well-formedness of generated MRs, a parser is usually specifically designed to reflect the domain-dependent grammar of MRs in the structure of the model (Zhong et al., 2017; Xu et al., 2017). To alleviate this issue, there have been recent efforts in neural semantic parsing with general-purpose grammar models (Xiao et al., 2016; Dong and Lapata, 2018). Yin and Neubig (2017) put forward a neural sequence-to-sequence model that generates tree-structured MRs using a series of tree-construction actions, guided by the task-specific context free grammar provided to the model a priori. Rabinovich et al. (2017) propose the abstract syntax networks (ASNs), where domain-specific MRs are represented by abstract syntax trees (ASTs, Fig. 2 Left) specified under the abstract syntax description language (ASDL) framework (Wang et al., 1997). An ASN employs a modular architecture, generating an AST using specifically designed neural networks for each const"
D18-2002,P17-1041,1,0.909458,"iversity {pcyin,gneubig}@cs.cmu.edu Abstract Because of these varying formalisms for MRs, the design of semantic parsers, particularly neural network-based ones has generally focused on a small subset of tasks — in order to ensure the syntactic well-formedness of generated MRs, a parser is usually specifically designed to reflect the domain-dependent grammar of MRs in the structure of the model (Zhong et al., 2017; Xu et al., 2017). To alleviate this issue, there have been recent efforts in neural semantic parsing with general-purpose grammar models (Xiao et al., 2016; Dong and Lapata, 2018). Yin and Neubig (2017) put forward a neural sequence-to-sequence model that generates tree-structured MRs using a series of tree-construction actions, guided by the task-specific context free grammar provided to the model a priori. Rabinovich et al. (2017) propose the abstract syntax networks (ASNs), where domain-specific MRs are represented by abstract syntax trees (ASTs, Fig. 2 Left) specified under the abstract syntax description language (ASDL) framework (Wang et al., 1997). An ASN employs a modular architecture, generating an AST using specifically designed neural networks for each construct in the ASDL gramma"
D18-2002,N18-2115,0,0.0846301,"Missing"
D18-2002,P18-1070,1,0.857934,", and even general-purpose programming languages like Python (Yin and Neubig, 2017; Rabinovich et al., 2017) and Java (Ling et al., 2016). • Generalization ability T RANX employs ASTs as a general-purpose intermediate meaning representation, and the task-dependent grammar is provided to the system as external knowledge to guide the parsing process, therefore decoupling the semantic parsing procedure with specificities of grammars. • Extensibility T RANX uses a simple transition system to parse NL utterances into tree1 Available at https://github.com/pcyin/tranX. An earilier version is used in Yin et al. (2018). 7 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 7–12 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics ASDL Grammar stmt ↦ Expr(expr value) expr ↦ Call(expr func, expr* args, keyword* keywords) |Attribute(expr value, | identifier attr) Name(identifier id) | Str(string s) pandas read top 100 lines in file.csv Abstract Syntax Tree (general-purpose intermediate MR) Meaning Representation (to domain-specific MR) Expr s2 ApplyConstr(Call) s3 ApplyConstr(Attr.) s4 GenToken(sorted)"
D18-2002,P16-1057,0,0.18233,"Missing"
D18-2002,N18-2093,0,0.118621,"Missing"
D18-2002,D15-1166,0,0.0571317,"n, ·) is defined similarly as Eq. (1). The copy probability of copying the i-th word in x is defined using a pointer network (Vinyals et al., 2015) p(xi |copy, a&lt;t , x) = softmax(h|i W˜ st ). cmp op = Equal |LessThan |GreaterThan 3 Figure 3: The λ-calculus ASDL grammar for G EO and ATIS, defined in Rabinovich et al. (2017) 3.1 Datasets To demonstrate the generalization and extensibility of T RANX, we deploy our parser on four semantic parsing and code generation tasks. bedding of the previous action. We maintain an embedding vector for each action. ˜ st is the attentional vector defined as in Luong et al. (2015) 3.1.1 Semantic Parsing We evaluate on G EO and ATIS datasets. G EO is a collection of 880 U.S. geographical questions (e.g., “Which states border Texas?”), and ATIS is a set of 5,410 inquiries of flight information (e.g., “Show me flights from Dallas to Baltimore”). The MRs in the two datasets are defined in λ-calculus logical forms (e.g., “lambda x (and (state x) (next to x texas))” and “lambda x (and (flight x dallas) (to x baltimore))”). We use the pre-processed datasets released by Dong and Lapata (2016). We use the ASDL grammar defined in Rabinovich et al. (2017), as listed in Fig. 3. ˜"
D18-2002,D07-1071,0,0.364266,"Missing"
D18-2002,N15-1162,0,0.196009,"Missing"
D18-2002,P15-1085,0,0.0398459,"l meaning representations (MRs). The target MRs can be defined according to a wide variety of formalisms. This include linguistically-motivated semantic representations that are designed to capture the meaning of any sentence such as λcalculus (Zettlemoyer and Collins, 2005) or the abstract meaning representations (Banarescu et al., 2013). Alternatively, for more task-driven approaches to semantic parsing, it is common for meaning representations to represent executable programs such as SQL queries (Zhong et al., 2017), robotic commands (Artzi and Zettlemoyer, 2013), smart phone instructions (Quirk et al., 2015), and even general-purpose programming languages like Python (Yin and Neubig, 2017; Rabinovich et al., 2017) and Java (Ling et al., 2016). • Generalization ability T RANX employs ASTs as a general-purpose intermediate meaning representation, and the task-dependent grammar is provided to the system as external knowledge to guide the parsing process, therefore decoupling the semantic parsing procedure with specificities of grammars. • Extensibility T RANX uses a simple transition system to parse NL utterances into tree1 Available at https://github.com/pcyin/tranX. An earilier version is used in"
D19-1091,K17-2001,0,0.327688,"Missing"
D19-1091,P17-1183,0,0.0482938,"results in the future. ν α π ρ ο σ α ρ τ ή σ ο υ μ ε ax at ax at Figure 3: Attention visualization examples. The inflected form is generated from top to bottom. quently, the lemma attention properly copies the stem, and then the tag attention attends first over PFV and then over PL and 3 in order to construct the correct suffix for perfective and 3rd person plural. 5 Related Work 6 The inflection task in high-resource settings has been extensively studied through the SIGMORPHON shared tasks. Notably, the best models explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure"
D19-1091,N16-1077,1,0.862619,"on towards even better results in the future. ν α π ρ ο σ α ρ τ ή σ ο υ μ ε ax at ax at Figure 3: Attention visualization examples. The inflected form is generated from top to bottom. quently, the lemma attention properly copies the stem, and then the tag attention attends first over PFV and then over PL and 3 in order to construct the correct suffix for perfective and 3rd person plural. 5 Related Work 6 The inflection task in high-resource settings has been extensively studied through the SIGMORPHON shared tasks. Notably, the best models explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingu"
D19-1091,P17-1182,0,0.0483132,"s explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure language similarity using lexical overlap. We attest that this data-based measure is less informative and more suspect to variation, so we instead use the genetic typological information to quantify correlations between performance improvements and language distance. Our novel two-step process decoder architecture bares similarities with multi-source models (Anastasopoulos and Chiang, 2018; Zoph and Knight, 2016) which provide two contexts from two encoded sources to the decoder. A similar disentangled enc"
D19-1091,K17-2002,0,0.0752231,"struct the correct suffix for perfective and 3rd person plural. 5 Related Work 6 The inflection task in high-resource settings has been extensively studied through the SIGMORPHON shared tasks. Notably, the best models explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure language similarity using lexical overlap. We attest that this data-based measure is less informative and more suspect to variation, so we instead use the genetic typological information to quantify correlations between performance improvements and language distance. Our novel two-step process decod"
D19-1091,L18-1293,0,0.0523411,"Missing"
D19-1091,Q18-1039,0,0.0271142,"ignments. Briefly, this means that if the i-th source character/word is aligned to the j-th target one (i ← j), then alignments i+1← j+1 or i← j+1 are also quite likely. In a neural architecture this can be approximated by providing the attention weight vector from the previous timestep as input to the function that computes the attention weights. We refer the reader to Cohn et al. (2016) for exact details. Adversarial Language Discriminator When training multilingual systems, encouraging the encoder to learn language-invariant representations can often lead to improvements (Xie et al., 2017; Chen et al., 2018), as it forces the model to truly work in a multilingual setting. We achieve that by introducing a language discriminator (Ganin et al., 2016). This additional component receives the last output of the (bi-directional) intermediate lemma representations hNx and outputs a prediction yl of the source language such that yl =softmax(MLP(hNx )). The discriminator is trained to predict the language by minimizing a standard cross-entropy 3 This heuristic number is largely arbitrary and could potentially be tuned to different values for each language. 986 the length of the region, though allowing for"
D19-1091,E17-2002,0,0.01635,"end of training helps a little more in terms of accuracy, but most importantly it speeds up training. Table 2 also reports the development set accuracy when using hallucinated data. Although the large improvements are also proportionally reflected in the test set, these numbers are not directly comparable to the rest, as the development set data were used in the hallucination process. 4 Analysis We analyze the results over various groupings of languages to elucidate the properties of our models over the 100 quite diverse language pairs. We use typological information from the URIEL database (Littell et al., 2017) in this analysis. Single Language Transfer We first focus on the test languages for which a single transfer language 6 Table 2 includes unpublished results kindly shared by Wu and Cotterell, the authors of the baseline. 989 L2 L1+L2 +Ll +H +Ll + H H turkish persian bashkir uzbek all azeri 81 55 57 47 84 77 63 59 55 71 80 74 66 74 83 81 69 67 70 87 66.7±0.9 urdu sanskrit hindi greek all bengali 42 44 49 42 49 32 38 52 46 50 66 66 67 65 64 67 65 65 67 62 63.7±4.0 turkish bashkir uzbek all crimean tatar 87 59 60 82 80 60 60 81 85 70 72 88 89 69 67 80 71.3±1.1 finnish hungarian estonian all ingri"
D19-1091,W19-6012,0,0.0948557,"Missing"
D19-1091,K17-2010,0,0.393635,"ion characters are sampled uniformly for the alphabet, rather than attempting to sample from a more informed distribution, which has potential for further improvements. Overall, we hallucinated 10,000 examples for each lowresource language, creating an additional hallucinated dataset H. A visualized example of our hallucination process is outlined in Figure 2. Out of the three regions with matching aligned characters (thick lines), we identify two with length equal to three or more. In the hallucinated example (bottom of the figure), we sample random characters for the inside of such regions. Silfverberg et al. (2017) have proposed a data hallucination method conceptually quite similar to ours, which treats the single longest common continuous substring between lemma and form as the stem. Their approach would be effectively similar to ours for languages with affixal steminvariant morphology, but it would likely fail in more complicated morphological phenomena like apophony, stem alternation (conceptually similar to infix morphology) or in the root-and-pattern morphology of semitic languages. Consider the apophony example from the past participle form gschwommen of the lemma schwimmen in Swiss German: our a"
D19-1091,P19-1148,0,0.572689,"k). The lemma and inflected forms are aligned at the character level. The inside of stem-considered parts (highlighted) are substituted with random characters, creating hallucinated triples (bottom). Additional Structural Biases for Attention Incorporating structural biases in the model’s architecture or in the training objective can lead to improvements in performance, especially for tasks where the attention mechanism is expected to behave similarly to an alignment model, like MT. This idea has been successfully applied to the inflection task and is at the core of the state-of-theart model (Wu and Cotterell, 2019). One bias we deem important is coverage of all input characters and tags from the attentions. Intuitively, this entails encouraging the model to “look at” the whole input. We take the approach of Cohn et al. (2016) and add two regularizers over the final attention matrices, encouraging them to also sum to one column-wise: −λ k Σ j atjm − I k2 stem Original triple lemma loss Ll similar to Lample et al. (2018). However, in order to encourage the encoder to learn language-invariant representations, we reverse the gradients flowing from that component into the encoder during back-propagation. 2.2"
D19-1091,K17-2005,1,0.859182,"r perfective and 3rd person plural. 5 Related Work 6 The inflection task in high-resource settings has been extensively studied through the SIGMORPHON shared tasks. Notably, the best models explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure language similarity using lexical overlap. We attest that this data-based measure is less informative and more suspect to variation, so we instead use the genetic typological information to quantify correlations between performance improvements and language distance. Our novel two-step process decoder architecture bares simil"
D19-1091,N16-1004,0,0.0422123,"milar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure language similarity using lexical overlap. We attest that this data-based measure is less informative and more suspect to variation, so we instead use the genetic typological information to quantify correlations between performance improvements and language distance. Our novel two-step process decoder architecture bares similarities with multi-source models (Anastasopoulos and Chiang, 2018; Zoph and Knight, 2016) which provide two contexts from two encoded sources to the decoder. A similar disentangled encoding was also used by Ács (2018) for their SIGMORPHON 2018 submission. We in fact experimented with this architecture but preConclusion With this work we advance the state-of-the-art for morphological inflection on low-resource languages by 15 points, through a novel architecture, data hallucination, and a variety of training techniques. Our two-step attention decoder follows an intuitive order, also enhancing interpretability. We also suggest that complicated methods for copying and forcing monoton"
D19-1091,D15-1166,0,\N,Missing
D19-1091,W16-2007,0,\N,Missing
D19-1091,D18-1103,1,\N,Missing
D19-1091,K18-3016,0,\N,Missing
D19-1091,W16-2002,0,\N,Missing
D19-1143,D18-1399,0,0.0389209,"Missing"
D19-1143,J82-2005,0,0.729886,"Missing"
D19-1143,E09-1011,0,0.0350952,"ozaki et al., 2010b; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007), and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012), Arabic (Badr et al., 2009), or Japanese (Isozaki et al., 2010b). In experiments we use Isozaki et al. (2010b)’s method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004), (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers. Word-by-word Translation To generate data for training MT models, we next perform wordby-word translation of ts into pseudo-source sentence"
D19-1143,D16-1025,0,0.0279016,"ference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1388–1394, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. However, similar pre-ordering methods have not proven useful in NMT (Du and Way, 2017), largely because high-resource scenarios NMT is much more effective at learning reordering than previous SMT methods were (Bentivogli et al., 2016). However, in low-resource scenarios it is less realistic to expect that NMT could learn this reordering from scratch on its own. Here we ask “how can we efficiently leverage the monolingual target data to improve the performance of the NMT system in low-resource, syntactically divergent language pairs?” We tackle this problem via a simple two-step data augmentation method: (1) we first reorder monolingual target sentences to create source-ordered target sentences as shown in Fig. 1, (2) we then replace the words in the reordered sentences with source words using a bilingual dictionary, and ad"
D19-1143,J16-2001,0,0.138444,"ce and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering. This paper proposes a method to create pseudoparallel sentences for NMT for language pairs with divergent syntactic structures. Prior to NMT, word reordering was a major challenge for statistical machine translation (SMT), and many techniques have emerged over the years to address this challenge (Xia and McCord, 2004; Bisazza and Federico, 2016). Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005); Isozaki et al. (2010b); Fig. 1). Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzmán et al., 2019). Hence we focus on semi-supervised methods in this paper. 1388 Proceedings of the 2019 Conference on Empirical Methods in Natural Language P"
D19-1143,P16-1185,0,0.0169685,"arly in English, are often much easier to find, making semisupervised approaches that can use monolingual data a desirable solution to this problem.2 1 I bought a new car . https://github.com/violet-zct/pytorch-.reorder-nmt 2 Unsupervised MT (Artetxe et al., 2018a) has achieved success on simulated low-resource scenarios with related lanSemi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods such as backtranslation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016) or adding an auxiliary autoencoding task on monolingual data (Cheng et al., 2016; He et al., 2016; Currey et al., 2017). However, both methods have problems with lowresource and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering. This paper proposes a method to create pseudoparallel sentences for NMT for language pairs with divergent syntactic structures. Prior to NMT, word reordering was a major challenge for statistical machine translation (SMT), and ma"
D19-1143,P05-1066,0,0.28743,"Missing"
D19-1143,W17-4715,0,0.161331,"r to find, making semisupervised approaches that can use monolingual data a desirable solution to this problem.2 1 I bought a new car . https://github.com/violet-zct/pytorch-.reorder-nmt 2 Unsupervised MT (Artetxe et al., 2018a) has achieved success on simulated low-resource scenarios with related lanSemi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods such as backtranslation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016) or adding an auxiliary autoencoding task on monolingual data (Cheng et al., 2016; He et al., 2016; Currey et al., 2017). However, both methods have problems with lowresource and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering. This paper proposes a method to create pseudoparallel sentences for NMT for language pairs with divergent syntactic structures. Prior to NMT, word reordering was a major challenge for statistical machine translation (SMT), and many techniques have emerged over the yea"
D19-1143,Q19-1007,0,0.025787,"ll over the world collected from ReliefWeb.5 To sub-select a relevant subset of this corpus, we use the cross-entropy filtering (Moore and Lewis, 2010) to select 400k that are most like the in-domain English data. For parallel data, like many low-resource languages, we only have access to data from the Bible6 and Wikipedia language links (the total number of parallel Uyghur-English Wikipedia titles is 3,088), but no other in-domain parallel data. We run GIZA++ on this parallel data to obtain an alignment dictionary. We learn the bilingual word embeddings via the supervised Geometric approach (Jawanpuria et al., 2019) on FastText (Grave et al., 2018) pre-trained Uyghur and English monolingual embeddings. 1390 5 6 https://reliefweb.int https://bible.is source reference supervised ours source reference supervised ours しかし ， 回転 速度 が 大き すぎ る と ， 逆向き の 変形 が 生じ る the too high rotation speed produces the reverse deformation however , the deformation of &lt;unk&gt; and the deformation of &lt;unk&gt; is caused by the dc rate however , the deformation of &lt;unk&gt; is generated when the rotation rate is large . à 8000  ñ ®Â  Kñê k éK. Q®J . K QJ.K , áJ . ÂKQ®J . K QJ.K ÈA 3 . ù®JËP 3  AJKñ JJK 29 : 3 ùJ¯A ¯ ,  - 12 Á"
D19-1143,W17-4123,0,0.296484,"e pairs (Neubig and Hu, 2018; Guzmán et al., 2019). Hence we focus on semi-supervised methods in this paper. 1388 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1388–1394, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. However, similar pre-ordering methods have not proven useful in NMT (Du and Way, 2017), largely because high-resource scenarios NMT is much more effective at learning reordering than previous SMT methods were (Bentivogli et al., 2016). However, in low-resource scenarios it is less realistic to expect that NMT could learn this reordering from scratch on its own. Here we ask “how can we efficiently leverage the monolingual target data to improve the performance of the NMT system in low-resource, syntactically divergent language pairs?” We tackle this problem via a simple two-step data augmentation method: (1) we first reorder monolingual target sentences to create source-ordered"
D19-1143,W17-3204,0,0.0314519,"ios find significant improvements over other semi-supervised alternatives1 1 English: Figure 1: An English sentence re-ordered into Japanese order using the rule-based method of Isozaki et al. (2010b), and its reference Japanese translation. Introduction While neural machine translation (NMT; Bahdanau et al. (2015); Vaswani et al. (2017)) now represents the state of the art in the majority of large-scale MT benchmarks (Bojar et al., 2017), it is highly dependent on the availability of copious parallel resources; NMT under-performs previous phrase-based methods when the training data is small (Koehn and Knowles, 2017). Unfortunately, million-sentence parallel corpora are often unavailable for many language pairs. Conversely, monolingual sentences, particularly in English, are often much easier to find, making semisupervised approaches that can use monolingual data a desirable solution to this problem.2 1 I bought a new car . https://github.com/violet-zct/pytorch-.reorder-nmt 2 Unsupervised MT (Artetxe et al., 2018a) has achieved success on simulated low-resource scenarios with related lanSemi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods su"
D19-1143,P10-4002,0,0.0447087,"our model and the supervised counterpart. Results and Comparison Baselines In Tab. 1, we compare our models with baselines including regular supervised training (sup) and back-translation (Sennrich et al., 2016) (back).7 To demonstrate the effectiveness of the reordering, we also compare our method against a copy-based data-augmentation method (No-reorder) where the original English sentences t ∈ Q rather than the reordered ones ts are translated via the bilingual lexicon.8 For each of the above settings, we also experimented with a phrased-based statistical machine translation (SMT) system (Dyer et al., 2010). In Tab. 1, we only show the results with supervised data and back-translation for SMT, since we observed that the data augmentation method performs poorly with SMT (complete results are presented in the Appendix). Main Results In Tab. 1, we observe consistent improvements on both ja-en and ug-en translation tasks against other baseline methods. First, comparing our results with the NMT models trained using the same amount of parallel data, our word reordering-based semi-supervised models consistently outperform standard NMT models by a large margin. In the case that we have no access to in-d"
D19-1143,P10-2041,0,0.0294996,"fication. Further details regarding the experimental settings are in the supplementary material. 4 2 0 &lt;10 [10,20) [20,30) [30,40) [40,50) [50,60) &gt;=60 Figure 2: Comparison of BLEU score w.r.t different sentence lengths. set. The LORELEI language pack also contains the bilingual lexicons between Uyghur and English, and thousands of in-domain English sentences. We also use a large monolingual English corpus containing sentences related to various incidents occurring all over the world collected from ReliefWeb.5 To sub-select a relevant subset of this corpus, we use the cross-entropy filtering (Moore and Lewis, 2010) to select 400k that are most like the in-domain English data. For parallel data, like many low-resource languages, we only have access to data from the Bible6 and Wikipedia language links (the total number of parallel Uyghur-English Wikipedia titles is 3,088), but no other in-domain parallel data. We run GIZA++ on this parallel data to obtain an alignment dictionary. We learn the bilingual word embeddings via the supervised Geometric approach (Jawanpuria et al., 2019) on FastText (Grave et al., 2018) pre-trained Uyghur and English monolingual embeddings. 1390 5 6 https://reliefweb.int https:/"
D19-1143,L18-1550,0,0.140993,"To generate data for training MT models, we next perform wordby-word translation of ts into pseudo-source sentence sˆ using a bilingual dictionary (Xie et al., 2018).3 There are many ways we can obtain this dictionary: even for many low-resource languages with a paucity of bilingual text, we can obtain manually-curated lexicons with reasonable coverage, or run unsupervised word alignment on whatever parallel data we have available. In addition, we can induce word translations for more words in target language using methods for bilingual lexicon induction over pre-trained word embeddings (e.g. Grave et al. (2018)). 3 Experiments We evaluate our method on two language pairs: Japanese-to-English (ja-en) and Uyghur-toEnglish (ug-en). Japanese and Uyghur are phylogenetically distant languages, but they share similar SOV syntactic structure, which is greatly divergent from English SVO structure. 3.1 Experimental Setup For both language pairs, we use an attentionbased encoder-decoder NMT model with a onelayer bidirectional LSTM as the encoder and onelayer uni-directional LSTM as the decoder.4 Embeddings and LSTM states were set to 300 and 3 We also performed extensive preliminary experiments that learned bi"
D19-1143,D19-1632,0,0.0515842,"atistical machine translation (SMT), and many techniques have emerged over the years to address this challenge (Xia and McCord, 2004; Bisazza and Federico, 2016). Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005); Isozaki et al. (2010b); Fig. 1). Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzmán et al., 2019). Hence we focus on semi-supervised methods in this paper. 1388 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1388–1394, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. However, similar pre-ordering methods have not proven useful in NMT (Du and Way, 2017), largely because high-resource s"
D19-1143,2007.mtsummit-papers.29,0,0.0641363,", t)}, where sˆ is a pseudo-parallel sentence automatically created in two steps of (1) word reordering, and (2) word-byword translation. Word Reordering The first step reorders monolingual target sentences t ∈ Q into the source order ts . Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT (Bisazza and Federico, 2016). Reordering can be done either using rules based on linguistic knowledge (Isozaki et al., 2010b; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007), and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012), Arabic (Badr et al., 2009), or Japanese (Isozaki et al., 2010b). In experiments we use Isozaki et al. (2010b)’s method of reor"
D19-1143,W13-2233,0,0.0698103,"n-sentence parallel corpora are often unavailable for many language pairs. Conversely, monolingual sentences, particularly in English, are often much easier to find, making semisupervised approaches that can use monolingual data a desirable solution to this problem.2 1 I bought a new car . https://github.com/violet-zct/pytorch-.reorder-nmt 2 Unsupervised MT (Artetxe et al., 2018a) has achieved success on simulated low-resource scenarios with related lanSemi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods such as backtranslation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016) or adding an auxiliary autoencoding task on monolingual data (Cheng et al., 2016; He et al., 2016; Currey et al., 2017). However, both methods have problems with lowresource and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering. This paper proposes a method to create pseudoparallel sentences for NMT for language pairs with divergent syntactic structur"
D19-1143,D10-1092,0,0.0786448,"Missing"
D19-1143,W10-1736,0,0.157179,"k-translation itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of trainingtime supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives1 1 English: Figure 1: An English sentence re-ordered into Japanese order using the rule-based method of Isozaki et al. (2010b), and its reference Japanese translation. Introduction While neural machine translation (NMT; Bahdanau et al. (2015); Vaswani et al. (2017)) now represents the state of the art in the majority of large-scale MT benchmarks (Bojar et al., 2017), it is highly dependent on the availability of copious parallel resources; NMT under-performs previous phrase-based methods when the training data is small (Koehn and Knowles, 2017). Unfortunately, million-sentence parallel corpora are often unavailable for many language pairs. Conversely, monolingual sentences, particularly in English, are often much e"
D19-1143,C12-1125,0,0.0278611,"ased on linguistic knowledge (Isozaki et al., 2010b; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007), and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012), Arabic (Badr et al., 2009), or Japanese (Isozaki et al., 2010b). In experiments we use Isozaki et al. (2010b)’s method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004), (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers. Word-by-word Translation To generate data for training MT models, we next perform wordby-word translation of ts"
D19-1143,D18-1103,1,0.92659,"ajor challenge for statistical machine translation (SMT), and many techniques have emerged over the years to address this challenge (Xia and McCord, 2004; Bisazza and Federico, 2016). Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005); Isozaki et al. (2010b); Fig. 1). Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzmán et al., 2019). Hence we focus on semi-supervised methods in this paper. 1388 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1388–1394, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. However, similar pre-ordering methods have not proven useful in NMT (Du and Way, 2017), largely b"
D19-1143,J03-1002,0,0.0146323,"data. We then randomly sub-sample low-resource datasets of 3k, 6k, 10k, and 20k parallel sentences, and use the remainder of the 400k English sentences as monolingual data. We duplicate the number of parallel sentences by 5 times in the training data augmented with the reordered pairs. For settings with supervised parallel sentences of 3k, 6k, 10k and 20k, we set the maximum vocabulary size of both Japanese and English to be 10k, 10k, 15k and 20k respectively. To automatically learn a high-precision dictionary on the small amount of parallel data we have available for training, we use GIZA++ (Och and Ney, 2003) to learn alignments in both directions then take the intersection of alignments. We then learn the bilingual word embeddings with DeMaBWE (Zhou et al., 2019), an unsupervised method that has shown strong results on syntactically divergent language pairs. We give the more reliable alignments extracted from GIZA++ high priority by querying the alignment dictionary first, then follow by querying the embedding-induced dictionary. When an English word is not within any vocabulary, we output the English word as-is into the pseudo-source sentence. Model 10 5 0 &lt;10 [10,20) [20,30) [30,40) (b) Ug-En ["
D19-1143,P16-1009,0,0.318971,"ften unavailable for many language pairs. Conversely, monolingual sentences, particularly in English, are often much easier to find, making semisupervised approaches that can use monolingual data a desirable solution to this problem.2 1 I bought a new car . https://github.com/violet-zct/pytorch-.reorder-nmt 2 Unsupervised MT (Artetxe et al., 2018a) has achieved success on simulated low-resource scenarios with related lanSemi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods such as backtranslation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016) or adding an auxiliary autoencoding task on monolingual data (Cheng et al., 2016; He et al., 2016; Currey et al., 2017). However, both methods have problems with lowresource and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering. This paper proposes a method to create pseudoparallel sentences for NMT for language pairs with divergent syntactic structures. Prior to NMT, word r"
D19-1143,C04-1073,0,0.201529,"roblems with lowresource and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering. This paper proposes a method to create pseudoparallel sentences for NMT for language pairs with divergent syntactic structures. Prior to NMT, word reordering was a major challenge for statistical machine translation (SMT), and many techniques have emerged over the years to address this challenge (Xia and McCord, 2004; Bisazza and Federico, 2016). Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005); Isozaki et al. (2010b); Fig. 1). Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzmán et al., 2019). Hence we focus on semi-supervised methods in this paper. 1388 Proceedings of the 2019 Conference on Empirical"
D19-1143,D18-1034,1,0.80607,"10b). In experiments we use Isozaki et al. (2010b)’s method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004), (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers. Word-by-word Translation To generate data for training MT models, we next perform wordby-word translation of ts into pseudo-source sentence sˆ using a bilingual dictionary (Xie et al., 2018).3 There are many ways we can obtain this dictionary: even for many low-resource languages with a paucity of bilingual text, we can obtain manually-curated lexicons with reasonable coverage, or run unsupervised word alignment on whatever parallel data we have available. In addition, we can induce word translations for more words in target language using methods for bilingual lexicon induction over pre-trained word embeddings (e.g. Grave et al. (2018)). 3 Experiments We evaluate our method on two language pairs: Japanese-to-English (ja-en) and Uyghur-toEnglish (ug-en). Japanese and Uyghur are p"
D19-1143,N19-1161,1,0.886249,"Missing"
D19-1147,D11-1033,0,0.0511909,"omain tags to control the output domain, but it still needs a in-domain parallel corpus and our architecture allows more flexible modifications than just adding additional tags. Unsupervised domain adaptation techniques for NMT can be divided into data- and model-centric methods (Chu and Wang, 2018). Data-centric approaches mainly focus on selecting or generating the domain-related data using existing in-domain monolingual data. Both the copy method (Currey et al., 2017) and back-translation (Sennrich et al., 2016a) are representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult to find similar parall"
D19-1147,N19-1191,0,0.168073,"centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult to find similar parallel sentences in domain adaptation settings. 5 Conclusion In this work, we propose a simple yet effective unsupervised domain adaptation technique for neural machine translation, which adapts the model by domain-aware feature embeddings learned with language modeling. Experimental results demonstrate the effectiveness of the proposed approach across settings. In addition, analysis reveals that our method allows us to control the output domain of translation results. Future work include designing more sophisticated architectures and combinatio"
D19-1147,P18-1008,0,0.0324094,"the performance of the model.1 1 θ Layer N … (N ) task … (N ) θ base (N ) θ domain … … + (1) task (1) θ base (0 ) θ task (0 ) θ base θ Layer 1 (1) θ domain + Word Embeddings Input Task (0 ) θ domain Input Word Input Domain Figure 1: Main architecture of DAFE. Embedding learners generate domain- and task-specific features at each layer, which are then integrated into the output of the base network. Introduction While neural machine translation (NMT) systems have proven to be effective in scenarios where large amounts of in-domain data are available (Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018), they have been demonstrated to perform poorly when the test domain does not match the training data (Koehn and Knowles, 2017). Collecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised sett"
D19-1147,P17-2061,0,0.0197637,"Knowles, 2017). Collecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the d"
D19-1147,C18-1111,0,0.0634817,"rk for NMT focus on the setting where a small amount of indomain data is available. Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) methods first train an NMT model on outof-domain data and then fine-tune it on the indomain data. Similar to our work, Kobus et al. 1420 (2017) propose to use domain tags to control the output domain, but it still needs a in-domain parallel corpus and our architecture allows more flexible modifications than just adding additional tags. Unsupervised domain adaptation techniques for NMT can be divided into data- and model-centric methods (Chu and Wang, 2018). Data-centric approaches mainly focus on selecting or generating the domain-related data using existing in-domain monolingual data. Both the copy method (Currey et al., 2017) and back-translation (Sennrich et al., 2016a) are representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two mo"
D19-1147,W17-4715,0,0.347043,"com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and synthetic and natural data are treated equally. Sharing embeddings may be sub-optimal as data from different domains are inherently different. This problem is exacerbated 1417 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confe"
D19-1147,P13-2119,1,0.821256,"he output domain, but it still needs a in-domain parallel corpus and our architecture allows more flexible modifications than just adding additional tags. Unsupervised domain adaptation techniques for NMT can be divided into data- and model-centric methods (Chu and Wang, 2018). Data-centric approaches mainly focus on selecting or generating the domain-related data using existing in-domain monolingual data. Both the copy method (Currey et al., 2017) and back-translation (Sennrich et al., 2016a) are representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult to find similar parallel sentences in dom"
D19-1147,W17-4713,0,0.0627002,"re representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult to find similar parallel sentences in domain adaptation settings. 5 Conclusion In this work, we propose a simple yet effective unsupervised domain adaptation technique for neural machine translation, which adapts the model by domain-aware feature embeddings learned with language modeling. Experimental results demonstrate the effectiveness of the proposed approach across settings. In addition, analysis reveals that our method allows us to control the output domain of translation results. Future work include designing more sophisticated arch"
D19-1147,P16-1009,0,0.503387,"he research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and synthetic and natural data are treated equally. Sharing embeddings may be sub-optimal as data from different domains are inherently different. This problem is exacerbated 1417 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1417–1422, c Hong Kong, China, November 3–7, 2019. 2019 As"
D19-1147,P16-1162,0,0.77214,"he research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and synthetic and natural data are treated equally. Sharing embeddings may be sub-optimal as data from different domains are inherently different. This problem is exacerbated 1417 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1417–1422, c Hong Kong, China, November 3–7, 2019. 2019 As"
D19-1147,tiedemann-2012-parallel,0,0.0526779,"out-of-domain lm in data and {θbase , θtask , θdomain } for in-domain data. Training strategy. Our training strategy is shown in Algorithm 1. The ultimate goal is in mt } to learn a set of parameters {θbase , θdomain , θtask for in-domain machine translation. While out-of-domain parallel data allows us to train out mt }, the monolingual data help {θbase , θdomain , θtask in out the model learn both θdomain and θdomain . 3 Experiments 3.1 Setup Datasets. We validate our models in two different data settings. First, we train on the law, medical and IT datasets of the German-English OPUS corpus (Tiedemann, 2012) and test our methods’ ability to adapt from one domain to another. The dataset contain 2K development and test sentences in each domain, and about 715K, 1M and 337K training sentences respectively. These datasets are relatively small and the domains are quite distant from each other. In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018) and law, medical OPUS datasets. For this setting, we consider two language pairs, namely Czech and German to English. The Czech-English and GermanEnglish datasets consist of 1M and 4.5M sentences and"
D19-1147,kobus-etal-2017-domain,0,0.026352,"is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and synthetic and natural data are treated equally. Sharing embeddings may be sub-optimal as data from different domains are inherently different. This pro"
D19-1147,N18-2080,0,0.0178104,"ollecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and syn"
D19-1147,W17-3204,0,0.061295,"(0 ) θ base θ Layer 1 (1) θ domain + Word Embeddings Input Task (0 ) θ domain Input Word Input Domain Figure 1: Main architecture of DAFE. Embedding learners generate domain- and task-specific features at each layer, which are then integrated into the output of the base network. Introduction While neural machine translation (NMT) systems have proven to be effective in scenarios where large amounts of in-domain data are available (Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018), they have been demonstrated to perform poorly when the test domain does not match the training data (Koehn and Knowles, 2017). Collecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 201"
D19-1147,2015.iwslt-evaluation.11,0,0.417714,"test domain does not match the training data (Koehn and Knowles, 2017). Collecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the"
D19-1147,P10-2041,0,0.128139,"(2017) propose to use domain tags to control the output domain, but it still needs a in-domain parallel corpus and our architecture allows more flexible modifications than just adding additional tags. Unsupervised domain adaptation techniques for NMT can be divided into data- and model-centric methods (Chu and Wang, 2018). Data-centric approaches mainly focus on selecting or generating the domain-related data using existing in-domain monolingual data. Both the copy method (Currey et al., 2017) and back-translation (Sennrich et al., 2016a) are representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult"
D19-1147,N19-4007,1,0.697209,"Missing"
D19-1370,K16-1002,0,0.17065,"(Yang et al., 2017; Xu and Durrett, 2018). 3 Pelsmaeker and Aziz (2019) thoroughly investigate using more complicated priors/posteriors (Rezende and Mohamed, 2015) but find only marginal improvements. PPL with ELBO directly since the gap between ELBO and log marginal likelihood might be large, particularly when the posterior does not collapse. Reconstruction loss (Recon). Reconstruction loss is equivalent to the negative reconstruction term in ELBO: −Ez∼qφ (z|x) [log pθ (x|z)]. It characterizes how well the latent code can be used to recover the input. Number of active units (AU, Burda et al. (2016)). Active units correspond to the dimensions of z that covary with observations after the model is trained. More active units usually indicates richer latent representations (Burda et al., 2016). Specifically, a dimension is “active” when it is sensitive to the change in observations x. Here we follow (Burda et al., 2016) and classify a latent dimension z as active if Cov(x, Ez∼q(z|x) [z])) &gt; 0.01. In addition to the metrics above, we include KL between prior and posterior approximation, as well as the negative ELBO, for reference – though we find that these quantities are only partially descr"
D19-1370,D17-1066,0,0.0330264,"17; Xu and Durrett, 2018; Kim et al., 2018; Dieng et al., 2018; He et al., 2019; Pelsmaeker and Aziz, 2019). When a strong decoder (e.g. the LSTM (Hochreiter and Schmidhuber, 1997)) is employed, training often falls into a trivial local optimum where the decoder learns to ignore the latent variable and the encoder fails to encode any information. This phenomenon is referred to as “posterior collapse” (Bowman et al., 2016). Existing efforts tackling this problem include re-weighting the KL loss (Bowman et al., 2016; Kingma et al., 2016; Liu et al., 2019), changing the model (Yang et al., 2017; Semeniuta et al., 2017; Xu and Durrett, 2018), and modifying the training procedure (He et al., 2019). After conducting an empirical examination of the state-of-the-art methods (Section 2), we find that they have difficulty striking a good balance between language modeling and representation learning. In this paper, we present a practically effective combination of two simple heuristic techniques for improving VAE learning: (1) pretraining the inference network using an autoencoder objective and (2) thresholding the KL term in the ELBO objective (also known as “free bits” (Kingma et al., 2016)). The former techniqu"
D19-1370,D18-1480,0,0.088437,"ariable models using neural networks. The generative model of VAEs first samples a latent vector z from a prior p(z), then applies a neural decoder p(x|z) to produce x conditioned on the latent code z. VAEs are trained ∗ Equal contribution. Code is available at https://github.com/ bohanli/vae-pretraining-encoder. 1 where qφ (z|x) represents an approximate posterior distribution (i.e. the encoder or inference network) and pθ (x|z) is the generative distribution (i.e. the decoder). However, modeling text with VAEs has proven to be challenging, and is an open research problem (Yang et al., 2017; Xu and Durrett, 2018; Kim et al., 2018; Dieng et al., 2018; He et al., 2019; Pelsmaeker and Aziz, 2019). When a strong decoder (e.g. the LSTM (Hochreiter and Schmidhuber, 1997)) is employed, training often falls into a trivial local optimum where the decoder learns to ignore the latent variable and the encoder fails to encode any information. This phenomenon is referred to as “posterior collapse” (Bowman et al., 2016). Existing efforts tackling this problem include re-weighting the KL loss (Bowman et al., 2016; Kingma et al., 2016; Liu et al., 2019), changing the model (Yang et al., 2017; Semeniuta et al., 2017;"
D19-1437,Q19-1042,0,0.0622647,"com/XuezheMax/flowseq T Y P✓ (yt |y&lt;t , x). (1) Each factor, P✓ (yt |y&lt;t , x), can be implemented by function approximators such as RNNs (Bahdanau et al., 2015) and Transformers (Vaswani et al., 2017). This factorization takes the complicated problem of joint estimation over an exponentially large output space of outputs y, and turns it into a sequence of tractable multi-class classification problems predicting yt given the previous words, allowing for simple maximum loglikelihood training. However, this assumption of left-to-right factorization may be sub-optimal from a modeling perspective (Gu et al., 2019; Stern et al., 2019), and generation of outputs must be done through a linear left-to-right pass through the output tokens using beam search, which is not easily parallelizable on hardware such as GPUs. Recently, there has been work on nonautoregressive sequence generation for neural machine translation (NMT; Gu et al. (2018); Lee et al. (2018); Ghazvininejad et al. (2019)) and language modeling (Ziegler and Rush, 2019). Nonautoregressive models attempt to model the joint distribution P✓ (y|x) directly, decoupling the dependencies of decoding history during generation. 4282 Proceedings of the"
D19-1437,D18-1149,0,0.119466,"sequence of tractable multi-class classification problems predicting yt given the previous words, allowing for simple maximum loglikelihood training. However, this assumption of left-to-right factorization may be sub-optimal from a modeling perspective (Gu et al., 2019; Stern et al., 2019), and generation of outputs must be done through a linear left-to-right pass through the output tokens using beam search, which is not easily parallelizable on hardware such as GPUs. Recently, there has been work on nonautoregressive sequence generation for neural machine translation (NMT; Gu et al. (2018); Lee et al. (2018); Ghazvininejad et al. (2019)) and language modeling (Ziegler and Rush, 2019). Nonautoregressive models attempt to model the joint distribution P✓ (y|x) directly, decoupling the dependencies of decoding history during generation. 4282 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4282–4292, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics A na¨ıve solution is to assume that each token of the target sequence is independent given the inpu"
D19-1437,D18-1336,0,0.188113,"Missing"
D19-1437,P18-1130,1,0.823148,"oder, the decoder and the posterior networks, together with the multi-scale architecture of the prior flow. The architecture of each flow step is in Figure 3. “correct” token at each step t with zt as input. In this case, FlowSeq reduces to the baseline model in Eq. (2). To escape this undesired local optimum, we apply token-level dropout to randomly drop an entire token when calculating the posterior, to ensure the model also has to learn how to use contextual information. This technique is similar to the “masked language model” in previous studies (Melamud et al., 2016; Devlin et al., 2018; Ma et al., 2018). 3.3 Decoder As the decoder, we take the latent sequence z as input, run it through several layers of a neural sequence model such as a Transformer, then directly predict the output tokens in y individually and independently. Notably, unlike standard seq2seq decoders, we do not perform causal masking to prevent attending to future tokens, making the model fully non-autoregressive. 3.4 Flow Architecture for Prior The flow architecture is based on Glow (Kingma and Dhariwal, 2018). It consists of a series of steps of flow, combined in a multi-scale architecture (see Figure 2.) Each step of flow"
D19-1437,K16-1006,0,0.0154009,"architecture of FlowSeq, including the encoder, the decoder and the posterior networks, together with the multi-scale architecture of the prior flow. The architecture of each flow step is in Figure 3. “correct” token at each step t with zt as input. In this case, FlowSeq reduces to the baseline model in Eq. (2). To escape this undesired local optimum, we apply token-level dropout to randomly drop an entire token when calculating the posterior, to ensure the model also has to learn how to use contextual information. This technique is similar to the “masked language model” in previous studies (Melamud et al., 2016; Devlin et al., 2018; Ma et al., 2018). 3.3 Decoder As the decoder, we take the latent sequence z as input, run it through several layers of a neural sequence model such as a Transformer, then directly predict the output tokens in y individually and independently. Notably, unlike standard seq2seq decoders, we do not perform causal masking to prevent attending to future tokens, making the model fully non-autoregressive. 3.4 Flow Architecture for Prior The flow architecture is based on Glow (Kingma and Dhariwal, 2018). It consists of a series of steps of flow, combined in a multi-scale architec"
D19-1437,N19-4009,0,0.0332461,"all the input tokens can be fed into the RNN in parallel. This makes it possible to use highly-optimized implementations of RNNs such as those provided by cuDNN.3 Thus while RNNs do experience some drop in speed, it is less extreme than that experienced when using autoregressive models. 4 4.1 Experiments Experimental Setups Translation Datasets We evaluate FlowSeq on three machine translation benchmark datasets: WMT2014 DE-EN (around 4.5M sentence pairs), WMT2016 RO-EN (around 610K sentence pairs) and a smaller dataset IWSLT2014 DE-EN (around 150K sentence pairs). We use scripts from fairseq (Ott et al., 2019) to preprocess WMT2014 and IWSLT2014, where the preprocessing steps follow Vaswani et al. (2017) for WMT2014. We use the data provided in Lee et al. (2018) for WMT2016. For both WMT datasets, the source and target languages share the same set of BPE embeddings while for IWSLT2014 we use separate embeddings. During training, we filter out sentences longer than 80 for WMT dataset and 60 for IWSLT, respectively. 3 https://devblogs.nvidia.com/optimizing-recurrentneural-networks-cudnn-5/ 4287 Models WMT2014 EN-DE DE-EN WMT2016 EN-RO RO-EN IWSLT2014 DE-EN WMT2014 EN-DE DE-EN Models Raw Data WMT2016"
D19-1437,D15-1044,0,0.0570656,"al latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art nonautoregressive NMT models and almost constant decoding time w.r.t the sequence length.1 1 (c) (b) Figure 1: (a) Autoregressive (b) non-autoregressive and (c) our proposed sequence generation models. x is the source, y is the target, and z are latent variables. ken in the sequence given the input sequence and previously generated tokens: P✓ (y|x) = t=1 Introduction Neural sequence-to-sequence (seq2seq) models (Bahdanau et al., 2015; Rush et al., 2015; Vinyals et al., 2015; Vaswani et al., 2017) generate an output sequence y = {y1 , . . . , yT } given an input sequence x = {x1 , . . . , xT 0 } using conditional probabilities P✓ (y|x) predicted by neural networks (parameterized by ✓). Most seq2seq models are autoregressive, meaning that they factorize the joint probability of the output sequence given the input sequence P✓ (y|x) into the product of probabilities over the next to1 ⇤ Equal contribution, in alphabetical order. https://github.com/XuezheMax/flowseq T Y P✓ (yt |y&lt;t , x). (1) Each factor, P✓ (yt |y&lt;t , x), can be implemented by fu"
D19-1520,P17-1042,0,0.0334043,"iew can be seen in Figure 1. In the following sections, we describe each of these three steps in detail. 2.1 Cross-lingual Transfer Learning The goal of cross-lingual learning is to take a recognizer trained in a source language, and transfer it to a target language. Our approach to doing so for NER follows that of Xie et al. (2018), and we provide a brief review in this section. To begin with, we assume access to two sets of pre-trained monolingual word embeddings in the source and target languages, X and Y , one small bilingual lexicon, either provided or obtained in an unsupervised manner (Artetxe et al., 2017; Conneau et al., 2017a), and labeled training data in the source language. Using these resources, we train bilingual word embeddings (BWE) to create a word-to-word translation dictionary, and finally 5165 use this dictionary to translate the source training data into the target language, which we use to train an NER model. To learn BWE, we first obtain a linear mapping W by solving the following objective: W ∗ = arg min kW XD − YD kF s.t. W W &gt; = I, W where XD and YD correspond to the aligned word embeddings from the bilingual lexicon. F denotes the Frobenius norm. We can first compute the P"
D19-1520,D09-1031,0,0.0436874,"oaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artieres (2014) discuss the strategies for acquiring partially labeled data. Wanvarie et al. (2011); Neubig et al. (2011); Sperber et al. (2014) show the advantages of training a model on this partially labeled data. All above methods focus on either token or full sequence annotation. The most s"
D19-1520,D16-1153,1,0.842591,"in the same amount of time, in the future we plan to explore mixed-mode annotation where we choose either full sequences or spans for annotation. 4 Related Work Cross-Lingual Transfer: Transferring knowledge from high-resource languages has been extensively used for improving low-resource NER. More common approaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, where"
D19-1520,D18-1366,1,0.834225,"me, in the future we plan to explore mixed-mode annotation where we choose either full sequences or spans for annotation. 4 Related Work Cross-Lingual Transfer: Transferring knowledge from high-resource languages has been extensively used for improving low-resource NER. More common approaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artie"
D19-1520,P17-1171,0,0.020423,"ation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data. The code is publicly available here.1 1 Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work"
D19-1520,N04-4028,0,0.0219771,"etup: We use cross-lingual transfer (§2.1) to train our initial NER model and test on the target language. This is the same setting as Xie et al. (2018) and serves as our baseline. Then we use several active learning strategies to select data for manual annotation using this trained NER model. We compare our proposed ETAL strategy with the following baseline strategies: SAL: Select whole sequences for which the model has least confidence in the most likely labeling (Culotta and McCallum, 2005). CFEAL: Select least confident spans within a sequence using the confidence field estimation method (Culotta and McCallum, 2004). RAND: Select spans randomly from the unlabeled set for annotation. In this experimental setting, we simulate manual annotation by using gold labels for the data selected by active learning. At each subsequent run, we annotate 200 tokens and fine-tune the NER model on all the data acquired so far, which is then used to select data for the next run of annotation. 3.2.1 Results Figure 2 summarizes the results for all datasets across the different experimental settings. Each data-point on the x-axis corresponds to the NER performance after annotating 200 additional tokens. CT denotes using cross"
D19-1520,R11-1017,0,0.0258073,": AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artieres (2014) discuss the strategies for acquiring partially labeled data. Wanvarie et al. (2011); Neubig et al. (2011); Sperber et al. (2014) show the advantages of training a model on this partially labeled data. All above methods focus on either token or full sequence annotation. The most similar work to ours perhaps is that of 5171 (a) Selected spans using ETAL strategy are highlighted for the human annotator to annotate. (b) Human annotator correcting the span boundary and assigning the correct entity type. (c) Human annotator assigning the correct entity type only since selected span boundary is correct. (d) Partially-annotated sequences after being annotated by the human annotator."
D19-1520,P17-2093,0,0.0594355,"ges of training a model on this partially labeled data. All above methods focus on either token or full sequence annotation. The most similar work to ours perhaps is that of 5171 (a) Selected spans using ETAL strategy are highlighted for the human annotator to annotate. (b) Human annotator correcting the span boundary and assigning the correct entity type. (c) Human annotator assigning the correct entity type only since selected span boundary is correct. (d) Partially-annotated sequences after being annotated by the human annotator. Figure 5: Example of the human annotation process for Hindi. Fang and Cohn (2017), which selects informative word types for low-resource POS tagging. However, their method requires the annotator to annotate single tokens, which is not trivially applicable for multi-word entities in practical settings. 5 Conclusion In this paper, we presented a study on how to efficiently bootstrap NER systems for low-resource languages using a combination of cross-lingual transfer learning and active learning. We conducted both simulated and human annotation experiments across different languages and found that: 1) cross-lingual transfer is a powerful tool, constantly beating systems witho"
D19-1520,N13-1014,0,0.0705967,"target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artieres (2014) discuss the strategies for acquiring partially labeled data. Wanvarie et al. (2011); Neubig et al. (2011); Sperber et al. (2014) show the advantages of training a model on this partially labeled data. All above methods focus on either token or full sequence annotation. The most similar work to ours perhaps is that of 5171 (a) Selected spans using ETAL strategy are highlighted for th"
D19-1520,D12-1010,0,0.0273269,"Missing"
D19-1520,W04-3250,0,0.352316,"Missing"
D19-1520,N16-1030,0,0.0106021,"y available here.1 1 Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recogn"
D19-1520,W17-2314,0,0.0209704,"ap NER systems for low-resource languages using a combination of cross-lingual transfer learning and active learning. We conducted both simulated and human annotation experiments across different languages and found that: 1) cross-lingual transfer is a powerful tool, constantly beating systems without using transfer; 2) our proposed recipe works the best among known active learning baselines; 3) our proposed active learning strategy saves annotator much effort while ensuring high quality. In future, to account for different levels of annotator expertise, we plan to combine proactive learning (Li et al., 2017) with our proposed method. Acknowledgement The authors would like to thank Sachin Kumar, Kundan Krishna, Aldrian Obaja Muis, Shirley Anugrah Hayati, Rodolfo Vega and Ramon Sanabria for participating in the human annotation experiments. This work is sponsored by Defense Advanced Research Projects Agency Information Innovation Office (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15-C0114. This research was supported also in part by DARPA grant FA875018-2-0018 funded under the AIDA program. The views and conclusions containe"
D19-1520,P16-1101,0,0.316991,"The code is publicly available here.1 1 Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-r"
D19-1520,D14-1097,0,0.123437,"dvances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use cross-lingual transfer learning (Yarowsky et al., 2001; Ammar et al., 2016), which applies a model trained on another language to low-resource languages, to provide a good preliminary model to start the bootstrapping process. Specifically, we use the model of Xie et al. (2018), which reports strong results on a number of language pairs. Next, on top of this transferred model we further employ active learning (Settles and Craven, 2008; Marcheggiani and Artieres, 2014), which helps improve annotation efficiency by using model predictions to select informative, rather than random, data for human annotators. Finally, the model is fine-tuned on data obtained using active learning to improve accuracy in the target language. Within this recipe, the choice of specific method for choosing and annotating data within active learning is highly important to minimize human effort. One relatively standard method used in previous work on NER is to select full sequences based on a criterion for the uncertainty of the entities recognized therein (Culotta and McCallum, 2005"
D19-1520,D17-1269,0,0.0303035,"Missing"
D19-1520,P11-2093,1,0.898661,"सु ीम कोट नेमांगा जवाब BORGIORG Cross-Lingual Transfer Learning English labeled dataset label Target language unlabeled dataset Target Language labeled dataset query spans ू ल और श क क कमी पर सु ीम कोट नेमांगा जवाब Active Learning Figure 1: Our proposed recipe: cross-lingual transfer is used for projecting annotations from an English labeled dataset to the target language. Entity-targeted active learning is then used to select informative sub-spans which are likely entities for humans to annotate. Finally, the NER model is fine-tuned on this partially-labeled dataset. sentence is of interest (Neubig et al., 2011; Sperber et al., 2014). Inspired by this finding and considering the fact that named entities are both important and sparse, we propose an entity-targeted strategy to save annotator effort. Specifically, we select uncertain subspans of tokens within a sequence that are most likely named entities. This way, the annotators only need to assign types to the chosen subspans without having to read and annotate the full sequence. To cope with the resulting partial annotation of sequences, we apply a constrained version of conditional random fields (CRFs), partial CRFs, during training that only lear"
D19-1520,N18-1202,0,0.0119744,"Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use c"
D19-1520,D08-1112,0,0.75947,"ally, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use cross-lingual transfer learning (Yarowsky et al., 2001; Ammar et al., 2016), which applies a model trained on another language to low-resource languages, to provide a good preliminary model to start the bootstrapping process. Specifically, we use the model of Xie et al. (2018), which reports strong results on a number of language pairs. Next, on top of this transferred model we further employ active learning (Settles and Craven, 2008; Marcheggiani and Artieres, 2014), which helps improve annotation efficiency by using model predictions to select informative, rather than random, data for human annotators. Finally, the model is fine-tuned on data obtained using active learning to improve accuracy in the target language. Within this recipe, the choice of specific method for choosing and annotating data within active learning is highly important to minimize human effort. One relatively standard method used in previous work on NER is to select full sequences based on a criterion for the uncertainty of the entities recognized t"
D19-1520,W17-2630,0,0.0230424,"ferring knowledge from high-resource languages has been extensively used for improving low-resource NER. More common approaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artieres (2014) discuss the strategies for acquiring partially labeled data. Wanvarie et al. (2011); Neubig et al. (2011); Sperber et al. (2014) show the advantages o"
D19-1520,C08-1113,0,0.249022,"ed by this finding and considering the fact that named entities are both important and sparse, we propose an entity-targeted strategy to save annotator effort. Specifically, we select uncertain subspans of tokens within a sequence that are most likely named entities. This way, the annotators only need to assign types to the chosen subspans without having to read and annotate the full sequence. To cope with the resulting partial annotation of sequences, we apply a constrained version of conditional random fields (CRFs), partial CRFs, during training that only learn from the annotated subspans (Tsuboi et al., 2008; Wanvarie et al., 2011). To evaluate our proposed methods, we conducted simulated active learning experiments on 5 languages: Spanish, Dutch, German, Hindi and Indonesian. Additionally, to study our method in a more practical setting, we conduct human annotation experiments on two low-resource languages, Indonesian and Hindi, and one simulated low-resource language, Spanish. In sum, this paper makes the following contributions: 1. We present a bootstrapping recipe for improving low-resource NER. With just onetenth of tokens annotated, our proposed entity-targeted active learning method provid"
D19-1520,D18-1034,1,0.560569,"“how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use cross-lingual transfer learning (Yarowsky et al., 2001; Ammar et al., 2016), which applies a model trained on another language to low-resource languages, to provide a good preliminary model to start the bootstrapping process. Specifically, we use the model of Xie et al. (2018), which reports strong results on a number of language pairs. Next, on top of this transferred model we further employ active learning (Settles and Craven, 2008; Marcheggiani and Artieres, 2014), which helps improve annotation efficiency by using model predictions to select informative, rather than random, data for human annotators. Finally, the model is fine-tuned on data obtained using active learning to improve accuracy in the target language. Within this recipe, the choice of specific method for choosing and annotating data within active learning is highly important to minimize human effor"
D19-1520,N16-1033,0,0.0198852,"language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data. The code is publicly available here.1 1 Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a"
D19-1520,H01-1035,0,0.231617,"ese models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use cross-lingual transfer learning (Yarowsky et al., 2001; Ammar et al., 2016), which applies a model trained on another language to low-resource languages, to provide a good preliminary model to start the bootstrapping process. Specifically, we use the model of Xie et al. (2018), which reports strong results on a number of language pairs. Next, on top of this transferred model we further employ active learning (Settles and Craven, 2008; Marcheggiani and Artieres, 2014), which helps improve annotation efficiency by using model predictions to select informative, rather than random, data for human annotators. Finally, the model is fine-tuned on data o"
D19-1520,D08-1063,0,0.0801523,"for creating a highquality entity gazetteer under a short time budget. Since a naive strategy of SAL allows for more labelled data to be acquired in the same amount of time, in the future we plan to explore mixed-mode annotation where we choose either full sequences or spans for annotation. 4 Related Work Cross-Lingual Transfer: Transferring knowledge from high-resource languages has been extensively used for improving low-resource NER. More common approaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategi"
D19-1520,Q14-1014,1,0.934275,"ब BORGIORG Cross-Lingual Transfer Learning English labeled dataset label Target language unlabeled dataset Target Language labeled dataset query spans ू ल और श क क कमी पर सु ीम कोट नेमांगा जवाब Active Learning Figure 1: Our proposed recipe: cross-lingual transfer is used for projecting annotations from an English labeled dataset to the target language. Entity-targeted active learning is then used to select informative sub-spans which are likely entities for humans to annotate. Finally, the NER model is fine-tuned on this partially-labeled dataset. sentence is of interest (Neubig et al., 2011; Sperber et al., 2014). Inspired by this finding and considering the fact that named entities are both important and sparse, we propose an entity-targeted strategy to save annotator effort. Specifically, we select uncertain subspans of tokens within a sequence that are most likely named entities. This way, the annotators only need to assign types to the chosen subspans without having to read and annotate the full sequence. To cope with the resulting partial annotation of sequences, we apply a constrained version of conditional random fields (CRFs), partial CRFs, during training that only learn from the annotated su"
D19-1520,W02-2024,0,0.192088,"ain. We also compare the NER performance using two other training schemes: C ORPUS AUG, where we train the model on the concatenated corpus of transferred data and the newly acquired data, and C ORPUS AUG +F INE T UNE, where we additionally fine-tune the model trained using C ORPUS AUG on just the newly acquired data. 3 Experiments In this section, we evaluate the effectiveness of our proposed strategy in both simulated (§3.2) and human-annotation experiments (§3.3). 3.1 Experimental Settings Datasets: The first evaluation set includes the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for Spanish (from the Romance family), Dutch and German (like English, from the Germanic family). We use the standard corpus splits for train/dev/test. The second evaluation set is for the low-resource setting where we use the Indonesian (from the Austronesian family), Hindi (from the Indo-Aryan family) and Spanish datasets released by the Linguistic Data Consortium (LDC).2 We generate the train/dev/test split by random sampling. Details of the corpus statistics are in the Appendix. English-transferred Data: We use the same experimental settings and resou"
D19-1520,W03-0419,0,\N,Missing
D19-1520,D14-1162,0,\N,Missing
D19-5601,W18-2716,0,0.0593982,"Missing"
D19-5601,W04-1013,0,0.0851055,"-text NLG and MT along two axes: • MT+NLG: RotoWire, WMT19, Monolingual RotoWire refers to the RotoWire dataset (Wiseman et al., 2017) (train/valid), WMT19 refers to the set of parallel corpora allowable by the WMT 2019 English-German task, and Monolingual refers to monolingual data allowable by the same WMT 2019 task, pre-trained embeddings (e.g., GloVe (Pennington et al., 2014)), pre-trained contextualized embeddings (e.g., BERT (Devlin et al., 2019)), pre-trained language models (e.g., GPT-2 (Radford et al., 2019)). Textual Accuracy Measures: We used BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for texutal accuracy compared to reference summaries. Content Accuracy Measures: We evaluate the fidelity of the generated content to the input data using relation generation (RG), content selection (CS), and content ordering (CO) metrics (Wiseman et al., 2017). 2 model for both languages together, using a shared BPE vocabulary obtained from target game summaries and by prefixing the target text with the target language indicator. For MT and MT+NLG tracks, they mined the in-domain data by extracting basketball-related texts from Newscrawl when one of the following conditions are m"
D19-5601,D14-1162,0,0.0824718,"Missing"
D19-5601,D15-1044,0,0.0588225,"types of inputs. The results of the shared task are summarized in Sections 3 and 4. Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 3rd Workshop on Neural Machine Translation and Generation (WNGT 2019) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (Rush et al., 2015), NLG from structured data (Wen et al., 2015), dialog response generation (Vinyals and Le, 2015), among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: this year we continued to encourage submissions that not only advance the state of the art through algorithmic advances, but also analyze and understand the current state of the art, pointing to future research directions. Towards this Summary of Research Contributions We published a call for long papers, extended abstracts for pre"
D19-5601,W18-6502,0,0.0159592,"ocument-level Generation and Translation # documents Avg. # tokens (En) Avg. # tokens (De) Vocabulary size (En) Vocabulary size (De) The first shared task at the workshop focused on document-level generation and translation. Many recent attempts at NLG have focused on sentencelevel generation (Lebret et al., 2016; Gardent et al., 2017). However, real world language generation applications tend to involve generation of much larger amount of text such as dialogues or multisentence summaries. The inputs to NLG systems also vary from structured data such as tables (Lebret et al., 2016) or graphs (Wang et al., 2018), to textual data (Nallapati et al., 2016). Because of such difference in data and domain, comparison between different methods has been nontrivial. This task aims to (1) push forward such document-level generation technology by providing a testbed, and (2) examine the differences between generation based on different types of inputs including both structured data and translations in another language. In particular, we provided the following 6 tracks which focus on different input/output requirements: Valid Test 242 323 320 4163 5425 240 328 324 - 241 329 325 - Table 1: Data statistics of Roto"
D19-5601,D13-1176,0,\N,Missing
D19-5601,D15-1199,0,\N,Missing
D19-5601,P02-1040,0,\N,Missing
D19-5601,P10-2041,0,\N,Missing
D19-5601,W14-3302,0,\N,Missing
D19-5601,D17-1239,0,\N,Missing
D19-5601,W14-7001,0,\N,Missing
D19-5601,W17-4717,0,\N,Missing
D19-5601,W17-3518,0,\N,Missing
D19-5601,N19-1423,0,\N,Missing
D19-5606,D11-1033,0,0.121651,"is to assign higher weights to in-domain data than out-of-domain data. Using LMs or monolingual data to address domain adaptation has been investigated by several researchers (Sennrich et al., 2016a; Currey et al., 2017; Hu et al., 2019). Moore and Lewis (2010); 66 and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. Axelrod et al. (2011) use LMs to score the out-ofdomain data and then select data that are similar to in-domain text based on the resulting scores, a paradigm adapted by Duh et al. (2013) to neural models. Gulcehre et al. (2015) propose two fusion techniques, namely shallow fusion and deep fusion, to integrate LM and NMT model. Shallow fusion mainly combines LM and NMT model during decoding while deep fusion integrates the two models during training. Researchers have also proposed to perform adaptation for NMT by retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; B"
D19-5606,W14-3302,0,0.0742296,"Missing"
D19-5606,P17-2061,0,0.0315653,"-Log Probability 25 20 15 10 5 0 LAW MEDICAL Language Models Figure 1: Mean log probabilities of NMT models and LMs trained on law and medical domains for the words (”needle”, ”hepatic”, ”complete”, ”justify”, ”suspend”). LM and NMT probabilities are correlated for each domain. (More examples in Section 5.1.) more sensitive medical records. Therefore, it is essential to explore effective methods for utilizing out-of-domain data to train models that generalize well to in-domain data. There is a rich literature in domain adaptation for neural networks (Luong and Manning, 2015; Tan et al., 2017; Chu et al., 2017; Ying et al., 2018). In particular, we focus on two lines of work that are conducive to unsupervised adaptation, where there is no training data available in the target domain. The first line of work focuses on aligning representations of data from different domains with the goal of improving data sharing across the two domains using techniques such as mean maximum discrepancy (Long et al., 2015) or adversarial training (Ganin et al., 2016; Sankaranarayanan and Balaji, 2017). However, these methods attempt to smooth over the differences in the domains by learning domain-invariant Most recent"
D19-5606,C18-1111,0,0.0557631,"Long et al. (2015) propose deep adaptation networks that minimize a multiple kernel maximum mean discrepancy (MK-MMD) between source and target domains. Sankaranarayanan and Balaji (2017) on the other hand utilize adversarial training to match different domains. Researchers have also tried to use language models for unsupervised domain adaptation. For example, Siddhant et al. (2019) propose to apply Embeddings from Language Models (ELMo) (Peters et al., 2018) and its variants in unsupervised transfer learning. 6.2 Domain Adaptation for NMT Domain adaptation is an active research topic in NMT (Chu and Wang, 2018). Many previous works focus on the setting where a small amount of in-domain data is available. For instance, continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) is one of the most popular methods, whose basic idea is to first train an NMT model on out-of-domain data and then finetune it on the in-domain data. Also, Wang et al. (2017) propose instance weighting methods for NMT domain adaptation problem, the main goal of which is to assign higher weights to in-domain data than out-of-domain data. Using LMs or monolingual data to address domain adaptation has been investig"
D19-5606,N19-1423,0,0.00799698,"the target domain. The first line of work focuses on aligning representations of data from different domains with the goal of improving data sharing across the two domains using techniques such as mean maximum discrepancy (Long et al., 2015) or adversarial training (Ganin et al., 2016; Sankaranarayanan and Balaji, 2017). However, these methods attempt to smooth over the differences in the domains by learning domain-invariant Most recent success of deep neural networks rely on the availability of high quality and labeled training data (He et al., 2017; Vaswani et al., 2017; Povey et al., 2018; Devlin et al., 2019). In particular, neural machine translation (NMT) models tend to perform poorly if they are not trained with enough parallel data from the test domain (Koehn and Knowles, 2017). However, it is not realistic to collect large amounts of parallel data in all possible domains due to the high cost of data collection. Moreover, certain domains by nature have far less data than others. For example, there is much more news produced and publicly available than at 15 0 Introduction 1 Code is available zdou0830/DDA. 20 https://github.com/ 59 Proceedings of the 3rd Workshop on Neural Generation and Transl"
D19-5606,2015.iwslt-evaluation.11,0,0.326022,"l settings.1 1 10 5 LAW NMT Models MEDICAL -Log Probability 25 20 15 10 5 0 LAW MEDICAL Language Models Figure 1: Mean log probabilities of NMT models and LMs trained on law and medical domains for the words (”needle”, ”hepatic”, ”complete”, ”justify”, ”suspend”). LM and NMT probabilities are correlated for each domain. (More examples in Section 5.1.) more sensitive medical records. Therefore, it is essential to explore effective methods for utilizing out-of-domain data to train models that generalize well to in-domain data. There is a rich literature in domain adaptation for neural networks (Luong and Manning, 2015; Tan et al., 2017; Chu et al., 2017; Ying et al., 2018). In particular, we focus on two lines of work that are conducive to unsupervised adaptation, where there is no training data available in the target domain. The first line of work focuses on aligning representations of data from different domains with the goal of improving data sharing across the two domains using techniques such as mean maximum discrepancy (Long et al., 2015) or adversarial training (Ganin et al., 2016; Sankaranarayanan and Balaji, 2017). However, these methods attempt to smooth over the differences in the domains by le"
D19-5606,P13-2119,1,0.878862,"Missing"
D19-5606,P10-2041,0,0.103586,"le. For instance, continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) is one of the most popular methods, whose basic idea is to first train an NMT model on out-of-domain data and then finetune it on the in-domain data. Also, Wang et al. (2017) propose instance weighting methods for NMT domain adaptation problem, the main goal of which is to assign higher weights to in-domain data than out-of-domain data. Using LMs or monolingual data to address domain adaptation has been investigated by several researchers (Sennrich et al., 2016a; Currey et al., 2017; Hu et al., 2019). Moore and Lewis (2010); 66 and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. Axelrod et al. (2011) use LMs to score the out-ofdomain data and then select data that are similar to in-domain text based on the resulting scores, a paradigm adapted by Duh et al. (2013) to neural models. Gulcehre et al. (2015) propose two fusion techniques, na"
D19-5606,N19-4007,1,0.794877,"8.48 9.28* 13.71 15.08 16.40* 30.12 30.34 30.69 28.88 28.72 28.85 LAW MED IT w/o copying monolingual data Koehn and Knowles (2017) 12.1 Baseline 13.60 LM-Shallow 13.74 DDA-Shallow 16.39* w/ copying monolingual data Baseline 17.14 LM-Deep 17.74 DDA-Deep 18.02† w/ back-translated data Baseline 22.89 LM-Deep 23.58 DDA-Deep 23.74 IT Table 1: Translation accuracy (BLEU; Papineni et al. (2002)) under different settings. The first three rows list the language pair, the source domain, and the target domain. “LAW”, “MED” and “IT” represent law, medical and IT domains, respectively. We use compare-mt (Neubig et al., 2019) to perform significance tests (Koehn, 2004) and statistical significance compared with the best baseline is indicated with ∗ (p &lt; 0.005) and † (p &lt; 0.05). et al., 2014) which contain data from several domains and test on the multilingual TED test sets of Duh (2018).5 We consider two language pairs for this setting, namely Czech and German to English. The Czech-English and German-English datasets consist of about 1M and 4.5M sentences respectively and the development and test sets contain about 2K sentences. Byte-pair encoding (Sennrich et al., 2016b) is employed to process training data into"
D19-5606,W17-4713,0,0.0624502,"Missing"
D19-5606,P02-1040,0,0.103736,"Missing"
D19-5606,N18-1202,0,0.140267,", DDA-Shallow can significantly improve the baseline model by over 2 BLEU points. However, the DDA-Deep model cannot outperform baselines by a large margin, probably because the baseline models are strong when adapting from a general domain to a specific domain and thus additional adaptation strategies can only lead to incremental improvements. 5 5.1 5.2 In this section, we try to fuse different parts of LMs and NMT models. Prior works have tried different strategies such as fusing the hidden states of LMs with NMT models (Gulcehre et al., 2015) or combining multiple layers of a deep network (Peters et al., 2018). Therefore, it would be interesting to find out which combination of hidden vectors in our DDA-Deep method would be more helpful. Specifically, we try to fuse word embeddings, hidden states and output probabilities. Analysis Domain Differences between NMT Models and LMs 10 log pN M T −LAW − log pN M T −M ED Fusing Different Parts of the Models Components LAW-MED MED-LAW Word-Embed Hidden States Word-Embed & Hidden States 17.43 18.02 5.26 5.85 18.00 5.79 8 Table 2: Performance of DDA-Deep when fusing different parts of models on the law and medical datasets. 6 4 We conduct experiments on the l"
D19-5606,P19-1286,1,0.747684,"in data is available. For instance, continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) is one of the most popular methods, whose basic idea is to first train an NMT model on out-of-domain data and then finetune it on the in-domain data. Also, Wang et al. (2017) propose instance weighting methods for NMT domain adaptation problem, the main goal of which is to assign higher weights to in-domain data than out-of-domain data. Using LMs or monolingual data to address domain adaptation has been investigated by several researchers (Sennrich et al., 2016a; Currey et al., 2017; Hu et al., 2019). Moore and Lewis (2010); 66 and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. Axelrod et al. (2011) use LMs to score the out-ofdomain data and then select data that are similar to in-domain text based on the resulting scores, a paradigm adapted by Duh et al. (2013) to neural models. Gulcehre et al. (2015) propose t"
D19-5606,P16-1009,0,0.227643,"IT domains, respectively. We use compare-mt (Neubig et al., 2019) to perform significance tests (Koehn, 2004) and statistical significance compared with the best baseline is indicated with ∗ (p &lt; 0.005) and † (p &lt; 0.05). et al., 2014) which contain data from several domains and test on the multilingual TED test sets of Duh (2018).5 We consider two language pairs for this setting, namely Czech and German to English. The Czech-English and German-English datasets consist of about 1M and 4.5M sentences respectively and the development and test sets contain about 2K sentences. Byte-pair encoding (Sennrich et al., 2016b) is employed to process training data into subwords with a vocabulary size of 50K for both settings. fusion learns to combine hidden states of LMin and NMT-out. We denote shallow fusion and deep fusion as “LM-Shallow” and “LM-Deep”. 2) The copied monolingual data model (Currey et al., 2017) which copies target in-domain monolingual data to the source side to form synthetic indomain data. 3) Back-translation (Sennrich et al., 2016a) which enriches the training data by generating synthetic in-domain parallel data via a targetto-source NMT model which is trained on a outof-domain corpus. Models"
D19-5606,P17-4012,0,0.0330913,"n states of LMin and NMT-out. We denote shallow fusion and deep fusion as “LM-Shallow” and “LM-Deep”. 2) The copied monolingual data model (Currey et al., 2017) which copies target in-domain monolingual data to the source side to form synthetic indomain data. 3) Back-translation (Sennrich et al., 2016a) which enriches the training data by generating synthetic in-domain parallel data via a targetto-source NMT model which is trained on a outof-domain corpus. Models. NMT-out is a 500 dimensional 2-layer attentional LSTM encoder-decoder model (Bahdanau et al., 2015) implemented on top of OpenNMT (Klein et al., 2017). LM-in and LM-out are also 2-layer LSTMs with hidden sizes of 500. Here we mainly test on RNN-based models, but there is nothing architecture-specific in our methods preventing them from being easily adapted to other architectures such as the Transformer model (Vaswani et al., 2017). 4.2 Main Results 4.2.1 Adapting Between Domains The first 6 result columns of Table 1 show the experimental results on the OPUS dataset. We can see the LM-Shallow model can only marginally improve and sometimes even harms the performance of baseline models. On the other hand, our proposed DDA-Shallow model can ou"
D19-5606,P16-1162,0,0.388422,"IT domains, respectively. We use compare-mt (Neubig et al., 2019) to perform significance tests (Koehn, 2004) and statistical significance compared with the best baseline is indicated with ∗ (p &lt; 0.005) and † (p &lt; 0.05). et al., 2014) which contain data from several domains and test on the multilingual TED test sets of Duh (2018).5 We consider two language pairs for this setting, namely Czech and German to English. The Czech-English and German-English datasets consist of about 1M and 4.5M sentences respectively and the development and test sets contain about 2K sentences. Byte-pair encoding (Sennrich et al., 2016b) is employed to process training data into subwords with a vocabulary size of 50K for both settings. fusion learns to combine hidden states of LMin and NMT-out. We denote shallow fusion and deep fusion as “LM-Shallow” and “LM-Deep”. 2) The copied monolingual data model (Currey et al., 2017) which copies target in-domain monolingual data to the source side to form synthetic indomain data. 3) Back-translation (Sennrich et al., 2016a) which enriches the training data by generating synthetic in-domain parallel data via a targetto-source NMT model which is trained on a outof-domain corpus. Models"
D19-5606,W04-3250,0,0.24503,"88 28.72 28.85 LAW MED IT w/o copying monolingual data Koehn and Knowles (2017) 12.1 Baseline 13.60 LM-Shallow 13.74 DDA-Shallow 16.39* w/ copying monolingual data Baseline 17.14 LM-Deep 17.74 DDA-Deep 18.02† w/ back-translated data Baseline 22.89 LM-Deep 23.58 DDA-Deep 23.74 IT Table 1: Translation accuracy (BLEU; Papineni et al. (2002)) under different settings. The first three rows list the language pair, the source domain, and the target domain. “LAW”, “MED” and “IT” represent law, medical and IT domains, respectively. We use compare-mt (Neubig et al., 2019) to perform significance tests (Koehn, 2004) and statistical significance compared with the best baseline is indicated with ∗ (p &lt; 0.005) and † (p &lt; 0.05). et al., 2014) which contain data from several domains and test on the multilingual TED test sets of Duh (2018).5 We consider two language pairs for this setting, namely Czech and German to English. The Czech-English and German-English datasets consist of about 1M and 4.5M sentences respectively and the development and test sets contain about 2K sentences. Byte-pair encoding (Sennrich et al., 2016b) is employed to process training data into subwords with a vocabulary size of 50K for b"
D19-5606,W17-3204,0,0.147504,"using techniques such as mean maximum discrepancy (Long et al., 2015) or adversarial training (Ganin et al., 2016; Sankaranarayanan and Balaji, 2017). However, these methods attempt to smooth over the differences in the domains by learning domain-invariant Most recent success of deep neural networks rely on the availability of high quality and labeled training data (He et al., 2017; Vaswani et al., 2017; Povey et al., 2018; Devlin et al., 2019). In particular, neural machine translation (NMT) models tend to perform poorly if they are not trained with enough parallel data from the test domain (Koehn and Knowles, 2017). However, it is not realistic to collect large amounts of parallel data in all possible domains due to the high cost of data collection. Moreover, certain domains by nature have far less data than others. For example, there is much more news produced and publicly available than at 15 0 Introduction 1 Code is available zdou0830/DDA. 20 https://github.com/ 59 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 59–69 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d 2 features, and in the ca"
D19-5606,tiedemann-2012-parallel,0,0.046241,"out ; sLM-in ; sNMT-out , Deep Adaptation DDA-Shallow only functions during decoding time so there is almost no learning involved. In addition, hyper-parameter β is the same for all 2 Note that this quantity is simply proportional to the log probability, so it is importantPto re-normalize the probability after interpolation to ensure k p(yt = k) = 1. 4 Experiments 4.1 Datasets. We test both DDA-Shallow and DDADeep in two different data settings. In the first setting we use the dataset of Koehn and Knowles (2017), training on the law, medical and IT datasets of the German-English OPUS corpus3 (Tiedemann, 2012). The standard splits contain 2K development and test sentences in each domain, and about 715K, 1M and 337K training sentences respectively. In the second setting, we train our models on the WMT-14 datasets4 (Bojar 3 4 61 Setup http://opus.nlpl.eu https://www.statmt.org/wmt14/translation-task.html Method De-En MED LAW IT LAW MED Cs-En De-En WMT TED TED 3.5 4.34 4.41 5.49* 3.9 4.57 4.54 5.89* 2.0 3.29 3.41 4.51* 1.9 4.30 4.29 5.87* 6.5 8.56 8.15 10.29* 24.25 24.29 26.52* 24.00 24.03 25.53* 6.14 6.01 6.51* 5.09 5.16 5.85* 4.59 4.87 5.39* 5.09 5.01 5.52† 10.65 11.88 12.48* 25.60 25.98 26.44* 24.5"
D19-5606,P16-1008,0,0.0241589,"results on unsupervised domain adaptation for NMT, the translation results still fall behind the gold reference by a large margin and the DDA-Deep performs much worse than the baseline model under a continued training setting as demonstrated in previous sections. In this section, we specify some limitations with our proposed methods and list a few future directions. The objectives of LMs and NMT models are inherently different: LMs care more about the fluency whereas NMT models also need to consider translation adequacy, that is, the translations should faithfully reflect the source sentence (Tu et al., 2016). Therefore, directly integrating LMs with NMT models might have a negative impact on adequacy. To verify this hypothesis, under the continued training setting we adopt a decoding-time coverage penalty (Wu et al., 2016), which is a simple yet effective strategy to reduce the number of dropped tokens. As shown in Table 5, the coverage penalty can improve the deep adaptation method by more than 5 BLEU points while the baseline model can only be improved by 2 BLEU points. The best DDA-Deep method outperforms the baseline by 1.03 BLEU points. These results suggest some promising future directions"
D19-5606,D17-1155,0,0.0891377,"e, Siddhant et al. (2019) propose to apply Embeddings from Language Models (ELMo) (Peters et al., 2018) and its variants in unsupervised transfer learning. 6.2 Domain Adaptation for NMT Domain adaptation is an active research topic in NMT (Chu and Wang, 2018). Many previous works focus on the setting where a small amount of in-domain data is available. For instance, continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) is one of the most popular methods, whose basic idea is to first train an NMT model on out-of-domain data and then finetune it on the in-domain data. Also, Wang et al. (2017) propose instance weighting methods for NMT domain adaptation problem, the main goal of which is to assign higher weights to in-domain data than out-of-domain data. Using LMs or monolingual data to address domain adaptation has been investigated by several researchers (Sennrich et al., 2016a; Currey et al., 2017; Hu et al., 2019). Moore and Lewis (2010); 66 and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and"
D19-5606,1983.tc-1.13,0,0.645632,"Missing"
D19-6127,C10-1032,0,0.270922,"ation systems. Source Language Wikipedia (Wsrc ): KB and corresponding text in the source language. Similarly to English Wikipedia, this can be used to obtain mention-entity maps or train disambiguation systems, but the size of Wikipedia is relatively small for most low-resource languages. Bilingual Entity Maps (M): A map between source language entities and English entities. One common source of this map is Wikipedia interlanguage links between the source language and English. These inter-language links can directly Introduction Entity linking (EL; Bunescu and Pas¸ca (2006); Cucerzan (2007); Dredze et al. (2010); Hoffart et al. (2011)) identifies entity mentions in a document and associates them with their corresponding entries in a structured Knowledge Base (KB) (Shen et al., 2015), such as Wikipedia or Freebase (Bollacker et al., 2008). EL involves two main steps: (1) candidate generation, retrieving a list of candidate KB entries for each entity mention, and (2) disambiguation, selecting the most likely entry from the candidate list. In this work, we focus on cross-lingual entity linking (XEL; McNamee et al. (2011), Ji et al. (2015)), where the document is in a (source) language that is different"
D19-6127,D17-1277,0,0.117861,"Missing"
D19-6127,P16-1059,0,0.508042,"y et al., 2018). The performance of an end-to-end XEL system is measured by accuracy: the proportion of mentions whose predictions are correct. We follow Yamada et al. (2017); Ganea and Hofmann (2017) and focus on in-KB accuracy; we ignore mentions whose linked entity does not exist in the KB in this work. 244 3 Baseline Model of this, we propose a method for calibrated combination of these two methods in Section 5.1. This section describes existing methods for candidate generation and disambiguation, and our baseline XEL system, which is heavily inspired by existing works (Ling et al., 2015; Globerson et al., 2016; Pan et al., 2017). We investigate the effect of resource constraints on this system in Section 4. Based on empirical observations, we propose our improved XEL system in Section 5 and present its results in Section 6. 3.1 3.2 Featurization and Linear Scoring Next, we move to the entity disambiguation step, which we further decompose into (1) the design of features and (2) the choice of inference model that combines these features together. 3.2.1 Featurization Unfortunately for low-resource settings, many XEL disambiguation models rely on extensive resources such as E and Wsrc (Sil et al., 201"
D19-6127,D11-1072,0,0.325007,"Missing"
D19-6127,E06-1002,0,0.145697,"Missing"
D19-6127,K18-1050,0,0.0187097,"or binary features, we attempt to deal with the noise and sparsity inherent in the co-occurrence counts of fg1 . To tackle noise, we calculate the smoothed Positive Pointwise Mutual Information (PPMI) (Church and Hanks, 1990; Ganea et al., 2016) between two entities as fg2 , which robustly estimates how much more the two entities cooccur than we expect by chance. To tackle sparsity, we incorporate English entity embeddings of Yamada et al. (2017), and calculate embedding similarity between two entities as fg3 . Similar techniques have also been used by existing works (Ganea and Hofmann, 2017; Kolitsas et al., 2018). We also add the hyperlink count fg4 between a pair of entities as, if entity ei ’s Wikipedia page mentions ej , they are likely to be related. We name our proposed feature set that includes all features listed in Table 1 as F EAT. 5.3 T T sl (ei,j |D) = Wl2 (σ(Wl1 Φ(ei,j ))) T + Wl3 Φ(ei,j ) where W1l ∈ Rdl ×hl , W2l ∈ Rhl ×1 and W3l ∈ Rdl×1. σ is a non-linear function, for which we use leaky rectified linear units (Leaky ReLu; Maas et al. (2013)). We add a linear addition of the input to alleviate the gradient vanishing problem. Equation (4) is revised in a similar way. As discussed in Equa"
D19-6127,J90-1003,0,0.214056,"unary features, we consider the number of mentions an entity is related to as fl3 , where we consider the entity ei,j related to mention mk if it co-occurs with any candidate entity of mk (Moro et al., 2014). We also add the entity prior score fl2 among the whole Wikipedia (Yamada et al., 2017) to reflect the entity’s overall salience. The exact match number fl4 indicates mention coreference. For binary features, we attempt to deal with the noise and sparsity inherent in the co-occurrence counts of fg1 . To tackle noise, we calculate the smoothed Positive Pointwise Mutual Information (PPMI) (Church and Hanks, 1990; Ganea et al., 2016) between two entities as fg2 , which robustly estimates how much more the two entities cooccur than we expect by chance. To tackle sparsity, we incorporate English entity embeddings of Yamada et al. (2017), and calculate embedding similarity between two entities as fg3 . Similar techniques have also been used by existing works (Ganea and Hofmann, 2017; Kolitsas et al., 2018). We also add the hyperlink count fg4 between a pair of entities as, if entity ei ’s Wikipedia page mentions ej , they are likely to be related. We name our proposed feature set that includes all featur"
D19-6127,D07-1074,0,0.169929,"entity disambiguation systems. Source Language Wikipedia (Wsrc ): KB and corresponding text in the source language. Similarly to English Wikipedia, this can be used to obtain mention-entity maps or train disambiguation systems, but the size of Wikipedia is relatively small for most low-resource languages. Bilingual Entity Maps (M): A map between source language entities and English entities. One common source of this map is Wikipedia interlanguage links between the source language and English. These inter-language links can directly Introduction Entity linking (EL; Bunescu and Pas¸ca (2006); Cucerzan (2007); Dredze et al. (2010); Hoffart et al. (2011)) identifies entity mentions in a document and associates them with their corresponding entries in a structured Knowledge Base (KB) (Shen et al., 2015), such as Wikipedia or Freebase (Bollacker et al., 2008). EL involves two main steps: (1) candidate generation, retrieving a list of candidate KB entries for each entity mention, and (2) disambiguation, selecting the most likely entry from the candidate list. In this work, we focus on cross-lingual entity linking (XEL; McNamee et al. (2011), Ji et al. (2015)), where the document is in a (source) langu"
D19-6127,Q15-1023,0,0.519079,"al., 2018; Upadhyay et al., 2018). The performance of an end-to-end XEL system is measured by accuracy: the proportion of mentions whose predictions are correct. We follow Yamada et al. (2017); Ganea and Hofmann (2017) and focus on in-KB accuracy; we ignore mentions whose linked entity does not exist in the KB in this work. 244 3 Baseline Model of this, we propose a method for calibrated combination of these two methods in Section 5.1. This section describes existing methods for candidate generation and disambiguation, and our baseline XEL system, which is heavily inspired by existing works (Ling et al., 2015; Globerson et al., 2016; Pan et al., 2017). We investigate the effect of resource constraints on this system in Section 4. Based on empirical observations, we propose our improved XEL system in Section 5 and present its results in Section 6. 3.1 3.2 Featurization and Linear Scoring Next, we move to the entity disambiguation step, which we further decompose into (1) the design of features and (2) the choice of inference model that combines these features together. 3.2.1 Featurization Unfortunately for low-resource settings, many XEL disambiguation models rely on extensive resources such as E a"
D19-6127,N16-1072,0,0.268714,"ාජධා,යට අය0 රටව2… Sinhala Figure 1: XEL for two low-resource languages – Oromo and Sinhala, linking source mentions to entity “Netherlands” in English Wikipedia. of the KB. Following recent work (Sil et al., 2018; Upadhyay et al., 2018), we use English Wikipedia as this KB. Figure 1 shows an example. XEL to English from major languages such Spanish and Chinese has been carefully studied, and significant progress has been made. Success in these languages can be largely attributed to the availability of rich resources. Specifically, the following is a list of resources required by recent works (Tsai and Roth, 2016; Pan et al., 2017; Sil et al., 2018; Upadhyay et al., 2018): English Wikipedia (Weng ): The target KB and a large corpus of text. Importantly, the text is annotated with anchor text linking between entity mentions (e.g. “Holland” in the body text of an article) and the page for the entity (e.g. “Netherlands”). These annotations can be used to extract mentionentity maps for entity candidate generation, and to directly train entity disambiguation systems. Source Language Wikipedia (Wsrc ): KB and corresponding text in the source language. Similarly to English Wikipedia, this can be used to obta"
D19-6127,D18-1270,0,0.375945,"Missing"
D19-6127,I11-1029,0,0.388613,"irectly Introduction Entity linking (EL; Bunescu and Pas¸ca (2006); Cucerzan (2007); Dredze et al. (2010); Hoffart et al. (2011)) identifies entity mentions in a document and associates them with their corresponding entries in a structured Knowledge Base (KB) (Shen et al., 2015), such as Wikipedia or Freebase (Bollacker et al., 2008). EL involves two main steps: (1) candidate generation, retrieving a list of candidate KB entries for each entity mention, and (2) disambiguation, selecting the most likely entry from the candidate list. In this work, we focus on cross-lingual entity linking (XEL; McNamee et al. (2011), Ji et al. (2015)), where the document is in a (source) language that is different from the (target) language 1 Code is available at shuyanzhou/burn_xel Oromo https://github.com/ 243 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 243–252 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 and unambiguously link entities in the source language KB to the English KB. Multilingual Embeddings (E): These embeddings map words in different languages to the same vector space. The availabi"
D19-6127,Q17-1028,0,0.431467,". We denote the gold entity as e∗ . Performance of candidate generation is measured by gold candidate recall: the proportion of mentions whose top-n candidate list contains the gold entity over all test mentions. This recall upper-bounds performance of an entity disambiguation system. In the consideration of the computational cost of the more complicated downstream disambiguation model, this n is often 30 or smaller (Sil et al., 2018; Upadhyay et al., 2018). The performance of an end-to-end XEL system is measured by accuracy: the proportion of mentions whose predictions are correct. We follow Yamada et al. (2017); Ganea and Hofmann (2017) and focus on in-KB accuracy; we ignore mentions whose linked entity does not exist in the KB in this work. 244 3 Baseline Model of this, we propose a method for calibrated combination of these two methods in Section 5.1. This section describes existing methods for candidate generation and disambiguation, and our baseline XEL system, which is heavily inspired by existing works (Ling et al., 2015; Globerson et al., 2016; Pan et al., 2017). We investigate the effect of resource constraints on this system in Section 4. Based on empirical observations, we propose our impr"
D19-6127,Q14-1019,0,0.036546,", as well as novel features specifically designed to tackle the low-resource scenario. We intentionally avoid features that take source language context words into consideration, as these would be heavily reliant on Weng and M and weaken the transferability of the model. The formulation and resource requirements of unary and binary features are shown in the top and bottom halves of Table 1 respectively. For unary features, we consider the number of mentions an entity is related to as fl3 , where we consider the entity ei,j related to mention mk if it co-occurs with any candidate entity of mk (Moro et al., 2014). We also add the entity prior score fl2 among the whole Wikipedia (Yamada et al., 2017) to reflect the entity’s overall salience. The exact match number fl4 indicates mention coreference. For binary features, we attempt to deal with the noise and sparsity inherent in the co-occurrence counts of fg1 . To tackle noise, we calculate the smoothed Positive Pointwise Mutual Information (PPMI) (Church and Hanks, 1990; Ganea et al., 2016) between two entities as fg2 , which robustly estimates how much more the two entities cooccur than we expect by chance. To tackle sparsity, we incorporate English e"
D19-6127,P17-1178,0,0.283873,"hala Figure 1: XEL for two low-resource languages – Oromo and Sinhala, linking source mentions to entity “Netherlands” in English Wikipedia. of the KB. Following recent work (Sil et al., 2018; Upadhyay et al., 2018), we use English Wikipedia as this KB. Figure 1 shows an example. XEL to English from major languages such Spanish and Chinese has been carefully studied, and significant progress has been made. Success in these languages can be largely attributed to the availability of rich resources. Specifically, the following is a list of resources required by recent works (Tsai and Roth, 2016; Pan et al., 2017; Sil et al., 2018; Upadhyay et al., 2018): English Wikipedia (Weng ): The target KB and a large corpus of text. Importantly, the text is annotated with anchor text linking between entity mentions (e.g. “Holland” in the body text of an article) and the page for the entity (e.g. “Netherlands”). These annotations can be used to extract mentionentity maps for entity candidate generation, and to directly train entity disambiguation systems. Source Language Wikipedia (Wsrc ): KB and corresponding text in the source language. Similarly to English Wikipedia, this can be used to obtain mention-entity"
D19-6127,P16-1213,0,0.192001,"the advantages of B URN are: (1) it is easy to implement with existing neural network toolkits, (2) parameters can be learned endto-end, (3) it considers non-linear combinations over more fine-grained features and thus has potential to fit more complex combination patterns, (4) it can model (distance) relations between mentions in the document. Given unary feature vector Φ(ei,j ) with dl features, B URN replaces the linear combination in Equation (1) with two fully connected layers: Next, we introduce the feature set for our disambiguation model, including features inspired by previous work (Sil and Florian, 2016; Ganea et al., 2016; Pan et al., 2017), as well as novel features specifically designed to tackle the low-resource scenario. We intentionally avoid features that take source language context words into consideration, as these would be heavily reliant on Weng and M and weaken the transferability of the model. The formulation and resource requirements of unary and binary features are shown in the top and bottom halves of Table 1 respectively. For unary features, we consider the number of mentions an entity is related to as fl3 , where we consider the entity ei,j related to mention mk if it co-o"
E14-4025,strapparava-valitutti-2004-wordnet,0,0.156175,"s that answer this very question, or more formally “given a particular emotion, what are the most prevalent events (or situations, contexts) that provoke it?”1 Information about these emotion-provoking events is potentially useful for emotion recognition (recognizing emotion based on events mentioned in a dialogue), response generation (providing an answer to emotion-related questions), and answering social-science related questions (discovering events that affect the emotion of a particular segment of the population). 1 This is in contrast to existing sentiment lexicons (Riloff et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Velikovich et al., 2010; Mohammad and Turney, 2013), which only record the sentiment orientation of particular words (such as “meet” or “friend”), which, while useful, are less directly connected to the emotions than the events themselves. 2 Manual Creation of Events In order to create a small but clean set of goldstandard data for each emotion, we first performed 128 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128–132, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational"
E14-4025,N10-1119,0,0.0291115,"mally “given a particular emotion, what are the most prevalent events (or situations, contexts) that provoke it?”1 Information about these emotion-provoking events is potentially useful for emotion recognition (recognizing emotion based on events mentioned in a dialogue), response generation (providing an answer to emotion-related questions), and answering social-science related questions (discovering events that affect the emotion of a particular segment of the population). 1 This is in contrast to existing sentiment lexicons (Riloff et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Velikovich et al., 2010; Mohammad and Turney, 2013), which only record the sentiment orientation of particular words (such as “meet” or “friend”), which, while useful, are less directly connected to the emotions than the events themselves. 2 Manual Creation of Events In order to create a small but clean set of goldstandard data for each emotion, we first performed 128 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128–132, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Emotions happiness sadness anger fear sur"
E14-4025,esuli-sebastiani-2006-sentiwordnet,0,0.0164046,"s very question, or more formally “given a particular emotion, what are the most prevalent events (or situations, contexts) that provoke it?”1 Information about these emotion-provoking events is potentially useful for emotion recognition (recognizing emotion based on events mentioned in a dialogue), response generation (providing an answer to emotion-related questions), and answering social-science related questions (discovering events that affect the emotion of a particular segment of the population). 1 This is in contrast to existing sentiment lexicons (Riloff et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Velikovich et al., 2010; Mohammad and Turney, 2013), which only record the sentiment orientation of particular words (such as “meet” or “friend”), which, while useful, are less directly connected to the emotions than the events themselves. 2 Manual Creation of Events In order to create a small but clean set of goldstandard data for each emotion, we first performed 128 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128–132, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Emotions happine"
E14-4025,P11-1038,0,0.0114375,"his section, we describe an experimental evaluation of the accuracy of automatic extraction of emotion-provoking events. 5.1 Experimental Setup We use Twitter3 as a source of data, as it is it provides a massive amount of information, and also because users tend to write about what they are doing as well as their thoughts, feelings and emotions. We use a data set that contains more than 30M English tweets posted during the course of six weeks in June and July of 2012. To remove noise, we perform a variety of preprocessing, removing emoticons and tags, normalizing using the scripts provided by Han and Baldwin (2011), and Han et al. (2012). CoreNLP4 was used to get the information about part-of-speech, syntactic parses, and lemmas. We prepared four systems for comparison. As a baseline, we use a method that only uses the original seed pattern mentioned in Section 3 to acquire emotion-provoking events. We also evaluate expansions to this method with clustering, with pattern expansion, and with both. We set a 10 iteration limit on the Espresso algorithm and after each iteration, we add the 20 3 2 http://www.twitter.com http://nlp.stanford.edu/software/ corenlp.shtml In the current work we did not allow anno"
E14-4025,D12-1039,0,0.0389339,"Missing"
E14-4025,P06-1015,0,0.0314114,"e shared by more than one person. It should be noted that this will not come anywhere close to covering the entirety of human emotion, but as each event is shared by at least two people in a relatively small sample, any attempt to create a comprehensive dictionary of emotion-provoking events should at least be able to cover the pairs in this collection. We show the most common three events for each emotion in Table 1. 3 3.1 Pattern Expansion Pattern expansion, or bootstrapping algorithms are widely used in the information extraction field (Ravichandran and Hovy, 2002). In particular Espresso (Pantel and Pennacchiotti, 2006) is known as a state-of-the-art pattern expansion algorithm widely used in acquiring relationships between entities. We omit the details of the algorithm for space concerns, but note that applying the algorithm to our proposed task is relatively straightforward, and allows us to acquire additional patterns that may be matched to improve the coverage over the single seed pattern. We do, however, make two changes to the algorithm. The first is that, as we are interested in extracting events instead of entities, we impose the previously mentioned restriction of one verb phrase and one noun phrase"
E14-4025,P02-1006,0,0.0415296,"ly, for each emotion we extract all the events that are shared by more than one person. It should be noted that this will not come anywhere close to covering the entirety of human emotion, but as each event is shared by at least two people in a relatively small sample, any attempt to create a comprehensive dictionary of emotion-provoking events should at least be able to cover the pairs in this collection. We show the most common three events for each emotion in Table 1. 3 3.1 Pattern Expansion Pattern expansion, or bootstrapping algorithms are widely used in the information extraction field (Ravichandran and Hovy, 2002). In particular Espresso (Pantel and Pennacchiotti, 2006) is known as a state-of-the-art pattern expansion algorithm widely used in acquiring relationships between entities. We omit the details of the algorithm for space concerns, but note that applying the algorithm to our proposed task is relatively straightforward, and allows us to acquire additional patterns that may be matched to improve the coverage over the single seed pattern. We do, however, make two changes to the algorithm. The first is that, as we are interested in extracting events instead of entities, we impose the previously men"
E14-4025,W03-0404,0,0.109919,"Missing"
E14-4025,C08-1111,0,0.219614,"ecognition. In this paper, we describe work on creating prevalence-ranked dictionaries of emotionprovoking events through both manual labor and automatic information extraction. To create a manual dictionary of events, we perform a survey asking 30 participants to describe events that caused them to feel a particular emotion, and manually cleaned and aggregated the results into a ranked list. Next, we propose several methods for extracting events automatically from large data from the Web, which will allow us to increase the coverage over the smaller manually created dictionary. We start with Tokuhisa et al. (2008)’s patterns as a baseline, and examine methods for improving precision and coverage through the use of seed expansion and clustering. Finally, we discuss evaluation measures for the proposed task, and perform an evaluation of the automatically extracted emotion-provoking events. The acquired events will be provided publicly upon acceptance of the paper. This paper is concerned with the discovery and aggregation of events that provoke a particular emotion in the person who experiences them, or emotion-provoking events. We first describe the creation of a small manually-constructed dictionary of"
E17-1088,D16-1047,0,0.0201262,"recognition of these languages, a difficult challenge. One of the touted advantages of neural network language models (NNLMs) is their ability to model sparse data (Bengio et al., 2003; Gandhe et al., 2014). However, despite the success of NNLMs 937 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2013a), leading to many further investigations (Chen et al., 2013; Pennington et al., 2014; Shazeer et al., 2016; Bhatia et al., 2016). A key application of word embeddings has been in the initializing of neural network architectures for a wide variety of NLP tasks with limited annotated data (Frome et al., 2013; Zhang et al., 2014; Zoph et al., 2016; Lau and Baldwin, 2016). guage inform embeddings trained with little target language data? Secondly, can such CLWEs improve language modeling in low-resource contexts by initializing the parameters of an NNLM? To answer these questions, we scale down the available monolingual data of the target language to as few as 1k sentences, while maintaining a large source language dataset"
E17-1088,P16-1186,0,0.0258344,"Missing"
E17-1088,D15-1131,0,0.060784,"e (Graves, 2013; Zaremba et al., 2014). Word embeddings have became more popular through the application of shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the s"
E17-1088,W13-3520,0,0.016042,"judgements of word similarity. Here we follow the same evaluation procedure, except where we simulate a lowresource language by reducing the availability of target English monolingual text while preserving a large quantity of source language text from other languages. This allows us to evaluate the CLWEs intrinsically using the WordSim353 task (Finkelstein et al., 2001) before progressing to downstream language modeling where we additionally consider other target languages. We trained a variety of embeddings on English Wikipedia data of between 1k and 128k sentences from the training data of Al-Rfou et al. (2013). In terms of transcribed speech data, this roughly 1 Hyperparameters for both mono and cross-lingual word embeddings: iters=15, negative=25, size=200, window=48, otherwise default. Smaller window sizes led to similar results for monolingual methods. 2 We also tried Italian, Dutch, German and Serbian, yielding similar results but omitted for presentation. 939 1,000 0.6 800 Perplexity Spearman’s ρ 0.8 0.4 0.2 600 400 0.0 1,000 10,000 200 100,000 1,000 Sentences GNC –de CBOW –ru SG –fi –ja –es MKN3 50 Figure 1: Performance of different embeddings on the WordSim353 task with different amounts of"
E17-1088,D16-1136,1,0.594676,"so been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation, leading to the creation of high-quality lexicons, despite the dearth of transcriptions. For example, the training of a quality speech recognition system for Yongning Na, a Sino-Tibetan language spoken by approximately 40k people, is hindered by this lack of data (Do et al., 2014) despite significant linguistic investigation of the language (Michaud, 2008; Michaud, 2016). In this paper we address two research questions. First, is the quality of CLWEs dependent on having large"
E17-1088,E14-1049,0,0.06185,"rk architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the skip-gram (SG) and CBOW methods of Mikolov et al. (2013a) as imˇ uˇrek and plemented in the Gensim package (Reh˚ Sojka, 2010). We"
E17-1088,kamholz-etal-2014-panlex,0,0.0859548,"News Corpus embeddings with 300 dimensions, trained on 100 billion words. The CLWEs were trained using the method of Duong et al. (2016) since their method addresses polysemy which is rampant in dictionaries. The same 1k-128k sentence English Wikipedia data was used but with an additional 5 million sentences of Wikipedia data in a source language. The source languages include Japanese, German, Russian, Finnish, and Spanish, which represent languages of varying similarity with English, some with great morphological and syntactic differences. To relate the languages, we used the PanLex lexicon (Kamholz et al., 2014). Following Duong et al. (2016), we used the default window size of 48 so that the whole sentence’s context is almost always taken into account. This mitigates the effect of word re-ordering between languages. We trained with an embedding dimension of 200 for all data sizes as a larger dimension turned out to be helpful in capturing information from the source side.1 lexicon to connect the languages and represent the words in a common vector space. The model builds on the continuous bag-of-words (CBOW) model (Mikolov et al., 2013a) which learns embeddings by predicting words given their contex"
E17-1088,C12-1089,0,0.037298,"odels (Hochreiter and Schmidhuber, 1997) for modeling long-ranging statistical influences have been shown to be effective (Graves, 2013; Zaremba et al., 2014). Word embeddings have became more popular through the application of shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that in"
E17-1088,P14-2037,0,0.00965011,"Missing"
E17-1088,N15-1157,0,0.0789589,"e of NLP problems have also been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation, leading to the creation of high-quality lexicons, despite the dearth of transcriptions. For example, the training of a quality speech recognition system for Yongning Na, a Sino-Tibetan language spoken by approximately 40k people, is hindered by this lack of data (Do et al., 2014) despite significant linguistic investigation of the language (Michaud, 2008; Michaud, 2016). In this paper we address two research questions. First, is the quality of CLWEs depen"
E17-1088,W16-1609,0,0.0426659,"emains unclear whether their advantages transfer to scenarios with extremely limited amounts of data. Appropriate initialization of parameters in neural network frameworks has been shown to be beneficial across a wide variety of domains, including speech recognition, where unsupervised pretraining of deep belief networks was instrumental in attaining breakthrough performance (Hinton et al., 2012). Neural network approaches to a range of NLP problems have also been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation, leading to the creatio"
E17-1088,W11-2123,0,0.0461598,"Missing"
E17-1088,W16-1614,0,0.00570031,"Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the skip-gram (SG) and CBOW methods of Mikolov et al. (2013a) as imˇ uˇrek and plemented in the Gensim package (Reh˚ Sojka, 2010). We additionally used off-the-shelf CBOW Google News Corpus embeddings with 300 dimensions, trained on"
E17-1088,P14-1011,0,0.0293781,"Graves, 2013), it remains unclear whether their advantages transfer to scenarios with extremely limited amounts of data. Appropriate initialization of parameters in neural network frameworks has been shown to be beneficial across a wide variety of domains, including speech recognition, where unsupervised pretraining of deep belief networks was instrumental in attaining breakthrough performance (Hinton et al., 2012). Neural network approaches to a range of NLP problems have also been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation"
E17-1088,D16-1163,0,0.0176967,"pite the success of NNLMs 937 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2013a), leading to many further investigations (Chen et al., 2013; Pennington et al., 2014; Shazeer et al., 2016; Bhatia et al., 2016). A key application of word embeddings has been in the initializing of neural network architectures for a wide variety of NLP tasks with limited annotated data (Frome et al., 2013; Zhang et al., 2014; Zoph et al., 2016; Lau and Baldwin, 2016). guage inform embeddings trained with little target language data? Secondly, can such CLWEs improve language modeling in low-resource contexts by initializing the parameters of an NNLM? To answer these questions, we scale down the available monolingual data of the target language to as few as 1k sentences, while maintaining a large source language dataset. We assess intrinsic embedding quality by considering correlation with human judgment on the WordSim353 test set (Finkelstein et al., 2001). We then perform language modeling experiments where we initialize the parame"
E17-1088,D13-1141,0,0.0113705,"midhuber, 1997) for modeling long-ranging statistical influences have been shown to be effective (Graves, 2013; Zaremba et al., 2014). Word embeddings have became more popular through the application of shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the"
E17-1088,D16-1124,1,0.822625,"esults of tuning the dimensions of the hidden layer in the LSTM with respect to perplexity on the validation set,3 as well as tuning the order of n-grams used by the MKN language model. A dimension of 100 yielded a good compromise between the smaller and larger training data sizes, while an order 5 MKN model performed slightly better than its lower-order brethren.4 Interestingly, MKN strongly outperforms the LSTM on low quantities of data, with the LSTM language model not reaching parity until between 16k and 32k sentences of data. This is consistent with the results of Chen et al. (2015) and Neubig and Dyer (2016) that show that n-gram models are typically better for rare words, and here our vocabulary is large but training data small since the data are random Wikipedia sentences. However these findings are inconsistent with the belief that NNLMs have the ability to cope well with sparse data conditions because of the smooth distributions that arise from using dense vector representations of words (Bengio et al., 2003). Traditional smoothing stands strong. 4.2 1,000 Perplexity 800 600 400 200 1,000 10,000 100,000 Sentences MKN –nl LSTM –el mono –ja GNC –fi Figure 3: Perplexity of LSTMs when pre-trained"
E17-1088,D14-1162,0,0.112314,"ng, which is a key tool for facilitating speech recognition of these languages, a difficult challenge. One of the touted advantages of neural network language models (NNLMs) is their ability to model sparse data (Bengio et al., 2003; Gandhe et al., 2014). However, despite the success of NNLMs 937 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2013a), leading to many further investigations (Chen et al., 2013; Pennington et al., 2014; Shazeer et al., 2016; Bhatia et al., 2016). A key application of word embeddings has been in the initializing of neural network architectures for a wide variety of NLP tasks with limited annotated data (Frome et al., 2013; Zhang et al., 2014; Zoph et al., 2016; Lau and Baldwin, 2016). guage inform embeddings trained with little target language data? Secondly, can such CLWEs improve language modeling in low-resource contexts by initializing the parameters of an NNLM? To answer these questions, we scale down the available monolingual data of the target language to as few as 1k sentences, while"
E17-1088,W14-1613,0,0.0121171,"shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the skip-gram (SG) and CBOW methods of Mikolov et al. (2013a) as imˇ uˇrek and plemented in the Gensim packag"
E17-1099,N12-1048,0,0.596074,"ITE Figure 1: Example output from the proposed framework in DE → EN simultaneous translation. The heat-map represents the soft alignment between the incoming source sentence (left, upto-down) and the emitted translation (top, leftto-right). The length of each column represents the number of source words being waited for before emitting the translation. Best viewed when zoomed digitally. Introduction Simultaneous translation, the task of translating content in real-time as it is produced, is an important tool for real-time understanding of spoken lectures or conversations (F¨ugen et al., 2007; Bangalore et al., 2012). Different from the typical machine translation (MT) task, in which translation quality is paramount, simultaneous translation requires balancing the trade-off between translation quality and time delay to ensure that users receive translated content in an expeditious manner (Mieno et al., 2015). A number of methods have been proposed to solve this problem, mostly in the context of phrase-based machine translation. These methods are based on a segmenter, which receives the input one word at a time, then decides when to send it to a MT system that translates each 1 Code and data can be found a"
E17-1099,D14-1140,0,0.667149,"Missing"
E17-1099,P14-2090,1,0.870912,"anslation quality is paramount, simultaneous translation requires balancing the trade-off between translation quality and time delay to ensure that users receive translated content in an expeditious manner (Mieno et al., 2015). A number of methods have been proposed to solve this problem, mostly in the context of phrase-based machine translation. These methods are based on a segmenter, which receives the input one word at a time, then decides when to send it to a MT system that translates each 1 Code and data can be found at https://github. com/nyu-dl/dl4mt-simul-trans. segment independently (Oda et al., 2014) or with a minimal amount of language model context (Bangalore et al., 2012). Independently of simultaneous translation, accuracy of standard MT systems has greatly improved with the introduction of neural-networkbased MT systems (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014). Very recently, there have been a few efforts to apply NMT to simultaneous translation either through heuristic modifications to the decoding process (Cho and Esipova, 2016), or through the training of an independent segmentation network that chooses when to perform output using a standard NMT model (Satija and Pin"
E17-1099,P02-1040,0,0.113931,"put words X = {x1 , ..., xTs } to be translated in real-time. We define the simultaneous translation task as sequentially making two interleaved decisions: READ or WRITE . More precisely, the translator READ s a source word xη from the input buffer in chronological order as translation context, or WRITEs a translated word yτ onto the output buffer, resulting in output sentence Y = {y1 , ..., yTt }, and action sequence A = {a1 , ..., aT } consists of Ts READs and Tt WRITEs, so T = Ts + Tt . Similar to standard MT, we have a measure Q(Y ) to evaluate the translation quality, such as BLEU score (Papineni et al., 2002). For simultaneous translation we are also concerned with the fact that each action incurs a time delay D(A). D(A) will mainly be influenced by delay caused by READ, as this entails waiting for a human speaker to continue speaking (about 0.3s per word for an average speaker), while WRITE consists of generating a few words from a machine translaFigure 2: Illustration of the proposed framework: at each step, the NMT environment (left) computes a candidate translation. The recurrent agent (right) will the observation including the candidates and send back decisions–READ or WRITE. tion system, whi"
E17-1099,N13-1023,0,0.369226,"the decoder’s hypothesis. This is one of the limitations of the proposed framework, as the NMT environment is trained on complete source sentences and it may be difficult to predict the verb that has not been seen in the source sentence. One possible way is to fine-tune the NMT model on incomplete sentences to boost its prediction ability. We will leave this as future work. 7 Related Work Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F¨ugen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013). In this approach, the incoming speech stream required to be translated are first recognized and segmented based on an automatic speech recognition (ASR) system. The translation model then works independently based on each of these segments, potentially limiting the quality of translation. To avoid using a fixed segmentation algorithm, Oda et al. (2014) introduced a trainable segmentation component into their system, so that the segmentation leads to better translation quality. Grissom II et al. (2014) proposed a similar framework, however, based on reinforcement l"
E17-1099,P04-1077,0,0.0655476,"simultaneous machine translation, a reward must consider both quality and delay. Quality We evaluate the translation quality using metrics such as BLEU (Papineni et al., 2002). The BLEU score is defined as the weighted geometric average of the modified n-gram precision BLEU0 , multiplied by the brevity penalty BP to punish a short translation. In practice, the vanilla 1055 BLEU score is not a good metric at sentence level because being a geometric average, the score will reduce to zero if one of the precisions is zero. To avoid this, we used a smoothed version of BLEU for our implementation (Lin and Och, 2004). BLEU(Y, Y ∗ ) = BP · BLEU0 (Y, Y ∗ ), (5) where Y ∗ is the reference and Y is the output. We decompose BLEU and use the difference of partial BLEU scores as the reward, that is:  ∆BLEU0 (Y, Y ∗ , t) t<T rtQ = (6) BLEU(Y, Y ∗ ) t=T where Y t is the cumulative output at t (Y 0 = ∅), and ∆BLEU0 (Y, Y ∗ , t) = BLEU0 (Y t , Y ∗ ) − BLEU0 (Y t−1 , Y ∗ ). Obviously, if at = READ, no new words are written into Y , yielding rtQ = 0. Note that we do not multiply BP until the end of the sentence, as it would heavily penalize partial translation results. Delay As another critical feature, delay judges"
E17-1099,D16-1138,0,0.0160829,"SEQ) learning. Jaitly et al. (2015) proposed a SEQ 2 SEQ ASR model that takes fixedsized segments of the input sequence and outputs tokens based on each segment in real-time. It is trained with alignment information using supervised learning. A similar idea for online ASR is proposed by Luo et al. (2016). Similar to Satija and Pineau (2016), they also used reinforcement learning to decide whether to emit a token while reading a new input at each step. Although sharing some similarities, ASR is very different from simultaneous MT with a more intuitive definition for segmentation. In addition, Yu et al. (2016) recently proposed an online alignment model to help sentence compression and morphological inflection. They regarded the alignment between the input and output sequences as a hidden variable, and performed transitions over the input and output sequence. By contrast, the proposed READ and WRITE actions do not necessarily to be performed on aligned words (e.g. in Fig. 1), and are learned to balance the trade-off of quality and delay. 8 Conclusion We propose a unified framework to do neural simultaneous machine translation. To trade off quality and delay, we extensively explore various targets f"
E17-1117,P16-1231,0,0.223878,"Missing"
E17-1117,C02-1126,0,0.0882932,"Missing"
E17-1117,D16-1257,0,0.541811,"n all past actions. The joint probability estimate p(x, y) can be used for both phrase-structure parsing (finding arg maxy p(y |x)) and language modeling (finding p(x) by marginalizing over the set of possible parses for x). Both inference problems can be solved using an importance sampling procedure.4 We report all RNNG performance based on the corrigendum to Dyer et al. (2016). 3 Composition is Key Given the same data, under both the discriminative and generative settings RNNGs were found to parse with significantly higher accuracy than (respectively) the models of Vinyals et al. (2015) and Choe and Charniak (2016) that represent y as a “linearized” sequence of symbols and parentheses without explicitly capturing the tree structure, or even constraining the y to be a well-formed tree (see Table 1). Vinyals et al. (2015) directly predict the sequence of nonterminals, “shifts” (which consume a terminal symbol), and parentheses from left to right, conditional on the input terminal sequence x, while Choe and Charniak (2016) used a sequential LSTM language model on the same linearized trees to create a generative variant of the Vinyals et al. (2015) model. The generative model is used to re-rank parse candid"
E17-1117,P97-1003,0,0.860051,"guistics: Volume 1, Long Papers, pages 1249–1258, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Recurrent Neural Network Grammars An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals.3 Formally, the RNNG is defined by a triple hN, Σ, Θi, where N denotes the set of nonterminal symbols (NP, VP, etc.), Σ the set of all terminal symbols (we assume that N ∩ Σ = ∅), and Θ the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in Θ and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars. The RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequen"
E17-1117,de-marneffe-etal-2006-generating,0,0.0559403,"Missing"
E17-1117,P81-1022,0,0.79275,"Missing"
E17-1117,N16-1024,1,0.631277,"ial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model’s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis. 1 Introduction In this paper, we focus on a recently proposed class of probability distributions, recurrent neural network grammars (RNNGs; Dyer et al., 2016), designed to model syntactic derivations of sentences. We focus on RNNGs as generative probabilistic models over trees, as summarized in §2. Fitting a probabilistic model to data has often been understood as a way to test or confirm some aspect of a theory. We talk about a model’s assumptions and sometimes explore its parameters or posteriors over its latent variables in order to gain understanding of what it “discovers” from the data. In some sense, such models can be thought of as mini-scientists. Neural networks, including RNNGs, are capable of representing larger classes of hypotheses tha"
E17-1117,Q16-1023,0,0.0670931,"Missing"
E17-1117,P02-1017,0,0.525775,"(that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the nonterminal types should be easily inferred given the endocentrically-composed representation, and that ablating the nonterminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG. This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992). A key finding from Klein and Manning (2002) was that, 1255 1 2 3 4 5 6 7 8 9 10 Noun phrases Canadian (0.09) Auto (0.31) Workers (0.2) union (0.22) president (0.18) no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29) nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25) the (0.1) Jamaica (0.1) Tourist (0.03) Board (0.17) ad (0.20) account (0.40) the (0.0) final (0.18) hour (0.81) their (0.0) first (0.23) test (0.77) Apple (0.62) , (0.02) Compaq (0.1) and (0.01) IBM (0.25) both (0.02) stocks (0.03) and ("
E17-1117,P03-1054,0,0.278815,"e 1, Long Papers, pages 1249–1258, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Recurrent Neural Network Grammars An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals.3 Formally, the RNNG is defined by a triple hN, Σ, Θi, where N denotes the set of nonterminal symbols (NP, VP, etc.), Σ the set of all terminal symbols (we assume that N ∩ Σ = ∅), and Θ the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in Θ and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars. The RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequence of actions to construct"
E17-1117,D15-1278,0,0.0303018,"f understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sentiment and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state-ofthe-art parsing performance. Extensive prior work on"
E17-1117,N16-1082,0,0.0221556,"put the verb as the head of the verb phrase. Another interesting finding is that the model pays attention to polarity information, where negations are almost always assigned nontrivial attention weights.7 Furthermore, we find that the model attends to the conjunction terminal in conjunctions of verb phrases (e.g., “VP → VP and VP”, 10), reinforcing the similar finding for conjunction of noun phrases. PPs. In almost all cases, the model attends to the preposition terminal instead of the noun phrases or complete clauses under it, regardless of the type of preposition. Even when the preposi7 Cf. Li et al. (2016), where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntax and language modeling. 1254 tional phrase is only used to make a connection between two noun phrases (e.g., “PP → NP after NP”, 10), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rules, where prepositions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007). 3"
E17-1117,P92-1017,0,0.673069,"ecessary. If the endocentric hypothesis is true (that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the nonterminal types should be easily inferred given the endocentrically-composed representation, and that ablating the nonterminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG. This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992). A key finding from Klein and Manning (2002) was that, 1255 1 2 3 4 5 6 7 8 9 10 Noun phrases Canadian (0.09) Auto (0.31) Workers (0.2) union (0.22) president (0.18) no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29) nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25) the (0.1) Jamaica (0.1) Tourist (0.03) Board (0.17) ad (0.20) account (0.40) the (0.0) final (0.18) hour (0.81) their (0.0) first (0.23) test (0.77) Apple (0.62) , (0.02) Compaq (0.1) and (0.0"
E17-1117,P06-1055,0,0.0272857,"information, even when trained on a fairly small amount of syntactic data. SBAR ADVP WHADVP S PP ADVP above NP SBAR because S PP by NP PP to NP SBAR for S PP at NP PP to NP PP after NP PP at NP PP by NP PP to NP PP under NP PP on NP SBAR as S PP ADVP above NP SBAR than S SBAR SBAR and SBAR PP than NP PP about NP PP about NP SBAR WHADVP SBAR that S SBAR that S SBAR that S SBAR S SBAR WHNP S SBAR S SBAR S PP from S SBAR WHNP S SBAR S SBAR S SBAR WHNP S PP of S PP of NP PP of NP PP of NP PP of NP Figure 5: Sample of PP and SBAR phrase representations. 7 and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006). In a similar work, Sangati and Zuidema (2009) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The"
E17-1117,E09-1080,0,0.0214366,"ly small amount of syntactic data. SBAR ADVP WHADVP S PP ADVP above NP SBAR because S PP by NP PP to NP SBAR for S PP at NP PP to NP PP after NP PP at NP PP by NP PP to NP PP under NP PP on NP SBAR as S PP ADVP above NP SBAR than S SBAR SBAR and SBAR PP than NP PP about NP PP about NP SBAR WHADVP SBAR that S SBAR that S SBAR that S SBAR S SBAR WHNP S SBAR S SBAR S PP from S SBAR WHNP S SBAR S SBAR S SBAR WHNP S PP of S PP of NP PP of NP PP of NP PP of NP Figure 5: Sample of PP and SBAR phrase representations. 7 and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006). In a similar work, Sangati and Zuidema (2009) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models"
E17-1117,D16-1159,0,0.0208633,"ads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sen"
E17-1117,D16-1137,0,0.0306962,", essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sentiment and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG"
E17-1117,W07-2416,0,0.0109927,"preposi7 Cf. Li et al. (2016), where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntax and language modeling. 1254 tional phrase is only used to make a connection between two noun phrases (e.g., “PP → NP after NP”, 10), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rules, where prepositions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007). 3 2.5 2 1.5 1 ADJP VP NP PP QP SBAR Figure 3: Average perplexity of the learned attention vectors on the test set (blue), as opposed to the average perplexity of the uniform distribution (red), computed for each major phrase type. 5.2 Comparison to Existing Head Rules To better measure the overlap between the attention vectors and existing head rules, we converted the trees in PTB §23 into a dependency representation using the attention weights. In this case, the attention weight functions as a “dynamic” head rule, where all other constituents within the same composed phrase are considered t"
E17-1117,J98-4004,0,0.453425,"and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state-ofthe-art parsing performance. Extensive prior work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and nonterminal (Johnson, 1998; Klein and Manning, 2003) augmentations. The conjecture that fine-grained nonterminal rules and labels can be discovered given weaker bracketing structures was based on several studies (Chiang Conclusion We probe what recurrent neural network grammars learn about syntax, through ablation scenarios and a novel variant with a gated attention mechanism on the composition function. The composition function, a key differentiator between the RNNG and other neural models of syntax, is crucial for good performance. Using the attention vectors we discover that the model is learning something similar t"
I11-1087,J96-1002,0,0.0526627,"Missing"
I11-1087,W06-1615,0,0.0740221,"total size of the partial annotation pool produced by using all rules was 248,148 dependencies out of 1,010,648 annotation candidates (not counting the last word of sentences, which has no dependency). The baseline case only used the EHJ-train with no partial annotations from the 5 Related Work There has been a significant amount of work on how to utilize in-domain data to improve the accuracy of parsing. The majority of this work has focused on using unlabeled data in combination with self-training (Roark and Bacchiani, 2003; McClosky et al., 2006) or other semi-supervised learning methods (Blitzer et al., 2006; Nivre et al., 2007; Suzuki et al., 2009). Roark and Bacchiani (2003) also present work on supervised domain adaptation, although this focuses on the utilization of an already-existing indomain corpus. There has also been some work on efficient annotation of data for parsing (Tang et al., 2002; Osborne and Baldridge, 2004; Sassano and Kurohashi, 2010). Most previous work focuses on picking efficient sentences to annotate for parsing, but Sassano and Kurohashi (2010) also present a method for using partially annotated data with deterministic dependency parsers, which can be trivially estimated"
I11-1087,W06-2920,0,0.415876,"eature vector φ = hφ1 , φ2 , . . . , φm i is a vector of non-negative values calculated from features on pairs (x, j), with corresponding weights given by the parameter vector θ = hθ1 , θ2 , . . . , θm i. We estimate θ from sentences annotated with dependencies. It should be noted that the probability p(di ) depends only on i, j, and the inputs w, t, which ensures that it is estimated independently for each wi . Because paˆ rameter estimation does not involve computing d, 2 Pointwise estimation for dependency parsing This work follows the standard setting of recent work on dependency parsing (Buchholz and Marsi, 2006). Given as input a sequence of words, w = hw1 , w2 , . . . , wn i, the goal is to output a dependency tree d = hd1 , d2 , . . . , dn i, where di ≡ j when the head of wi is wj .1 We assume that di = 0 for some word wi in a sentence, which indicates that wi is the head of the sentence. we do not apply the maximum spanning tree algorithm in training. 2.1 A Pointwise MST Parser 2.2 Features The parsing model we pursue in this paper is McDonald et al. (2005)’s edge-factored model. A score, σ(di ), is assigned to each edge (i.e. dependency) di , and parsing finds a dependency tree, ˆ that maximizes"
I11-1087,W01-0521,0,0.0265859,"s trained on fully annotated data. 1 Introduction Parsing is one of the fundamental building blocks of natural language processing, with applications ranging from machine translation (Yamada and Knight, 2001) to information extraction (Miyao et al., 2009). However, while statistical parsers achieve higher and higher accuracies on in-domain text, the creation of data to train these parsers is labor-intensive, which becomes a bottleneck for smaller languages. In addition, it is also a well known fact that accuracy plummets when tested on sentences of a different domain than the training corpus (Gildea, 2001; Petrov et al., 2010), and that in-domain data can be annotated to make up for this weakness. In this paper, we propose a maximum spanning tree (MST) parser that helps ameliorate these problems by allowing for the efficient development of training data. This is done through a combination of an efficient corpus annotation strategy, and a novel parsing method. For corpus construction, we use partial annotation, which allows an 776 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 776–784, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP by the"
I11-1087,W07-1522,0,0.0314663,"ormed, out of all possible annotations to have annotators 2 We take a language-independent approach that does not make any assumptions about the unit of tokenization or the meaning of tags used. 778 ID 01 02 03 04 05 06 07 phrase-based dependency corpus (fully annotated) head phrase 02 党内/noun の/part. ? 07 議論/noun は /part. 04 「 /symbol 保守/noun ? 05 二/noun 党/suff. 論/noun は /part. ? 06 よろし /adj. く/infl. な/adj. い/infl. 。/symbol 」/symbol と /part. ? 07 い/verb う/infl. ? – もの/noun だ /aux. 。/symbol Figure 2: An example of phrase-based dependency annotation for a sentence. the NAIST Text Corpus (NTC) (Iida et al., 2007) to create a small partially-annotated target domain corpus. The NTC consists of newspaper articles from the Mainichi Shimbun.3 Figure 2 shows an example sentence from this corpus annotated with phrase dependencies. To aid the construction of conversion rules, we chose three broad categories of words - content words, function words, and punctuation symbols - that provide clues to the structure of a phrase. Before we explain our rules, we will give a short explanation of these three categories. We defined content words as nouns, verbs, adjectives, interjections, prenominal adjectives, suffixes,"
I11-1087,W02-2016,0,0.511521,"ave no labels because almost all nouns are connected to a verb with a case marker and many important labels are obvious. The words are not annotated with POS tags, so we used a Japanese POS tagger, KyTea (Neubig et al., 2011), trained on about 40k sentences from the BCCWJ (Maekawa, 2008). For the general domain experiments we compared the following systems, using projective parsing algorithms for training because of the assumptions about Japanese parsing outlined in Section 2.1. Theoretically the training time of our method is proportional to the number of annotated dependencies. In line with Kudo and Matsumoto (2002), we make two assumptions about Japanese dependency parsing. First, because Japanese is a headfinal language we assume that every word except the final one in a sentence depends on one of the words located to its right. Second, we assume that all dependencies are projective, in other words that edges in the dependency tree do not cross each other. These assumptions limit the number of candidate heads for a word, reducing the training time. Because all parsers were trained with projective algorithms, the first assumption is most likely the main reason for the difference in training times betwee"
I11-1087,N04-1012,0,0.0282352,"nificant amount of work on how to utilize in-domain data to improve the accuracy of parsing. The majority of this work has focused on using unlabeled data in combination with self-training (Roark and Bacchiani, 2003; McClosky et al., 2006) or other semi-supervised learning methods (Blitzer et al., 2006; Nivre et al., 2007; Suzuki et al., 2009). Roark and Bacchiani (2003) also present work on supervised domain adaptation, although this focuses on the utilization of an already-existing indomain corpus. There has also been some work on efficient annotation of data for parsing (Tang et al., 2002; Osborne and Baldridge, 2004; Sassano and Kurohashi, 2010). Most previous work focuses on picking efficient sentences to annotate for parsing, but Sassano and Kurohashi (2010) also present a method for using partially annotated data with deterministic dependency parsers, which can be trivially estimated from partially annotated data. 782 Parsing Accuracy on NKN-test 0.900 Dependency Accuracy Dependency Accuracy 0.970 0.965 0.960 0.955 Malt MST PW 0.950 0 40000 80000 0.890 0.880 0.870 0.860 0.850 0.840 120000 baseline LAST PAREN FFS CF INFLECT FUNCT all Dependency Conversion Rule Training Set Size (Dependencies) Figure 5:"
I11-1087,I08-7018,0,0.0315213,"nually and all the words are annotated with their heads manually. The Japanese data provided by the CoNLL organizers (Buchholz and Marsi, 2006) are the result of an automatic conversion from phrase (bunsetsu) dependencies. For a more appropriate evaluation we have prepared a word-based dependency data set. The dependencies have no labels because almost all nouns are connected to a verb with a case marker and many important labels are obvious. The words are not annotated with POS tags, so we used a Japanese POS tagger, KyTea (Neubig et al., 2011), trained on about 40k sentences from the BCCWJ (Maekawa, 2008). For the general domain experiments we compared the following systems, using projective parsing algorithms for training because of the assumptions about Japanese parsing outlined in Section 2.1. Theoretically the training time of our method is proportional to the number of annotated dependencies. In line with Kudo and Matsumoto (2002), we make two assumptions about Japanese dependency parsing. First, because Japanese is a headfinal language we assume that every word except the final one in a sentence depends on one of the words located to its right. Second, we assume that all dependencies are"
I11-1087,D10-1069,0,0.0304919,"ully annotated data. 1 Introduction Parsing is one of the fundamental building blocks of natural language processing, with applications ranging from machine translation (Yamada and Knight, 2001) to information extraction (Miyao et al., 2009). However, while statistical parsers achieve higher and higher accuracies on in-domain text, the creation of data to train these parsers is labor-intensive, which becomes a bottleneck for smaller languages. In addition, it is also a well known fact that accuracy plummets when tested on sentences of a different domain than the training corpus (Gildea, 2001; Petrov et al., 2010), and that in-domain data can be annotated to make up for this weakness. In this paper, we propose a maximum spanning tree (MST) parser that helps ameliorate these problems by allowing for the efficient development of training data. This is done through a combination of an efficient corpus annotation strategy, and a novel parsing method. For corpus construction, we use partial annotation, which allows an 776 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 776–784, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP by the maximum spanning tree"
I11-1087,P06-1043,0,0.0492846,"measured the results in the same way as the individual rules. The total size of the partial annotation pool produced by using all rules was 248,148 dependencies out of 1,010,648 annotation candidates (not counting the last word of sentences, which has no dependency). The baseline case only used the EHJ-train with no partial annotations from the 5 Related Work There has been a significant amount of work on how to utilize in-domain data to improve the accuracy of parsing. The majority of this work has focused on using unlabeled data in combination with self-training (Roark and Bacchiani, 2003; McClosky et al., 2006) or other semi-supervised learning methods (Blitzer et al., 2006; Nivre et al., 2007; Suzuki et al., 2009). Roark and Bacchiani (2003) also present work on supervised domain adaptation, although this focuses on the utilization of an already-existing indomain corpus. There has also been some work on efficient annotation of data for parsing (Tang et al., 2002; Osborne and Baldridge, 2004; Sassano and Kurohashi, 2010). Most previous work focuses on picking efficient sentences to annotate for parsing, but Sassano and Kurohashi (2010) also present a method for using partially annotated data with de"
I11-1087,N03-1027,0,0.0339771,"word-based dependencies and measured the results in the same way as the individual rules. The total size of the partial annotation pool produced by using all rules was 248,148 dependencies out of 1,010,648 annotation candidates (not counting the last word of sentences, which has no dependency). The baseline case only used the EHJ-train with no partial annotations from the 5 Related Work There has been a significant amount of work on how to utilize in-domain data to improve the accuracy of parsing. The majority of this work has focused on using unlabeled data in combination with self-training (Roark and Bacchiani, 2003; McClosky et al., 2006) or other semi-supervised learning methods (Blitzer et al., 2006; Nivre et al., 2007; Suzuki et al., 2009). Roark and Bacchiani (2003) also present work on supervised domain adaptation, although this focuses on the utilization of an already-existing indomain corpus. There has also been some work on efficient annotation of data for parsing (Tang et al., 2002; Osborne and Baldridge, 2004; Sassano and Kurohashi, 2010). Most previous work focuses on picking efficient sentences to annotate for parsing, but Sassano and Kurohashi (2010) also present a method for using partiall"
I11-1087,H05-1066,0,0.196071,"Missing"
I11-1087,P10-1037,0,0.259285,", Yusuke Miyao2 , Graham Neubig1 , Shinsuke Mori1 1 Graduate School of Informatics, Kyoto University Yoshida Honmachi, Sakyo-ku, Kyoto, Japan 2 National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan flannery@ar.media.kyoto-u.ac.jp, yusuke@nii.ac.jp, neubig@ar.media.kyoto-u.ac.jp, forest@i.kyoto-u.ac.jp Abstract annotator to skip annotation of unnecessary edges, focusing their efforts only on the ones that will provide the maximal gains in accuracy. While partial annotation has been shown to be an effective annotation strategy for a number of tasks (Tsuboi et al., 2008; Sassano and Kurohashi, 2010; Neubig and Mori, 2010), traditional MST parsers such as that of McDonald et al. (2005) cannot be learned from partially annotated data. The reason for this is that they use structural prediction methods that must be learned from fully annotated sentences. However, a number of recent works (Liang et al., 2008; Neubig et al., 2011) have found that it is possible to ignore structure and still achieve competitive accuracy on tasks such as part-of-speech (POS) tagging. Similarly, recent work on dependency parsing (Spreyer and Kuhn, 2009; Spreyer et al., 2010) has shown that training constraints c"
I11-1087,W09-1104,0,0.127786,"on strategy for a number of tasks (Tsuboi et al., 2008; Sassano and Kurohashi, 2010; Neubig and Mori, 2010), traditional MST parsers such as that of McDonald et al. (2005) cannot be learned from partially annotated data. The reason for this is that they use structural prediction methods that must be learned from fully annotated sentences. However, a number of recent works (Liang et al., 2008; Neubig et al., 2011) have found that it is possible to ignore structure and still achieve competitive accuracy on tasks such as part-of-speech (POS) tagging. Similarly, recent work on dependency parsing (Spreyer and Kuhn, 2009; Spreyer et al., 2010) has shown that training constraints can be relaxed to allow MST parsers to be trained from partially annotated sentences, with only a small reduction in parsing accuracy. In this approach the scoring function used to evaluate potential dependency trees is modified so that it does not penalize trees consistent with the partial annotations used for training. Our formulation of an MST parser is based on an even stronger independence assumption, namely that the score of each edge is independent of the other edges in the dependency tree. While this does have the potential to"
I11-1087,2008.amta-papers.15,0,0.050818,"Missing"
I11-1087,spreyer-etal-2010-training,0,0.34034,"of tasks (Tsuboi et al., 2008; Sassano and Kurohashi, 2010; Neubig and Mori, 2010), traditional MST parsers such as that of McDonald et al. (2005) cannot be learned from partially annotated data. The reason for this is that they use structural prediction methods that must be learned from fully annotated sentences. However, a number of recent works (Liang et al., 2008; Neubig et al., 2011) have found that it is possible to ignore structure and still achieve competitive accuracy on tasks such as part-of-speech (POS) tagging. Similarly, recent work on dependency parsing (Spreyer and Kuhn, 2009; Spreyer et al., 2010) has shown that training constraints can be relaxed to allow MST parsers to be trained from partially annotated sentences, with only a small reduction in parsing accuracy. In this approach the scoring function used to evaluate potential dependency trees is modified so that it does not penalize trees consistent with the partial annotations used for training. Our formulation of an MST parser is based on an even stronger independence assumption, namely that the score of each edge is independent of the other edges in the dependency tree. While this does have the potential to decrease accuracy, it"
I11-1087,neubig-mori-2010-word,1,0.85591,"ig1 , Shinsuke Mori1 1 Graduate School of Informatics, Kyoto University Yoshida Honmachi, Sakyo-ku, Kyoto, Japan 2 National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan flannery@ar.media.kyoto-u.ac.jp, yusuke@nii.ac.jp, neubig@ar.media.kyoto-u.ac.jp, forest@i.kyoto-u.ac.jp Abstract annotator to skip annotation of unnecessary edges, focusing their efforts only on the ones that will provide the maximal gains in accuracy. While partial annotation has been shown to be an effective annotation strategy for a number of tasks (Tsuboi et al., 2008; Sassano and Kurohashi, 2010; Neubig and Mori, 2010), traditional MST parsers such as that of McDonald et al. (2005) cannot be learned from partially annotated data. The reason for this is that they use structural prediction methods that must be learned from fully annotated sentences. However, a number of recent works (Liang et al., 2008; Neubig et al., 2011) have found that it is possible to ignore structure and still achieve competitive accuracy on tasks such as part-of-speech (POS) tagging. Similarly, recent work on dependency parsing (Spreyer and Kuhn, 2009; Spreyer et al., 2010) has shown that training constraints can be relaxed to allow M"
I11-1087,D09-1058,0,0.0158008,"produced by using all rules was 248,148 dependencies out of 1,010,648 annotation candidates (not counting the last word of sentences, which has no dependency). The baseline case only used the EHJ-train with no partial annotations from the 5 Related Work There has been a significant amount of work on how to utilize in-domain data to improve the accuracy of parsing. The majority of this work has focused on using unlabeled data in combination with self-training (Roark and Bacchiani, 2003; McClosky et al., 2006) or other semi-supervised learning methods (Blitzer et al., 2006; Nivre et al., 2007; Suzuki et al., 2009). Roark and Bacchiani (2003) also present work on supervised domain adaptation, although this focuses on the utilization of an already-existing indomain corpus. There has also been some work on efficient annotation of data for parsing (Tang et al., 2002; Osborne and Baldridge, 2004; Sassano and Kurohashi, 2010). Most previous work focuses on picking efficient sentences to annotate for parsing, but Sassano and Kurohashi (2010) also present a method for using partially annotated data with deterministic dependency parsers, which can be trivially estimated from partially annotated data. 782 Parsin"
I11-1087,P11-2093,1,0.923583,"otator to skip annotation of unnecessary edges, focusing their efforts only on the ones that will provide the maximal gains in accuracy. While partial annotation has been shown to be an effective annotation strategy for a number of tasks (Tsuboi et al., 2008; Sassano and Kurohashi, 2010; Neubig and Mori, 2010), traditional MST parsers such as that of McDonald et al. (2005) cannot be learned from partially annotated data. The reason for this is that they use structural prediction methods that must be learned from fully annotated sentences. However, a number of recent works (Liang et al., 2008; Neubig et al., 2011) have found that it is possible to ignore structure and still achieve competitive accuracy on tasks such as part-of-speech (POS) tagging. Similarly, recent work on dependency parsing (Spreyer and Kuhn, 2009; Spreyer et al., 2010) has shown that training constraints can be relaxed to allow MST parsers to be trained from partially annotated sentences, with only a small reduction in parsing accuracy. In this approach the scoring function used to evaluate potential dependency trees is modified so that it does not penalize trees consistent with the partial annotations used for training. Our formula"
I11-1087,P02-1016,0,0.19976,"here has been a significant amount of work on how to utilize in-domain data to improve the accuracy of parsing. The majority of this work has focused on using unlabeled data in combination with self-training (Roark and Bacchiani, 2003; McClosky et al., 2006) or other semi-supervised learning methods (Blitzer et al., 2006; Nivre et al., 2007; Suzuki et al., 2009). Roark and Bacchiani (2003) also present work on supervised domain adaptation, although this focuses on the utilization of an already-existing indomain corpus. There has also been some work on efficient annotation of data for parsing (Tang et al., 2002; Osborne and Baldridge, 2004; Sassano and Kurohashi, 2010). Most previous work focuses on picking efficient sentences to annotate for parsing, but Sassano and Kurohashi (2010) also present a method for using partially annotated data with deterministic dependency parsers, which can be trivially estimated from partially annotated data. 782 Parsing Accuracy on NKN-test 0.900 Dependency Accuracy Dependency Accuracy 0.970 0.965 0.960 0.955 Malt MST PW 0.950 0 40000 80000 0.890 0.880 0.870 0.860 0.850 0.840 120000 baseline LAST PAREN FFS CF INFLECT FUNCT all Dependency Conversion Rule Training Set"
I11-1087,C04-1010,0,0.012227,"otations used for training. Our formulation of an MST parser is based on an even stronger independence assumption, namely that the score of each edge is independent of the other edges in the dependency tree. While this does have the potential to decrease accuracy, it has a number of advantages such as the ability to use partially annotated data, faster speed, and simple implementation. We perform an evaluation of the proposed method on a Japanese dependency parsing task. First, we compare the proposed method to both a traditional MST parser (McDonald et al., 2005), and a deterministic parser (Nivre and Scholz, 2004). We find that despite the lack of structure in our prediction method, the proposed method is still able to achieve accuracy similar to that of We introduce a maximum spanning tree (MST) dependency parser that can be trained from partially annotated corpora, allowing for effective use of available linguistic resources and reduction of the costs of preparing new training data. This is especially important for domain adaptation in a real-world situation. We use a pointwise approach where each edge in the dependency tree for a sentence is estimated independently. Experiments on Japanese dependenc"
I11-1087,C08-1113,1,0.837066,"ora Daniel Flannery1 , Yusuke Miyao2 , Graham Neubig1 , Shinsuke Mori1 1 Graduate School of Informatics, Kyoto University Yoshida Honmachi, Sakyo-ku, Kyoto, Japan 2 National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan flannery@ar.media.kyoto-u.ac.jp, yusuke@nii.ac.jp, neubig@ar.media.kyoto-u.ac.jp, forest@i.kyoto-u.ac.jp Abstract annotator to skip annotation of unnecessary edges, focusing their efforts only on the ones that will provide the maximal gains in accuracy. While partial annotation has been shown to be an effective annotation strategy for a number of tasks (Tsuboi et al., 2008; Sassano and Kurohashi, 2010; Neubig and Mori, 2010), traditional MST parsers such as that of McDonald et al. (2005) cannot be learned from partially annotated data. The reason for this is that they use structural prediction methods that must be learned from fully annotated sentences. However, a number of recent works (Liang et al., 2008; Neubig et al., 2011) have found that it is possible to ignore structure and still achieve competitive accuracy on tasks such as part-of-speech (POS) tagging. Similarly, recent work on dependency parsing (Spreyer and Kuhn, 2009; Spreyer et al., 2010) has show"
I11-1087,nivre-etal-2006-maltparser,0,0.360234,"d, reducing the training time. Because all parsers were trained with projective algorithms, the first assumption is most likely the main reason for the difference in training times between PW and MST. For other languages where possible heads can be located both to the left and right of a word, we expect training times to increase. Our pointwise approach can be extended to handle these languages by changing the constraint on heads from j > i to j 6= i for all di = j. This is an important direction for future work now that we have confirmed that this approach is effective for Japanese. 1. Malt: Nivre et al. (2006)’s MaltParser, using Nivre’s arc-eager algorithm and the option for strict root handling. 2. MST: McDonald et al. (2005)’s MST Parser, using k-best parse size with k=5. 3. PW: Our system, where pointwise estimation is used to estimate dependencies. Stochastic gradient descent training was used to train log-linear models. We performed a second experiment in the general domain to measure the impact of the training corpus size on parsing accuracy. To make smaller training corpora, we set a fixed number of dependency annotations and then sequentially selected sentences from EHJ-train until the des"
I11-1087,P01-1067,0,0.118633,"ources and reduction of the costs of preparing new training data. This is especially important for domain adaptation in a real-world situation. We use a pointwise approach where each edge in the dependency tree for a sentence is estimated independently. Experiments on Japanese dependency parsing show that this approach allows for rapid training and achieves accuracy comparable to state-ofthe-art dependency parsers trained on fully annotated data. 1 Introduction Parsing is one of the fundamental building blocks of natural language processing, with applications ranging from machine translation (Yamada and Knight, 2001) to information extraction (Miyao et al., 2009). However, while statistical parsers achieve higher and higher accuracies on in-domain text, the creation of data to train these parsers is labor-intensive, which becomes a bottleneck for smaller languages. In addition, it is also a well known fact that accuracy plummets when tested on sentences of a different domain than the training corpus (Gildea, 2001; Petrov et al., 2010), and that in-domain data can be annotated to make up for this weakness. In this paper, we propose a maximum spanning tree (MST) parser that helps ameliorate these problems b"
I11-1087,D07-1096,0,\N,Missing
I11-1108,2010.eamt-1.37,0,0.0128883,"ompleted in order, a delay in any part would result in delays for the overall process. Thus, it was necessary to create working tools as fast as possible, even if this meant making sacrifices in accuracy and refining later. • Translation of information to foreign languages: This includes compilation and sharing of technical term multilingual dictionaries, automatic translation of earthquake information, and provision of the translated information in four major languages spoken by foreigners in Japan. We are definitely not the first to focus on the disaster-related natural language processing. Lewis (2010) reports the development project of Haitian Creole translation system in rapid response to the Haiti earthquake in 2010. While their motivations have a lot in common with ours, such as the necessity to set up a deployable system in a very short time span. Corvey et al. (2010) describe the annotation of a corpus about the Oklahoma wildfires, aiming at provision of broadscale information as opposed to safety information mining about individuals. There have also been a number of works on detecting general trends from twitter, including work by Sakaki et al. (2010), who detect earthquakes based on"
I11-1108,W10-0701,0,0.0292763,"to single named entities. Rules scanned the corpus in order, finding the first of three POS tags: “first name (FNAME),” “last name (LNAME),” or “place name (PNAME).” These words are labeled with PERSON, PERSON, or LOCATION NE tags respectively. Continuing in the order of the corpus, all words directly following a marked NE are merged if marked with one of the three previously mentioned POS tags, or as a “suffix (SUF)10 ”. An example of the three step While the annotators were generally more skilled and motivated than those in previous attempts to create language resources using crowdsourcing (Callison-Burch and Dredze, 2010; Finin et al., 2010), given the rapid nature by which the project developed, annotation started before tagging standards were put in place, leading to some inconsistency in the tagged corpus. In retrospect, despite the speed of the project, it would have been helpful to spend some more time thinking about what information was really necessary for the task, and have more experienced annotators do a quick test run before opening annotation to the broader volunteer base. In addition, as many of the annotators were less experienced, explicitly allowing the annotators to “pass” on difficult instan"
I11-1108,P11-1037,0,0.0234734,"Missing"
I11-1108,W10-0512,0,0.123934,"e same family. “I found Taro, Jiro, and Hanako Tanaka of Sendai at an evacuation shelter. [仙台市に住む田中太郎、次郎、 花子が避難所にいました。]” or “The Tanaka family who live in Sendai has been reached! [仙台市に住む田中一家の安否が確認でき ました。]” b) Many person names that are likely written in logographic kanji in normal text were instead written phonetically (using katakana or hiragana). 16 136 Table 6: Label sequences where the errors occurred with Model 2 (t = 0.45). While we focused mainly on extracting information about missing people, it has also been noted that Twitter is a source of other information in disaster situations (Corvey et al., 2010; Vieweg et al., 2010). For example, “50 people in Kesennuma city have evacuated to a hill behind the city hall. [気仙沼で被災し、市民会館の裏山に 50 人 避難しています]” includes the number of people (50) and a concrete location (a hill behind the city hall), and could be a good indicator of where to concentrate rescue efforts. This could be an area of very practical application for recent research in verifying reliability of information on Twitter (Kawahara et al., 2008; Qazvinian et al., 2011). achieved by a fewer number of annotators, reducing inconsistency issues. 6 Application of the System The final step was ve"
I11-1108,I08-7018,0,0.024004,"hemselves. As time was critical, the linguistic analysis tools actually used in the project were based on widely available general domain resources, as well as domain-specific resources gathered within the very early stages of the project. The general domain language resources consisted of the Balanced Corpus of Contemporary Written Japanese 5 http://japan.person-finder.appspot. com/ 6 966 http://trans-aid.jp/ANPI_NLP (in Japanese). 61,376 tweets were collected from March 13th 1:37am until March 14th 16:45pm. A typical tweet containing safety information looked like the following 8 : (BCCWJ) (Maekawa, 2008) and the UniDic dictionary (Den et al., 2008), which are high quality and annotated with a variety of tags. The domain-specific resources gathered specifically for the project and used in the analysis tools described in §4 included: 気仙沼市の田中太郎・花子さんと連絡 が取れません！どなたか消息をご存知 ありませんでしょうか？ TANAKA Taro and Hanako who lived in Kesennuma City can’t be reached. Does anybody know where they are? • The dictionary used in the open source Mozc Japanese Input method, which contained 50,848 first names and 26,519 last names and was provided by the maintainer of the project. From the large corpus of tweets, we hop"
I11-1108,P11-2093,1,0.916313,"afety information on this site! Survivor list of XXX City: http://... (Non-Japanese or nonsense postings) Count 405 1,154 93 4,438 280 1,903 24,035 773 1,235 Table 1: Safety information tags on tweets depend on MA, and cannot be developed until MA is in place. Thus, we utilized existing general domain resources, and added new resources as they were collected in a domain-adaptation framework. For this task we used KyTea9 , an open source morphological analysis tool notable for being relatively robust to out-of-domain data, and being able to flexibly incorporate a variety of language resources (Neubig et al., 2011). We trained a word segmentation (WS) and POS tagging model for KyTea using the BCCWJ and UniDic as a base. We trained the POS tagging model, but in order to facilitate NE recognition farther down the pipeline, we replaced all proper nouns with their subcategory tag (“first name,” “place name,” etc.). We also added a corpus of conversational and news text (CN Corpus) that was only annotated with word boundaries, and a large list of Japanese first and last names. We indicate the model trained with all of these resources as ORIG. While the POS tagger works on a word-byword basis, most named enti"
I11-1108,den-etal-2008-proper,0,0.0129672,"istic analysis tools actually used in the project were based on widely available general domain resources, as well as domain-specific resources gathered within the very early stages of the project. The general domain language resources consisted of the Balanced Corpus of Contemporary Written Japanese 5 http://japan.person-finder.appspot. com/ 6 966 http://trans-aid.jp/ANPI_NLP (in Japanese). 61,376 tweets were collected from March 13th 1:37am until March 14th 16:45pm. A typical tweet containing safety information looked like the following 8 : (BCCWJ) (Maekawa, 2008) and the UniDic dictionary (Den et al., 2008), which are high quality and annotated with a variety of tags. The domain-specific resources gathered specifically for the project and used in the analysis tools described in §4 included: 気仙沼市の田中太郎・花子さんと連絡 が取れません！どなたか消息をご存知 ありませんでしょうか？ TANAKA Taro and Hanako who lived in Kesennuma City can’t be reached. Does anybody know where they are? • The dictionary used in the open source Mozc Japanese Input method, which contained 50,848 first names and 26,519 last names and was provided by the maintainer of the project. From the large corpus of tweets, we hope to discover two pieces of information. Firs"
I11-1108,D11-1147,0,0.025353,"Missing"
I11-1108,W10-0713,0,0.0458195,"ed the POS tagging model, but in order to facilitate NE recognition farther down the pipeline, we replaced all proper nouns with their subcategory tag (“first name,” “place name,” etc.). We also added a corpus of conversational and news text (CN Corpus) that was only annotated with word boundaries, and a large list of Japanese first and last names. We indicate the model trained with all of these resources as ORIG. While the POS tagger works on a word-byword basis, most named entities consist of multiple words. Previous work has developed linguistic resources for English NE tagging on Twitter (Finin et al., 2010), but again considering the short time frame, we developed a simple rule-based system to connect multiple words into single named entities. Rules scanned the corpus in order, finding the first of three POS tags: “first name (FNAME),” “last name (LNAME),” or “place name (PNAME).” These words are labeled with PERSON, PERSON, or LOCATION NE tags respectively. Continuing in the order of the corpus, all words directly following a marked NE are merged if marked with one of the three previously mentioned POS tags, or as a “suffix (SUF)10 ”. An example of the three step While the annotators were gener"
I11-1108,D11-1141,0,0.0191222,"tive learning for WS fixed segmentation errors such as correcting the improperly segmented “平浄 水場” to the properly segmenting “平 浄水 場” (Taira water purifyTable 2: Resources used in building the model with names, word counts, whether each corpus is annotated with WS and POS information, and which group the resource was added in. NE tagging process is shown in Figure 2. 4.2 Domain Adaptation While this classifier worked well on general domain data, it is known that accuracy greatly decreases for text in different domains or styles than the training data (Finin et al., 2010; Neubig et al., 2011; Ritter et al., 2011). In the tweet data there were a large number of place and person names specific to the disaster-stricken region, as well as a large number of linguistic phenomena specific to tweets, and thus it was necessary to add a number of language resources (summarized in Table 2) to adapt the text processing tools to the new domain. To improve the accuracy on person and place names, we added the language resources previously described in section 3. The combination of these dictionaries is indicated by “+Names” in Table 2, and a model trained adding these resources is indicated with + DICT. Finally, to"
I11-1108,P11-2008,0,0.0172011,"Missing"
I11-1108,D11-1136,0,0.0625604,"Missing"
I17-1016,W16-2206,0,0.0245607,"er- translation under-translation Reduce over-translation unrelated translation No difference 2 11 10 4 73 Table 10: Human evaluation results. Reranking PBMT Outputs with NMT We also did experiments that use the NMT score as an additional feature to rerank PBMT outputs (unique 1, 000-best list). The results are shown 159 reason why NMT is more likely to produce completely unrelated translations, over-translation or under-translation compared to traditional SMT. To relieve NMT of these problems, there are methods that modify the NMT neural network structure (Tu et al., 2016; Meng et al., 2016; Alkhouli et al., 2016) while we rerank NMT outputs by exploiting knowledge from traditional SMT. There are also existing methods that rerank NMT outputs by using target-bidirectional NMT models (Liu et al., 2016; Sennrich et al., 2016a). Their reranking method aims to overcome the issue of unbalanced accuracy in NMT outputs while our reranking method aims to solve the inadequacy problem of NMT. in Table 11. We also copy results of baseline PBMT and NMT from Table 4 for direct comparison. As we can see, using NMT to rerank PBMT outputs achieved improvements over the baseline PBMT system. However, when the baseline N"
I17-1016,D16-1162,1,0.935856,"ct There are a number of methods that combine the two paradigms to address their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT outputs, but do not impose hard alignment constraints like traditional SMT systems and therefore cannot effectively solve all over-translation or under-translation problems. In this paper, we propose a method that exploits an existing phrase-based translation model to compute the phrase-based decoding cost for a given NMT translation.1 That is, we force a phrase-based translation s"
I17-1016,J93-2003,0,0.0533469,"er all source words and does not provides exact mutually-exclusive word or phrase level alignments. As a result, it is known that attentional NMT systems make mistakes in over- or undertranslation (Cohn et al., 2016; Mi et al., 2016). 3 3.1 Phrase-based SMT In phrase-based SMT (Koehn et al., 2003), a phrase-based translation rule r includes a source phrase, a target phrase and a translation score S (r). Phrase-based translation rules can be extracted from the word-aligned training set and then used to translate new sentences. Word alignments for the training set can be obtained by IBM models (Brown et al., 1993). Phrase-based decoding uses a list of translation rules to translate source phrases in the input sentence and generate target phrases from left to right. A basic concept in phrase-based decoding is hypotheses. As shown in Figure 1, the hypothesis H1 consists of two rules r1 and r2 . The score of a hypothesis S (H) can be calculated as the product of the scores of all applied rules.3 An existing hypothesis can be expanded into a new hypothesis by applying a new rule. As shown in Figure 1, H1 can be expanded into H2 , H3 and H4 . H2 cannot be further expanded, because it covers all source words"
I17-1016,W15-5003,1,0.831286,"Eiichro Sumita1 Graham Neubig3,2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Japan 3 Language Technologies Institute, Carnegie Mellon University, USA jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp gneubig@cs.cmu.edu, s-nakamura@is.naist.jp Abstract There are a number of methods that combine the two paradigms to address their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT out"
I17-1016,J03-1002,0,0.0122746,"#Words #Sents #Words #Sents #Words #Vocab #Sents #Words #Sents #Words SOURCE TARGET 1.90M 52.2M 49.7M 113K 376K 3,003 67.6K 63.0K 2,169 46.8K 44.0K 1.99M 54.4M 60.4M 114K 137K 3,003 71.1K 81.1K 1.5K 27.1K 29.8K 954K 40.4M 37.2M 504K 288K 2K 77.5K 75.4K 2K 58.1K 55.5K 3.14M 104M 118M 273K 150K 2K 66.5K 74.6K 2K 70.6K 78.5K Table 3: Data sets. are both 512. We used Byte-pair encoding (BPE) (Sennrich et al., 2016b) and set the vocabulary size to be 50K. We used the Adam algorithm for optimization. To obtain a phrase-based translation rule table for our forced decoding algorithm, we used GIZA++ (Och and Ney, 2003) and grow-diagfinal-and heuristic to obtain symmetric word alignments for the training set. Then we extracted the rule table using Moses (Koehn et al., 2007). Experiments Settings We evaluated the proposed approach for Englishto-Chinese (en-zh), English-to-Japanese (en-ja), English-to-German (en-de) and English-to-French (en-fr) translation tasks. For the en-zh and enja tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 For the en-de and en-fr tasks, we used version 7 of the Europarl corpus as training data, WMT 2014 test sets as our devel"
I17-1016,W16-2323,0,0.404803,"opose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP rule table. To solve this problem, we propose"
I17-1016,P16-1162,0,0.763028,"opose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP rule table. To solve this problem, we propose"
I17-1016,W04-3250,0,0.0605453,"anese. We built attentional NMT systems with Lamtram7 . Word embedding size and hidden layer size 5.2 Results and Analysis Table 4 shows results of the phrase-based SMT system8 , the baseline NMT system, the lexicon integration method (Arthur et al., 2016) and the proposed reranking method. We tested three features for reranking: the NMT score Pn , the forced decoding score Sd and a word penalty (WP) feature, which is the length of the translation. The best NMT system and the systems that have no significant difference from the best NMT system at the p &lt; 0.05 level using bootstrap resampling (Koehn, 2004) are shown in bold font. As we can see, integrating lexical translation probabilities improved the baseline NMT system 5 Note that NTCIR-9 only contained a Chinese-to-English translation task, we used English as the source language in our experiments. In NTCIR-9, the development and test sets were both provided for the zh-en task while only the test set was provided for the en-ja task. We used the sentences from the NTCIR-8 en-ja and ja-en test sets as the development set in our experiments. 6 http://sourceforge.net/projects/mecab/files/ 7 https://github.com/neubig/lamtram 8 We used the defaul"
I17-1016,P16-1159,0,0.0418421,"Missing"
I17-1016,P07-2045,0,0.0120866,"4.4M 60.4M 114K 137K 3,003 71.1K 81.1K 1.5K 27.1K 29.8K 954K 40.4M 37.2M 504K 288K 2K 77.5K 75.4K 2K 58.1K 55.5K 3.14M 104M 118M 273K 150K 2K 66.5K 74.6K 2K 70.6K 78.5K Table 3: Data sets. are both 512. We used Byte-pair encoding (BPE) (Sennrich et al., 2016b) and set the vocabulary size to be 50K. We used the Adam algorithm for optimization. To obtain a phrase-based translation rule table for our forced decoding algorithm, we used GIZA++ (Och and Ney, 2003) and grow-diagfinal-and heuristic to obtain symmetric word alignments for the training set. Then we extracted the rule table using Moses (Koehn et al., 2007). Experiments Settings We evaluated the proposed approach for Englishto-Chinese (en-zh), English-to-Japanese (en-ja), English-to-German (en-de) and English-to-French (en-fr) translation tasks. For the en-zh and enja tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 For the en-de and en-fr tasks, we used version 7 of the Europarl corpus as training data, WMT 2014 test sets as our development sets and WMT 2015 test sets as our test sets. The detailed statistics for training, development and test sets are given in Table 3. The word segmentat"
I17-1016,E17-2058,0,0.093465,"Missing"
I17-1016,W17-3204,0,0.0413309,"nguage pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP rule table. To solve this problem, we propose a soft forced decoding algorithm, which is based on the standard phrase-based decoding algorithm and integrates new types of translation rules (deleting a source word or inserting a target word). The proposed forced decoding algorithm can always su"
I17-1016,P16-2049,0,0.0730172,"am Neubig3,2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Japan 3 Language Technologies Institute, Carnegie Mellon University, USA jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp gneubig@cs.cmu.edu, s-nakamura@is.naist.jp Abstract There are a number of methods that combine the two paradigms to address their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT outputs, but do not impose h"
I17-1016,N03-1017,0,0.218879,"phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP"
I17-1016,N16-1046,1,0.898579,"Missing"
I17-1016,P16-1008,0,0.0263944,"nder translation. both under- and over- translation under-translation Reduce over-translation unrelated translation No difference 2 11 10 4 73 Table 10: Human evaluation results. Reranking PBMT Outputs with NMT We also did experiments that use the NMT score as an additional feature to rerank PBMT outputs (unique 1, 000-best list). The results are shown 159 reason why NMT is more likely to produce completely unrelated translations, over-translation or under-translation compared to traditional SMT. To relieve NMT of these problems, there are methods that modify the NMT neural network structure (Tu et al., 2016; Meng et al., 2016; Alkhouli et al., 2016) while we rerank NMT outputs by exploiting knowledge from traditional SMT. There are also existing methods that rerank NMT outputs by using target-bidirectional NMT models (Liu et al., 2016; Sennrich et al., 2016a). Their reranking method aims to overcome the issue of unbalanced accuracy in NMT outputs while our reranking method aims to solve the inadequacy problem of NMT. in Table 11. We also copy results of baseline PBMT and NMT from Table 4 for direct comparison. As we can see, using NMT to rerank PBMT outputs achieved improvements over the baselin"
I17-1016,C16-1205,0,0.0455265,"both under- and over- translation under-translation Reduce over-translation unrelated translation No difference 2 11 10 4 73 Table 10: Human evaluation results. Reranking PBMT Outputs with NMT We also did experiments that use the NMT score as an additional feature to rerank PBMT outputs (unique 1, 000-best list). The results are shown 159 reason why NMT is more likely to produce completely unrelated translations, over-translation or under-translation compared to traditional SMT. To relieve NMT of these problems, there are methods that modify the NMT neural network structure (Tu et al., 2016; Meng et al., 2016; Alkhouli et al., 2016) while we rerank NMT outputs by exploiting knowledge from traditional SMT. There are also existing methods that rerank NMT outputs by using target-bidirectional NMT models (Liu et al., 2016; Sennrich et al., 2016a). Their reranking method aims to overcome the issue of unbalanced accuracy in NMT outputs while our reranking method aims to solve the inadequacy problem of NMT. in Table 11. We also copy results of baseline PBMT and NMT from Table 4 for direct comparison. As we can see, using NMT to rerank PBMT outputs achieved improvements over the baseline PBMT system. Howe"
I17-1016,W12-3158,0,0.0325086,"Missing"
I17-1016,D16-1096,0,0.0382651,"ling method to obtain a more diverse n-best list. We test the proposed method on English-toChinese, English-to-Japanese, English-to-German and English-to-French translation tasks, obtaining large improvements over a strong NMT baseline that already incorporates discrete lexicon features. 2 As we can see, NMT only learns an attention (alignment) distribution for each target word over all source words and does not provides exact mutually-exclusive word or phrase level alignments. As a result, it is known that attentional NMT systems make mistakes in over- or undertranslation (Cohn et al., 2016; Mi et al., 2016). 3 3.1 Phrase-based SMT In phrase-based SMT (Koehn et al., 2003), a phrase-based translation rule r includes a source phrase, a target phrase and a translation score S (r). Phrase-based translation rules can be extracted from the word-aligned training set and then used to translate new sentences. Word alignments for the training set can be obtained by IBM models (Brown et al., 1993). Phrase-based decoding uses a list of translation rules to translate source phrases in the input sentence and generate target phrases from left to right. A basic concept in phrase-based decoding is hypotheses. As"
I17-1016,P10-1049,0,0.157145,"to generate new hypotheses with phrase-based SMT, but instead use the phrase-based model to calculate scores for NMT output. In order to do so, we can perform forced decoding, which is very similar to the algorithm in the previous section but discards all partial hypotheses that do not match the NMT output. However, the NMT output is not limited by the phrase-based rule table, so there may be no decoding path that completely matches the NMT output when using only the phrase-based rules. To remedy this problem, inspired by previous work in forced decoding for training phrase-based SMT systems (Wuebker et al., 2010, 2012) we propose a soft forced decoding algorithm that can always successfully find a decoding path for a source sentence F and an NMT translation E. First, we introduce two new types of rules R1 and R2 . s (null → e) = unalign (e) |T | (6) where unalign (e) is how many times e is unaligned in T . One motivation for Equations 5 and 6 is that function words usually have high frequencies, but do not have as clear a correspondence with a word in the other language as content words. As a result, in the training set function words are more often unaligned than content words. As an example, Table"
I17-1016,D13-1112,0,0.0602895,"Missing"
I17-1016,P17-1139,0,0.0346417,"s their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT outputs, but do not impose hard alignment constraints like traditional SMT systems and therefore cannot effectively solve all over-translation or under-translation problems. In this paper, we propose a method that exploits an existing phrase-based translation model to compute the phrase-based decoding cost for a given NMT translation.1 That is, we force a phrase-based translation system to take in the source sentence and generate an NMT translation. The"
I17-1016,W06-0127,0,0.0264745,"evaluated the proposed approach for Englishto-Chinese (en-zh), English-to-Japanese (en-ja), English-to-German (en-de) and English-to-French (en-fr) translation tasks. For the en-zh and enja tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 For the en-de and en-fr tasks, we used version 7 of the Europarl corpus as training data, WMT 2014 test sets as our development sets and WMT 2015 test sets as our test sets. The detailed statistics for training, development and test sets are given in Table 3. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. We built attentional NMT systems with Lamtram7 . Word embedding size and hidden layer size 5.2 Results and Analysis Table 4 shows results of the phrase-based SMT system8 , the baseline NMT system, the lexicon integration method (Arthur et al., 2016) and the proposed reranking method. We tested three features for reranking: the NMT score Pn , the forced decoding score Sd and a word penalty (WP) feature, which is the length of the translation. The best NMT system and the systems that have no significant difference from the best NMT system at the p &lt; 0.05 lev"
J16-1001,W05-0909,0,0.00337933,"l accuracy as the average of the sentence-level accuracies. All optimization algorithms that are applicable to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this distinction important from the optimization point of view. The most commonly used MT evaluation measure BLEU (Papineni et al. 2002) is defined on the corpus level, and we will cover it in detail as it plays an important role in some of the methods that follow. Of course, there have been many other evaluation measures proposed since BLEU, with TER (Snover et al. 2006) and METEOR (Banerjee and Lavie 2005) being among the most widely used. The great majority of metrics other than BLEU are defined on the sentence level, and thus are conducive to optimization algorithms that require sentence-level evaluation measures. We discuss the role of evaluation in MT optimization more completely in Section 8.3. 2.5.1 BLEU. BLEU is defined as the geometric mean of n-gram precisions (usually for n from 1 to 4), and a brevity penalty to prevent short sentences from receiving unfairly high evaluation scores. For a single reference sentence e and a corresponding system output eˆ , we can define cn (ˆe ) as the"
J16-1001,N12-1062,0,0.0483162,"Missing"
J16-1001,J96-1002,0,0.0773772,"Missing"
J16-1001,P08-1024,0,0.0799565,"Missing"
J16-1001,D08-1023,0,0.393701,"ords e, and fires a feature for each pair. It is also possible to condition lexical features on the surrounding context in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features between every pair of words in the source or target sentences (Watanabe et al. 2007), or integrate bigrams on the target side (Watanabe et al. 2007). Of these, the former two can be calculated from source and local target context, but target bigrams require target bigram context and are thus non-local features. One final variety of features that has proven useful is syntax-based features (Blunsom and Osborne 2008; Marton and Resnik 2008). In particular, phrase-based and hierarchical phrase-based translations do not directly consider syntax (in the linguistic sense) in the construction of the models, so introducing this information in the form of features has a potential for benefit. One way to introduce this information is to parse the input sentence before translation, and use the information in the parse tree in the calculation of features. For example, we can count the number of times a phrase or translation rule matches, or partially matches (Marton and Resnik 2008), a span with a particular label"
J16-1001,J93-2003,0,0.0683852,"Missing"
J16-1001,W11-2103,0,0.0597144,"Missing"
J16-1001,W08-0304,0,0.0693471,"Missing"
J16-1001,N10-1080,0,0.0554388,"Missing"
J16-1001,D10-1059,0,0.0220325,"ce (Macherey et al. 2008) or hypergraph (Kumar et al. 2009). It is possible to perform MERT over these sorts of packed data structures by observing the fact that the envelopes used in MERT can be expressed as a semiring (Dyer 2010a; Sokolov and Yvon 2011), allowing for exact calculation of the full envelope for all hypotheses in a lattice or hypergraph using polynomial-time dynamic programming (the forward algorithm or inside algorithm, respectively). There has also been work to improve the accuracy of the k-best approximation by either sampling k-best candidates from the translation lattice (Chatterjee and Cancedda 2010), or performing forced decoding to find derivations that achieve the reference translation, and adding them to the k-best list (Liang, Zhang, and Zhao 2012). The second weakness of MERT is that it has no concept of regularization, causing it to overfit the training data if there are too many features, and there have been several attempts to incorporate regularization to ameliorate this problem. Cer, Jurafsky, and Manning (2008) propose a method to incorporate regularization by not choosing the plateau in the loss curve that minimizes the loss itself, but choosing the point considering the loss"
J16-1001,N12-1047,0,0.0128094,"bed in Section 3.4, possibly with the addition of a regularizer, is also a relatively standard problem in the machine learning literature. Methods to solve Equation (25) include sequential minimization optimization (Platt 1999), dual coordinate descent (Hsieh et al. 2008), as well as the quadratic program solvers used in standard SVMs (Joachims 1998). It should also be noted that there have also been several attempts to apply marginbased online learning algorithms explained in Section 6.3, but in a batch setting where the whole training corpus is decoded before each iteration of optimization (Cherry and Foster 2012; Gimpel and Smith 2012). We will explain these methods in more detail later, but it should be noted that the advantage of using these methods in a batch 27 Computational Linguistics Volume 42, Number 1 setting mainly lies in simplicity; for online learning it is often necessary to directly implement the optimization procedure within the decoder, whereas in a batch setting the implementation of the decoding and optimization algorithm can be performed separately. 5.4 Ranking and Linear Regression Optimization The rank-based loss described in Section 3.5 is essentially the combination of multipl"
J16-1001,J07-2003,0,0.0707349,"he sentences, and E ( f ) as the collection of all possible target language sentences that can be obtained by translating f . Machine translation systems perform this translation process by dividing the translation of a full sentence into the translation and recombination of smaller parts, which are represented as hidden variables, which together form a derivation. For example, in phrase-based translation (Koehn, Och, and Marcu 2003), the hidden variables will be the alignment between the phrases of the source and target sentences, and in tree-based translation models (Yamada and Knight 2001; Chiang 2007), the hidden variables will represent the latent tree structure used to generate the translation. We will define D( f ) to be the space of possible derivations that can be acquired from source sentence f , and d ∈ D( f ) to be one of those derivations. Any particular derivation d will correspond to exactly one e ∈ E ( f ), although the opposite is not true (the derivation uniquely determines the translation, but there can be multiple derivations corresponding to a particular translation). We also define tuple he, di consisting of a target sentence and its corresponding derivation, and T ( f )"
J16-1001,N09-1025,0,0.0253903,"Missing"
J16-1001,D08-1024,0,0.0189905,"Missing"
J16-1001,W12-3159,0,0.0192693,"he partial hypothesis he∗(i) , d∗(i) i by the greatest margin (the point of “maximum violation”). Search-aware tuning (Liu and Huang 2014) is a method that is able to consider search errors using an arbitrary optimization method. It does so by defining an evaluation measure for not only full sentences, but also partial derivations that occur during the search process, and optimizes parameters for k-best lists of partial derivations. Finally, there has also been some work on optimizing features not of the model itself, but parameters of the search process, using the downhill simplex algorithm (Chung and Galley 2012). Using this method, it is possible to adjust the beam width, distortion penalty, or other parameters that actually affect the size and shape of the derivation space, as opposed to simply rescoring hypotheses within it. 9. Conclusion In this survey article, we have provided a review of the current state-of-the-art in machine translation optimization, covering batch optimization, online optimization, expansions to large scale data, and a number of other topics. While these optimization algorithms have already led to large improvements in machine translation accuracy, the task of MT optimization"
J16-1001,Q14-1031,0,0.0261986,"Missing"
J16-1001,P11-2031,0,0.0170828,"om restarts, the results will generally change over multiple training runs, with the changes often being quite significant. Some research has shown that this randomness can be stabilized somewhat by improving the ability of the line-search algorithm to find a globally good solution by choosing random seeds more intelligently (Moore and Quirk 2008; Foster and Kuhn 2009) or by searching in directions that consider multiple features at once, instead of using the simple coordinate ascent as described in Figure 4 (Cer, Jurafsky, and Manning 2008). Orthogonally to actual improvement of the results, Clark et al. (2011) suggest that because randomness is a fundamental feature of MERT and other optimization algorithms for MT, it is better experimental practice to perform optimization multiple times, and report the resulting means and standard deviations over various optimization runs. It is also possible to optimize the MERT objective using other optimization algorithms. For example, Suzuki, Duh, and Nagata (2011) present a method for using particle swarm optimization, a distributed algorithm where many “particles” are each associated with a parameter vector, and the particle updates its vector in a way such"
J16-1001,2012.amta-papers.4,0,0.0361923,"Missing"
J16-1001,W02-1001,0,0.348853,"major advance in SMT is the discriminative training framework proposed by Och and Ney (2002) and Och (2003), who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus. By describing the scoring function for MT as a flexibly parameterizable loglinear model, and describing discriminative algorithms to optimize these parameters, it became possible to think of MT like many other structured prediction problems, such as POS tagging or parsing (Collins 2002). However, within the general framework of structured prediction, MT stands apart in many ways, and as a result requires a number of unique design decisions not necessary in other frameworks (as summarized in Table 1). The first is the search space that must be considered. The search space in MT is generally too large to expand exhaustively, so it is necessary to decide which subset of all the possible hypotheses should be used in optimization. In addition, the evaluation of MT accuracy is not straightforward, with automatic evaluation measures for MT still being researched to this day. From t"
J16-1001,P04-1015,0,0.012725,"eously in decoding (Sankaran, Sarkar, and Duh 2013). 8.4 Search and Optimization As mentioned in Section 2.4, because MT decoders perform approximate search, they may make search errors and not find the hypothesis that achieves the highest model score. There have been a few attempts to consider this fact in the optimization process. For example, in the perceptron algorithm of Section 6.2 it is known that the convergence guarantees of the structured perceptron no longer hold when using approximate search. The first method that can be used to resolve this problem is the early updating strategy (Collins and Roark 2004; Cowan, Ku˘cerov´a, and Collins 2006). The early updating strategy is a variety of bold updates, where the decoder output e∗(i) must be exactly equal to the reference e(i) . Decoding proceeds as normal, but the moment the correct hypothesis e(i) can no longer be produced by any hypothesis in the search space (i.e., a search error has occurred), search is stopped and update is performed using only the partial derivation. The second method is the max-violation perceptron (Huang, Fayong, and Guo 2012; Yu et al. 2013). In the max-violation perceptron, forced decoding is performed to acquire a der"
J16-1001,W06-1628,0,0.0446193,"Missing"
J16-1001,D13-1107,0,0.0514763,"Missing"
J16-1001,P09-1064,0,0.0637746,"Missing"
J16-1001,E14-1042,0,0.0183584,"Missing"
J16-1001,N15-1106,0,0.0450051,"Missing"
J16-1001,N12-1017,0,0.0139369,"cally translated using a ma N chine translation system to acquire MT results Eˆ = eˆ (i) i=1 , which are then compared to the corresponding references. The closer the MT output is to the reference, the better it is deemed to be, according to automatic evaluation. In addition, as there are often 9 Computational Linguistics Volume 42, Number 1 many ways to translate a particular sentence, it is also possible to perform evaluation with multiple references created by different translators. There has also been some work on encoding a huge number of references in a lattice, created either by hand (Dreyer and Marcu 2012) or by automatic paraphrasing (Zhou, Lin, and Hovy 2006). One major distinction between optimization measures is whether they are calculated on the corpus level or the sentence level. Corpus-level measures are calculated by taking statistics over the whole corpus, whereas sentence-level measures are calculated by measuring sentence-level accuracy, and defining the corpus-level accuracy as the average of the sentence-level accuracies. All optimization algorithms that are applicable to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this dist"
J16-1001,D09-1114,0,0.0268945,"ing, and Gaussian kernels (Nguyen, Mahajan, and He 2007), or the n-spectrum string kernel for finding associations between the source and target strings (Wang, Shawe-Taylor, and Szedmak 2007). Neural networks are another popular method for modeling nonlinearities, and it has been shown that neural networks can effectively be used to calculate new local features for MT (Liu et al. 2013). Methods such as boosting or stacking, which combine together multiple parameterizations of the translation model, have been incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta 2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding (Razmara and Sarkar 2013). Regression decision trees have also been introduced as a method for inducing nonlinear functions, incorporated through history-based search algorithms (Turian, Wellington, and Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with data from the target"
J16-1001,P08-2010,0,0.132413,"and overcoming this problem is the main obstacle to applying nonlinear models to MT (or structured learning in general). A number of countermeasures to this problem exist: Reranking: The most simple and commonly used method for incorporating nonlinearity, or other highly nonlocal features that cannot be easily incorporated in search, is through the use of reranking (Shen, Sarkar, and Och 2004). In this case, a system optimized using a standard linear model is used to create a k-best list of outputs, and this k-best list is then reranked using the nonlinear model (Nguyen, Mahajan, and He 2007; Duh and Kirchhoff 2008). Because we are now only dealing with fully expanded hypotheses, scoring becomes trivial, but reranking also has the major downsides of potentially missing useful hypotheses not included in the k-best list,9 and requiring time directly proportional to the size of the k-best list. Local Nonlinearity: Another possibility is to first use a nonlinear function to calculate local features, which are then used as part of the standard linear model (Liu et al. 2013). Alternatively, it is possible to treat feature-value pairs as new binary features (Clark, Dyer, and Lavie 2014). In this case, all effec"
J16-1001,P12-1001,0,0.0457195,"Missing"
J16-1001,N10-1033,0,0.032939,"Missing"
J16-1001,W09-0426,0,0.343238,"reasonable. 2.3.3 Summary features. Although sparse features are useful, training of sparse features is an extremely difficult optimization problem, and at this point there is still no method that has been widely demonstrated as being able to robustly estimate the parameters of millions of features. Because of this, a third approach of first training the parameters of sparse features, then condensing the sparse features into dense features and performing one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ"
J16-1001,W12-3160,0,0.0168486,"T1 Tt=+21 w(t) end procedure Figure 11 Online learning with synchronous update. 38 . Mix parameters Neubig and Watanabe Optimization for Statistical Machine Translation PS where s=1 µs = 1. As µs , it is possible to use a uniform distribution, or a weight proportional to the number of online updates performed at each shard (McDonald, Hall, and Mann 2010; Simianer, Riezler, and Dyer 2012). It should be noted that this algorithm can be considered a variety of the MapReduce framework, allowing for relatively straightforward implementation using parallel processing infrastructure such as Hadoop (Eidelman 2012). Simianer, Riezler, and Dyer (2012) propose another method for mixing parameters that, instead of averaging at each iteration, chooses to preserve only the parameters that have been learned over all shards, and sets all the remaining parameters to zero, allowing for a simple sort of feature selection. In particular, we define a S × M matrix h i&gt; ¯ (t+1) = w ¯ 1(t+1) |. . . |w ¯ S(t+1) , ¯ s(t+1) at each shard as w that combines the parameters w takes the L2 norm of each matrix column, and averages the columns with high norm values while setting the rest to zero. 7.1.2 Asynchronous Update. Whi"
J16-1001,P13-1110,0,0.03306,"Missing"
J16-1001,P02-1001,0,0.0859128,"(22) as pi,k = PK exp(si,k ) k0 =1 exp(si,k0 ) (53) Next, we define the expectation of the n-gram (gn ∈ ek ) frequency as cn,i,k , the expectation of the number of n-gram matches as mn,i,k , and the expectation of the reference length as ri,k . These values can be calculated as: cn,i,k = |{gn ∈ ek } |· pi,k (i) mn,i,k = |{gn ∈ ek } ∩ {gn ∈ e(i) } |· pi,k ri,k = |e(i) |· pi,k It should be noted that although these equations apply to k-best lists, it is also possible to calculate statistics over lattices or forests using dynamic programming algorithms and tools such as the expectation semiring (Eisner 2002; Li and Eisner 2009). 30 Neubig and Watanabe Optimization for Statistical Machine Translation xBLEU is calculated from these expected sufficient statistics: xBLEU = 4 Y n=1 P N PK i=1 k=1 mn,i,k P N PK i=1 k=1 cn,i,k ! 14 PN PK ·φ 1− i=1 k=1 ri,k PN P K i=1 k=1 c1,i,k ! (54) where φ(x) is the brevity penalty. Compared to the risk minimization in Equation (24), we define our optimization problem as the maximization of xBLEU: `xBLEU (F, E, C; γ, w) = − xBLEU (55) It is possible to calculate a gradient for xBLEU, allowing for optimization using gradient-based optimization methods, and we explain"
J16-1001,N13-1025,0,0.032251,"Missing"
J16-1001,W09-0439,0,0.0251881,"a number of extensions to the MERT framework have been proposed to resolve these problems. The first weakness of MERT is the randomness in the optimization process. Because each iteration of the training algorithm generally involves a number of random restarts, the results will generally change over multiple training runs, with the changes often being quite significant. Some research has shown that this randomness can be stabilized somewhat by improving the ability of the line-search algorithm to find a globally good solution by choosing random seeds more intelligently (Moore and Quirk 2008; Foster and Kuhn 2009) or by searching in directions that consider multiple features at once, instead of using the simple coordinate ascent as described in Figure 4 (Cer, Jurafsky, and Manning 2008). Orthogonally to actual improvement of the results, Clark et al. (2011) suggest that because randomness is a fundamental feature of MERT and other optimization algorithms for MT, it is better experimental practice to perform optimization multiple times, and report the resulting means and standard deviations over various optimization runs. It is also possible to optimize the MERT objective using other optimization algori"
J16-1001,D08-1089,0,0.0144523,"ommon to add a word penalty feature that measures the length of translation e to compensate for this. Similarly, phrase penalty or rule penalty features express the trade-off between longer or shorter derivations. There exist other features that are dependent on the underlying MT system model. Phrase-based MT heavily relies on the distortion probabilities that are computed by the distance on the source side of target-adjacent phrase pairs. More refined lexicalized reordering models estimate the parameters from the training data based on the relative distance of two phrase pairs (Tillman 2004; Galley and Manning 2008). 2.3.2 Sparse features. Although dense features form the foundation of most SMT systems, in recent years the ability to define richer feature sets and directly optimize the system using rich features has been shown to allow for significant increases in accuracy. On the other hand, large and sparse feature sets make the MT optimization problem significantly harder, and many of the optimization methods we will cover in the rest of this survey are aimed at optimizing rich feature sets. The first variety of sparse features that we can think of are phrase features or rule features, which count the"
J16-1001,D11-1004,0,0.0365469,"Missing"
J16-1001,D13-1201,0,0.0171802,"on to ameliorate this problem. Cer, Jurafsky, and Manning (2008) propose a method to incorporate regularization by not choosing the plateau in the loss curve that minimizes the loss itself, but choosing the point considering the loss values for a few surrounding plateaus, helping to avoid points that have a low loss but are surrounded by plateaus with higher loss. It is also possible to incorporate regularization into MERT-style line search using an SVM-inspired marginbased objective (Hayashi et al. 2009) or by using scale-invariant regularization methods such as L0 or a scaled version of L2 (Galley et al. 2013). 26 Neubig and Watanabe Optimization for Statistical Machine Translation The final weakness of MERT is that it has computational problems when scaling to large numbers of features. When using only a standard set of 20 or so features, MERT is able to perform training in reasonable time, but the number of line searches, and thus time, required in Algorithm 4 scales linearly with the number of features. Thus training of hundreds of features is time-consuming, and there are no published results training standard MERT on thousands or millions of features. It should be noted, however, that Galley e"
J16-1001,N13-1048,0,0.0136588,", which is generally initialized to a value η(1) and gradually reduced according to a function update(· ) as learning progresses. One standard method for updating η(t) according to the following formula η(t+1) ← η(1) 1 + t/T (70) allows for a guarantee of convergence (Collins et al. 2008). In Equation (69), the parameters are updated and we obtain w(t+1) . Within this framework, in the perceptron algorithm η(t) is set to a fixed value, and in MIRA the amount of update changes for every mini-batch. SGD-style online gradient-based methods have been used in translation for optimizing risk-based (Gao and He 2013), ranking-based (Watanabe 2012; Green et al. 2013), and other (Tillmann and Zhang 2006) objectives. When the regularization term Ω(w) is not differentiable, such as L1 regularization, it is a common practice to use forward-backward splitting (FOBOS) (Duchi and Singer 2009; Green et al. 2013) in which the optimization is performed in two steps: 1 w(t+ 2 ) ← w(t) − η(t+1) ∆`(F˜ (t) , E˜ (t) , C˜ (t) ; w(t) ) (71) w(t+1) ← arg min 1 kw − w(t+ 2 ) k22 + η(t+1) λΩ(w) 2 w (72) 1 36 Neubig and Watanabe Optimization for Statistical Machine Translation First, we perform updates without considering the"
J16-1001,D11-1083,0,0.01934,"lume 42, Number 1 each feature function can be decomposed over each step, and Equation (1) can be expressed by heˆ , dˆ i = arg max he,di∈T ( f ) j−1 |w| X i wi |d| X j−1 hi (dj , ρi (d1 )) (9) j=1 where hi (dj , ρi (d1 )) is a feature function for the jth step decomposed from the global feature function of hi ( f , e, d). As mentioned in the previous section, non-local features require information that cannot be calculated directly from the rule itself, and j−1 ρi (d1 ) is a variable that defines the residual information to score this ith feature funcj−1 tion using the partial derivation d1 (Gesmundo and Henderson 2011; Green, Cer, and Manning 2014). For example, in phrase-based translation, for an n-gram language j −1 model feature, ρi (d1 ) will be the n − 1 word suffix of the partial translation (Koehn, Och, and Marcu 2003). The local feature functions, such as phrase translation probabilij −1 ties in Section 2.3.1, require no context from partial derivations, and thus ρi (d1 ) = ∅. The problem of decoding is treated as a search problem in which partial derivations ˙ in Equation (9) are enumerated to form hypotheses or states. In d˙ together with ρi (d) phrase-based MT, search is carried out by enumerati"
J16-1001,D13-1111,0,0.0246035,"Missing"
J16-1001,D09-1023,0,0.0216448,"ontributes to the denominator. Thus, intuitively, the softmax objective prefers parameter settings that assign high scores to the oracle translations, and lower scores to any other members of c(i) that are not oracles. It should be noted that this loss can be calculated from a k-best list by iterating over the entire list and calculating the numerators and denominators in Equation (19). It is also possible, but more involved, to calculate over lattices or forests by using dynamic programming algorithms such as the forward–backward or inside–outside algorithms (Blunsom, Cohn, and Osborne 2008; Gimpel and Smith 2009). 3.3 Risk-Based Loss In contrast to softmax loss, which can be viewed as a probabilistic version of zero–one loss, risk defines a probabilistic version of the translation error (Smith and Eisner 2006; Zens, Hasan, and Ney 2007; Li and Eisner 2009; He and Deng 2012). Specifically, risk is based on the expected error incurred by a probabilistic model parameterized by w. This combines the advantages of the probabilistic model in softmax loss with the direct consideration of translation accuracy afforded by using error directly. In comparison to error, it also has the advantage of being different"
J16-1001,N12-1023,0,0.0463339,"e˙ (i) , e¯ (i) ) − w&gt; ∆h( f (i) , e˙ (i) , d˙ , e¯ (i) , d¯ ) (37) which is the largest margin in the k-best list. Explaining more intuitively, this criterion provides a bias towards selecting hypotheses with high error, making the learning algorithm work harder to correctly classify very bad hypotheses than it does for hypotheses that are only slightly worse than the oracle. Inference methods that consider the loss as in Equations (35) and (36) are called loss-augmented inference (Taskar et al. 2005) methods, and can minimize losses with respect to the candidate with the largest violation. Gimpel and Smith (2012) take this a step further, defining a structured ramp loss that additionally considers Equations (28) and (29) within this framework. 5. Batch Methods Now that we have explained the details of calculating loss functions used in machine translation, we turn to the actual algorithms used in optimizing using these loss functions. In this section, we cover batch learning approaches to MT optimization. Batch learning works by considering the entire training data on every update of the parameters, in contrast to online learning (covered in the following section), which considers only part of the dat"
J16-1001,E14-1047,0,0.0144677,"ing large-scale optimization, nonlinear models, domain-dependent optimization, and the effect of MT evaluation measures or search on optimization. Finally, we discuss the current state of affairs in MT optimization, and point out some unresolved problems that will likely be the target of further research in optimization for MT. 1. Introduction Machine translation (MT) has long been both one of the most promising applications of natural language processing technology and one of the most elusive. However, over approximately the past decade, huge gains in translation accuracy have been achieved (Graham et al. 2014), and commercial systems deployed for hundreds of language pairs are being used by hundreds of millions of users. There are many reasons for these advances in the accuracy and coverage of MT, but among them two particularly stand out: statistical machine translation (SMT) techniques that make it possible to learn statistical models from data, and massive increases in the amount of data available to learn SMT models. ∗ 8916-5 Takayama-cho, Ikoma, Nara, Japan. E-mail: neubig@is.naist.jp. ∗∗ 6-10-1 Roppongi, Minato-ku, Tokyo, Japan. E-mail: tarow@google.com. This work was mostly done while the se"
J16-1001,W14-3360,0,0.0168226,"ecomes an individual choice, and thus the ranking loss is the sum of these individual losses. As the binary classifier, it is possible to use perceptron, hinge, or softmax losses between the correct and incorrect answers. It should be noted that standard ranking techniques make a hard decision between candidates with higher and lower error, which can cause problems when the ranking by error does not correlate well with the ranking measured by the model. The crossentropy ranking loss solves this problem by softly fitting the model distribution to the distribution of ranking measured by errors (Green et al. 2014). 3.6 Mean Squared Error Loss Finally, mean squared error loss is another method that does not make a hard zero– one decision between the better and worse candidates, but instead attempts to directly estimate the difference in scores (Bazrafshan, Chung, and Gildea 2012). This is done by first finding the difference in errors between the two candidates ∆ err(e(i) , e∗ , e) and defining the loss as the mean squared error of the difference between the inverse of the difference in the errors and the difference in the model scores5 : `mse (F, E, C; w) = 1 N (C) N X X X i=1 he∗ ,d∗ i∈o(i) he,di∈c(i)"
J16-1001,P13-1031,0,0.0142589,"and gradually reduced according to a function update(· ) as learning progresses. One standard method for updating η(t) according to the following formula η(t+1) ← η(1) 1 + t/T (70) allows for a guarantee of convergence (Collins et al. 2008). In Equation (69), the parameters are updated and we obtain w(t+1) . Within this framework, in the perceptron algorithm η(t) is set to a fixed value, and in MIRA the amount of update changes for every mini-batch. SGD-style online gradient-based methods have been used in translation for optimizing risk-based (Gao and He 2013), ranking-based (Watanabe 2012; Green et al. 2013), and other (Tillmann and Zhang 2006) objectives. When the regularization term Ω(w) is not differentiable, such as L1 regularization, it is a common practice to use forward-backward splitting (FOBOS) (Duchi and Singer 2009; Green et al. 2013) in which the optimization is performed in two steps: 1 w(t+ 2 ) ← w(t) − η(t+1) ∆`(F˜ (t) , E˜ (t) , C˜ (t) ; w(t) ) (71) w(t+1) ← arg min 1 kw − w(t+ 2 ) k22 + η(t+1) λΩ(w) 2 w (72) 1 36 Neubig and Watanabe Optimization for Statistical Machine Translation First, we perform updates without considering the regularization term in Equation (71). Second, the"
J16-1001,D14-1130,0,0.0147892,"ecomes an individual choice, and thus the ranking loss is the sum of these individual losses. As the binary classifier, it is possible to use perceptron, hinge, or softmax losses between the correct and incorrect answers. It should be noted that standard ranking techniques make a hard decision between candidates with higher and lower error, which can cause problems when the ranking by error does not correlate well with the ranking measured by the model. The crossentropy ranking loss solves this problem by softly fitting the model distribution to the distribution of ranking measured by errors (Green et al. 2014). 3.6 Mean Squared Error Loss Finally, mean squared error loss is another method that does not make a hard zero– one decision between the better and worse candidates, but instead attempts to directly estimate the difference in scores (Bazrafshan, Chung, and Gildea 2012). This is done by first finding the difference in errors between the two candidates ∆ err(e(i) , e∗ , e) and defining the loss as the mean squared error of the difference between the inverse of the difference in the errors and the difference in the model scores5 : `mse (F, E, C; w) = 1 N (C) N X X X i=1 he∗ ,d∗ i∈o(i) he,di∈c(i)"
J16-1001,N13-1035,0,0.017328,"meters, it is useful to make the distinction between in-domain translation (when the model training data matches the test domain) and cross-domain translation (when the model training data mismatches the test domain). In cross-domain translation, fewer long rules will be used, and translation probabilities will be less reliable, and the parameters must change accordingly to account for this (Pecina, Toral, and van Genabith 2012). It has also been shown that building TMs for several domains and tuning the parameters to maximize translation accuracy can improve MT accuracy on the target domain (Haddow 2013). Another option for making the distinction between in-domain and out-of-domain data is by firing different features for in-domain and out-of-domain training data, allowing for the learning of different weights for different domains (Clark, Lavie, and Dyer 2012). 8.3 Evaluation Measures and Optimization In the entirety of this article, we have assumed that optimization for MT aims to reduce MT error defined using an evaluation measure, generally BLEU. However, as mentioned in Section 2.5, evaluation of MT is an active research field, and there are many alternatives in addition to BLEU. Thus, i"
J16-1001,W11-2130,0,0.0496357,"Missing"
J16-1001,N07-2015,0,0.063938,"Missing"
J16-1001,2009.iwslt-papers.3,1,0.788497,"training data if there are too many features, and there have been several attempts to incorporate regularization to ameliorate this problem. Cer, Jurafsky, and Manning (2008) propose a method to incorporate regularization by not choosing the plateau in the loss curve that minimizes the loss itself, but choosing the point considering the loss values for a few surrounding plateaus, helping to avoid points that have a low loss but are surrounded by plateaus with higher loss. It is also possible to incorporate regularization into MERT-style line search using an SVM-inspired marginbased objective (Hayashi et al. 2009) or by using scale-invariant regularization methods such as L0 or a scaled version of L2 (Galley et al. 2013). 26 Neubig and Watanabe Optimization for Statistical Machine Translation The final weakness of MERT is that it has computational problems when scaling to large numbers of features. When using only a standard set of 20 or so features, MERT is able to perform training in reasonable time, but the number of line searches, and thus time, required in Algorithm 4 scales linearly with the number of features. Thus training of hundreds of features is time-consuming, and there are no published re"
J16-1001,P12-1031,0,0.0291857,"Summary features. Although sparse features are useful, training of sparse features is an extremely difficult optimization problem, and at this point there is still no method that has been widely demonstrated as being able to robustly estimate the parameters of millions of features. Because of this, a third approach of first training the parameters of sparse features, then condensing the sparse features into dense features and performing one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according t"
J16-1001,2009.mtsummit-posters.8,0,0.0230133,"-the-loop optimization is prohibitive, so Zaidan and Callison-Burch (2009) propose a method that re-uses partial hypotheses in evaluation. Saluja, Lane, and Zhang (2012) also propose a method for incorporating binary good/bad input into optimization, with the motivation that this sort of feedback is easier for human annotators to provide than generating new reference sentences. Finally, there is also some work on optimizing multiple evaluation metrics at one time. The easiest way to do so is to simply use the linear interpolation of two or more metrics as the error function (Dyer et al. 2009; He and Way 2009; Servan and Schwenk 2011): ˆ = error(E, E) L X ˆ ρi errori (E, E) (78) i=1 where L is the number of error functions, and ρi is a manually set interpolation coefficient for its respective error function. There are also more sophisticated methods based on the idea of optimizing towards Pareto-optimal hypotheses (Duh et al. 43 Computational Linguistics Volume 42, Number 1 2012), which achieve errors lower than all other hypotheses on at least one evaluation measure, ˆ &lt; errori (E, E0 )} pareto(E, E ) = {Eˆ ∈ E : ∀E0 ∈E ∃i errori (E, E) (79) To incorporate this concept of Pareto optimality into o"
J16-1001,D11-1125,0,0.0228166,"Missing"
J16-1001,P07-1019,0,0.0120787,"K+ algorithm (Chappelier and Rajman 1998) on the source side and generating partial derivations for progressively longer source spans. Because of the enormous ˙ in each partial derivation, beam search search space brought about by maintaining ρi (d) is used to heuristically prune the search space. As a result, the search is inexact because of the search error caused by heuristic pruning, in which the best scoring hypothesis is not necessarily optimal in terms of given model parameters. The search is efficiently carried out by merging equivalent states encoded as ρ (Koehn, Och, and Marcu 2003; Huang and Chiang 2007), and the space is succinctly represented by compact data structures, such as graphs (Ueffing, Och, and Ney 2002) (or lattices) in phrase-based MT (Koehn, Och, and Marcu 2003) and hypergraphs (Klein and Manning 2004) (or packed forests) in tree-based MT (Huang and Chiang 2007). These data structures may be directly used as compact representations of all derivations for optimization. However, using these data structures directly can be unwieldly, and thus it is more common to obtain a k-best list as an approximation of the derivation space. Figure 1(a) shows an example of k-best English transla"
J16-1001,N12-1015,0,0.0318911,"Missing"
J16-1001,2006.amta-papers.8,0,0.0149369,"Missing"
J16-1001,J10-4005,0,0.0445813,"tween MT evaluation and optimization. Finally, we conclude in Section 9, overviewing the methods described, making a brief note about which methods see the most use in actual systems, and outlining some of the unsolved problems in the optimization of MT systems. 2. Machine Translation Preliminaries and Definitions Before delving into the details of actual optimization algorithms, we first introduce preliminaries and definitions regarding MT in general and the MT optimization problem in particular. We focus mainly on the aspects of MT that are relevant to optimization, and readers may refer to Koehn (2010) or Lopez (2008) for more details about MT in general. 2.1 Machine Translation Machine translation is the problem of automatically translating from one natural language to another. Formally, we define this problem by specifying F to be the collection of all source sentences to be translated, f ∈ F as one of the sentences, and E ( f ) as the collection of all possible target language sentences that can be obtained by translating f . Machine translation systems perform this translation process by dividing the translation of a full sentence into the translation and recombination of smaller parts,"
J16-1001,P07-2045,0,0.0118186,"nd h(· ) are M-dimensional, and bm is an M-dimensional vector where the m-th element is 1 and the rest of the elements are zero. For the T iterations, we decide the dimension m of the feature vector (line 6), and for each possible weight vector w(j) + γbm choose the γ ∈ R that minimizes `error (· ) using line search (line 7). Then, among the γ for each of the M search dimensions, we perform an update using γˆ that affords the largest reduction in error (lines 9 and 10). This algorithm can be deemed a variety of steepest descent, which is a standard method used in most implementations of MERT (Koehn et al. 2007). Another alternative is a variant of coordinate descent (e.g., Powell’s method), in which search and update is performed in each dimension. One feature of MERT is that it is known to easily fall into local optima of the error function. Because of this, it is standard to choose R starting points (line 4), perform ˆ that optimization starting at each of these starting points, and finally choose the w minimizes the loss from the weights acquired from each of the R random restarts. The R starting points are generally chosen so that one of the points is the best w from the previous iteration, and"
J16-1001,N03-1017,0,0.0562814,"Missing"
J16-1001,W07-0733,0,0.0253917,"nd Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with data from the target domain it is possible to achieve gains in accuracy. In machine translation, this is also very true, although much of the work on domain adaptation has focused on adapting the model learning process prior to explicit optimization towards an evaluation measure (Koehn and Schroeder 2007). However, there are a few works on optimization-based domain adaptation in MT, as we will summarize subsequently. One relatively simple way of performing domain adaptation is by selecting a subset of the training data that is similar to the data that we want to translate (Li et al. 2010). This can be done by selecting sentences that are similar to our test corpus, or even selecting adaptation data for each individual test sentence (Liu et al. 2012). If no parallel data exist in the target domain, it has also been shown that first automatically translating data from the source to the target la"
J16-1001,P09-1019,0,0.0301324,"an one dimension, or all dimensions at a single time. However, as MERT remains a fundamentally computationally hard problem, this method takes large amounts of time for larger training sets or feature spaces. It should be noted that instability in MERT is not entirely due to the fact that search is random, but also due to the fact that k-best lists are poor approximations of the whole space of possible translations. One way to improve this approximation is by performing MERT over an exponentially large number of hypotheses encoded in a translation lattice (Macherey et al. 2008) or hypergraph (Kumar et al. 2009). It is possible to perform MERT over these sorts of packed data structures by observing the fact that the envelopes used in MERT can be expressed as a semiring (Dyer 2010a; Sokolov and Yvon 2011), allowing for exact calculation of the full envelope for all hypotheses in a lattice or hypergraph using polynomial-time dynamic programming (the forward algorithm or inside algorithm, respectively). There has also been work to improve the accuracy of the k-best approximation by either sampling k-best candidates from the translation lattice (Chatterjee and Cancedda 2010), or performing forced decodin"
J16-1001,2008.eamt-1.14,0,0.0566391,"uch as the Parzen window, binning, and Gaussian kernels (Nguyen, Mahajan, and He 2007), or the n-spectrum string kernel for finding associations between the source and target strings (Wang, Shawe-Taylor, and Szedmak 2007). Neural networks are another popular method for modeling nonlinearities, and it has been shown that neural networks can effectively be used to calculate new local features for MT (Liu et al. 2013). Methods such as boosting or stacking, which combine together multiple parameterizations of the translation model, have been incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta 2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding (Razmara and Sarkar 2013). Regression decision trees have also been introduced as a method for inducing nonlinear functions, incorporated through history-based search algorithms (Turian, Wellington, and Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with da"
J16-1001,D08-1088,0,0.0544982,"Missing"
J16-1001,C10-1075,0,0.0242919,"the parameters with data from the target domain it is possible to achieve gains in accuracy. In machine translation, this is also very true, although much of the work on domain adaptation has focused on adapting the model learning process prior to explicit optimization towards an evaluation measure (Koehn and Schroeder 2007). However, there are a few works on optimization-based domain adaptation in MT, as we will summarize subsequently. One relatively simple way of performing domain adaptation is by selecting a subset of the training data that is similar to the data that we want to translate (Li et al. 2010). This can be done by selecting sentences that are similar to our test corpus, or even selecting adaptation data for each individual test sentence (Liu et al. 2012). If no parallel data exist in the target domain, it has also been shown that first automatically translating data from the source to the target language or vice versa, then using this data for optimization and model training is also helpful (Ueffing, Haffari, and Sarkar 2007; Li et al. 2011; Zhao et al. 2011) In addition, in a computer-assisted translation scenario, it is possible to reflect post-edited translations back into the o"
J16-1001,D09-1005,0,0.220713,"s can be calculated from a k-best list by iterating over the entire list and calculating the numerators and denominators in Equation (19). It is also possible, but more involved, to calculate over lattices or forests by using dynamic programming algorithms such as the forward–backward or inside–outside algorithms (Blunsom, Cohn, and Osborne 2008; Gimpel and Smith 2009). 3.3 Risk-Based Loss In contrast to softmax loss, which can be viewed as a probabilistic version of zero–one loss, risk defines a probabilistic version of the translation error (Smith and Eisner 2006; Zens, Hasan, and Ney 2007; Li and Eisner 2009; He and Deng 2012). Specifically, risk is based on the expected error incurred by a probabilistic model parameterized by w. This combines the advantages of the probabilistic model in softmax loss with the direct consideration of translation accuracy afforded by using error directly. In comparison to error, it also has the advantage of being differentiable, allowing for easier optimization. To define this error, we define a scaling parameter γ ≥ 0 and use it in the calculation of each hypothesis’s probability pγ,w (e, d |f , c) = P exp(γw&gt; h( f , e, d)) &gt; 0 0 he0 ,d0 i∈c exp(γw h( f , e , d ))"
J16-1001,D11-1085,0,0.0167481,"imple way of performing domain adaptation is by selecting a subset of the training data that is similar to the data that we want to translate (Li et al. 2010). This can be done by selecting sentences that are similar to our test corpus, or even selecting adaptation data for each individual test sentence (Liu et al. 2012). If no parallel data exist in the target domain, it has also been shown that first automatically translating data from the source to the target language or vice versa, then using this data for optimization and model training is also helpful (Ueffing, Haffari, and Sarkar 2007; Li et al. 2011; Zhao et al. 2011) In addition, in a computer-assisted translation scenario, it is possible to reflect post-edited translations back into the optimization process as new indomain training data (Mathur, Mauro, and Federico 2013; Denkowski, Dyer, and Lavie 2014). Once adaptation data have been chosen, it is necessary to decide how to use the data. The most straightforward way is to simply use these in-domain data in optimization, but if the data set is small it is preferable to combine both in- and out-ofdomain data to achieve more robust parameter estimates. This is essentially equivalent to t"
J16-1001,P06-1096,0,0.01512,"Missing"
J16-1001,C04-1072,0,0.013434,"is defined as the longest reference with a length shorter than or equal to eˆ (i) . 2.5.2 BLEU+1. One thing to notice here is that BLEU is calculated by taking statistics over the entire corpus, and thus it is a corpus-level measure. There is nothing inherently preventing us from calculating BLEU on a single sentence, but in the single-sentence case it is common for the number of matches of higher order n-grams to become zero, resulting in a BLEU score of zero for the entire sentence. One common solution to this problem is the use of a smoothed version of BLEU, commonly referred to as BLEU+1 (Lin and Och 2004). In BLEU+1, we add one to the numerators and denominators of each n-gram of order greater than one c0n (ˆe ) = |{gn ∈ eˆ } |+ δ(n &gt; 1) m0n (e, eˆ ) = |{gn ∈ eˆ } ∩ {g0 n ∈ e} |+ δ(n &gt; 1) where δ(· ) is a function that takes a value of 1 when the corresponding statement is true. We can then re-define a sentence-level BLEU using these smoothed counts BLEU’(e, eˆ ) = 1 4  0 Y m ({e1 , . . . , eM }, eˆ ) 4 n n =1 c0n (ˆe ) · BP(e, eˆ ) (12) and the corpus-level evaluation can be re-defined as the average of sentence level evaluations ˆ = 1 BLEU’(E, E) N N X BLEU’(e(i) , eˆ (i) ) (13) i=1 It has"
J16-1001,D12-1037,1,0.931558,"izing other losses such as those based on probabilistic models (Section 5.2), error margins (Section 5.3), ranking (Section 5.4), and risk (Section 5.5). 5.1 Error Minimization 5.1.1 Minimum Error Rate Training Overview. Minimum error rate training (MERT) (Och 2003) is one of the first, and is currently the most widely used, method for MT optimization, and focuses mainly on direct minimization of the error described in Section 3.1. Because error is not continuously differentiable, MERT uses optimization methods that do not require the calculation of a gradient, such as iterative line search 7 Liu et al. (2012) propose a method to avoid over-aggressive moves in parameter space by considering the balance between increase in the evaluation score and the similarity with the parameters on the previous iteration. 21 Computational Linguistics 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: Volume 42, Number 1 procedure MERT(F, E, C) ˆ ←∅ w for r ∈ {1 . . . R} do w(1) ∼ RM . Initialize randomly for t ∈ {1 . . . T} do . Until convergence for m ∈ {1 . . . M} do . For each dimension γˆ m ← arg minγ `error (F, E, C; w(t) + γbm ) . Search end for . Descent γˆ ← arg minγˆ m `error (F, E, C; w(t) + γˆ"
J16-1001,D14-1209,0,0.0277259,"Missing"
J16-1001,P13-1078,1,0.733513,"g one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according to Equation (1). In general, the inference is intractable if we enumerate all possible derivations in T ( f ) and rank each derivation by the model. We assume that a derivation is composed of a set of steps d = d1 , d2 , · · · , d|d| (8) where each dj is a step—for example, a phrase pair in phrase-based MT or a synchronous rule in tree-based MT—ordered in a particular way. We also assume that 7 Computational Linguistics Volume 42, Number"
J16-1001,C12-2071,1,0.926133,"izing other losses such as those based on probabilistic models (Section 5.2), error margins (Section 5.3), ranking (Section 5.4), and risk (Section 5.5). 5.1 Error Minimization 5.1.1 Minimum Error Rate Training Overview. Minimum error rate training (MERT) (Och 2003) is one of the first, and is currently the most widely used, method for MT optimization, and focuses mainly on direct minimization of the error described in Section 3.1. Because error is not continuously differentiable, MERT uses optimization methods that do not require the calculation of a gradient, such as iterative line search 7 Liu et al. (2012) propose a method to avoid over-aggressive moves in parameter space by considering the balance between increase in the evaluation score and the similarity with the parameters on the previous iteration. 21 Computational Linguistics 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: Volume 42, Number 1 procedure MERT(F, E, C) ˆ ←∅ w for r ∈ {1 . . . R} do w(1) ∼ RM . Initialize randomly for t ∈ {1 . . . T} do . Until convergence for m ∈ {1 . . . M} do . For each dimension γˆ m ← arg minγ `error (F, E, C; w(t) + γbm ) . Search end for . Descent γˆ ← arg minγˆ m `error (F, E, C; w(t) + γˆ"
J16-1001,I13-1032,1,0.943906,"g one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according to Equation (1). In general, the inference is intractable if we enumerate all possible derivations in T ( f ) and rank each derivation by the model. We assume that a derivation is composed of a set of steps d = d1 , d2 , · · · , d|d| (8) where each dj is a step—for example, a phrase pair in phrase-based MT or a synchronous rule in tree-based MT—ordered in a particular way. We also assume that 7 Computational Linguistics Volume 42, Number"
J16-1001,P13-2067,0,0.0231633,"s on the effect of the metric used in optimization on human assessments of the generated translations (Cer, Manning, and Jurafsky 2010; Callison-Burch et al. 2011). These studies showed the rather surprising result that despite the fact that other evaluation measures had proven superior to BLEU with regards to post facto correlation with human evaluation, a BLEU-optimized system proved superior to systems tuned using other metrics. Since this result, however, there have been other reports stating that systems optimized using other metrics such as TESLA (Liu, Dahlmeier, and Ng 2011) and MEANT (Lo et al. 2013) achieve superior results to BLEU-optimized systems. There have also been attempts to directly optimize not automatic, but human evaluation measures of translation quality (Zaidan and Callison-Burch 2009). However, the cost of performing this sort of human-in-the-loop optimization is prohibitive, so Zaidan and Callison-Burch (2009) propose a method that re-uses partial hypotheses in evaluation. Saluja, Lane, and Zhang (2012) also propose a method for incorporating binary good/bad input into optimization, with the motivation that this sort of feedback is easier for human annotators to provide t"
J16-1001,D08-1076,0,0.0316358,"according to a particular γ (Figure 6a). After finding the envelope, for each line that participates in the envelope, we can calculate the sufficient statistics necessary for calculating the loss `error (· ) and error error(· ). For example, given the envelope in Figure 6a, Figure 6b is an example of the sentence-wise loss with respect to γ. The envelope shown in Equation (41) can also be viewed as the problem of finding a convex hull in computational geometry. A standard and efficient algorithm for finding a convex hull of multiple lines is the sweep line algorithm (Bentley and Ottmann 1979; Macherey et al. 2008) (see Figure 7). Here, we assume L is a set of the lines corresponding to the K translation candidates in c(i) , each line l ∈ L is expressed as ha(l), b(l), γ(l)i with intercept a(l) = a( f (i) , e, d), and slope b(l) = b( f (i) , e, d). Furthermore, we define γ(l) as an intersection initialized to −∞. S ORT L INES (L) in Figure 3 sorts the lines in the order of their slope b(l), and if two lines lk1 have the same slope, lk2 chooses the one with the larger intercept a(lk1 ) &gt; a(lk2 ) and deletes the other. We next process the sorted set of lines L0 (|L0 |≤ K) in order of ascending slope (line"
J16-1001,P08-1114,0,0.0208734,"e for each pair. It is also possible to condition lexical features on the surrounding context in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features between every pair of words in the source or target sentences (Watanabe et al. 2007), or integrate bigrams on the target side (Watanabe et al. 2007). Of these, the former two can be calculated from source and local target context, but target bigrams require target bigram context and are thus non-local features. One final variety of features that has proven useful is syntax-based features (Blunsom and Osborne 2008; Marton and Resnik 2008). In particular, phrase-based and hierarchical phrase-based translations do not directly consider syntax (in the linguistic sense) in the construction of the models, so introducing this information in the form of features has a potential for benefit. One way to introduce this information is to parse the input sentence before translation, and use the information in the parse tree in the calculation of features. For example, we can count the number of times a phrase or translation rule matches, or partially matches (Marton and Resnik 2008), a span with a particular label, based on the assumption"
J16-1001,W13-2237,0,0.017363,"Missing"
J16-1001,P05-1012,0,0.151284,"Missing"
J16-1001,N10-1069,0,0.0269632,"Missing"
J16-1001,C08-1074,0,0.208528,"optima of the error function. Because of this, it is standard to choose R starting points (line 4), perform ˆ that optimization starting at each of these starting points, and finally choose the w minimizes the loss from the weights acquired from each of the R random restarts. The R starting points are generally chosen so that one of the points is the best w from the previous iteration, and the remaining R − 1 have each element of w chosen randomly and uniformly from some interval, although it has also been shown that more intelligent choice of initial points can result in better final scores (Moore and Quirk 2008). 5.1.2 Line Search for MERT. Although the majority of this process is relatively straightforward, the line search in Line 7 of Figure 4 requires a bit more explanation. In this step, we would like to choose the γ that results in the ordering of hypotheses in c(i) that achieves the lowest error. In order to do so, MERT uses an algorithm that allows for 22 Neubig and Watanabe Optimization for Statistical Machine Translation exact enumeration of which of the K candidates in c(i) will be chosen for each value of γ. Concretely, we define  &gt; arg max w(j) + γbm h( f (i) , e, d) (38) he,di∈c(i) &gt; ="
J16-1001,C12-1121,0,0.0296408,"Missing"
J16-1001,P13-2003,0,0.0395626,"Missing"
J16-1001,W14-3316,0,0.0229636,"see that even after over ten years, MERT is still the dominant optimization algorithm. However, starting in WMT 2013, we can see a move to systems based on MIRA, and to a lesser extent ranking, particularly in the most competitive systems. In these systems, the preferred choice of an optimization algorithm seems to be MERT when using up to 20 features, and MIRA when using a large number of features (up to several hundred). There are fewer examples of systems using large numbers of features (tens of thousands, or millions) in actual competitive systems, with a few exceptions (Dyer et al. 2009; Neidert et al. 2014; Wuebker et al. 2014). In the case when a large number of sparse features are used, it is most common to use a softmax or risk-based objective and gradient-based optimization algorithms, often combining the features into summary features and performing a final tuning pass with MERT. The fact that algorithms other than MERT are seeing adoption in competitive systems for shared tasks is a welcome sign for the future of MT optimization research. However, there are still many open questions in the field, a few of which can be outlined here: Stable Training with Millions of Features: At the moment"
J16-1001,W07-0710,0,0.0679013,"Missing"
J16-1001,P03-1021,0,0.209352,"Linguistics Computational Linguistics Volume 42, Number 1 Within the SMT framework, there have been two revolutions in the way we mathematically model the translation process. The first was the pioneering work of Brown et al. (1993), who proposed the idea of SMT, and described methods for estimation of the parameters used in translation. In that work, the parameters of a word-based generative translation model were optimized to maximize the conditional likelihood of the training corpus. The second major advance in SMT is the discriminative training framework proposed by Och and Ney (2002) and Och (2003), who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus. By describing the scoring function for MT as a flexibly parameterizable loglinear model, and describing discriminative algorithms to optimize these parameters, it became possible to think of MT like many other structured prediction problems, such as POS tagging or parsing (Collins 2002). However, within the general framework of structured prediction, MT stands apart in many ways"
J16-1001,P02-1038,0,0.156287,"tion for Computational Linguistics Computational Linguistics Volume 42, Number 1 Within the SMT framework, there have been two revolutions in the way we mathematically model the translation process. The first was the pioneering work of Brown et al. (1993), who proposed the idea of SMT, and described methods for estimation of the parameters used in translation. In that work, the parameters of a word-based generative translation model were optimized to maximize the conditional likelihood of the training corpus. The second major advance in SMT is the discriminative training framework proposed by Och and Ney (2002) and Och (2003), who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus. By describing the scoring function for MT as a flexibly parameterizable loglinear model, and describing discriminative algorithms to optimize these parameters, it became possible to think of MT like many other structured prediction problems, such as POS tagging or parsing (Collins 2002). However, within the general framework of structured prediction, MT stands apa"
J16-1001,J03-1002,0,0.0233519,"t vector w from the set of possible weight vectors RM .1 Optimization is also widely called tuning in the SMT literature. In addition, because of the exponentially large number of possible translations in E ( f ) that must be considered, it is necessary to take advantage of the problem structure, making MT optimization an instance of structured learning. 2.2 Model Construction The first step of creating a machine translation system is model construction, in which translation models (TMs) are extracted from a large parallel corpus. The TM is usually created by first aligning the parallel text (Och and Ney 2003), using this text to extract multi-word phrase pairs or synchronous grammar rules (Koehn, Och, and Marcu 2003; Chiang 2007), and scoring these rules according to several features explained in more detail in Section 2.3. The construction of the TM is generally performed first in a manner that does not directly consider the optimization of translation accuracy, followed by an optimization step that explicitly considers the accuracy achieved by the system.2 In this survey, we focus on the optimization step, and thus do not cover elements of model construction that do not directly optimize an obje"
J16-1001,P02-1040,0,0.110893,"res is whether they are calculated on the corpus level or the sentence level. Corpus-level measures are calculated by taking statistics over the whole corpus, whereas sentence-level measures are calculated by measuring sentence-level accuracy, and defining the corpus-level accuracy as the average of the sentence-level accuracies. All optimization algorithms that are applicable to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this distinction important from the optimization point of view. The most commonly used MT evaluation measure BLEU (Papineni et al. 2002) is defined on the corpus level, and we will cover it in detail as it plays an important role in some of the methods that follow. Of course, there have been many other evaluation measures proposed since BLEU, with TER (Snover et al. 2006) and METEOR (Banerjee and Lavie 2005) being among the most widely used. The great majority of metrics other than BLEU are defined on the sentence level, and thus are conducive to optimization algorithms that require sentence-level evaluation measures. We discuss the role of evaluation in MT optimization more completely in Section 8.3. 2.5.1 BLEU. BLEU is defin"
J16-1001,D09-1147,0,0.0513919,"Missing"
J16-1001,C12-1135,0,0.057601,"Missing"
J16-1001,C12-2091,0,0.0158071,"chair Figure 1 Example of a k-best list, lattice, and forest. Another class of decoding problem is forced decoding, in which the output from a decoder is forced to match with a reference translation of the input sentence. In phrasebased MT, this is implemented by adding additional features to reward hypotheses that match with the given target sentence (Liang, Zhang, and Zhao 2012; Yu et al. 2013). In MT using synchronous grammars, it is carried out by biparsing over two languages, for instance, by a variant of the CYK algorithm (Wu 1997) or by a more efficient two-step algorithm (Dyer 2010b; Peitz et al. 2012). Even if we perform forced decoding, we are still not guaranteed that the decoder will be able to produce the reference translation (because of unknown words, reordering limits, or other factors). This problem can be resolved by preserving the prefix of partial derivations (Yu et al. 2013), or by allowing approximate matching of the target side (Liang, Zhang, and Zhao 2012). It is also possible to create a neighborhood of a forced decoding derivation by adding additional hyperedges to the true derivation, which allows for efficient generation of negative examples for discriminative learning a"
J16-1001,P13-2060,0,0.0178879,"ring kernel for finding associations between the source and target strings (Wang, Shawe-Taylor, and Szedmak 2007). Neural networks are another popular method for modeling nonlinearities, and it has been shown that neural networks can effectively be used to calculate new local features for MT (Liu et al. 2013). Methods such as boosting or stacking, which combine together multiple parameterizations of the translation model, have been incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta 2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding (Razmara and Sarkar 2013). Regression decision trees have also been introduced as a method for inducing nonlinear functions, incorporated through history-based search algorithms (Turian, Wellington, and Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with data from the target domain it is possible to achieve gains in accuracy. In machine translation, this is a"
J16-1001,W10-1748,0,0.153366,"that calculates not the expectation of the error itself, but the expectation of the sufficient statistics used in calculating the error. In contrast to sentence-level approximations or formulations such as linear BLEU, the expectation of the sufficient statistics can be calculated directly on the corpus level. Because of this, by maximizing the evaluation derived by these expected statistics, it is possible to directly optimize for a corpus-level error, in a manner similar to MERT (Pauls, Denero, and Klein 2009). When this is applied to BLEU in particular, this measure is often called xBLEU (Rosti et al. 2010, 2011) and the required sufficient statistics include n-gram counts and matched n-gram counts. We define the kth translation candidate in c(i) as hek , dk i, its score as si,k = γw&gt; h( f (i) , ek , dk ), and the probability in Equation (22) as pi,k = PK exp(si,k ) k0 =1 exp(si,k0 ) (53) Next, we define the expectation of the n-gram (gn ∈ ek ) frequency as cn,i,k , the expectation of the number of n-gram matches as mn,i,k , and the expectation of the reference length as ri,k . These values can be calculated as: cn,i,k = |{gn ∈ ek } |· pi,k (i) mn,i,k = |{gn ∈ ek } ∩ {gn ∈ e(i) } |· pi,k ri,k ="
J16-1001,W11-2119,0,0.0440587,"Missing"
J16-1001,2010.amta-papers.31,0,0.0123991,"all number of these k(k − 1)/2 hypotheses for use in optimization, which has been shown empirically to allow for increases in training speed without decreases in accuracy. For example, Hopkins and May (2011) describe a method dubbed pairwise ranking optimization that selects 5,000 pairs randomly for each sentence, and among these random pairs using the 50 with the largest difference in error for training the classifier. Other selection heuristics—for example, avoiding training on candidate pairs with overly different scores (Nakov, Guzm´an, and Vogel 2013), or performing Monte Carlo sampling (Roth et al. 2010; Haddow, Arun, and Koehn 2011)—are also possible and potentially increase accuracy. Recently, there has also been a method proposed that uses an efficient ranking SVM formulation that alleviates the need for this sampling and explicitly performs ranking over all pairs (Dreyer and Dong 2015). The mean squared error loss described in Section 3.6, which is similar to ranking loss in that it will prefer a proper ordering of the k-best list, is much easier to optimize. This loss can be minimized using standard techniques for solving least-squared-error linear regression (Press et al. 2007). 5.5 Ri"
J16-1001,2012.amta-papers.14,0,0.0513567,"Missing"
J16-1001,C10-2124,0,0.0524463,"Missing"
J16-1001,N13-1115,0,0.429063,"Missing"
J16-1001,N13-1034,0,0.0211213,", training of sparse features is an extremely difficult optimization problem, and at this point there is still no method that has been widely demonstrated as being able to robustly estimate the parameters of millions of features. Because of this, a third approach of first training the parameters of sparse features, then condensing the sparse features into dense features and performing one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according to Equation (1). In general, the inference is intractable if w"
J16-1001,N04-1023,0,0.090152,"Missing"
J16-1001,P12-1002,0,0.0423328,"Missing"
J16-1001,P06-2101,0,0.208252,"are not oracles. It should be noted that this loss can be calculated from a k-best list by iterating over the entire list and calculating the numerators and denominators in Equation (19). It is also possible, but more involved, to calculate over lattices or forests by using dynamic programming algorithms such as the forward–backward or inside–outside algorithms (Blunsom, Cohn, and Osborne 2008; Gimpel and Smith 2009). 3.3 Risk-Based Loss In contrast to softmax loss, which can be viewed as a probabilistic version of zero–one loss, risk defines a probabilistic version of the translation error (Smith and Eisner 2006; Zens, Hasan, and Ney 2007; Li and Eisner 2009; He and Deng 2012). Specifically, risk is based on the expected error incurred by a probabilistic model parameterized by w. This combines the advantages of the probabilistic model in softmax loss with the direct consideration of translation accuracy afforded by using error directly. In comparison to error, it also has the advantage of being differentiable, allowing for easier optimization. To define this error, we define a scaling parameter γ ≥ 0 and use it in the calculation of each hypothesis’s probability pγ,w (e, d |f , c) = P exp(γw&gt; h( f ,"
J16-1001,2006.amta-papers.25,0,0.0903222,"cy, and defining the corpus-level accuracy as the average of the sentence-level accuracies. All optimization algorithms that are applicable to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this distinction important from the optimization point of view. The most commonly used MT evaluation measure BLEU (Papineni et al. 2002) is defined on the corpus level, and we will cover it in detail as it plays an important role in some of the methods that follow. Of course, there have been many other evaluation measures proposed since BLEU, with TER (Snover et al. 2006) and METEOR (Banerjee and Lavie 2005) being among the most widely used. The great majority of metrics other than BLEU are defined on the sentence level, and thus are conducive to optimization algorithms that require sentence-level evaluation measures. We discuss the role of evaluation in MT optimization more completely in Section 8.3. 2.5.1 BLEU. BLEU is defined as the geometric mean of n-gram precisions (usually for n from 1 to 4), and a brevity penalty to prevent short sentences from receiving unfairly high evaluation scores. For a single reference sentence e and a corresponding system outpu"
J16-1001,E12-1013,0,0.0488057,"Missing"
J16-1001,2012.amta-papers.17,0,0.0766389,"Missing"
J16-1001,2011.eamt-1.33,0,0.0184665,"feature spaces. It should be noted that instability in MERT is not entirely due to the fact that search is random, but also due to the fact that k-best lists are poor approximations of the whole space of possible translations. One way to improve this approximation is by performing MERT over an exponentially large number of hypotheses encoded in a translation lattice (Macherey et al. 2008) or hypergraph (Kumar et al. 2009). It is possible to perform MERT over these sorts of packed data structures by observing the fact that the envelopes used in MERT can be expressed as a semiring (Dyer 2010a; Sokolov and Yvon 2011), allowing for exact calculation of the full envelope for all hypotheses in a lattice or hypergraph using polynomial-time dynamic programming (the forward algorithm or inside algorithm, respectively). There has also been work to improve the accuracy of the k-best approximation by either sampling k-best candidates from the translation lattice (Chatterjee and Cancedda 2010), or performing forced decoding to find derivations that achieve the reference translation, and adding them to the k-best list (Liang, Zhang, and Zhao 2012). The second weakness of MERT is that it has no concept of regularizat"
J16-1001,I11-1073,0,0.0579941,"Missing"
J16-1001,D13-1083,0,0.0144122,"ing corpus, but with respect to a subset of data sampled from the corpus. This has consequences for the calculation of translation quality when using a corpus-level evaluation measure such as BLEU. For example, when choosing an oracle for oracle-based optimization methods, the oracles chosen when considering the entire corpus will be different from the oracles chosen when considering a mini-batch. In general, the amount of difference between the corpus-level and mini-batch level oracles will vary depending on the size of a mini-batch, with larger mini-batches providing a better approximation (Tan et al. 2013; Watanabe 2012). Thus, when using smaller batches, especially single sentences, it is necessary to use methods to approximate the corpus-level error function as covered in the next two sections. 6.1.1 Approximation with a Pseudo-Corpus. The first method to approximate the corpuslevel evaluation measure relies on creating a pseudo-corpus, and using it to augment the statistics used in the mini-batch error calculation (Watanabe et al. 2007). Specifically, n oN given the training data hF, Ei = hf (i) , e(i) i , we define its corresponding pseudoi=1  (i) N corpus E¯ = e¯ i=1 . E¯ could be, for e"
J16-1001,N04-4026,0,0.0168543,"s, and it is common to add a word penalty feature that measures the length of translation e to compensate for this. Similarly, phrase penalty or rule penalty features express the trade-off between longer or shorter derivations. There exist other features that are dependent on the underlying MT system model. Phrase-based MT heavily relies on the distortion probabilities that are computed by the distance on the source side of target-adjacent phrase pairs. More refined lexicalized reordering models estimate the parameters from the training data based on the relative distance of two phrase pairs (Tillman 2004; Galley and Manning 2008). 2.3.2 Sparse features. Although dense features form the foundation of most SMT systems, in recent years the ability to define richer feature sets and directly optimize the system using rich features has been shown to allow for significant increases in accuracy. On the other hand, large and sparse feature sets make the MT optimization problem significantly harder, and many of the optimization methods we will cover in the rest of this survey are aimed at optimizing rich feature sets. The first variety of sparse features that we can think of are phrase features or rule"
J16-1001,P06-1091,0,0.0312915,"g to a function update(· ) as learning progresses. One standard method for updating η(t) according to the following formula η(t+1) ← η(1) 1 + t/T (70) allows for a guarantee of convergence (Collins et al. 2008). In Equation (69), the parameters are updated and we obtain w(t+1) . Within this framework, in the perceptron algorithm η(t) is set to a fixed value, and in MIRA the amount of update changes for every mini-batch. SGD-style online gradient-based methods have been used in translation for optimizing risk-based (Gao and He 2013), ranking-based (Watanabe 2012; Green et al. 2013), and other (Tillmann and Zhang 2006) objectives. When the regularization term Ω(w) is not differentiable, such as L1 regularization, it is a common practice to use forward-backward splitting (FOBOS) (Duchi and Singer 2009; Green et al. 2013) in which the optimization is performed in two steps: 1 w(t+ 2 ) ← w(t) − η(t+1) ∆`(F˜ (t) , E˜ (t) , C˜ (t) ; w(t) ) (71) w(t+1) ← arg min 1 kw − w(t+ 2 ) k22 + η(t+1) λΩ(w) 2 w (72) 1 36 Neubig and Watanabe Optimization for Statistical Machine Translation First, we perform updates without considering the regularization term in Equation (71). Second, the regularization term is applied in Equ"
J16-1001,P13-2072,0,0.0148718,"for MT (Liu et al. 2013). Methods such as boosting or stacking, which combine together multiple parameterizations of the translation model, have been incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta 2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding (Razmara and Sarkar 2013). Regression decision trees have also been introduced as a method for inducing nonlinear functions, incorporated through history-based search algorithms (Turian, Wellington, and Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with data from the target domain it is possible to achieve gains in accuracy. In machine translation, this is also very true, although much of the work on domain adaptation has focused on adapting the model learning process prior to explicit optimization towards an evaluation measure (Koehn and Schroeder 2007). However, there are a few works on optimization-based domain adaptation in MT, as w"
J16-1001,D08-1065,0,0.0296565,"06). The motivation for cooling is that if we start with a large T, the earlier steps using a smoother function will allow us to approach the global optimum, and the later steps will allow us to approach the actual error function. It should be noted that in Equation (24), and the discussion up to this point, we have been using not the corpus-based error, but the sentence-based error err(e(i) , e). There have also been attempts to make the risk minimization framework applicable to corpus-level error error(· ), specifically BLEU. We will discuss two such methods. 5.5.1 Linear BLEU. Linear BLEU (Tromble et al. 2008) provides an approximation for corpus-level BLEU that can be divided among sentences. Linear BLEU uses a Taylor expansion to approximate the effect that the sufficient statistics of any particular sentence will have on corpus-level BLEU. We define r as the total length of the reference translations, c as the total length of the candidates, and cn and mn (1 ≤ n ≤ 4) as the translation candidate’s number of n-grams, and number of n-grams that match the reference respectively. Taking the equation for corpus-level BLEU (Papineni et al. 2002) and assuming that the n-gram counts are approximately eq"
J16-1001,P07-1004,0,0.0720857,"Missing"
J16-1001,W02-1021,0,0.0686007,"Missing"
J16-1001,2005.eamt-1.36,0,0.0394239,"o(i) ← {he, di ∼ c(i) } or o(i) ← ∅ repeat for i ∈ P ERMUTE ({1, . . . , N} ) do . Random order o(i) ← ∅ s←∞ for k ∈ {1, . . . ,K} do (1) (i−1) s0 ← error E, {o1 , . . . , o1 (i) (i+1) , ck , o1 (N) , . . . , o1 }  if s0 &lt; s then . Update the oracle (i) (i) o ← {ck } s ← s0 else if s0 = s then . Same error value (i) o(i) ← o(i) ∪ {ck } end if end for end for until convergence . If O doesn’t change, converged return O end procedure Figure 2 Greedy search for an oracle. However, when using a corpus-level error function we need a slightly more sophisticated method, such as the greedy method of Venugopal and Vogel (2005). In this method (Figure 2), the oracle is first initialized either as an empty set or by randomly picking from the candidates. Next, we iterate randomly through the translation candidates in c(i) , try replacing the current oracle o(i) with the candidate, and check the change in the error function (Line 9), and if the error decreases, replace the oracle with the tested candidate. This process is repeated until there is no change in O. 4.3 Selecting Oracles for Margin-Based Methods Considering the hinge loss of Equation (30), the 1-best and oracle candidates are acquired according to Equation"
J16-1001,N07-2047,0,0.0806296,"Missing"
J16-1001,N12-1026,1,0.922215,"ith respect to a subset of data sampled from the corpus. This has consequences for the calculation of translation quality when using a corpus-level evaluation measure such as BLEU. For example, when choosing an oracle for oracle-based optimization methods, the oracles chosen when considering the entire corpus will be different from the oracles chosen when considering a mini-batch. In general, the amount of difference between the corpus-level and mini-batch level oracles will vary depending on the size of a mini-batch, with larger mini-batches providing a better approximation (Tan et al. 2013; Watanabe 2012). Thus, when using smaller batches, especially single sentences, it is necessary to use methods to approximate the corpus-level error function as covered in the next two sections. 6.1.1 Approximation with a Pseudo-Corpus. The first method to approximate the corpuslevel evaluation measure relies on creating a pseudo-corpus, and using it to augment the statistics used in the mini-batch error calculation (Watanabe et al. 2007). Specifically, n oN given the training data hF, Ei = hf (i) , e(i) i , we define its corresponding pseudoi=1  (i) N corpus E¯ = e¯ i=1 . E¯ could be, for example, either t"
J16-1001,D07-1080,1,0.831583,"er, and Dyer (2012) also propose features using the “shape” of translation rules, transforming a rule X → hne X 1 pas, did not X 1 i (5) into a string simply indicating whether each word is a terminal (T) or non-terminal (N) N → hT N T, T T Ni (6) Count-based features can also be extended to cover other features of the translation, such as phrase or rule bigrams, indicating which phrases or rules tend to be used together (Simianer, Riezler, and Dyer 2012). Another alternative for the creation of features that are sparse, but less sparse than features of phrases or rules, are lexical features (Watanabe et al. 2007). Lexical features, 6 Neubig and Watanabe Optimization for Statistical Machine Translation similar to lexical weighting, focus on the correspondence between the individual words that are included in a phrase or rule. The simplest variety of lexical features remembers which source words f are aligned with which target words e, and fires a feature for each pair. It is also possible to condition lexical features on the surrounding context in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features between every pair of words in the source or target sentences (Watanabe"
J16-1001,J97-3002,0,0.0677689,"ull the presidency in full delegation (b) lattice (c) forest the chair Figure 1 Example of a k-best list, lattice, and forest. Another class of decoding problem is forced decoding, in which the output from a decoder is forced to match with a reference translation of the input sentence. In phrasebased MT, this is implemented by adding additional features to reward hypotheses that match with the given target sentence (Liang, Zhang, and Zhao 2012; Yu et al. 2013). In MT using synchronous grammars, it is carried out by biparsing over two languages, for instance, by a variant of the CYK algorithm (Wu 1997) or by a more efficient two-step algorithm (Dyer 2010b; Peitz et al. 2012). Even if we perform forced decoding, we are still not guaranteed that the decoder will be able to produce the reference translation (because of unknown words, reordering limits, or other factors). This problem can be resolved by preserving the prefix of partial derivations (Yu et al. 2013), or by allowing approximate matching of the target side (Liang, Zhang, and Zhao 2012). It is also possible to create a neighborhood of a forced decoding derivation by adding additional hyperedges to the true derivation, which allows f"
J16-1001,P10-1049,0,0.0633434,"Missing"
J16-1001,2014.iwslt-evaluation.22,0,0.0376691,"ver ten years, MERT is still the dominant optimization algorithm. However, starting in WMT 2013, we can see a move to systems based on MIRA, and to a lesser extent ranking, particularly in the most competitive systems. In these systems, the preferred choice of an optimization algorithm seems to be MERT when using up to 20 features, and MIRA when using a large number of features (up to several hundred). There are fewer examples of systems using large numbers of features (tens of thousands, or millions) in actual competitive systems, with a few exceptions (Dyer et al. 2009; Neidert et al. 2014; Wuebker et al. 2014). In the case when a large number of sparse features are used, it is most common to use a softmax or risk-based objective and gradient-based optimization algorithms, often combining the features into summary features and performing a final tuning pass with MERT. The fact that algorithms other than MERT are seeing adoption in competitive systems for shared tasks is a welcome sign for the future of MT optimization research. However, there are still many open questions in the field, a few of which can be outlined here: Stable Training with Millions of Features: At the moment, there is still no st"
J16-1001,P11-2074,0,0.014221,"dense features and performing one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according to Equation (1). In general, the inference is intractable if we enumerate all possible derivations in T ( f ) and rank each derivation by the model. We assume that a derivation is composed of a set of steps d = d1 , d2 , · · · , d|d| (8) where each dj is a step—for example, a phrase pair in phrase-based MT or a synchronous rule in tree-based MT—ordered in a particular way. We also assume that 7 Computational Linguistics"
J16-1001,D11-1081,0,0.0885023,"re sparse, but less sparse than features of phrases or rules, are lexical features (Watanabe et al. 2007). Lexical features, 6 Neubig and Watanabe Optimization for Statistical Machine Translation similar to lexical weighting, focus on the correspondence between the individual words that are included in a phrase or rule. The simplest variety of lexical features remembers which source words f are aligned with which target words e, and fires a feature for each pair. It is also possible to condition lexical features on the surrounding context in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features between every pair of words in the source or target sentences (Watanabe et al. 2007), or integrate bigrams on the target side (Watanabe et al. 2007). Of these, the former two can be calculated from source and local target context, but target bigrams require target bigram context and are thus non-local features. One final variety of features that has proven useful is syntax-based features (Blunsom and Osborne 2008; Marton and Resnik 2008). In particular, phrase-based and hierarchical phrase-based translations do not directly consider syntax (in the linguistic sense) in the const"
J16-1001,D13-1026,0,0.0149338,"eters will result in overfitting, learning parameters that heavily favor using these memorized multi-word phrases, which will not be present in a separate test set. 1 It should be noted that although most work on MT optimization is concerned with linear models (and thus we will spend the majority of this article discussing optimization of these models), optimization using non-linear models is also possible, and is discussed in Section 8.1. 2 It should also be noted there have been a few recent attempts to jointly perform rule extraction and optimization, doing away with this two-step process (Xiao and Xiong 2013). 4 Neubig and Watanabe Optimization for Statistical Machine Translation The traditional way to solve this problem is to train the TM on a large parallel corpus on the order of hundreds of thousands to tens of millions of sentences, then perform optimization of parameters on a separate set of data consisting of around one thousand sentences, often called the development set. When learning the weights for larger feature sets, however, a smaller development set is often not sufficient, and it is common to perform cross-validation, holding out some larger portion of the training set for parameter"
J16-1001,P01-1067,0,0.0850855,"lated, f ∈ F as one of the sentences, and E ( f ) as the collection of all possible target language sentences that can be obtained by translating f . Machine translation systems perform this translation process by dividing the translation of a full sentence into the translation and recombination of smaller parts, which are represented as hidden variables, which together form a derivation. For example, in phrase-based translation (Koehn, Och, and Marcu 2003), the hidden variables will be the alignment between the phrases of the source and target sentences, and in tree-based translation models (Yamada and Knight 2001; Chiang 2007), the hidden variables will represent the latent tree structure used to generate the translation. We will define D( f ) to be the space of possible derivations that can be acquired from source sentence f , and d ∈ D( f ) to be one of those derivations. Any particular derivation d will correspond to exactly one e ∈ E ( f ), although the opposite is not true (the derivation uniquely determines the translation, but there can be multiple derivations corresponding to a particular translation). We also define tuple he, di consisting of a target sentence and its corresponding derivation"
J16-1001,D13-1112,0,0.0876147,"port fully the chair . X will X . the X X will support X X the X of X will X support X fully fully the presidency chinese support delegation china in full the presidency in full delegation (b) lattice (c) forest the chair Figure 1 Example of a k-best list, lattice, and forest. Another class of decoding problem is forced decoding, in which the output from a decoder is forced to match with a reference translation of the input sentence. In phrasebased MT, this is implemented by adding additional features to reward hypotheses that match with the given target sentence (Liang, Zhang, and Zhao 2012; Yu et al. 2013). In MT using synchronous grammars, it is carried out by biparsing over two languages, for instance, by a variant of the CYK algorithm (Wu 1997) or by a more efficient two-step algorithm (Dyer 2010b; Peitz et al. 2012). Even if we perform forced decoding, we are still not guaranteed that the decoder will be able to produce the reference translation (because of unknown words, reordering limits, or other factors). This problem can be resolved by preserving the prefix of partial derivations (Yu et al. 2013), or by allowing approximate matching of the target side (Liang, Zhang, and Zhao 2012). It"
J16-1001,D09-1006,0,0.0239333,"e rather surprising result that despite the fact that other evaluation measures had proven superior to BLEU with regards to post facto correlation with human evaluation, a BLEU-optimized system proved superior to systems tuned using other metrics. Since this result, however, there have been other reports stating that systems optimized using other metrics such as TESLA (Liu, Dahlmeier, and Ng 2011) and MEANT (Lo et al. 2013) achieve superior results to BLEU-optimized systems. There have also been attempts to directly optimize not automatic, but human evaluation measures of translation quality (Zaidan and Callison-Burch 2009). However, the cost of performing this sort of human-in-the-loop optimization is prohibitive, so Zaidan and Callison-Burch (2009) propose a method that re-uses partial hypotheses in evaluation. Saluja, Lane, and Zhang (2012) also propose a method for incorporating binary good/bad input into optimization, with the motivation that this sort of feedback is easier for human annotators to provide than generating new reference sentences. Finally, there is also some work on optimizing multiple evaluation metrics at one time. The easiest way to do so is to simply use the linear interpolation of two or"
J16-1001,D07-1055,0,0.0643423,"Missing"
J16-1001,N09-2006,0,0.0244941,"w(1) ∼ RM . Initialize randomly for t ∈ {1 . . . T} do . Until convergence for m ∈ {1 . . . M} do . For each dimension γˆ m ← arg minγ `error (F, E, C; w(t) + γbm ) . Search end for . Descent γˆ ← arg minγˆ m `error (F, E, C; w(t) + γˆ m bm ) m (t+1) (t) w ← w + γˆ b . Update end for ˆ then if `error (F, E, C; w(T+1) ) &lt; `error (F, E, C; w) ˆ ← w(T+1) w end if end for ˆ return w end procedure Figure 4 Minimum error rate training (MERT). inspired by Powell’s method (Och 2003; Press et al. 2007), or the Downhill-Simplex method (Nelder-Mead method) (Press et al. 2007; Zens, Hasan, and Ney 2007; Zhao and Chen 2009). The algorithm for MERT using line search is shown in Figure 4. Here, we assume that w and h(· ) are M-dimensional, and bm is an M-dimensional vector where the m-th element is 1 and the rest of the elements are zero. For the T iterations, we decide the dimension m of the feature vector (line 6), and for each possible weight vector w(j) + γbm choose the γ ∈ R that minimizes `error (· ) using line search (line 7). Then, among the γ for each of the M search dimensions, we perform an update using γˆ that affords the largest reduction in error (lines 9 and 10). This algorithm can be deemed a varie"
J16-1001,I11-1072,0,0.0216474,"forming domain adaptation is by selecting a subset of the training data that is similar to the data that we want to translate (Li et al. 2010). This can be done by selecting sentences that are similar to our test corpus, or even selecting adaptation data for each individual test sentence (Liu et al. 2012). If no parallel data exist in the target domain, it has also been shown that first automatically translating data from the source to the target language or vice versa, then using this data for optimization and model training is also helpful (Ueffing, Haffari, and Sarkar 2007; Li et al. 2011; Zhao et al. 2011) In addition, in a computer-assisted translation scenario, it is possible to reflect post-edited translations back into the optimization process as new indomain training data (Mathur, Mauro, and Federico 2013; Denkowski, Dyer, and Lavie 2014). Once adaptation data have been chosen, it is necessary to decide how to use the data. The most straightforward way is to simply use these in-domain data in optimization, but if the data set is small it is preferable to combine both in- and out-ofdomain data to achieve more robust parameter estimates. This is essentially equivalent to the standard domain-"
J16-1001,W06-1610,0,0.0263346,"Missing"
J16-1001,W06-3601,0,\N,Missing
J16-1001,D11-1035,0,\N,Missing
J16-1001,W05-0836,0,\N,Missing
J16-1001,W10-1711,0,\N,Missing
J16-1001,2010.iwslt-evaluation.22,0,\N,Missing
J16-1001,W14-3302,0,\N,Missing
K17-2005,D16-1116,0,0.0605972,"Missing"
K17-2005,D16-1031,0,0.0287355,"prior distribution but has no contribution to the decoder. To force the decoder to use the latent variables, we take the following two approaches which are similar to Bowman et al. (2016). KL-Divergence Annealing: We add a coefficient λ to the KL cost and gradually anneal it from zero to a predefined threshold λm . At the early stage of training, we set λ to be zero and let the model first figure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost. This technique can also be seen in (Koˇcisk`y et al., 2016; Miao and Blunsom, 2016). Input Dropout in the Decoder: Besides annealing the KL cost, we also randomly drop out the 4 Architecture for Morphological Reinflection The overall model architecture is shown in Fig. 1. Each character and each label is associated with a continuous vector. We employ Gated Recurrent Units (GRUs) for the encoder and decoder. We use only single directional GRUs as the encoder for the input word x(s) . u is the hidden representation of x(s) which is the last hidden state of GRUs. and is used as the input for the inference model on z. We represent µ(u) and σ 2 (u) as MLPs and sample z from N (µ("
K17-2005,K16-1002,0,0.0568913,"s Latent Variables We observe that with the vanilla implementation the KL cost quickly decreases to near zero, setting qφ (z|x) equal to standard normal distribution. In this case, the RNN decoder can easily degenerate into an RNN language model. Hence, the latent variables are ignored by the decoder and cannot encode any useful information. The latent variable z learns an undesirable distribution that coincides with the imposed prior distribution but has no contribution to the decoder. To force the decoder to use the latent variables, we take the following two approaches which are similar to Bowman et al. (2016). KL-Divergence Annealing: We add a coefficient λ to the KL cost and gradually anneal it from zero to a predefined threshold λm . At the early stage of training, we set λ to be zero and let the model first figure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost. This technique can also be seen in (Koˇcisk`y et al., 2016; Miao and Blunsom, 2016). Input Dropout in the Decoder: Besides annealing the KL cost, we also randomly drop out the 4 Architecture for Morphological Reinflection The overall model archite"
K17-2005,P17-1029,1,0.868237,"are two tasks in SIGMORPHON 2017, which are morphology inflection (task 1) and paradigm completion (task 2) respectively. We participated in task 1, inflection generation, in which the goal is to output the inflected form of a lemma given a set of desired morphological tags.1 Experimental results found that our model works relatively well on the shared task 1 without extensive tuning of hyper-parameters and languagespecific features. This paper describes the CMU submission to shared task 1 of SIGMORPHON 2017. The system is based on the multi-space variational encoder-decoder (MSVED) method of Zhou and Neubig (2017), which employs both continuous and discrete latent variables for the variational encoder-decoder and is trained in a semi-supervised fashion. We discuss some language-specific errors and present result analysis. 1 Introduction In morphologically rich languages, different affixes (i.e. prefixes, infixes, suffixes) can be combined with the lemma to reflect various syntactic and semantic features of a word. In many areas of natural language processing (NLP) it is important that systems are able to correctly analyze and generate different morphological forms, including previously unseen forms. Th"
K17-2005,D13-1174,0,0.0229089,"sed fashion. We discuss some language-specific errors and present result analysis. 1 Introduction In morphologically rich languages, different affixes (i.e. prefixes, infixes, suffixes) can be combined with the lemma to reflect various syntactic and semantic features of a word. In many areas of natural language processing (NLP) it is important that systems are able to correctly analyze and generate different morphological forms, including previously unseen forms. The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation (Chahuneau et al., 2013) and information retrieval (Darwish and Oard, 2007). Accordingly, learning morphological reinflection patterns from labeled data is an important challenge. The Universal Morphological Reinflection task at SIGMORPHON 2017 (Cotterell and Sch¨utze, 2017) is an evaluation campaign aimed at systems that tackle the task of morphological inflection. It extends the SIGMORPHON 2016 Morphological Reinflection by conducting tasks in 52 languages instead of 10 Cotterell et al. (2016). In our system submission, we utilize multispace variational encoder-decoders (MSVEDs), which are a varitional encoder-deco"
K17-2005,W16-2002,0,0.0699015,"Missing"
K17-2005,W16-2010,0,0.161517,"Missing"
K19-1022,K17-3004,0,0.0306181,"on the languages discussed in this work, use discriminative models. Kanayama et al. (2017) had tremendous success on Japanese using a wildly different approach. They train a model to identify likely syntactic heads, then assume that all other words simply attach in a left-branching structure, which works due to the strictly head-final nature of Japanese. Dozat et al. (2017) train a discriminative neural parser which uses a BiLSTM to generate hidden representations of each word (Kiperwasser and Goldberg, 2016). These representations are used to score arcs, which are greedily added to the tree. Björkelund et al. (2017) perform best on Arabic, using an ensemble of many different types of bottom-up discriminative parsers. They have each of twelve parsers score potential arcs, learn a weighting function to combine them, and use the Chu-Liu-Edmonds algorithm (Chu, 1965; Edmonds, 1967) to output final parses. All three of these discriminative models are very effective for analysis of a sentence, none of them are able to be converted into a similar generative model. At best, the biaffine model of Dozat et al. (2017) could generate a bag of dependencies without order information, which makes it impractical as the"
K19-1022,P15-2142,0,0.0158506,"its subject noun, may have large entropy when choosing the verb, but an easier time choosing the subject since it can condition on the verb limiting its choices to appropriate semantic classes, person, 233 Top-Down Bottom-Up Structure 4.97 7.42 Terminals 67.9 63.3 6 Related Work Most work on discriminative dependency parsing follows the bottom-up paradigm (Nivre, 2003; Nivre et al., 2007; Dyer et al., 2015; Kiperwasser and Goldberg, 2016), but top-down models have also shown some promise (Zhang et al., 2015). Generative dependency models go back to Hays (1964), but most existing such models (Buys and Blunsom, 2015; Jiang et al., 2016) have relied on independence assumptions whether used for parsing, unsupervised dependency induction, or language modeling. Buys and Blunsom (2015) also describe a generative bottom-up neural parser, but use hand-crafted input features and limit the model to third-order features. Titov and Henderson (2010) explore a generative parsing model with no independence assumptions based on sigmoid belief networks (Neal, 1992) instead of RNNs. The CoNLL 2017 shared task saw many different models succeed at parsing Universal Dependencies. Most of the top contenders, including the be"
K19-1022,N18-1086,0,0.0321065,"Missing"
K19-1022,K17-3002,0,0.35251,"el, and our bottom-up model to obtain a score for the parse from each. We combine these scores using weights learned to optimize performance on the development set (Och, 2003). 3 Data Sets 3.2 Baseline Models On the language modeling task we compare against a standard LSTM-based language model baseline (Mikolov et al., 2010), using 1024dimensional 2-layer LSTM cells, and optimized using Adam (Kingma and Ba, 2014). For the parsing task we compare against the discriminative parser of Dyer et al. (2015), a bottom-up transition-based parser that uses stackLSTMs, as well as the overall top system (Dozat et al., 2017) from the 2017 CoNLL shared task on multilingual dependency parsing (Zeman et al., 2017). That work uses a discriminative graphbased parser that uses a biaffine scoring function to score each potential arc. Moreover, it uses character-level representations to deal with morphology and a PoS tagger more sophisticated than UDPipe – two major changes from the shared task’s default pipeline. These two differences afford them a substantial advantage over our approach which only modifies the parsing step of the pipeline. Finally, we show the results of an oracle system looking at the 1000-best lists"
K19-1022,P15-1033,1,0.909014,"e gold-standard data using in our language modeling experiments. In the second, we again train on gold data, but use UDPipe (Straka and Straková, 2017) to segment, tokenize, and POS tag the dev and test sets starting from raw text, following the default scenario and most participants in the CoNLL 2017 shared task. very STOP-L STOP-R the very tall old man Figure 4: Examples of embedding two subtrees in the top-down model. A subtree is embedded using an LSTM over its child subtrees (solid lines) with a gated residual connection from the root word to the final embedding (dotted lines). parser of Dyer et al. (2015), a discriminative neural stack-LSTM-based bottom-up parser, as our proposal distribution q(x, y) and compute the approximate marginal using N = 1000 samples per P p(x,y) sentence: p(x) ≈ N1 N i=1 q(x,y) . 2.4 Parsing Evaluation through Reranking In order to evaluate our model as a parser we would ideally like to efficiently find the MAP parse tree given an input sentence. Unfortunately, due to the unbounded dependencies across the sequences of actions used by our models this inference is infeasible. As such, we instead rerank a list of 1000 samples produced by the baseline discriminative pars"
K19-1022,N16-1024,1,0.899149,"tom up and the other top down, which profoundly changes the estimation problem faced by the learner. We evaluate the two models on three typologically different languages: English, Arabic, and Japanese. We find that both generative models improve parsing performance over a discriminative baseline, but, in contrast to RNNGs, they are significantly less effective than non-syntactic LSTM language models. Little difference between the tree construction orders is observed for either parsing or language modeling. 1 Chris Dyer DeepMind cdyer@google.com Introduction Recurrent neural network grammars (Dyer et al., 2016, RNNGs) are syntactic language models that use predicted syntactic structures to determine the topology of the recurrent networks they use to predict subsequent words. Not only can they learn to model language better than non-syntactic language models, but the conditional distributions over parse trees given sentences produce excellent parsers (Fried et al., 2017). In this paper, we introduce and evaluate two new dependency syntax language models which are based on a recurrent neural network (RNN) 1 We release code for these two models, which can be found at https://github.com/armatthews/ dep"
K19-1022,P17-2025,0,0.08465,"han non-syntactic LSTM language models. Little difference between the tree construction orders is observed for either parsing or language modeling. 1 Chris Dyer DeepMind cdyer@google.com Introduction Recurrent neural network grammars (Dyer et al., 2016, RNNGs) are syntactic language models that use predicted syntactic structures to determine the topology of the recurrent networks they use to predict subsequent words. Not only can they learn to model language better than non-syntactic language models, but the conditional distributions over parse trees given sentences produce excellent parsers (Fried et al., 2017). In this paper, we introduce and evaluate two new dependency syntax language models which are based on a recurrent neural network (RNN) 1 We release code for these two models, which can be found at https://github.com/armatthews/ dependency-lm. 2 In this work, we limit ourselves to models that are capable only of generating projective dependency trees. 227 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 227–237 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 0 $ Top-down 1 sings 2 John 3 STOP-L 5 STOP-L 6 12 STOP-R wel"
K19-1022,W03-3017,0,0.181107,"n on why the two models’ performances are so similar. Unfortunately the fact that the models use different conditioning contexts makes direct comparison of sub-sentential scores impossible. The topdown model, which generates the verb before its subject noun, may have large entropy when choosing the verb, but an easier time choosing the subject since it can condition on the verb limiting its choices to appropriate semantic classes, person, 233 Top-Down Bottom-Up Structure 4.97 7.42 Terminals 67.9 63.3 6 Related Work Most work on discriminative dependency parsing follows the bottom-up paradigm (Nivre, 2003; Nivre et al., 2007; Dyer et al., 2015; Kiperwasser and Goldberg, 2016), but top-down models have also shown some promise (Zhang et al., 2015). Generative dependency models go back to Hays (1964), but most existing such models (Buys and Blunsom, 2015; Jiang et al., 2016) have relied on independence assumptions whether used for parsing, unsupervised dependency induction, or language modeling. Buys and Blunsom (2015) also describe a generative bottom-up neural parser, but use hand-crafted input features and limit the model to third-order features. Titov and Henderson (2010) explore a generative"
K19-1022,P04-1013,0,0.112698,"Missing"
K19-1022,D16-1073,0,0.0253279,"ave large entropy when choosing the verb, but an easier time choosing the subject since it can condition on the verb limiting its choices to appropriate semantic classes, person, 233 Top-Down Bottom-Up Structure 4.97 7.42 Terminals 67.9 63.3 6 Related Work Most work on discriminative dependency parsing follows the bottom-up paradigm (Nivre, 2003; Nivre et al., 2007; Dyer et al., 2015; Kiperwasser and Goldberg, 2016), but top-down models have also shown some promise (Zhang et al., 2015). Generative dependency models go back to Hays (1964), but most existing such models (Buys and Blunsom, 2015; Jiang et al., 2016) have relied on independence assumptions whether used for parsing, unsupervised dependency induction, or language modeling. Buys and Blunsom (2015) also describe a generative bottom-up neural parser, but use hand-crafted input features and limit the model to third-order features. Titov and Henderson (2010) explore a generative parsing model with no independence assumptions based on sigmoid belief networks (Neal, 1992) instead of RNNs. The CoNLL 2017 shared task saw many different models succeed at parsing Universal Dependencies. Most of the top contenders, including the best scoring systems on"
K19-1022,K17-3028,0,0.0672997,"Missing"
K19-1022,K17-3001,0,0.150413,"nning over entries from oldest to newest. The resulting vector h is then passed through an MLP, and then a softmax over the three possible action types. If the SHIFT action is taken, the vector h is re-used and passed through a separate MLP and softmax over the vocabulary to choose an individual word to generate. If one of the two REDUCE actions is chosen, the top two elements from the stack are popped, concatenated (with the head-to-be first, followed by the child), and passed through an MLP. The result is a vector representing a new subtree that is then pushed onto the stack. Kuncoro et al. (2017) Marginalization Traditionally a language model takes a sentence x and assigns it a probability p(x). Since our syntax-based language models jointly predicts the probability p(x, y) of a sequence of terminals x and a tree y, we must marginalize over trees to get the total P probability assigned to a sentence x, p(x) = y∈T (x) p(x, y), where T (x) represents the set of all possible dependency trees over a sentence x. Unfortunately the size of T (x) grows exponentially in the length of x, making explicit marginalization infeasible. Instead we use importance sampling to approximate the marginal ("
K19-1022,P03-1021,0,0.0141966,"ces of actions used by our models this inference is infeasible. As such, we instead rerank a list of 1000 samples produced by the baseline discriminative parser, a combination process that has been shown to improve performance by combining the different knowledge learned by the discriminative and generative models (Fried et al., 2017). For each hypothesis parse in the sample list we query the discriminative parser, our top-down model, and our bottom-up model to obtain a score for the parse from each. We combine these scores using weights learned to optimize performance on the development set (Och, 2003). 3 Data Sets 3.2 Baseline Models On the language modeling task we compare against a standard LSTM-based language model baseline (Mikolov et al., 2010), using 1024dimensional 2-layer LSTM cells, and optimized using Adam (Kingma and Ba, 2014). For the parsing task we compare against the discriminative parser of Dyer et al. (2015), a bottom-up transition-based parser that uses stackLSTMs, as well as the overall top system (Dozat et al., 2017) from the 2017 CoNLL shared task on multilingual dependency parsing (Zeman et al., 2017). That work uses a discriminative graphbased parser that uses a biaf"
K19-1022,K17-3009,0,0.0165193,"English, Japanese, and Arabic, as provided for the 2017 CoNLL shared task on universal dependency parsing. In all languages we convert all singleton terminal symbols to a special UNK token. See Table 1 for details regarding the size of these data sets. For language modeling we evaluate using the gold sentence segmentations, word tokenizations, and part of speech tags given in the data. For parsing, we evaluate in two scenarios. In the first, we train and test on the same gold-standard data using in our language modeling experiments. In the second, we again train on gold data, but use UDPipe (Straka and Straková, 2017) to segment, tokenize, and POS tag the dev and test sets starting from raw text, following the default scenario and most participants in the CoNLL 2017 shared task. very STOP-L STOP-R the very tall old man Figure 4: Examples of embedding two subtrees in the top-down model. A subtree is embedded using an LSTM over its child subtrees (solid lines) with a gated residual connection from the root word to the final embedding (dotted lines). parser of Dyer et al. (2015), a discriminative neural stack-LSTM-based bottom-up parser, as our proposal distribution q(x, y) and compute the approximate margina"
K19-1022,Q16-1023,0,0.165867,". Unfortunately the fact that the models use different conditioning contexts makes direct comparison of sub-sentential scores impossible. The topdown model, which generates the verb before its subject noun, may have large entropy when choosing the verb, but an easier time choosing the subject since it can condition on the verb limiting its choices to appropriate semantic classes, person, 233 Top-Down Bottom-Up Structure 4.97 7.42 Terminals 67.9 63.3 6 Related Work Most work on discriminative dependency parsing follows the bottom-up paradigm (Nivre, 2003; Nivre et al., 2007; Dyer et al., 2015; Kiperwasser and Goldberg, 2016), but top-down models have also shown some promise (Zhang et al., 2015). Generative dependency models go back to Hays (1964), but most existing such models (Buys and Blunsom, 2015; Jiang et al., 2016) have relied on independence assumptions whether used for parsing, unsupervised dependency induction, or language modeling. Buys and Blunsom (2015) also describe a generative bottom-up neural parser, but use hand-crafted input features and limit the model to third-order features. Titov and Henderson (2010) explore a generative parsing model with no independence assumptions based on sigmoid belief"
K19-1022,E17-1117,1,0.800545,"sing an LSTM running over entries from oldest to newest. The resulting vector h is then passed through an MLP, and then a softmax over the three possible action types. If the SHIFT action is taken, the vector h is re-used and passed through a separate MLP and softmax over the vocabulary to choose an individual word to generate. If one of the two REDUCE actions is chosen, the top two elements from the stack are popped, concatenated (with the head-to-be first, followed by the child), and passed through an MLP. The result is a vector representing a new subtree that is then pushed onto the stack. Kuncoro et al. (2017) Marginalization Traditionally a language model takes a sentence x and assigns it a probability p(x). Since our syntax-based language models jointly predicts the probability p(x, y) of a sequence of terminals x and a tree y, we must marginalize over trees to get the total P probability assigned to a sentence x, p(x) = y∈T (x) p(x, y), where T (x) represents the set of all possible dependency trees over a sentence x. Unfortunately the size of T (x) grows exponentially in the length of x, making explicit marginalization infeasible. Instead we use importance sampling to approximate the marginal ("
K19-1022,J93-2004,0,0.0647921,"eneration events show, the top-down model generates recursively from the root, whereas the bottomup model generates from left to right. ing distracted by accidental correlations), while in the other it may be more distant. These differences thus imply that the two models will have different structural biases, but it is not at all clear whether one should out perform the other. We therefore explore to what extent this choice of construction order affects performance, and we evaluate the proposed models on language modeling and parsing tasks across three typologically different languages (§3). (Marcus et al., 1993). Finally, we observe only minimal differences in language modeling performance for top-down and bottom-up models. This result is surprising in light of how different the estimation problems are, but it is a clear demonstration of the ability of RNNs to learn to extract relevant features from data presented in any different but consistent orders. Our findings (§4) show that, like RNNGs, generative dependency models make good parsers. Given the small scale of the Universal Dependency corpora, this result is also in line with previous work which shows that joint generative models offer very samp"
L16-1314,bazillon-etal-2008-manual,0,0.0834192,"Missing"
L16-1314,D14-1172,0,0.0283067,"ther differences are caused by transcriber characteristics or by experimental settings. More generally, our experiments involve “random” factors that are difficult to control for, and that potentially have a significant influence on our observations. In fact, this is a common problem in user studies. Recently, linear mixed-effects models 2 www.msperber.com/research/lrec-iterative-gui (short: mixed models) have become popular as a convenient way of dealing with such situations. For instance, mixed models have been used for error analysis in ASR (Goldwater et al., 2010) and machine translation (Federico et al., 2014), and for analysis of post-editing for translation (Green et al., 2013). Mixed models are specified by the following components: • Response variable: The central quantity for which we wish to determine how it is influenced by other measured covariates. In our experiments, this will be the post-correction error rate or the transcription time. • Fixed effects: Numerical or categorical attributes that influence the response variable in a meaningful way. In this paper, we assume a linear relationship. In the case of categorical variables, the assumption is that the observations include all values"
L16-1314,2012.iwslt-papers.10,1,0.873966,"Missing"
L16-1314,N10-1024,0,0.169441,"Missing"
L16-1314,Q14-1014,1,0.861183,"Our goal in this paper is to design a computer-assisted transcription user interface such that the outcome quality is optimized while avoiding unnecessary effort. The key interface feature we investigate is support for iterative transcription. This term is borrowed from iterative human computation processes (Little et al., 2010), in which humans solve tasks by improving upon a previously obtained solution. We consider computer-assisted transcription performed in an efficient segment-by-segment fashion, where only lowconfidence segments are selected for manual transcription (Roy and Roy, 2009; Sperber et al., 2014b). Our iterative interfaces then provide the initial transcription as created by the ASR as a starting point for each segment, upon which the transcriber improves (cf. Figure 1). The benefit of the iterative interfaces is that the transcriber can simply use the initially correct parts from the ASR as-is, and focus attention on the problematic parts. Ideally, words that were recognized correctly by the ASR will not be changed, reducing the chance of correction errors. In addition, the iterative approach can assist transcription of parts that are difficult for the transcriber to understand by p"
L18-1530,L16-1632,1,0.828001,"because they constitute phonological units (syllable rhymes; Na syllables are composed of an onset, a rhyme, and a tone). Concerning improvements to the original transcriptions, we addressed cases where the same phoneme had inconsistent representation in the corpus, such as /wæ / and /w æ/, as well as an instance where the unicode representation of a single phoneme was sometimes v+nasality+syllabic diacritic and sometimes v+syllabic diacritic+nasality. We computed the Na results of Tables 1-3 using the larger suite of 224 minutes and these preprocessing changes. For Chatino, we used data of Ćavar et al. (2016) from the GORILLA language archive for Eastern Chatino of San Juan Quiahije, Oaxaca, Mexico (Cavar et al., 2016) for the purposes of comparing phoneme and tone prediction with Na when data restriction is in place. We used up to 50 minutes of data for training, 6 minutes for validation and 6 minutes for testing. The phoneme inventory we used consists of 31 labels along with 14 tone labels. For both languages, preprocessing involved removing punctuation and any other symbols that are not phonemes, tones or the tone 3358 Chatino Na Input Output PER ↓ TER ↓ PER ↓ TER ↓ TGB-F1 ↑ fbank fbank+pitch f"
L18-1530,C16-1328,0,0.0709237,"nemic transcription; French, English and Chinese translations; target label sequences: (1) phonemes only, (2) tones only, (3) phonemes and tones together, and (4) phonemes and tones with tone group boundary markers, “|”. labels are collapsed. The use of an underlying recurrent neural network allows the model to implicitly model context via the parameters of the LSTM, despite the independent frame-wise label predictions of the CTC network. It is this feature of the architecture that makes it a promising tool for tonal prediction, since tonal information is suprasegmental, spanning many frames (Mortensen et al., 2016). Context beyond the immediate local signal is indispensable for tonal prediction, and longranging context is especially important in the case of morphotonologically rich languages such as Na and Chatino. Past work distinguishes between embedded tonal modelling, where phoneme and tone labels are jointly predicted, and explicit tonal modelling, where they are predicted separately (Lee et al., 2002). We compare several training objectives for the purposes of phoneme and tone prediction. This includes separate prediction of 1. phonemes and 2. tones, as well as 3. jointly predict phonemes and tone"
mori-neubig-2014-language,W04-3236,0,\N,Missing
mori-neubig-2014-language,W04-3230,0,\N,Missing
mori-neubig-2014-language,C04-1067,0,\N,Missing
mori-neubig-2014-language,C94-1032,0,\N,Missing
mori-neubig-2014-language,C96-2202,1,\N,Missing
mori-neubig-2014-language,C08-1113,1,\N,Missing
mori-neubig-2014-language,P09-1117,0,\N,Missing
mori-neubig-2014-language,P02-1064,0,\N,Missing
mori-neubig-2014-language,I13-1018,0,\N,Missing
mori-neubig-2014-language,P09-1058,0,\N,Missing
mori-neubig-2014-language,mori-etal-2014-flow,1,\N,Missing
mori-neubig-2014-language,I08-7018,0,\N,Missing
mori-neubig-2014-language,neubig-mori-2010-word,1,\N,Missing
mori-neubig-2014-language,P11-2093,1,\N,Missing
N15-1033,C10-3010,0,0.0134665,"n and Hoang, 2007) is also an example where an LM over a second 6 Results for model score, a more direct measure of search errors, were largely similar. stream of factors (for example POS tags, classes, or lemmas) has been shown to increase accuracy. These factors are limited, however, by the strong constraint of being associated with a single word and not allowing reordering, and thus are not applicable to our setting of using multiple languages. There has also been work on using multiple languages to improve the quality of extracted translation lexicons or topic models (Mausam et al., 2009; Baldwin et al., 2010; Mimno et al., 2009). These are not concerned with multi-target translation, but may provide us with useful hints about how to generate more effective multi-target translation models. 8 Conclusion In this paper, we have proposed a method for multitarget translation using a generalization of SCFGs, and proposed methods to learn and perform search over the models. In experiments, we found that these models are effective in the case when a strong LM exists in a second target that is highly related to the first target of interest. As the overall framework of multi-target translation is broad-reac"
N15-1033,D07-1090,0,0.0358374,"ld not decide which is correct. However, if they were additionally given English T2 translations corresponding to each of the Chinese translations, they could easily choose the third as the most natural, even without knowing a word of Chinese. Translating this into MT terminology, this is equivalent to generating two corresponding target sentences E1 and E2 , and using the naturalness of E2 to help decide which E1 to generate. Language models (LMs) are the traditional tool for assessing the naturalness of sentences, and it is widely known that larger and stronger LMs greatly help translation (Brants et al., 2007). It is easy to think of a situation where we can only create a weak LM for T1, but much more easily create a strong LM for T2. For example, T1 could be an under-resourced language, or a new entrant to the EU or UN. As a concrete method to realize multi-target translation, we build upon Chiang (2007)’s framework of synchronous context free grammars (SCFGs), which we first overview in Section 2.2 SCFGs are an extension of context-free grammars that define rules that synchronously generate source and target strings F and E. We expand this to a new formalism of multi-synchronous CFGs (MSCFGs, Sec"
N15-1033,2012.eamt-1.60,0,0.014986,"pair, from the source F to the target E. However, in many actual translation situations, identical documents are translated not from one language to another, but between a large number of different languages. Examples of this abound in commercial translation, and prominent open data sets used widely by the MT community include UN documents in 6 languages (Eisele and Chen, 2010), European Parliament Proceedings in 21 languages 1 Code and data to replicate the experiments can be found at http://phontron.com/project/naacl2015 (Koehn, 2005), and video subtitles on TED in as many as 50 languages (Cettolo et al., 2012). However, despite this abundance of multilingual data, there have been few attempts to take advantage of it. One exception is the multi-source SMT method of Och and Ney (2001), which assumes a situation where we have multiple source sentences, and would like to combine the translations from these sentences to create a better, single target translation. In this paper, we propose a framework of multitarget SMT. In multi-target translation, we translate F to not a single target E, but to a set of sentences E = hE1 , E2 , . . . , E|E |i in multiple target languages (which we will abbreviate T1, T"
N15-1033,J07-2003,0,0.631893,"rating two corresponding target sentences E1 and E2 , and using the naturalness of E2 to help decide which E1 to generate. Language models (LMs) are the traditional tool for assessing the naturalness of sentences, and it is widely known that larger and stronger LMs greatly help translation (Brants et al., 2007). It is easy to think of a situation where we can only create a weak LM for T1, but much more easily create a strong LM for T2. For example, T1 could be an under-resourced language, or a new entrant to the EU or UN. As a concrete method to realize multi-target translation, we build upon Chiang (2007)’s framework of synchronous context free grammars (SCFGs), which we first overview in Section 2.2 SCFGs are an extension of context-free grammars that define rules that synchronously generate source and target strings F and E. We expand this to a new formalism of multi-synchronous CFGs (MSCFGs, Section 3) that simultaneously generate not just two, but an arbitrary number of strings hF, E1 , E2 , . . . , EN i. We describe how to acquire these from data (Section 4), and how to perform search, including calculation of LM probabilities over multiple target language strings (Section 5). To evaluate"
N15-1033,P11-2031,0,0.0115844,"oder, we use the Travatar (Neubig, 2013) toolkit, and implement all necessary extensions to the decoder and rule extraction code to allow for multiple targets. Unless otherwise specified, we use joint search with a pop limit of 2,000, and T1 rule pruning with a limit of 10 rules per source rule. BLEU is used for both tuning and evaluating all models. In particular, we tune and evaluate all models based on T1 BLEU, simulating a situation similar to that in the introduction, where we want to use a large LM in T2 to help translation in T1. In order to control for optimizer instability, we follow Clark et al. (2011)’s recommendation of performing tuning 3 times, and reporting the average of the runs along with statistical significance obtained by pairwise bootstrap resampling (Koehn, 2004). 6.2 Main Experimental Results In this section we first perform experiments to investigate the effectiveness of the overall framework of multi-target translation. We assess four models, starting with standard single-target SCFGs and moving gradually towards our full MSCFG model: SCFG: A standard SCFG grammar with only the source and T1. SCFG+T2Al: SCFG constrained during rule extraction to only extract rules that also"
N15-1033,eisele-chen-2010-multiun,0,0.19766,"ulti-target translation, where a second target language is used to assess the quality of the first target language. Introduction In statistical machine translation (SMT), the great majority of work focuses on translation of a single language pair, from the source F to the target E. However, in many actual translation situations, identical documents are translated not from one language to another, but between a large number of different languages. Examples of this abound in commercial translation, and prominent open data sets used widely by the MT community include UN documents in 6 languages (Eisele and Chen, 2010), European Parliament Proceedings in 21 languages 1 Code and data to replicate the experiments can be found at http://phontron.com/project/naacl2015 (Koehn, 2005), and video subtitles on TED in as many as 50 languages (Cettolo et al., 2012). However, despite this abundance of multilingual data, there have been few attempts to take advantage of it. One exception is the multi-source SMT method of Och and Ney (2001), which assumes a situation where we have multiple source sentences, and would like to combine the translations from these sentences to create a better, single target translation. In t"
N15-1033,D07-1091,0,0.0259752,"n for future work is search algorithms that can combine the advantages of these two approaches. 7 Related Work While there is very little previous work on multitarget translation, there is one line of work by Gonz´alez and Casacuberta (2006) and P´erez et al. (2007), which adapts a WFST-based model to output multiple targets. However, this purely monotonic method is unable to perform non-local reordering, and thus is not applicable most language pairs. It is also motivated by efficiency concerns, as opposed to this work’s objective of learning from a T2 language. Factored machine translation (Koehn and Hoang, 2007) is also an example where an LM over a second 6 Results for model score, a more direct measure of search errors, were largely similar. stream of factors (for example POS tags, classes, or lemmas) has been shown to increase accuracy. These factors are limited, however, by the strong constraint of being associated with a single word and not allowing reordering, and thus are not applicable to our setting of using multiple languages. There has also been work on using multiple languages to improve the quality of extracted translation lexicons or topic models (Mausam et al., 2009; Baldwin et al., 20"
N15-1033,N03-1017,0,0.00891142,"of having a strong T2 LM to help with T1 translation, we perform experiments on translation of United Nations documents (Section 6). These experiments, and our subsequent analysis, show that the framework of multi-target translation can, indeed, provide significant gains in accuracy (of up to 1.5 BLEU points), particularly when the two target languages in question are similar. 2 Synchronous Context-Free Grammars We first briefly cover SCFGs, which are widely used in MT, most notably in the framework of hierarchi2 One could also consider a multi-target formulation of phrase-based translation (Koehn et al., 2003), but generating multiple targets while considering reordering in phrase-based search is not trivial. We leave this to future work. 294 (a) SCFG Grammar r1: X → <X1 of the X2, X1 des X2> r2: X → <activity, activités> r3: X → <chambers, chambres> <X1, X1> r1 Derivation <X2 of the X3, X2 des X3> r2 <activity of the X3, activités des X3> r3 <activity of the chambers, activités des chambres> (b) MSCFG Grammar r1: X → <X1 of the X2, X1 des X2, X2 的 X1> r2: X → <activity, activités, 活动 > r3: X → <chambers, chambres, 分庭 > <X1, X1, X1> r1 Derivation <X2 of the X3, X2 des X3, X3 的 X2> r2 <activity of t"
N15-1033,W04-3250,0,0.0404655,"ed, we use joint search with a pop limit of 2,000, and T1 rule pruning with a limit of 10 rules per source rule. BLEU is used for both tuning and evaluating all models. In particular, we tune and evaluate all models based on T1 BLEU, simulating a situation similar to that in the introduction, where we want to use a large LM in T2 to help translation in T1. In order to control for optimizer instability, we follow Clark et al. (2011)’s recommendation of performing tuning 3 times, and reporting the average of the runs along with statistical significance obtained by pairwise bootstrap resampling (Koehn, 2004). 6.2 Main Experimental Results In this section we first perform experiments to investigate the effectiveness of the overall framework of multi-target translation. We assess four models, starting with standard single-target SCFGs and moving gradually towards our full MSCFG model: SCFG: A standard SCFG grammar with only the source and T1. SCFG+T2Al: SCFG constrained during rule extraction to only extract rules that also match the T2 alignments. This will help measure the effect, if any, of being limited by T2 alignments in rule extraction. MSCFG-T2LM: The MSCFG, without using the T2 LM. Compare"
N15-1033,2005.mtsummit-papers.11,0,0.0103944,"he great majority of work focuses on translation of a single language pair, from the source F to the target E. However, in many actual translation situations, identical documents are translated not from one language to another, but between a large number of different languages. Examples of this abound in commercial translation, and prominent open data sets used widely by the MT community include UN documents in 6 languages (Eisele and Chen, 2010), European Parliament Proceedings in 21 languages 1 Code and data to replicate the experiments can be found at http://phontron.com/project/naacl2015 (Koehn, 2005), and video subtitles on TED in as many as 50 languages (Cettolo et al., 2012). However, despite this abundance of multilingual data, there have been few attempts to take advantage of it. One exception is the multi-source SMT method of Och and Ney (2001), which assumes a situation where we have multiple source sentences, and would like to combine the translations from these sentences to create a better, single target translation. In this paper, we propose a framework of multitarget SMT. In multi-target translation, we translate F to not a single target E, but to a set of sentences E = hE1 , E2"
N15-1033,R09-1040,0,0.0448042,"Missing"
N15-1033,P09-1030,0,0.0128469,"ine translation (Koehn and Hoang, 2007) is also an example where an LM over a second 6 Results for model score, a more direct measure of search errors, were largely similar. stream of factors (for example POS tags, classes, or lemmas) has been shown to increase accuracy. These factors are limited, however, by the strong constraint of being associated with a single word and not allowing reordering, and thus are not applicable to our setting of using multiple languages. There has also been work on using multiple languages to improve the quality of extracted translation lexicons or topic models (Mausam et al., 2009; Baldwin et al., 2010; Mimno et al., 2009). These are not concerned with multi-target translation, but may provide us with useful hints about how to generate more effective multi-target translation models. 8 Conclusion In this paper, we have proposed a method for multitarget translation using a generalization of SCFGs, and proposed methods to learn and perform search over the models. In experiments, we found that these models are effective in the case when a strong LM exists in a second target that is highly related to the first target of interest. As the overall framework of multi-target tra"
N15-1033,P04-1084,0,0.0432955,"ciency, and limit the number of terminals to limit model size (in our experiments, we set this limit to five). 4 4.2 X → hγ, α1 , ..., αN i. (4) Training Multi-Synchronous Grammars This section describes how, given a set of parallel sentences in N languages, we can create translation models (TMs) using MSCFGs. 4 We will also make the restriction that indices are linear and non-deleting, indicating that each non-terminal index present in any of the strings will appear exactly once in all of the strings. Thus, MSCFGs can also be thought of as a subset of the “generalized multi-text grammars” of Melamed et al. (2004). 295 MSCFG Rule Extraction In this section, we generalize the rule extraction process in the previous section to accommodate multiple targets. We do so by first independently creating alignments between the source corpus F, and each of N target corpora {E1 , . . . , EN }. Given a particular sentence we now have source F , N target strings {E1 , . . . , EN }, and N alignments {A1 , . . . , AN }. We next independently extract initial phrases for each of the N languages using the standard bilingual phrase-extract algorithm, yielding initial phrase sets {BP1 , . . . , BPN }. Finally, we convert t"
N15-1033,D09-1092,0,0.0299287,"also an example where an LM over a second 6 Results for model score, a more direct measure of search errors, were largely similar. stream of factors (for example POS tags, classes, or lemmas) has been shown to increase accuracy. These factors are limited, however, by the strong constraint of being associated with a single word and not allowing reordering, and thus are not applicable to our setting of using multiple languages. There has also been work on using multiple languages to improve the quality of extracted translation lexicons or topic models (Mausam et al., 2009; Baldwin et al., 2010; Mimno et al., 2009). These are not concerned with multi-target translation, but may provide us with useful hints about how to generate more effective multi-target translation models. 8 Conclusion In this paper, we have proposed a method for multitarget translation using a generalization of SCFGs, and proposed methods to learn and perform search over the models. In experiments, we found that these models are effective in the case when a strong LM exists in a second target that is highly related to the first target of interest. As the overall framework of multi-target translation is broad-reaching, there are a sti"
N15-1033,P13-4016,1,0.739364,"milarity. We use English as our source sentence in all cases, as it is the most common actual source language for UN documents. To prepare the data, we first deduplicate the sentences in the corpus, then hold out 1,500 sentences each for tuning and test. In our basic training setup, we use 100k sentences for training both the TM and the T1 LM. This somewhat small number is to simulate a T1 language that has relatively few resources. For the T2 language, we assume we have a large language model trained on all of the UN data, amounting to 3.5M sentences total. As a decoder, we use the Travatar (Neubig, 2013) toolkit, and implement all necessary extensions to the decoder and rule extraction code to allow for multiple targets. Unless otherwise specified, we use joint search with a pop limit of 2,000, and T1 rule pruning with a limit of 10 rules per source rule. BLEU is used for both tuning and evaluating all models. In particular, we tune and evaluate all models based on T1 BLEU, simulating a situation similar to that in the introduction, where we want to use a large LM in T2 to help translation in T1. In order to control for optimizer instability, we follow Clark et al. (2011)’s recommendation of"
N15-1033,2001.mtsummit-papers.46,0,0.587206,"number of different languages. Examples of this abound in commercial translation, and prominent open data sets used widely by the MT community include UN documents in 6 languages (Eisele and Chen, 2010), European Parliament Proceedings in 21 languages 1 Code and data to replicate the experiments can be found at http://phontron.com/project/naacl2015 (Koehn, 2005), and video subtitles on TED in as many as 50 languages (Cettolo et al., 2012). However, despite this abundance of multilingual data, there have been few attempts to take advantage of it. One exception is the multi-source SMT method of Och and Ney (2001), which assumes a situation where we have multiple source sentences, and would like to combine the translations from these sentences to create a better, single target translation. In this paper, we propose a framework of multitarget SMT. In multi-target translation, we translate F to not a single target E, but to a set of sentences E = hE1 , E2 , . . . , E|E |i in multiple target languages (which we will abbreviate T1, T2, etc.). This, in a way, can be viewed as the automated version of the multi-lingual dissemination of content performed by human translators when creating data for the UN, Eur"
N15-1033,J03-1002,0,0.00528918,"erminal and non-terminal symbols.4 In this paper, for notational convenience, we will use a specialized version of Equation 3 in which we define a single γ as the source side string, and α1 , ...αN as an arbitrary number N of target side strings: 4.1 SCFG Rule Extraction First, we briefly outline rule extraction for SCFGs in the standard two-language case, as proposed by Chiang (2007). We first start by preparing two corpora in the source and target language, F and E, and obtaining word alignments for each sentence automatically, using a technique such as the IBM models implemented by GIZA++ (Och and Ney, 2003). We then extract initial phrases for each sentence. Given a source f1J , target eI1 , and alignment A = {hi1 , i0 1 i, . . . , hi|A |, i0 |A |i} where i and i0 represent indices of aligned words in F and E respectively. First, based on this alignment, we extract all pairs of j0 j0 j |BP | |BP | phrases BP = {hfij11 , ei0 11 i, . . . , hfi|BP , ei0 |BP i}, | | where fij11 is a substring of f1J spanning from i1 to j0 j1 , and ei0 11 is analogous for the target side. The 0 Therefore, at each derivation step, one non-terminal in γ is chosen and all the nonterminals with same indices in α1 , ...,"
N15-1033,W07-0708,0,0.0133987,"Missing"
N15-1033,P12-1001,1,\N,Missing
N15-3009,J93-2004,0,0.0526534,"e of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, (Matsuzaki et al., 2005; Petrov et al., 2006)) framework is among the most popular for several reasons. The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank (Marcus et al., 1993), and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above). The second is the availablity of easy-to-use tools, most notably the Berkeley Parser,2 but also including Egret,3 and BUBS Parser.4 However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our knowledge, all existing tools produce a certain number of failed parses when run on large data sets. In this paper, we introduce Ckylark, a new PCFG-LA parser specifically designed for robustness. Specifically, Ckylark"
N15-3009,P05-1010,0,0.0533528,"ortant from the view of downstream applications is parser robustness, or the ability to return at least some parse regardless of the input. Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, (Matsuzaki et al., 2005; Petrov et al., 2006)) framework is among the most popular for several reasons. The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank (Marcus et al., 1993), and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above). The second is the availablity of easy-to-use tools, most notably the Berkeley Parser,2 but also including Egret,3 and BUBS Parser.4 However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our"
N15-3009,P08-1023,0,0.0266681,"id underflow without other expensive operations: Calculating P (X) is not trivial, but we can retrieve these values using the graph propagation algorithm proposed by Petrov and Klein (2007). 4 Experiments We evaluated parsing accuracies of our parser Ckylark and conventional PCFG-LA parsers: Berkeley Parser and Egret. Berkeley Parser is a conventional PCFG-LA parser written in Java with some additional optimization techniques. Egret is also a conventional PCFG-LA parser in C++ which can generate a parsing forest that can be used in downstream application such forest based machine translation (Mi et al., 2008). Q(X → w) ≡ P ′ (X → w)/sl (w), (2) 4.1 Dataset and Tools Q(X → Y ) ≡ P (X → Y ), (3) Table 1 shows summaries of each dataset. We used GrammarTrainer in the Berkeley Parser to train a PCFG-LA grammar with the Penn Treebank WSJ dataset section 2 to 22 (WSJ-train/dev). Egret and Ckylark can use the same model as the Berkeley Parser so we can evaluate only the performance of the parsers using the same grammar. Each parser is run on a Debian 7.1 machine with an Intel Core i7 CPU (3.40GHz, 4 cores, 8MB caches) and 4GB RAM. We chose 2 datasets to evaluate the performances of each parser. First, WSJ"
N15-3009,P14-2024,1,0.692936,"failure: outputting intermediate results when coarse-to-fine analysis fails, smoothing lexicon probabilities, and scaling probabilities to avoid underflow. An experiment shows that this allows millions of sentences can be parsed without any failures, in contrast to other publicly available PCFG-LA parsers. Ckylark is implemented in C++, and is available opensource under the LGPL license.1 1 Introduction Parsing accuracy is important. Parsing accuracy has been shown to have a significant effect on downstream applications such as textual entailment (Yuret et al., 2010) and machine translation (Neubig and Duh, 2014), and most work on parsing evaluates accuracy to some extent. However, one element that is equally, or perhaps even more, important from the view of downstream applications is parser robustness, or the ability to return at least some parse regardless of the input. Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among"
N15-3009,N07-1051,0,0.0743748,",Z (1) where X is any pre-terminal (part-of-speech) symbol in the grammar, w is any word, and wunk is the unknown word. λ is an interpolation factor between w and wunk , and should be small enough to cause no effect when the parser can generate the result without interpolation. Our implementation uses λ = 10−10 . 3.3 Probability Scaling To solve the problem of underflow, we modify model probabilities as Equations (2) to (4) to avoid underflow without other expensive operations: Calculating P (X) is not trivial, but we can retrieve these values using the graph propagation algorithm proposed by Petrov and Klein (2007). 4 Experiments We evaluated parsing accuracies of our parser Ckylark and conventional PCFG-LA parsers: Berkeley Parser and Egret. Berkeley Parser is a conventional PCFG-LA parser written in Java with some additional optimization techniques. Egret is also a conventional PCFG-LA parser in C++ which can generate a parsing forest that can be used in downstream application such forest based machine translation (Mi et al., 2008). Q(X → w) ≡ P ′ (X → w)/sl (w), (2) 4.1 Dataset and Tools Q(X → Y ) ≡ P (X → Y ), (3) Table 1 shows summaries of each dataset. We used GrammarTrainer in the Berkeley Parser"
N15-3009,P06-1055,0,0.47607,"downstream applications is parser robustness, or the ability to return at least some parse regardless of the input. Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, (Matsuzaki et al., 2005; Petrov et al., 2006)) framework is among the most popular for several reasons. The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank (Marcus et al., 1993), and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above). The second is the availablity of easy-to-use tools, most notably the Berkeley Parser,2 but also including Egret,3 and BUBS Parser.4 However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our knowledge, all existin"
N15-3009,S10-1009,0,0.0488826,"Missing"
N16-1003,W10-2916,0,0.0187418,"replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences that contain phrases that are fre"
N16-1003,P10-1088,0,0.121589,"be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences that contain phrases that are frequent in monolingual data but not in"
N16-1003,P11-2071,0,0.0432275,"Missing"
N16-1003,2005.iwslt-1.7,0,0.645248,"s large corpora can be collected, for example by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a numbe"
N16-1003,2014.amta-workshop.3,0,0.0133335,"filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Select sentences randomly. 4gram-rand: Select n-gram strings of length of up to 4 in random order. sent-by-4gram-freq: Select the sentence including the most frequent uncovered phrase with length of up to 4 words (baseline 1, §3.1). 4gram-freq: Select the most frequent uncovered phrase wi"
N16-1003,E12-1025,0,0.512627,"Missing"
N16-1003,D14-1130,0,0.0163888,"many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences that contain phrases that are frequent in monolingual data but not in bilingual data (Eck et al., 2005), have low confid"
N16-1003,P09-1021,0,0.381297,"web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences tha"
N16-1003,N09-1047,0,0.0215798,"xample by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are metho"
N16-1003,W11-2123,0,0.0105239,"after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Select sentences randomly. 4gram-rand: Select n-gram strings of length of up to 4 in random order. sent-by-4gram-freq: Select the sentence including the most frequent uncovered phrase with length of up to 4 words (baseline 1, §3.1). 4gram-freq: Select the most frequent uncovered phrase with length of up to 4 words (baseline 2, §3.2). maxsubst-freq: Select the most frequent uncovered maximal phrase (proposed, §4.1) reduced-maxsubst-freq: Select the most"
N16-1003,N03-1017,0,0.0128806,"the English-Japanese translation task, we adopted the broad-coverage example sentence corpus provided with the Eijiro dictionary4 as general domain data, and the ASPEC5 scientific paper abstract corpus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github"
N16-1003,P07-2045,0,0.0114659,"dopted the broad-coverage example sentence corpus provided with the Eijiro dictionary4 as general domain data, and the ASPEC5 scientific paper abstract corpus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Sel"
N16-1003,P11-2093,1,0.756098,". En: 46.4M Words Ja: 57.6M Words 1790 Sent. 1790 Sent. Table 1: Details of parallel data from WMT20143 as a base parallel data source and EMEA (Tiedemann, 2009), PatTR (W¨aschle and Riezler, 2012), and Wikipedia titles, used in the medical translation task, as the target domain data. For the English-Japanese translation task, we adopted the broad-coverage example sentence corpus provided with the Eijiro dictionary4 as general domain data, and the ASPEC5 scientific paper abstract corpus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Mos"
N16-1003,J03-1002,0,0.00479721,"pus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Select sentences randomly. 4gram-rand: Select n-gram strings of length of up to 4 in random order. sent-by-4gram-freq: Select the sentence including the most"
N16-1003,P03-1021,0,0.0212038,"To simulate a realistic active learning scenario, we started from given parallel data in the general domain and sequentially added additional source language data in a specific target domain. For the English-French translation task, we adopted the Europarl corpus 2 The method does not distinguish between equivalent word sequences even if they have different tree structures Lang Pair En-Fr En-Ja Domain Dataset General (Base) Train Medical Train (Target) Test Dev General (Base) Train Scientific Train (Target) Test Dev For the tuning of decoding parameters, since it is not realistic to run MERT (Och, 2003) at each retraining step, we tuned the parameters to maximize the BLEU score (Papineni et al., 2002) for the baseline system, and re-used the parameters thereafter. We compare the following 8 segment selection methods, including 2 random selection methods, 2 conventional methods and 4 proposed methods: Amount 1.89M Sent. En: 47.6M Words Fr: 49.4M Words 15.5M Sent. En: 393M Words Fr: 418M Words 1000 Sent. 500 Sent. 414k Sent. En: 6.72M Words Ja: 9.69M Words 1.87M Sent. En: 46.4M Words Ja: 57.6M Words 1790 Sent. 1790 Sent. Table 1: Details of parallel data from WMT20143 as a base parallel data s"
N16-1003,N15-3009,1,0.751844,", §3.2). maxsubst-freq: Select the most frequent uncovered maximal phrase (proposed, §4.1) reduced-maxsubst-freq: Select the most frequent uncovered semi-maximal phrase (proposed, §4.1) struct-freq: Select the most frequent uncovered phrase extracted from the subtrees (proposed, §4.2). reduced-struct-freq: Select the most frequent uncovered semi-maximal phrase extracted from the subtrees (proposed, §4.1 and §4.2). To generate oracle translations, we used an SMT system trained on all of the data in both the general and target-domain corpora. To generate parse trees, we used the Ckylark parser (Oda et al., 2015). 5.2 Results and Discussion Comparison of efficiency: In Figure 3, we show the evaluation score results by the number of additional source words up to 100k and 1M words. We can see that in English-French translation, the accuracy of the selection methods using parse trees grows more rapidly than other methods and was significantly better even at the point of 1M additional words. In the case of English-Japanese translation, the gains over 4-gram frequency are much smaller, but the proposed methods still consistently perform as well or better than the other methods. Besides, in all the graphs w"
N16-1003,P02-1040,0,0.0981299,"the general domain and sequentially added additional source language data in a specific target domain. For the English-French translation task, we adopted the Europarl corpus 2 The method does not distinguish between equivalent word sequences even if they have different tree structures Lang Pair En-Fr En-Ja Domain Dataset General (Base) Train Medical Train (Target) Test Dev General (Base) Train Scientific Train (Target) Test Dev For the tuning of decoding parameters, since it is not realistic to run MERT (Och, 2003) at each retraining step, we tuned the parameters to maximize the BLEU score (Papineni et al., 2002) for the baseline system, and re-used the parameters thereafter. We compare the following 8 segment selection methods, including 2 random selection methods, 2 conventional methods and 4 proposed methods: Amount 1.89M Sent. En: 47.6M Words Fr: 49.4M Words 15.5M Sent. En: 393M Words Fr: 418M Words 1000 Sent. 500 Sent. 414k Sent. En: 6.72M Words Ja: 9.69M Words 1.87M Sent. En: 46.4M Words Ja: 57.6M Words 1790 Sent. 1790 Sent. Table 1: Details of parallel data from WMT20143 as a base parallel data source and EMEA (Tiedemann, 2009), PatTR (W¨aschle and Riezler, 2012), and Wikipedia titles, used in"
N16-1003,J03-3002,0,0.0863344,"Missing"
N16-1003,D08-1112,0,0.0420495,"gual data (Eck et al., 2005), have low confidence according to the MT system (Haffari et al., 2009), or are predicted to be poor translations by an MT quality estimation system (Ananthakrishnan et al., 2010). However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate. To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks (Settles and Craven, 2008; Bloodgood and Callison-Burch, 2010; Tomanek and Hahn, 2009; Sperber et al., 2014). For MT in particular, Bloodgood and Callison-Burch (2010) have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words (§3). Nevertheless, our experiments identified two major practical problems with this method. First, as shown in Figure 1 (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency. Second, it is common to see fragments o"
N16-1003,Q14-1014,1,0.851551,"et al., 2009), or are predicted to be poor translations by an MT quality estimation system (Ananthakrishnan et al., 2010). However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate. To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks (Settles and Craven, 2008; Bloodgood and Callison-Burch, 2010; Tomanek and Hahn, 2009; Sperber et al., 2014). For MT in particular, Bloodgood and Callison-Burch (2010) have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words (§3). Nevertheless, our experiments identified two major practical problems with this method. First, as shown in Figure 1 (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency. Second, it is common to see fragments of complex phrases such as “one of the preceding,” which may be difficult for worker"
N16-1003,P09-1117,0,0.0296401,"the MT system (Haffari et al., 2009), or are predicted to be poor translations by an MT quality estimation system (Ananthakrishnan et al., 2010). However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate. To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks (Settles and Craven, 2008; Bloodgood and Callison-Burch, 2010; Tomanek and Hahn, 2009; Sperber et al., 2014). For MT in particular, Bloodgood and Callison-Burch (2010) have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words (§3). Nevertheless, our experiments identified two major practical problems with this method. First, as shown in Figure 1 (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency. Second, it is common to see fragments of complex phrases such as “one of the preceding,” which may"
N16-1003,W08-0305,0,0.0285635,"Missing"
N16-1003,P11-1122,0,0.0204067,"sed parse subtree selection method Figure 1: Conventional and proposed data selection methods Introduction In statistical machine translation (SMT) (Brown et al., 1993), large quantities of high-quality bilingual data are essential to achieve high translation accuracy. While in many cases large corpora can be collected, for example by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators."
N16-1003,J07-2003,0,\N,Missing
N16-1077,E14-1060,0,0.356302,"roach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflect"
N16-1077,N15-1107,0,0.591763,"on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table contains the inflected for"
N16-1077,D15-1041,1,0.717699,"out of characters. These  character vectors are parameters that are learned by our model, exactly as other character vectors. Regarding the second difference, to provide the model the ability to learn the transformation of semantics from input to output, we apply an affine transformation on the encoded vector e: e ← Wtrans e + btrans (5) where, Wtrans , btrans are the transformation parameters. Also, in the encoder we use a bidirectional LSTM (Graves et al., 2005) instead of a uni-directional LSTM, as it has been shown to capture the sequence information more effectively (Ling et al., 2015; Ballesteros et al., 2015; Bahdanau et al., 2015). Our resultant inflection generation model is shown in Figure 3. 637 4.1 Supervised Learning The parameters of our model are the set of character vectors, the transformation parameters (Wtrans , btrans ), and the parameters of the encoder and decoder LSTMs (§3.2). We use negative loglikelihood of the output character sequence as the loss function: XT 0 −log p(~y |~x) = − log p(yt |e, ~y&lt;t ) (6) t=1 We minimize the loss using stochastic updates with AdaDelta (Zeiler, 2012). This is our purely supervised model for inflection generation and we evaluate it in two different"
N16-1077,D13-1174,1,0.807869,"sult of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an al"
N16-1077,N13-1140,1,0.855557,"sult of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an al"
N16-1077,E03-1009,0,0.0140071,"phology has been particularly useful in statistical machine translation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 641 Figure 5: Plot of character vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al.,"
N16-1077,P11-1004,0,0.0372597,"ses and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed tra"
N16-1077,N15-1140,0,0.0257116,"Missing"
N16-1077,Q15-1031,0,0.0314936,", back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer. Our model obtains state-"
N16-1077,N16-1080,0,0.0507045,"Missing"
N16-1077,D11-1057,0,0.0342392,"004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection"
N16-1077,N13-1138,0,0.784674,"tions. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wik"
N16-1077,P02-1001,0,0.31819,"al. (2015), and Nicolai et al. (2015), by DDN13, AFH14, AFH15, and NCK15 respectively. These models perform inflection generation as string transduction and largely consist of three major components: (1) Character alignment of word forms in a table; (2) Extraction of string transformation rules; (3) Application of rules to new root forms. The first step is learning character alignments across inflected forms in a table. Figure 2 (a) shows alignment between three word forms of Kalb. Different models use different heuristic algorithms for alignments such as edit distance, dynamic edit distance (Eisner, 2002; Oncina and Sebban, 2006), and longest subsequence alignment (Bergroth et al., 2000). Aligning characters across word forms provide spans of characters that have changed and spans that remain unchanged. These spans are used to extract rules for inflection generation for different inflection types as shown in Figure 2 (b)–(d). By applying the extracted rules to new root forms, inflected words can be generated. DDN13 use a semi-Markov model (Sarawagi and Cohen, 2004) to predict what rules should be applied, using character n-grams (n = 1 to 4) as features. AFH14 and AFH15 use substring features"
N16-1077,E12-1068,0,0.158263,"cted forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impra"
N16-1077,H05-1085,0,0.0322115,"n from the back to the front vowels. 7 Related Work Similar to the encoder in our framework, Rastogi et al. (2016) extract sub-word features using a forwardbackward LSTM from a word, and use them in a traditional weighted FST to generate inflected forms. Neural encoder-decoder models of string transduction have also been used for sub-word level transformations like grapheme-to-phoneme conversion (Yao and Zweig, 2015; Rao et al., 2015). Generation of inflectional morphology has been particularly useful in statistical machine translation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 641 Figure 5: Plot of character vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonologic"
N16-1077,D11-1125,0,0.036259,"ed corpus. We use this language model to make predictions about the next character in the sequence given the previous characters, in following two settings. Output Reranking. In the first setting, we first train the inflection generation model using the supervised setting as described in §4.1. While making predictions for inflections, we use beam search to generate possible output character sequences and rerank them using the language model probability along with other easily extractable features as described in Table 2. We use pairwise ranking optimization (PRO) to learn the reranking model (Hopkins and May, 2011). The reranker is trained on the beam output of dev set and evaluated on test set. Language Model Interpolation. In the second setting, we interpolate the probability of observing the next character according to the language model with the probability according to our inflection genferent (equ. 5), and observed consistently worse results. 638 root forms 2764 2027 4055 6400 7249 11200 6957 Infl. 8 27 57 28 53 9 48 Table 3: The number of root forms and types of inflections across datasets. eration model. Thus, the loss function becomes: 1 XT 0 −log p(~y |~x) = − log p(yt |e, ~y&lt;t ) t=1 Z − λlog"
N16-1077,W14-2804,0,0.0911554,"ection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table cont"
N16-1077,J94-3001,0,0.687857,"and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological process"
N16-1077,D15-1176,1,0.0464737,"Missing"
N16-1077,P07-1017,0,0.372308,"m Kalb (calf) when it is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they ca"
N16-1077,Q15-1012,0,0.0435906,"nization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer."
N16-1077,N15-1093,0,0.72903,"state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table contains the inflected form of a given root word"
N16-1077,W04-0409,0,0.110421,"Missing"
N16-1077,J96-1003,0,0.0654284,"al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ("
N16-1077,N09-1024,0,0.0607111,"vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to seq"
N16-1077,N16-1076,0,0.151754,"Missing"
N16-1077,P08-1084,0,0.0451158,"haracter vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural netw"
N16-1077,P08-1059,0,0.285332,"is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1"
N16-1077,W04-0109,0,0.0383169,"(Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large numb"
N16-1077,P00-1027,0,0.147447,"cation of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, ba"
N16-1077,D14-1179,0,\N,Missing
N18-1010,P17-1091,0,0.0392487,"? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumentation quality, which has been the focus of much prior work. Wachsmuth et al. (2017) reviewed theories of argumentation quality assessment and suggested a unified framework. Prior research has focused mainly on the presentation of an argument and some aspects in this framework without considering the OH’s reasoning. Specific examples include politeness, sentiment (Tan et al., 2016; Wei et al., 2016), grammaticality, factuality, topic-relatedness (Habernal and Gurevych, 2016b), argument structure (Niculae et al., 2017), topics (Wang et al., 2017), and argumentative strategies (e.g., anecdote, testimony, statistics) (Al Khatib et al., 2017). Some of these aspects have been used as features to predict debate winners (Wang et al., 2017) and view changes (Tan et al., 2016). Habernal and Gurevych (2016a) used crowdsourcing to develop an ontology of reasons for strong/weak arguments. • RQ3. What kinds of interactions between arguments are captured by the model? We use our model to predict whether a challenger’s argument has impacted the OH’s view and compare the result with several baseline models. We also presen"
N18-1010,D16-1129,0,0.0407885,"more likely to change the OH’s view when addressed? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumentation quality, which has been the focus of much prior work. Wachsmuth et al. (2017) reviewed theories of argumentation quality assessment and suggested a unified framework. Prior research has focused mainly on the presentation of an argument and some aspects in this framework without considering the OH’s reasoning. Specific examples include politeness, sentiment (Tan et al., 2016; Wei et al., 2016), grammaticality, factuality, topic-relatedness (Habernal and Gurevych, 2016b), argument structure (Niculae et al., 2017), topics (Wang et al., 2017), and argumentative strategies (e.g., anecdote, testimony, statistics) (Al Khatib et al., 2017). Some of these aspects have been used as features to predict debate winners (Wang et al., 2017) and view changes (Tan et al., 2016). Habernal and Gurevych (2016a) used crowdsourcing to develop an ontology of reasons for strong/weak arguments. • RQ3. What kinds of interactions between arguments are captured by the model? We use our model to predict whether a challenger’s argument has impacted the OH’s view and compare the result"
N18-1010,D17-1261,0,0.205193,"he joint construction of knowledge. Especially modeling the knowledge co-construction process requires understanding of both the substance of viewpoints and how the substance of an argument connects with what it is arguing against. Prior work on argumentation in the NLP community, however, has focused mainly on the first goal and has often reduced the concept of a viewpoint as a discrete 1 Our code is available at https://github.com/ yohanjo/aim. 103 Proceedings of NAACL-HLT 2018, pages 103–116 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics winners (Potash and Rumshisky, 2017; Zhang et al., 2016; Wang et al., 2017; Prabhakaran et al., 2013) and winning negotiation games (Keizer et al., 2017), this paper addresses a different angle: predicting whether an argument against an OH’s reasoning will successfully impact the OH’s view. Some prior work investigates factors that underlie viewpoint changes (Tan et al., 2016; Lukin et al., 2017; Hidey et al., 2017; Wei et al., 2016), but none target our task of identifying the specific arguments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have"
N18-1010,P16-1150,0,0.0442761,"more likely to change the OH’s view when addressed? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumentation quality, which has been the focus of much prior work. Wachsmuth et al. (2017) reviewed theories of argumentation quality assessment and suggested a unified framework. Prior research has focused mainly on the presentation of an argument and some aspects in this framework without considering the OH’s reasoning. Specific examples include politeness, sentiment (Tan et al., 2016; Wei et al., 2016), grammaticality, factuality, topic-relatedness (Habernal and Gurevych, 2016b), argument structure (Niculae et al., 2017), topics (Wang et al., 2017), and argumentative strategies (e.g., anecdote, testimony, statistics) (Al Khatib et al., 2017). Some of these aspects have been used as features to predict debate winners (Wang et al., 2017) and view changes (Tan et al., 2016). Habernal and Gurevych (2016a) used crowdsourcing to develop an ontology of reasons for strong/weak arguments. • RQ3. What kinds of interactions between arguments are captured by the model? We use our model to predict whether a challenger’s argument has impacted the OH’s view and compare the result"
N18-1010,I13-1042,0,0.024531,"ge co-construction process requires understanding of both the substance of viewpoints and how the substance of an argument connects with what it is arguing against. Prior work on argumentation in the NLP community, however, has focused mainly on the first goal and has often reduced the concept of a viewpoint as a discrete 1 Our code is available at https://github.com/ yohanjo/aim. 103 Proceedings of NAACL-HLT 2018, pages 103–116 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics winners (Potash and Rumshisky, 2017; Zhang et al., 2016; Wang et al., 2017; Prabhakaran et al., 2013) and winning negotiation games (Keizer et al., 2017), this paper addresses a different angle: predicting whether an argument against an OH’s reasoning will successfully impact the OH’s view. Some prior work investigates factors that underlie viewpoint changes (Tan et al., 2016; Lukin et al., 2017; Hidey et al., 2017; Wei et al., 2016), but none target our task of identifying the specific arguments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have been impacted. Particularly, we aim to answer the following quest"
N18-1010,E17-1017,0,0.0130562,"ments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have been impacted. Particularly, we aim to answer the following questions: • RQ1. Does the architecture of vulnerable region detection and interaction encoding help to predict changes in view? • RQ2. Can the model identify vulnerable sentences, which are more likely to change the OH’s view when addressed? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumentation quality, which has been the focus of much prior work. Wachsmuth et al. (2017) reviewed theories of argumentation quality assessment and suggested a unified framework. Prior research has focused mainly on the presentation of an argument and some aspects in this framework without considering the OH’s reasoning. Specific examples include politeness, sentiment (Tan et al., 2016; Wei et al., 2016), grammaticality, factuality, topic-relatedness (Habernal and Gurevych, 2016b), argument structure (Niculae et al., 2017), topics (Wang et al., 2017), and argumentative strategies (e.g., anecdote, testimony, statistics) (Al Khatib et al., 2017). Some of these aspects have been used"
N18-1010,Q17-1016,0,0.247771,"odeling the knowledge co-construction process requires understanding of both the substance of viewpoints and how the substance of an argument connects with what it is arguing against. Prior work on argumentation in the NLP community, however, has focused mainly on the first goal and has often reduced the concept of a viewpoint as a discrete 1 Our code is available at https://github.com/ yohanjo/aim. 103 Proceedings of NAACL-HLT 2018, pages 103–116 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics winners (Potash and Rumshisky, 2017; Zhang et al., 2016; Wang et al., 2017; Prabhakaran et al., 2013) and winning negotiation games (Keizer et al., 2017), this paper addresses a different angle: predicting whether an argument against an OH’s reasoning will successfully impact the OH’s view. Some prior work investigates factors that underlie viewpoint changes (Tan et al., 2016; Lukin et al., 2017; Hidey et al., 2017; Wei et al., 2016), but none target our task of identifying the specific arguments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have been impacted. Particularly, we aim to"
N18-1010,P16-2032,0,0.0473329,"ble at https://github.com/ yohanjo/aim. 103 Proceedings of NAACL-HLT 2018, pages 103–116 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics winners (Potash and Rumshisky, 2017; Zhang et al., 2016; Wang et al., 2017; Prabhakaran et al., 2013) and winning negotiation games (Keizer et al., 2017), this paper addresses a different angle: predicting whether an argument against an OH’s reasoning will successfully impact the OH’s view. Some prior work investigates factors that underlie viewpoint changes (Tan et al., 2016; Lukin et al., 2017; Hidey et al., 2017; Wei et al., 2016), but none target our task of identifying the specific arguments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have been impacted. Particularly, we aim to answer the following questions: • RQ1. Does the architecture of vulnerable region detection and interaction encoding help to predict changes in view? • RQ2. Can the model identify vulnerable sentences, which are more likely to change the OH’s view when addressed? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumenta"
N18-1010,P14-5010,0,\N,Missing
N18-1010,W14-4012,0,\N,Missing
N18-1010,D17-1070,0,\N,Missing
N18-1010,E17-1070,0,\N,Missing
N18-1010,W17-5102,0,\N,Missing
N18-1010,D17-1141,0,\N,Missing
N18-1010,N18-1036,0,\N,Missing
N18-1120,D16-1162,1,0.868289,"based method with respect to accuracy, speed, and simplicity of implementation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translatio"
N18-1120,W17-4716,0,0.0252546,"l corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of transla"
N18-1120,D17-1148,0,0.0329483,"ety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Gre"
N18-1120,W17-4713,0,0.397612,"-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen1 Note that there are existing retrieval-based methods for phrase-based and hierarchical phrase-based translation (Lopez, 2007; Germann, 2015). However, these methods do not improve translation quality but rather aim to improve the efficiency of the translation models. 1325 Proceedings of NAACL-HLT 2018, pages 1325–1335 c New Orleans, Loui"
N18-1120,1999.tc-1.8,0,0.191708,"17), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen1 Note that there are existing"
N18-1120,P07-2045,0,0.00831454,"43.76 50.15 METEOR 36.69 39.50 36.57 39.18 en-fr BLEU 57.26 62.60 57.67 63.27 METEOR 43.51 45.83 43.66 46.24 en-es BLEU 55.76 60.51 55.78 60.54 METEOR 42.53 44.58 42.55 44.64 Table 2: Translation results. TRAIN DEV TEST Average Length en-de 674K 1,636 1,689 31 en-fr 665K 1,733 1,710 29 en-es 663K 1,662 1,696 29 dev test Table 3: Data sets. The last line is the average length of English sentences. directions: English-to-German (en-de), Englishto-French (en-fr) and English-to-Spanish (en-es). We cleaned the data by removing repeated sentences and used the train-truecaser.perl script from Moses (Koehn et al., 2007) to truecase the corpus. Then we selected 2000 sentence pairs as development and test sets, respectively. The rest was used as the training set. We removed sentences longer than 80 and 100 from the training and development/test sets respectively. The final numbers of sentence pairs contained in the training, development and test sets are shown in Table 3.5 We applied byte pair encoding (Sennrich et al., 2016b) and set the vocabulary size to be 20K. For translation piece collection, we use GIZA++ (Och and Ney, 2003) and the grow-diag-final-and heuristic (Koehn et al., 2003) to obtain symmetric"
N18-1120,W17-3204,0,0.0504375,"Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT usi"
N18-1120,N03-1017,0,0.0776975,"translation time, and compares favorably to another alternative retrievalbased method with respect to accuracy, speed, and simplicity of implementation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information abou"
N18-1120,L18-1146,0,0.239139,"Missing"
N18-1120,D07-1104,0,0.0403502,"translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen1 Note that there are existing retrieval-based methods for phrase-based and hierarchical phrase-based translation (Lopez, 2007; Germann, 2015). However, these methods do not improve translation quality but rather aim to improve the efficiency of the translation models. 1325 Proceedings of NAACL-HLT 2018, pages 1325–1335 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Input: requirements Retrieved: requirements Vorschriften in relation in für to the relation die operational to Eignung the von suitability suitability Um@@ of of bulk carriers terminals schlags@@ anlagen Figure 1: A word-aligned sentence pair retrieved for an input sentence. Red words are unedited words obtained"
N18-1120,N18-1031,0,0.0217635,"Missing"
N18-1120,J03-1002,0,0.00854324,"moving repeated sentences and used the train-truecaser.perl script from Moses (Koehn et al., 2007) to truecase the corpus. Then we selected 2000 sentence pairs as development and test sets, respectively. The rest was used as the training set. We removed sentences longer than 80 and 100 from the training and development/test sets respectively. The final numbers of sentence pairs contained in the training, development and test sets are shown in Table 3.5 We applied byte pair encoding (Sennrich et al., 2016b) and set the vocabulary size to be 20K. For translation piece collection, we use GIZA++ (Och and Ney, 2003) and the grow-diag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments for the training set. We trained an attentional NMT model as our baseline system. The settings for NMT are shown in Table 4. We also compared our method with the search engine guided NMT model (SGNMT, Gu et al. (2017)) in Section 4.5. Word embedding GRU dimension Optimizer Initial learning rate Beam size 512 1024 adam 0.0001 5 Table 4: NMT settings. 5 We put the datasets used in our experiments on Github https://github.com/jingyiz/Data-sampled-preprocessed NMT Ours NMT Ours en-de 1.000 1.005 0.995 1"
N18-1120,P16-1162,0,0.472084,", an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrievalbased method with respect to accuracy, speed, and simplicity of implementation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problem"
N18-1120,E17-2058,0,0.0499781,"Missing"
N18-1120,2011.mtsummit-papers.37,1,0.902992,"ables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help"
N18-1120,P17-2089,1,0.897275,"Missing"
N18-1120,W17-4742,0,0.0321322,"Missing"
N18-1120,P17-1139,0,0.0211901,"functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or"
N18-1120,C16-1170,0,0.0180518,"s or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target tech"
N18-1120,W16-2323,0,\N,Missing
N18-1121,D16-1025,0,0.0197097,"em incorporating a more direct representation of context achieves the correct translation (blue words). Definitions of corresponding blue and red words are in parenthesis. Introduction ∗ Charges against four other men were found not proven . target-sentence state, deciding which word to output next. As a result of this ability to capture long-distance dependencies, NMT has achieved great improvements in a number of areas that have bedeviled traditional methods such as phrasebased MT (PBMT; Koehn et al. (2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016). One other phenomenon that was poorly handled by PBMT was homographs – words that have the same surface form but multiple senses. As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b; Pu et al., 2017) or phrase-sense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena. Thus, we may wonder: do NMT systems suffer from the same problems when translating homographs? Or are the recurrent nets applied in the encoding step, and the strong language model in the decoding step enough t"
N18-1121,D17-1151,0,0.0253999,"already captured by the context network, and may not be necessary in the encoder itself. We further compared the two systems on two different languages, French and Chinese. We achieved 0.5-0.8 BLEU improvement, showing our proposed models are stable and consistent across different language pairs. The results are shown in Table 2. To show that our 3-layer models are properly trained, we ran a 3-layer bidirectional encoder with residual networks on En-Fr and got 27.45 for WMT13 and 30.60 for WMT14, which is similarly lower than the two layer result. It should be noted that previous work such as Britz et al. (2017) have 1341 language en → de en → fr en → zh System baseline best baseline best baseline best F1 0.401 0.426 (+0.025) 0.467 0.480 (+0.013) 0.578 0.590 (+0.012) Homograph Precision 0.422 0.449 (+0.027) 0.484 0.496 (+0.012) 0.587 0.599 (+0.012) Recall 0.382 0.405 (+0.023) 0.451 0.465 (+0.014) 0.570 0.581 (+0.011) F1 0.547 0.553 (+0.006) 0.605 0.613 (+0.008) 0.573 0.581 (+0.008) All Words Precision 0.569 0.576 (+0.007) 0.623 0.630 (+0.007) 0.605 0.612 (+0.007) Recall 0.526 0.532 (+0.006) 0.587 0.596 (+0.009) 0.544 0.552 (+0.008) Table 3: Translation results for homographs and all words in our NMT"
N18-1121,P15-1072,0,0.085726,"l translations. F1 score is calculated between the two sets of words. After acquiring the F1 score for each word, we bucket the F1 scores by the number of senses, and plot the average score of four consecutive buckets as shown in Fig. 2. As we can see from the results, the F1 score for words decreases as the number of senses increases for three different language 4 Neural Word Sense Disambiguation Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words. Recent research tackles this problem with neural models and has shown state-of-the art results on WSD datasets (K˚ageb¨ack and Salomonsson, 2016; Yuan et al., 2016). In this section, we will summarize three methods for WSD which we will further utilize as three different context networks to improve NMT. Neural bag-of-words (NBOW) Kalchbrenner et al. (2014); Iyyer et al. (2015) have shown success by representing full sentences with a co"
N18-1121,2007.tmi-papers.6,0,0.172049,"which word to output next. As a result of this ability to capture long-distance dependencies, NMT has achieved great improvements in a number of areas that have bedeviled traditional methods such as phrasebased MT (PBMT; Koehn et al. (2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016). One other phenomenon that was poorly handled by PBMT was homographs – words that have the same surface form but multiple senses. As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b; Pu et al., 2017) or phrase-sense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena. Thus, we may wonder: do NMT systems suffer from the same problems when translating homographs? Or are the recurrent nets applied in the encoding step, and the strong language model in the decoding step enough to alleviate all problems of word sense ambiguity? In §3 we first attempt to answer this question quantitatively by examining the word translation 1336 Proceedings of NAACL-HLT 2018, pages 1336–1345 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Comput"
N18-1121,D07-1007,0,0.442529,"which word to output next. As a result of this ability to capture long-distance dependencies, NMT has achieved great improvements in a number of areas that have bedeviled traditional methods such as phrasebased MT (PBMT; Koehn et al. (2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016). One other phenomenon that was poorly handled by PBMT was homographs – words that have the same surface form but multiple senses. As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b; Pu et al., 2017) or phrase-sense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena. Thus, we may wonder: do NMT systems suffer from the same problems when translating homographs? Or are the recurrent nets applied in the encoding step, and the strong language model in the decoding step enough to alleviate all problems of word sense ambiguity? In §3 we first attempt to answer this question quantitatively by examining the word translation 1336 Proceedings of NAACL-HLT 2018, pages 1336–1345 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Comput"
N18-1121,P07-1005,0,0.104849,"WSD and capturing multi-senses includes work leveraging LSTM (K˚ageb¨ack and Salomonsson, 2016; Yuan et al., 2016), which we extended as a context network in our paper and predicting senses with word ˇ embeddings that capture context. Suster et al. (2016); Kawakami and Dyer (2016) also showed that bilingual data improves WSD. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulated the task of WSD for Statistical Machine Translation (SMT) as predicting possible target translations which directly improves the accuracy of machine translation. Following this reformulation, Chan et al. (2007); Carpuat and Wu (2007a,b) integrated WSD systems into phrase-based systems. Xiong and Zhang (2014) breaks the process into two stages. First predicts the sense of the ambiguous source word. The predicted word senses together with other context features are then used to predict possible target translation. Within the framework of Neural MT, there are works that has similar motivation to ours. Choi et al. (2017) leverage the NBOW as context and gate the word-embedding on both encoder and decoder side. However, their work does not distinguish context vectors for words in the same sequence, in co"
N18-1121,D14-1110,0,0.0641072,"Missing"
N18-1121,J13-3008,0,0.0616187,"Missing"
N18-1121,N13-1073,0,0.411252,". To demonstrate this more concretely, in Fig. 2 we show the translation accuracy of an NMT system with respect to words of varying levels of ambiguity. Specifically, we use the best baseline NMT system to translate three different language pairs from WMT test set (detailed in §6) and plot the F1-score of word translations by the number of senses that they have. The number of senses for a word is acquired from the Cambridge English dictionary,2 after excluding stop words.3 We evaluate the translation performance of words in the source side by aligning them to the target side using fast-align (Dyer et al., 2013). The aligner outputs a set of target words to which the source words aligns for both the reference translation and the model translations. F1 score is calculated between the two sets of words. After acquiring the F1 score for each word, we bucket the F1 scores by the number of senses, and plot the average score of four consecutive buckets as shown in Fig. 2. As we can see from the results, the F1 score for words decreases as the number of senses increases for three different language 4 Neural Word Sense Disambiguation Word sense disambiguation (WSD) is the task of resolving the ambiguity of h"
N18-1121,P15-1162,0,0.026582,"Missing"
N18-1121,W16-5307,0,0.049564,"Missing"
N18-1121,P14-1062,0,0.0270014,"1336 Proceedings of NAACL-HLT 2018, pages 1336–1345 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics accuracy of a baseline NMT system as a function of the number of senses that each word has. Results demonstrate that standard NMT systems make a significant number of errors on homographs, a few of which are shown in Fig. 1. With this result in hand, we propose a method for more directly capturing contextual information that may help disambiguate difficult-to-translate homographs. Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K˚ageb¨ack ˇ and Salomonsson, 2016; Yuan et al., 2016; Suster et al., 2016), examining three methods inspired by this literature (§4). In order to incorporate this information into NMT, we examine two methods: gating the word-embeddings in the model (similarly to Choi et al. (2017)), and concatenating the context-aware representation to the word embedding (§5). To evaluate the effectiveness of our method, we compare our context-aware models with a strong baseline (Luong et al., 2015) on the EnglishGerman, English-French, and English-Chinese WMT dataset. We show that our p"
N18-1121,P17-4012,0,0.0377115,"ing rate of 1 and we begin to halve the learning rate every 1340 epoch once it overfits. 6 (2) We train until the model converges. (i.e. the difference between the perplexity for the current epoch and the previous epoch is less than 0.01) (3) We batched the instances with the same length and our maximum mini-batch size is 256, and (4) the normalized gradient is rescaled whenever its norm exceeds 5. (6) Dropout is applied between vertical RNN stacks with probability 0.3. Additionally, the context network is trained jointly with the encoder-decoder architecture. Our model is built upon OpenNMT (Klein et al., 2017) with the default settings unless otherwise noted. 6.2 Experimental Results In this section, we compare our proposed contextaware NMT models with baseline models on English-German dataset. Our baseline models are encoder-decoder models using global-general attention and input feeding on the decoder side as described in §2, varying the settings on the encoder side. Our proposed model builds upon baseline models by concatenating or gating different types of context vectors. We use LSTM for encoder, decoder, and context network. The decoder is the same across baseline models and proposed models,"
N18-1121,W04-3250,0,0.24346,"context-aware NMT systems with strong baseline models on each dataset. 4 We use the development set as testing data because the official test set hasn’t been released. 5 https://sites.google.com/site/ iwsltevaluation2015/mt-track System en → de baseline best en → fr baseline best en → zh baseline best BLEU WMT’14 WMT’15 21.05 23.83 21.80 24.52 WMT’13 WMT’14 28.21 31.55 28.77 32.39 WMT’17 24.07 24.81 Table 2: Results on three different language pairs - The best proposed models (BiLSTM+Concat+uni) are significantly better (p-value < 0.001) than baseline models using paired bootstrap resampling (Koehn, 2004). 6.1 Training Details We limit our vocabularies to be the top 50K most frequent words for both source and target language. Words not in these shortlisted vocabularies are converted into an hunki token. When training our NMT systems, following Bahdanau et al. (2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle minibatches as we proceed. We train our model with the following settings using SGD as our optimization method. (1) We start with a learning rate of 1 and we begin to halve the learning rate every 1340 epoch once it overfits. 6 (2) We train until the model conv"
N18-1121,P07-2045,0,0.0187514,"used for German and newstest2012 is used for French. For Chinese, we use a combination of News Commentary v12 and the CWMT Corpus as the training set and held out 2357 sentences as the development set. Translation performances are reported in case-sensitive BLEU on newstest2014 (2737 sentences), newstest2015 (2169 sentences) for German, newstest2013 (3000 sentences), newstest2014 (3003 sentences) for French, and newsdev2017 (2002 sentences) for Chinese.4 Details about tokenization are as follows. For German, we use the tokenized dataset from Luong et al. (2015); for French, we used the moses (Koehn et al., 2007) tokenization script with the “-a” flag; for Chinese, we split sequences of Chinese characters, but keep sequences of non-Chinese characters as they are, using the script from IWSLT Evaluation 2015.5 We compare our context-aware NMT systems with strong baseline models on each dataset. 4 We use the development set as testing data because the official test set hasn’t been released. 5 https://sites.google.com/site/ iwsltevaluation2015/mt-track System en → de baseline best en → fr baseline best en → zh baseline best BLEU WMT’14 WMT’15 21.05 23.83 21.80 24.52 WMT’13 WMT’14 28.21 31.55 28.77 32.39 W"
N18-1121,N03-1017,0,0.0725651,". (space) Figure 1: Homographs where the baseline system makes mistakes (red words) but our proposed system incorporating a more direct representation of context achieves the correct translation (blue words). Definitions of corresponding blue and red words are in parenthesis. Introduction ∗ Charges against four other men were found not proven . target-sentence state, deciding which word to output next. As a result of this ability to capture long-distance dependencies, NMT has achieved great improvements in a number of areas that have bedeviled traditional methods such as phrasebased MT (PBMT; Koehn et al. (2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016). One other phenomenon that was poorly handled by PBMT was homographs – words that have the same surface form but multiple senses. As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b; Pu et al., 2017) or phrase-sense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena. Thus, we may wonder: do NMT systems suffer from the same problems when translating homographs? Or ar"
N18-1121,P10-4014,0,0.307501,"source words aligns for both the reference translation and the model translations. F1 score is calculated between the two sets of words. After acquiring the F1 score for each word, we bucket the F1 scores by the number of senses, and plot the average score of four consecutive buckets as shown in Fig. 2. As we can see from the results, the F1 score for words decreases as the number of senses increases for three different language 4 Neural Word Sense Disambiguation Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words. Recent research tackles this problem with neural models and has shown state-of-the art results on WSD datasets (K˚ageb¨ack and Salomonsson, 2016; Yuan et al., 2016). In this section, we will summarize three methods for WSD which we will further utilize as three different context networks to improve NMT. Neural bag-of-words (NBOW) Kalchbrenner et al. (2014);"
N18-1121,D15-1166,0,0.314642,"omographs. Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K˚ageb¨ack ˇ and Salomonsson, 2016; Yuan et al., 2016; Suster et al., 2016), examining three methods inspired by this literature (§4). In order to incorporate this information into NMT, we examine two methods: gating the word-embeddings in the model (similarly to Choi et al. (2017)), and concatenating the context-aware representation to the word embedding (§5). To evaluate the effectiveness of our method, we compare our context-aware models with a strong baseline (Luong et al., 2015) on the EnglishGerman, English-French, and English-Chinese WMT dataset. We show that our proposed model outperforms the baseline in the overall BLEU score across three different language pairs. Quantitative analysis demonstrates that our model performs better on translating homographs. Lastly, we show sample translations of the baseline system and our proposed model. 2 Neural Machine Translation We follow the global-general-attention NMT architecture with input-feeding proposed by Luong et al. (2015), which we will briefly summarize here. The neural network models the conditional distribution"
N18-1121,W04-0838,0,0.162261,"f target words to which the source words aligns for both the reference translation and the model translations. F1 score is calculated between the two sets of words. After acquiring the F1 score for each word, we bucket the F1 scores by the number of senses, and plot the average score of four consecutive buckets as shown in Fig. 2. As we can see from the results, the F1 score for words decreases as the number of senses increases for three different language 4 Neural Word Sense Disambiguation Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words. Recent research tackles this problem with neural models and has shown state-of-the art results on WSD datasets (K˚ageb¨ack and Salomonsson, 2016; Yuan et al., 2016). In this section, we will summarize three methods for WSD which we will further utilize as three different context networks to improve NMT. Neural bag-of-words (NBOW) Kalchbre"
N18-1121,W15-5003,1,0.818278,"but our proposed system incorporating a more direct representation of context achieves the correct translation (blue words). Definitions of corresponding blue and red words are in parenthesis. Introduction ∗ Charges against four other men were found not proven . target-sentence state, deciding which word to output next. As a result of this ability to capture long-distance dependencies, NMT has achieved great improvements in a number of areas that have bedeviled traditional methods such as phrasebased MT (PBMT; Koehn et al. (2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016). One other phenomenon that was poorly handled by PBMT was homographs – words that have the same surface form but multiple senses. As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b; Pu et al., 2017) or phrase-sense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena. Thus, we may wonder: do NMT systems suffer from the same problems when translating homographs? Or are the recurrent nets applied in the encoding step, and the strong language model in"
N18-1121,P96-1006,0,0.709372,"er outputs a set of target words to which the source words aligns for both the reference translation and the model translations. F1 score is calculated between the two sets of words. After acquiring the F1 score for each word, we bucket the F1 scores by the number of senses, and plot the average score of four consecutive buckets as shown in Fig. 2. As we can see from the results, the F1 score for words decreases as the number of senses increases for three different language 4 Neural Word Sense Disambiguation Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words. Recent research tackles this problem with neural models and has shown state-of-the art results on WSD datasets (K˚ageb¨ack and Salomonsson, 2016; Yuan et al., 2016). In this section, we will summarize three methods for WSD which we will further utilize as three different context networks to improve NMT. Neural"
N18-1121,W17-4701,0,0.120577,"ext. As a result of this ability to capture long-distance dependencies, NMT has achieved great improvements in a number of areas that have bedeviled traditional methods such as phrasebased MT (PBMT; Koehn et al. (2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016). One other phenomenon that was poorly handled by PBMT was homographs – words that have the same surface form but multiple senses. As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b; Pu et al., 2017) or phrase-sense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena. Thus, we may wonder: do NMT systems suffer from the same problems when translating homographs? Or are the recurrent nets applied in the encoding step, and the strong language model in the decoding step enough to alleviate all problems of word sense ambiguity? In §3 we first attempt to answer this question quantitatively by examining the word translation 1336 Proceedings of NAACL-HLT 2018, pages 1336–1345 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics"
N18-1121,W17-4702,0,0.115597,"xp(score(g t−1 , hk )) (6) We can then compute at as follows, at = n X αt,k hk (7) k=1 Finally, we compute the distribution over yt as, 1337 ˆ t = tanh(W 1 [g t ; at ]) g (8) ˆt) p(yt |y<t , X) = softmax(W 2 g (9) F1 0.7 0.6 0.5 0.4 0.3 0.2 0.1 pairs. This demonstrates that the translation performance of current NMT systems on words with more senses is significantly decreased from that for words with fewer senses. From this result, it is evident that modern NMT architectures are not enough to resolve the problem of homographs on their own. The result corresponds to the findings in prior work (Rios et al., 2017). en-de en-fr en-zh 0 5 10 15 number of senses 20 Figure 2: Translation performance of words with different numbers of senses. 3 NMT’s Problems with Homographs As described in Eqs. (2) and (3), NMT models encode the words using recurrent encoders, theoretically endowing them with the ability to handle homographs through global sentential context. However, despite the fact that they have this ability, our qualitative observation of NMT results revealed a significant number of ambiguous words being translated incorrectly, casting doubt on whether the standard NMT setup is able to appropriately l"
N18-1121,N16-1160,0,0.0934137,"Missing"
N18-1121,H05-1097,0,0.334056,"standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015). Recent research on tackling WSD and capturing multi-senses includes work leveraging LSTM (K˚ageb¨ack and Salomonsson, 2016; Yuan et al., 2016), which we extended as a context network in our paper and predicting senses with word ˇ embeddings that capture context. Suster et al. (2016); Kawakami and Dyer (2016) also showed that bilingual data improves WSD. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulated the task of WSD for Statistical Machine Translation (SMT) as predicting possible target translations which directly improves the accuracy of machine translation. Following this reformulation, Chan et al. (2007); Carpuat and Wu (2007a,b) integrated WSD systems into phrase-based systems. Xiong and Zhang (2014) breaks the process into two stages. First predicts the sense of the ambiguous source word. The predicted word senses together with other context features are then used to predict possible target translation. Within the framework of Neural MT, there are works that has similar"
N18-1121,P14-1137,0,0.0506407,"Yuan et al., 2016), which we extended as a context network in our paper and predicting senses with word ˇ embeddings that capture context. Suster et al. (2016); Kawakami and Dyer (2016) also showed that bilingual data improves WSD. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulated the task of WSD for Statistical Machine Translation (SMT) as predicting possible target translations which directly improves the accuracy of machine translation. Following this reformulation, Chan et al. (2007); Carpuat and Wu (2007a,b) integrated WSD systems into phrase-based systems. Xiong and Zhang (2014) breaks the process into two stages. First predicts the sense of the ambiguous source word. The predicted word senses together with other context features are then used to predict possible target translation. Within the framework of Neural MT, there are works that has similar motivation to ours. Choi et al. (2017) leverage the NBOW as context and gate the word-embedding on both encoder and decoder side. However, their work does not distinguish context vectors for words in the same sequence, in contrast to the method in this paper, and our results demonstrate that this is an important feature o"
N18-1121,P95-1026,0,0.754706,"or each example, we show sentence in source language (src), the human translated reference (ref), the translation generated by our best context-aware model (best), and the translation generated by baseline model (base). We also highlight the word with multiple senses in source language in bold, the corresponding correctly translated words in blue and wrongly translated words in red. The definitions of words in blue or red are in parenthesis. 7 Related Work Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015). Recent research on tackling WSD and capturing multi-senses includes work leveraging LSTM (K˚ageb¨ack and Salomonsson, 2016; Yuan et al., 2016), which we extended as a context network in our paper and predicting senses with word ˇ embeddings that capture context. Suster et al. (2016); Kawakami and Dyer (2016) also showed that bilingual data improves WSD. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulated th"
N18-1130,J92-1002,0,0.743957,"g et al., 2011; Ling et al., 2015; Kim et al., 2016). Unsupervised morphology has also been shown to improve the representations used by a log-bilinear LM (Botha and Blunsom, 2014). Jozefowicz et al. (2016) explore many interesting such architectures, and compare with fully character-based models. 1442 While these models allow for the elegant encoding of novel word forms they lack an open vocabulary. Open-vocabulary hybrid models alleviate this problem, extending the benefits of character-level representations to the generation. Such hybrid models with open vocabularies have been around since Brown et al. (1992). More recently, Chung et al. (2016) and Hwang and Sung (2016) describe methods of modelling sentences at both the word and character levels, using mechanisms to allow both a word-internal model that captures shortrange dependencies and a word-external model to capture longer-range dependencies. These models have been successfully applied to machine translation by Luong and Manning (2016), who use a character-level model to predict translations of out of vocabulary words. Our work falls in this category—we combine multiple representation levels while maintaining the ability to generate any cha"
N18-1130,N13-1140,1,0.705653,"arly challenging due to the vast set of potential word forms and the sparsity with which they appear in corpora. Traditional closed vocabulary models are unable to produce word forms unseen in training data and unable to generalize sub-word patterns found in data. The most straightforward solution is to treat language as a sequence of characters (Sutskever et al., 2011). However, models that operate at two levels—a character level and a word level— have better performance (Chung et al., 2016). Another solution is to use morphological information, which has shown benefits in non-neural models (Chahuneau et al., 2013). In this paper, we present a model that combines these approaches in a fully neural framework. Our model incorporates explicit morphological knowledge (e.g. from a finite-state morphological analyzer/generator) into a neural language model, combining it with existing word- and characterlevel modelling techniques, in order to create a model capable of successfully modelling morphologically complex languages. In particular, our model achieves three desirable properties. First, it conditions on all available (intrasentential) context, allowing it, in principle, to capture long-range dependencies"
N18-1130,P10-4002,1,0.764207,"wer than 25 times in the training corpus. Arrows indicate line wrapping. 4.1 As an extrinsic evaluation we test whether our language model improves machine translation between Turkish and English. While we could transform our model into a source-conditioned translation model, we choose here to focus on testing our model as an external unconditional language model, leaving the conditional version for future work. Since neural machine translation systems struggle with low-resource languages (Koehn and Knowles, 2017), we choose to introduce the score of our LM as an additional feature to a cdec (Dyer et al., 2010) hierarchical MT system. We train on the WMT 2016 Turkish–English data set, and perform n-best reranking after re-tuning weights with the new feature. The results, shown in Table 4 demonstrate small but significant gains in both directions, particularly into Turkish, where modelling productive morphology should be more important. 4.2 Lang. Pair TR-EN Machine Translation Morphological Disambiguation Our model is a joint model over words and the latent processes giving rise to those words (i.e., which generation process was selected and, for the EN-TR System Baseline Morph. Input Baseline Morph."
N18-1130,W17-3204,0,0.0335844,"forms well on were seen hundreds or thousands of times in the training corpus. Words in bold were seen fewer than 25 times in the training corpus. Arrows indicate line wrapping. 4.1 As an extrinsic evaluation we test whether our language model improves machine translation between Turkish and English. While we could transform our model into a source-conditioned translation model, we choose here to focus on testing our model as an external unconditional language model, leaving the conditional version for future work. Since neural machine translation systems struggle with low-resource languages (Koehn and Knowles, 2017), we choose to introduce the score of our LM as an additional feature to a cdec (Dyer et al., 2010) hierarchical MT system. We train on the WMT 2016 Turkish–English data set, and perform n-best reranking after re-tuning weights with the new feature. The results, shown in Table 4 demonstrate small but significant gains in both directions, particularly into Turkish, where modelling productive morphology should be more important. 4.2 Lang. Pair TR-EN Machine Translation Morphological Disambiguation Our model is a joint model over words and the latent processes giving rise to those words (i.e., wh"
N18-1130,P16-1057,0,0.0619416,"Missing"
N18-1130,D16-1124,1,0.768662,"epresented as a sequence of characters. 2.1 Word generation mixture model In typical RNNLMs the probability of the ith word in a sentence, wi given the preceding words is computed by using an RNN to encode the context followed by a softmax: p(wi |w<i ) = p(wi |hi = ϕRNN (w1 , . . . , wi−1 )) = softmax(Whi + b) where ϕRNN is an RNN that reads a sequence of words and returns a fixed sized vector encoding, W is a weight matrix, and b is a bias. In this work, we will use a mixture model over M different models for generating words in place of the single softmax over words (Miyamoto and Cho, 2016; Neubig and Dyer, 2016): p(wi |hi ) = = M X mi =1 M X mi =1 p(wi , mi |hi ) p(mi |hi )p(wi |hi , mi ), where mi ∈ [1, M ] indicates the model used to generate word wi . To ensure tractability for training and inference, we assume that mi is conditionally independent of all m<i , given the sequence of word forms w<i . We use three (M = 3) component models: (1) directly sampling a word from a finite vocabulary (mi = WORD), (2) generating a word as a sequence of characters (mi = CHARS), and (3) generating as a sequence of (abstract) morphemes which are then stitched together using a handwritten morphological transducer"
N18-1130,C16-1018,1,0.846285,"sentence likelihoods using this model is intractable, but posterior inference over mi and ai is feasible since the normalization factors cancel and therefore do not need to be computed. For our experiments we use the data set of Yuret and Türe (2006) who manually disambiguated from among the possible forms identified by an FST. We significantly out-perform the simple baseline of randomly guessing, and our results are competitive with Yatbaz and Yuret (2009), although they evaluated on a different dataset so they are not directly comparable. Furthermore, we also compare to a supervised model (Shen et al., 2016). While unsupervised techniques can’t hope to exceed supervised accuracies, this comparison provides insight into the difficulty of the problem. See Table 5 for results. 5 Related Work Purely Character-based or Subword-based LMs have a rich history going all the way back to Markov (1906)’s work modelling Russian character-by-character with his namesake models. More recently Sutskever et al. (2011) were the first to apply RNNs to character-level language modelling, leveraging their ability to handle the longrange dependencies required to model language at the character level. It is also possibl"
N18-1130,N06-1042,0,0.0537358,"ective (Besag, 1975). Y LPL = p(wi |w−i ) i = YX i m p(mi = m |w−i )p(wi |m, w−i ) We note that although this model has a very different semantics from the directed one, the PL training objective is identical to the directed model’s, the only difference is that features are based both on the past and future, rather than only the past. Similarly to training, evaluating sentence likelihoods using this model is intractable, but posterior inference over mi and ai is feasible since the normalization factors cancel and therefore do not need to be computed. For our experiments we use the data set of Yuret and Türe (2006) who manually disambiguated from among the possible forms identified by an FST. We significantly out-perform the simple baseline of randomly guessing, and our results are competitive with Yatbaz and Yuret (2009), although they evaluated on a different dataset so they are not directly comparable. Furthermore, we also compare to a supervised model (Shen et al., 2016). While unsupervised techniques can’t hope to exceed supervised accuracies, this comparison provides insight into the difficulty of the problem. See Table 5 for results. 5 Related Work Purely Character-based or Subword-based LMs have"
N18-1130,D15-1176,1,0.907096,"n machine translation and morphological disambiguation tasks. 2 Multi-level RNNLMs Recurrent neural network language models are composed of three parts: (a) an encoder, which turns a context word into a vector, (b) a recurrent backbone that turns a sequence of word vectors that represent the ordered sequence of context vectors into a single vector, and (c) a generator, which assigns a probability to each word that could follow the given context. RNNLMs often use the same process for (a) and (c), but there is no reason why these processes cannot be decoupled. For example, Kim et al. (2016) and Ling et al. (2015) compose character-level representations for their word encoder, but generate words using a softmax whose probabilities rely on inner products between the current context vector and type-specific word embeddings. In our model both the word generator (§2.1) and the word encoder (§2.2) compute representations that leverage three different views of words: frequent words have their own parameters, words that can be analyzed/generated by an analyzer are represented in terms of sequences of abstract morphemes, and all words are represented as a sequence of characters. 2.1 Word generation mixture mod"
N18-1130,P16-1100,0,0.0451799,"Missing"
N18-2084,P16-1101,0,0.00649203,"e when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.1 1 Q1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3) Introduction Q2 Do pre-trained embeddings help more when the size of the training data is small? (§4) Pre-trained word embeddings have proven to be highly useful in neural network models for NLP tasks such as sequence tagging (Lample et al., 2016; Ma and Hovy, 2016) and text classification (Kim, 2014). However, it is much less common to use such pre-training in NMT (Wu et al., 2016), largely because the large-scale training corpora used for tasks such as WMT2 tend to be several orders of magnitude larger than the annotated data available for other tasks, such as the Penn Treebank (Marcus et al., 1993). However, for lowresource languages or domains, it is not necessarily the case that bilingual data is available in abundance, and therefore the effective use of monolingual data becomes a more desirable option. Researchers have worked on a number of methods"
N18-2084,J93-2004,0,0.0617517,"uages? (§3) Introduction Q2 Do pre-trained embeddings help more when the size of the training data is small? (§4) Pre-trained word embeddings have proven to be highly useful in neural network models for NLP tasks such as sequence tagging (Lample et al., 2016; Ma and Hovy, 2016) and text classification (Kim, 2014). However, it is much less common to use such pre-training in NMT (Wu et al., 2016), largely because the large-scale training corpora used for tasks such as WMT2 tend to be several orders of magnitude larger than the annotated data available for other tasks, such as the Penn Treebank (Marcus et al., 1993). However, for lowresource languages or domains, it is not necessarily the case that bilingual data is available in abundance, and therefore the effective use of monolingual data becomes a more desirable option. Researchers have worked on a number of methods for using monolingual data in NMT systems (Cheng et al., 2016; He et al., 2016; Ramachandran et al., 2016). Among these, pre-trained word embeddings have been used either in standard Q3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5) Q4 Is it helpful to align the emb"
N18-2084,P16-1185,0,0.0171519,"r, it is much less common to use such pre-training in NMT (Wu et al., 2016), largely because the large-scale training corpora used for tasks such as WMT2 tend to be several orders of magnitude larger than the annotated data available for other tasks, such as the Penn Treebank (Marcus et al., 1993). However, for lowresource languages or domains, it is not necessarily the case that bilingual data is available in abundance, and therefore the effective use of monolingual data becomes a more desirable option. Researchers have worked on a number of methods for using monolingual data in NMT systems (Cheng et al., 2016; He et al., 2016; Ramachandran et al., 2016). Among these, pre-trained word embeddings have been used either in standard Q3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5) Q4 Is it helpful to align the embedding spaces between the source and target languages? (§6) Q5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? (§7) 2 Experimental Setup In order to perform experiments in a controlled, multilingual setting, we created a parallel corpus from TED talks transcripts.3 Specific"
N18-2084,W17-3203,1,0.593253,"Missing"
N18-2084,N16-1101,0,0.0167082,"mmatically incorrect sentences. The incomprehension of core vocabulary causes deviation of the sentence semantics and thus increases the uncertainty in predicting next words, generating several phrasal loops which are typical in NMT systems. sary in the context of NMT, since the NMT system can already learn a reasonable projection of word embeddings during its normal training process. 7 Analysis Q5: Effect of Multilinguality Finally, it is of interest to consider pre-training in multilingual translation systems that share an encoder or decoder between multiple languages (Johnson et al., 2016; Firat et al., 2016), which is another promising way to use additional data (this time from another language) as a way to improve NMT. Specifically, we train a model using our pairs of similar low-resource and higher-resource languages, and test on only the low-resource language. For those three pairs, the similarity of G L/P T is the highest while B E/RU is the lowest. We report the results in Table 5. When applying pre-trained embeddings, the gains in each translation pair are roughly in order of their similarity, with G L/P T showing the largest gains, and B E/RU showing a small decrease. In addition, it is al"
N18-2084,W17-5708,0,0.0207124,"t6 on Wikipedia7 for each language. These word embeddings (Mikolov et al., 2017) incorporate character-level, phrase-level and positional information of words and are trained using CBOW algorithm (Mikolov et al., 2013). The dimension of word embeddings is set to 300. The embedding layer weights of our model are initialized using these pre-trained word vectors. In baseline models without pre-training, we use Glorot and Bengio (2010)’s uniform initialization. 3 std std providing additional experimental evidence supporting the findings of other recent work on using pre-trained embeddings in NMT (Neishi et al., 2017; Artetxe et al., 2017; Gangi and Federico, 2017), we also examine whether pre-training is useful across a wider variety of language pairs and if it is more useful on the source or target side of a translation pair. The results in Table 2 clearly demonstrate that pre-training the word embeddings in the source and/or target languages helps to increase the BLEU scores to some degree. Comparing the second and third columns, we can see the increase is much more significant with pre-trained source language embeddings. This indicates that the majority of the gain from pre-trained word embeddings res"
N18-2084,W18-1818,1,0.748872,"rols for language characteristics and also improves the possibility of transfer learning in multi-lingual models (in §7). They also represent different language families – G L/P T are Romance; A Z/T R are Turkic; B E/RU are Slavic – allowing for comparison across languages with different caracteristics. Tokenization was done using Moses tokenizer4 and hard punctuation symbols were used to identify sentence boundaries. Table 1 shows data sizes. For our experiments, we use a standard 1-layer encoder-decoder model with attention (Bahdanau et al., 2014) with a beam size of 5 implemented in xnmt5 (Neubig et al., 2018). Training uses a batch size of 32 and the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.0002, decaying the learning rate by 0.5 when development loss decreases (Denkowski and Neubig, 2017). We evaluate the model’s performance using BLEU metric (Papineni et al., 2002). We use available pre-trained word embeddings (Bojanowski et al., 2016) trained using fastText6 on Wikipedia7 for each language. These word embeddings (Mikolov et al., 2017) incorporate character-level, phrase-level and positional information of words and are trained using CBOW algorithm (Mikolov et al.,"
N18-2084,P02-1040,0,0.1146,"acteristics. Tokenization was done using Moses tokenizer4 and hard punctuation symbols were used to identify sentence boundaries. Table 1 shows data sizes. For our experiments, we use a standard 1-layer encoder-decoder model with attention (Bahdanau et al., 2014) with a beam size of 5 implemented in xnmt5 (Neubig et al., 2018). Training uses a batch size of 32 and the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.0002, decaying the learning rate by 0.5 when development loss decreases (Denkowski and Neubig, 2017). We evaluate the model’s performance using BLEU metric (Papineni et al., 2002). We use available pre-trained word embeddings (Bojanowski et al., 2016) trained using fastText6 on Wikipedia7 for each language. These word embeddings (Mikolov et al., 2017) incorporate character-level, phrase-level and positional information of words and are trained using CBOW algorithm (Mikolov et al., 2013). The dimension of word embeddings is set to 300. The embedding layer weights of our model are initialized using these pre-trained word vectors. In baseline models without pre-training, we use Glorot and Bengio (2010)’s uniform initialization. 3 std std providing additional experimental"
N18-2084,D14-1181,0,0.0151792,"ings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.1 1 Q1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3) Introduction Q2 Do pre-trained embeddings help more when the size of the training data is small? (§4) Pre-trained word embeddings have proven to be highly useful in neural network models for NLP tasks such as sequence tagging (Lample et al., 2016; Ma and Hovy, 2016) and text classification (Kim, 2014). However, it is much less common to use such pre-training in NMT (Wu et al., 2016), largely because the large-scale training corpora used for tasks such as WMT2 tend to be several orders of magnitude larger than the annotated data available for other tasks, such as the Penn Treebank (Marcus et al., 1993). However, for lowresource languages or domains, it is not necessarily the case that bilingual data is available in abundance, and therefore the effective use of monolingual data becomes a more desirable option. Researchers have worked on a number of methods for using monolingual data in NMT s"
N19-1010,W04-2209,0,0.009335,"the window. • Word frequency: We anticipate that interpreters often leave rarer source words untranslated because they are probably more difficult to recall from memory. On the other hand, we would expect loan words, words adopted from a foreign language with little or no modification, to be easier to recognize and translate for an interpreter. We extract the binned unigram frequency of the current source word from the large monolingual Google Web 1T Ngrams corpus (Brants and Franz, 2006). We define a loan word as an English word with a Katakana translation in the bilingual dictionaries (eij; Breen, 2004). First, we use the Stanford POS Tagger (Toutanova et al., 2003) on the source subtitle transcripts to identify word chunks with a POS tag in {CD, NN, NNS, NNP, NNPS}, discarding words with other tags. After performing word segmentation on the Japanese data using KyTea (Neubig et al., 2011), we automatically detect for translation coverage between the source subtitles, SI, and translator transcripts with a string-matching program, according to the relevance and coverage tests from §2. The En↔Ja E IJIRO (2.1m entries) (eij) and E DICT (393k entries) (Breen, 2004) bilingual dictionaries are comb"
N19-1010,W04-3250,0,0.0197797,"# POS tag Optimal freq threshold 45.4 49.7 43.6 48.1 29.6 32.9 SVM (all features) − elapsed time − word timing − word freq − characteristic/syntax 58.9 58.8 58.2 59.4 59.3 53.5 53.0 53.2 52.5 55.1 39.1 38.8 38.5 39.1 42.5 Select POS Optimal freq SVM Table 3: Average precision score cross-validation results with feature ablation for the untranslated term class on test data. Optimal word frequency threshold is determined on dev set of each fold. Evaluation performed on a word-level. Highest numbers per column are bolded. Each setting is statistically significant at p < 0.05 by paired bootstrap (Koehn, 2004). Table 4: B-rank output from our model contrasted with baselines. Type I errors are in red, type II errors in orange, and correctly tagged untranslated terminology in blue. interpreter, and observe a decline in performance across all methods with an increase in interpreter experience. We believe that this is due to a decrease in the number of untranslated terminology as experience increases (i.e., class imbalance) coupled with the difficulty of predicting such exclusive word occurrences from only source speech and textual cues. Ablation results in Table 3 show that not all of the features are"
N19-1010,2016.tc-1.5,0,0.0278041,"erpreters to quickly understand the source words and generate accurate translations. Therefore, professional simultaneous interpreters often work in pairs (Mill´an and Bartrina, 2012); while one interpreter performs, the other notes certain challenging items, such as dates, lists, names, or numbers (Jones, 2002). Computers are ideally suited to the task of recalling items given their ability to store large amounts of information, which can be accessed almost instantaneously. As a result, there has been recent interest in developing computer-assisted interpretation (CAI; Plancqueel and Werner; Fantinuoli (2016, 2017b)) tools that have the ability to display glossary terms mentioned by a speaker, such as names, numbers, and entities, to an interpreter in a real-time setting. Such systems have the potential to reduce cognitive load on interpreters by allowing them to concentrate on fluent and accurate production of the target message. These tools rely on automatic speech recognition (ASR) to transcribe the source speech, and display terms occurring in a prepared glossary. While displaying all terminology in a glossary achieves high recall of terms, it suffers from low precision. This could potentiall"
N19-1010,P16-1101,0,0.0165817,"her or not words matching the termhood constraint (in blue) are likely to be left untranslated in SI. allows words that are of other POS tags from being classified as untranslated terminology and greatly reduces the class imbalance issue when training the classifier.3 likely to leave a term untranslated. We thus define these features, and resort to machine-learned classifiers to integrate them and improve performance. State-of-the-art sequence tagging models process sequences in both directions prior to making a globally normalized prediction for each item in the sequence (Huang et al., 2015; Ma and Hovy, 2016). However, the streaming, realtime nature of simultaneous interpretation constrains our model to sequentially process data from left-to-right and make local, monotonic predictions (as noted in Oda et al. (2014); Grissom II et al. (2014), among others). Therefore, we use a sliding-window, linear support vector machine (SVM) classifier (Cortes and Vapnik, 1995; Joachims, 1998) that uses only local features of the history to make independent predictions, as depicted in Fig. 3.2 Formally, given a sequence of source words with their side information (such as timings or POS tags) S = s0:N , we slide"
N19-1010,P11-2093,1,0.686248,"on, to be easier to recognize and translate for an interpreter. We extract the binned unigram frequency of the current source word from the large monolingual Google Web 1T Ngrams corpus (Brants and Franz, 2006). We define a loan word as an English word with a Katakana translation in the bilingual dictionaries (eij; Breen, 2004). First, we use the Stanford POS Tagger (Toutanova et al., 2003) on the source subtitle transcripts to identify word chunks with a POS tag in {CD, NN, NNS, NNP, NNPS}, discarding words with other tags. After performing word segmentation on the Japanese data using KyTea (Neubig et al., 2011), we automatically detect for translation coverage between the source subtitles, SI, and translator transcripts with a string-matching program, according to the relevance and coverage tests from §2. The En↔Ja E IJIRO (2.1m entries) (eij) and E DICT (393k entries) (Breen, 2004) bilingual dictionaries are combined to provide term translations. Additionally, we construct individual dictionaries for each TED talk with key acronyms, proper names, and other exclusive terms (e.g., UNESCO, CO2, conflict-free, Pareto-improving) to increase this automatic coverage. Nouns are lemmatized prior to lookup i"
N19-1010,P14-2090,1,0.848545,"the class imbalance issue when training the classifier.3 likely to leave a term untranslated. We thus define these features, and resort to machine-learned classifiers to integrate them and improve performance. State-of-the-art sequence tagging models process sequences in both directions prior to making a globally normalized prediction for each item in the sequence (Huang et al., 2015; Ma and Hovy, 2016). However, the streaming, realtime nature of simultaneous interpretation constrains our model to sequentially process data from left-to-right and make local, monotonic predictions (as noted in Oda et al. (2014); Grissom II et al. (2014), among others). Therefore, we use a sliding-window, linear support vector machine (SVM) classifier (Cortes and Vapnik, 1995; Joachims, 1998) that uses only local features of the history to make independent predictions, as depicted in Fig. 3.2 Formally, given a sequence of source words with their side information (such as timings or POS tags) S = s0:N , we slide a window W of size k incrementally across S, extracting features φ(si−k+1:i+1 ) from si and its k − 1 predecessors. Since our definition of terminology only allows for nouns and numbers, we restrict prediction"
N19-1010,D14-1140,0,0.0370956,"Missing"
N19-1010,P15-1020,1,0.857376,"specially if working alone. Thus, we monitor the number of minutes elapsed in the talk and the index of the word in the talk/current sentence to inform the classifier. • Word timing: We intuit that a presenter’s quick speaking rate can cause the simultaneous interpreter to potentially drop some terminology. We obtain word timing informa2 We also experimented with a unidirectional LSTM tagger (Hochreiter and Schmidhuber, 1997; Graves, 2012), but found it ineffective on our small amount of annotated data. 3 We note that a streaming POS tagger would have to be used in a real-time setting, as in (Oda et al., 2015). 112 (Shimizu et al., 2014), which consists of source subtitle transcripts, En→Ja offline translations, and interpretations of English TED talk videos from professional simultaneous interpreters with 1, 4, and 15 years of experience, who are dubbed B-rank, A-rank, and S-rank4 . TED talks offer a unique and challenging format for simultaneous interpreters because the speakers typically talk indepth about a single topic, and such there are many new terms that are difficult for an interpreter to process consistently and reliably. The prevalence of this difficult terminology presents an interesti"
N19-1010,P02-1040,0,0.105195,"Missing"
N19-1010,shimizu-etal-2014-collection,1,0.936361,"I) tools that could analyze the spoken word and detect terms likely to be untranslated by an interpreter could reduce translation error and improve interpreter performance. In this paper, we propose a task of predicting which terminology simultaneous interpreters will leave untranslated, and examine methods that perform this task using supervised sequence taggers. We describe a number of task-specific features explicitly designed to indicate when an interpreter may struggle with translating a word. Experimental results on a newly-annotated version of the NAIST Simultaneous Translation Corpus (Shimizu et al., 2014) indicate the promise of our proposed method.1 1 Introduction Simultaneous interpretation (SI) is the act of translating speech in real-time with minimal delay, and is crucial in facilitating international commerce, government meetings, or judicial settings involving non-native language speakers (Bendazzoli and Sandrelli, 2005; Hewitt et al., 1998). However, SI is a cognitively demanding task that requires both active listening to the speaker and careful monitoring of the interpreter’s own output. Even accomplished interpreters with years of training can struggle with unfamiliar concepts, fast"
N19-1010,P18-2105,1,0.848396,"numbers, and entities, to an interpreter in a real-time setting. Such systems have the potential to reduce cognitive load on interpreters by allowing them to concentrate on fluent and accurate production of the target message. These tools rely on automatic speech recognition (ASR) to transcribe the source speech, and display terms occurring in a prepared glossary. While displaying all terminology in a glossary achieves high recall of terms, it suffers from low precision. This could potentially have the unwanted effect of cognitively overwhelming the interpreter with too many term suggestions (Stewart et al., 2018). Thus, an important desideratum of this technology is to only provide terminology Simultaneous interpretation, the translation of speech from one language to another in realtime, is an inherently difficult and strenuous task. One of the greatest challenges faced by interpreters is the accurate translation of difficult terminology like proper names, numbers, or other entities. Intelligent computerassisted interpreting (CAI) tools that could analyze the spoken word and detect terms likely to be untranslated by an interpreter could reduce translation error and improve interpreter performance. In"
N19-1010,N03-1033,0,0.134103,"machine (SVM) classifier (Cortes and Vapnik, 1995; Joachims, 1998) that uses only local features of the history to make independent predictions, as depicted in Fig. 3.2 Formally, given a sequence of source words with their side information (such as timings or POS tags) S = s0:N , we slide a window W of size k incrementally across S, extracting features φ(si−k+1:i+1 ) from si and its k − 1 predecessors. Since our definition of terminology only allows for nouns and numbers, we restrict prediction to words of the corresponding POS tags Q = {CD, NN, NNS, NNP, NNPS} using the Stanford POS tagger (Toutanova et al., 2003). That is, we assign a POS tag pi to each word from si and only extract features/predict using the classifier if pi ∈ Q; otherwise we always assign the Outside tag. This dis3.3 Task-specific Features Due to the fact that only a small amount of humaninterpreted human-annotated data can be created for this task, it is imperative that we give the model the precise information it needs to generalize well. To this end, we propose multiple taskspecific, non-lexical features to inform the classifier about certain patterns that may indicate terminology likely to be left untranslated. • Elapsed time: A"
N19-1119,N18-1118,0,0.0224075,"the sentence length that was proposed earlier (longer sentence scores are products over more terms in [0, 1] and are thus likely to be smaller). We thus propose the following difficulty heuristic: drarity (si ) , − Ni X log pˆ(wki ), (3) k=1 where we use logarithms of word probabilities to prevent numerical errors. Note that negation is used because we define less likely (i.e., more rare) sentences as more difficult. These are just two examples of difficulty metrics, and it is easy to conceive of other metrics such as the occurrence of homographs (Liu et al., 2018) or context-sensitive words (Bawden et al., 2018), the examination of which we leave for future work. 2.2 Competence Functions For this paper, we propose two simple functional forms for c(t) and justify them with some intuition. More sophisticated strategies that depend on the loss function, the loss gradient, or on the learner’s performance on held-out data, are possible, but we do not consider them in this paper. Linear: This is a simple way to define c(t). Given an initial value c0 , c(0) ≥ 0 and a slope parameter r, we define: c(t) , min (1, tr + c0 ) . (4) In this case, new training examples are constantly being introduced during the tr"
N19-1119,P10-1088,0,0.0823459,"Missing"
N19-1119,N09-1047,0,0.0622144,"Missing"
N19-1119,D13-1176,0,0.0572781,"Missing"
N19-1119,kocmi-bojar-2017-curriculum,0,0.147614,"uistics of Elman (1993) and Krueger and Dayan (2009). The main motivation is that training algorithms can perform better if training data is presented in a specific order, starting from easy examples and moving on to more difficult ones, as the learner becomes more competent. In the case of machine learning, it can also be thought of as a means to avoid getting stuck in bad local optima early on in training. An overview of the proposed framework is shown in Figure 1. Notably, we are not the first to examine curriculum learning for NMT, although other related works have met with mixed success. Kocmi and Bojar (2017) explore impact of several curriculum heuristics on training a translation system for a single epoch, presenting the training examples in an easy-to-hard order based on sentence length and vocabulary frequency. However, their strategy introduces all training samples during the first epoch, and how this affects learning in following epochs is not clear, with official evaluation results (Bojar et al., 2017b) indicating that final performance may indeed be hurt with this strategy. Contemporaneously to our work, Zhang et al. (2018) further propose to split the training samples into a predefined nu"
N19-1119,N18-1121,1,0.775598,"length, throughout training. formation about the sentence length that was proposed earlier (longer sentence scores are products over more terms in [0, 1] and are thus likely to be smaller). We thus propose the following difficulty heuristic: drarity (si ) , − Ni X log pˆ(wki ), (3) k=1 where we use logarithms of word probabilities to prevent numerical errors. Note that negation is used because we define less likely (i.e., more rare) sentences as more difficult. These are just two examples of difficulty metrics, and it is easy to conceive of other metrics such as the occurrence of homographs (Liu et al., 2018) or context-sensitive words (Bawden et al., 2018), the examination of which we leave for future work. 2.2 Competence Functions For this paper, we propose two simple functional forms for c(t) and justify them with some intuition. More sophisticated strategies that depend on the loss function, the loss gradient, or on the learner’s performance on held-out data, are possible, but we do not consider them in this paper. Linear: This is a simple way to define c(t). Given an initial value c0 , c(0) ≥ 0 and a slope parameter r, we define: c(t) , min (1, tr + c0 ) . (4) In this case, new training examp"
N19-1119,D18-1039,1,0.936632,"24 GBs of system memory. During training, we use a label smoothing factor of 0.1 (Wu et al., 2016) and the AMSGrad optimizer (Reddi et al., 2018) with its default parameters in TensorFlow, and a batch size of 5,120 tokens 1166 # Train 133k 224k 4.5m # Dev 768 1080 3003 # Test 1268 1133 2999 Plain SL Linear RNN SL Sqrt 30 30.00 SR Sqrt Transformer 27.50 28.00 Table 1: Number of parallel sentences in each dataset. “k” stands for “thousand” and “m” stands for “million”. 25 20 0 5000 Step RNN 10000 Data Preprocessing. Our experiments are performed using the machine translation library released by Platanios et al. (2018). We use the same data preprocessing approach the authors used in their experiments. While training, we consider sentences up to length 200. Similar to them, for the IWSLT-15 experiments we use a per-language vocabulary which contains the 20,000 most frequently We emphasize that we did not run experiments with other architectures or configurations, and thus our baseline architectures were not chosen because they were favorable to our method, but rather because they were frequently mentioned in existing literature. 20 25 0 10000 Step 20000 20 0 50000 Step WMT16 : En → De 30 26.50 BLEU Transform"
N19-1119,P16-1162,0,0.391724,"Missing"
N19-1119,D08-1112,0,0.072346,"l words in a sentence to obtain a single difficulty score for that sentence. Previous research has proposed various pooling operations, such as minimum, maximum, and average (Zhang et al., 2018), but they show that they do not work well in practice. We propose a different approach. Ultimately, what might be most important is the overall likelihood of a sentence as that contains information about both word frequency and, implicitly, sentence length. An approximation to this likelihood is the product of the unigram probabilities, which is related to previous work in the area of active learning (Settles and Craven, 2008). This product can be thought of as an approximate language model (assuming words are sampled independently) and also implicitly incorporates in1 NMT models typically first pick up information about producing sentences of correct length. It can be argued that presenting only short sentences first may lead to learning a strong bias for the sentence lengths. In our experiments, we did not observe this to be an issue as the models kept improving and predicting sentences of correct length, throughout training. formation about the sentence length that was proposed earlier (longer sentence scores ar"
N19-1119,P16-5005,0,0.054726,"Missing"
N19-1119,D13-1141,0,0.0190191,"Missing"
N19-1161,P18-1073,0,0.538937,"apped point in the English space. Introduction Cross-lingual word embeddings represent words in different languages in a single vector space, capturing the syntactic and semantic similarity of words across languages in a way conducive to use in computational models (Upadhyay et al., 2016; Ruder et al., 2017). These embeddings have been shown to be an effective tool for cross-lingual NLP, e.g. the transfer of models trained on highresource languages to low-resource ones (Klementiev et al., 2012; Guo et al., 2015; Zoph et al., 2016; Zhang et al., 2018; Gu et al., 2018) or unsupervised learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Hermann and Blunsom, 2014), optionally augmented with monolingual corpora (Gouws et al., 2015). In contrast, “offline” approaches learn a bilingual mapping function or multilingual projections from pre-trained monolingual word embeddings or feature vectors (Haghighi et al., 2008; Mikolov et al., 2013; Faruqui and Dyer, 2014). In this w"
N19-1161,D18-1399,0,0.411004,"apped point in the English space. Introduction Cross-lingual word embeddings represent words in different languages in a single vector space, capturing the syntactic and semantic similarity of words across languages in a way conducive to use in computational models (Upadhyay et al., 2016; Ruder et al., 2017). These embeddings have been shown to be an effective tool for cross-lingual NLP, e.g. the transfer of models trained on highresource languages to low-resource ones (Klementiev et al., 2012; Guo et al., 2015; Zoph et al., 2016; Zhang et al., 2018; Gu et al., 2018) or unsupervised learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Hermann and Blunsom, 2014), optionally augmented with monolingual corpora (Gouws et al., 2015). In contrast, “offline” approaches learn a bilingual mapping function or multilingual projections from pre-trained monolingual word embeddings or feature vectors (Haghighi et al., 2008; Mikolov et al., 2013; Faruqui and Dyer, 2014). In this w"
N19-1161,D16-1250,0,0.0655146,"Missing"
N19-1161,P17-1042,0,0.0646189,"Missing"
N19-1161,E14-1049,0,0.0710642,"sed learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Hermann and Blunsom, 2014), optionally augmented with monolingual corpora (Gouws et al., 2015). In contrast, “offline” approaches learn a bilingual mapping function or multilingual projections from pre-trained monolingual word embeddings or feature vectors (Haghighi et al., 2008; Mikolov et al., 2013; Faruqui and Dyer, 2014). In this work, we focus on this latter offline approach. The goal of bilingual embedding is to learn a shared embedding space where words possessing similar meanings are projected to nearby points. Early work focused on supervised methods maximizes the similarity of the embeddings of words that exist in a manually-created dictionary, according to some similarity metric (Mikolov et al., 2013; Faruqui and Dyer, 2014; Jawanpuria et al., 2018; Joulin et al., 2018). In contrast, recently proposed unsupervised methods frame this problem as minimization of some form of distance between the whole set"
N19-1161,N18-1032,0,0.0135984,"embedding space while the blue point is a mapped point in the English space. Introduction Cross-lingual word embeddings represent words in different languages in a single vector space, capturing the syntactic and semantic similarity of words across languages in a way conducive to use in computational models (Upadhyay et al., 2016; Ruder et al., 2017). These embeddings have been shown to be an effective tool for cross-lingual NLP, e.g. the transfer of models trained on highresource languages to low-resource ones (Klementiev et al., 2012; Guo et al., 2015; Zoph et al., 2016; Zhang et al., 2018; Gu et al., 2018) or unsupervised learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Hermann and Blunsom, 2014), optionally augmented with monolingual corpora (Gouws et al., 2015). In contrast, “offline” approaches learn a bilingual mapping function or multilingual projections from pre-trained monolingual word embeddings or feature vectors (Haghighi et al., 2008; Mikolov e"
N19-1161,P15-1119,0,0.0245636,"nk point is a continuous training sample in the Japanese embedding space while the blue point is a mapped point in the English space. Introduction Cross-lingual word embeddings represent words in different languages in a single vector space, capturing the syntactic and semantic similarity of words across languages in a way conducive to use in computational models (Upadhyay et al., 2016; Ruder et al., 2017). These embeddings have been shown to be an effective tool for cross-lingual NLP, e.g. the transfer of models trained on highresource languages to low-resource ones (Klementiev et al., 2012; Guo et al., 2015; Zoph et al., 2016; Zhang et al., 2018; Gu et al., 2018) or unsupervised learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Hermann and Blunsom, 2014), optionally augmented with monolingual corpora (Gouws et al., 2015). In contrast, “offline” approaches learn a bilingual mapping function or multilingual projections from pre-trained monolingual word embedd"
N19-1161,P08-1088,0,0.0642161,"g et al., 2018; Gu et al., 2018) or unsupervised learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Hermann and Blunsom, 2014), optionally augmented with monolingual corpora (Gouws et al., 2015). In contrast, “offline” approaches learn a bilingual mapping function or multilingual projections from pre-trained monolingual word embeddings or feature vectors (Haghighi et al., 2008; Mikolov et al., 2013; Faruqui and Dyer, 2014). In this work, we focus on this latter offline approach. The goal of bilingual embedding is to learn a shared embedding space where words possessing similar meanings are projected to nearby points. Early work focused on supervised methods maximizes the similarity of the embeddings of words that exist in a manually-created dictionary, according to some similarity metric (Mikolov et al., 2013; Faruqui and Dyer, 2014; Jawanpuria et al., 2018; Joulin et al., 2018). In contrast, recently proposed unsupervised methods frame this problem as minimization"
N19-1161,D18-1160,1,0.835163,"al embedding spaces. 2 To learn in this paradigm, instead of using the pre-trained word embeddings as fixed training samples, at every training step we obtain samples from the Gaussian mixture space. Thus, our method is exploring the entire embedding space instead of only the specific points assigned for observed words. To calculate the density of the transformed samples, we use volume-preserving invertible transformations over the target word embeddings, which make it possible to perform density matching in a principled and efficient way (Rezende and Mohamed, 2015; Papamakarios et al., 2017; He et al., 2018). We also have three additional ingredients in the model that proved useful in stabilizing training: (1) a back-translation loss to allow the model to learn the mapping jointly in both directions, (2) an identical-word-matching loss that provides weak supervision by encouraging the model to have words with identical spellings be mapped to a similar place in the space, and (3) frequency-matching based Gaussian mixture weights that accounts for the approximate frequencies of aligned words. Empirical results are strong; our method is able to effectively learn bilingual embeddings Background: Norm"
N19-1161,P14-1006,0,0.0259334,"2016; Ruder et al., 2017). These embeddings have been shown to be an effective tool for cross-lingual NLP, e.g. the transfer of models trained on highresource languages to low-resource ones (Klementiev et al., 2012; Guo et al., 2015; Zoph et al., 2016; Zhang et al., 2018; Gu et al., 2018) or unsupervised learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Hermann and Blunsom, 2014), optionally augmented with monolingual corpora (Gouws et al., 2015). In contrast, “offline” approaches learn a bilingual mapping function or multilingual projections from pre-trained monolingual word embeddings or feature vectors (Haghighi et al., 2008; Mikolov et al., 2013; Faruqui and Dyer, 2014). In this work, we focus on this latter offline approach. The goal of bilingual embedding is to learn a shared embedding space where words possessing similar meanings are projected to nearby points. Early work focused on supervised methods maximizes the similarity of the embeddings of words that exi"
N19-1161,D18-1043,0,0.220196,"Missing"
N19-1161,D18-1330,0,0.156344,"Missing"
N19-1161,C12-1089,0,0.11088,"to word frequency. The pink point is a continuous training sample in the Japanese embedding space while the blue point is a mapped point in the English space. Introduction Cross-lingual word embeddings represent words in different languages in a single vector space, capturing the syntactic and semantic similarity of words across languages in a way conducive to use in computational models (Upadhyay et al., 2016; Ruder et al., 2017). These embeddings have been shown to be an effective tool for cross-lingual NLP, e.g. the transfer of models trained on highresource languages to low-resource ones (Klementiev et al., 2012; Guo et al., 2015; Zoph et al., 2016; Zhang et al., 2018; Gu et al., 2018) or unsupervised learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Hermann and Blunsom, 2014), optionally augmented with monolingual corpora (Gouws et al., 2015). In contrast, “offline” approaches learn a bilingual mapping function or multilingual projections from pre-trained monol"
N19-1161,P18-1072,0,0.0590911,"Missing"
N19-1161,P16-1157,0,0.0330231,"mologically distant and/or morphologically rich languages.1 English Space 猫 canine mapping function ⽝犬 cat bird Figure 1: An illustration of our method. Thicker lines represent higher mixture weights, linked to word frequency. The pink point is a continuous training sample in the Japanese embedding space while the blue point is a mapped point in the English space. Introduction Cross-lingual word embeddings represent words in different languages in a single vector space, capturing the syntactic and semantic similarity of words across languages in a way conducive to use in computational models (Upadhyay et al., 2016; Ruder et al., 2017). These embeddings have been shown to be an effective tool for cross-lingual NLP, e.g. the transfer of models trained on highresource languages to low-resource ones (Klementiev et al., 2012; Guo et al., 2015; Zoph et al., 2016; Zhang et al., 2018; Gu et al., 2018) or unsupervised learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Herma"
N19-1161,N18-1190,0,0.0242791,"Zhang et al., 2017; Grave 1588 Proceedings of NAACL-HLT 2019, pages 1588–1598 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2018). While these methods have shown impressive results for some language pairs despite the lack of supervision, regarding the embedding space as a set of discrete points has some limitations. First, expressing embeddings as a single point in the space doesn’t take into account the inherent uncertainty involved in learning embeddings, which can cause embedding spaces to differ significantly between training runs (Wendlandt et al., 2018). Second, even in a fixed embedding space the points surrounding those of words that actually exist in the pre-trained vocabulary also often are coherent points in the embedding space. that achieve competitive or superior results on the MUSE dataset (Conneau et al., 2017) over state-of-the-art published results on bilingual word translation and cross-lingual word similarity tasks. The results are particularly encouraging on etymologically distant or morphologically rich languages, as our model is able to explore the integration over the embedding space by treating the space as a continuous one"
N19-1161,N15-1104,0,0.113042,"Missing"
N19-1161,D18-1268,0,0.605082,"ed embedding space where words possessing similar meanings are projected to nearby points. Early work focused on supervised methods maximizes the similarity of the embeddings of words that exist in a manually-created dictionary, according to some similarity metric (Mikolov et al., 2013; Faruqui and Dyer, 2014; Jawanpuria et al., 2018; Joulin et al., 2018). In contrast, recently proposed unsupervised methods frame this problem as minimization of some form of distance between the whole set of discrete word vectors in the chosen vocabulary, e.g. Wasserstein distance or Jensen–Shannon divergence (Xu et al., 2018; Conneau et al., 2017; Zhang et al., 2017; Grave 1588 Proceedings of NAACL-HLT 2019, pages 1588–1598 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2018). While these methods have shown impressive results for some language pairs despite the lack of supervision, regarding the embedding space as a set of discrete points has some limitations. First, expressing embeddings as a single point in the space doesn’t take into account the inherent uncertainty involved in learning embeddings, which can cause embedding spaces to differ significantly"
N19-1161,D17-1207,0,0.263655,"ng similar meanings are projected to nearby points. Early work focused on supervised methods maximizes the similarity of the embeddings of words that exist in a manually-created dictionary, according to some similarity metric (Mikolov et al., 2013; Faruqui and Dyer, 2014; Jawanpuria et al., 2018; Joulin et al., 2018). In contrast, recently proposed unsupervised methods frame this problem as minimization of some form of distance between the whole set of discrete word vectors in the chosen vocabulary, e.g. Wasserstein distance or Jensen–Shannon divergence (Xu et al., 2018; Conneau et al., 2017; Zhang et al., 2017; Grave 1588 Proceedings of NAACL-HLT 2019, pages 1588–1598 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2018). While these methods have shown impressive results for some language pairs despite the lack of supervision, regarding the embedding space as a set of discrete points has some limitations. First, expressing embeddings as a single point in the space doesn’t take into account the inherent uncertainty involved in learning embeddings, which can cause embedding spaces to differ significantly between training runs (Wendlandt et al.,"
N19-1161,D16-1163,0,0.0277237,"inuous training sample in the Japanese embedding space while the blue point is a mapped point in the English space. Introduction Cross-lingual word embeddings represent words in different languages in a single vector space, capturing the syntactic and semantic similarity of words across languages in a way conducive to use in computational models (Upadhyay et al., 2016; Ruder et al., 2017). These embeddings have been shown to be an effective tool for cross-lingual NLP, e.g. the transfer of models trained on highresource languages to low-resource ones (Klementiev et al., 2012; Guo et al., 2015; Zoph et al., 2016; Zhang et al., 2018; Gu et al., 2018) or unsupervised learning (Artetxe et al., 2018c). 1 Code/scripts can be found at https://github. com/violet-zct/DeMa-BWE. There are two major paradigms in the learning of cross-lingual word embeddings: “online” and “offline”. “Online” methods learn the crosslingual embeddings directly from parallel corpora (Hermann and Blunsom, 2014), optionally augmented with monolingual corpora (Gouws et al., 2015). In contrast, “offline” approaches learn a bilingual mapping function or multilingual projections from pre-trained monolingual word embeddings or feature vec"
N19-1161,Q17-1010,0,\N,Missing
N19-1190,D11-1033,0,0.04654,"negatively affect performance. Human generated text on the internet and social media are a particularly rich source of natural noise (Eisenstein, 2013; Baldwin et al., 2015) which causes pronounced problems for MT (Michel and Neubig, 2018). Robustness to noise in MT can be treated as a domain adaptation problem (Koehn and Knowles, 2017) and several attempts have been made to 1916 Proceedings of NAACL-HLT 2019, pages 1916–1920 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics handle noise from this perspective. Notable approaches (Li et al., 2010; Axelrod et al., 2011) include training on varying amounts of data from the target domain. Luong and Manning (2015) suggest the use of fine-tuning on varying amounts of target domain data, and Barone et al. (2017) note a logarithmic relationship between the amount of data used in fine-tuning and the relative success of MT models. Other approaches to domain adaptation include weighting of domains in the system objective function (Wang et al., 2017) and specifically curated datasets for adaptation (Blodgett et al., 2017). Kobus et al. (2016) introduce a method of domain tagging to assist neural models in differentiat"
N19-1190,W15-4319,0,0.0281574,"Missing"
N19-1190,2015.iwslt-evaluation.11,0,0.234432,"e propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom 1 . 1 2. Are we able to improve the performance of vanilla MT systems on noisy data by leveraging artificially generated noise? Introduction Machine Translation (MT) systems have been shown to exhibit severely degraded performance when required to translate of out-of-domain or noisy data (Luong and Manning, 2015; Sakaguchi et al., 2016; Belinkov and Bisk, 2017). This is particularly pronounced when systems trained on clean, formalized parallel data such as Europarl (Koehn, 2005), are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance (Michel and Neubig, 2018). Improving the robustness of MT systems to naturally occurring noise presents an important and interesting task. Recent work on MT robustness (Belinkov and Bisk, 2017) has demonstrated the need to build or adapt systems that a"
N19-1190,D18-1050,1,0.928866,"systems on noisy data by leveraging artificially generated noise? Introduction Machine Translation (MT) systems have been shown to exhibit severely degraded performance when required to translate of out-of-domain or noisy data (Luong and Manning, 2015; Sakaguchi et al., 2016; Belinkov and Bisk, 2017). This is particularly pronounced when systems trained on clean, formalized parallel data such as Europarl (Koehn, 2005), are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance (Michel and Neubig, 2018). Improving the robustness of MT systems to naturally occurring noise presents an important and interesting task. Recent work on MT robustness (Belinkov and Bisk, 2017) has demonstrated the need to build or adapt systems that are resilient to such noise. We approach the problem of adapting to noisy data aiming to answer two primary research questions: ∗ These authors contributed equally Code available at https://github.com/ MysteryVaibhav/robust_mtnt 1 In this work we present two primary methods of synthesizing natural noise, in accordance with the types of noise identified in prior work as na"
N19-1190,D17-1156,0,0.0167174,"ounced problems for MT (Michel and Neubig, 2018). Robustness to noise in MT can be treated as a domain adaptation problem (Koehn and Knowles, 2017) and several attempts have been made to 1916 Proceedings of NAACL-HLT 2019, pages 1916–1920 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics handle noise from this perspective. Notable approaches (Li et al., 2010; Axelrod et al., 2011) include training on varying amounts of data from the target domain. Luong and Manning (2015) suggest the use of fine-tuning on varying amounts of target domain data, and Barone et al. (2017) note a logarithmic relationship between the amount of data used in fine-tuning and the relative success of MT models. Other approaches to domain adaptation include weighting of domains in the system objective function (Wang et al., 2017) and specifically curated datasets for adaptation (Blodgett et al., 2017). Kobus et al. (2016) introduce a method of domain tagging to assist neural models in differentiating domains. Whilst the above approaches have shown success in specifically adapting across domains, we contend that adaptation to noise is a nuanced task and treating the problem as a simple"
N19-1190,W17-4408,0,0.0286286,"Missing"
N19-1190,N13-1037,0,0.0839259,"ring noise presents an important and interesting task. Recent work on MT robustness (Belinkov and Bisk, 2017) has demonstrated the need to build or adapt systems that are resilient to such noise. We approach the problem of adapting to noisy data aiming to answer two primary research questions: ∗ These authors contributed equally Code available at https://github.com/ MysteryVaibhav/robust_mtnt 1 In this work we present two primary methods of synthesizing natural noise, in accordance with the types of noise identified in prior work as naturally occurring in internet and social media based text (Eisenstein, 2013; Michel and Neubig, 2018). Specifically, we introduce a synthetic noise induction model which heuristically introduces types of noise unique to social media text and labeled back translation (Sennrich et al., 2015a), a data-driven method to emulate target noise. We present a series of experiments based on the Machine Translation of Noisy Text (MTNT) data set (Michel and Neubig, 2018) through which we demonstrate improved resilience of a vanilla MT system by adaptation using artificially noised data. 2 Related Work Szegedy et al. (2013) demonstrate the fragility of neural networks to noisy inp"
N19-1190,W18-2709,0,0.0421295,"odel which heuristically introduces types of noise unique to social media text and labeled back translation (Sennrich et al., 2015a), a data-driven method to emulate target noise. We present a series of experiments based on the Machine Translation of Noisy Text (MTNT) data set (Michel and Neubig, 2018) through which we demonstrate improved resilience of a vanilla MT system by adaptation using artificially noised data. 2 Related Work Szegedy et al. (2013) demonstrate the fragility of neural networks to noisy input. This fragility has been shown to extend to MT systems (Belinkov and Bisk, 2017; Khayrallah and Koehn, 2018) where both artificial and natural noise are shown to negatively affect performance. Human generated text on the internet and social media are a particularly rich source of natural noise (Eisenstein, 2013; Baldwin et al., 2015) which causes pronounced problems for MT (Michel and Neubig, 2018). Robustness to noise in MT can be treated as a domain adaptation problem (Koehn and Knowles, 2017) and several attempts have been made to 1916 Proceedings of NAACL-HLT 2019, pages 1916–1920 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics handle noise from th"
N19-1190,D17-1155,0,0.0235793,"Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics handle noise from this perspective. Notable approaches (Li et al., 2010; Axelrod et al., 2011) include training on varying amounts of data from the target domain. Luong and Manning (2015) suggest the use of fine-tuning on varying amounts of target domain data, and Barone et al. (2017) note a logarithmic relationship between the amount of data used in fine-tuning and the relative success of MT models. Other approaches to domain adaptation include weighting of domains in the system objective function (Wang et al., 2017) and specifically curated datasets for adaptation (Blodgett et al., 2017). Kobus et al. (2016) introduce a method of domain tagging to assist neural models in differentiating domains. Whilst the above approaches have shown success in specifically adapting across domains, we contend that adaptation to noise is a nuanced task and treating the problem as a simple domain adaptation task may fail to fully account for the varied types of noise that can occur in internet and social media text. Experiments that specifically handle noise include text normalization approaches (Baldwin et al., 2015) and"
N19-1190,N18-2084,1,0.82594,"the success of finetuning which we leverage in the current work. The dataset consists of naturally noisy data from social media sources in both English-French and English-Japanese pairs. In our experimentation we utilize the subset of the data for English to French which contains data scraped from Reddit2 . The data set contains training, validation and test data. The training data is used in fine-tuning of our model as outlined below. All results are reported on the MTNT test set for French-English. We additionally use other datasets including Europarl (EP) (Koehn, 2005) and TED talks (TED) (Ye et al., 2018) for training our models as described in §5. 2 www.reddit.com # Sentences Pruned Size Europarl (EP) Ted talk (TED) Noisy Text (MTNT) 2,007,723 192,304 19,161 1,859,898 181,582 18,112 Table 1: Statistics about different datasets used in our experiments. We prune each dataset to retain sentences with length ≤ 50. 4 Baseline Model Our baseline MT model architecture consists of a bidirectional Long Short-Term Memory (LSTM) network encoder-decoder model with two layers. The hidden and embedding sizes are set to 256 and 512, respectively. We also employ weighttying (Press and Wolf, 2016) between the"
N19-1190,2005.mtsummit-papers.11,0,0.203647,"make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom 1 . 1 2. Are we able to improve the performance of vanilla MT systems on noisy data by leveraging artificially generated noise? Introduction Machine Translation (MT) systems have been shown to exhibit severely degraded performance when required to translate of out-of-domain or noisy data (Luong and Manning, 2015; Sakaguchi et al., 2016; Belinkov and Bisk, 2017). This is particularly pronounced when systems trained on clean, formalized parallel data such as Europarl (Koehn, 2005), are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance (Michel and Neubig, 2018). Improving the robustness of MT systems to naturally occurring noise presents an important and interesting task. Recent work on MT robustness (Belinkov and Bisk, 2017) has demonstrated the need to build or adapt systems that are resilient to such noise. We approach the problem of adapting to noisy data aiming to answer two primary research questions: ∗ These authors contributed equally Code av"
N19-1190,W17-3204,0,0.027464,"artificially noised data. 2 Related Work Szegedy et al. (2013) demonstrate the fragility of neural networks to noisy input. This fragility has been shown to extend to MT systems (Belinkov and Bisk, 2017; Khayrallah and Koehn, 2018) where both artificial and natural noise are shown to negatively affect performance. Human generated text on the internet and social media are a particularly rich source of natural noise (Eisenstein, 2013; Baldwin et al., 2015) which causes pronounced problems for MT (Michel and Neubig, 2018). Robustness to noise in MT can be treated as a domain adaptation problem (Koehn and Knowles, 2017) and several attempts have been made to 1916 Proceedings of NAACL-HLT 2019, pages 1916–1920 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics handle noise from this perspective. Notable approaches (Li et al., 2010; Axelrod et al., 2011) include training on varying amounts of data from the target domain. Luong and Manning (2015) suggest the use of fine-tuning on varying amounts of target domain data, and Barone et al. (2017) note a logarithmic relationship between the amount of data used in fine-tuning and the relative success of MT models. Other ap"
N19-1190,C10-1075,0,0.031531,"oise are shown to negatively affect performance. Human generated text on the internet and social media are a particularly rich source of natural noise (Eisenstein, 2013; Baldwin et al., 2015) which causes pronounced problems for MT (Michel and Neubig, 2018). Robustness to noise in MT can be treated as a domain adaptation problem (Koehn and Knowles, 2017) and several attempts have been made to 1916 Proceedings of NAACL-HLT 2019, pages 1916–1920 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics handle noise from this perspective. Notable approaches (Li et al., 2010; Axelrod et al., 2011) include training on varying amounts of data from the target domain. Luong and Manning (2015) suggest the use of fine-tuning on varying amounts of target domain data, and Barone et al. (2017) note a logarithmic relationship between the amount of data used in fine-tuning and the relative success of MT models. Other approaches to domain adaptation include weighting of domains in the system objective function (Wang et al., 2017) and specifically curated datasets for adaptation (Blodgett et al., 2017). Kobus et al. (2016) introduce a method of domain tagging to assist neural"
N19-1190,kobus-etal-2017-domain,0,\N,Missing
N19-1314,D18-1316,0,0.653847,"Knight, 2016) or toxicity (Hosseini et al., 2017) classification to cite a few. In MT, methods have been proposed to attack word-based (Zhao et al., 2018; Cheng et al., 2018) and character-based (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a) models. However these works side-step the question of meaning preservation in the source: they mostly focus on target side evaluation. Finally there is work centered around meaning-preserving adversarial attacks for NLP via paraphrase generation (Iyyer et al., 2018) or rule-based approaches (Jia and Liang, 2017; Ribeiro et al., 2018; Naik et al., 2018; Alzantot et al., 2018). However the proposed attacks are highly engineered and focused on English. 7 Conclusion This paper highlights the importance of performing meaning-preserving adversarial perturbations for NLP models (with a focus on seq2seq). We proposed a general evaluation framework for adversarial perturbations and compared various automatic metrics as proxies for human judgment to instantiate this framework. We then confirmed that, in the context of MT, “naive” attacks do not preserve meaning in general, and proposed alternatives to remedy this issue. Finally, we have shown the utility of adversarial tra"
N19-1314,S17-2001,0,0.198989,"continuous, making minuscule perturbations largely imperceptible to the human eye. In discrete spaces such as natural language sentences, the situation is more problematic; even a flip of a single word or character is generally perceptible by a human reader. Thus, most of the mathematical framework in previous work is not directly applicable to discrete text data. Moreover, there is no canonical distance metric for textual data like the `p norm in real-valued vector spaces such as images, and evaluating the level of semantic similarity between two sentences is a field of research of its own (Cer et al., 2017). This elicits a natural question: what does the term “adversarial perturbation” mean in the context of natural language processing (NLP)? We propose a simple but natural criterion for adversarial examples in NLP, particularly untargeted2 attacks on seq2seq models: adversarial examples should be meaning-preserving on the source side, but meaning-destroying on the target side. The focus on explicitly evaluating meaning preservation is in contrast to previous work on adversarial examples for seq2seq models (Belinkov and Bisk, 2018; Zhao et al., 2018; Cheng et al., 2018; Ebrahimi et al., 2018a)."
N19-1314,W14-3348,0,0.027604,"valuation is expensive, slow and sometimes difficult to obtain, for example in the case of low-resource languages. This makes automatic metrics that do not require human intervention appealing for experimental research. This section describes 3 evaluation metrics commonly used as alternatives to human evaluation, in particular to evaluate translation models.5 BLEU: (Papineni et al., 2002) is an automatic metric based on n-gram precision coupled with a penalty for shorter sentences. It relies on exact word-level matches and therefore cannot detect synonyms or morphological variations. METEOR: (Denkowski and Lavie, 2014) first estimates alignment between the two sentences and then computes unigram F-score (biased towards recall) weighted by a penalty for longer sentences. Importantly, METEOR uses stemming, synonymy and paraphrasing information to perform alignments. On the downside, it requires language specific resources. chrF: (Popovi´c, 2015) is based on the character n-gram F-score. In particular we will use the chrF2 score (based on the F2-score — recall is given more importance), following the recommendations from Popovi´c (2016). By operating on a sub-word level, it can reflect the semantic similarity"
N19-1314,C18-1055,0,0.307329,"ts own (Cer et al., 2017). This elicits a natural question: what does the term “adversarial perturbation” mean in the context of natural language processing (NLP)? We propose a simple but natural criterion for adversarial examples in NLP, particularly untargeted2 attacks on seq2seq models: adversarial examples should be meaning-preserving on the source side, but meaning-destroying on the target side. The focus on explicitly evaluating meaning preservation is in contrast to previous work on adversarial examples for seq2seq models (Belinkov and Bisk, 2018; Zhao et al., 2018; Cheng et al., 2018; Ebrahimi et al., 2018a). Nonetheless, this feature is extremely important; given two sentences with equivalent meaning, we would expect a good model to produce two outputs with 2 Here we use the term untargeted in the same sense as (Ebrahimi et al., 2018a): an attack whose goal is simply to decrease performance with respect to a reference translation. 3103 Proceedings of NAACL-HLT 2019, pages 3103–3114 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics equivalent meaning. In other words, any meaningpreserving perturbation that results in the model output changing drasti"
N19-1314,P18-2006,0,0.181197,"ts own (Cer et al., 2017). This elicits a natural question: what does the term “adversarial perturbation” mean in the context of natural language processing (NLP)? We propose a simple but natural criterion for adversarial examples in NLP, particularly untargeted2 attacks on seq2seq models: adversarial examples should be meaning-preserving on the source side, but meaning-destroying on the target side. The focus on explicitly evaluating meaning preservation is in contrast to previous work on adversarial examples for seq2seq models (Belinkov and Bisk, 2018; Zhao et al., 2018; Cheng et al., 2018; Ebrahimi et al., 2018a). Nonetheless, this feature is extremely important; given two sentences with equivalent meaning, we would expect a good model to produce two outputs with 2 Here we use the term untargeted in the same sense as (Ebrahimi et al., 2018a): an attack whose goal is simply to decrease performance with respect to a reference translation. 3103 Proceedings of NAACL-HLT 2019, pages 3103–3114 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics equivalent meaning. In other words, any meaningpreserving perturbation that results in the model output changing drasti"
N19-1314,P15-1162,0,0.0605545,"Missing"
N19-1314,N18-1170,0,0.0587869,"., 2016; Samanta and Mehta, 2017; Ebrahimi et al., 2018b), malware (Grosse et al., 2016), gender (Reddy and Knight, 2016) or toxicity (Hosseini et al., 2017) classification to cite a few. In MT, methods have been proposed to attack word-based (Zhao et al., 2018; Cheng et al., 2018) and character-based (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a) models. However these works side-step the question of meaning preservation in the source: they mostly focus on target side evaluation. Finally there is work centered around meaning-preserving adversarial attacks for NLP via paraphrase generation (Iyyer et al., 2018) or rule-based approaches (Jia and Liang, 2017; Ribeiro et al., 2018; Naik et al., 2018; Alzantot et al., 2018). However the proposed attacks are highly engineered and focused on English. 7 Conclusion This paper highlights the importance of performing meaning-preserving adversarial perturbations for NLP models (with a focus on seq2seq). We proposed a general evaluation framework for adversarial perturbations and compared various automatic metrics as proxies for human judgment to instantiate this framework. We then confirmed that, in the context of MT, “naive” attacks do not preserve meaning in"
N19-1314,D17-1215,0,0.0863782,"al., 2018b), malware (Grosse et al., 2016), gender (Reddy and Knight, 2016) or toxicity (Hosseini et al., 2017) classification to cite a few. In MT, methods have been proposed to attack word-based (Zhao et al., 2018; Cheng et al., 2018) and character-based (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a) models. However these works side-step the question of meaning preservation in the source: they mostly focus on target side evaluation. Finally there is work centered around meaning-preserving adversarial attacks for NLP via paraphrase generation (Iyyer et al., 2018) or rule-based approaches (Jia and Liang, 2017; Ribeiro et al., 2018; Naik et al., 2018; Alzantot et al., 2018). However the proposed attacks are highly engineered and focused on English. 7 Conclusion This paper highlights the importance of performing meaning-preserving adversarial perturbations for NLP models (with a focus on seq2seq). We proposed a general evaluation framework for adversarial perturbations and compared various automatic metrics as proxies for human judgment to instantiate this framework. We then confirmed that, in the context of MT, “naive” attacks do not preserve meaning in general, and proposed alternatives to remedy"
N19-1314,D15-1166,0,0.0111275,"4.2). Second, we use this evaluation framework to compare various adversarial attacks and demonstrate that adversarial attacks that are explicitly constrained to preserve meaning receive better assessment scores (§4.3). 4.1 Experimental setting validation data, and keep the 2015 and 2016 test sets as test data. The data is tokenized with the Moses tokenizer (Koehn et al., 2007). The exact data statistics can be found in Appendix A.2. MT Models: We perform experiments with two common neural machine translation (NMT) models. The first is an LSTM based encoderdecoder architecture with attention (Luong et al., 2015). It uses 2-layer encoders and decoders, and dot-product attention. We set the word embedding dimension to 300 and all others to 500. The second model is a self-attentional Transformer (Vaswani et al., 2017), with 6 1024-dimensional encoder and decoder layers and 512 dimensional word embeddings. Both the models are trained with Adam (Kingma and Ba, 2014), dropout (Srivastava et al., 2014) of probability 0.3 and label smoothing (Szegedy et al., 2016) with value 0.1. We experiment with both word based models (vocabulary size fixed at 40k) and subword based models (BPE (Sennrich et al., 2016) wit"
N19-1314,ma-cieri-2006-corpus,0,0.0095708,".1, we have not given an exact description of the semantic similarity scores ssrc and stgt . Indeed, automatically evaluating the semantic similarity between two sentences is an open area of research and it makes sense to decouple the definition of adversarial examples from the specific method used to measure this similarity. In this section, we will discuss manual and automatic metrics that may be used to calculate it. 2.2.1 Human Judgment Judgment by speakers of the language of interest is the de facto gold standard metric for semantic similarity. Specific criteria such as adequacy/fluency (Ma and Cieri, 2006), acceptability (Goto et al., 2013), and 6-level semantic similarity (Cer et al., 2017) have been used in evaluations of MT and sentence embedding methods. In the context of adversarial attacks, we propose the following 6-level evaluation scheme, which is motivated by previous measures, but designed to be (1) symmetric, like Cer et al. (2017), (2) and largely considers meaning preservation but at the very low and high levels considers fluency of the output4 , like Goto et al. (2013): How would you rate the similarity between the meaning of these two sentences? 0. The meaning is completely diff"
N19-1314,C18-1198,1,0.789193,"gender (Reddy and Knight, 2016) or toxicity (Hosseini et al., 2017) classification to cite a few. In MT, methods have been proposed to attack word-based (Zhao et al., 2018; Cheng et al., 2018) and character-based (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a) models. However these works side-step the question of meaning preservation in the source: they mostly focus on target side evaluation. Finally there is work centered around meaning-preserving adversarial attacks for NLP via paraphrase generation (Iyyer et al., 2018) or rule-based approaches (Jia and Liang, 2017; Ribeiro et al., 2018; Naik et al., 2018; Alzantot et al., 2018). However the proposed attacks are highly engineered and focused on English. 7 Conclusion This paper highlights the importance of performing meaning-preserving adversarial perturbations for NLP models (with a focus on seq2seq). We proposed a general evaluation framework for adversarial perturbations and compared various automatic metrics as proxies for human judgment to instantiate this framework. We then confirmed that, in the context of MT, “naive” attacks do not preserve meaning in general, and proposed alternatives to remedy this issue. Finally, we have shown the ut"
N19-1314,P02-1040,0,0.107099,"n is the same but the details differ 4. Meaning is essentially equal but some expressions are unnatural 5. Meaning is essentially equal and the two sentences are well-formed Englisha a 2.2.2 Automatic Metrics Unfortunately, human evaluation is expensive, slow and sometimes difficult to obtain, for example in the case of low-resource languages. This makes automatic metrics that do not require human intervention appealing for experimental research. This section describes 3 evaluation metrics commonly used as alternatives to human evaluation, in particular to evaluate translation models.5 BLEU: (Papineni et al., 2002) is an automatic metric based on n-gram precision coupled with a penalty for shorter sentences. It relies on exact word-level matches and therefore cannot detect synonyms or morphological variations. METEOR: (Denkowski and Lavie, 2014) first estimates alignment between the two sentences and then computes unigram F-score (biased towards recall) weighted by a penalty for longer sentences. Importantly, METEOR uses stemming, synonymy and paraphrasing information to perform alignments. On the downside, it requires language specific resources. chrF: (Popovi´c, 2015) is based on the character n-gram"
N19-1314,W15-3049,0,0.101445,"Missing"
N19-1314,W16-2341,0,0.150567,"Missing"
N19-1314,W18-6319,0,0.034388,"models (vocabulary size fixed at 40k) and subword based models (BPE (Sennrich et al., 2016) with 30k operations). For word-based models, we perform &lt;unk> replacement, replacing &lt;unk> tokens in the translated sentences with the source words with the highest attention value during inference. The full experimental setup and source code are available at https://github. com/pmichel31415/translate/tree/ paul/pytorch_translate/research/ adversarial/experiments. Automatic Metric Implementations: To evaluate both sentence and corpus level BLEU score, we first de-tokenize the output and use sacreBLEU8 (Post, 2018) with its internal intl tokenization, to keep BLEU scores agnostic to tokenization. We compute METEOR using the official implementation9 . ChrF is reported with the sacreBLEU implementation on detokenized text with default parameters. A toolkit implementing the evaluation framework described in §2.1 for these metrics is released at https://github. com/pmichel31415/teapot-nlp. 4.2 Data: Following previous work on adversarial examples for seq2seq models (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a), we perform all experiments on the IWSLT2016 dataset (Cettolo et al., 2016) in the {French,Ger"
N19-1314,W16-5603,0,0.0225311,"ves robustness while not impacting test performance as much as unconstrained attacks. 6 Related work Following seminal work on adversarial attacks by Szegedy et al. (2013), Goodfellow et al. (2014) introduced gradient-based attacks and adversarial training. Since then, a variety of attack (Moosavi3110 Dezfooli et al., 2016) and defense (Ciss´e et al., 2017; Kolter and Wong, 2017) mechanisms have been proposed. Adversarial examples for NLP specifically have seen attacks on sentiment (Papernot et al., 2016; Samanta and Mehta, 2017; Ebrahimi et al., 2018b), malware (Grosse et al., 2016), gender (Reddy and Knight, 2016) or toxicity (Hosseini et al., 2017) classification to cite a few. In MT, methods have been proposed to attack word-based (Zhao et al., 2018; Cheng et al., 2018) and character-based (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a) models. However these works side-step the question of meaning preservation in the source: they mostly focus on target side evaluation. Finally there is work centered around meaning-preserving adversarial attacks for NLP via paraphrase generation (Iyyer et al., 2018) or rule-based approaches (Jia and Liang, 2017; Ribeiro et al., 2018; Naik et al., 2018; Alzantot et a"
N19-1314,P18-1079,0,0.0898436,"(Grosse et al., 2016), gender (Reddy and Knight, 2016) or toxicity (Hosseini et al., 2017) classification to cite a few. In MT, methods have been proposed to attack word-based (Zhao et al., 2018; Cheng et al., 2018) and character-based (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a) models. However these works side-step the question of meaning preservation in the source: they mostly focus on target side evaluation. Finally there is work centered around meaning-preserving adversarial attacks for NLP via paraphrase generation (Iyyer et al., 2018) or rule-based approaches (Jia and Liang, 2017; Ribeiro et al., 2018; Naik et al., 2018; Alzantot et al., 2018). However the proposed attacks are highly engineered and focused on English. 7 Conclusion This paper highlights the importance of performing meaning-preserving adversarial perturbations for NLP models (with a focus on seq2seq). We proposed a general evaluation framework for adversarial perturbations and compared various automatic metrics as proxies for human judgment to instantiate this framework. We then confirmed that, in the context of MT, “naive” attacks do not preserve meaning in general, and proposed alternatives to remedy this issue. Finally, w"
N19-1314,P16-1162,0,0.0684295,"ntion (Luong et al., 2015). It uses 2-layer encoders and decoders, and dot-product attention. We set the word embedding dimension to 300 and all others to 500. The second model is a self-attentional Transformer (Vaswani et al., 2017), with 6 1024-dimensional encoder and decoder layers and 512 dimensional word embeddings. Both the models are trained with Adam (Kingma and Ba, 2014), dropout (Srivastava et al., 2014) of probability 0.3 and label smoothing (Szegedy et al., 2016) with value 0.1. We experiment with both word based models (vocabulary size fixed at 40k) and subword based models (BPE (Sennrich et al., 2016) with 30k operations). For word-based models, we perform &lt;unk> replacement, replacing &lt;unk> tokens in the translated sentences with the source words with the highest attention value during inference. The full experimental setup and source code are available at https://github. com/pmichel31415/translate/tree/ paul/pytorch_translate/research/ adversarial/experiments. Automatic Metric Implementations: To evaluate both sentence and corpus level BLEU score, we first de-tokenize the output and use sacreBLEU8 (Post, 2018) with its internal intl tokenization, to keep BLEU scores agnostic to tokenizati"
N19-1350,N06-1017,0,0.0413271,"lows, we explain existing tasks that are related to our work. Our task is closely related to word sense disambiguation (WSD) (Navigli, 2009), which identifies a pre-defined sense for the target word with its context. Although we can use it to solve our task by retrieving the definition sentence for the sense identified by WSD, it requires a substantial amount of training data to handle a different set of meanings of each word, and cannot handle words (or senses) which are not registered in the dictionary. Although some studies have attempted to detect novel senses of words for given contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) co"
N19-1350,P18-2043,0,0.141504,"s or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation (Ni and Wang, 2017) and definition generation (Noraset et al., 2017; Gadetsky et al., 2018), our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work. 1 Figure 1: Local & Global Context-aware Description generator (LOG-CaD). Introduction When we read news text with emerging entities, text in unfamiliar domains, or text in foreign languages, we often encounter expressions (words or phrases) whose senses we do not understand. In such cases, we may first try"
N19-1350,H92-1045,0,0.356747,"get phrase’s embedding induced from massive text. We performed experiments on three existing datasets and one newly built from Wikipedia and Wikidata. The experimental results confirmed that the local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is important when the target phrase is polysemous, rare, or unseen. As future work, we plan to modify our model to use multiple contexts in text to improve the quality of descriptions, considering the “one sense per discourse” hypothesis (Gale et al., 1992). We will release the newly built Wikipedia dataset and the experimental codes for the academic and industrial communities at https://github.com/shonosuke/ ishiwatari-naacl2019 to facilitate the reproducibility of our results and their use in various application contexts. Acknowledgements The authors are grateful to Thanapon Noraset for sharing the details of his implementation of the previous work. We also thank the anonymous reviewers for their careful reading of our paper and insightful comments, and the members of Kitsuregawa-Toyoda-Nemoto-Yoshinaga-Goda laboratory in the University of Tok"
N19-1350,W09-2503,0,0.041638,"s for given contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, Noraset et al. (2017) introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) proposed a definition generation method that works with polysemous words in dictionaries. The"
N19-1350,D12-1066,0,0.0129473,"contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, Noraset et al. (2017) introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) proposed a definition generation method that works with polysemous words in dictionaries. They presented a model"
N19-1350,I17-2070,0,0.273408,"the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation (Ni and Wang, 2017) and definition generation (Noraset et al., 2017; Gadetsky et al., 2018), our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work. 1 Figure 1: Local & Global Context-aware Description generator (LOG-CaD). Introduction When we read news text with emerging entities, text in unfamiliar domains, or text in foreign languages, we often encounter expressions (words or phra"
N19-1350,P02-1040,0,0.103786,"o predict descriptions. Also, it cannot directly use the local context to predict the words in descriptions. This is because the I-Attention model indirectly uses the local context only to disambiguate the phrase embedding xtrg as x0trg = xtrg m, PI FFNN(hi ) m = σ(Wm i=1 + bm ). I Context: #1 #2 after being enlarged by publisher daniel o’neill it was reportedly one of the largest and most prosperous newspapers in the united states. in 1967 he returned to belfast where he met fellow belfast artist daniel o’neill. Reference: american journalist (18) Automatic Evaluation Table 4 shows the BLEU (Papineni et al., 2002) scores of the output descriptions. We can see that the LOG-CaD model consistently outperforms the three baselines in all four datasets. This result indicates that using both local and global contexts helps describe the unknown words/phrases correctly. While the http://pytorch.org/ Input: daniel o’neill (17) All four models (Table 3) are implemented with the PyTorch framework (Ver. 1.0.0).11 11 Table 6: Descriptions for a word in WordNet. (16) Here, the FFNN(·) function is a feed-forward neural network that maps the encoded local contexts hi to another space. The mapped local contexts are then"
N19-1350,W04-3250,0,0.0275215,"8: Descriptions for a word in Wikipedia. Manual Evaluation To compare the proposed model and the strongest baseline in Table 4 (i.e., the Local model), we performed a human evaluation on our dataset. We randomly selected 100 samples from the test set of the Wikipedia dataset and asked three native English speakers to rate the output descriptions from 1 to 5 points as: 1) completely wrong or self-definition, 2) correct topic with wrong information, 3) correct but incomplete, 4) small details missing, 5) correct. The averaged scores are reported in Table 5. Pair-wise bootstrap resampling test (Koehn, 2004) for the annotated scores has shown that the superiority of LOG-CaD over the Local model is statistically significant (p &lt; 0.01). dataset, both the Local and LOG-CaD models can describe the word/phrase considering its local context. For example, both the Local and LOG-CaD models could generate “american” in the description for “daniel o’neill” given “united states” in context #1, while they could generate “british” given “belfast” in context #2. A similar trend can also be observed in Table 8, where LOG-CaD could generate the locational expressions such as “philippines” and “british” given the"
N19-1350,P14-1025,0,0.0151069,"plain existing tasks that are related to our work. Our task is closely related to word sense disambiguation (WSD) (Navigli, 2009), which identifies a pre-defined sense for the target word with its context. Although we can use it to solve our task by retrieving the definition sentence for the sense identified by WSD, it requires a substantial amount of training data to handle a different set of meanings of each word, and cannot handle words (or senses) which are not registered in the dictionary. Although some studies have attempted to detect novel senses of words for given contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsententia"
N19-1350,P16-1100,0,0.120675,"ncluded in a given sentence with the target phrase (i.e., the X in Eq. (1)) as “local context,” and the implicit contextual information in massive text as “global context.” While both local and global contexts are crucial for humans to understand unfamiliar phrases, are they also useful for machines to generate descriptions? To verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase. 3468 3.2 Proposed model Figure 1 shows an illustration of our LOG-CaD model. Similarly to the standard encoder-decoder model with attention (Bahdanau et al., 2015; Luong and Manning, 2016), it has a context encoder and a description decoder. The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a gate function similar to Noraset et al. (2017) to dynamically control how the global and local contexts influence the description. Local & global context encoders We first describe how to model local and global contexts. Given a sentence X and a phrase Xtrg , a bidirectional LSTM (Gers et al., 1999) encoder generates a sequence of continuous vectors"
N19-1350,J10-3003,0,0.0217848,"by WSD, it requires a substantial amount of training data to handle a different set of meanings of each word, and cannot handle words (or senses) which are not registered in the dictionary. Although some studies have attempted to detect novel senses of words for given contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, Noraset et al. (2017) introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs,"
N19-4007,P12-3024,0,0.0529868,"Missing"
N19-4007,D17-1263,0,0.0431914,"Missing"
N19-4007,D10-1092,0,0.0201707,"u et al., 2015) SlovakEnglish machine translation systems from Neubig and Hu (2018). Aggregate Score Analysis The first variety of analysis is not unique to compare-mt, answering the standard question posed by most research papers: “given two systems, which one has better accuracy overall?” It can calculate scores according to standard BLEU (Papineni et al., 2002), as well as other measures such as output-to-reference length ratio (which can discover systematic biases towards generating too-long or too-short sentences) or alternative evaluation metrics such as chrF (Popovi´c, 2015) and RIBES (Isozaki et al., 2010). compare-mt also has an extensible Scorer class, which will be used to expand the metrics supported by compare-mt in the future, and can be used by users to implement their own metrics as well. Confidence intervals and significance of differences in these scores can be measured using bootstrap resampling (Koehn, 2004). Fig. 1 shows the concrete results of this analysis on our PBMT and NMT systems. From the results we can see that the NMT achieves higher BLEU but shorter sentence length, while there is no significant difference in RIBES. Basic Analysis using compare-mt Using compare-mt with th"
N19-4007,D16-1025,0,0.0477723,"Missing"
N19-4007,W04-3250,0,0.164897,"standard BLEU (Papineni et al., 2002), as well as other measures such as output-to-reference length ratio (which can discover systematic biases towards generating too-long or too-short sentences) or alternative evaluation metrics such as chrF (Popovi´c, 2015) and RIBES (Isozaki et al., 2010). compare-mt also has an extensible Scorer class, which will be used to expand the metrics supported by compare-mt in the future, and can be used by users to implement their own metrics as well. Confidence intervals and significance of differences in these scores can be measured using bootstrap resampling (Koehn, 2004). Fig. 1 shows the concrete results of this analysis on our PBMT and NMT systems. From the results we can see that the NMT achieves higher BLEU but shorter sentence length, while there is no significant difference in RIBES. Basic Analysis using compare-mt Using compare-mt with the default settings is as simple as typing compare-mt ref sys1 sys2 Bucketed Analysis A second, and more nuanced, variety of analysis supported by where ref is a manually curated reference file, and sys1 and sys2 are the outputs of two systems that we would like to compare. These analy2 In fact, all of the figures and t"
N19-4007,J10-4005,0,0.0142806,"for unusual phenomena requires perusing a large number of examples. There is also a risk that confirmation bias will simply affirm preexisting assumptions. If a developer has some hypothesis about specifically what phenomena their method should be helping with, they can develop scripts to automatically test these assumptions. However, this requires deep intuitions with respect to what changes to expect in advance, which cannot be taken for granted in beginning researchers Introduction Tasks involving the generation of natural language are ubiquitous in NLP, including machine translation (MT; Koehn (2010)), language generation from structured data (Reiter and Dale, 2000), summarization (Mani, 1999), dialog response generation (Oh and Rudnicky, 2000), image captioning (Mitchell et al., 2012). Unlike tasks that involve prediction of a single label such as text classification, natural language texts are nuanced, and there are not clear yes/no distinctions about whether outputs are correct or not. Evaluation measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2011), and many others attempt to give an 1 Code http://github.com/neulab/compare-mt and video de"
N19-4007,N03-1017,0,0.0146971,".00 [79.39,80.64] 94.79 [94.10,95.49] NMT 24.03 [23.33,24.65] 80.00 [79.44,80.92] 93.82 [92.90,94.85] Win? s2&gt;s1 p&lt;0.001 p=0.44 s1&gt;s2 p&lt;0.001 Table 1: Aggregate score analysis with scores, confidence intervals, and pairwise significance tests. sis results can be written to the terminal in text format, but can also be written to a formatted HTML file with charts and LaTeX tables that can be directly used in papers or reports.2 In this section, we demonstrate the types of analysis that are provided by this standard usage of compare-mt. Specifically, we use the example of comparing phrase-based (Koehn et al., 2003) and neural (Bahdanau et al., 2015) SlovakEnglish machine translation systems from Neubig and Hu (2018). Aggregate Score Analysis The first variety of analysis is not unique to compare-mt, answering the standard question posed by most research papers: “given two systems, which one has better accuracy overall?” It can calculate scores according to standard BLEU (Papineni et al., 2002), as well as other measures such as output-to-reference length ratio (which can discover systematic biases towards generating too-long or too-short sentences) or alternative evaluation metrics such as chrF (Popovi´"
N19-4007,H05-1098,0,0.0776723,"se Abstraction One feature that greatly improves the flexibility of analysis is compare-mt’s ability to do analysis over arbitrary word labels. For example, we can perform word accuracy analysis where we bucket the words by POS tags, as shown in 4. In the case of the PBMT vs. NMT analysis above, this uncovers the interesting fact that PBMT was better at generating base-form verbs, whereas NMT was better at generating conjugated verbs. This can also be applied to the n-gram analysis, finding which POS n-grams are generated well by one system or another, a type of analysis that was performed by Chiang et al. (2005) to understand differences in reordering between different systems. Labels are provided by external files, where there is one label per word in the reference and system outputs, which means that generating these labels can be an arbitrary pre-processing step performed by the user without any direct modifications to the compare-mt code itself. These labels do not have to be POS tags, of course, and can Sentence Example Analysis Finally, compare-mt makes it possible to analyze and compare individual sentence examples based on statistics, or differences of statistics. Specifically, we can calcula"
N19-4007,P05-3025,0,0.115516,"Missing"
N19-4007,D17-2021,0,0.0495899,"Missing"
N19-4007,W11-2107,0,0.0489386,"ks involving the generation of natural language are ubiquitous in NLP, including machine translation (MT; Koehn (2010)), language generation from structured data (Reiter and Dale, 2000), summarization (Mani, 1999), dialog response generation (Oh and Rudnicky, 2000), image captioning (Mitchell et al., 2012). Unlike tasks that involve prediction of a single label such as text classification, natural language texts are nuanced, and there are not clear yes/no distinctions about whether outputs are correct or not. Evaluation measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2011), and many others attempt to give an 1 Code http://github.com/neulab/compare-mt and video demo https://youtu.be/K-MNPOGKnDQ are available. 35 Proceedings of NAACL-HLT 2019: Demonstrations, pages 35–41 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics or others not intimately familiar with the task at hand. In addition, creation of special-purpose oneoff analysis scripts is time-consuming. In this paper, we present compare-mt, a tool for holistic comparison and analysis of the results of language generation systems. The main use case of compare-mt,"
N19-4007,P17-1106,0,0.0439721,"Missing"
N19-4007,W04-1013,0,0.0643424,"ers Introduction Tasks involving the generation of natural language are ubiquitous in NLP, including machine translation (MT; Koehn (2010)), language generation from structured data (Reiter and Dale, 2000), summarization (Mani, 1999), dialog response generation (Oh and Rudnicky, 2000), image captioning (Mitchell et al., 2012). Unlike tasks that involve prediction of a single label such as text classification, natural language texts are nuanced, and there are not clear yes/no distinctions about whether outputs are correct or not. Evaluation measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2011), and many others attempt to give an 1 Code http://github.com/neulab/compare-mt and video demo https://youtu.be/K-MNPOGKnDQ are available. 35 Proceedings of NAACL-HLT 2019: Demonstrations, pages 35–41 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics or others not intimately familiar with the task at hand. In addition, creation of special-purpose oneoff analysis scripts is time-consuming. In this paper, we present compare-mt, a tool for holistic comparison and analysis of the results of language generation system"
N19-4007,W07-0707,0,0.0888478,"Missing"
N19-4007,D18-1050,1,0.843757,"splay the 38 both aggregate score analysis (with significance tests), sentence bucket analysis, or sentence example analysis. • If a user wanted to bucket words according to a different type of statistic or feature, they could implement their own instance of a Bucketer class, and use this in the word accuracy analysis. 4 Example Use-cases To emphasize compare-mt’s practical utility, we also provide examples of how it has already been used in analyses in published research papers: Figs. 4 and 5 of Wang et al. (2018) use sentence-level bucketed analysis. Tab. 7 of Qi et al. (2018) and Tab. 8 of Michel and Neubig (2018) show the results of n-gram analysis. Fig. 2 of Qi et al. (2018), Fig. 4 of Sachan and Neubig (2018), Tab. 5 of Kumar and Tsvetkov (2019) show the results of word accuracy analysis. Figure 3: Word F-measure bucketed by POS tag. also be used for other kinds of analysis. For example, one may perform analysis to find accuracy of generation of words with particular morphological tags (Popovi´c et al., 2006), or words that appear in a sentiment lexicon (Mohammad et al., 2016). Source-side Analysis While most analysis up until this point focused on whether a particular word on the target side is acc"
N19-4007,J11-4002,0,0.0898392,"Missing"
N19-4007,E12-1076,0,0.0249987,"e hypothesis about specifically what phenomena their method should be helping with, they can develop scripts to automatically test these assumptions. However, this requires deep intuitions with respect to what changes to expect in advance, which cannot be taken for granted in beginning researchers Introduction Tasks involving the generation of natural language are ubiquitous in NLP, including machine translation (MT; Koehn (2010)), language generation from structured data (Reiter and Dale, 2000), summarization (Mani, 1999), dialog response generation (Oh and Rudnicky, 2000), image captioning (Mitchell et al., 2012). Unlike tasks that involve prediction of a single label such as text classification, natural language texts are nuanced, and there are not clear yes/no distinctions about whether outputs are correct or not. Evaluation measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2011), and many others attempt to give an 1 Code http://github.com/neulab/compare-mt and video demo https://youtu.be/K-MNPOGKnDQ are available. 35 Proceedings of NAACL-HLT 2019: Demonstrations, pages 35–41 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computatio"
N19-4007,N18-2084,1,0.85179,"erence in this measure, then display the 38 both aggregate score analysis (with significance tests), sentence bucket analysis, or sentence example analysis. • If a user wanted to bucket words according to a different type of statistic or feature, they could implement their own instance of a Bucketer class, and use this in the word accuracy analysis. 4 Example Use-cases To emphasize compare-mt’s practical utility, we also provide examples of how it has already been used in analyses in published research papers: Figs. 4 and 5 of Wang et al. (2018) use sentence-level bucketed analysis. Tab. 7 of Qi et al. (2018) and Tab. 8 of Michel and Neubig (2018) show the results of n-gram analysis. Fig. 2 of Qi et al. (2018), Fig. 4 of Sachan and Neubig (2018), Tab. 5 of Kumar and Tsvetkov (2019) show the results of word accuracy analysis. Figure 3: Word F-measure bucketed by POS tag. also be used for other kinds of analysis. For example, one may perform analysis to find accuracy of generation of words with particular morphological tags (Popovi´c et al., 2006), or words that appear in a sentiment lexicon (Mohammad et al., 2016). Source-side Analysis While most analysis up until this point focused on whether a pa"
N19-4007,W18-6327,1,0.796318,"entence example analysis. • If a user wanted to bucket words according to a different type of statistic or feature, they could implement their own instance of a Bucketer class, and use this in the word accuracy analysis. 4 Example Use-cases To emphasize compare-mt’s practical utility, we also provide examples of how it has already been used in analyses in published research papers: Figs. 4 and 5 of Wang et al. (2018) use sentence-level bucketed analysis. Tab. 7 of Qi et al. (2018) and Tab. 8 of Michel and Neubig (2018) show the results of n-gram analysis. Fig. 2 of Qi et al. (2018), Fig. 4 of Sachan and Neubig (2018), Tab. 5 of Kumar and Tsvetkov (2019) show the results of word accuracy analysis. Figure 3: Word F-measure bucketed by POS tag. also be used for other kinds of analysis. For example, one may perform analysis to find accuracy of generation of words with particular morphological tags (Popovi´c et al., 2006), or words that appear in a sentiment lexicon (Mohammad et al., 2016). Source-side Analysis While most analysis up until this point focused on whether a particular word on the target side is accurate or not, it is also of interest what source-side words are or are not accurately translated. co"
N19-4007,W18-6307,0,0.0474648,"Missing"
N19-4007,E17-2060,0,0.0792381,"Missing"
N19-4007,Y05-1014,0,0.1563,"Missing"
N19-4007,P11-4010,0,0.0860557,"Missing"
N19-4007,2011.mtsummit-papers.60,0,0.0506685,"Missing"
N19-4007,vilar-etal-2006-error,0,0.173637,"Missing"
N19-4007,D18-1103,1,0.826576,"Win? s2&gt;s1 p&lt;0.001 p=0.44 s1&gt;s2 p&lt;0.001 Table 1: Aggregate score analysis with scores, confidence intervals, and pairwise significance tests. sis results can be written to the terminal in text format, but can also be written to a formatted HTML file with charts and LaTeX tables that can be directly used in papers or reports.2 In this section, we demonstrate the types of analysis that are provided by this standard usage of compare-mt. Specifically, we use the example of comparing phrase-based (Koehn et al., 2003) and neural (Bahdanau et al., 2015) SlovakEnglish machine translation systems from Neubig and Hu (2018). Aggregate Score Analysis The first variety of analysis is not unique to compare-mt, answering the standard question posed by most research papers: “given two systems, which one has better accuracy overall?” It can calculate scores according to standard BLEU (Papineni et al., 2002), as well as other measures such as output-to-reference length ratio (which can discover systematic biases towards generating too-long or too-short sentences) or alternative evaluation metrics such as chrF (Popovi´c, 2015) and RIBES (Isozaki et al., 2010). compare-mt also has an extensible Scorer class, which will b"
N19-4007,D18-1509,1,0.820283,"EU score), sort the sentences in the test set according to the difference in this measure, then display the 38 both aggregate score analysis (with significance tests), sentence bucket analysis, or sentence example analysis. • If a user wanted to bucket words according to a different type of statistic or feature, they could implement their own instance of a Bucketer class, and use this in the word accuracy analysis. 4 Example Use-cases To emphasize compare-mt’s practical utility, we also provide examples of how it has already been used in analyses in published research papers: Figs. 4 and 5 of Wang et al. (2018) use sentence-level bucketed analysis. Tab. 7 of Qi et al. (2018) and Tab. 8 of Michel and Neubig (2018) show the results of n-gram analysis. Fig. 2 of Qi et al. (2018), Fig. 4 of Sachan and Neubig (2018), Tab. 5 of Kumar and Tsvetkov (2019) show the results of word accuracy analysis. Figure 3: Word F-measure bucketed by POS tag. also be used for other kinds of analysis. For example, one may perform analysis to find accuracy of generation of words with particular morphological tags (Popovi´c et al., 2006), or words that appear in a sentiment lexicon (Mohammad et al., 2016). Source-side Analysi"
N19-4007,W00-0306,0,0.0894871,"isting assumptions. If a developer has some hypothesis about specifically what phenomena their method should be helping with, they can develop scripts to automatically test these assumptions. However, this requires deep intuitions with respect to what changes to expect in advance, which cannot be taken for granted in beginning researchers Introduction Tasks involving the generation of natural language are ubiquitous in NLP, including machine translation (MT; Koehn (2010)), language generation from structured data (Reiter and Dale, 2000), summarization (Mani, 1999), dialog response generation (Oh and Rudnicky, 2000), image captioning (Mitchell et al., 2012). Unlike tasks that involve prediction of a single label such as text classification, natural language texts are nuanced, and there are not clear yes/no distinctions about whether outputs are correct or not. Evaluation measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2011), and many others attempt to give an 1 Code http://github.com/neulab/compare-mt and video demo https://youtu.be/K-MNPOGKnDQ are available. 35 Proceedings of NAACL-HLT 2019: Demonstrations, pages 35–41 c Minneapolis, Minnesota, June 2 - Jun"
N19-4007,P02-1040,0,0.105666,"r granted in beginning researchers Introduction Tasks involving the generation of natural language are ubiquitous in NLP, including machine translation (MT; Koehn (2010)), language generation from structured data (Reiter and Dale, 2000), summarization (Mani, 1999), dialog response generation (Oh and Rudnicky, 2000), image captioning (Mitchell et al., 2012). Unlike tasks that involve prediction of a single label such as text classification, natural language texts are nuanced, and there are not clear yes/no distinctions about whether outputs are correct or not. Evaluation measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2011), and many others attempt to give an 1 Code http://github.com/neulab/compare-mt and video demo https://youtu.be/K-MNPOGKnDQ are available. 35 Proceedings of NAACL-HLT 2019: Demonstrations, pages 35–41 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics or others not intimately familiar with the task at hand. In addition, creation of special-purpose oneoff analysis scripts is time-consuming. In this paper, we present compare-mt, a tool for holistic comparison and analysis of the results of languag"
N19-4007,W15-3049,0,0.151711,"Missing"
N19-4007,C08-1141,0,0.0218532,"Missing"
N19-4007,W06-3101,0,0.119802,"Missing"
neubig-mori-2010-word,den-etal-2008-proper,0,\N,Missing
neubig-mori-2010-word,W07-1516,0,\N,Missing
neubig-mori-2010-word,C08-1113,1,\N,Missing
neubig-mori-2010-word,I08-7018,0,\N,Missing
P11-1064,N10-1028,0,0.636111,"tributions, and thus the parameters for the Pitman-Yor process will be different for each distribution. Further, as ll and lr must be smaller than l, Pt,l no longer contains itself as a base measure, and is thus not deficient. An example of the actual discount values learned in one of the experiments described in Section 7 is shown in Figure 2. It can be seen that, as expected, the discounts for short phrases are lower than 636 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is the management of phrase co"
P11-1064,P09-1088,0,0.778629,"nts. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strategy to genera"
P11-1064,J93-2003,0,0.0978707,"Missing"
P11-1064,W10-1703,0,0.0131603,"he proposed method on translation tasks from four languages, French, German, Spanish, and Japanese, into English. 638 TM (en) TM (other) LM (en) Tune (en ) Tune (other) Test (en) Test (other) de-en 1.80M 1.85M 52.7M 49.8k 47.2k 65.6k 62.7k es-en 1.62M 1.82M 52.7M 49.8k 52.6k 65.6k 68.1k fr-en 1.35M 1.56M 52.7M 49.8k 55.4k 65.6k 72.6k ja-en 2.38M 2.78M 44.7M 68.9k 80.4k 40.4k 48.7k Table 1: The number of words in each corpus for TM and LM training, tuning, and testing. 7.1 Experimental Setup The data for French, German, and Spanish are from the 2010 Workshop on Statistical Machine Translation (Callison-Burch et al., 2010). We use the news commentary corpus for training the TM, and the news commentary and Europarl corpora for training the LM. For Japanese, we use data from the NTCIR patent translation task (Fujii et al., 2008). We use the first 100k sentences of the parallel corpus for the TM, and the whole parallel corpus for the LM. Details of both corpora can be found in Table 1. Corpora are tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets. We compare the accuracy of o"
P11-1064,W07-0403,0,0.436106,"able that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-"
P11-1064,J07-2003,0,0.350869,"a fraction of the size of most heuristic extraction methods. Finally, we varied the size of the parallel corpus for the Japanese-English task from 50k to 400k senFor future work, we plan to refine HLEN to use a more appropriate model of phrase length than the uniform distribution, particularly by attempting to bias against phrase pairs where one of the two phrases is much longer than the other. In addition, we will test probabilities learned using the proposed model with an ITG-based decoder. We will also examine the applicability of the proposed model in the context of hierarchical phrases (Chiang, 2007), or in alignment using syntactic structure (Galley et al., 2006). It is also worth examining the plausibility of variational inference as proposed by Cohen et al. (2010) in the alignment context. Acknowledgments This work was performed while the first author was supported by the JSPS Research Fellowship for Young Scientists. References Figure 4: The effect of corpus size on the accuracy (a) and phrase table size (b) for each method (Japanese-English). tences and measured the effect of corpus size on translation accuracy. From the results in Figure 4 (a), it can be seen that at all corpus size"
P11-1064,N10-1081,0,0.293077,"rates from the symbol distribution Px , then from the phrase distribution Pt , while HIER generates directly from Pt , which falls back to divide-and-conquer based on Px when necessary. It can be seen that while Pt in FLAT only generates minimal phrases, Pt in HIER generates (and thus memorizes) phrases at all levels of granularity. 4.1 Length-based Parameter Tuning There are still two problems with HIER, one theoretical, and one practical. Theoretically, HIER contains itself as its base measure, and stochastic process models that include themselves as base measures are deficient, as noted in Cohen et al. (2010). Practically, while the Pitman-Yor process in HIER shares the parameters s and d over all phrase pairs in the model, long phrase pairs are much more sparse those of long phrases. In particular, phrase pairs of length up to six (for example, |e |= 3, |f |= 3) are given discounts of nearly zero while larger phrases are more heavily discounted. We conjecture that this is related to the observation by Koehn et al. (2003) that using phrases where max(|e|, |f |) ≤ 3 cause significant improvements in BLEU score, while using larger phrases results in diminishing returns. Figure 2: Learned discount va"
P11-1064,P05-1066,0,0.0803727,"Missing"
P11-1064,P08-2007,0,0.0326897,"Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pf lat (he, f i; θx , θt ) is parameterized by a phrase table θt and a symbol distribution θx . We use the following generative story as a representative of the flat ITG model. 1. Generate symbol x from the multinomial distribution Px (x; θx ). x can take the values TERM, REG , or INV . 2. According to the x take the following actions. (a) If x = TERM, generate a phrase pair from the phrase table Pt (he, f i; θt ). (b) If x = REG, a regular ITG rule, generate phrase pairs he1 , f1 i and he2 , f2 i from"
P11-1064,P10-1147,0,0.261523,"the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size. 1 Introduction The training of translation models for phrasebased statistical machine translation (SMT) systems (Koehn et al., 2003) takes unaligned bilingual training data as input, and outputs a scored table of phrase pairs. This phrase table is traditionally generated by going through a pipeline of two steps, first generating word (or minimal phrase) alignments, then extracting a phrase table that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one importan"
P11-1064,W06-3105,0,0.0185751,"lso for phrases that, while not directly included in the model, are composed of two high probability child phrases. It should be noted that while for FLAT and HIER Pt can be used directly, as HLEN learns separate models for each length, we must combine these probabilities into a single value. We do this by setting Pt (he, f i) = Pt,l (he, f i)c(l)/ L ∑ c(˜l) ˜ l=1 for every phrase pair, where l = |e |+ |f |and c(l) is the number of phrases of length l in the sample. We call this model-based extraction method MOD. 5.3 Sample Combination As has been noted in previous works, (Koehn et al., 2003; DeNero et al., 2006) exhaustive phrase extraction tends to out-perform approaches that use syntax or generative models to limit phrase boundaries. DeNero et al. (2006) state that this is because generative models choose only a single phrase segmentation, and thus throw away many good phrase pairs that are in conflict with this segmentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the jo"
P11-1064,D08-1033,0,0.563293,"Missing"
P11-1064,P06-1121,0,0.17041,"ds. Finally, we varied the size of the parallel corpus for the Japanese-English task from 50k to 400k senFor future work, we plan to refine HLEN to use a more appropriate model of phrase length than the uniform distribution, particularly by attempting to bias against phrase pairs where one of the two phrases is much longer than the other. In addition, we will test probabilities learned using the proposed model with an ITG-based decoder. We will also examine the applicability of the proposed model in the context of hierarchical phrases (Chiang, 2007), or in alignment using syntactic structure (Galley et al., 2006). It is also worth examining the plausibility of variational inference as proposed by Cohen et al. (2010) in the alignment context. Acknowledgments This work was performed while the first author was supported by the JSPS Research Fellowship for Young Scientists. References Figure 4: The effect of corpus size on the accuracy (a) and phrase table size (b) for each method (Japanese-English). tences and measured the effect of corpus size on translation accuracy. From the results in Figure 4 (a), it can be seen that at all corpus sizes, the results from all three methods are comparable, with insign"
P11-1064,D07-1103,0,0.0694819,"mentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, not one. Adaptor gramma"
P11-1064,P07-2045,0,0.00525629,"Missing"
P11-1064,N03-1017,0,0.562956,"not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size. 1 Introduction The training of translation models for phrasebased statistical machine translation (SMT) systems (Koehn et al., 2003) takes unaligned bilingual training data as input, and outputs a scored table of phrase pairs. This phrase table is traditionally generated by going through a pipeline of two steps, first generating word (or minimal phrase) alignments, then extracting a phrase table that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieve"
P11-1064,2005.iwslt-1.8,0,0.00647738,"tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets. We compare the accuracy of our proposed method of joint phrase alignment and extraction using the FLAT, HIER and HLEN models, with a baseline of using word alignments from GIZA ++ and heuristic phrase extraction. Decoding is performed using Moses (Koehn and others, 2007) using the phrase tables learned by each method under consideration, as well as standard bidirectional lexical reordering probabilities (Koehn et al., 2005). Maximum phrase length is limited to 7 in all models, and for the LM we use an interpolated Kneser-Ney 5-gram model. For GIZA ++, we use the standard training regimen up to Model 4, and combine alignments with grow-diag-final-and. For the proposed models, we train for 100 iterations, and use the final sample acquired at the end of the training process for our experiments using a single sample6 . In addition, 6 For most models, while likelihood continued to increase gradually for all 100 iterations, BLEU score gains plateaued after 5-10 iterations, likely due to the strong prior information Al"
P11-1064,N06-1014,0,0.0794575,"f |; λ) 1 M0 (he, f i) =(Pm1 (f |e)Puni (e)Pm1 (e|f )Puni (f )) 2 . Ppois is the Poisson distribution with the average length parameter λ. As long phrases lead to sparsity, we set λ to a relatively small value to allow us to bias against overly long phrases4 . Pm1 is the word-based Model 1 (Brown et al., 1993) probability of one phrase given the other, which incorporates word-based alignment information as prior knowledge in the phrase translation probability. We take the geometric mean5 of the Model 1 probabilities in both directions to encourage alignments that are supported by both models (Liang et al., 2006). It should be noted that while Model 1 probabilities are used, they are only soft constraints, compared with the hard constraint of choosing a single word alignment used in most previous phrase extraction approaches. For Pbu , if g is the non-null phrase in e and f , we calculate the probability as follows: Pbu (he, f i) = Puni (g)Ppois (|g|; λ)/2. Note that Pbu is divided by 2 as the probability is considering null alignments in both directions. 4 Hierarchical ITG Model While in FLAT only minimal phrases were memorized by the model, as DeNero et al. (2008) note We choose 10−2 , 10−3 , or 10−"
P11-1064,W02-1018,0,0.126279,"Missing"
P11-1064,W07-0715,0,0.0636315,"n conflict with this segmentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, n"
P11-1064,W99-0604,0,0.0831392,"on which value gives the best performance on the development set. 5 The probabilities of the geometric mean do not add to one, but we found empirically that even when left unnormalized, this provided much better results than the using the arithmetic mean, which is more theoretically correct. 3 and we confirm in the experiments in Section 7, using only minimal phrases leads to inferior translation results for phrase-based SMT. Because of this, previous research has combined FLAT with heuristic phrase extraction, which exhaustively combines all adjacent phrases permitted by the word alignments (Och et al., 1999). We propose an alternative, fully statistical approach that directly models phrases at multiple granularities, which we will refer to as HIER. By doing so, we are able to do away with heuristic phrase extraction, creating a fully probabilistic model for phrase probabilities that still yields competitive results. Similarly to FLAT, HIER assigns a probability Phier (he, f i; θx , θt ) to phrase pairs, and is parameterized by a phrase table θt and a symbol distribution θx . The main difference from the generative story of the traditional ITG model is that symbols and phrase pairs are generated i"
P11-1064,W09-3804,0,0.435467,"be seen that, as expected, the discounts for short phrases are lower than 636 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is the management of phrase counts. As a phrase pair ta may have been generated from two smaller component phrases tb and tc , when a sample containing ta is removed from the distribution, it may also be necessary to decrement the counts of tb and tc as well. The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solu"
P11-1064,P06-1124,0,0.832206,"tribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strategy to generate phrase pairs that do not exist (or are given low probability) in the phrase distribution. We combine this model with the Bayesian nonparametric Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), realizing ITG-based divide and conquer through a novel formulation where the Pitman-Yor process uses two copies of itself as a 632 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632–641, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics base measure. As a result of this modeling strategy, phrases of multiple granularities are generated, and thus memorized, by the Pitman-Yor process. This makes it possible to directly use probabilities of the phrase model as a replacement for the phrase table generated by heuri"
P11-1064,J97-3002,0,0.466135,"le values of the hidden parameters: ∫ P (e|f , hE, Fi) = P (e|f , θ)P (θ|hE, Fi). (1) θ If θ takes the form of a scored phrase table, we can use traditional methods for phrase-based SMT to find P (e|f , θ) and concentrate on creating a model for P (θ|hE, Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pf lat (he, f i; θx , θt ) is parameterized by a phrase table θt and a symbol distribution θx . We use the following generative story as a representative of the flat ITG model. 1. Generate symbol x from the multinomial distribution Px (x;"
P11-1064,P08-1012,0,0.360881,"t with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and con"
P11-2093,C00-1004,0,0.0226465,"text as input, and outputs a string of morphemes annotated with parts of speech (POSs). As MA is the first step in Japanese NLP, its accuracy directly affects the accuracy of NLP systems as a whole. In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000). Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called “pointwise” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art stru"
P11-2093,P07-1007,0,0.00818117,"cribed in the following section. 3 Domain Adaptation for Morphological Analysis NLP is now being used in domains such as medical text and legal documents, and it is necessary that MA be easily adaptable to these areas. In a domain adaptation situation, we have at our disposal both annotated general domain data, and unannotated target domain data. We would like to annotate the target domain data efficiently to achieve a maximal gain in accuracy for a minimal amount of work. Active learning has been used as a way to pick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here. When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008). However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data. Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010). In partial annotation, data that will not contribute to the improv"
P11-2093,W00-1322,0,0.101669,"Missing"
P11-2093,P09-1058,0,0.00709397,"e annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. 2 Japanese Morphological Analysis Japanese MA takes an unsegmented string of characters xI1 as input, segments it into morphemes wJ1 , and annotates each morpheme with a part of speech tJ1 . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string from unsegmented text (Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et al., 2009). In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method. 2.1 Joint Sequence-Based MA Japanese MA has traditionally used sequence based models, finding a maximal POS sequence for enType Character n-gram Char. Type n-gram WS Only POS Only Figure 1: Joint MA (a) performs maximization over the entire sequence, while two-step MA (b) maximizes the 4 boundary and 4 POS tags independently. Type Unigram Bigram Feature Strings tj , tj wj , c(wj ), tj c(wj ) tj−1 tj , tj−1 tj wj−1 , tj−1 tj wj , tj−1 tj wj−1 wj Table 1: Feat"
P11-2093,W04-3230,0,0.778036,"string of morphemes annotated with parts of speech (POSs). As MA is the first step in Japanese NLP, its accuracy directly affects the accuracy of NLP systems as a whole. In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000). Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called “pointwise” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach (Kud"
P11-2093,I08-7018,0,0.014986,"y annotated data has been presented by Tsuboi et al. (2008). However, when using partial annotation, CRFs’ already slow training time becomes slower still, as they must be trained over every sequence that has at least one annotated point. Training time is important in an active learning situation, as an annotator must wait while the model is being re-trained. 4 Experiments In order to test the effectiveness of pointwise MA, we did an experiment measuring accuracy both on in-domain data, and in a domain-adaptation situation. We used the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), specifying the whitepaper, news, and books sections as our general domain corpus, and the web text section as our target domain corpus (Table 3). As a representative of joint sequence-based MA described in 2.1, we used MeCab (Kudo, 2006), an open source implementation of Kudo et al. (2004)’s CRF-based method (we will call this JOINT). For the pointwise two-step method, we trained logistic regression models with the LIBLINEAR toolkit (Fan et al., 2008) using the features described in Section 2.2 (2- LR). In addition, we trained a CRF-based model with the CRFSuite toolkit (Okazaki, 2007) using"
P11-2093,C94-1032,0,0.0284615,"g of Japanese text as input, and outputs a string of morphemes annotated with parts of speech (POSs). As MA is the first step in Japanese NLP, its accuracy directly affects the accuracy of NLP systems as a whole. In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000). Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called “pointwise” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accura"
P11-2093,C04-1067,0,0.00975807,"ular sentence are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. 2 Japanese Morphological Analysis Japanese MA takes an unsegmented string of characters xI1 as input, segments it into morphemes wJ1 , and annotates each morpheme with a part of speech tJ1 . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string from unsegmented text (Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et al., 2009). In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method. 2.1 Joint Sequence-Based MA Japanese MA has traditionally used sequence based models, finding a maximal POS sequence for enType Character n-gram Char. Type n-gram WS Only POS Only Figure 1: Joint MA (a) performs maximization over the entire sequence, while two-step MA (b) maximizes the 4 boundary and 4 POS tags independently. Type Unigram Bigram Feature Strings tj , tj wj , c(wj ), tj c(wj ) tj−1 tj , tj−1 tj wj−1 , tj−1 tj wj , tj−"
P11-2093,neubig-mori-2010-word,1,0.450813,"(Escudero et al., 2000). Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called “pointwise” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach (Kudo et al., 2004) on in-domain data, and is significantly more robust to out-of-domain data. We also show that pointwise MA can be adapted to new domains with minimal effort through the combination of active learning and partial annotation (Tsuboi et al., 2008), where only informative parts of a particular sentence are annotated. In"
P11-2093,W04-3236,0,0.0194173,"h the combination of active learning and partial annotation (Tsuboi et al., 2008), where only informative parts of a particular sentence are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. 2 Japanese Morphological Analysis Japanese MA takes an unsegmented string of characters xI1 as input, segments it into morphemes wJ1 , and annotates each morpheme with a part of speech tJ1 . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string from unsegmented text (Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et al., 2009). In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method. 2.1 Joint Sequence-Based MA Japanese MA has traditionally used sequence based models, finding a maximal POS sequence for enType Character n-gram Char. Type n-gram WS Only POS Only Figure 1: Joint MA (a) performs maximization over the entire sequence, while two-step MA (b) maximizes the 4 boundary and 4 POS tags ind"
P11-2093,C04-1081,0,0.0439892,"ry as features (Table 2). Specifically dictionary features for word segmentation ls and rs are active if a string of length s included in the dictionary is present directly to the left or right of the present word boundary, and is is active if the present word boundary is included in a dictionary word of length s. Dictionary feature djk for POS estimation indicates whether the current word wj occurs as a dictionary entry with tag tk . Previous work using this two-stage approach has used sequence-based prediction methods, such as maximum entropy Markov models (MEMMs) or CRFs (Ng and Low, 2004; Peng et al., 2004). However, as Liang et al. (2008) note, and we confirm, sequence-based predictors are often not necessary when an appropriately rich feature set is used. One important difference between our formulation and that of Liang et al. (2008) and all other previous methods is that we rely only on features that are directly calculable from the surface string, without using estimated information such as word boundaries or neighboring POS tags2 . This allows for training from sentences that are partially annotated as described in the following section. 3 Domain Adaptation for Morphological Analysis NLP i"
P11-2093,W10-0104,0,0.0352595,"Missing"
P11-2093,W07-1516,0,0.0171938,"n situation, we have at our disposal both annotated general domain data, and unannotated target domain data. We would like to annotate the target domain data efficiently to achieve a maximal gain in accuracy for a minimal amount of work. Active learning has been used as a way to pick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here. When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008). However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data. Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010). In partial annotation, data that will not contribute to the improvement of the classifier is left untagged. For example, if there is a single difficult word in a long sentence, only the word boundaries and POS of the difficult word will be tagged. “Dif2 Dictionary features are active if the string exists, re"
P11-2093,P10-1037,0,0.0291745,"ick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here. When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008). However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data. Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010). In partial annotation, data that will not contribute to the improvement of the classifier is left untagged. For example, if there is a single difficult word in a long sentence, only the word boundaries and POS of the difficult word will be tagged. “Dif2 Dictionary features are active if the string exists, regardless of whether it is treated as a single word in wJ1 , and thus can be calculated without the word segmentation result. Type General Target Train 782k 153k Test 87.5k 17.3k Table 3: General and target domain corpus sizes in words. ficult” words can be selected using active learning a"
P11-2093,P02-1064,0,0.0990311,"(xr xr+1 xr+2 ) ls , rs , is wj , c(wj ), djk Table 2: Features for the two-step model. xl and xr indicate the characters to the left and right of the word boundary or word wj in question. ls , rs , and is represent the left, right, and inside dictionary features, while djk indicates that tag k exists in the dictionary for word j. 2.2 2-Step Pointwise MA In our research, we take a two-step approach, first segmenting character sequence xI1 into the word sequence wJ1 with the highest probability, then tagging each word with parts of speech tJ1 . This approach is shown in Figure 1 (b). We follow Sassano (2002) in formulating word segmentation as a binary classification problem, estimating boundary tags bI−1 1 . Tag bi = 1 indicates that a word boundary exists between characters xi and xi+1 , while bi = 0 indicates that a word boundary does not exist. POS estimation can also be formulated as a multi-class classification problem, where we choose one tag tj for each word wj . These two classification problems can be solved by tools in the standard machine learning toolbox such as logistic regression (LR), support vector machines (SVMs), or conditional random fields (CRFs). We use information about the"
P11-2093,D08-1112,0,0.0215178,"t our disposal both annotated general domain data, and unannotated target domain data. We would like to annotate the target domain data efficiently to achieve a maximal gain in accuracy for a minimal amount of work. Active learning has been used as a way to pick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here. When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008). However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data. Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010). In partial annotation, data that will not contribute to the improvement of the classifier is left untagged. For example, if there is a single difficult word in a long sentence, only the word boundaries and POS of the difficult word will be tagged. “Dif2 Dictionary features are active if the string exists, regardless of whether it is t"
P11-2093,C08-1113,1,0.943702,"” prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010). While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach (Kudo et al., 2004) on in-domain data, and is significantly more robust to out-of-domain data. We also show that pointwise MA can be adapted to new domains with minimal effort through the combination of active learning and partial annotation (Tsuboi et al., 2008), where only informative parts of a particular sentence are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. 2 Japanese Morphological Analysis Japanese MA takes an unsegmented string of characters xI1 as input, segments it into morphemes wJ1 , and annotates each morpheme with a part of speech tJ1 . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string"
P12-1018,P02-1051,0,0.0205265,"ey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumer"
P12-1018,I08-1033,0,0.0173092,"ed by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word"
P12-1018,N10-1028,0,0.511558,"r)I(as,S,u,U )I(aS,t,U,v ) s≤S≤t u≤U ≤v + X X Px (inv)I(as,S,U,v )I(aS,t,u,U ) s≤S≤t u≤U ≤v where Px (str) and Px (inv) are the probability of straight and inverted ITG productions. While the exact calculation of these probabilities can be performed in O(n6 ) time, where n is the 2 Pt can be specified according to Bayesian statistics as described by Neubig et al. (2011). 168 length of the sentence, this is impractical for all but the shortest sentences. Thus it is necessary to use methods to reduce the search space such as beamsearch based chart parsing (Saers et al., 2009) or slice sampling (Blunsom and Cohn, 2010).3 In this section we propose the use of a look-ahead probability to increase the efficiency of this chart parsing. Taking the example of Saers et al. (2009), spans are pushed onto a different queue based on their size, and queues are processed in ascending order of size. Agendas can further be trimmed based on a histogram beam (Saers et al., 2009) or probability beam (Neubig et al., 2011) compared to the best hypothesis a ˆ. In other words, we have a queue discipline based on the inside probability, and all spans ak where I(ak ) &lt; cI(ˆ a) are pruned. c is a constant describing the width of th"
P12-1018,P09-1088,0,0.0977356,"normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). 167 enough information to allow for effective alignment with its corresponding elements in eI1 . While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there have been advances in many-to-many alignment techniques that are able to align multi-element chunks on both sides of the translation (Marcu and Wong, 2002; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). Many-to-many methods can be expected to achieve superior results on character-based alignment, as the aligner can use information about substrings, which may correspond to letters, morphemes, words, or short phrases. Here, we focus on the model presented by Neubig et al. (2011), which uses Bayesian inference in the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more i"
P12-1018,W07-0735,0,0.0131682,"is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of wor"
P12-1018,J93-2003,0,0.0971778,"ir. We represent our target and source sentences as eI1 and f J1 . ei and fj represent single elements of the target and source sentences respectively. These may be words in word-based alignment models or single characters in character-based alignment models.1 We define our alignment as aK 1 , where each element is a span ak = hs, t, u, vi indicating that the target string es , . . . , et and source string fu , . . . , fv are aligned to each-other. 3.1 One-to-Many Alignment The most well-known and widely-used models for bitext alignment are for one-to-many alignment, including the IBM models (Brown et al., 1993) and HMM alignment model (Vogel et al., 1996). These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P (eI1 |f J1 , aK 1 ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2"
P12-1018,2002.tmi-papers.3,0,0.0215762,"in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods f"
P12-1018,W08-0336,0,0.0274774,"Missing"
P12-1018,D09-1075,0,0.0227118,"ion (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach is attractive conceptual"
P12-1018,P06-3003,0,0.165382,"ter-based) Model 1 probability, which can be efficiently calculated using the dynamic programming algorithm described by Brown et al. (1993). However, for reasons previously stated in Section 3, these methods are less satisfactory when performing character-based alignment, as the amount of information contained in a character does not allow for proper alignment. 5.2 Substring Co-occurrence Priors Instead, we propose a method for using raw substring co-occurrence statistics to bias alignments towards substrings that often co-occur in the entire training corpus. This is similar to the method of Cromieres (2006), but instead of using these cooccurrence statistics as a heuristic alignment criterion, we incorporate them as a prior probability in a statistical model that can take into account mutual exclusivity of overlapping substrings in a sentence. We define this prior probability using three counts over substrings c(e), c(f ), and c(e, f ). c(e) and c(f ) count the total number of sentences in which the substrings e and f occur respectively. c(e, f ) is a count of the total number of sentences in which the substring e occurs on the target side, and f occurs on the source side. We perform the calcula"
P12-1018,D08-1033,0,0.0307241,"Missing"
P12-1018,W11-2107,0,0.014441,"riments, although it does indicate that we must have access to tokenized data for the development set. 7 171 6.2 Quantitative Evaluation Table 2 presents a quantitative analysis of the translation results for each of the proposed methods. As previous research has shown that it is more difficult to translate into morphologically rich languages than into English (Koehn, 2005), we perform experiments translating in both directions for all language pairs. We evaluate translation quality using BLEU score (Papineni et al., 2002), both on the word and character level (with n = 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. It can be seen that character-based translation with all of the proposed alignment improvements greatly exceeds character-based translation using one-to-many alignment, confirming that substringbased information is necessary for accurate alignments. When compared with word-based translation, character-based translation achieves better, comparable, or inferior results on character-based BLEU, comparable or inferior results on METEOR, and inferior results on word-based BLEU. The differences between the evaluation metrics are due to the fact that character-based translation of"
P12-1018,H05-1085,0,0.0216227,"ollection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine"
P12-1018,P09-1104,0,0.122759,"when co-occurrence counts are used. More importantly, they allow for more aggressive beam pruning, increasing sampling speed from 1.3 sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6 sent/s for Japanese. 7 Conclusion and Future Directions This paper demonstrated that character-based translation can act as a unified framework for handling difficult problems in translation: morphology, compound words, transliteration, and segmentation. One future challenge includes scaling training up to longer sentences, which can likely be achieved through methods such as the heuristic span pruning of Haghighi et al. (2009) or sentence splitting of Vilar et al. (2007). Monolingual data could also be used to improve estimates of our substring-based prior. In addition, error analysis showed that wordbased translation performed better than characterbased translation on reordering and lexical choice, indicating that improved decoding (or pre-ordering) and language modeling tailored to character-based translation will likely greatly improve accuracy. Finally, we plan to explore the middle ground between word-based and character based translation, allowing for the flexibility of character-based translation, while usin"
P12-1018,N07-1018,0,0.0294521,"ng, 2003), and tic-tac-toe pruning for wordbased ITGs (Zhang and Gildea, 2005). As the calculation of the actual outside probability O(ak ) is just as expensive as parsing itself, it is necessary to approximate this with heuristic function O∗ that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2 , compared with the n3 amortized time of the tic-tac-toe pruning 3 Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson et al., 2007) can be used to compensate. However, we found that this had no significant effect on results, so we omit the Metropolis-in-Gibbs step for experiments. algorithm described by (Zhang et al., 2008a). During the calculation of the phrase generation probabilities Pt , we save the best inside probability I ∗ for each monolingual span. Ie∗ (s, t) = max If∗ (u, v) = max {˜ a=h˜ s,t˜,˜ u,˜ v i;˜ s=s,t˜=t} Pt (˜ a) {˜ a=h˜ s,t˜,˜ u,˜ v i;˜ u=u,˜ v =v} Pt (˜ a) For each language independently, we calculate forward probabilities α and backward probabilities β. For example, αe (s) is the maximum probabilit"
P12-1018,N03-1016,0,0.0120405,"is unwise to ignore competing hypotheses during beam pruning. Particularly, the alignment “les/1960s” competes with the high-probability alignment “les/the,” so intuitively should be a good candidate for pruning. However its probability is only slightly higher than “ann´ees/1960s,” which has no competing hypotheses and thus should not be trimmed. In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I(ak ), but also the outside probability O(ak ), the probability of generating all spans other than ak , as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for wordbased ITGs (Zhang and Gildea, 2005). As the calculation of the actual outside probability O(ak ) is just as expensive as parsing itself, it is necessary to approximate this with heuristic function O∗ that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2 , compared with the n3 amortized time of the tic-tac-toe pruning 3 Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson e"
P12-1018,N03-1017,0,0.0487988,"n et al., 1993) and HMM alignment model (Vogel et al., 1996). These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P (eI1 |f J1 , aK 1 ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2003). However, in order for one-to-many alignment methods to be effective, each fj must contain 1 Some previous work has also performed alignment using morphological analyzers to normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). 167 enough information to allow for effective alignment with its corresponding elements in eI1 . While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there hav"
P12-1018,W04-3250,0,0.0751149,"35.45 en-fi 13.22 / 58.50 / 27.03 13.12 / 59.27 / 27.09 04.58 / 35.09 / 11.76 12.14 / 59.02 / 25.31 en-fr 32.19 / 69.20 / 52.39 31.66 / 69.61 / 51.98 10.31 / 42.84 / 25.06 27.74 / 67.44 / 48.56 en-ja 20.79 / 27.01 / 38.41 20.26 / 28.34 / 38.34 01.48 / 00.72 / 06.67 17.90 / 28.46 / 35.71 Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant difference from the best system according to the bootstrap resampling method at p = 0.05 (Koehn, 2004). source and target were 100 characters or less,6 the total size of which is shown in Table 1. In characterbased translation, white spaces between words were treated as any other character and not given any special treatment. Evaluation was performed on tokenized and lower-cased data. For alignment, we use the GIZA++ implementation of one-to-many alignment7 and the pialign implementation of the phrasal ITG models8 modified with the proposed improvements. For GIZA++, we used the default settings for word-based alignment, but used the HMM model for character-based alignment to allow for alignmen"
P12-1018,2005.mtsummit-papers.11,0,0.090289,"ed enhancements to the model. Finally, we perform a qualitative analysis, which finds that character-based translation can handle unsegmented text, conjugation, and proper names in a unified framework with no additional processing. 2 Related Work on Data Sparsity in SMT As traditional SMT systems treat all words as single tokens without considering their internal structure, major problems of data sparsity occur for less frequent tokens. In fact, it has been shown that there is a direct negative correlation between vocabulary 166 size (and thus sparsity) of a language and translation accuracy (Koehn, 2005). Sparsity causes trouble for alignment models, both in the form of incorrectly aligned uncommon words, and in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regu"
P12-1018,N03-2016,0,0.0319786,"logical analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), alth"
P12-1018,N04-4015,0,0.0188944,"f garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named en"
P12-1018,P11-1140,0,0.0221195,"e segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and K"
P12-1018,W02-1018,0,0.0382455,"alignment using morphological analyzers to normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). 167 enough information to allow for effective alignment with its corresponding elements in eI1 . While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there have been advances in many-to-many alignment techniques that are able to align multi-element chunks on both sides of the translation (Marcu and Wong, 2002; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). Many-to-many methods can be expected to achieve superior results on character-based alignment, as the aligner can use information about substrings, which may correspond to letters, morphemes, words, or short phrases. Here, we focus on the model presented by Neubig et al. (2011), which uses Bayesian inference in the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the pr"
P12-1018,P11-1090,0,0.014234,"nsduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more in the following section. Phrasal ITGs are ITGs that allow for non-terminals that can emit phrase pairs with multiple elements on both the source and target sides. It should be noted that there are other many-to-many alignment methods that have been used for simultaneously discovering morphological boundaries over multiple languages (Snyder and Barzilay, 2008; Naradowsky and Toutanova, 2011), but these have generally been applied to single words or short phrases, and it is not immediately clear that they will scale to aligning full sentences. 4 Look-Ahead Biparsing In this work, we experiment with the alignment method of Neubig et al. (2011), which can achieve competitive accuracy with a much smaller phrase table than traditional methods. This is important in the character-based translation context, as we would like to use phrases that contain large numbers of characters without creating a phrase table so large that it cannot be used in actual decoding. In this framework, trainin"
P12-1018,P11-1064,1,0.701097,"of traditional word-based systems using only character strings. We draw upon recent advances in many-to-many alignment, which allows for the automatic choice of the length of units to be aligned. As these units may be at the character, subword, word, or multi-word phrase level, we conjecture that this will allow for better character alignments than one-to-many alignment techniques, and will allow for better translation of uncommon words than traditional word-based models by breaking down words into their component parts. We also propose two improvements to the manyto-many alignment method of Neubig et al. (2011). One barrier to applying many-to-many alignment models to character strings is training cost. In the inversion transduction grammar (ITG) framework (Wu, 1997), which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit. As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of Saers et al. (2009) by augmenting it with look-ahead probabilities in the spirit of A* search. Secondly, we describe a method to seed the search pr"
P12-1018,C10-1092,0,0.0075172,"1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach is attractive conceptually, previous research"
P12-1018,C00-2162,0,0.113372,"d uncommon words, and in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophistica"
P12-1018,J03-1002,0,0.00795056,"traditional SMT systems treat all words as single tokens without considering their internal structure, major problems of data sparsity occur for less frequent tokens. In fact, it has been shown that there is a direct negative correlation between vocabulary 166 size (and thus sparsity) of a language and translation accuracy (Koehn, 2005). Sparsity causes trouble for alignment models, both in the form of incorrectly aligned uncommon words, and in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more diffi"
P12-1018,P02-1040,0,0.101074,"atmt.org/moses/ 11 We chose this set-up to minimize the effect of tuning criterion on our experiments, although it does indicate that we must have access to tokenized data for the development set. 7 171 6.2 Quantitative Evaluation Table 2 presents a quantitative analysis of the translation results for each of the proposed methods. As previous research has shown that it is more difficult to translate into morphologically rich languages than into English (Koehn, 2005), we perform experiments translating in both directions for all language pairs. We evaluate translation quality using BLEU score (Papineni et al., 2002), both on the word and character level (with n = 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. It can be seen that character-based translation with all of the proposed alignment improvements greatly exceeds character-based translation using one-to-many alignment, confirming that substringbased information is necessary for accurate alignments. When compared with word-based translation, character-based translation achieves better, comparable, or inferior results on character-based BLEU, comparable or inferior results on METEOR, and inferior results on word-based BLEU. The"
P12-1018,W09-3804,0,0.613151,"els by breaking down words into their component parts. We also propose two improvements to the manyto-many alignment method of Neubig et al. (2011). One barrier to applying many-to-many alignment models to character strings is training cost. In the inversion transduction grammar (ITG) framework (Wu, 1997), which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit. As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of Saers et al. (2009) by augmenting it with look-ahead probabilities in the spirit of A* search. Secondly, we describe a method to seed the search process using counts of all substring pairs in the corpus to bias the phrase alignment model. We do this by defining prior probabilities based on these substring counts within the Bayesian phrasal ITG framework. An evaluation on four language pairs with differing morphological properties shows that for distant language pairs, character-based SMT can achieve translation accuracy comparable to word-based systems. In addition, we perform ablation studies, showing that thes"
P12-1018,P08-1084,0,0.0198035,"n the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more in the following section. Phrasal ITGs are ITGs that allow for non-terminals that can emit phrase pairs with multiple elements on both the source and target sides. It should be noted that there are other many-to-many alignment methods that have been used for simultaneously discovering morphological boundaries over multiple languages (Snyder and Barzilay, 2008; Naradowsky and Toutanova, 2011), but these have generally been applied to single words or short phrases, and it is not immediately clear that they will scale to aligning full sentences. 4 Look-Ahead Biparsing In this work, we experiment with the alignment method of Neubig et al. (2011), which can achieve competitive accuracy with a much smaller phrase table than traditional methods. This is important in the character-based translation context, as we would like to use phrases that contain large numbers of characters without creating a phrase table so large that it cannot be used in actual dec"
P12-1018,P11-1024,0,0.0120071,"oblem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt wi"
P12-1018,P06-1122,0,0.012896,"s in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliter"
P12-1018,2009.eamt-1.3,0,0.0804142,"for languages with explicit word The first author is now affiliated with the Nara Institute of Science and Technology. These difficulties occur because we are translating sequences of words as our basic unit. On the other hand, Vilar et al. (2007) examine the possibility of instead treating each sentence as sequences of characters to be translated. This method is attractive, as it is theoretically able to handle all sparsity phenomena in a single unified framework, but has only been shown feasible between similar language pairs such as Spanish-Catalan (Vilar et al., 2007), Swedish-Norwegian (Tiedemann, 2009), and ThaiLao (Sornlertlamvanich et al., 2008), which have a strong co-occurrence between single characters. As Vilar et al. (2007) state and we confirm, accurate translations cannot be achieved when applying traditional translation techniques to character-based translation for less similar language pairs. In this paper, we propose improvements to the alignment process tailored to character-based machine translation, and demonstrate that it is, in fact, possible to achieve translation accuracies that ap165 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,"
P12-1018,W07-0705,0,0.459683,"d eI1 is assumed to be a word in the source and target languages. However, the definition of a “word” is often problematic. The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (Chang et al., 2008). Even for languages with explicit word The first author is now affiliated with the Nara Institute of Science and Technology. These difficulties occur because we are translating sequences of words as our basic unit. On the other hand, Vilar et al. (2007) examine the possibility of instead treating each sentence as sequences of characters to be translated. This method is attractive, as it is theoretically able to handle all sparsity phenomena in a single unified framework, but has only been shown feasible between similar language pairs such as Spanish-Catalan (Vilar et al., 2007), Swedish-Norwegian (Tiedemann, 2009), and ThaiLao (Sornlertlamvanich et al., 2008), which have a strong co-occurrence between single characters. As Vilar et al. (2007) state and we confirm, accurate translations cannot be achieved when applying traditional translation"
P12-1018,C96-2141,0,0.510173,"ces as eI1 and f J1 . ei and fj represent single elements of the target and source sentences respectively. These may be words in word-based alignment models or single characters in character-based alignment models.1 We define our alignment as aK 1 , where each element is a span ak = hs, t, u, vi indicating that the target string es , . . . , et and source string fu , . . . , fv are aligned to each-other. 3.1 One-to-Many Alignment The most well-known and widely-used models for bitext alignment are for one-to-many alignment, including the IBM models (Brown et al., 1993) and HMM alignment model (Vogel et al., 1996). These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P (eI1 |f J1 , aK 1 ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2003). However, in order for one-to-many align"
P12-1018,J97-3002,0,0.511044,"th of units to be aligned. As these units may be at the character, subword, word, or multi-word phrase level, we conjecture that this will allow for better character alignments than one-to-many alignment techniques, and will allow for better translation of uncommon words than traditional word-based models by breaking down words into their component parts. We also propose two improvements to the manyto-many alignment method of Neubig et al. (2011). One barrier to applying many-to-many alignment models to character strings is training cost. In the inversion transduction grammar (ITG) framework (Wu, 1997), which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit. As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of Saers et al. (2009) by augmenting it with look-ahead probabilities in the spirit of A* search. Secondly, we describe a method to seed the search process using counts of all substring pairs in the corpus to bias the phrase alignment model. We do this by defining prior probabilities based on these substring"
P12-1018,P05-1059,0,0.00986268,"arly, the alignment “les/1960s” competes with the high-probability alignment “les/the,” so intuitively should be a good candidate for pruning. However its probability is only slightly higher than “ann´ees/1960s,” which has no competing hypotheses and thus should not be trimmed. In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I(ak ), but also the outside probability O(ak ), the probability of generating all spans other than ak , as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for wordbased ITGs (Zhang and Gildea, 2005). As the calculation of the actual outside probability O(ak ) is just as expensive as parsing itself, it is necessary to approximate this with heuristic function O∗ that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2 , compared with the n3 amortized time of the tic-tac-toe pruning 3 Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson et al., 2007) can be used to compensate. However, we found that this h"
P12-1018,P08-1012,0,0.710838,"n to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach"
P12-1018,W08-0335,0,0.0813058,"n to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach"
P12-1018,corston-oliver-gamon-2004-normalizing,0,\N,Missing
P12-1018,P10-3006,0,\N,Missing
P12-1018,I08-8003,0,\N,Missing
P12-1018,J98-4003,0,\N,Missing
P13-2119,D10-1092,1,0.436516,"Missing"
P13-2119,W12-3139,0,0.0208124,"Missing"
P13-2119,N06-2001,0,0.028358,"Missing"
P13-2119,W04-3250,0,0.464764,"Missing"
P13-2119,W12-2703,0,0.060629,"Missing"
P13-2119,D11-1033,0,0.191828,"vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams. 1 2 Data Selection Method We employ the data selection method of (Axelrod et al., 2011), which builds upon (Moore and Lewis, 2010). The intuition is to select general-domain sentences that are similar to indomain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large g"
P13-2119,P10-2041,0,0.175474,"language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams. 1 2 Data Selection Method We employ the data selection method of (Axelrod et al., 2011), which builds upon (Moore and Lewis, 2010). The intuition is to select general-domain sentences that are similar to indomain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentenc"
P13-2119,C90-3038,0,0.351908,"with English (en) as source and German (de), Spanish (es), French (fr), Russian (ru) as target. This is the in-domain corpus, and consists of TED Talk transcripts covering topics in technology, entertainment, and design. As general-domain corpora, we collected bitext from the WMT2013 campaign, including CommonCrawl and NewsCommentary for all 4 languages, Europarl for de/es/fr, UN for es/fr, Gigaword for fr, and Yandex for ru. The indomain data is divided into a training set (for SMT 1 Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. 2 The recurrent states are unrolled for several time-steps, then stochastic gradient descent is applied. 679 en-de en-es In-domain Training Set #sentence 129k 140k #t"
P13-2119,2012.eamt-1.60,0,0.0186427,"ontext as an identity (n-gram hit-or-miss) function on [w(t − 1), w(t − 2), . . .], neural LMs summarize the context by a hidden state vector s(t). This is a continuous vector of dimension |S |whose elements are predicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 3 Experiment Setup We experimented with four language pairs in the WIT3 corpus (Cettolo et al., 2012), with English (en) as source and German (de), Spanish (es), French (fr), Russian (ru) as target. This is the in-domain corpus, and consists of TED Talk transcripts covering topics in technology, entertainment, and design. As general-domain corpora, we collected bitext from the WMT2013 campaign, including CommonCrawl and NewsCommentary for all 4 languages, Europarl for de/es/fr, UN for es/fr, Gigaword for fr, and Yandex for ru. The indomain data is divided into a training set (for SMT 1 Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007"
P13-2119,2012.amta-papers.19,0,0.122261,"Missing"
P13-2119,2010.iwslt-papers.5,1,0.84743,"one defines the score of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized crossentropy of e on the English in-domain LM. GENE (e) is the length-normalized cross-entropy 678 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678–683, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0 (t), . . . , wk (t), . . . w|W |(t)]  Figure"
P13-2119,P02-1040,0,0.0858846,"Missing"
P13-2119,W12-2702,0,0.0173223,"Missing"
P13-2119,W12-3154,0,0.0129056,"core of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized crossentropy of e on the English in-domain LM. GENE (e) is the length-normalized cross-entropy 678 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678–683, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0 (t), . . . , wk (t), . . . w|W |(t)]  Figure 1: Recurrent neural LM. w"
P13-2119,2006.amta-papers.25,0,0.0371213,"Missing"
P13-2119,C12-1173,0,0.0163768,"Missing"
P13-2119,I08-2088,0,0.0198878,"Missing"
P13-2119,D10-1044,0,\N,Missing
P13-4016,J07-2003,0,0.923576,"to-string systems, on the other hand, it is necessary to have available or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process. In this demo paper, we describe Travatar, an open-source tree-to-string or forest-to-string translation system that can be used as a tool for translation using source-side syntax, and as a platform for research into syntax-based translation methods. In particular, compared to other decoders which mainly implement syntax-based translation in the synchronous context-free grammar (SCFG) framework (Chiang, 2007), Travatar is built upon the tree transducer framework (Graehl and Knight, 2004), a richer formalism that can help capture important distinctions between parse trees, as we show in Section 2. Travatar includes a fully documented training and testing regimen that was modeled around that of Moses, making it possible for users familiar with Moses to get started with Travatar quickly. The framework of the software is also designed to be extensible, so the toolkit is applicable for other tree-to-string transduction tasks. In the evaluation of the decoder on EnglishJapanese machine translation, we p"
P13-4016,W02-1001,0,0.0591167,"ies, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences between systems using bootstrap resampling (Koehn, 2004). 1 93 http://marisa-trie.googlecode.com 4 Experiments 4.1 Experimental Setup In our experiments, we validated the performance of the translation toolkit on English-J"
P13-4016,N10-1128,0,0.0116099,"oftware There are a number of open-source software packages that support tree-to-string translation in the SCFG framework. For example, Moses (Koehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008)."
P13-4016,P06-1121,0,0.0667488,"he data has been pre-processed, a treeto-string model can be trained with the training pipeline included in the toolkit. Like the training pipeline for Moses, there is a single script that performs alignment, rule extraction, scoring, and parameter initialization. Language model training can be performed using a separate toolkit, and instructions are provided in the documentation. For word alignment, the Travatar training pipeline is integrated with GIZA++ (Och and Ney, 2003) by default, but can also use alignments from any other aligner. Rule extraction is performed using the GHKM algorithm (Galley et al., 2006) and its extension to rule extraction from forests (Mi and Huang, 2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are For tuning the parameters of the model, Travatar natively supports minimum error"
P13-4016,P08-1112,0,0.00975996,"Missing"
P13-4016,N04-1014,0,0.862653,"le or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process. In this demo paper, we describe Travatar, an open-source tree-to-string or forest-to-string translation system that can be used as a tool for translation using source-side syntax, and as a platform for research into syntax-based translation methods. In particular, compared to other decoders which mainly implement syntax-based translation in the synchronous context-free grammar (SCFG) framework (Chiang, 2007), Travatar is built upon the tree transducer framework (Graehl and Knight, 2004), a richer formalism that can help capture important distinctions between parse trees, as we show in Section 2. Travatar includes a fully documented training and testing regimen that was modeled around that of Moses, making it possible for users familiar with Moses to get started with Travatar quickly. The framework of the software is also designed to be extensible, so the toolkit is applicable for other tree-to-string transduction tasks. In the evaluation of the decoder on EnglishJapanese machine translation, we perform a comparison to Moses’s phrase-based, hierarchicalphrase-based, and SCFG-"
P13-4016,2011.iwslt-evaluation.24,0,0.0119636,"emented using cube pruning (Chiang, 2007) is used to adjust the trade-off between search speed and translation accuracy. The source side of the translation model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.1 Rule lookup is performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). The Travatar Machine Translation Toolkit In this section, we describe the overall framework of the Travatar decoder, following the order of the training pipeline. 3.1 Data Preprocessing This consists of parsing the source side sentence and tokenizing the target side sentences. Travatar can decode input in the bracketed format of the Penn Treebank, or also in forest format. There is documentation and scripts for using Travatar with several parsers for English, Chinese, and Japanese included with the toolkit. 3.2 Training 3.4 Tuning and Evaluation Once the data has been pre-processed, a treeto"
P13-4016,P11-2093,1,0.351473,"cal phrase-based models were trained with the default settings according to tutorials on each web site. For all systems, we use a 5-gram Kneser-Ney smoothed language model. Alignment for each system was performed using either GIZA++3 or Nile4 with main results reported for the aligner that achieved the best accuracy on the dev set, and a further comparison shown in the auxiliary experiments in Section 4.3. Tuning was performed with minimum error rate training to maximize BLEU over 200-best lists. Tokenization was performed with the Stanford tokenizer for English, and the KyTea word segmenter (Neubig et al., 2011) for Japanese. For all tree-to-string systems we use Egret5 as an English parser, as we found it to achieve high accuracy, and it allows for the simple output of forests. Rule extraction was performed using onebest trees, which were right-binarized, and lowercased post-parsing. For Travatar, composed rules of up to size 4 and a maximum of 2 non-terminals and 7 terminals for each rule were used. Nullaligned words were only attached to the top node, and no count normalization was performed, in contrast to Moses, which performs count normalization and exhaustive null word attachment. Decoding was"
P13-4016,W11-2123,0,0.00482795,"The decoding itself is performed using the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). Beam-search implemented using cube pruning (Chiang, 2007) is used to adjust the trade-off between search speed and translation accuracy. The source side of the translation model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.1 Rule lookup is performed using left-toright depth-first search, which can be implemented as prefix lookup in the trie for efficient search. The language model storage uses the implementation in KenLM (Heafield, 2011), and particularly the implementation that maintains left and right language model states for syntax-based MT (Heafield et al., 2011). The Travatar Machine Translation Toolkit In this section, we describe the overall framework of the Travatar decoder, following the order of the training pipeline. 3.1 Data Preprocessing This consists of parsing the source side sentence and tokenizing the target side sentences. Travatar can decode input in the bracketed format of the Penn Treebank, or also in forest format. There is documentation and scripts for using Travatar with several parsers for English, C"
P13-4016,D10-1027,0,0.0181432,"which has been reported to achieve higher accuracy than the Stanford parser on several domains (Kummerfeld et al., 2012). From the translation results, we can see that STAN - 2 http://statmt.org/moses/ http://code.google.com/p/giza-pp/ 4 http://code.google.com/p/nile/ As Nile is a supervised aligner, we trained it on the alignments provided with the KFTT. 5 http://code.google.com/p/ egret-parser/ 3 6 http://nlp.stanford.edu/software/ lex-parser.shtml 94 PBMT HIER STAN - T 2 S EGRET- T 2 S EGRET- F 2 S GIZA++ BLEU RIBES 22.28 68.37 22.05 70.29 21.47 70.94 22.82 71.90 23.35 71.77 search speed (Huang and Mi, 2010). In addition, there is a preliminary implementation of the ability to introduce target-side syntactic information, either through hard constraints as in tree-to-tree translation systems (Graehl and Knight, 2004), or through soft constraints, as in syntax-augmented machine translation (Zollmann and Venugopal, 2006). Finally, we will provide better support of parallelization through the entire pipeline to increase the efficiency of training and decoding. Acknowledgements: We thank Kevin Duh and an anonymous reviewer for helpful comments. Part of this work was supported by JSPS KAKENHI Grant Num"
P13-4016,J03-1002,0,0.00698891,"Travatar with several parsers for English, Chinese, and Japanese included with the toolkit. 3.2 Training 3.4 Tuning and Evaluation Once the data has been pre-processed, a treeto-string model can be trained with the training pipeline included in the toolkit. Like the training pipeline for Moses, there is a single script that performs alignment, rule extraction, scoring, and parameter initialization. Language model training can be performed using a separate toolkit, and instructions are provided in the documentation. For word alignment, the Travatar training pipeline is integrated with GIZA++ (Och and Ney, 2003) by default, but can also use alignments from any other aligner. Rule extraction is performed using the GHKM algorithm (Galley et al., 2006) and its extension to rule extraction from forests (Mi and Huang, 2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phr"
P13-4016,2006.amta-papers.8,0,0.010814,"rk described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 3.3 Decoding Given a translation model Travatar is able to decode parsed input sentences to generate translations. The decoding itself is performed using the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). Beam-search implemented using cube pruning (Chiang, 2007) is used to adjust the trade-off between search speed and translation accuracy. The source side of the translation model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.1 Rule lookup is performed using left-toright depth-first search,"
P13-4016,P03-1021,0,0.0117765,"rule extraction from forests (Mi and Huang, 2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit als"
P13-4016,P02-1040,0,0.106403,"ent of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the s"
P13-4016,D10-1092,0,0.0590542,"Missing"
P13-4016,P10-1017,0,0.0413127,"Missing"
P13-4016,W10-1736,0,0.0277829,"either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences betwe"
P13-4016,D07-1078,0,0.0299373,"d instructions are provided in the documentation. For word alignment, the Travatar training pipeline is integrated with GIZA++ (Och and Ney, 2003) by default, but can also use alignments from any other aligner. Rule extraction is performed using the GHKM algorithm (Galley et al., 2006) and its extension to rule extraction from forests (Mi and Huang, 2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implem"
P13-4016,P07-2045,0,0.0297737,"f Science and Technology 8916-5 Takayama-cho, Ikoma-shi, Nara, Japan neubig@is.naist.jp Abstract McCord, 2004; Isozaki et al., 2010b), and treebased decoding techniques, which take a tree or forest as input and choose the reordering and translation jointly (Yamada and Knight, 2001; Liu et al., 2006; Mi et al., 2008). While pre-ordering is not able to consider both translation and reordering in a joint model, it is useful in that it is done before the actual translation process, so it can be performed with a conventional translation pipeline using a standard phrase-based decoder such as Moses (Koehn et al., 2007). For tree-to-string systems, on the other hand, it is necessary to have available or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process. In this demo paper, we describe Travatar, an open-source tree-to-string or forest-to-string translation system that can be used as a tool for translation using source-side syntax, and as a platform for research into syntax-based translation methods. In particular, compared to other decoders which mainly implement syntax-based translation in the synchronous context-free grammar (SCFG)"
P13-4016,P10-1034,0,0.0147739,"parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 3.3 Decoding Given a translation model Travatar is able to decode parsed input sentences to generate translations. The decoding itself is performed using the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). Beam-search implemented using"
P13-4016,P12-3022,0,0.0235427,"ftware packages that support tree-to-string translation in the SCFG framework. For example, Moses (Koehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 3.3 Decoding Given a translation mod"
P13-4016,W04-3250,0,0.705023,"that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences between systems using bootstrap resampling (Koehn, 2004). 1 93 http://marisa-trie.googlecode.com 4 Experiments 4.1 Experimental Setup In our experiments, we validated the performance of the translation toolkit on English-Japanese translation of Wikipedia articles, as specified by the Kyoto Free Translation Task (KFTT) (Neubig, 2011). Training used the 405k sentences of training data of length under 60, tuning was performed on the development set, and testing was performed on the test set using the BLEU and RIBES measures. As baseline systems we use the Moses2 implementation of phrase-based (MOSES - PBMT), hierarchical phrase-based (MOSES - HIER), a"
P13-4016,P09-1019,0,0.0130464,"2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar et al., 2009). This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future. The Travatar toolkit also provides an evaluation program that can calculate t"
P13-4016,C04-1073,0,0.15511,"Missing"
P13-4016,P12-3004,0,0.0182008,"ules supported by SCFGs and tree transducers is shown in Figure 1. In this example, the first rule is a simple multi-word noun phrase, the second example is an example of a delexicalized rule expressing translation from English SVO word order to Japanese SOV word order. The third and fourth examples are translations of a verb, noun phrase, and prepositional phrase, where the third rule has 2.2 The State of Open Source Software There are a number of open-source software packages that support tree-to-string translation in the SCFG framework. For example, Moses (Koehn et al., 2007) and NiuTrans (Xiao et al., 2012) support the annotation of source-side syntactic labels, and taking parse trees (or in the case of NiuTrans, forests) as input. There are also a few other decoders that support other varieties of using source-side syntax to help improve translation or global reordering. For example, the cdec decoder (Dyer et al., 2010) supports the context-free-reordering/finitestate-translation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 stored as sparse vectors by default, which allows for scoring using an arbitrarily large num"
P13-4016,D12-1096,0,0.0144124,"RAV- T 2 S, with BLEU slightly and RIBES greatly exceeding that of MOSES - T 2 S. 4.3 Effect of Alignment/Parsing In addition, as auxiliary results, we present a comparison of Travatar’s tree-to-string and forest-tostring systems using different alignment methods and syntactic parsers to examine the results on translation (Table 2). For parsers, we compared Egret with the Stanford parser.6 While we do not have labeled data to calculate parse accuracies with, Egret is a clone of the Berkeley parser, which has been reported to achieve higher accuracy than the Stanford parser on several domains (Kummerfeld et al., 2012). From the translation results, we can see that STAN - 2 http://statmt.org/moses/ http://code.google.com/p/giza-pp/ 4 http://code.google.com/p/nile/ As Nile is a supervised aligner, we trained it on the alignments provided with the KFTT. 5 http://code.google.com/p/ egret-parser/ 3 6 http://nlp.stanford.edu/software/ lex-parser.shtml 94 PBMT HIER STAN - T 2 S EGRET- T 2 S EGRET- F 2 S GIZA++ BLEU RIBES 22.28 68.37 22.05 70.29 21.47 70.94 22.82 71.90 23.35 71.77 search speed (Huang and Mi, 2010). In addition, there is a preliminary implementation of the ability to introduce target-side syntactic"
P13-4016,P01-1067,0,0.0894209,"Missing"
P13-4016,P06-1077,0,0.30827,"ranslation framework described by Dyer and Resnik (2010). The Akamon decoder (Wu et al., 2012) supports translation using head-driven 92 stored as sparse vectors by default, which allows for scoring using an arbitrarily large number of feature functions. phrase structure grammars as described by Wu et al. (2010). However, to our knowledge, while there is a general-purpose tool for tree automata in general (May and Knight, 2006), there is no open-source toolkit implementing the SMT pipeline in the tree transducer framework, despite it being a target of active research (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008). 3 3.3 Decoding Given a translation model Travatar is able to decode parsed input sentences to generate translations. The decoding itself is performed using the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). Beam-search implemented using cube pruning (Chiang, 2007) is used to adjust the trade-off between search speed and translation accuracy. The source side of the translation model is stored using a space-efficient trie data structure (Yata, 2012) implemented using the marisa-trie toolkit.1 Rule lookup is performed using left-toright"
P13-4016,W06-3119,0,0.0360694,"trained it on the alignments provided with the KFTT. 5 http://code.google.com/p/ egret-parser/ 3 6 http://nlp.stanford.edu/software/ lex-parser.shtml 94 PBMT HIER STAN - T 2 S EGRET- T 2 S EGRET- F 2 S GIZA++ BLEU RIBES 22.28 68.37 22.05 70.29 21.47 70.94 22.82 71.90 23.35 71.77 search speed (Huang and Mi, 2010). In addition, there is a preliminary implementation of the ability to introduce target-side syntactic information, either through hard constraints as in tree-to-tree translation systems (Graehl and Knight, 2004), or through soft constraints, as in syntax-augmented machine translation (Zollmann and Venugopal, 2006). Finally, we will provide better support of parallelization through the entire pipeline to increase the efficiency of training and decoding. Acknowledgements: We thank Kevin Duh and an anonymous reviewer for helpful comments. Part of this work was supported by JSPS KAKENHI Grant Number 25730136. Nile BLEU RIBES 22.37 68.43 21.77 69.31 22.44 72.02 23.15 72.32 23.97 73.27 Table 2: Translation results (BLEU, RIBES), for several translation models (PBMT, Hiero, T2S, F2S), aligners (GIZA++, Nile), and parsers (Stanford, Egret). T 2 S significantly underperforms EGRET- T 2 S , confirming that the e"
P13-4016,D08-1022,0,0.0300937,"h the training pipeline included in the toolkit. Like the training pipeline for Moses, there is a single script that performs alignment, rule extraction, scoring, and parameter initialization. Language model training can be performed using a separate toolkit, and instructions are provided in the documentation. For word alignment, the Travatar training pipeline is integrated with GIZA++ (Och and Ney, 2003) by default, but can also use alignments from any other aligner. Rule extraction is performed using the GHKM algorithm (Galley et al., 2006) and its extension to rule extraction from forests (Mi and Huang, 2008). There are also a number of options implemented, including rule composition, attachment of nullaligned target words at either the highest point in the tree, or at every possible position, and left and right binarization (Galley et al., 2006; Wang et al., 2007). Rule scoring uses a standard set of forward and backward conditional probabilities, lexicalized translation probabilities, phrase frequency, and word and phrase counts. Rule scores are For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) (Och, 2003) and is extension to hypergraphs (Kumar"
P13-4016,P08-1023,0,0.243925,"al, losing the ability to distinguish between the very information that parsers are designed to disambiguate. In traditional tree-to-string translation methods, the translator uses a single one-best parse tree output by a syntactic parser, but parse errors have the potential to degrade the quality of translation. An important advance in tree-to-string translation that helps ameliorate this difficulity is forest-to-string translation, which represents a large number of potential parses as a packed forest, allowing the translator to choose between these parses during the process of translation (Mi et al., 2008). Figure 1: Tree-to-string translation rules for SCFGs and tree transducers. translation. Based on the results, we find that treeto-string, and particularly forest-to-string, translation using Travatar provides competitive or superior accuracy to all of these techniques. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. 2 Tree-to-String Translation 2.1 Overview Tree-to-string translation uses syntactic information to improve translation by first parsing the source sentence, then using this source-"
P13-4016,P10-4002,0,\N,Missing
P14-2024,2008.amta-srw.1,0,0.0300649,"-mentioned elements must be taken seriously in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first step in examining and codifying the more important elements that make it possible to construct a highly accurate T2S MT system. To do so, we perform an empirical study of the effect of parsing accuracy, packed forest input, alignment accuracy, and search. The reason why we"
P14-2024,P06-1002,0,0.0273261,"http://plata.ar.media.kyoto-u.ac.jp/tool/EDA 145 Name GIZA++ Nile/16 Nile/4 Nile crease in BLEU at the cost of an increase in decoding time. Interestingly, the increases in BLEU did not show any sign of saturating even when setting the n-best cutoff to 200, although larger cutoffs resulted in exceedingly large translation forests that required large amounts of memory. Alignment Overview The second element that we investigate is alignment accuracy. It has been noted in many previous works that significant gains in alignment accuracy do not make a significant difference in translation results (Ayan and Dorr, 2006; Ganchev et al., 2008). However, none of these works have explicitly investigated the effect on T2S translation, so it is not clear whether these results carry over to our current situation. As our baseline aligner, we use the GIZA++ implementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the discriminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit.6 This method has the ability to use source- and targetside syntactic information, and has been shown to improve the accuracy of S2"
P14-2024,J07-2003,0,0.0123811,"0.71 1.75 2.96 4.80 1.75 4.34 8.73 en-ja HS en-ja CP 0.09 0.08 0.14 0.13 0.25 0.37 0.57 0.24 0.44 0.64 1.21 2.22 3.97 1.60 3.83 5.74 ja-en HS ja-en CP 30 0.0 29 100 1000 100 10000 1000 Pop Limit 10000 Figure 3: Hypergraph search (HS) and cube pruning (CP) results for F2S and T2S. Numbers above and below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripher"
P14-2024,P05-1066,0,0.0379111,"nt element of statistical machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see large increases of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods based on syntactic pre- or post-ordering. 143 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143"
P14-2024,D12-1109,0,0.0120695,"3: Hypergraph search (HS) and cube pruning (CP) results for F2S and T2S. Numbers above and below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripheral elements that contribute greatly to the accuracy of T2S machine translation: parsing, alignment, and search. Put together, a T2S system that uses the more effective settings for these three elements greatly out"
P14-2024,I11-1087,1,0.67766,"le model of the Berkeley parser tends to have the higher accuracy of the two, so if the accuracy of a system using this model is higher then it is likely that parsing accuracy is important for T2S translation. Instead of the Berkeley Parser itself, we use a clone Egret,4 which achieves nearly identical accuracy, and is able to output packed forests for use in MT, as mentioned below. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other element in the phrase. For Japanese, our first method uses the MSTbased pointwise dependency parser of Flannery et al. (2011), as implemented in the Eda toolkit.5 In order to convert dependencies into phrasestructure trees typically used in T2S translation, we use the head rules implemented in the Travatar toolkit. In addition, we also train a latent variable CFG using the Berkeley Parser and use Egret for parsing. Both models are trained on the Japanese Word Dependency Treebank (Mori et al., 2014). In addition, Mi et al. (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. By doing so, it is possible to resolve some of the ambigu"
P14-2024,J07-3002,0,0.0225012,"Missing"
P14-2024,P06-1121,0,0.0606722,"nment accuracy, and search. The reason why we choose these elements is that past work that has reported low accuracy for T2S systems has often neglected to consider one or all of these elements. 1 Introduction In recent years, syntactic parsing is being viewed as an ever-more important element of statistical machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see l"
P14-2024,P08-1112,0,0.0227997,"Missing"
P14-2024,P12-2061,0,0.0149337,"ison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which are state-ofthe-art methods for the language pairs. Specifically, for en-ja translation we use the head finalization pre-ordering method of (Isozaki et al., 2010b), and for ja-en translation, we use the syntactic postordering method of (Goto et al., 2012). For all systems, T2S or otherwise, the language model is a Kneser-Ney 5-gram, and tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 2.2 ja-en BLEU RIBES 30.49 69.80 29.41 69.51 29.42 73.85 31.15 72.87 33.70 75.94 Table 1: Overall results for five systems. we compare the 3 non-T2S baselines with two T2S systems that vary the settings of the parser, alignment, and search, as described in the following Sections 3, 4, and 5. The first system “T2Sall” is a system that uses the worst settings1 for each of these elements, while the second system “T2S+all” use"
P14-2024,N04-1014,0,0.0156771,"ever, after accounting for all these elements we see large increases of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods based on syntactic pre- or post-ordering. 143 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143–149, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Experimental Setup 2.1 System PBMT Hiero Pre/Post T2S-all T2S+all Systems Compared In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight, 2004), constructed using the Travatar toolkit (Neubig, 2013). Rules are extracted using the GHKM algorithm (Galley et al., 2006), and rules with up to 5 composed minimal rules, up to 2 nonterminals, and up to 10 terminals are used. We also prepare 3 baselines not based on T2S to provide a comparison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two me"
P14-2024,N13-1116,0,0.029048,"d below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripheral elements that contribute greatly to the accuracy of T2S machine translation: parsing, alignment, and search. Put together, a T2S system that uses the more effective settings for these three elements greatly outperforms a system that uses more standard settings, as well as the current state-of-the-art o"
P14-2024,D10-1092,1,0.0713085,"Missing"
P14-2024,W10-1736,1,0.771966,"nd up to 10 terminals are used. We also prepare 3 baselines not based on T2S to provide a comparison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which are state-ofthe-art methods for the language pairs. Specifically, for en-ja translation we use the head finalization pre-ordering method of (Isozaki et al., 2010b), and for ja-en translation, we use the syntactic postordering method of (Goto et al., 2012). For all systems, T2S or otherwise, the language model is a Kneser-Ney 5-gram, and tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 2.2 ja-en BLEU RIBES 30.49 69.80 29.41 69.51 29.42 73.85 31.15 72.87 33.70 75.94 Table 1: Overall results for five systems. we compare the 3 non-T2S baselines with two T2S systems that vary the settings of the parser, alignment, and search, as described in the following Sections 3, 4, and 5. The first system “T2Sall” is a system t"
P14-2024,2012.amta-papers.27,0,0.0218767,"y in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first step in examining and codifying the more important elements that make it possible to construct a highly accurate T2S MT system. To do so, we perform an empirical study of the effect of parsing accuracy, packed forest input, alignment accuracy, and search. The reason why we choose these elements is that past work"
P14-2024,P07-2045,0,0.00710243,"imental Setup 2.1 System PBMT Hiero Pre/Post T2S-all T2S+all Systems Compared In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight, 2004), constructed using the Travatar toolkit (Neubig, 2013). Rules are extracted using the GHKM algorithm (Galley et al., 2006), and rules with up to 5 composed minimal rules, up to 2 nonterminals, and up to 10 terminals are used. We also prepare 3 baselines not based on T2S to provide a comparison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which are state-ofthe-art methods for the language pairs. Specifically, for en-ja translation we use the head finalization pre-ordering method of (Isozaki et al., 2010b), and for ja-en translation, we use the syntactic postordering method of (Goto et al., 2012). For all systems, T2S or otherwise, the language model is a Kneser-Ney 5-gram, and tuning is performed to maximize BLEU score usi"
P14-2024,W04-3250,0,0.0191526,"n translation over data from the NTCIR PatentMT task (Goto et al., 2011), the most standard benchmark task for these language pairs. We use the training data from NTCIR 7/8, a total of approximately 3.0M sentences, and perform tuning on the NTCIR 7 dry run, testing on the NTCIR 7 formal run data. As evaluation measures, we use the standard BLEU (Papineni et al., 2002) as well as RIBES (Isozaki et al., 2010a), a reorderingbased metric that has been shown to have high correlation with human evaluations on the NTCIR data. We measure significance of results using bootstrap resampling at p &lt; 0.05 (Koehn, 2004). In tables, bold numbers indicate the best system and all systems that were not significantly different from the best system. 2.3 en-ja BLEU RIBES 35.84 72.89 34.45 72.94 36.69 77.05 36.23 76.60 40.84 80.15 3 Parsing 3.1 Parsing Overview As T2S translation uses parse trees both in training and testing of the system, an accurate syntactic parser is required. In order to test the extent that parsing accuracy affects translation, we use two 1 Stanford/Eda, GIZA++, pop-limit 5000 cube pruning. forests, Nile, pop-limit 5000 hypergraph search. 3 We have also observed similar trends on other genres"
P14-2024,D12-1096,0,0.010034,"s of en-ja 24.55→30.81, ja-en 19.28→22.46, zh-ja 15.22→20.67, ja-zh 30.88→33.89. 2 Egret Motivational Experiment Before going into a detailed analysis, we first present results that stress the importance of the elements described in the introduction. To do so, 144 different syntactic parsers and examine the translation accuracy realized by each parser. For English, the two most widely referenced parsers are the Stanford Parser and Berkeley Parser. In this work, we compare the Stanford Parser’s CFG model, with the Berkeley Parser’s latent variable model. In previous reports, it has been noted (Kummerfeld et al., 2012) that the latent variable model of the Berkeley parser tends to have the higher accuracy of the two, so if the accuracy of a system using this model is higher then it is likely that parsing accuracy is important for T2S translation. Instead of the Berkeley Parser itself, we use a clone Egret,4 which achieves nearly identical accuracy, and is able to output packed forests for use in MT, as mentioned below. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other element in the phrase. For Japanese, our first method uses the MSTbased pointwis"
P14-2024,P06-1077,0,0.0394828,"ms has often neglected to consider one or all of these elements. 1 Introduction In recent years, syntactic parsing is being viewed as an ever-more important element of statistical machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see large increases of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods ba"
P14-2024,P11-1128,0,0.026314,"Missing"
P14-2024,P08-1023,0,0.495906,"nd JapaneseEnglish pairs, we show how a basic T2S system that performs on par with phrasebased systems can be improved by 2.6-4.6 BLEU, greatly exceeding existing stateof-the-art methods. These results indicate that T2S systems indeed hold much promise, but the above-mentioned elements must be taken seriously in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first"
P14-2024,mori-etal-2014-japanese,0,0.0216529,"ow. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other element in the phrase. For Japanese, our first method uses the MSTbased pointwise dependency parser of Flannery et al. (2011), as implemented in the Eda toolkit.5 In order to convert dependencies into phrasestructure trees typically used in T2S translation, we use the head rules implemented in the Travatar toolkit. In addition, we also train a latent variable CFG using the Berkeley Parser and use Egret for parsing. Both models are trained on the Japanese Word Dependency Treebank (Mori et al., 2014). In addition, Mi et al. (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. By doing so, it is possible to resolve some of the ambiguity in syntactic interpretation at translation time, potentially increasing translation accuracy. However, the great majority of recent works on T2S translation do not consider multiple syntactic parses (e.g. Liu et al. (2011), Zhang et al. (2011)), and thus it is important to confirm the potential gains that could be acquired by taking ambiguity into account. 3.2 en-ja BLEU"
P14-2024,W13-4604,1,0.812763,"ntly different from the best system. 2.3 en-ja BLEU RIBES 35.84 72.89 34.45 72.94 36.69 77.05 36.23 76.60 40.84 80.15 3 Parsing 3.1 Parsing Overview As T2S translation uses parse trees both in training and testing of the system, an accurate syntactic parser is required. In order to test the extent that parsing accuracy affects translation, we use two 1 Stanford/Eda, GIZA++, pop-limit 5000 cube pruning. forests, Nile, pop-limit 5000 hypergraph search. 3 We have also observed similar trends on other genres and language pairs. For example, in a Japanese-Chinese/English medical conversation task (Neubig et al., 2013), forests, alignment, and search resulted in BLEU increases of en-ja 24.55→30.81, ja-en 19.28→22.46, zh-ja 15.22→20.67, ja-zh 30.88→33.89. 2 Egret Motivational Experiment Before going into a detailed analysis, we first present results that stress the importance of the elements described in the introduction. To do so, 144 different syntactic parsers and examine the translation accuracy realized by each parser. For English, the two most widely referenced parsers are the Stanford Parser and Berkeley Parser. In this work, we compare the Stanford Parser’s CFG model, with the Berkeley Parser’s laten"
P14-2024,P07-1019,0,0.0233075,"a-en HS ja-en CP 30 0.0 29 100 1000 100 10000 1000 Pop Limit 10000 Figure 3: Hypergraph search (HS) and cube pruning (CP) results for F2S and T2S. Numbers above and below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripheral elements that contribute greatly to the accuracy of T2S machine translation: parsing, alignment, and search. Put together, a T2S system that"
P14-2024,P13-4016,1,0.241763,"es of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods based on syntactic pre- or post-ordering. 143 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143–149, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Experimental Setup 2.1 System PBMT Hiero Pre/Post T2S-all T2S+all Systems Compared In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight, 2004), constructed using the Travatar toolkit (Neubig, 2013). Rules are extracted using the GHKM algorithm (Galley et al., 2006), and rules with up to 5 composed minimal rules, up to 2 nonterminals, and up to 10 terminals are used. We also prepare 3 baselines not based on T2S to provide a comparison with other systems in the literature. The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al., 2007). We use default settings, except for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which a"
P14-2024,D10-1027,0,0.0170447,"Limit 10000 Figure 3: Hypergraph search (HS) and cube pruning (CP) results for F2S and T2S. Numbers above and below the lines indicate time in seconds/sentence for HS and CP respectively. Search Overview Finally, we examine the effect that the choice of search algorithm has on the accuracy of translation. The most standard search algorithm for T2S translation is bottom-up beam search using cube pruning (CP, Chiang (2007)). However, there are a number of other search algorithms that have been proposed for tree-based translation in general (Huang and Chiang, 2007) or T2S systems in particular (Huang and Mi, 2010; Feng et al., 2012). In this work, we compare CP and the hypergraph search (HS) method of Heafield et al. (2013), which is also a bottom-up pruning algorithm but performs more efficient search by grouping together similar language model states. 5.2 1.81 0.72 1.07 0.0 30 100 1000 100 10000 5 Search 5.1 38 0.6 37 35 0.4 34 0.33 0.42 6 Conclusion In this paper, we discussed the importance of three peripheral elements that contribute greatly to the accuracy of T2S machine translation: parsing, alignment, and search. Put together, a T2S system that uses the more effective settings for these three"
P14-2024,J03-1002,0,0.00756703,"ted in exceedingly large translation forests that required large amounts of memory. Alignment Overview The second element that we investigate is alignment accuracy. It has been noted in many previous works that significant gains in alignment accuracy do not make a significant difference in translation results (Ayan and Dorr, 2006; Ganchev et al., 2008). However, none of these works have explicitly investigated the effect on T2S translation, so it is not clear whether these results carry over to our current situation. As our baseline aligner, we use the GIZA++ implementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the discriminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit.6 This method has the ability to use source- and targetside syntactic information, and has been shown to improve the accuracy of S2T translation. We trained Nile and tested both methods on the Japanese-English alignments provided with the Kyoto Free Translation Task (Neubig, 2011) (430k parallel sentences, 1074 manually aligned training sentences, and 120 manually aligned test sentences).7 As creating manual alignm"
P14-2024,P03-1021,0,0.0210137,"t for setting the reordering limit or maximum chart span to the best-performing value of 24. As our last baselines, we use two methods based on syntactic pre- or post-ordering, which are state-ofthe-art methods for the language pairs. Specifically, for en-ja translation we use the head finalization pre-ordering method of (Isozaki et al., 2010b), and for ja-en translation, we use the syntactic postordering method of (Goto et al., 2012). For all systems, T2S or otherwise, the language model is a Kneser-Ney 5-gram, and tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 2.2 ja-en BLEU RIBES 30.49 69.80 29.41 69.51 29.42 73.85 31.15 72.87 33.70 75.94 Table 1: Overall results for five systems. we compare the 3 non-T2S baselines with two T2S systems that vary the settings of the parser, alignment, and search, as described in the following Sections 3, 4, and 5. The first system “T2Sall” is a system that uses the worst settings1 for each of these elements, while the second system “T2S+all” uses the best settings.2 The results for the systems are shown in Table 1. The most striking result is that T2S+all significantly exceeds all of the baselines, even including"
P14-2024,P02-1040,0,0.0960476,"will investigate the contribution of each of these elements in detail in the following sections. In the remainder of the paper settings follow T2S+all except when otherwise noted. Data and Evaluation We perform all of our experiments on en-ja and ja-en translation over data from the NTCIR PatentMT task (Goto et al., 2011), the most standard benchmark task for these language pairs. We use the training data from NTCIR 7/8, a total of approximately 3.0M sentences, and perform tuning on the NTCIR 7 dry run, testing on the NTCIR 7 formal run data. As evaluation measures, we use the standard BLEU (Papineni et al., 2002) as well as RIBES (Isozaki et al., 2010a), a reorderingbased metric that has been shown to have high correlation with human evaluations on the NTCIR data. We measure significance of results using bootstrap resampling at p &lt; 0.05 (Koehn, 2004). In tables, bold numbers indicate the best system and all systems that were not significantly different from the best system. 2.3 en-ja BLEU RIBES 35.84 72.89 34.45 72.94 36.69 77.05 36.23 76.60 40.84 80.15 3 Parsing 3.1 Parsing Overview As T2S translation uses parse trees both in training and testing of the system, an accurate syntactic parser is require"
P14-2024,P10-1017,0,0.0695133,"is alignment accuracy. It has been noted in many previous works that significant gains in alignment accuracy do not make a significant difference in translation results (Ayan and Dorr, 2006; Ganchev et al., 2008). However, none of these works have explicitly investigated the effect on T2S translation, so it is not clear whether these results carry over to our current situation. As our baseline aligner, we use the GIZA++ implementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the discriminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit.6 This method has the ability to use source- and targetside syntactic information, and has been shown to improve the accuracy of S2T translation. We trained Nile and tested both methods on the Japanese-English alignments provided with the Kyoto Free Translation Task (Neubig, 2011) (430k parallel sentences, 1074 manually aligned training sentences, and 120 manually aligned test sentences).7 As creating manual alignment data is costly, we also created two training sets that consisted of 1/4 and 1/16 of the total data to test if we can achieve an effect with sm"
P14-2024,P08-1066,0,0.00443162,"earch. The reason why we choose these elements is that past work that has reported low accuracy for T2S systems has often neglected to consider one or all of these elements. 1 Introduction In recent years, syntactic parsing is being viewed as an ever-more important element of statistical machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see large increases of ac"
P14-2024,2011.mtsummit-papers.36,1,0.958722,"cal machine translation (SMT) systems, particularly for translation between languages with large differences in word order. There are many ways of incorporating syntax into MT systems, including the use of string-to-tree translation (S2T) to ensure the syntactic well-formedness of the output (Galley et al., 2006; Shen et al., 2008), tree-to-string (T2S) using source-side parsing as a hint during the translation process (Liu et al., 2006), or preor post-ordering to help compensate for reordering problems experienced by non-syntactic methods such as phrase-based MT (PBMT) (Collins et al., 2005; Sudoh et al., 2011). Among these, T2S As a result of our tests on English-Japanese (enja) and Japanese-English (ja-en) machine translation, we find that a T2S system not considering these elements performs only slightly better than a standard PBMT system. However, after accounting for all these elements we see large increases of accuracy, with the final system greatly exceeding not only standard PBMT, but also state-of-the-art methods based on syntactic pre- or post-ordering. 143 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143–149, c Baltimore, Ma"
P14-2024,D11-1020,0,0.00523042,"be taken seriously in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first step in examining and codifying the more important elements that make it possible to construct a highly accurate T2S MT system. To do so, we perform an empirical study of the effect of parsing accuracy, packed forest input, alignment accuracy, and search. The reason why we choose these elem"
P14-2024,P12-2062,0,0.0174251,"4 2.05 10 Forest n-best Cutoff 100 Figure 1: BLEU scores using various levels of forest pruning. Numbers in the graph indicate decoding time in seconds/sentence. Egret achieves greater accuracy than that using the other two parsers. This improvement is particularly obvious in RIBES, indicating that an increase in parsing accuracy has a larger effect on global reordering than on lexical choice. When going from T2S to F2S translation using Egret, we see another large gain in accuracy, although this time with the gain in BLEU being more prominent. We believe this is related to the observation of Zhang and Chiang (2012) that F2S translation is not necessarily helping fixing parsing errors, but instead giving the translation system the freedom to ignore the parse somewhat, allowing for less syntactically motivated but more fluent translations. As passing some degree of syntactic ambiguity on to the decoder through F2S translation has proven useful, a next natural question is how much of this ambiguity we need to preserve in our forest. The pruning criterion that we use for the forest is based on including all edges that appear in one or more of the n-best parses, so we perform translation setting n to 1 (tree"
P14-2024,P11-1084,0,0.0742786,"ow a basic T2S system that performs on par with phrasebased systems can be improved by 2.6-4.6 BLEU, greatly exceeding existing stateof-the-art methods. These results indicate that T2S systems indeed hold much promise, but the above-mentioned elements must be taken seriously in construction of these systems. However, building an accurate T2S system is not trivial. On one hand, there have been multiple reports (mainly from groups with a long history of building T2S systems) stating that systems using source-side syntax greatly out-perform phrasebased systems (Mi et al., 2008; Liu et al., 2011; Zhang et al., 2011; Tamura et al., 2013). On the other hand, there have been also been multiple reports noting the exact opposite result that sourceside syntax systems perform worse than Hiero, S2T, PBMT, or PBMT with pre-ordering (Ambati and Lavie, 2008; Xie et al., 2011; Kaljahi et al., 2012). In this paper, we argue that this is due to the fact that T2S systems have the potential to achieve high accuracy, but are also less robust, with a number of peripheral elements having a large effect on translation accuracy. Our motivation in writing this paper is to provide a first step in examining and codifying the m"
P14-2090,N03-1033,0,0.00952551,"introduce regularization into the Greedy+DP algorithm, with the evaluation function ω rewrit553 Algorithm 2 Greedy+DP segmentation search Φ0 ← ∅ for k = 1 to K do for j = 0 to k − 1 do Φ′ ← {ϕ : c(ϕ; F) = k − j ∧ ϕ  ∈ Φj } #words f e Train M T 21.8M 20.3M En-De Train Seg. 424k 390k Test 27.6k 25.4k Train M T 13.7M 19.7M En-Ja Train Seg. 401k 550k Test 8.20k 11.9k Table 1: Size of M T training, segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy sear"
P14-2090,2012.eamt-1.60,0,0.00455234,"completely described in Algorithms 1 and 2. However, these algorithms require a large amount of computation and simple implementations of them are too slow to finish in realistic time. Because the heaviest parts of the algorithm are the calculation of M T and EV , we can greatly improve efficiency by memoizing the results of these functions, only recalculating on new input. 3 Experiments 3.1 Experimental Settings 3.2 Results and Discussion We evaluated the performance of our segmentation strategies by applying them to English-German and English-Japanese TED speech translation data from WIT3 (Cettolo et al., 2012). For EnglishGerman, we used the TED data and splits from the IWSLT2013 evaluation campaign (Cettolo et al., 2013), as well as 1M sentences selected from the out-of-domain training data using the method of Duh et al. (2013). For English-Japanese, we used TED data and the dictionary entries and sentences from EIJIRO.4 Table 1 shows summaries of the datasets we used. 4 Type Figures 4 and 5 show the results of evaluation for each segmentation strategy measured by BLEU and RIBES respectively. The horizontal axis is the mean number of words in the generated translation units. This value is proporti"
P14-2090,2013.iwslt-evaluation.1,0,0.0607953,"Missing"
P14-2090,P13-2119,1,0.641725,"hm are the calculation of M T and EV , we can greatly improve efficiency by memoizing the results of these functions, only recalculating on new input. 3 Experiments 3.1 Experimental Settings 3.2 Results and Discussion We evaluated the performance of our segmentation strategies by applying them to English-German and English-Japanese TED speech translation data from WIT3 (Cettolo et al., 2012). For EnglishGerman, we used the TED data and splits from the IWSLT2013 evaluation campaign (Cettolo et al., 2013), as well as 1M sentences selected from the out-of-domain training data using the method of Duh et al. (2013). For English-Japanese, we used TED data and the dictionary entries and sentences from EIJIRO.4 Table 1 shows summaries of the datasets we used. 4 Type Figures 4 and 5 show the results of evaluation for each segmentation strategy measured by BLEU and RIBES respectively. The horizontal axis is the mean number of words in the generated translation units. This value is proportional to the delay experienced during simultaneous speech translation (Rangarajan Sridhar et al., 2013) and thus a smaller value is desirable. RP, Greedy, and Greedy+DP methods have multiple results in these graphs because t"
P14-2090,D10-1092,0,0.0248496,"segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy search and a linear SVM (using surrounding word/POS 1, 2 and 3-grams as features) to learn the segmentation model. Greedy+DP is the algorithm that introduces grouping the positions in the source sentence by POS bigrams. Punct-Predict is the method using predicted positions of punctuation (Rangarajan Sridhar et al., 2013). RP is the method using right probability (Fujita et al., 2013). ten as belo"
P14-2090,P07-2045,0,0.00281304,"j ∧ ϕ  ∈ Φj } #words f e Train M T 21.8M 20.3M En-De Train Seg. 424k 390k Test 27.6k 25.4k Train M T 13.7M 19.7M En-Ja Train Seg. 401k 550k Test 8.20k 11.9k Table 1: Size of M T training, segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy search and a linear SVM (using surrounding word/POS 1, 2 and 3-grams as features) to learn the segmentation model. Greedy+DP is the algorithm that introduces grouping the positions in the source sentence by P"
P14-2090,C04-1072,0,0.0577816,"n this work, we define ω as the sum of the evaluation measure for each parallel sentence pair ⟨fj , ej ⟩: ω(S) := N ∑ EV (M T (fj , S), ej ), (3) j=1 where M T (f , S) represents the concatenation of all partial translations {M T (f (n) )} given the segments S as shown in Figure 1. Equation (3) indicates that we assume all parallel sentences to be independent of each other, and the evaluation measure is calculated for each sentence separately. This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). 1. Decide the mean number of words µ and the machine translation evaluation measure EV as parameters of algorithm. We can use an automatic evaluation measure such as BLEU (Papineni et al., 2002) as EV . Then, we calculate the number of sub-sentential segmentation boundaries K that we will need to insert into F to achieve an average segment length µ: ⌋ ) ( ⌊∑ f ∈F |f | − N . (1) K := max 0, µ 3. Make a segmentation model MS ∗ by treating the obtained segmentation boundaries S ∗ as positive labels, all other positions as negative labels, and training a classifier to distinguish between them. T"
P14-2090,2006.iwslt-papers.1,0,0.0144169,"is one example of such an application. When translating dialogue, the length of each utterance will usually be short, so the system can simply start the translation process when it detects the end of an utterance. However, in the case of lectures, for example, there is often no obvious boundary between utterances. Thus, translation systems require a method of deciding the timing at which to start the translation process. Using estimated ends of sentences as the timing with which to start translation, in the same way as a normal text translation, is a straightforward solution to this problem (Matusov et al., 2006). However, this approach 2 The method using RP can decide relative frequency of segmentation by changing a parameter, but guessing the length of a translation unit from this parameter is not trivial. 1 The implementation is available at http://odaemon.com/docs/codes/greedyseg.html. 551 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 551–556, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics lation accuracy as measured by BLEU or another evaluation measure. We evaluate our methods on a speech"
P14-2090,P11-2093,1,0.45417,"rit553 Algorithm 2 Greedy+DP segmentation search Φ0 ← ∅ for k = 1 to K do for j = 0 to k − 1 do Φ′ ← {ϕ : c(ϕ; F) = k − j ∧ ϕ  ∈ Φj } #words f e Train M T 21.8M 20.3M En-De Train Seg. 424k 390k Test 27.6k 25.4k Train M T 13.7M 19.7M En-Ja Train Seg. 401k 550k Test 8.20k 11.9k Table 1: Size of M T training, segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy search and a linear SVM (using surrounding word/POS 1, 2 and 3-grams as features) to learn"
P14-2090,P02-1040,0,0.0982654,"tion of all partial translations {M T (f (n) )} given the segments S as shown in Figure 1. Equation (3) indicates that we assume all parallel sentences to be independent of each other, and the evaluation measure is calculated for each sentence separately. This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). 1. Decide the mean number of words µ and the machine translation evaluation measure EV as parameters of algorithm. We can use an automatic evaluation measure such as BLEU (Papineni et al., 2002) as EV . Then, we calculate the number of sub-sentential segmentation boundaries K that we will need to insert into F to achieve an average segment length µ: ⌋ ) ( ⌊∑ f ∈F |f | − N . (1) K := max 0, µ 3. Make a segmentation model MS ∗ by treating the obtained segmentation boundaries S ∗ as positive labels, all other positions as negative labels, and training a classifier to distinguish between them. This classifier is used to detect segmentation boundaries at test time. Steps 1. and 3. of the above procedure are trivial. In contrast, choosing a good segmentation according to Equation (2) is di"
P14-2090,N13-1023,0,0.679115,"s reason, segmentation strategies, which separate the input at appropriate positions other than end of the sentence, have been studied. A number of segmentation strategies for simultaneous speech translation have been proposed in recent years. F¨ugen et al. (2007) and Bangalore et al. (2012) propose using prosodic pauses in speech recognition to denote segmentation boundaries, but this method strongly depends on characteristics of the speech, such as the speed of speaking. There is also research on methods that depend on linguistic or non-linguistic heuristics over recognized text (Rangarajan Sridhar et al., 2013), and it was found that a method that predicts the location of commas or periods achieves the highest performance. Methods have also been proposed using the phrase table (Yarmohammadi et al., 2013) or the right probability (RP) of phrases (Fujita et al., 2013), which indicates whether a phrase reordering occurs or not. However, each of the previously mentioned methods decides the segmentation on the basis of heuristics, so the impact of each segmentation strategy on translation performance is not directly considered. In addition, the mean number of words in the translation unit, which strongly"
P14-2090,N12-1048,0,\N,Missing
P14-2090,I13-1141,0,\N,Missing
P14-2090,federico-etal-2012-iwslt,0,\N,Missing
P15-1020,P13-2121,0,0.0185406,"and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved h"
P15-1020,2006.amta-papers.8,0,0.0443221,"II et al. (2014), who describe a method that predicts sentence-final verbs using reinforcement learning (e.g. Figure 1 (b)). This approach has the potential to greatly decrease the delay in translation from verb-final languages to verbinitial languages (such as German-English), but is also limited to only this particular case. In this paper, we propose a more general method that focuses on a different variety of information: unseen syntactic constituents. This method is motivated by our desire to apply translation models that use source-side parsing, such as tree-to-string (T2S) translation (Huang et al., 2006) or syntactic pre-ordering (Xia and McCord, 2004), which have been shown to greatly improve translation accuracy over syntactically divergent language pairs. However, conventional methods for parsing are not directly applicable to the partial sentences that arise in simultaneous MT. The reason for this, as explained in detail in Section 3, is that parsing methods generally assume that they are given input that forms a complete syntactic phrase. Looking at the example in Figure 1, after the speaker has spoken the words “I think” we have a partial sentence that will only be complete once we obse"
P15-1020,D10-1092,0,0.0151267,"y, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syntactic and semantic units in the original sentence, allowing us to evaluate the robustness of translation against poor segmentation boundaries. The second method is the state-of-the-art segmentation strategy proposed by Oda et al. (2014), which"
P15-1020,P07-2045,0,0.00788058,"I et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syn"
P15-1020,C04-1072,0,0.0585426,"Missing"
P15-1020,J93-2004,0,0.0498969,"tisfy these conditions. As shown in the Figure 3, there is ambiguity regarding syntactic constituents to be predicted (e.g. we can choose either [ NP ] or [ DT , NN ] as R for w = [ “this”, “is” ]). These conditions avoid ambiguity of which syntactic constituents should predicted for partial sentences in the training data. Looking at the example, Figures 3(d1) and 3(e1) satisfy these conditions, but 3(d2) and 3(e2) do not. Figure 4 shows the statistics of the lengths of L and R sequences extracted according to these criteria for all substrings of the WSJ datasets 2 to 23 of the Penn Treebank (Marcus et al., 1993), a standard training set for English syntactic parsers. From the figure we can see that lengths of up to 2 constituents cover the majority of cases for both L and R, but a significant number of cases require longer strings. Thus methods that predict a fixed number of constituents are not appropriate here. In Algorithm 1, we show the method we propose to Algorithmically, parsing with predicted syntactic constituents can be achieved by simply treating each syntactic constituent as another word in the input sequence and using a standard parsing algorithm such as the CKY algorithm. In this proces"
P15-1020,2006.iwslt-papers.1,0,0.0292458,"mation needed 1 Introduction Speech translation is an application of machine translation (MT) that converts utterances from the speaker’s language into the listener’s language. One of the most identifying features of speech translation is the fact that it must be performed in real time while the speaker is speaking, and thus it is necessary to split a constant stream of words into translatable segments before starting the translation process. Traditionally, speech translation assumes that each segment corresponds to a sentence, and thus performs sentence boundary detection before translation (Matusov et al., 2006). However, full sentences can be long, particularly in formal speech such as lectures, and if translation does not start until explicit ends of 198 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 198–207, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Process of English-Japanese simultaneous translation with sentence segmentation. tic prediction to MT, including the proposal of a heuristic method that examines whether a future co"
P15-1020,P11-2093,1,0.840277,"these tags to future work. 6.1.2 Simultaneous Translation Next, we evaluate the performance of T2S simultaneous translation adopting the two proposed methods. We use data of TED talks from the English-Japanese section of WIT3 (Cettolo et al., 2012), and also append dictionary entries and examples in Eijiro3 to the training data to increase the vocabulary of the translation model. The total number of sentences/entries is 2.49M (WIT3, Eijiro), 998 (WIT3), and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to"
P15-1020,2012.eamt-1.60,0,0.0111743,"uents. Creating a language model that contains probabilities for these tags in the appropriate places is not trivial, so for simplicity, we simply assume that every syntactic constituent tag is an unknown word, and that the output of translation consists of both translated normal words and non-translated tags as shown in Figure 5. We relegate a more complete handling of these tags to future work. 6.1.2 Simultaneous Translation Next, we evaluate the performance of T2S simultaneous translation adopting the two proposed methods. We use data of TED talks from the English-Japanese section of WIT3 (Cettolo et al., 2012), and also append dictionary entries and examples in Eijiro3 to the training data to increase the vocabulary of the translation model. The total number of sentences/entries is 2.49M (WIT3, Eijiro), 998 (WIT3), and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we"
P15-1020,P13-4016,1,0.829485,"redictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al."
P15-1020,J03-1002,0,0.00408816,"of sentences/entries is 2.49M (WIT3, Eijiro), 998 (WIT3), and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki"
P15-1020,2014.iwslt-papers.8,0,0.0146061,"are given an incoming stream of words f , which we are expected to translate. As the f is long, we would like to begin translating before we reach the end of the stream. Previous methods to do so can generally be categorized into incremental decoding methods, and sentence segmentation methods. In incremental decoding, each incoming word is fed into the decoder one-by-one, and the decoder updates the search graph with the new words and decides whether it should begin translation. Incremental decoding methods have been proposed for phrase-based (Sankaran et al., 2010; Yarmohammadi et al., 2013; Finch et al., 2014) and hierarchical phrase-based (Siahbani et al., 2014) SMT Specifically the method consists of two parts: First, we propose a method that trains a statistical model to predict future syntactic constituents based on features of the input segment (Section 4). Second, we demonstrate how to apply this syntac199 models.1 Incremental decoding has the advantage of using information about the decoding graph in the choice of translation timing, but also requires significant changes to the internal workings of the decoder, precluding the use of standard decoding tools or techniques. Sentence segmentatio"
P15-1020,P03-1021,0,0.0381705,"203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syntactic and semantic units in the original sentence, allowing us to evaluate the robustness of translation against poor seg"
P15-1020,P14-2090,1,0.854392,"rovide a simpler alternative by first dividing f into subsequences of 1 or more words [f (1) , . . . , f (N ) ]. These segments are then translated with a traditional decoder into output sequences [e(1) , . . . , e(N ) ], which each are output as soon as translation finishes. Many methods have been proposed to perform segmentation, including the use of prosodic boundaries (F¨ugen et al., 2007; Bangalore et al., 2012), predicting punctuation marks (Rangarajan Sridhar et al., 2013), reordering probabilities of phrases (Fujita et al., 2013), or models to explicitly optimize translation accuracy (Oda et al., 2014). Previous work often assumes that f is a single sentence, and focus on sub-sentential segmentation, an approach we follow in this work. Sentence segmentation methods have the obvious advantage of allowing for translation as soon as a segment is decided. However, the use of the shorter segments also makes it necessary to translate while part of the utterance is still unknown. As a result, segmenting sentences more aggressively often results in a decrease translation accuracy. This is a problem in phrase-based MT, the framework used in the majority of previous research on simultaneous translati"
P15-1020,N15-3009,1,0.857129,"Missing"
P15-1020,D14-1140,0,0.315517,"Missing"
P15-1020,P02-1040,0,0.0940091,"nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syntactic and semantic units in the original sentence, allowing us to evaluate the robustness of translation against poor segmentation boundaries. The second method i"
P15-1020,N13-1023,0,0.362073,"internal workings of the decoder, precluding the use of standard decoding tools or techniques. Sentence segmentation methods (Figure 2) provide a simpler alternative by first dividing f into subsequences of 1 or more words [f (1) , . . . , f (N ) ]. These segments are then translated with a traditional decoder into output sequences [e(1) , . . . , e(N ) ], which each are output as soon as translation finishes. Many methods have been proposed to perform segmentation, including the use of prosodic boundaries (F¨ugen et al., 2007; Bangalore et al., 2012), predicting punctuation marks (Rangarajan Sridhar et al., 2013), reordering probabilities of phrases (Fujita et al., 2013), or models to explicitly optimize translation accuracy (Oda et al., 2014). Previous work often assumes that f is a single sentence, and focus on sub-sentential segmentation, an approach we follow in this work. Sentence segmentation methods have the obvious advantage of allowing for translation as soon as a segment is decided. However, the use of the shorter segments also makes it necessary to translate while part of the utterance is still unknown. As a result, segmenting sentences more aggressively often results in a decrease translat"
P15-1020,P06-2088,0,0.113368,"Figure 3(e2) by appending only NN after the unit. 3 Parsing Incomplete Sentences 3.1 Difficulties in Incomplete Parsing In standard phrase structure parsing, the parser assumes that each input string is a complete sentence, or at least a complete phrase. For example, Figure 3 (a) shows the phrase structure of the complete sentence “this is a pen.” However, in the case of simultaneous translation, each translation unit 3.2 Formulation of Incomplete Parsing 1 There is also one previous rule-based system that uses syntax in incremental translation, but it is language specific and limited domain (Ryu et al., 2006), and thus difficult to compare with our SMT-based system. It also does not predict unseen constituents, relying only on the observed segment. A typical model for phrase structure parsing is the probabilistic context-free grammar (PCFG). Parsing is performed by finding the parse tree T that 200 maximizes the PCFG probability given a sequence of words w ≡ [w1 , w2 , · · · , wn ] as shown by Eq. (2): T ∗ ≡ arg max Pr(T |w) T It should be noted that here L refers to syntactic constituents that have already been seen in the past. Thus, it is theoretically possible to store past parse trees as hist"
P15-1020,W10-1733,0,0.0784773,"on In simultaneous translation, we assume that we are given an incoming stream of words f , which we are expected to translate. As the f is long, we would like to begin translating before we reach the end of the stream. Previous methods to do so can generally be categorized into incremental decoding methods, and sentence segmentation methods. In incremental decoding, each incoming word is fed into the decoder one-by-one, and the decoder updates the search graph with the new words and decides whether it should begin translation. Incremental decoding methods have been proposed for phrase-based (Sankaran et al., 2010; Yarmohammadi et al., 2013; Finch et al., 2014) and hierarchical phrase-based (Siahbani et al., 2014) SMT Specifically the method consists of two parts: First, we propose a method that trains a statistical model to predict future syntactic constituents based on features of the input segment (Section 4). Second, we demonstrate how to apply this syntac199 models.1 Incremental decoding has the advantage of using information about the decoding graph in the choice of translation timing, but also requires significant changes to the internal workings of the decoder, precluding the use of standard de"
P15-1020,C04-1073,0,0.0449431,"redicts sentence-final verbs using reinforcement learning (e.g. Figure 1 (b)). This approach has the potential to greatly decrease the delay in translation from verb-final languages to verbinitial languages (such as German-English), but is also limited to only this particular case. In this paper, we propose a more general method that focuses on a different variety of information: unseen syntactic constituents. This method is motivated by our desire to apply translation models that use source-side parsing, such as tree-to-string (T2S) translation (Huang et al., 2006) or syntactic pre-ordering (Xia and McCord, 2004), which have been shown to greatly improve translation accuracy over syntactically divergent language pairs. However, conventional methods for parsing are not directly applicable to the partial sentences that arise in simultaneous MT. The reason for this, as explained in detail in Section 3, is that parsing methods generally assume that they are given input that forms a complete syntactic phrase. Looking at the example in Figure 1, after the speaker has spoken the words “I think” we have a partial sentence that will only be complete once we observe the following SBAR. Our method attempts to pr"
P15-1020,N12-1048,0,\N,Missing
P15-1020,I13-1141,0,\N,Missing
P15-2094,N15-1033,1,0.878686,"Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich monolingual resources in pivot languages such as English, and SMT can utilize this additional information to improve the translation quality. To utilize information about the pivot language at translation time, we train a Multi-Synchronous Context-free Grammar (MSCFG) (Neubig et al., 2015), a generalized extension of synchronous CFGs (SCFGs) (Chiang, 2007), that can generate strings in multiple languages at the same time. To create the MSCFG, we triangulate source-pivot and pivot-target SCFG rule tables not into a single source-target SCFG, but into a source-target-pivot MSCFG rule table that remembers the pivot. During decoding, we use language models over both the target and the pivot to assess the naturalness of the derivation. We perform experiments on pivot translation of Europarl proceedings, which show that our method indeed provide significant gains in accuracy (of up t"
P15-2094,P13-4016,1,0.853226,"MSCFG +PivotLM 2M † 25.75 † 24.58 ‡ 22.29 † 19.40 † 29.95 ‡ 25.64 † 19.19 ‡ 31.00 ‡ 26.22 ‡ 18.52 † 29.31 † 29.02 Table 1: Results for each method. Bold indicates the highest BLEU score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG († : p < 0.05, ‡ : p < 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: 4.2 Experimental Results The result of experiments using all combinations of pivot translation tasks for 4 languages via English is shown in Table 1. From the results, we can see that the proposed triangulation method con"
P15-2094,P03-1021,0,0.0312701,"score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG († : p < 0.05, ‡ : p < 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: 4.2 Experimental Results The result of experiments using all combinations of pivot translation tasks for 4 languages via English is shown in Table 1. From the results, we can see that the proposed triangulation method considering pivot LMs outperforms the traditional triangulation method for all language pairs, and translation with larger pivot LMs improves the BLEU scores. For all languages, the"
P15-2094,P02-1040,0,0.0927681,"29.02 Table 1: Results for each method. Bold indicates the highest BLEU score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG († : p < 0.05, ‡ : p < 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: 4.2 Experimental Results The result of experiments using all combinations of pivot translation tasks for 4 languages via English is shown in Table 1. From the results, we can see that the proposed triangulation method considering pivot LMs outperforms the traditional triangulation method for all language pairs, and translation with large"
P15-2094,N07-1061,0,0.884546,"e translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 573 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally w"
P15-2094,D14-1174,0,0.0648612,"et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 573 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich m"
P15-2094,J07-2003,0,0.870535,"Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich monolingual resources in pivot languages such as English, and SMT can utilize this additional information to improve the translation quality. To utilize information about the pivot language at translation time, we train a Multi-Synchronous Context-free Grammar (MSCFG) (Neubig et al., 2015), a generalized extension of synchronous CFGs (SCFGs) (Chiang, 2007), that can generate strings in multiple languages at the same time. To create the MSCFG, we triangulate source-pivot and pivot-target SCFG rule tables not into a single source-target SCFG, but into a source-target-pivot MSCFG rule table that remembers the pivot. During decoding, we use language models over both the target and the pivot to assess the naturalness of the derivation. We perform experiments on pivot translation of Europarl proceedings, which show that our method indeed provide significant gains in accuracy (of up to 1.2 BLEU points), in all combinations of 4 languages with English"
P15-2094,P07-1092,0,0.805068,"n In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 573 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this a"
P15-2094,W08-0333,0,0.349773,"ional triangulation method, information about pivot phrases that behave as bridges between source and target phrases is lost after learning phrase pairs, as shown in Figure 1 (b). To overcome these problems, we propose a novel triangulation method that remembers the pivot phrase connecting source and target in the records of phrase/rule table, and estimates a joint translation probability from the source to target Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate"
P15-2094,J93-1004,0,\N,Missing
P15-2094,J93-2003,0,\N,Missing
P15-2094,2005.mtsummit-papers.11,0,\N,Missing
P16-1130,P10-2002,0,0.0185124,"ver, the information contained in Rule4 will be considered as context features for Rule1. Therefore, this is no longer an advantage for the CSRS model as long as we use rich enough context features, which could be the reason why using both the CSRS and CSRS-MINI models cannot further improve the translation quality compared to using only the CSRS-MINI model. 5 Related Work The rule selection problem for syntax-based SMT has received much attention. He et al. (2008) proposed a lexicalized rule selection model to perform context-sensitive rule selection for hierarchical phrase-base translation. Cui et al. (2010) introduced a joint rule selection model for hierarchical phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach. However, these two models adopted linear classifiers similar to those used in the MERS model (Liu et al., 2008), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabili"
P16-1130,P14-1129,0,0.0707017,"Missing"
P16-1130,N13-1001,0,0.0201378,"Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on paral"
P16-1130,P16-1078,0,0.0171099,"tion models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syntactic NMT model (Eriguchi et al., 2016), which extends the original sequence-to-sequence NMT model with the source-side phrase structure. Although this model takes source-side syntax into consideration, it still produces target words one by one as a sequence. In contrast, the tree-based translation rules used in our model can take advantage of the hierarchical structures of both source and target languages. 6 Conclusion In this paper, we propose a CSRS model for syntax-based SMT, which is learned by a feedforward neural network on a continuous space. Compared with the previous MERS model that used discrete representations of words"
P16-1130,N04-1035,0,0.0816271,"n rules used in translations down into minimal rules and multiply all probabilities to calculate the necessary features. PP NP IN DT NN on the table 在 桌子 上 extract Rule1 Source tree NN Target string table NP 桌子 DT Rule2 NN the DT Rule3 桌子 table NP 4 Experiments 4.1 NN the x0 PP x0 NP IN DT NN Rule4 on the PP table 在 桌子 上 IN NP 在 x0 上 on x0 PP Rule5 NP IN DT NN Rule6 on the x0 在 x0 上 Figure 3: Rules. and many may only appear a few times in the corpus. To reduce these problems of sparsity, we propose another improvement to the model, specifically through the use of minimal rules. Minimal rules (Galley et al., 2004) are translation rules that cannot be split into two smaller rules. For example, in Figure 3, Rule2 is not a minimal rule, since Rule2 can be split into Rule1 and Rule3. In the same way, Rule4 and Rule6 are not minimal while Rule1, Rule3 and Rule5 are minimal. Minimal rules are more frequent than nonminimal rules and have richer training data. Hence, we can expect that a rule selection model trained on minimal rules will suffer less from data sparsity problems. Besides, without non-minimal rules, the rule selection model will need less memSetting We evaluated the proposed approach for Englisht"
P16-1130,P06-1121,0,0.0645963,".1K 1.5K 27.1K 29.8K 954K 40.4M 37.2M 504K 288K 2K 77.5K 75.4K 2K 58.1K 55.5K 3.14M 104M 118M 273K 150K 2K 66.5K 74.6K 2K 70.6K 78.5K Base MERS CSRS MERS-MINI CSRS-MINI CSRS vs. MERS CSRS-MINI vs. MERS-MINI MERS-MINI vs. MERS CSRS-MINI vs. CSRS EC 29.42 29.75 30.12 30.53 31.63 EJ 37.10 37.76 37.83 38.14 38.32 ED &gt;&gt; &gt;&gt; − &gt; EF &gt;&gt; − &gt;&gt; − EC &gt; &gt;&gt; &gt;&gt; &gt;&gt; EJ − − &gt;&gt; &gt;&gt; Table 3: Significance test results. The symbol &gt;&gt; (&gt;) represents a significant difference at the p < 0.01 (p < 0.05) level and the symbol - represents no significant difference at the p < 0.05 level. performed using the GHKM algorithm (Galley et al., 2006) and the maximum numbers of nonterminals and terminals contained in one rule were set to 2 and 10 respectively. Note that when extracting minimal rules, we release this limit. The decoding algorithm is the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). For English parsing, we used Egret8 , which is able to output packed forests for decoding. We trained the CSRS models (CSRS and CSRSMINI) on translation rules extracted from the training set. Translation rules extracted from the development set were used as validation data for model training to avoid over-fitting. For differe"
P16-1130,P14-1066,0,0.0278847,"8), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus. Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2"
P16-1130,N04-1014,0,0.0657252,"ules. We tested our model on different translation tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 1 S NP VP NP PRP VBD DT NN I caught a thief 我 抓 了 一个 贼 NP NP PRP VBD DT NN I caught a cold 我 得 了 感冒 Rule Extraction S S VP VP NP VBD NP NP VBD NP x0 caught x1 x0 caught x1 x0 抓 了 x1 x0 得 了 x1 Figure 1: An ambiguous source subtree with different translations (English-to-Chinese). Introduction In syntax-based statistical machine translation (SMT), especially tree-to-string (Liu et al., 2006; Graehl and Knight, 2004) and forest-to-string (Mi et al., 2008) SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence. A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals. One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correct rules during decoding is"
P16-1130,C08-1041,0,0.018049,"tion than minimal rules. For example, in Figure 3, Rule4 contains more information than Rule1, which could be an advantage for rule selection. However, the information contained in Rule4 will be considered as context features for Rule1. Therefore, this is no longer an advantage for the CSRS model as long as we use rich enough context features, which could be the reason why using both the CSRS and CSRS-MINI models cannot further improve the translation quality compared to using only the CSRS-MINI model. 5 Related Work The rule selection problem for syntax-based SMT has received much attention. He et al. (2008) proposed a lexicalized rule selection model to perform context-sensitive rule selection for hierarchical phrase-base translation. Cui et al. (2010) introduced a joint rule selection model for hierarchical phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach. However, these two models adopted linear classifiers similar to those used in the MERS model (Liu et al., 2008), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to lea"
P16-1130,P15-1001,0,0.0379898,"a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on pl"
P16-1130,W04-3250,0,0.140364,"owing their work, the iteration number was set to be 100 and the Gaussian prior was set to be 1. We also compared the original MERS model and the MERS model trained only on minimal rules (MERS-MINI) to test the benefit of using minimal rules for model training. The MERS and CSRS models were both used to calculate features used to rerank unique 1,000best outputs of the baseline system. Tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 4.2 Results Table 2 shows the translation results and Table 3 shows significance test results using bootstrap resampling (Koehn, 2004): “Base” stands for the baseline system without any; “MERS”, “CSRS”, “MERS-MINI” and “CSRS-MINI” means the outputs of the baseline system were reranked using features from the MERS, CSRS, MERS-MINI and CSRS-MINI models respectively. Generally, the CSRS model outperformed the MERS model and the CSRS-MINI model outperformed the MERSMINI model on different translation tasks. In addition, using minimal rules for model training benefitted both the MERS and CSRS models. Table 4 shows translation examples in the EC task to demonstrate the reason why our approach improved accuracy. Among all translati"
P16-1130,P08-1023,0,0.171613,"ion tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 1 S NP VP NP PRP VBD DT NN I caught a thief 我 抓 了 一个 贼 NP NP PRP VBD DT NN I caught a cold 我 得 了 感冒 Rule Extraction S S VP VP NP VBD NP NP VBD NP x0 caught x1 x0 caught x1 x0 抓 了 x1 x0 得 了 x1 Figure 1: An ambiguous source subtree with different translations (English-to-Chinese). Introduction In syntax-based statistical machine translation (SMT), especially tree-to-string (Liu et al., 2006; Graehl and Knight, 2004) and forest-to-string (Mi et al., 2008) SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence. A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals. One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correct rules during decoding is a major challenge for SMT in general,"
P16-1130,P13-4016,1,0.856935,"on tasks. For the ED and EF tasks, the translation systems are trained on Europarl v7 parallel corpus and tested on the WMT 2015 translation task.4 The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 The detailed statistics for training, development and test sets are given in Table 1. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. For each translation task, we used Travatar (Neubig, 2013) to train a forest-to-string translation system. GIZA++ (Och and Ney, 2003) was used for word alignment. A 5-gram language model was trained on the target side of the training corpus using the IRST-LM Toolkit7 with modified Kneser-Ney smoothing. Rule extraction was 4 The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. 5 Note that NTCIR-9 only contained a Chinese-to-English translation task. Because we want to test the proposed approach with a simi"
P16-1130,J03-1002,0,0.00539023,"on Europarl v7 parallel corpus and tested on the WMT 2015 translation task.4 The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 The detailed statistics for training, development and test sets are given in Table 1. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. For each translation task, we used Travatar (Neubig, 2013) to train a forest-to-string translation system. GIZA++ (Och and Ney, 2003) was used for word alignment. A 5-gram language model was trained on the target side of the training corpus using the IRST-LM Toolkit7 with modified Kneser-Ney smoothing. Rule extraction was 4 The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. 5 Note that NTCIR-9 only contained a Chinese-to-English translation task. Because we want to test the proposed approach with a similarly accurate parsing model across our tasks, we used English as the sourc"
P16-1130,P03-1021,0,0.0729483,"stems. Table 1: Data sets. 8 ED 15.00 15.62 16.15 15.77 16.49 ing instances for their model were extracted from the training set. Following their work, the iteration number was set to be 100 and the Gaussian prior was set to be 1. We also compared the original MERS model and the MERS model trained only on minimal rules (MERS-MINI) to test the benefit of using minimal rules for model training. The MERS and CSRS models were both used to calculate features used to rerank unique 1,000best outputs of the baseline system. Tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 4.2 Results Table 2 shows the translation results and Table 3 shows significance test results using bootstrap resampling (Koehn, 2004): “Base” stands for the baseline system without any; “MERS”, “CSRS”, “MERS-MINI” and “CSRS-MINI” means the outputs of the baseline system were reranked using features from the MERS, CSRS, MERS-MINI and CSRS-MINI models respectively. Generally, the CSRS model outperformed the MERS model and the CSRS-MINI model outperformed the MERSMINI model on different translation tasks. In addition, using minimal rules for model training benefitted both the MERS and CSRS mod"
P16-1130,C12-2104,0,0.0248686,"phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach. However, these two models adopted linear classifiers similar to those used in the MERS model (Liu et al., 2008), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus. Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minima"
P16-1130,P15-1004,0,0.0145628,"odel for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syntactic NMT model (Eriguchi et al., 2016), which extends the original sequence-to-sequence NMT model with the source-side phrase structure. Although this model tak"
P16-1130,P11-1086,0,0.0154809,"space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al."
P16-1130,D13-1140,0,0.0199508,"MERS probability feature, and, h2 is a penalty feature counting the number of predictions made by the MERS model. 3 where v ∈ {0, 1} is an indicator of whether t˜ is translated into e˜. This is in contrast to the MERS model, which treated the rule selection problem as a multi-class classification task. If instead we attempted to estimate output probabilities for all different e˜, the cost of estimating the normalization coefficient would be prohibitive, as the number of unique output-side word strings e˜ is large. There are a number of remedies to this, including noise contrastive estimation (Vaswani et al., 2013), but the binary approximation method has been reported to have better performance (Zhang et al., 2015). To learn this model, we use a feed-forward neural network with structure similar to neural network language models (Vaswani et al., 2013). The input of the neural rule selection model is a vector representation for t˜, another vector representation for e˜, and a set of ξ vector representations for both source-side and target-side context words of r: In our model, C (r) is calculated differently depending on the number of nonterminals included in the rule. Specifically, Equation 7 defines Co"
P16-1130,P06-1077,0,0.310903,"d to non-minimal rules. We tested our model on different translation tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 1 S NP VP NP PRP VBD DT NN I caught a thief 我 抓 了 一个 贼 NP NP PRP VBD DT NN I caught a cold 我 得 了 感冒 Rule Extraction S S VP VP NP VBD NP NP VBD NP x0 caught x1 x0 caught x1 x0 抓 了 x1 x0 得 了 x1 Figure 1: An ambiguous source subtree with different translations (English-to-Chinese). Introduction In syntax-based statistical machine translation (SMT), especially tree-to-string (Liu et al., 2006; Graehl and Knight, 2004) and forest-to-string (Mi et al., 2008) SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence. A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals. One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correc"
P16-1130,P14-1011,0,0.01589,"from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus. Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method t"
P16-1130,D08-1010,0,0.241026,"tion rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correct rules during decoding is a major challenge for SMT in general, and syntax-based models are no exception. There have been several methods proposed to resolve this ambiguity. The most simple method, used in the first models of tree-to-string translation (Liu et al., 2006), estimated the probability of a translation rule by relative frequencies. For example, in Figure 1, the rule that occurs more times in the training data will have a higher score. Later, Liu et al. (2008) proposed a maximum entropy based rule selection (MERS, Section 2) model for syntax-based SMT, which used contextual information for rule selection, such as words surrounding a rule and words covered by nonterminals in a rule. For example, to choose the correct rule from the two rules in Figure 1 for decoding a particular input sentence, if the source phrase covered by “x1” is “a thief” and this child phrase 1372 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1372–1381, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Ling"
P16-1130,D15-1250,1,0.828569,"S model. 3 where v ∈ {0, 1} is an indicator of whether t˜ is translated into e˜. This is in contrast to the MERS model, which treated the rule selection problem as a multi-class classification task. If instead we attempted to estimate output probabilities for all different e˜, the cost of estimating the normalization coefficient would be prohibitive, as the number of unique output-side word strings e˜ is large. There are a number of remedies to this, including noise contrastive estimation (Vaswani et al., 2013), but the binary approximation method has been reported to have better performance (Zhang et al., 2015). To learn this model, we use a feed-forward neural network with structure similar to neural network language models (Vaswani et al., 2013). The input of the neural rule selection model is a vector representation for t˜, another vector representation for e˜, and a set of ξ vector representations for both source-side and target-side context words of r: In our model, C (r) is calculated differently depending on the number of nonterminals included in the rule. Specifically, Equation 7 defines Cout (r, n) to be context words (n-grams) around r and Cin (r, n, Xk ) to be boundary words (n-grams) cov"
P16-1130,D15-1166,0,0.012445,"t al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these mode"
P16-1130,W06-0127,0,0.0362305,"o-German (ED), English-to-French (EF), English-to-Chinese (EC) and English-to-Japanese (EJ) translation tasks. For the ED and EF tasks, the translation systems are trained on Europarl v7 parallel corpus and tested on the WMT 2015 translation task.4 The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 The detailed statistics for training, development and test sets are given in Table 1. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. For each translation task, we used Travatar (Neubig, 2013) to train a forest-to-string translation system. GIZA++ (Och and Ney, 2003) was used for word alignment. A 5-gram language model was trained on the target side of the training corpus using the IRST-LM Toolkit7 with modified Kneser-Ney smoothing. Rule extraction was 4 The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. 5 Note that NTCIR-9 only contained"
P16-1130,P15-1002,0,0.0492218,"Missing"
P16-1130,P15-1003,0,0.0142853,"ibuted representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syntactic NMT model (Eriguchi et al., 2016), which extends the original sequence-to-sequence NMT model with the source-si"
P17-1029,W16-2004,0,0.0357259,"arwish and Oard, 2007) in these languages. As shown in 1, re-inflection of an inflected form given the target linguistic labels is a challenging subtask of handling morphology as a whole, in which we take as input an inflected form (in the example, “playing”) and labels representing the desired form (“pos=Verb, tense=Past”) and must generate the desired form (“played”). Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016). There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Sch¨utze, 2016a,b) have achieved state-of-art results on this task. One feature of these neural models however, is that they are trained in a Introduction This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output sequence that reflects the con"
P17-1029,K16-1002,0,0.0235279,"cuss details of the learning process that prove 313 useful to its success. 4.1 this case, the RNN decoder can easily rely on the true output of last time step during training to decode the next token, which degenerates into an RNN language model. Hence, the latent variables are ignored by the decoder and cannot encode any useful information. The latent variable z learns an undesirable distribution that coincides with the imposed prior distribution but has no contribution to the decoder. To force the decoder to use the latent variables, we take the following two approaches which are similar to Bowman et al. (2016). KL-Divergence Annealing: We add a coefficient λ to the KL cost and gradually anneal it from zero to a predefined threshold λm . At the early stage of training, we set λ to be zero and let the model first figure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost. Although we are not optimizing the tight variational lower bound, the model balances well between generation and regularization. This technique can also be seen in (Koˇcisk`y et al., 2016; Miao and Blunsom, 2016). Input Dropout in the Decoder: Bes"
P17-1029,W16-2010,0,0.0107769,"Missing"
P17-1029,D13-1174,0,0.0211978,"anguages.1 1 POS=Verb, Tense=Past played Model Supervised Learning plays Semi-Supervised Learning Figure 1: Standard supervised labeled sequence transduction, and our proposed semi-supervised method. 2016), which we will use as an example in our description and test bed for our models. In morphologically rich languages, different affixes (i.e. prefixes, infixes, suffixes) can be combined with the lemma to reflect various syntactic and semantic features of a word. The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation (Chahuneau et al., 2013; Toutanova et al., 2008) or information retrieval (Darwish and Oard, 2007) in these languages. As shown in 1, re-inflection of an inflected form given the target linguistic labels is a challenging subtask of handling morphology as a whole, in which we take as input an inflected form (in the example, “playing”) and labels representing the desired form (“pos=Verb, tense=Past”) and must generate the desired form (“played”). Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and ext"
P17-1029,P16-2090,0,0.0527031,"Missing"
P17-1029,D16-1140,1,0.224453,"ey are trained in a Introduction This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output sequence that reflects the content of the input sequence and desiderata specified by the labels. Several examples of these tasks exist in prior work: using labels to moderate politeness in machine translation results (Sennrich et al., 2016), modifying the output language of a machine translation system (Johnson et al., 2016), or controlling the length of a summary in summarization (Kikuchi et al., 2016). In particular, however, we are motivated by the task of morphological reinflection (Cotterell et al., 1 An implementation of our model are available at https://github.com/violet-zct/ MSVED-morph-reinflection. 310 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 310–320 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1029 2 largely supervised fashion (top of Fig. 1), using data explicitly labeled with the input sequence and labels, along with the output representation. N"
P17-1029,W16-2002,0,0.0196161,"Missing"
P17-1029,N13-1138,0,0.0160935,"r information retrieval (Darwish and Oard, 2007) in these languages. As shown in 1, re-inflection of an inflected form given the target linguistic labels is a challenging subtask of handling morphology as a whole, in which we take as input an inflected form (in the example, “playing”) and labels representing the desired form (“pos=Verb, tense=Past”) and must generate the desired form (“played”). Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016). There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Sch¨utze, 2016a,b) have achieved state-of-art results on this task. One feature of these neural models however, is that they are trained in a Introduction This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output"
P17-1029,D16-1116,0,0.0259308,"Missing"
P17-1029,N16-1077,1,0.494461,"challenging subtask of handling morphology as a whole, in which we take as input an inflected form (in the example, “playing”) and labels representing the desired form (“pos=Verb, tense=Past”) and must generate the desired form (“played”). Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016). There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Sch¨utze, 2016a,b) have achieved state-of-art results on this task. One feature of these neural models however, is that they are trained in a Introduction This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output sequence that reflects the content of the input sequence and desiderata specified by the labels. Several examples of these tasks exist in prior work: usin"
P17-1029,D16-1031,0,0.0229859,"g two approaches which are similar to Bowman et al. (2016). KL-Divergence Annealing: We add a coefficient λ to the KL cost and gradually anneal it from zero to a predefined threshold λm . At the early stage of training, we set λ to be zero and let the model first figure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost. Although we are not optimizing the tight variational lower bound, the model balances well between generation and regularization. This technique can also be seen in (Koˇcisk`y et al., 2016; Miao and Blunsom, 2016). Input Dropout in the Decoder: Besides annealing the KL cost, we also randomly drop out the input token with a probability of β at each time step of the decoder during learning. The previous ground-truth token embedding is replaced with a zero vector when dropped. In this way, the RNN decoder could not fully rely on the ground-truth previous token, which ensures that the decoder uses information encoded in the latent variables. Learning Discrete Latent Variables One challenge in training our model is that it is not trivial to perform back-propagation through discrete random variables, and thu"
P17-1029,W16-2005,0,0.0523218,"Missing"
P17-1029,W16-2003,0,0.0211172,"gy as a whole, in which we take as input an inflected form (in the example, “playing”) and labels representing the desired form (“pos=Verb, tense=Past”) and must generate the desired form (“played”). Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016). There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Sch¨utze, 2016a,b) have achieved state-of-art results on this task. One feature of these neural models however, is that they are trained in a Introduction This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output sequence that reflects the content of the input sequence and desiderata specified by the labels. Several examples of these tasks exist in prior work: using labels to moderate politeness in"
P17-1029,N16-1005,0,0.00595385,"attentional encoder-decoder models (Kann and Sch¨utze, 2016a,b) have achieved state-of-art results on this task. One feature of these neural models however, is that they are trained in a Introduction This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output sequence that reflects the content of the input sequence and desiderata specified by the labels. Several examples of these tasks exist in prior work: using labels to moderate politeness in machine translation results (Sennrich et al., 2016), modifying the output language of a machine translation system (Johnson et al., 2016), or controlling the length of a summary in summarization (Kikuchi et al., 2016). In particular, however, we are motivated by the task of morphological reinflection (Cotterell et al., 1 An implementation of our model are available at https://github.com/violet-zct/ MSVED-morph-reinflection. 310 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 310–320 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.186"
P17-1029,W16-2011,0,0.0372519,"ial to creating applications such as machine translation (Chahuneau et al., 2013; Toutanova et al., 2008) or information retrieval (Darwish and Oard, 2007) in these languages. As shown in 1, re-inflection of an inflected form given the target linguistic labels is a challenging subtask of handling morphology as a whole, in which we take as input an inflected form (in the example, “playing”) and labels representing the desired form (“pos=Verb, tense=Past”) and must generate the desired form (“played”). Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016). There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Sch¨utze, 2016a,b) have achieved state-of-art results on this task. One feature of these neural models however, is that they are trained in a Introduction This paper proposes a model for labeled sequence transduction tasks,"
P17-1029,P08-1059,0,0.0211779,"ense=Past played Model Supervised Learning plays Semi-Supervised Learning Figure 1: Standard supervised labeled sequence transduction, and our proposed semi-supervised method. 2016), which we will use as an example in our description and test bed for our models. In morphologically rich languages, different affixes (i.e. prefixes, infixes, suffixes) can be combined with the lemma to reflect various syntactic and semantic features of a word. The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation (Chahuneau et al., 2013; Toutanova et al., 2008) or information retrieval (Darwish and Oard, 2007) in these languages. As shown in 1, re-inflection of an inflected form given the target linguistic labels is a challenging subtask of handling morphology as a whole, in which we take as input an inflected form (in the example, “playing”) and labels representing the desired form (“pos=Verb, tense=Past”) and must generate the desired form (“played”). Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and extracted transduction rules"
P17-1029,D16-1050,0,0.0964277,"Missing"
P17-1029,P16-5005,0,\N,Missing
P17-1041,D15-1198,0,0.0108628,"iel Tarlow, Andrew D. Gordon, and Yi Wei. 2015. Bimodal modelling of source code and natural language. In Proceedings of ICML. volume 37. David Alvarez-Melis and Tommi S. Jaakkola. 2017. Tree-structured decoding with doubly recurrent neural networks. In Proceedings of ICLR. Semantic Parsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms. The target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk´y et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neu"
P17-1041,Q13-1005,0,0.105357,"itute Carnegie Mellon University pcyin@cs.cmu.edu Graham Neubig Language Technologies Institute Carnegie Mellon University gneubig@cs.cmu.edu Abstract In parallel, the NLP community has developed methods for data-driven semantic parsing, which attempt to map NL to structured logical forms executable by computers. These logical forms can be general-purpose meaning representations (Clark and Curran, 2007; Banarescu et al., 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al., 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al., 2015; Misra et al., 2015), among others. While these methods have the advantage of being learnable from data, compared to the programming languages (PLs) in use by programmers, the domain-specific languages targeted by these works have a schema and syntax that is relatively simple. Recently, Ling et al. (2016) have proposed a data-driven code generation method for high-level, general-purpose PLs like Python and Java. This work treats code generation as a sequence-tosequence modeling problem, and introduce methods to generate words from character-level models, and copy variable"
P17-1041,J84-2007,0,0.805453,"Missing"
P17-1041,W13-2322,0,0.0553119,"Missing"
P17-1041,P16-1069,0,0.00874209,"escriptions are tokenized using NLTK. We perform simple canonicalization for D JANGO, such as replacing quoted strings in the inputs with place holders. See supplementary materials for details. We extract unary closures whose frequency is larger than a threshold k (k = 30 for HS and 50 for D JANGO). Configuration The size of all embeddings is 128, except for node type embeddings, which is 64. The dimensions of RNN states and hidden layers are 256 and 50, respectively. Since our datasets are relatively small for a data-hungry neural model, we impose strong regularization using recurrent 7 Like Beltagy and Quirk (2016), we strip function parameters since they are mostly specific to users. 8 These two metrics are not ideal: accuracy only measures exact match and thus lacks the ability to give credit to semantically correct code that is different from the reference, while it is not clear whether BLEU provides an appropriate proxy for measuring semantics in the code generation task. A more intriguing metric would be directly measuring semantic/functional code equivalence, for which we present a pilot study at the end of this section (cf. Error Analysis). We leave exploring more sophisticated metrics (e.g. base"
P17-1041,D13-1160,0,0.471963,"General-Purpose Code Generation Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu Graham Neubig Language Technologies Institute Carnegie Mellon University gneubig@cs.cmu.edu Abstract In parallel, the NLP community has developed methods for data-driven semantic parsing, which attempt to map NL to structured logical forms executable by computers. These logical forms can be general-purpose meaning representations (Clark and Curran, 2007; Banarescu et al., 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al., 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al., 2015; Misra et al., 2015), among others. While these methods have the advantage of being learnable from data, compared to the programming languages (PLs) in use by programmers, the domain-specific languages targeted by these works have a schema and syntax that is relatively simple. Recently, Ling et al. (2016) have proposed a data-driven code generation method for high-level, general-purpose PLs like Python and Java. This work treats code generation as a sequence-tosequence modeling problem, and int"
P17-1041,P13-1127,0,0.0130959,"wever, we do find interesting examples indicating that the model learns to generalize beyond trivial 6 Related Work Code Generation and Analysis Most works on code generation focus on generating code for domain specific languages (DSLs) (Kushman and 447 7 Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), with neural network-based approaches recently explored (Liu et al., 2016; Parisotto et al., 2016; Balog et al., 2016). For general-purpose code generation, besides the general framework of Ling et al. (2016), existing methods often use language and task-specific rules and strategies (Lei et al., 2013; Raghothaman et al., 2016). A similar line is to use NL queries for code retrieval (Wei et al., 2015; Allamanis et al., 2015). The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016). Finally, our work falls into the broad field of probabilistic modeling of source code (Maddison and Tarlow, 2014; Nguyen et al., 2013). Our approach of factoring an AST using probabilistic models is closely related to Allamanis et al. (2015), which uses a factorized model to measure the semantic relatedness between NL and ASTs for code retrieval,"
P17-1041,P11-1060,0,0.107853,". Bimodal modelling of source code and natural language. In Proceedings of ICML. volume 37. David Alvarez-Melis and Tommi S. Jaakkola. 2017. Tree-structured decoding with doubly recurrent neural networks. In Proceedings of ICLR. Semantic Parsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms. The target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk´y et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neural decoder which constrains the space of nex"
P17-1041,J07-4004,0,0.0416061,"Missing"
P17-1041,P16-1057,0,0.580171,"Missing"
P17-1041,W10-2903,0,0.017128,"f ICML. volume 37. David Alvarez-Melis and Tommi S. Jaakkola. 2017. Tree-structured decoding with doubly recurrent neural networks. In Proceedings of ICLR. Semantic Parsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms. The target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk´y et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the query language for question answering. Finally, the"
P17-1041,P16-1004,0,0.786848,"al., 2014). See supplementary materials for detailed equations. : ct : pt : nft ], st variable terminal embedding of We estimate action probabilities in Eq. (2) using attentional neural encoder-decoder models with an information flow structured by the syntax trees. 1 ... ... Estimating Action Probabilities st = fLSTM ([at GenToken ApplyRule (3) 4 443 We maintain an embedding for each node type. ables the model to utilize the information of parent code segments to make more confident predictions. Similar approaches of injecting parent information were also explored in the S EQ 2T REE model in Dong and Lapata (2016)5 . 4.2.2 Calculating Action Probabilities In this section we explain how action probabilities p(at |x, a<t ) are computed based on st . A PPLY RULE The probability of applying rule r as the current action at is given by a softmax6 : p(at = A PPLY RULE[r]|x, a<t ) = softmax(WR · g(st )) |· e(r) (4) where g(·) is a non-linearity tanh(W·st +b), and e(r) the one-hot vector for rule r. G EN T OKEN As in § 3.2, a token v can be generated from a predefined vocabulary or copied from the input, defined as the marginal probability: Dataset HS D JANGO I FTTT Train Development Test 533 66 66 16,000 1,000"
P17-1041,P16-1154,0,0.0238765,"n Eq. (1). See supplementary materials for the pseudo-code of the inference algorithm. p(at = G EN T OKEN[v]|x, a<t ) = p(gen|x, a<t )p(v|gen, x, a<t ) + p(copy|x, a<t )p(v|copy, x, a<t ). The selection probabilities p(gen|·) and p(copy|·) are given by softmax(WS · st ). The probability of generating v from the vocabulary, p(v|gen, x, a<t ), is defined similarly as Eq. (4), except that we use the G EN T OKEN embedding matrix WG , and we concatenate the context vector ct with st as input. To model the copy probability, we follow recent advances in modeling copying mechanism in neural networks (Gu et al., 2016; Jia and Liang, 2016; Ling et al., 2016), and use a pointer network (Vinyals et al., 2015) to compute the probability of copying the i-th word from the input by attending to input representations {hi }: exp(!(hi , st , ct )) p(wi |copy, x, a<t ) = Pn , i0 =1 exp(!(hi0 , st , ct )) 5 5.1 Experimental Evaluation Datasets and Metrics H EARTH S TONE (HS) dataset (Ling et al., 2016) is a collection of Python classes that implement cards for the card game HearthStone. Each card comes with a set of fields (e.g., name, cost, and description), which we concatenate to create the input sequence. This da"
P17-1041,P15-1002,0,0.0188088,"3. Numbers for our systems are averaged over three runs. We compare primarily with two approaches: (1) Latent Predictor Network (LPN), a state-of-the-art sequenceto-sequence code generation model (Ling et al., 2016), and (2) S EQ 2T REE, a neural semantic parsing model (Dong and Lapata, 2016). S EQ 2T REE generates trees one node at a time, and the target grammar is not explicitly modeled a priori, but implicitly learned from data. We test both the original S EQ 2T REE model released by the authors and our revised one (S EQ 2T REE–UNK) that uses unknown word replacement to handle rare words (Luong et al., 2015). For completeness, we also compare with a strong neural machine translation (NMT) system (Neubig, 2015) using a standard encoder-decoder architecture with attention and unknown word replacement9 , and include numbers from other baselines used in Ling et al. (2016). On the HS dataset, which has relatively large ASTs, we use unary closure for our model and S EQ 2T REE, and for D JANGO we do not. Setup Preprocessing All input descriptions are tokenized using NLTK. We perform simple canonicalization for D JANGO, such as replacing quoted strings in the inputs with place holders. See supplementary"
P17-1041,P16-1195,0,0.040814,"nd 447 7 Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), with neural network-based approaches recently explored (Liu et al., 2016; Parisotto et al., 2016; Balog et al., 2016). For general-purpose code generation, besides the general framework of Ling et al. (2016), existing methods often use language and task-specific rules and strategies (Lei et al., 2013; Raghothaman et al., 2016). A similar line is to use NL queries for code retrieval (Wei et al., 2015; Allamanis et al., 2015). The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016). Finally, our work falls into the broad field of probabilistic modeling of source code (Maddison and Tarlow, 2014; Nguyen et al., 2013). Our approach of factoring an AST using probabilistic models is closely related to Allamanis et al. (2015), which uses a factorized model to measure the semantic relatedness between NL and ASTs for code retrieval, while our model tackles the more challenging generation task. Conclusion This paper proposes a syntax-driven neural code generation approach that generates an abstract syntax tree by sequentially applying actions from a grammar model. Experiments on"
P17-1041,P16-1002,0,0.061432,"pplementary materials for the pseudo-code of the inference algorithm. p(at = G EN T OKEN[v]|x, a<t ) = p(gen|x, a<t )p(v|gen, x, a<t ) + p(copy|x, a<t )p(v|copy, x, a<t ). The selection probabilities p(gen|·) and p(copy|·) are given by softmax(WS · st ). The probability of generating v from the vocabulary, p(v|gen, x, a<t ), is defined similarly as Eq. (4), except that we use the G EN T OKEN embedding matrix WG , and we concatenate the context vector ct with st as input. To model the copy probability, we follow recent advances in modeling copying mechanism in neural networks (Gu et al., 2016; Jia and Liang, 2016; Ling et al., 2016), and use a pointer network (Vinyals et al., 2015) to compute the probability of copying the i-th word from the input by attending to input representations {hi }: exp(!(hi , st , ct )) p(wi |copy, x, a<t ) = Pn , i0 =1 exp(!(hi0 , st , ct )) 5 5.1 Experimental Evaluation Datasets and Metrics H EARTH S TONE (HS) dataset (Ling et al., 2016) is a collection of Python classes that implement cards for the card game HearthStone. Each card comes with a set of fields (e.g., name, cost, and description), which we concatenate to create the input sequence. This dataset is relatively d"
P17-1041,D16-1183,0,0.0122906,"arsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms. The target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk´y et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the query language for question answering. Finally, the structured prediction approach proposed by Xiao et al. (2016) is closely related to our model in using the underlying grammar as prior knowledge to constrain the genera"
P17-1041,D16-1116,0,0.100641,"Missing"
P17-1041,P15-1096,0,0.0102489,"Graham Neubig Language Technologies Institute Carnegie Mellon University gneubig@cs.cmu.edu Abstract In parallel, the NLP community has developed methods for data-driven semantic parsing, which attempt to map NL to structured logical forms executable by computers. These logical forms can be general-purpose meaning representations (Clark and Curran, 2007; Banarescu et al., 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al., 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al., 2015; Misra et al., 2015), among others. While these methods have the advantage of being learnable from data, compared to the programming languages (PLs) in use by programmers, the domain-specific languages targeted by these works have a schema and syntax that is relatively simple. Recently, Ling et al. (2016) have proposed a data-driven code generation method for high-level, general-purpose PLs like Python and Java. This work treats code generation as a sequence-tosequence modeling problem, and introduce methods to generate words from character-level models, and copy variable names from input descriptions. However, u"
P17-1041,D16-1016,0,0.0144524,"and Tommi S. Jaakkola. 2017. Tree-structured decoding with doubly recurrent neural networks. In Proceedings of ICLR. Semantic Parsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms. The target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk´y et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the query language for question answering. Finally, the structured prediction approach proposed by Xia"
P17-1041,N13-1103,0,0.07493,"Missing"
P17-1041,D13-1161,0,0.0367569,"s Miltiadis Allamanis, Daniel Tarlow, Andrew D. Gordon, and Yi Wei. 2015. Bimodal modelling of source code and natural language. In Proceedings of ICML. volume 37. David Alvarez-Melis and Tommi S. Jaakkola. 2017. Tree-structured decoding with doubly recurrent neural networks. In Proceedings of ICLR. Semantic Parsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms. The target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk´y et al., 2016; Jia and Liang, 2016). Liang et"
P17-1041,P15-1142,0,0.0654612,"of source code and natural language. In Proceedings of ICML. volume 37. David Alvarez-Melis and Tommi S. Jaakkola. 2017. Tree-structured decoding with doubly recurrent neural networks. In Proceedings of ICLR. Semantic Parsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms. The target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk´y et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the quer"
P17-1041,P15-1085,0,0.56289,"ity pcyin@cs.cmu.edu Graham Neubig Language Technologies Institute Carnegie Mellon University gneubig@cs.cmu.edu Abstract In parallel, the NLP community has developed methods for data-driven semantic parsing, which attempt to map NL to structured logical forms executable by computers. These logical forms can be general-purpose meaning representations (Clark and Curran, 2007; Banarescu et al., 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al., 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al., 2015; Misra et al., 2015), among others. While these methods have the advantage of being learnable from data, compared to the programming languages (PLs) in use by programmers, the domain-specific languages targeted by these works have a schema and syntax that is relatively simple. Recently, Ling et al. (2016) have proposed a data-driven code generation method for high-level, general-purpose PLs like Python and Java. This work treats code generation as a sequence-tosequence modeling problem, and introduce methods to generate words from character-level models, and copy variable names from input des"
P17-1041,P16-1127,0,0.412589,"we propose a syntax-driven neural code generation model. The backbone of our approach is a grammar model (§ 3) which formalizes the generation story of a derivation AST into sequential application of actions that either apply production rules (§ 3.1), or emit terminal tokens (§ 3.2). The underlying syntax of the PL is therefore encoded in the grammar model a priori as the set of possible actions. Our approach frees the model from recovering the underlying grammar from limited training data, and instead enables the system to focus on learning the compositionality among existing grammar rules. Xiao et al. (2016) have noted that this imposition of structure on neural models is useful for semantic parsing, and we expect this to be even more important for general-purpose PLs where the syntax trees are larger and more complex. Second, we hypothesize that structural information helps to model information flow within the neural network, which naturally reflects the recursive structure of PLs. To test this, we extend a standard recurrent neural network (RNN) decoder to allow for additional neural connections which reflect the recursive structure of an AST (§ 4.2). As an example, when expanding the node ? in"
P17-1041,P15-1128,0,0.0452344,"vid Alvarez-Melis and Tommi S. Jaakkola. 2017. Tree-structured decoding with doubly recurrent neural networks. In Proceedings of ICLR. Semantic Parsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms. The target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk´y et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the query language for question answering. Finally, the structured predict"
P17-1041,W16-0105,1,0.410535,"ch aims to transform NL descriptions into executable logical forms. The target logical forms can be viewed as DSLs. The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016). Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016). Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kocisk´y et al., 2016; Jia and Liang, 2016). Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the query language for question answering. Finally, the structured prediction approach proposed by Xiao et al. (2016) is closely related to our model in using the underlying grammar as prior knowledge to constrain the generation process of derivation trees, while our method is based on a uni"
P17-1079,J92-4003,0,0.649568,"Missing"
P17-1079,P16-1186,0,0.0411675,"Missing"
P17-1079,D15-1249,0,0.0192774,"2; Mikolov et al., 2013) to the denominator of the softmax have also been proposed to reduce calculation at training. However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax. Vocabulary selection approaches (Mi et al., 2016; L’Hostis et al., 2016) can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality. Other methods using characters (Ling et al., 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency. Problem Description and Prior Work Formulation and Standard Softmax Most of current NMT models use one-hot representations to represent the words in the output vocabulary – each word w is represented by a unique sparse vector eid(w) ∈ RV , in which only one element at the position corresponding to the word ID id(w) ∈ {x ∈ N |1 ≤ x ≤ V } is 1, while others are 0. V represents the vocabulary size of the target language. NMT"
P17-1079,P07-2045,0,0.008746,"Missing"
P17-1079,P11-2093,1,0.742129,"Missing"
P17-1079,D15-1166,0,0.0750918,"train the parameters efficiently, and possible to perform decoding efficiently with choosing the candidate words from the full probability distribution. In particular, the method should be performed fast on general CPUs to suppress physical costs of computational resources for actual production systems. When handling broad or open domains, machine translation systems usually have to handle a large vocabulary as their inputs and outputs. This is particularly a problem in neural machine translation (NMT) models (Sutskever et al., 2014), such as the attention-based models (Bahdanau et al., 2014; Luong et al., 2015) shown in Figure 1. In these models, the output layer is required to generate a specific word from an internal vector, and a large vocabulary size tends to require a large amount of computation to predict each of the candidate word probabilities. Because this is a significant problem for neural language and translation models, there are a number of methods proposed to resolve this problem, which we detail in Section 2.2. However, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in NMT systems: Compatibilit"
P17-1079,P02-1040,0,0.0973991,"Missing"
P17-1079,P16-2021,0,0.0214666,"ion matrix of the output layer sparser than a fully-connected softmax, and can reduce time/space computation amount by ignoring zero part of the matrix. However, this method restricts the usage of hidden layer, and the size of the matrix is still in proportion to V . Sampling-based approximations (Mnih and Teh, 2012; Mikolov et al., 2013) to the denominator of the softmax have also been proposed to reduce calculation at training. However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax. Vocabulary selection approaches (Mi et al., 2016; L’Hostis et al., 2016) can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality. Other methods using characters (Ling et al., 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency. Problem Description and Prior Work Formulation and Standard Softmax Most of current NMT models use"
P17-1079,P16-1162,0,0.0527334,"ions (Mnih and Teh, 2012; Mikolov et al., 2013) to the denominator of the softmax have also been proposed to reduce calculation at training. However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax. Vocabulary selection approaches (Mi et al., 2016; L’Hostis et al., 2016) can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality. Other methods using characters (Ling et al., 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency. Problem Description and Prior Work Formulation and Standard Softmax Most of current NMT models use one-hot representations to represent the words in the output vocabulary – each word w is represented by a unique sparse vector eid(w) ∈ RV , in which only one element at the position corresponding to the word ID id(w) ∈ {x ∈ N |1 ≤ x ≤ V } is 1, while others are 0. V represents the vocabulary size"
P17-1188,P11-2071,0,0.021334,"Missing"
P17-1188,P16-2044,0,0.0806634,"m simpler bag-of-words (Iyyer et al., 2015) and linear recurrent neural network (RNN) models (Sutskever et al., 2014; Kiros et al., 2015), to more sophisticated models using treestructured (Socher et al., 2013) or convolutional networks (Kalchbrenner et al., 2014). In fact, a growing body of evidence shows that it is essential to look below the word-level and consider compositionality within words themselves. For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016). Additionally, for languages with productive word formation (such as agglutination and compounding), models calculating morphologysensitive word representations have been found effective (Luong et al., 2013; Botha and Blunsom, 2014). These models help to learn more robust representations for rare words by exploiting morphological patterns, as opposed to models that operate purely on the lexical level as the atomic units. For many languages, compositionality stops at the character-level: characters are atomic units of meaning or pronunciation in the language, and no further decomposition can b"
P17-1188,P08-2015,0,0.0146265,"classification (Kim, 2014; Zhang et al., 2015). These models look at the sequential dependencies at the word or character-level and achieve the state-of-the-art results. These works inspire us to use CNN to extract features from image and serve as the input to the RNN. Our model is able to directly back-propagate the gradient all the way through the CNN, which generates visual embeddings, in a way such that the embedding can contain both semantic and visual information. Several techniques for reducing the rare words effects have been introduced in the literature, including spelling expansion (Habash, 2008), dictionary term expansion (Habash, 2008), proper name transliteration (Daum´e and Jagarlamudi, 2011), treating words as a sequence of characters (Luong and Manning, 2016), subword units (Sennrich et al., 2015), and reading text as bytes (Gillick et al., 2015). However, most of these techniques still have no mechanism for handling low frequency characters, which are the target of this work. Finally, there are works on improving embeddings with radicals, which explicitly splits Chinese characters into radicals based on a dictionary 2066 of what radicals are included in which characters (Li et"
P17-1188,P15-1162,0,0.0228997,"German. The red part of the characters are shared, and affects the pronunciation (top) or meaning (bottom). Introduction Compositionality—the fact that the meaning of a complex expression is determined by its structure and the meanings of its constituents—is a hallmark of every natural language (Frege and Austin, 1980; Szab´o, 2010). Recently, neural models have provided a powerful tool for learning how to compose words together into a meaning representation of whole sentences for many downstream tasks. This is done using models of various levels of sophistication, from simpler bag-of-words (Iyyer et al., 2015) and linear recurrent neural network (RNN) models (Sutskever et al., 2014; Kiros et al., 2015), to more sophisticated models using treestructured (Socher et al., 2013) or convolutional networks (Kalchbrenner et al., 2014). In fact, a growing body of evidence shows that it is essential to look below the word-level and consider compositionality within words themselves. For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016). Additionally, for la"
P17-1188,P14-1062,0,0.00993853,"cture and the meanings of its constituents—is a hallmark of every natural language (Frege and Austin, 1980; Szab´o, 2010). Recently, neural models have provided a powerful tool for learning how to compose words together into a meaning representation of whole sentences for many downstream tasks. This is done using models of various levels of sophistication, from simpler bag-of-words (Iyyer et al., 2015) and linear recurrent neural network (RNN) models (Sutskever et al., 2014; Kiros et al., 2015), to more sophisticated models using treestructured (Socher et al., 2013) or convolutional networks (Kalchbrenner et al., 2014). In fact, a growing body of evidence shows that it is essential to look below the word-level and consider compositionality within words themselves. For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016). Additionally, for languages with productive word formation (such as agglutination and compounding), models calculating morphologysensitive word representations have been found effective (Luong et al., 2013; Botha and Blunsom, 2014). These mo"
P17-1188,D07-1073,0,0.0131792,"ication dataset where the input is a Wikipedia article title in Chinese, Japanese, or Korean, and the output is the category to which the article belongs.3 This satisfies (1), because Wikipedia titles are short and thus each character in the title will be important to our decision about its category. It also satisfies (2), because Chinese, Japanese, and Korean have writing systems with large numbers of characters that decompose regularly as shown in Fig. 1. While this task in itself is novel, it is similar to previous work in named entity type inference using Wikipedia (Toral and Munoz, 2006; Kazama and Torisawa, 2007; Ratinov and Roth, 2009), which has proven useful for downstream named entity recognition systems. 2.1 Dataset Collection As the labels we would like to predict, we use 12 different main categories from the Wikipedia web page: Geography, Sports, Arts, Military, Economics, Transportation, Health Science, Education, Food Culture, Religion and Belief, Agriculture and Electronics. Wikipedia has a hierarchical structure, where each of these main categories has a number of subcategories, and each subcategory has its own subcategories, etc. We traverse this hierarchical structure, adding each main c"
P17-1188,D14-1181,0,0.0116824,"ion-based neural machine translation system (Deng et al., 2016). They tested on realworld rendered mathematical expressions paired with LaTeX markup and show the system is effective at generating accurate markup. Other than that, there are several works that combine visual information with text in improving machine translation (Sutskever et al., 2014), visual question answering, caption generation (Xu et al., 2015), etc. These works extract image representations from a pre-trained CNN (Zhu et al., 2016; Wang et al., 2016). Unrelated to images, CNNs have also been used for text classification (Kim, 2014; Zhang et al., 2015). These models look at the sequential dependencies at the word or character-level and achieve the state-of-the-art results. These works inspire us to use CNN to extract features from image and serve as the input to the RNN. Our model is able to directly back-propagate the gradient all the way through the CNN, which generates visual embeddings, in a way such that the embedding can contain both semantic and visual information. Several techniques for reducing the rare words effects have been introduced in the literature, including spelling expansion (Habash, 2008), dictionary"
P17-1188,D15-1098,0,0.164975,"Missing"
P17-1188,D15-1176,0,0.0329691,"f various levels of sophistication, from simpler bag-of-words (Iyyer et al., 2015) and linear recurrent neural network (RNN) models (Sutskever et al., 2014; Kiros et al., 2015), to more sophisticated models using treestructured (Socher et al., 2013) or convolutional networks (Kalchbrenner et al., 2014). In fact, a growing body of evidence shows that it is essential to look below the word-level and consider compositionality within words themselves. For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016). Additionally, for languages with productive word formation (such as agglutination and compounding), models calculating morphologysensitive word representations have been found effective (Luong et al., 2013; Botha and Blunsom, 2014). These models help to learn more robust representations for rare words by exploiting morphological patterns, as opposed to models that operate purely on the lexical level as the atomic units. For many languages, compositionality stops at the character-level: characters are atomic units of meaning or pronunciation in the l"
P17-1188,P16-1100,0,0.0414301,"Missing"
P17-1188,W13-3512,0,0.0506351,"or convolutional networks (Kalchbrenner et al., 2014). In fact, a growing body of evidence shows that it is essential to look below the word-level and consider compositionality within words themselves. For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016). Additionally, for languages with productive word formation (such as agglutination and compounding), models calculating morphologysensitive word representations have been found effective (Luong et al., 2013; Botha and Blunsom, 2014). These models help to learn more robust representations for rare words by exploiting morphological patterns, as opposed to models that operate purely on the lexical level as the atomic units. For many languages, compositionality stops at the character-level: characters are atomic units of meaning or pronunciation in the language, and no further decomposition can be done.1 However, for other languages, character-level compositionality, where a character’s meaning or pronunciation can 1 In English, for example, this is largely the case. 2059 Proceedings of the 55th Ann"
P17-1188,W06-2809,0,0.0282913,"e create a text classification dataset where the input is a Wikipedia article title in Chinese, Japanese, or Korean, and the output is the category to which the article belongs.3 This satisfies (1), because Wikipedia titles are short and thus each character in the title will be important to our decision about its category. It also satisfies (2), because Chinese, Japanese, and Korean have writing systems with large numbers of characters that decompose regularly as shown in Fig. 1. While this task in itself is novel, it is similar to previous work in named entity type inference using Wikipedia (Toral and Munoz, 2006; Kazama and Torisawa, 2007; Ratinov and Roth, 2009), which has proven useful for downstream named entity recognition systems. 2.1 Dataset Collection As the labels we would like to predict, we use 12 different main categories from the Wikipedia web page: Geography, Sports, Arts, Military, Economics, Transportation, Health Science, Education, Food Culture, Religion and Belief, Agriculture and Electronics. Wikipedia has a hierarchical structure, where each of these main categories has a number of subcategories, and each subcategory has its own subcategories, etc. We traverse this hierarchical st"
P17-1188,D16-1100,0,0.325904,"Missing"
P17-1188,W09-1119,0,0.0229491,"nput is a Wikipedia article title in Chinese, Japanese, or Korean, and the output is the category to which the article belongs.3 This satisfies (1), because Wikipedia titles are short and thus each character in the title will be important to our decision about its category. It also satisfies (2), because Chinese, Japanese, and Korean have writing systems with large numbers of characters that decompose regularly as shown in Fig. 1. While this task in itself is novel, it is similar to previous work in named entity type inference using Wikipedia (Toral and Munoz, 2006; Kazama and Torisawa, 2007; Ratinov and Roth, 2009), which has proven useful for downstream named entity recognition systems. 2.1 Dataset Collection As the labels we would like to predict, we use 12 different main categories from the Wikipedia web page: Geography, Sports, Arts, Military, Economics, Transportation, Health Science, Education, Food Culture, Religion and Belief, Agriculture and Electronics. Wikipedia has a hierarchical structure, where each of these main categories has a number of subcategories, and each subcategory has its own subcategories, etc. We traverse this hierarchical structure, adding each main category tag to all of its"
P17-1188,P15-2098,0,0.263772,"Missing"
P17-1188,D13-1170,0,0.00514238,"f a complex expression is determined by its structure and the meanings of its constituents—is a hallmark of every natural language (Frege and Austin, 1980; Szab´o, 2010). Recently, neural models have provided a powerful tool for learning how to compose words together into a meaning representation of whole sentences for many downstream tasks. This is done using models of various levels of sophistication, from simpler bag-of-words (Iyyer et al., 2015) and linear recurrent neural network (RNN) models (Sutskever et al., 2014; Kiros et al., 2015), to more sophisticated models using treestructured (Socher et al., 2013) or convolutional networks (Kalchbrenner et al., 2014). In fact, a growing body of evidence shows that it is essential to look below the word-level and consider compositionality within words themselves. For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016). Additionally, for languages with productive word formation (such as agglutination and compounding), models calculating morphologysensitive word representations have been found effective ("
P17-1188,P16-1162,0,\N,Missing
P18-1070,P16-1002,0,0.0611471,"e creation of which can be expensive, cumbersome, and time-consuming. Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models. These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as indirect supervision (Clarke et al. (2010); Liang et al. (2011); Berant et al. (2013), inter alia), or dataaugmentation techniques that automatically generate pseudo-parallel corpora using hand-crafted or induced grammars (Jia and Liang, 2016; Wang et al., 2015). In this work, we focus on semi-supervised learning, aiming to learn from both limited Introduction Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Lian"
P18-1070,N07-2021,0,0.0157725,"correlates well with the learning signal, yielding stable improvements over the supervised parser. This suggests the importance of using carefully designed baselines in REINFORCE learning, especially when the reward signal has large range (e.g., log-likelihoods). Semantic Parsing Most existing works alleviate issues of limited parallel data through weaklysupervised learning, using the denotations of MRs as indirect supervision (Reddy et al., 2014; Krishnamurthy et al., 2016; Neelakantan et al., 2016; Pasupat and Liang, 2015; Yin et al., 2016). For semi-supervised learning of semantic parsing, Kate and Mooney (2007) first explore using transductive SVMs to learn from a semantic parser’s predictions. Konstas et al. (2017) apply self-training to bootstrap an existing parser for AMR parsing. Kocisk´y et al. (2016) employ VAEs for semantic parsing, but in contrast to S TRUCT VAE’s structured representation of MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised semantic parsing, which exploits external linguistic analysis of utterances (e.g., dependency trees) and the schema of target knowledge bases to infer the latent MRs (Po"
P18-1070,W13-2322,0,0.0705869,"al. (2011); Berant et al. (2013), inter alia), or dataaugmentation techniques that automatically generate pseudo-parallel corpora using hand-crafted or induced grammars (Jia and Liang, 2016; Wang et al., 2015). In this work, we focus on semi-supervised learning, aiming to learn from both limited Introduction Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017), or generation of domainspecific (e.g., SQL) or general purpose programming languages (e.g., Python) (Quirk et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017). 1 z Code available at http://pcyin.me/struct vae 754 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 754–765 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational"
P18-1070,D13-1160,0,0.0981639,"s are also extremely data hungry: optimization of such models requires large amounts of training data of parallel NL utterances and manually annotated MRs, the creation of which can be expensive, cumbersome, and time-consuming. Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models. These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as indirect supervision (Clarke et al. (2010); Liang et al. (2011); Berant et al. (2013), inter alia), or dataaugmentation techniques that automatically generate pseudo-parallel corpora using hand-crafted or induced grammars (Jia and Liang, 2016; Wang et al., 2015). In this work, we focus on semi-supervised learning, aiming to learn from both limited Introduction Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra a"
P18-1070,K16-1002,0,0.147149,"Missing"
P18-1070,P17-1005,0,0.0746218,"Missing"
P18-1070,D16-1116,0,0.1033,"Missing"
P18-1070,P16-1185,0,0.0235575,"Interestingly, compared with the results in Tab. 1, we found that the gains are especially larger with few labeled examples — S TRUCTVAE-S EQ achieves improvements of 8-10 points when |L |&lt; 1000. These results suggest that semi-supervision is especially useful in improving a mediocre parser in low resource settings. 761 Accuracy 0.66 5 0.64 Semi-supervised Learning for NLP Semisupervised learning comes with a long history (Zhu, 2005), with applications in NLP from early work of self-training (Yarowsky, 1995), and graph-based methods (Das and Smith, 2011), to recent advances in auto-encoders (Cheng et al., 2016; Socher et al., 2011; Zhang et al., 2017) and deep generative methods (Xu et al., 2017). Our work follows the line of neural variational inference for text processing (Miao et al., 2016), and resembles Miao and Blunsom (2016), which uses VAEs to model summaries as discrete latent variables for semi-supervised summarization, while we extend the VAE architecture for more complex, tree-structured latent variables. StructVAE Sup. 0.62 0.0 0.2 0.4 0.6 0.8 1.0 λ Figure 6: Performance on D JANGO (|L |= 5000) w.r.t. the KL weight λ Accuracy 0.655 0.650 StructVAE 0.645 1000 5000 8000 12000 14000 Relat"
P18-1070,P17-1014,0,0.180612,".2.1). S TRUCT VAE: VAEs with Tree-structured Latent Variables Generative Story S TRUCT VAE follows the standard VAE architecture, and defines a generative story that explains how an NL utterance is generated: a latent meaning representation z is sampled from a prior distribution p(z) over MRs, which encodes the latent semantics of the utterance. A reconstruction model pθ (x|z) then decodes the sampled MR z into the observed NL utterance x. Both the prior p(z) and the reconstruction model p(x|z) takes tree-structured MRs as inputs. To model such inputs with rich internal structures, we follow Konstas et al. (2017), and model the distribution over a sequential surface representation of z, z s instead. Specifically, we have p(z) , p(z s ) and pθ (x|z) , pθ (x|z s )2 . For code generation, z s is simply the surface source code of the AST z. For semantic parsing, z s is the linearized s-expression of the logical form. Linearization allows us to use standard sequence-to-sequence networks to model p(z) and pθ (x|z). As we will explain in § 4.3, we find these two components perform well with linearization. Specifically, the prior is parameterized by a Long Short-Term Memory (LSTM) language model over z s . Th"
P18-1070,W10-2903,0,0.0542301,"; Zhong et al., 2017). However, these models are also extremely data hungry: optimization of such models requires large amounts of training data of parallel NL utterances and manually annotated MRs, the creation of which can be expensive, cumbersome, and time-consuming. Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models. These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as indirect supervision (Clarke et al. (2010); Liang et al. (2011); Berant et al. (2013), inter alia), or dataaugmentation techniques that automatically generate pseudo-parallel corpora using hand-crafted or induced grammars (Jia and Liang, 2016; Wang et al., 2015). In this work, we focus on semi-supervised learning, aiming to learn from both limited Introduction Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representa"
P18-1070,D16-1016,0,0.0195972,"on ATIS, in most settings it is worse than our LM baseline, and could be even worse than the supervised parser. On the other hand, our LM baseline correlates well with the learning signal, yielding stable improvements over the supervised parser. This suggests the importance of using carefully designed baselines in REINFORCE learning, especially when the reward signal has large range (e.g., log-likelihoods). Semantic Parsing Most existing works alleviate issues of limited parallel data through weaklysupervised learning, using the denotations of MRs as indirect supervision (Reddy et al., 2014; Krishnamurthy et al., 2016; Neelakantan et al., 2016; Pasupat and Liang, 2015; Yin et al., 2016). For semi-supervised learning of semantic parsing, Kate and Mooney (2007) first explore using transductive SVMs to learn from a semantic parser’s predictions. Konstas et al. (2017) apply self-training to bootstrap an existing parser for AMR parsing. Kocisk´y et al. (2016) employ VAEs for semantic parsing, but in contrast to S TRUCT VAE’s structured representation of MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised semantic parsing, which e"
P18-1070,P11-1144,0,0.0255418,"ith different inference/reconstruction networks and priors. Interestingly, compared with the results in Tab. 1, we found that the gains are especially larger with few labeled examples — S TRUCTVAE-S EQ achieves improvements of 8-10 points when |L |&lt; 1000. These results suggest that semi-supervision is especially useful in improving a mediocre parser in low resource settings. 761 Accuracy 0.66 5 0.64 Semi-supervised Learning for NLP Semisupervised learning comes with a long history (Zhu, 2005), with applications in NLP from early work of self-training (Yarowsky, 1995), and graph-based methods (Das and Smith, 2011), to recent advances in auto-encoders (Cheng et al., 2016; Socher et al., 2011; Zhang et al., 2017) and deep generative methods (Xu et al., 2017). Our work follows the line of neural variational inference for text processing (Miao et al., 2016), and resembles Miao and Blunsom (2016), which uses VAEs to model summaries as discrete latent variables for semi-supervised summarization, while we extend the VAE architecture for more complex, tree-structured latent variables. StructVAE Sup. 0.62 0.0 0.2 0.4 0.6 0.8 1.0 λ Figure 6: Performance on D JANGO (|L |= 5000) w.r.t. the KL weight λ Accuracy 0.6"
P18-1070,P17-1003,0,0.0558313,"2016; Wang et al., 2015). In this work, we focus on semi-supervised learning, aiming to learn from both limited Introduction Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017), or generation of domainspecific (e.g., SQL) or general purpose programming languages (e.g., Python) (Quirk et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017). 1 z Code available at http://pcyin.me/struct vae 754 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 754–765 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2.1 amounts of parallel NL-MR corpora, and unlabeled but readily-available NL utterances. We draw inspiration from recent success in applying variational auto-encoding"
P18-1070,P16-1004,0,0.632797,"nlabeled data as treestructured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, S TRUCT VAE outperforms strong supervised models.1 1 Structured Latent Semantic Space (MRs) Inference Model Reconstruction Model q (z|x) p✓ (x|z) Sort my_list in descending order Figure 1: Graphical Representation of S TRUCT VAE While these models have a long history (Zelle and Mooney, 1996; Tang and Mooney, 2001), recent advances are largely attributed to the success of neural network models (Xiao et al., 2016; Ling et al., 2016; Dong and Lapata, 2016; Iyer et al., 2017; Zhong et al., 2017). However, these models are also extremely data hungry: optimization of such models requires large amounts of training data of parallel NL utterances and manually annotated MRs, the creation of which can be expensive, cumbersome, and time-consuming. Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models. These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as ind"
P18-1070,P11-1060,0,0.0639471,"However, these models are also extremely data hungry: optimization of such models requires large amounts of training data of parallel NL utterances and manually annotated MRs, the creation of which can be expensive, cumbersome, and time-consuming. Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models. These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as indirect supervision (Clarke et al. (2010); Liang et al. (2011); Berant et al. (2013), inter alia), or dataaugmentation techniques that automatically generate pseudo-parallel corpora using hand-crafted or induced grammars (Jia and Liang, 2016; Wang et al., 2015). In this work, we focus on semi-supervised learning, aiming to learn from both limited Introduction Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu"
P18-1070,W17-2607,0,0.0143343,"representation of MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised semantic parsing, which exploits external linguistic analysis of utterances (e.g., dependency trees) and the schema of target knowledge bases to infer the latent MRs (Poon and Domingos, 2009; Poon, 2013). Another line of research is domain adaptation, which seeks to transfer a semantic parser learned from a source domain to the target domain of interest, therefore alleviating the need of parallel data from the target domain (Su and Yan, 2017; Fan et al., 2017; Herzig and Berant, 2018). Impact of the Prior p(z) Fig. 6 depicts the performance of S TRUCT VAE as a function of the KL term weight λ in Eq. (3). When S TRUCT VAE degenerates to a vanilla auto-encoder without the prior distribution (i.e., λ = 0), it under-performs the supervised baseline. This is in line with our observation in Tab. 3 showing that the prior helps identify unnatural samples. The performance of the model also drops when λ &gt; 0.1, suggesting that empirically controlling the influence of the prior to the inference model is important. 6 Conclusion We propose S TRUCT VAE, a deep g"
P18-1070,P16-1154,0,0.0230168,"d pθ (x|z) , pθ (x|z s )2 . For code generation, z s is simply the surface source code of the AST z. For semantic parsing, z s is the linearized s-expression of the logical form. Linearization allows us to use standard sequence-to-sequence networks to model p(z) and pθ (x|z). As we will explain in § 4.3, we find these two components perform well with linearization. Specifically, the prior is parameterized by a Long Short-Term Memory (LSTM) language model over z s . The reconstruction model is an attentional sequence-to-sequence network (Luong et al., 2015), augmented with a copying mechanism (Gu et al., 2016), allowing an out-ofvocabulary (OOV) entity in z s to be copied to x (e.g., the variable name my list in Fig. 1 and its AST in Fig. 2). We refer readers to Appendix B for details of the neural network architecture. 3.2.1 Generating ASTs with ASDL Grammar First, we present a brief introduction to ASDL. An AST can be generated by applying typed constructors in an ASDL grammar, such as those in Fig. 3 for the Python ASDL grammar. Each constructor specifies a language construct, and is assigned to a particular composite type. For example, the constructor Call has type expr (expression), and it den"
P18-1070,P16-1057,0,0.109762,"Missing"
P18-1070,P17-1097,0,0.0395794,"Missing"
P18-1070,D15-1166,0,0.0412505,"f z, z s instead. Specifically, we have p(z) , p(z s ) and pθ (x|z) , pθ (x|z s )2 . For code generation, z s is simply the surface source code of the AST z. For semantic parsing, z s is the linearized s-expression of the logical form. Linearization allows us to use standard sequence-to-sequence networks to model p(z) and pθ (x|z). As we will explain in § 4.3, we find these two components perform well with linearization. Specifically, the prior is parameterized by a Long Short-Term Memory (LSTM) language model over z s . The reconstruction model is an attentional sequence-to-sequence network (Luong et al., 2015), augmented with a copying mechanism (Gu et al., 2016), allowing an out-ofvocabulary (OOV) entity in z s to be copied to x (e.g., the variable name my list in Fig. 1 and its AST in Fig. 2). We refer readers to Appendix B for details of the neural network architecture. 3.2.1 Generating ASTs with ASDL Grammar First, we present a brief introduction to ASDL. An AST can be generated by applying typed constructors in an ASDL grammar, such as those in Fig. 3 for the Python ASDL grammar. Each constructor specifies a language construct, and is assigned to a particular composite type. For example, the c"
P18-1070,D18-1190,0,0.0459464,"MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised semantic parsing, which exploits external linguistic analysis of utterances (e.g., dependency trees) and the schema of target knowledge bases to infer the latent MRs (Poon and Domingos, 2009; Poon, 2013). Another line of research is domain adaptation, which seeks to transfer a semantic parser learned from a source domain to the target domain of interest, therefore alleviating the need of parallel data from the target domain (Su and Yan, 2017; Fan et al., 2017; Herzig and Berant, 2018). Impact of the Prior p(z) Fig. 6 depicts the performance of S TRUCT VAE as a function of the KL term weight λ in Eq. (3). When S TRUCT VAE degenerates to a vanilla auto-encoder without the prior distribution (i.e., λ = 0), it under-performs the supervised baseline. This is in line with our observation in Tab. 3 showing that the prior helps identify unnatural samples. The performance of the model also drops when λ &gt; 0.1, suggesting that empirically controlling the influence of the prior to the inference model is important. 6 Conclusion We propose S TRUCT VAE, a deep generative model with tree-"
P18-1070,D16-1031,0,0.431413,"se programming languages (e.g., Python) (Quirk et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017). 1 z Code available at http://pcyin.me/struct vae 754 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 754–765 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2.1 amounts of parallel NL-MR corpora, and unlabeled but readily-available NL utterances. We draw inspiration from recent success in applying variational auto-encoding (VAE) models in semisupervised sequence-to-sequence learning (Miao and Blunsom, 2016; Kocisk´y et al., 2016), and propose S TRUCT VAE — a principled deep generative approach for semi-supervised learning with tree-structured latent variables (Fig. 1). S TRUCTVAE is based on a generative story where the surface NL utterances are generated from treestructured latent MRs following the standard VAE architecture: (1) an off-the-shelf semantic parser functions as the inference model, parsing an observed NL utterance into latent meaning representations (§ 3.2); (2) a reconstruction model decodes the latent MR into the original observed utterance (§ 3.1). This formulation enables our"
P18-1070,P17-1089,0,0.0428266,"ructured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, S TRUCT VAE outperforms strong supervised models.1 1 Structured Latent Semantic Space (MRs) Inference Model Reconstruction Model q (z|x) p✓ (x|z) Sort my_list in descending order Figure 1: Graphical Representation of S TRUCT VAE While these models have a long history (Zelle and Mooney, 1996; Tang and Mooney, 2001), recent advances are largely attributed to the success of neural network models (Xiao et al., 2016; Ling et al., 2016; Dong and Lapata, 2016; Iyer et al., 2017; Zhong et al., 2017). However, these models are also extremely data hungry: optimization of such models requires large amounts of training data of parallel NL utterances and manually annotated MRs, the creation of which can be expensive, cumbersome, and time-consuming. Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models. These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as indirect supervision ("
P18-1070,D16-1183,0,0.0339731,"(2013), inter alia), or dataaugmentation techniques that automatically generate pseudo-parallel corpora using hand-crafted or induced grammars (Jia and Liang, 2016; Wang et al., 2015). In this work, we focus on semi-supervised learning, aiming to learn from both limited Introduction Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017), or generation of domainspecific (e.g., SQL) or general purpose programming languages (e.g., Python) (Quirk et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017). 1 z Code available at http://pcyin.me/struct vae 754 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 754–765 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2.1 amounts"
P18-1070,P15-1129,0,0.115958,"an be expensive, cumbersome, and time-consuming. Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models. These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as indirect supervision (Clarke et al. (2010); Liang et al. (2011); Berant et al. (2013), inter alia), or dataaugmentation techniques that automatically generate pseudo-parallel corpora using hand-crafted or induced grammars (Jia and Liang, 2016; Wang et al., 2015). In this work, we focus on semi-supervised learning, aiming to learn from both limited Introduction Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017), or"
P18-1070,P16-1127,0,0.133119,"odels latent MRs not observed in the unlabeled data as treestructured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, S TRUCT VAE outperforms strong supervised models.1 1 Structured Latent Semantic Space (MRs) Inference Model Reconstruction Model q (z|x) p✓ (x|z) Sort my_list in descending order Figure 1: Graphical Representation of S TRUCT VAE While these models have a long history (Zelle and Mooney, 1996; Tang and Mooney, 2001), recent advances are largely attributed to the success of neural network models (Xiao et al., 2016; Ling et al., 2016; Dong and Lapata, 2016; Iyer et al., 2017; Zhong et al., 2017). However, these models are also extremely data hungry: optimization of such models requires large amounts of training data of parallel NL utterances and manually annotated MRs, the creation of which can be expensive, cumbersome, and time-consuming. Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models. These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e"
P18-1070,P15-1142,0,0.0227583,"eline, and could be even worse than the supervised parser. On the other hand, our LM baseline correlates well with the learning signal, yielding stable improvements over the supervised parser. This suggests the importance of using carefully designed baselines in REINFORCE learning, especially when the reward signal has large range (e.g., log-likelihoods). Semantic Parsing Most existing works alleviate issues of limited parallel data through weaklysupervised learning, using the denotations of MRs as indirect supervision (Reddy et al., 2014; Krishnamurthy et al., 2016; Neelakantan et al., 2016; Pasupat and Liang, 2015; Yin et al., 2016). For semi-supervised learning of semantic parsing, Kate and Mooney (2007) first explore using transductive SVMs to learn from a semantic parser’s predictions. Konstas et al. (2017) apply self-training to bootstrap an existing parser for AMR parsing. Kocisk´y et al. (2016) employ VAEs for semantic parsing, but in contrast to S TRUCT VAE’s structured representation of MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised semantic parsing, which exploits external linguistic analysis of utterances"
P18-1070,P13-1092,0,0.0181815,"ansductive SVMs to learn from a semantic parser’s predictions. Konstas et al. (2017) apply self-training to bootstrap an existing parser for AMR parsing. Kocisk´y et al. (2016) employ VAEs for semantic parsing, but in contrast to S TRUCT VAE’s structured representation of MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised semantic parsing, which exploits external linguistic analysis of utterances (e.g., dependency trees) and the schema of target knowledge bases to infer the latent MRs (Poon and Domingos, 2009; Poon, 2013). Another line of research is domain adaptation, which seeks to transfer a semantic parser learned from a source domain to the target domain of interest, therefore alleviating the need of parallel data from the target domain (Su and Yan, 2017; Fan et al., 2017; Herzig and Berant, 2018). Impact of the Prior p(z) Fig. 6 depicts the performance of S TRUCT VAE as a function of the KL term weight λ in Eq. (3). When S TRUCT VAE degenerates to a vanilla auto-encoder without the prior distribution (i.e., λ = 0), it under-performs the supervised baseline. This is in line with our observation in Tab. 3"
P18-1070,D09-1001,0,0.0637251,"7) first explore using transductive SVMs to learn from a semantic parser’s predictions. Konstas et al. (2017) apply self-training to bootstrap an existing parser for AMR parsing. Kocisk´y et al. (2016) employ VAEs for semantic parsing, but in contrast to S TRUCT VAE’s structured representation of MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised semantic parsing, which exploits external linguistic analysis of utterances (e.g., dependency trees) and the schema of target knowledge bases to infer the latent MRs (Poon and Domingos, 2009; Poon, 2013). Another line of research is domain adaptation, which seeks to transfer a semantic parser learned from a source domain to the target domain of interest, therefore alleviating the need of parallel data from the target domain (Su and Yan, 2017; Fan et al., 2017; Herzig and Berant, 2018). Impact of the Prior p(z) Fig. 6 depicts the performance of S TRUCT VAE as a function of the KL term weight λ in Eq. (3). When S TRUCT VAE degenerates to a vanilla auto-encoder without the prior distribution (i.e., λ = 0), it under-performs the supervised baseline. This is in line with our observati"
P18-1070,P95-1026,0,0.554039,"t gains, demonstrating its compatibility with different inference/reconstruction networks and priors. Interestingly, compared with the results in Tab. 1, we found that the gains are especially larger with few labeled examples — S TRUCTVAE-S EQ achieves improvements of 8-10 points when |L |&lt; 1000. These results suggest that semi-supervision is especially useful in improving a mediocre parser in low resource settings. 761 Accuracy 0.66 5 0.64 Semi-supervised Learning for NLP Semisupervised learning comes with a long history (Zhu, 2005), with applications in NLP from early work of self-training (Yarowsky, 1995), and graph-based methods (Das and Smith, 2011), to recent advances in auto-encoders (Cheng et al., 2016; Socher et al., 2011; Zhang et al., 2017) and deep generative methods (Xu et al., 2017). Our work follows the line of neural variational inference for text processing (Miao et al., 2016), and resembles Miao and Blunsom (2016), which uses VAEs to model summaries as discrete latent variables for semi-supervised summarization, while we extend the VAE architecture for more complex, tree-structured latent variables. StructVAE Sup. 0.62 0.0 0.2 0.4 0.6 0.8 1.0 λ Figure 6: Performance on D JANGO ("
P18-1070,P15-1085,0,0.0378143,"on Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017), or generation of domainspecific (e.g., SQL) or general purpose programming languages (e.g., Python) (Quirk et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017). 1 z Code available at http://pcyin.me/struct vae 754 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 754–765 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2.1 amounts of parallel NL-MR corpora, and unlabeled but readily-available NL utterances. We draw inspiration from recent success in applying variational auto-encoding (VAE) models in semisupervised sequence-to-sequence learning (Miao and Blunsom, 2016; Kocisk´y et al., 2016), and propose"
P18-1070,P15-1128,0,0.0476364,"s (Jia and Liang, 2016; Wang et al., 2015). In this work, we focus on semi-supervised learning, aiming to learn from both limited Introduction Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017), or generation of domainspecific (e.g., SQL) or general purpose programming languages (e.g., Python) (Quirk et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017). 1 z Code available at http://pcyin.me/struct vae 754 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 754–765 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2.1 amounts of parallel NL-MR corpora, and unlabeled but readily-available NL utterances. We draw inspiration from recent success in applying varia"
P18-1070,P17-1105,0,0.516609,"pping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017), or generation of domainspecific (e.g., SQL) or general purpose programming languages (e.g., Python) (Quirk et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017). 1 z Code available at http://pcyin.me/struct vae 754 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 754–765 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2.1 amounts of parallel NL-MR corpora, and unlabeled but readily-available NL utterances. We draw inspiration from recent success in applying variational auto-encoding (VAE) models in semisupervised sequence-to-sequence learning (Miao and Blunsom, 2016; Kocisk´y et al., 2016), and propose S TRUCT VAE — a principled deep generative appr"
P18-1070,W16-0105,1,0.803244,"worse than the supervised parser. On the other hand, our LM baseline correlates well with the learning signal, yielding stable improvements over the supervised parser. This suggests the importance of using carefully designed baselines in REINFORCE learning, especially when the reward signal has large range (e.g., log-likelihoods). Semantic Parsing Most existing works alleviate issues of limited parallel data through weaklysupervised learning, using the denotations of MRs as indirect supervision (Reddy et al., 2014; Krishnamurthy et al., 2016; Neelakantan et al., 2016; Pasupat and Liang, 2015; Yin et al., 2016). For semi-supervised learning of semantic parsing, Kate and Mooney (2007) first explore using transductive SVMs to learn from a semantic parser’s predictions. Konstas et al. (2017) apply self-training to bootstrap an existing parser for AMR parsing. Kocisk´y et al. (2016) employ VAEs for semantic parsing, but in contrast to S TRUCT VAE’s structured representation of MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised semantic parsing, which exploits external linguistic analysis of utterances (e.g., dependency t"
P18-1070,Q14-1030,0,0.0321228,"s better performance on ATIS, in most settings it is worse than our LM baseline, and could be even worse than the supervised parser. On the other hand, our LM baseline correlates well with the learning signal, yielding stable improvements over the supervised parser. This suggests the importance of using carefully designed baselines in REINFORCE learning, especially when the reward signal has large range (e.g., log-likelihoods). Semantic Parsing Most existing works alleviate issues of limited parallel data through weaklysupervised learning, using the denotations of MRs as indirect supervision (Reddy et al., 2014; Krishnamurthy et al., 2016; Neelakantan et al., 2016; Pasupat and Liang, 2015; Yin et al., 2016). For semi-supervised learning of semantic parsing, Kate and Mooney (2007) first explore using transductive SVMs to learn from a semantic parser’s predictions. Konstas et al. (2017) apply self-training to bootstrap an existing parser for AMR parsing. Kocisk´y et al. (2016) employ VAEs for semantic parsing, but in contrast to S TRUCT VAE’s structured representation of MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervis"
P18-1070,P17-1041,1,0.911166,"tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs). This includes parsing to general-purpose logical forms such as λ-calculus (Zettlemoyer and Collins, 2005, 2007) and the abstract meaning representation (AMR, Banarescu et al. (2013); Misra and Artzi (2016)), as well as parsing to computerexecutable programs to solve problems such as question answering (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017), or generation of domainspecific (e.g., SQL) or general purpose programming languages (e.g., Python) (Quirk et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017). 1 z Code available at http://pcyin.me/struct vae 754 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 754–765 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2.1 amounts of parallel NL-MR corpora, and unlabeled but readily-available NL utterances. We draw inspiration from recent success in applying variational auto-encoding (VAE) models in semisupervised sequence-to-sequence learning (Miao and Blunsom, 2016; Kocisk´y et al., 2016), and propose S TRUCT VAE — a princ"
P18-1070,D11-1014,0,0.0117307,"ared with the results in Tab. 1, we found that the gains are especially larger with few labeled examples — S TRUCTVAE-S EQ achieves improvements of 8-10 points when |L |&lt; 1000. These results suggest that semi-supervision is especially useful in improving a mediocre parser in low resource settings. 761 Accuracy 0.66 5 0.64 Semi-supervised Learning for NLP Semisupervised learning comes with a long history (Zhu, 2005), with applications in NLP from early work of self-training (Yarowsky, 1995), and graph-based methods (Das and Smith, 2011), to recent advances in auto-encoders (Cheng et al., 2016; Socher et al., 2011; Zhang et al., 2017) and deep generative methods (Xu et al., 2017). Our work follows the line of neural variational inference for text processing (Miao et al., 2016), and resembles Miao and Blunsom (2016), which uses VAEs to model summaries as discrete latent variables for semi-supervised summarization, while we extend the VAE architecture for more complex, tree-structured latent variables. StructVAE Sup. 0.62 0.0 0.2 0.4 0.6 0.8 1.0 λ Figure 6: Performance on D JANGO (|L |= 5000) w.r.t. the KL weight λ Accuracy 0.655 0.650 StructVAE 0.645 1000 5000 8000 12000 14000 Related Works 16000 Size o"
P18-1070,D17-1127,0,0.0778566,"VAE’s structured representation of MRs, they model NL utterances as flat latent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised semantic parsing, which exploits external linguistic analysis of utterances (e.g., dependency trees) and the schema of target knowledge bases to infer the latent MRs (Poon and Domingos, 2009; Poon, 2013). Another line of research is domain adaptation, which seeks to transfer a semantic parser learned from a source domain to the target domain of interest, therefore alleviating the need of parallel data from the target domain (Su and Yan, 2017; Fan et al., 2017; Herzig and Berant, 2018). Impact of the Prior p(z) Fig. 6 depicts the performance of S TRUCT VAE as a function of the KL term weight λ in Eq. (3). When S TRUCT VAE degenerates to a vanilla auto-encoder without the prior distribution (i.e., λ = 0), it under-performs the supervised baseline. This is in line with our observation in Tab. 3 showing that the prior helps identify unnatural samples. The performance of the model also drops when λ &gt; 0.1, suggesting that empirically controlling the influence of the prior to the inference model is important. 6 Conclusion We propose S T"
P18-1070,D07-1071,0,0.441471,"Missing"
P18-1070,D17-1179,0,0.0660693,"in Tab. 1, we found that the gains are especially larger with few labeled examples — S TRUCTVAE-S EQ achieves improvements of 8-10 points when |L |&lt; 1000. These results suggest that semi-supervision is especially useful in improving a mediocre parser in low resource settings. 761 Accuracy 0.66 5 0.64 Semi-supervised Learning for NLP Semisupervised learning comes with a long history (Zhu, 2005), with applications in NLP from early work of self-training (Yarowsky, 1995), and graph-based methods (Das and Smith, 2011), to recent advances in auto-encoders (Cheng et al., 2016; Socher et al., 2011; Zhang et al., 2017) and deep generative methods (Xu et al., 2017). Our work follows the line of neural variational inference for text processing (Miao et al., 2016), and resembles Miao and Blunsom (2016), which uses VAEs to model summaries as discrete latent variables for semi-supervised summarization, while we extend the VAE architecture for more complex, tree-structured latent variables. StructVAE Sup. 0.62 0.0 0.2 0.4 0.6 0.8 1.0 λ Figure 6: Performance on D JANGO (|L |= 5000) w.r.t. the KL weight λ Accuracy 0.655 0.650 StructVAE 0.645 1000 5000 8000 12000 14000 Related Works 16000 Size of Unlabeled Data Figu"
P18-1070,D14-1135,0,0.326947,"Missing"
P18-1070,P17-1029,1,0.802515,"parser) using parallel corpora, and unsupervised learning by maximizing the variational lower bound of the likelihood of the unlabeled utterances (§ 3.3). In addition to these contributions to semisupervised semantic parsing, S TRUCT VAE contributes to generative model research as a whole, providing a recipe for training VAEs with structured latent variables. Such a structural latent space is contrast to existing VAE research using flat representations, such as continuous distributed representations (Kingma and Welling, 2013), discrete symbols (Miao and Blunsom, 2016), or hybrids of the two (Zhou and Neubig, 2017). We apply S TRUCT VAE to semantic parsing on the ATIS domain and Python code generation. As an auxiliary contribution, we implement a transition-based semantic parser, which uses Abstract Syntax Trees (ASTs, § 3.2) as intermediate MRs and achieves strong results on the two tasks. We then apply this parser as the inference model for semi-supervised learning, and show that with extra unlabeled data, S TRUCT VAE outperforms its supervised counterpart. We also demonstrate that S TRUCT VAE is compatible with different structured latent representations, applying it to a simple sequence-to-sequence"
P18-1130,P16-1231,0,0.345275,"rations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computati"
P18-1130,P15-1034,0,0.0202817,"ance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-t"
P18-1130,D15-1041,0,0.0343909,"eft-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting"
P18-1130,D16-1211,0,0.184897,"Missing"
P18-1130,D17-1209,0,0.0187661,"nt dependency annotation schemas, and achieve state-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-base"
P18-1130,D12-1133,0,0.0721718,"Missing"
P18-1130,W06-2920,0,0.707126,"in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the"
P18-1130,D14-1082,0,0.255712,"2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performan"
P18-1130,D16-1238,0,0.624302,"ith previous top-performing systems for comparison. Note that the results of S TACK P TR and our reimplementation of B I AF are the average of 5 repetitions instead of a single run. Our Full model significantly outperforms all the transition-based parsers on all three languages, and achieves better results than most graph-based parsers. Our 1408 System Chen and Manning (2014) Ballesteros et al. (2015) Dyer et al. (2015) Bohnet and Nivre (2012) Ballesteros et al. (2016) Kiperwasser and Goldberg (2016) Weiss et al. (2015) Andor et al. (2016) Kiperwasser and Goldberg (2016) Wang and Chang (2016) Cheng et al. (2016) Kuncoro et al. (2016) Ma and Hovy (2017) B I AF: Dozat and Manning (2017) B I AF: re-impl S TACK P TR: Org S TACK P TR: +gpar S TACK P TR: +sib S TACK P TR: Full T T T T T T T T G G G G G G G T T T T English UAS LAS 91.8 89.6 91.63 89.44 93.1 90.9 93.33 91.22 93.56 91.42 93.9 91.9 94.26 92.41 94.61 92.79 93.1 91.0 94.08 91.82 94.10 91.49 94.26 92.06 94.88 92.98 95.74 94.08 95.84 94.21 95.77 94.12 95.78 94.12 95.85 94.18 95.87 94.19 Chinese UAS LAS 83.9 82.4 85.30 83.72 87.2 85.7 87.3 85.9 87.65 86.21 87.6 86.1 – – – – 86.6 85.1 87.55 86.23 88.1 85.7 88.87 87.30 89.05 87.74 89.30 88.23 90.43 8"
P18-1130,Q16-1026,0,0.0270272,"to be introduced. The predefined order of children can have different alternatives, such as leftto-right or inside-out2 . In this paper, we adopt the inside-out order3 since it enables us to utilize second-order sibling information, which has been proven beneficial for parsing performance (McDonald and Pereira, 2006; Koo and Collins, 2010) (see § 3.4 for details). Figure 1 (b) depicts the architecture of S TACK P TR and the decoding procedure for the example sentence in Figure 1 (a). 3.2 Encoder The encoder of our parsing model is based on the bi-directional LSTM-CNN architecture (BLSTMCNNs) (Chiu and Nichols, 2016; Ma and Hovy, 2016) where CNNs encode character-level information of a word into its character-level repre2 Order the children by the distances to the head word on the left side, then the right side. 3 We also tried left-to-right order which obtained worse parsing accuracy than inside-out. 1405 sentation and BLSTM models context information of each word. Formally, for each word, the CNN, with character embeddings as inputs, encodes the character-level representation. Then the character-level representation vector is concate012nated 3456278 2965the 69 86 2embedding 5214 2523775 4395tofe"
P18-1130,P15-1033,0,0.0865232,"ly (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings o"
P18-1130,C96-1058,0,0.785418,"timent analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies"
P18-1130,W15-0802,1,0.779907,"s the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of mu"
P18-1130,Q16-1023,0,0.260785,"graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based arc"
P18-1130,P10-1001,0,0.638136,"ine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have"
P18-1130,D10-1125,0,0.0229124,"92±0.16] 93.57±0.12 [90.07±0.20] 87.59±0.36 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (201"
P18-1130,D16-1180,0,0.694907,"based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the corresponding asymptotic"
P18-1130,P81-1022,0,0.781098,"Missing"
P18-1130,P14-1130,0,0.0161117,"26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precisi"
P18-1130,D13-1203,0,0.0244038,"d parsers, yielding an efficient decoding algorithm with O(n2 ) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen"
P18-1130,N15-1142,0,0.0305836,"ned by certain head words. Our parser follows a similar kind of annotation process: starting from reading the whole sentence, and processing in a top-down manner by finding the main predicates first and only then search for sub-trees governed by them. When making latter decisions, the parser has access to the entire structure built in earlier steps. 3.8 Implementation Details Pre-trained Word Embeddings. For all the parsing models in different languages, we initialize word vectors with pretrained word embeddings. For Chinese, Dutch, English, German and Spanish, we use the structured-skipgram (Ling et al., 2015) embeddings. For other languages we use Polyglot embeddings (Al-Rfou et al., 2013). Optimization. Parameter optimization is performed with the Adam optimizer (Kingma and Ba, 2014) with β1 = β2 = 0.9. We choose an initial learning rate of η0 = 0.001. The learning rate η is annealed by multiplying a fixed decay rate ρ = 0.75 when parsing performance stops increasing on validation sets. To reduce the effects of “gradient exploding”, we use gradient clipping of 5.0 (Pascanu et al., 2013). Dropout Training. To mitigate overfitting, we apply dropout (Srivastava et al., 2014; Ma et al., 2017). For BL"
P18-1130,D15-1166,0,0.0166961,"e stack σ. Children: ch(wi ) denotes the list of all the children (modifiers) of word wi . 2.2 Pointer Networks Pointer Networks (P TR -N ET) (Vinyals et al., 2015) are a variety of neural network capable of learning the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model cannot be trivially expressed by standard sequence-to-sequence networks (Sutskever et al., 2014) due to the variable number of input positions in each sentence. P TR -N ET solves the problem by using attention (Bahdanau et al., 2015; Luong et al., 2015) as a pointer to select a member of the input sequence as the output. Formally, the words of the sentence x are fed one-by-one into the encoder (a multiple-layer bidirectional RNN), producing a sequence of encoder hidden states si . At each time step t, the decoder (a uni-directional RNN) receives the input from last step and outputs decoder hidden state ht . The attention vector at is calculated as follows: eti = score(ht , si ) at = softmax (et ) (1) where score(·, ·) is the attention scoring function, which has several variations such as dot-product, 1404 2 2 3 3 4 s1 s2 s3 s4 s5 s6 h1 $ Bu"
P18-1130,D15-1154,1,0.801978,"us et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). We use the same experimental settings as Kuncoro et al. (2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency (UD) Treebanks4 . For the CoNLL Treebanks, we use the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). The experimental settings are the same as Ma and Hovy (2015). For UD Treebanks, we select 12 languages. The details of the treebanks and experimental settings are in § 4.5 and Appendix B. Evaluation Metrics Parsing performance is measured with five metrics: unlabeled attachment score (UAS), labeled attachment score (LAS), unlabeled complete match (UCM), labeled complete match (LCM), and root accuracy (RA). Following previous work (Kuncoro et al., 2016; Dozat and Manning, 2017), we report results excluding punctuations for Chinese and English. For each experiment, we report the mean values with corresponding standard deviations over 5 repetitions. 1407"
P18-1130,P16-1101,1,0.832214,"edefined order of children can have different alternatives, such as leftto-right or inside-out2 . In this paper, we adopt the inside-out order3 since it enables us to utilize second-order sibling information, which has been proven beneficial for parsing performance (McDonald and Pereira, 2006; Koo and Collins, 2010) (see § 3.4 for details). Figure 1 (b) depicts the architecture of S TACK P TR and the decoding procedure for the example sentence in Figure 1 (a). 3.2 Encoder The encoder of our parsing model is based on the bi-directional LSTM-CNN architecture (BLSTMCNNs) (Chiu and Nichols, 2016; Ma and Hovy, 2016) where CNNs encode character-level information of a word into its character-level repre2 Order the children by the distances to the head word on the left side, then the right side. 3 We also tried left-to-right order which obtained worse parsing accuracy than inside-out. 1405 sentation and BLSTM models context information of each word. Formally, for each word, the CNN, with character embeddings as inputs, encodes the character-level representation. Then the character-level representation vector is concate012nated 3456278 2965the 69 86 2embedding 5214 2523775 4395tofeed into with word ve"
P18-1130,I17-1007,1,0.717476,"0.21] 90.10±0.27 [87.05±0.26] 93.25±0.05 [93.17±0.05] 94.77±0.05 [93.21±0.10] 93.38±0.08 [91.92±0.16] 93.57±0.12 [90.07±0.20] 87.59±0.36 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arc"
P18-1130,N16-1116,1,0.871211,"Missing"
P18-1130,P14-1126,1,0.849815,"tep towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of op"
P18-1130,C12-2077,1,0.903676,"l Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the corresponding asymptotic efficiency, but still maintains a global view of the sentence that proves essential for achieving competitive accuracy. Our S TACK P TR parser has a pointer network (Vinyals et al., 2015) as its backbone, and is equipped with an internal stack to maintain the order of head words in tree structures. The S TACK P TR parser performs parsing in an incremental, topdown, depth-first fashion;"
P18-1130,J93-2004,0,0.0662464,"idden states and 0.33 between layers. Following Dozat and Manning (2017), we also use embedding dropout with a rate of 0.33 on all word, character, and POS embeddings. Hyper-Parameters. Some parameters are chosen from those reported in Dozat and Manning (2017). We use the same hyper-parameters across the models on different treebanks and languages, due to time constraints. The details of the chosen hyper-parameters for all experiments are summarized in Appendix A. 4 Experiments 4.1 Setup We evaluate our S TACK P TR parser mainly on three treebanks: the English Penn Treebank (PTB version 3.0) (Marcus et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). We use the same experimental settings as Kuncoro et al. (2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency (UD) Treebanks4 . For the CoNLL Treebanks, we use the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). The experimental settings are the same as Ma and Hovy (201"
P18-1130,P13-2109,0,0.0287562,"6 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advan"
P18-1130,J11-1007,0,0.635811,"McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive s"
P18-1130,E06-1011,0,0.718934,"ly 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the corresponding asymptotic efficiency, but still maintains a global view of the sentence that proves essential for achieving competitive accuracy. Our S TACK P TR parser has a pointer network (Vinyals et al., 2015) as its backbone, and is equipped with an internal stack to maintain the order of head words in tree structures. The S TACK P TR parser performs parsing in an"
P18-1130,H05-1066,0,0.819927,"s (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 20"
P18-1130,P10-1142,0,0.0278987,"ition-based parsers, yielding an efficient decoding algorithm with O(n2 ) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zh"
P18-1130,D09-1143,0,0.011773,"ate-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially"
P18-1130,C04-1010,0,0.0584363,"rence resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information,"
P18-1130,Q17-1008,1,0.818078,"Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build d"
P18-1130,D11-1022,0,0.029926,"[90.07±0.20] 87.59±0.36 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does no"
P18-1130,petrov-etal-2012-universal,0,0.10217,"Missing"
P18-1130,P05-1012,0,0.857049,"s (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 20"
P18-1130,N15-1068,0,0.019183,"t Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precision for arcs further away from the root. Furthermore, the S TACK P TR parser"
P18-1130,W08-2121,0,0.156465,"Missing"
P18-1130,P15-1150,0,0.0877003,"Missing"
P18-1130,P16-1218,0,0.102311,"Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the co"
P18-1130,P15-1032,0,0.190239,"Missing"
P18-1130,C02-1145,0,0.226098,"2017), we also use embedding dropout with a rate of 0.33 on all word, character, and POS embeddings. Hyper-Parameters. Some parameters are chosen from those reported in Dozat and Manning (2017). We use the same hyper-parameters across the models on different treebanks and languages, due to time constraints. The details of the chosen hyper-parameters for all experiments are summarized in Appendix A. 4 Experiments 4.1 Setup We evaluate our S TACK P TR parser mainly on three treebanks: the English Penn Treebank (PTB version 3.0) (Marcus et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). We use the same experimental settings as Kuncoro et al. (2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency (UD) Treebanks4 . For the CoNLL Treebanks, we use the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). The experimental settings are the same as Ma and Hovy (2015). For UD Treebanks, we select 12 languages. The details of the"
P18-1130,W03-3023,0,0.337352,"applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is base"
P18-1130,P14-2107,0,0.015528,"79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precision for arcs further away from the root. Furtherm"
P18-1130,D14-1109,0,0.0152966,"49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precision for arcs further a"
P18-1130,P11-2033,0,0.0696926,"10; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propag"
P18-1130,W13-3520,0,\N,Missing
P18-1154,D16-1032,0,0.195865,"Missing"
P18-1154,D16-1128,0,0.139786,"Missing"
P18-1154,P09-1011,0,0.0572073,"we have introduced a new largescale data for game commentary generation. The commentaries cover a variety of aspects like move description, quality of move, and alternative moves. This leads to a content selection challenge, similar to that noted in Wiseman et al. (2017). Unlike Wiseman et al. (2017), our focus is on generating commentary for individual moves in a game, as opposed to game summaries from aggregate statistics as in their task. One of the first NLG datasets was the SUMTIME-METEO (Reiter et al., 2005) corpus with ≈ 500 record-text pairs for technical weather forecast generation. Liang et al (2009) worked on common weather forecast generation using the WEATHERGOV dataset, which has ≈ 10K record-text pairs. A criticism of WEATHERGOV dataset (Reiter, 2017) is that weather records themselves may have used templates and rules with optional human post-editing. There have been prior works on generating commentary for ROBOCUP matches (Chen and Mooney, 2008; Mei et al., 2015). The ROBOCUP dataset, however, is collected from 4 games and contains about 1K events in total. Our dataset is two orders of magnitude larger than the ROBOCUP dataset, and we hope that it provides a promising setting for f"
P18-1154,O90-1011,0,0.648433,"blocked by that move. Both descriptions are true, but the latter is most salient given the player’s goal. However, sometimes, none of the aspects may stand out as being most salient, and the most salient aspect may even change from commentator to commentator. Moreover, a human commentator may introduce variations in the aspects he or she chooses to talk about, in order to reduce monotony in the commentary. This makes the dataset a useful testbed not only for NLG but also for related work on modeling pragmatics in language (Liu et al., 2016). Prior work has explored game commentary generation. Liao and Chang (1990); Sadikov et al. (2006) have explored chess commentary generation, but for lack of large-scale training data their methods have been mainly rule-based. Kameko et al. (2015) have explored commentary generation for the game of Shogi, proposing a twostep process where salient terms are generated from the game state and then composed in a language model. In contrast, given the larger amount of training data available to us, our proposed model uses an end-to-end trainable neural architecture to predict commentaries given the game state. Our model conditions on semantic and pragmatic information abo"
P18-1154,D16-1230,0,0.0252276,"imply that the pawn was moved, or one may comment on how the check was blocked by that move. Both descriptions are true, but the latter is most salient given the player’s goal. However, sometimes, none of the aspects may stand out as being most salient, and the most salient aspect may even change from commentator to commentator. Moreover, a human commentator may introduce variations in the aspects he or she chooses to talk about, in order to reduce monotony in the commentary. This makes the dataset a useful testbed not only for NLG but also for related work on modeling pragmatics in language (Liu et al., 2016). Prior work has explored game commentary generation. Liao and Chang (1990); Sadikov et al. (2006) have explored chess commentary generation, but for lack of large-scale training data their methods have been mainly rule-based. Kameko et al. (2015) have explored commentary generation for the game of Shogi, proposing a twostep process where salient terms are generated from the game state and then composed in a language model. In contrast, given the larger amount of training data available to us, our proposed model uses an end-to-end trainable neural architecture to predict commentaries given the"
P18-1154,P02-1040,0,0.102226,"ross-entropy loss over the decoding outputs to train the model. 4 Experiments We split each of the data subsets in a 70:10:20 ratio into train, validation and test. All our models are implemented in Pytorch version 0.3.1 (Paszke et al., 2017). We use the ADAM optimizer (Kingma and Ba, 2014) with its default parameters and a mini-batch size of 32. Validation set perplexity is used for early-stopping. At test-time, we use greedy search to generate the model output. We observed that beam decoding does not lead to any significant improvement in terms of validation BLEU score. We observe the BLEU (Papineni et al., 2002) and BLEU-2 (Vedantam et al., 2015) scores to measure the performance of the models. Addi1665 Dataset Features TEMP NN (M+T+S) RAW MoveDesc GAC-sparse GAC (M+T) TEMP NN (M+T) RAW Quality GAC-sparse GAC(M+T+S) NN (M) RAW Comparative GAC-sparse GAC(M+T) tionally, we consider a measure to quantify the diversity in the generated outputs. Finally, we also conduct a human evaluation study. In the remainder of this section, we discuss baselines along with various experiments and results. 4.1 Baselines In this subsection we discuss the various baseline methods. Manually-defined template (TEMP) We devi"
P18-1154,J09-4008,0,0.159794,"Missing"
P18-1154,W12-1516,0,0.0199831,"and previous board (Ri ) information from the game. P (Si |Mi , Gi ) = P (Si |Mi , Ci , Ri ). We model this using an end-to-end trainable neural model, which models conjunctions of features using feature encoders. Our model employs a selection mechanism to select the salient features for a given chess move. Finally a LSTM recurrent neural network (Hochreiter and Schmidhuber, 1997) is used to generate the commentary text based on selected features from encoder. 3.1 Incorporating Domain Knowledge Past work shows that acquiring domain knowledge is critical for NLG systems (Reiter et al., 2003b; Mahamood and Reiter, 2012). Commentary texts cover a range of perspectives, including criticism or goodness of current move, possible alternate moves, quality of alternate moves, etc. To be able to make such comments, the model must learn about the quality of moves, as well as the set of valid moves for a given chess board state. We consider the following features to provide our model with necessary information to generate commentary texts (Figure 3): Move features fmove (Mi , Ci , Ri ) encode the current move information such as which piece moved, the position of the moved piece before and after the move was made, the"
P18-1247,P16-1184,0,0.152814,"e stable training. We considered BP to have reached convergence when the maximum residual error was below 0.05 or if the maximum number of iterations was reached (set to 40 in our experiments). We found that in crosslingual experiments, when tgt size = 100, the relatively large amount of data in the HRL was causing our model to overfit on the HRL and not generalize well to the LRL. As a solution to this, we upsampled the LRL data by a factor of 10 when tgt size = 100 for both the baseline and the proposed model. Evaluation: Previous work on morphological analysis (Cotterell and Heigold, 2017; Buys and Botha, 2016) has reported scores on average token-level accuracy and F1 measure. The average token level accuracy counts a tag set prediction as correct only it is an exact match with the gold tag set. On the other hand, F1 measure is measured on a tag-by-tag basis, which allows it to give partial credit to partially correct tag sets. Based on the characteristics of each evaluation measure, Accuracy will favor tag-set prediction models (like the baseline), and F1 measure will favor tag-wise prediction models (like our proposed method). Given the nature of the task, it seems reasonable to prefer getting so"
P18-1247,D17-1078,0,0.169973,"ylomova et al., 2017; Tsarfaty et al., 2010) and parsing (Tsarfaty et al., 2013), and errors in the upstream analysis may cascade to the downstream tasks. One difficulty, however, in creating these taggers is that only a limited amount of annotated data is available for a majority of the world’s languages to learn these morphological taggers. Fortunately, recent efforts in morphological annotation follow a standard annotation schema for these morphological tags across languages, and now the Universal Dependencies Treebank (Nivre et al., 2017) has tags according to this schema in 60 languages. Cotterell and Heigold (2017) have recently shown that combining this shared schema with cross-lingual training on a related high-resource language (HRL) gives improved performance 2653 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2653–2663 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tractable through belief propagation over the possible tag combinations, allowing the model to consider an exponential label space in polynomial time (§3.5). This model has several advantages: • The model is able to generate tag sets"
P18-1247,K17-2001,0,0.234214,"Tsarfaty et al., 2010) and parsing (Tsarfaty et al., 2013), and errors in the upstream analysis may cascade to the downstream tasks. One difficulty, however, in creating these taggers is that only a limited amount of annotated data is available for a majority of the world’s languages to learn these morphological taggers. Fortunately, recent efforts in morphological annotation follow a standard annotation schema for these morphological tags across languages, and now the Universal Dependencies Treebank (Nivre et al., 2017) has tags according to this schema in 60 languages. Cotterell and Heigold (2017) have recently shown that combining this shared schema with cross-lingual training on a related high-resource language (HRL) gives improved performance 2653 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2653–2663 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tractable through belief propagation over the possible tag combinations, allowing the model to consider an exponential label space in polynomial time (§3.5). This model has several advantages: • The model is able to generate tag sets"
P18-1247,D16-1256,0,0.0683105,"Missing"
P18-1247,D17-1074,0,0.0412084,"Missing"
P18-1247,N13-1138,0,0.0335973,"have tense. Figure 6: Language-specific pairwise weights for RU between Gender and Tense from the RU /B G model • Language Specific Trends: We visualized the learnt language-specific weights and looked for evidence of patterns corresponding to linguistic phenomenas observed in a language of interest. For instance, in Russian, verbs are gender-specific in past tense but not in other tenses. To analyze this, we plotted pairwise weights for Gender/Tense in Related Work There exist several variations of the task of prediction of morphological information from annotated data: paradigm completion (Durrett and DeNero, 2013; Cotterell et al., 2017b), morphological reinflection (Cotterell et al., 2017a), segmentation (Creutz et al., 2005; Cotterell et al., 2016) and tagging. Work on morphological tagging has broadly focused on structured prediction models such as CRFs, and neural network models. Amongst structured prediction approaches, M¨uller et al. (2013); M¨uller and Sch¨utze (2015) proposed the use of a higher-order CRF that is approximated using coarse-to-fine decoding. (M¨uller et al., 2015) proposed joint lemmatization and tagging using this framework. (Hajiˇc, 2000) was the first work that performed expe"
P18-1247,W96-0214,0,0.433688,"(yα(i) ) − X bα (yα )fg,k (yα ) yα α∈Cg where Cg denotes all the factors of type g, and we have omitted any dependence on x(i) and t for brevity—t is accessible through the factor index α. For the neural network factors, the features are given by a biLSTM. We backpropagate through to the biLSTM parameters using the partial derivative below, ∂`(i) (i) ∂fN N,k (yt,m , t) = λN N,k − X bt,m (yt,m )λN N,k yt,m where bt,m (·) is the variable belief corresponding to variable yt,m . ˆ at test time, To predict a sequence of tag sets y we use minimum Bayes risk (MBR) decoding (Bickel and Doksum, 1977; Goodman, 1996) for Hamming loss over tags. For a variable yt,m representing tag m at timestep t, we take yˆt,m = arg max bt,m (l). (13) Table 1: Dataset sizes. tgt size = 100 or 1,000 LRL sentences are added to HRL Train Language Pair DA / SV RU / BG FI / HU ES / PT Unique Tags 23 19 27 19 Tag Sets 224 798 2195 451 Table 2: Tag Set Sizes with tgt size=100 4 4.1 Experimental Setup Dataset We used the Universal Dependencies Treebank UD v2.1 (Nivre et al., 2017) for our experiments. We picked four low-resource/high-resource language pairs, each from a different family: Danish/Swedish (DA / SV), Russian/Bulgari"
P18-1247,A00-2013,0,0.509536,"Missing"
P18-1247,P98-1080,0,0.555751,"Missing"
P18-1247,E17-1048,0,0.448174,"Missing"
P18-1247,A94-1024,0,0.540882,"Missing"
P18-1247,E17-2018,0,0.220046,"ging. Work on morphological tagging has broadly focused on structured prediction models such as CRFs, and neural network models. Amongst structured prediction approaches, M¨uller et al. (2013); M¨uller and Sch¨utze (2015) proposed the use of a higher-order CRF that is approximated using coarse-to-fine decoding. (M¨uller et al., 2015) proposed joint lemmatization and tagging using this framework. (Hajiˇc, 2000) was the first work that performed experiments on multilingual morphological tagging. They proposed an exponential model and the use of a morphological dictionary. Buys and Botha (2016); Kirov et al. (2017) proposed a model that used tag projection of type and token constraints from a resource-rich language to a low-resource language for tagging. Most recent work has focused on characterbased neural models (Heigold et al., 2017), that can handle rare words and are hence more useful to model morphology than word-based models. These models first obtain a character-level representation of a token from a biLSTM or CNN, which is provided to a word-level biLSTM tagger. Heigold et al. (2017, 2016) compared several neural architectures to obtain these character-based representations and found the effect"
P18-1247,P16-1101,0,0.0334857,"ted with different strategies to facilitate cross-lingual training: a language ID for each token, a language-specific softmax and a joint language identification and tagging model. We have used this work as a baseline model for comparing with our proposed method. In contrast to earlier work on morphological tagging, we use a hybrid of neural and graphical 2660 model approaches. This combination has several advantages: we can make use of expressive feature representations from neural models while ensuring that our model is interpretable. Our work is similar in spirit to Huang et al. (2015) and Ma and Hovy (2016), who proposed models that use a CRF with features from neural models. For our graphical model component, we used a factorial CRF (Sutton et al., 2007), which is a generalization of a linear chain CRF with additional pairwise factors between cotemporal variables. 7 Conclusion and Future Work In this work, we proposed a novel framework for sequence tagging that combines neural networks and graphical models, and showed its effectiveness on the task of morphological tagging. We believe this framework can be extended to other sequence labeling tasks in NLP such as semantic role labeling. Due to th"
P18-1247,D15-1272,0,0.238028,"Missing"
P18-1247,P16-2067,0,0.0321647,"e words and are hence more useful to model morphology than word-based models. These models first obtain a character-level representation of a token from a biLSTM or CNN, which is provided to a word-level biLSTM tagger. Heigold et al. (2017, 2016) compared several neural architectures to obtain these character-based representations and found the effect of the neural network architecture to be minimal given the networks are carefully tuned. Cross-lingual transfer learning has previously boosted performance on tasks such as translation (Johnson et al., 2016) and POS tagging (Snyder et al., 2008; Plank et al., 2016). Cotterell and Heigold (2017) proposed a cross-lingual character-level neural morphological tagger. They experimented with different strategies to facilitate cross-lingual training: a language ID for each token, a language-specific softmax and a joint language identification and tagging model. We have used this work as a baseline model for comparing with our proposed method. In contrast to earlier work on morphological tagging, we use a hybrid of neural and graphical 2660 model approaches. This combination has several advantages: we can make use of expressive feature representations from neur"
P18-1247,D08-1109,0,0.0419717,", that can handle rare words and are hence more useful to model morphology than word-based models. These models first obtain a character-level representation of a token from a biLSTM or CNN, which is provided to a word-level biLSTM tagger. Heigold et al. (2017, 2016) compared several neural architectures to obtain these character-based representations and found the effect of the neural network architecture to be minimal given the networks are carefully tuned. Cross-lingual transfer learning has previously boosted performance on tasks such as translation (Johnson et al., 2016) and POS tagging (Snyder et al., 2008; Plank et al., 2016). Cotterell and Heigold (2017) proposed a cross-lingual character-level neural morphological tagger. They experimented with different strategies to facilitate cross-lingual training: a language ID for each token, a language-specific softmax and a joint language identification and tagging model. We have used this work as a baseline model for comparing with our proposed method. In contrast to earlier work on morphological tagging, we use a hybrid of neural and graphical 2660 model approaches. This combination has several advantages: we can make use of expressive feature repr"
P18-1247,D13-1032,0,0.293411,"Missing"
P18-1247,N15-1055,0,0.117406,"Missing"
P18-1247,W10-1401,0,0.0750122,"Missing"
P18-1247,J13-1003,0,0.0290302,"Missing"
P18-1247,W17-4115,0,0.0251076,"dk´a (1998), Oflazer and Kuru¨oz (1994), inter alia) is the task of predicting fine-grained annotations about the syntactic properties of tokens in a language such 1 Our code and data is publicly available www.github.com/chaitanyamalaviya/ NeuralFactorGraph. at as part-of-speech, case, or tense. For instance, in Figure 1, the given Portuguese sentence is labeled with the respective morphological tags such as Gender and its label value Masculine. The accuracy of morphological analyzers is paramount, because their results are often a first step in the NLP pipeline for tasks such as translation (Vylomova et al., 2017; Tsarfaty et al., 2010) and parsing (Tsarfaty et al., 2013), and errors in the upstream analysis may cascade to the downstream tasks. One difficulty, however, in creating these taggers is that only a limited amount of annotated data is available for a majority of the world’s languages to learn these morphological taggers. Fortunately, recent efforts in morphological annotation follow a standard annotation schema for these morphological tags across languages, and now the Universal Dependencies Treebank (Nivre et al., 2017) has tags according to this schema in 60 languages. Cotterell and Heigol"
P18-1247,P11-1089,0,\N,Missing
P18-2050,W17-3205,0,0.0442501,"Missing"
P18-2050,L18-1146,0,0.223448,"Missing"
P18-2050,2015.iwslt-evaluation.11,0,0.730444,"kers. The usual objective of NMT is to find parameters θ of the conditional distribution p(y |x; θ) to maximize the empirical likelihood. We argue that personal variations in language warrant decomposing the empirical distribution into |S |speaker specific domains Ds and learning a different set of parameters θs for each. This setting exhibits specific traits that set it apart from common domain adaptation settings: parametrization θs to improve translation for speaker s. The usual way of adapting from general domain parameters θ to θs is to retrain the full model on the domain specific data (Luong and Manning, 2015). Naively applying this approach in the context of personalizing a model for each speaker however has two main drawbacks: Parameter cost Maintaining a set of model parameters for each speaker is expensive. For example, the model in §2.1 has ≈47M parameters when the vocabulary size is 40k, as is the case in our experiments in §5. Assuming each parameter is stored as a 32bit float, every speaker-specific model costs ≈188MB. In a production environment with thousands to billions of speakers, this is impractical. 1. The number of speakers is very large. Our particular setting deals with |S |≈ 1800"
P18-2050,P17-2061,0,0.249857,"e data (even monolingual, let alone bilingual or parallel) for each speaker, compared to millions of sentences usually used in NMT. Overfitting Training each speaker model with very little data is a challenge, necessitating careful and heavy regularization (Miceli Barone et al., 2017) and an early stopping procedure. 3. As a consequence of 1, we can assume that many speakers share similar characteristics such as gender, social status, and as such may have similar associated domains.2 2.1 2.3 A more efficient domain adaptation technique is the domain token idea used in Sennrich et al. (2016a); Chu et al. (2017): introduce an additional token marking the domain in the source and/or the target sentence. In experiments, we add a token indicating the speaker at the start of the target sentence for each speaker. We refer to this method as the spk token method in the following. Note that in this case there is now only an embedding vector (of dimension 512 in our experiments) for each speaker. However, the resulting domain embedding are non-trivial to interpret (i.e. it is not clear what they tell us about the domain or speaker itself). Baseline NMT model All of our experiments are based on a standard neur"
P18-2050,D17-1156,0,0.0494208,"bit float, every speaker-specific model costs ≈188MB. In a production environment with thousands to billions of speakers, this is impractical. 1. The number of speakers is very large. Our particular setting deals with |S |≈ 1800 but our approaches should be able to accommodate orders of magnitude more speakers. 2. There is very little data (even monolingual, let alone bilingual or parallel) for each speaker, compared to millions of sentences usually used in NMT. Overfitting Training each speaker model with very little data is a challenge, necessitating careful and heavy regularization (Miceli Barone et al., 2017) and an early stopping procedure. 3. As a consequence of 1, we can assume that many speakers share similar characteristics such as gender, social status, and as such may have similar associated domains.2 2.1 2.3 A more efficient domain adaptation technique is the domain token idea used in Sennrich et al. (2016a); Chu et al. (2017): introduce an additional token marking the domain in the source and/or the target sentence. In experiments, we add a token indicating the speaker at the start of the target sentence for each speaker. We refer to this method as the spk token method in the following. N"
P18-2050,W17-3203,1,0.865106,"Missing"
P18-2050,P15-1162,0,0.0284981,"Missing"
P18-2050,D15-1130,0,0.0651531,"tion quality and accuracy with respect to speaker traits.1 Introduction The production of language varies depending on the speaker or author, be it to reflect personal traits (e.g. job, gender, role, dialect) or the topics that tend to be discussed (e.g. technology, law, religion). Current Neural Machine Translation (NMT) systems do not incorporate any explicit information about the speaker, and this forces the model to learn these traits implicitly. This is a difficult and indirect way to capture inter-personal variations, and in some cases it is impossible without external context (Table 1, Mirkin et al. (2015)). Recent work has incorporated side information about the author such as personality (Mirkin et al., 2015), gender (Rabinovich et al., 2017) or politeness (Sennrich et al., 2016a), but these methods can only handle phenomena where there are ex2 Problem Formulation and Baselines In the rest of this paper, we refer to the person producing the source sentence (speaker, author, 1 Data/code publicly available at http://www.cs.cmu.edu/∼pmichel1/ sated/ and https://github.com/neulab/ extreme-adaptation-for-personalized-translation respectively. 312 Proceedings of the 56th Annual Meeting of the Assoc"
P18-2050,P10-2041,0,0.0897703,"Missing"
P18-2050,W04-3250,0,0.129364,"We test three models base (a baseline ignoring speaker labels), full bias and fact bias. During training, we limit our vocabulary to the 40,000 most frequent words. Additionally, we discard any word appearing less than 2 times. Any word that doesn’t satisfy those conditions is replaced with an UNK token.5 All our models are implemented with the DyNet (Neubig et al., 2017) framework, and unless specified we use the default settings therein. We refer to appendix B for a detailed explanation of the training process. We translate the test set using beam search with beam size 5. strap resampling (Koehn, 2004). As shown in the table, both proposed methods give significant improvements in BLEU score, with the biggest gains in English to French (+0.99) and smaller gains in German and Spanish (+0.74 and +0.40 respectively). Reducing the number of parameters with fact bias gives slightly better (en-fr) or worse (en-de) BLEU score, but in those cases the results are still significantly better than the baseline. However, BLEU is not a perfect evaluation metric. In particular, we are interested in evaluating how much of the personal traits of each speaker our models capture. To gain more insight into this"
P18-2050,P07-2045,0,0.00533377,"n order to evaluate the effectiveness of our proposed methods, we construct a new dataset, Speaker Annotated TED (SATED) based on TED talks,4 with three language pairs, English-French (en-fr), English-German (en-de) and EnglishSpanish (en-es) and speaker annotation. The dataset consists of transcripts directly collected from https://www.ted.com/talks, and contains roughly 271K sentences in each language distributed among 2324 talks. We pre-process the data by removing sentences that don’t have any translation or are longer than 60 words, lowercasing, and tokenizing (using the Moses tokenizer (Koehn et al., 2007)). Factored speaker bias The biases for a set of speakers S on a vocabulary V can be represented as a matrix: B ∈ R|S|×|V| (4) (3) where each row of B is one speaker bias bs . In this formulation, the |S |rows are still linearly independent, meaning that B is high rank. In practical terms, this means that we cannot share information among users about how their vocabulary 3 Notably, while this limits the model to only handling word choice and does not explicitly allow it to model syntactic variations, favoring certain words over others can indirectly favor certain phenomena (e.g. favoring passi"
P18-2050,E17-2025,0,0.0283104,"hod in the following. Note that in this case there is now only an embedding vector (of dimension 512 in our experiments) for each speaker. However, the resulting domain embedding are non-trivial to interpret (i.e. it is not clear what they tell us about the domain or speaker itself). Baseline NMT model All of our experiments are based on a standard neural sequence to sequence model. We use one layer LSTMs as the encoder and decoder and the concat attention mechanism described in Luong and Manning (2015). We share the parameters in the embedding and softmax matrix of the decoder as proposed in Press and Wolf (2017). All the layers have dimension 512 except for the attention layer (dimension 256). To make our baseline competitive, we apply several regularization techniques such as dropout (Srivastava et al., 2014) in the output layer and within the LSTM (using the variant presented in Gal and Ghahramani, 2016). We also drop words in the target sentence with probability 0.1 according to Iyyer et al. (2015) and implement label smoothing as proposed in Szegedy et al. (2016) with coefficient 0.1. Appendix A provides a more thorough description of the baseline model. 2.2 Domain Token 3 Speaker-specific Vocabu"
P18-2050,E17-1101,0,0.183388,"hor, be it to reflect personal traits (e.g. job, gender, role, dialect) or the topics that tend to be discussed (e.g. technology, law, religion). Current Neural Machine Translation (NMT) systems do not incorporate any explicit information about the speaker, and this forces the model to learn these traits implicitly. This is a difficult and indirect way to capture inter-personal variations, and in some cases it is impossible without external context (Table 1, Mirkin et al. (2015)). Recent work has incorporated side information about the author such as personality (Mirkin et al., 2015), gender (Rabinovich et al., 2017) or politeness (Sennrich et al., 2016a), but these methods can only handle phenomena where there are ex2 Problem Formulation and Baselines In the rest of this paper, we refer to the person producing the source sentence (speaker, author, 1 Data/code publicly available at http://www.cs.cmu.edu/∼pmichel1/ sated/ and https://github.com/neulab/ extreme-adaptation-for-personalized-translation respectively. 312 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 312–318 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computati"
P18-2050,N16-1005,0,0.163364,"Missing"
P18-2050,P16-1162,0,0.856309,".g. job, gender, role, dialect) or the topics that tend to be discussed (e.g. technology, law, religion). Current Neural Machine Translation (NMT) systems do not incorporate any explicit information about the speaker, and this forces the model to learn these traits implicitly. This is a difficult and indirect way to capture inter-personal variations, and in some cases it is impossible without external context (Table 1, Mirkin et al. (2015)). Recent work has incorporated side information about the author such as personality (Mirkin et al., 2015), gender (Rabinovich et al., 2017) or politeness (Sennrich et al., 2016a), but these methods can only handle phenomena where there are ex2 Problem Formulation and Baselines In the rest of this paper, we refer to the person producing the source sentence (speaker, author, 1 Data/code publicly available at http://www.cs.cmu.edu/∼pmichel1/ sated/ and https://github.com/neulab/ extreme-adaptation-for-personalized-translation respectively. 312 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 312–318 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics etc. . . ) generical"
P18-2050,D17-1155,0,0.0561489,"Missing"
P18-2050,D16-1163,0,0.0615428,"Missing"
P18-2105,W14-3342,0,0.0134729,"tial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017). QuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE. Its effectiveness and flexibility make it an attractive candidate for our proposed task. There are two main modules to QuEst++: a feature extractor and a learning module. The feature extractor produces an intermediate representation of the source and translation in Ratio of non-specific words: Interpreters often compress output by replacing or omitting common nouns to avoid specific terminology (Sridhar et al., 2013), either t"
P18-2105,W16-2387,0,0.0129747,"mation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017). QuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE. Its effectiveness and flexibility make it an attractive candidate for our proposed task. There are two main modules to QuEst++: a feature extractor and a learning module. The feature extractor produces an intermediate representation of the source and translation in Ratio of non-specific words: Interpreters often compress output by replacing or omitting common nouns to avoid specific terminology (Sridhar et al., 2013), either to prevent redundancy or to ease cogniti"
P18-2105,Q17-1015,0,0.0235044,"Missing"
P18-2105,P02-1040,0,0.102145,"words: Sridhar et al. (2013) propose that interpreters regularly use pauses to gain more time to think and as a cognitive strategy to manage memory constraints. An increased number of hesitations or incomplete words in interpreter output might indicate that an interpreter is struggling to produce accurate output. In our particular case, both corpora we use in experiments are annotated for pauses and partial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017). QuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE. Its effectiveness and flexi"
P18-2105,C04-1046,0,0.19602,"ian (EN-IT) interpretation data attempting to answer these questions. 2 Quality Estimation for Interpretation Ratio of pauses/hesitations/incomplete words: Sridhar et al. (2013) propose that interpreters regularly use pauses to gain more time to think and as a cognitive strategy to manage memory constraints. An increased number of hesitations or incomplete words in interpreter output might indicate that an interpreter is struggling to produce accurate output. In our particular case, both corpora we use in experiments are annotated for pauses and partial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017"
P18-2105,2013.iwslt-papers.3,1,0.710879,"on metrics available for MT including WER (Su et al.), BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Denkowski and Lavie, 2014), all of which compare the similarity between reference translations and translations. Interpreter output is fundamentally different from any reference that we may use in evaluation because interpreters employ a range of economizing strategies such as segmentation, omission, generalization, and reformulation (Riccardi, 2005). As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (Shimizu et al., 2013). To mitigate this, we use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and contentfunction word distinctions, and thus should be better equipped to deal with the disparity between MT and SI. Better handling of these divergences 5 Interpreter Quality Experiments To evaluate the quality of our QE system, we use the Pearson’s r correlation between the predicted and true METEOR for each language pair (Graham, 2015). As a baseline, we train QuEst++ on the out-of-the-box feature set (Section 2). We use k-fold cross-validation individually on EN-JA, EN-FR, and EN-IT s"
P18-2105,W12-3102,0,0.103844,"Missing"
P18-2105,P15-4020,0,0.315887,"t of information passed to the interpreter, maximizing the quality of interpreter output. As a concrete method for estimating interpreter performance, we turn to existing work on QE for machine translation (MT) systems (Specia et al., 2010, 2015), which takes in the source sentence and MT-generated outputs and estimates a measure of quality. In doing so, we arrive at two natural research questions: 3 The default, out-of-the-box, sentence-level feature set for QuEst++ includes seventeen features such as number of tokens in source/target utterances, average token length, n-gram frequency, etc. (Specia et al., 2015). While this feature set is effective for evaluation of MT output, SI output is inherently different—full of pauses, hesitations, paraphrases, re-orderings and repetitions. In the following sections, we describe our methods to adapt QE to handle these phenomena. 1. Do existing methods for performing QE on MT output also allow for accurate estimation of interpreter performance, despite the inherent differences between MT and SI? 2. What unique aspects of the problem of interpreter performance estimation, such as the availability of prosody and other linguistic cues, can be exploited to further"
P18-2105,W14-3348,0,0.0113805,"are assessed for accuracy on the number of omissions, additions and the inaccurate renditions of lexical items and longer phrases (Altman, 1994), but recovery of content and correct terminology are highly valued. While no large corpus exists that has been manually annotated with these measures, they align with the phenomena that MT evaluation tries to solve. One important design decision is which evaluation metric to target in our QE system. There is an abundance of evaluation metrics available for MT including WER (Su et al.), BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Denkowski and Lavie, 2014), all of which compare the similarity between reference translations and translations. Interpreter output is fundamentally different from any reference that we may use in evaluation because interpreters employ a range of economizing strategies such as segmentation, omission, generalization, and reformulation (Riccardi, 2005). As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (Shimizu et al., 2013). To mitigate this, we use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and contentfunction wo"
P18-2105,N13-1023,0,0.0360684,"the availability of prosody and other linguistic cues, can be exploited to further improve the accuracy of our predictions? 3.1 Interpretation-specific Features To adapt QE to interpreter output, we augment the baseline feature set with four additional types of features that may indicate a struggling interpreter. The remainder of the paper describes methods and experiments on English-Japanese (ENJA), English-French (EN-FR), and English-Italian (EN-IT) interpretation data attempting to answer these questions. 2 Quality Estimation for Interpretation Ratio of pauses/hesitations/incomplete words: Sridhar et al. (2013) propose that interpreters regularly use pauses to gain more time to think and as a cognitive strategy to manage memory constraints. An increased number of hesitations or incomplete words in interpreter output might indicate that an interpreter is struggling to produce accurate output. In our particular case, both corpora we use in experiments are annotated for pauses and partial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely"
P18-2105,2016.tc-1.5,0,0.199189,"e METEOR evaluation metric) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy.1 1 Figure 1: Simultaneous interpretation scenarios We examine the task of estimating simultaneous interpreter performance: automatically predicting when interpreters are interpreting smoothly and when they are struggling. This has several immediate potential applications, one of which being in Computer-Assisted Interpretation (CAI). CAI is quickly gaining traction in the interpreting community, with software products such as InterpretBank (Fantinouli, 2016) deployed in interpreting booths to provide live and interactive terminology support. Figure 1(b) shows how this might work; both the interpreter and the CAI system receive the source message and the system displays assistive information in the form of terminology and informational support. While this might improve the quality of interpreter output, there is a danger that these systems will provide too much information and increase the cognitive load imposed upon the interpreter (Fantinouli, 2018). Intuitively, the ideal level of support depends on current interpreter performance. The system c"
P18-2105,P15-1174,0,0.119286,"n (Riccardi, 2005). As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (Shimizu et al., 2013). To mitigate this, we use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and contentfunction word distinctions, and thus should be better equipped to deal with the disparity between MT and SI. Better handling of these divergences 5 Interpreter Quality Experiments To evaluate the quality of our QE system, we use the Pearson’s r correlation between the predicted and true METEOR for each language pair (Graham, 2015). As a baseline, we train QuEst++ on the out-of-the-box feature set (Section 2). We use k-fold cross-validation individually on EN-JA, EN-FR, and EN-IT source-interpreter language pairs with a held-out development set and test set for each fold. For each experiment setting, we run the experiment for each fold (ten iterations for each set) and evaluate average Pearson’s r correlation on the development set. In our baseline setting, we extract features based on the default QuEst++ sentence-level feature set (baseline). We ablate baseline features through cross-validation and remove features rela"
P18-2105,J07-1003,0,0.0301372,"ated for pauses and partial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017). QuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE. Its effectiveness and flexibility make it an attractive candidate for our proposed task. There are two main modules to QuEst++: a feature extractor and a learning module. The feature extractor produces an intermediate representation of the source and translation in Ratio of non-specific words: Interpreters often compress output by replacing or omitting common nouns to avoid specific terminology (Sridhar et"
P18-2105,N16-1111,1,0.899657,"Missing"
P19-1018,Q17-1010,0,0.58293,"ty during training using both the ground truth accuracy and the unsupervised CSLS metric. As can be seen from Figure 2, BLISS(M) is significantly more stable than MUSE(U), converging to better accuracy and CSLS values. Furthermore, for en$zh, Vecmap(U)++ fails to converge, while MUSE is somewhat unstable. However, BLISS does not suffer from this issue. When the word vectors are not rich enough Datasets We evaluate our models against baselines on two popularly used datasets: the MUSE dataset and the VecMap dataset. The MUSE dataset used by Lample et al. (2018) consists of embeddings trained by Bojanowski et al. (2017) on Wikipedia and bilingual dictionaries generated by internal translation tools used at Facebook. The VecMap dataset introduced by Dinu and Baroni (2014) consists of the CBOW embeddings trained on the WacKy crawling corpora. The bilingual dictionaries were obtained from the Europarl word alignments. We use the standard training and test splits available for both the datasets. 4.3 Benefits of BLISS Benchmark Tasks: Results In Tables 3 and 4, we group the instantiations of BLISS(M/R) with it’s supervised counterparts. We use † to compare models within a group, and use bold do compare across dif"
P19-1018,C16-1171,0,0.0168717,"mon embedding space. Artetxe et al. (2017) and Søgaard et al. (2018) motivate the utility of using both the supervised seed dictionaries and, to some extent, the structure of the monolingual embedding spaces. They use iterative Procrustes refinement starting with a small seed dictionary to learn a mapping; but doing may lead to sub-optimal performance for distant language pairs. However, these methods are close to our methods in spirit, and consequently form the baselines for our experiments. Another avenue of research has been to try and modify the underlying embedding generation algorithms. Cao et al. (2016) modify the CBOW algorithm (Mikolov et al., 2013b) by augmenting the CBOW loss to match the first and second order moments from the source and target latent spaces, thereby ensuring the source and target embedding spaces follow the same distribution. Luong et al. (2015), in their work, use the aligned words to jointly learn the embedding spaces of both the source and target language, by trying to predict the context of a word in the other language, given an alignment. An issue with the proposed method is that it requires the retraining of embeddings, and cannot leverage a rich collection of pr"
P19-1018,E14-1049,0,0.0522659,"age pair. 5 (word2vec (Mikolov et al., 2013b) instead of fastText), the unsupervised method can completely fail to train. This can be observed for the case of en-de in Table 4. BLISS(M/R) does not face this problem: adding supervision, even in the form of 50 mapped words for the case of en-de, helps it to achieve reasonable performance. Related Work Mikolov et al. (2013a) first used anchor points to align two embedding spaces, leveraging the fact that these spaces exhibit similar structure across languages. Since then, several approaches have been proposed for learning bilingual dictionaries (Faruqui and Dyer, 2014; Zou et al., 2013; Xing 190 Figure 2: Training Stability of different language pairs (en-de), (en-ru), (en-zh) et al., 2015). Xing et al. (2015) showed that adding an orthogonal constraint significantly improves performance, and admits a closed form solution. This was further corroborated by the work of Smith et al. (2017), who showed that in orthogonality was necessary for self-consistency. Artetxe et al. (2016) showed the equivalence between the different methods, and their subsequent work (Artetxe et al., 2018a) analyzed different techniques proposed in various works (like embedding center"
P19-1018,P08-1088,0,0.155705,"ile leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.⇤ 1 Introduction Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Lample et al., 2018), finds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI uses methods that learn a mapping between two word embedding spaces ⇤ Equal Contribution Code to replicate the experiments presented in this work can be found at https://github.com/joelmoniz/ BLISS. ⇤ 184 Proceedings of the 57th Annual Meeting of the Association for Co"
P19-1018,D16-1250,0,0.0434592,"ding spaces, leveraging the fact that these spaces exhibit similar structure across languages. Since then, several approaches have been proposed for learning bilingual dictionaries (Faruqui and Dyer, 2014; Zou et al., 2013; Xing 190 Figure 2: Training Stability of different language pairs (en-de), (en-ru), (en-zh) et al., 2015). Xing et al. (2015) showed that adding an orthogonal constraint significantly improves performance, and admits a closed form solution. This was further corroborated by the work of Smith et al. (2017), who showed that in orthogonality was necessary for self-consistency. Artetxe et al. (2016) showed the equivalence between the different methods, and their subsequent work (Artetxe et al., 2018a) analyzed different techniques proposed in various works (like embedding centering, whitening etc.), and showed that leveraging a combination of different methods showed significant performance gains. pings from both the source and the target embedding spaces into a common embedding space and doing the translations in the common embedding space. Artetxe et al. (2017) and Søgaard et al. (2018) motivate the utility of using both the supervised seed dictionaries and, to some extent, the structu"
P19-1018,W13-2233,0,0.0321138,"he embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.⇤ 1 Introduction Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Lample et al., 2018), finds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI uses methods that learn a mapping between two word embedding spaces ⇤ Equal Contribution Code to replicate the experiments presented in this work can be found at https://github.com/joelmoniz/ BLISS. ⇤ 184 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 184–193 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics merous language pairs across two datasets. Our best model outperforms the state-of-the-art on 10 of 16 language pairs on the MUSE datasets. Our analysis (§4.4) dem"
P19-1018,P17-1042,0,0.691674,"rger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.⇤ 1 Introduction Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Lample et al., 2018), finds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI uses methods that learn a mapping between two word embedding spaces ⇤ Equal Contribution Code to replicate the experiments presented in this work can be found at https://github.com/joelmoniz/ BLISS. ⇤ 184 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 184–193 c Florence, Italy, July"
P19-1018,D18-1330,0,0.409153,"Missing"
P19-1018,P18-1073,0,0.542007,"nement using the CSLS distance starting from the mapping learnt by MUSE(S). We also use our proposed hubness filtering technique during the iterative refinement process (MUSE(HR)) which leads to small performance improvements. We consequently use the hubness filtering technique in all our models. RCSLS: Joulin et al. (2018) propose optimizing the CSLS distance‡ directly for the supervised matching pairs. This leads to significant improvements over MUSE(S) and achieves state of the art results for a majority of the language pairs at the time of writing. VecMap models: Artetxe et al. (2017) and Artetxe et al. (2018a) proposed two models, VecMap and VecMap++ which were based on Iterative Procrustes refinement starting from a small seed lexicon based on numeral matching. We also compare against two well known methods GeoMM (Jawanpuria et al., 2018) and Vecmap (U )++ (Artetxe et al., 2018b). These methods learn orthogonal mappings for both source and target spaces to a common embedding space, and Row 1 of Table 2 summarizes the GH distances obtained for different language pairs. We find that etymologically close languages such as en-fr and ru-uk have a very low GH distance and can possibly be aligned well"
P19-1018,K18-1021,0,0.0929994,"quires computing the nearest neighbors over the whole embedding space, this can also be considered a semi-supervised method. 188 subsequently translate in the common space. It can be seen that BLISS(M) and BLISS(R) outperform the MUSE baselines (MUSE(U), MUSE(R)) and RCSLS respectively. We observe that GeoMM and VecMap(U)++ outperform BLISS models on the VecMap datasets. A potential reason for this could be the slight disadvantage that BLISS suffers from because of translating in the target space, as opposed to in the common embedding space. This hypothesis is also supported by the results of Kementchedjhieva et al. (2018). All the hyperparameters for the experiments can be found in the Appendix (§A.4) BLISS models We instantiate two instances of our framework corresponding to the two supervised losses in the baseline methods mentioned above. BLISS(M) optimizes the cosine distance between supervised matching pairs as its supervised loss (LW |S ), while BLISS(R) optimizes the CSLS distance between these matching pairs for its LW |S . We use the unsupervised CSLS metric as a stopping criterion during training. This metric, introduced by Lample et al. (2018), computes the average cosine similarity between matched"
P19-1018,W14-1613,0,0.0416266,"the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.⇤ 1 Introduction Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Lample et al., 2018), finds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI uses methods that learn a mapping between two word embedding spaces ⇤ Equal Contribution Code to replicate the experiments presented in this work can be found at https://github.com/joelmoniz/ BLISS. ⇤ 184 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 184–193 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics merous language pairs across two datasets. Our best mo"
P19-1018,C12-1089,0,0.089074,"the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.⇤ 1 Introduction Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Lample et al., 2018), finds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI uses methods that learn a mapping between two word embedding spaces ⇤ Equal Contribution Code to replicate the experiments presented in this work can be found at https://github.com/joelmoniz/ BLISS. ⇤ 184 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 184–193 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics merous language pairs across two datasets. Our best model outperforms the state-of-the-art on 10 of 16 la"
P19-1018,N15-1104,0,0.509808,"Missing"
P19-1018,W15-1521,0,0.0273186,"all seed dictionary to learn a mapping; but doing may lead to sub-optimal performance for distant language pairs. However, these methods are close to our methods in spirit, and consequently form the baselines for our experiments. Another avenue of research has been to try and modify the underlying embedding generation algorithms. Cao et al. (2016) modify the CBOW algorithm (Mikolov et al., 2013b) by augmenting the CBOW loss to match the first and second order moments from the source and target latent spaces, thereby ensuring the source and target embedding spaces follow the same distribution. Luong et al. (2015), in their work, use the aligned words to jointly learn the embedding spaces of both the source and target language, by trying to predict the context of a word in the other language, given an alignment. An issue with the proposed method is that it requires the retraining of embeddings, and cannot leverage a rich collection of precomputed vectors (like ones provided by Word2Vec (Mikolov et al., 2013b), Glove (Pennington et al., 2014) and FastText (Bojanowski et al., 2017)). However, the validity of this orthogonality assumption has of late come into question: Zhang et al. (2017b) found that the"
P19-1018,P17-1179,0,0.671919,"ual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.⇤ 1 Introduction Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Lample et al., 2018), finds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI uses methods that learn a mapping between two word embedding spaces ⇤ Equal Contribution Code to replicate the experiments presented in this work can be found at https://github.com/joelmoniz/ BLISS. ⇤ 184 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 184–193"
P19-1018,D17-1207,0,0.306843,"ual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.⇤ 1 Introduction Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Lample et al., 2018), finds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI uses methods that learn a mapping between two word embedding spaces ⇤ Equal Contribution Code to replicate the experiments presented in this work can be found at https://github.com/joelmoniz/ BLISS. ⇤ 184 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 184–193"
P19-1018,D14-1162,0,0.0871904,"match the first and second order moments from the source and target latent spaces, thereby ensuring the source and target embedding spaces follow the same distribution. Luong et al. (2015), in their work, use the aligned words to jointly learn the embedding spaces of both the source and target language, by trying to predict the context of a word in the other language, given an alignment. An issue with the proposed method is that it requires the retraining of embeddings, and cannot leverage a rich collection of precomputed vectors (like ones provided by Word2Vec (Mikolov et al., 2013b), Glove (Pennington et al., 2014) and FastText (Bojanowski et al., 2017)). However, the validity of this orthogonality assumption has of late come into question: Zhang et al. (2017b) found that the Wasserstein distance between distant language pairs was considerably higher , while Søgaard et al. (2018) explored the orthogonality assumption using eigenvector similarity. We find our weak orthogonality constraint (along the lines of Zhang et al. (2017a)) when used in our semi-supervised framework to be more robust to this. There has also recently been an increasing focus on generating these bilingual mappings without an aligned"
P19-1018,D13-1141,0,0.0455078,"ikolov et al., 2013b) instead of fastText), the unsupervised method can completely fail to train. This can be observed for the case of en-de in Table 4. BLISS(M/R) does not face this problem: adding supervision, even in the form of 50 mapped words for the case of en-de, helps it to achieve reasonable performance. Related Work Mikolov et al. (2013a) first used anchor points to align two embedding spaces, leveraging the fact that these spaces exhibit similar structure across languages. Since then, several approaches have been proposed for learning bilingual dictionaries (Faruqui and Dyer, 2014; Zou et al., 2013; Xing 190 Figure 2: Training Stability of different language pairs (en-de), (en-ru), (en-zh) et al., 2015). Xing et al. (2015) showed that adding an orthogonal constraint significantly improves performance, and admits a closed form solution. This was further corroborated by the work of Smith et al. (2017), who showed that in orthogonality was necessary for self-consistency. Artetxe et al. (2016) showed the equivalence between the different methods, and their subsequent work (Artetxe et al., 2018a) analyzed different techniques proposed in various works (like embedding centering, whitening etc"
P19-1018,N18-2084,1,0.839397,"to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.⇤ 1 Introduction Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Lample et al., 2018), finds use in numerous NLP tasks like POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI uses methods that learn a mapping between two word embedding spaces ⇤ Equal Contribution Code to replicate the experiments presented in this work can be found at https://github.com/joelmoniz/ BLISS. ⇤ 184 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 184–193 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics merous language pairs across two datasets. Our best model outperforms the state-of-the-art on 10 of 16 language pairs on the MUSE datasets. Our analysis (§4.4) demonstrates that add"
P19-1018,P18-1072,0,0.389631,"Missing"
P19-1115,P08-1115,0,0.485401,"s. Introduction In many natural language processing tasks, graphbased representations have proven useful tools to enable models to deal with highly structured knowledge. Lattices are a common instance of graph-based representations that allows capturing a large number of alternative sequences in a compact form (Figure 1). Example applications include speech recognition lattices that represent alternative decoding choices (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs"
P19-1115,D17-1209,0,0.0512882,"Missing"
P19-1115,P18-1026,0,0.0190043,"Related Work The translation of lattices rather than sequences has been investigated with traditional machine translation models (Ney, 1999; Casacuberta et al., 2004; Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008; Dyer et al., 2008), but these approaches rely on independence assumptions in the decoding process that no longer hold for neural encoder-decoder models. Neural latticeto-sequence models were proposed by Su et al. (2017); Sperber et al. (2017), with promising results but slow computation speeds. Other related work includes gated graph neural networks (Li et al., 2016; Beck et al., 2018). As an alternative to these RNN-based models, GCNs have been investigated (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), and used for devising tree-tosequence models (Bastings et al., 2017; Marcheggiani et al., 2018). We are not aware of any application of GCNs to lattice modeling. Unlike our approach, GCNs consider only local context, must be combined with slower LSTM layers for good performance, and lack support for lattice scores. Our model builds on previous works on selfattentional models (Cheng et al., 2016; Parikh et al., 2016; Lin et al"
P19-1115,D16-1244,0,0.106931,"Missing"
P19-1115,D15-1166,0,0.0667507,"Missing"
P19-1115,2013.iwslt-papers.14,0,0.225797,"Missing"
P19-1115,N18-2078,0,0.0192845,"e approaches rely on independence assumptions in the decoding process that no longer hold for neural encoder-decoder models. Neural latticeto-sequence models were proposed by Su et al. (2017); Sperber et al. (2017), with promising results but slow computation speeds. Other related work includes gated graph neural networks (Li et al., 2016; Beck et al., 2018). As an alternative to these RNN-based models, GCNs have been investigated (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), and used for devising tree-tosequence models (Bastings et al., 2017; Marcheggiani et al., 2018). We are not aware of any application of GCNs to lattice modeling. Unlike our approach, GCNs consider only local context, must be combined with slower LSTM layers for good performance, and lack support for lattice scores. Our model builds on previous works on selfattentional models (Cheng et al., 2016; Parikh et al., 2016; Lin et al., 2017; Vaswani et al., 2017). The idea of masking has been used for various purposes, including occlusion of future information during training (Vaswani et al., 2017), introducing directionality (Shen et al., 2018) with good results for machine translation confirm"
P19-1115,P10-1134,0,0.0352991,"processing tasks, graphbased representations have proven useful tools to enable models to deal with highly structured knowledge. Lattices are a common instance of graph-based representations that allows capturing a large number of alternative sequences in a compact form (Figure 1). Example applications include speech recognition lattices that represent alternative decoding choices (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs to tree-structured inputs prevents convenient us"
P19-1115,D13-1170,0,0.00348044,"al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs to tree-structured inputs prevents convenient use of batched computation. An alternative model, graph convolutional networks (GCN) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), is much faster but considers only local context and therefore requires combination with slower RNN layers for typical natural language processing tasks (Bastings et al., 2017; Cetoli et al., 2017; Vashishth et al., 2018). For li"
P19-1115,D17-1145,1,0.3653,"tion lattices that represent alternative decoding choices (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs to tree-structured inputs prevents convenient use of batched computation. An alternative model, graph convolutional networks (GCN) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), is much faster but considers only local context and therefore requires combination with slower RNN layers for typical natural language process"
P19-1115,P15-1150,0,0.0850618,"Missing"
P19-1115,P18-1149,0,0.0225762,"structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs to tree-structured inputs prevents convenient use of batched computation. An alternative model, graph convolutional networks (GCN) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), is much faster but considers only local context and therefore requires combination with slower RNN layers for typical natural language processing tasks (Bastings et al., 2017; Cetoli et al., 2017; Vashishth et al., 2018). For linear sequence modeling, self-attention (Cheng et al., 2016; Parikh et al., 2016; Lin et al., 2017; Vaswani et al., 2017) now provides an alternative to RNNs. Self-attention encodes sequences by relating sequence items to one another through computation of pairwise similarity, with addition of positional encoding to model positions of words in a linear sequence. Self-attention has gained popularity thanks to strong empirical results and computational efficiency afforded by paralleliz1185 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1185–"
P19-1115,2005.iwslt-1.2,0,0.694097,"and inference. 1 c 0.6 b 0.8 d 0.2 1 1 f E 1 1 1 g e Figure 1: Example of a node-labeled lattice. Nodes are labeled with word tokens and posterior scores. Introduction In many natural language processing tasks, graphbased representations have proven useful tools to enable models to deal with highly structured knowledge. Lattices are a common instance of graph-based representations that allows capturing a large number of alternative sequences in a compact form (Figure 1). Example applications include speech recognition lattices that represent alternative decoding choices (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et"
P19-1286,D16-1162,1,0.800355,"method does not depend on model architectures, which makes it orthogonal to these model-based methods. Our work shows that apart from strengthening the target-side decoder, direct supervision over the in-domain unseen words is essential for domain adaptation. Similar to this, a variety of methods focus on solving OOV problems in translation. Daum´e III and Jagarlamudi (2011) induce lexicons for unseen words and construct phrase tables for statistical machine translation. However, it is nontrivial to integrate lexicon into NMT models that lack explicit use of phrase tables. With regard to NMT, Arthur et al. (2016) use a lexicon to bias the probability of the NMT system and show promising improvements. Luong and Manning (2015) propose to emit OOV target words by their corresponding source words and do post-translation for those OOV words with a dictionary. Fadaee et al. (2017) propose an effective data augmentation method that generates sentence pairs containing rare words in synthetically created contexts, but this requires parallel training data not available in the fully unsupervised adaptation setting. Arcan and Buitelaar (2017) leverage a domainspecific lexicon to replace unknown words after decodi"
P19-1286,Q17-1010,0,0.0129557,"te that these domains are very distant from each other. Following Koehn and Knowles (2017), we process all the data with byte-pair encoding (Sennrich et al., 2016b) to construct a vocabulary of 50K subwords. To build an unaligned monolingual corpus for each domain, we randomly shuffle the parallel corpus and split the corpus into two parts with equal numbers of parallel sentences. We use the target and source sentences of the first and second halves respectively. We combine all the unaligned monolingual source and target sentences on all five domains to train a skip-gram model using fasttext (Bojanowski et al., 2017). We obtain source and target word embeddings in 512 dimensions by running 10 epochs with a context window of 10, and 10 negative samples. Corpus Medical IT Subtitles Law Koran Words 12,867,326 2,777,136 106,919,386 15,417,835 9,598,717 Sentences 1,094,667 333,745 13,869,396 707,630 478,721 W/S 11.76 8.32 7.71 21.80 20.05 Table 2: Corpus statistics over five domains. 3.2 Main Results We first compare DALI with other adaptation strategies on both RNN-based and Transformerbased NMT models. Table 1 shows the performance of the two models when trained on one domain (columns) and tested on another"
P19-1286,W17-4712,0,0.0784365,"In this paper, we try to fill this gap, examining domain adaptation methods for NMT specifically focusing on correctly translating unknown words. As noted by Chu and Wang (2018), there are two important distinctions to make in adaptation methods for MT. The first is data requirements; supervised adaptation relies on in-domain parallel data, and unsupervised adaptation has no such requirement. There is also a distinction between model-based and data-based methods. Modelbased methods make explicit changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervis"
P19-1286,P16-1185,0,0.017489,"thetic parallel data. Recent databased methods such as back-translation (Sennrich et al., 2016a) and copy-based methods (Currey et al., 2017) mainly focus on improving fluency of the output sentences and translation of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, model-based methods change model architectures to leverage monolingual corpus by introducing an extra learning objective, such as auto-encoder objective (Cheng et al., 2016) and language modeling objective (Ramachandran et al., 2017). Another line of research on using monolingual data is unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018b,a; Yang et al., 2018). These methods use word-for-word translation as a component, but require a careful design of model architectures, and do not explicitly tackle the domain adaptation problem. Our proposed data-based method does not depend on model architectures, which makes it orthogonal to these model-based methods. Our work shows that apart from strengthening the target-side decoder, direct supervi"
P19-1286,W17-4714,0,0.0624612,"Missing"
P19-1286,C18-1111,0,0.0594611,"ve domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines. 1 Introduction Neural machine translation (NMT) has demonstrated impressive performance when trained on large-scale corpora (Bojar et al., 2018). However, it has also been noted that NMT models trained on corpora in a particular domain tend to perform poorly when translating sentences in a significantly different domain (Chu and Wang, 2018; Koehn and Knowles, 2017). Previous work in the context of phrase-based statistical machine translation (Daum´e III and Jagarlamudi, 2011) has noted that unseen (OOV) words account for a large portion of translation errors when switching to new domains. However this problem of OOV words in cross-domain transfer is under-examined Code/scripts are released at https://github.com/ junjiehu/dali. in the context of NMT, where both training methods and experimental results will differ greatly. In this paper, we try to fill this gap, examining domain adaptation methods for NMT specifically focusing o"
P19-1286,W17-4715,0,0.379184,"chitecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017). However, while these methods have potential to strengthen the target-language decoder through addition of in-domain target data, they do not explicitly provide direct supervision of domainspecific words, which we ar"
P19-1286,P11-2071,0,0.0967527,"Missing"
P19-1286,D17-1158,0,0.0505273,"y focusing on correctly translating unknown words. As noted by Chu and Wang (2018), there are two important distinctions to make in adaptation methods for MT. The first is data requirements; supervised adaptation relies on in-domain parallel data, and unsupervised adaptation has no such requirement. There is also a distinction between model-based and data-based methods. Modelbased methods make explicit changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-trans"
P19-1286,P17-2090,0,0.019089,"to this, a variety of methods focus on solving OOV problems in translation. Daum´e III and Jagarlamudi (2011) induce lexicons for unseen words and construct phrase tables for statistical machine translation. However, it is nontrivial to integrate lexicon into NMT models that lack explicit use of phrase tables. With regard to NMT, Arthur et al. (2016) use a lexicon to bias the probability of the NMT system and show promising improvements. Luong and Manning (2015) propose to emit OOV target words by their corresponding source words and do post-translation for those OOV words with a dictionary. Fadaee et al. (2017) propose an effective data augmentation method that generates sentence pairs containing rare words in synthetically created contexts, but this requires parallel training data not available in the fully unsupervised adaptation setting. Arcan and Buitelaar (2017) leverage a domainspecific lexicon to replace unknown words after decoding. Zhao et al. (2018) design a contextual memory module in an NMT system to memorize translations of rare words. Kothur et al. (2018) treats an annotated lexicon as parallel sentences and continues training the NMT system on the lexicon. Though all these works lever"
P19-1286,D15-1147,0,0.0704678,"Missing"
P19-1286,P17-4012,0,0.0999685,"Missing"
P19-1286,kobus-etal-2017-domain,0,0.0414302,"18), there are two important distinctions to make in adaptation methods for MT. The first is data requirements; supervised adaptation relies on in-domain parallel data, and unsupervised adaptation has no such requirement. There is also a distinction between model-based and data-based methods. Modelbased methods make explicit changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a),"
P19-1286,W17-3204,0,0.363591,"ty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines. 1 Introduction Neural machine translation (NMT) has demonstrated impressive performance when trained on large-scale corpora (Bojar et al., 2018). However, it has also been noted that NMT models trained on corpora in a particular domain tend to perform poorly when translating sentences in a significantly different domain (Chu and Wang, 2018; Koehn and Knowles, 2017). Previous work in the context of phrase-based statistical machine translation (Daum´e III and Jagarlamudi, 2011) has noted that unseen (OOV) words account for a large portion of translation errors when switching to new domains. However this problem of OOV words in cross-domain transfer is under-examined Code/scripts are released at https://github.com/ junjiehu/dali. in the context of NMT, where both training methods and experimental results will differ greatly. In this paper, we try to fill this gap, examining domain adaptation methods for NMT specifically focusing on correctly translating un"
P19-1286,W18-2708,0,0.0171436,"propose to emit OOV target words by their corresponding source words and do post-translation for those OOV words with a dictionary. Fadaee et al. (2017) propose an effective data augmentation method that generates sentence pairs containing rare words in synthetically created contexts, but this requires parallel training data not available in the fully unsupervised adaptation setting. Arcan and Buitelaar (2017) leverage a domainspecific lexicon to replace unknown words after decoding. Zhao et al. (2018) design a contextual memory module in an NMT system to memorize translations of rare words. Kothur et al. (2018) treats an annotated lexicon as parallel sentences and continues training the NMT system on the lexicon. Though all these works leverage a lexicon to address the problem of OOV words, none specifically target translating in-domain OOV words under a domain adaptation setting. 5 Conclusion In this paper, we propose a data-based, unsupervised adaptation method that focuses on domain adaption by lexicon induction (DALI) for mitigating unknown word problems in NMT. We conduct extensive experiments to show consistent improvements of two popular NMT models through the usage of our proposed method. Fu"
P19-1286,W11-2132,0,0.0308622,"umvent the domain shift problem by jointly learning domain discrimination and the translation. Joty et al. (2015) and Wang et al. (2017) address the domain adaptation problem by assigning higher weight to out-ofdomain parallel sentences that are close to the indomain corpus. Our proposed method focuses on solving the adaptation problem with no in-domain parallel sentences, a strict unsupervised setting. Prior work on using monolingual data to do data augmentation could be easily adapted to the domain adaptation setting. Early studies on databased methods such as self-enhancing (Schwenk, 2008; Lambert et al., 2011) translate monolingual source sentences by a statistical machine translation system, and continue training the system on the synthetic parallel data. Recent databased methods such as back-translation (Sennrich et al., 2016a) and copy-based methods (Currey et al., 2017) mainly focus on improving fluency of the output sentences and translation of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, model-based methods change"
P19-1286,J82-2005,0,0.551376,"Missing"
P19-1286,2015.iwslt-evaluation.11,0,0.70816,"nd unsupervised adaptation has no such requirement. There is also a distinction between model-based and data-based methods. Modelbased methods make explicit changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017). However, while these methods have pote"
P19-1286,J03-1002,0,0.0141981,"ord embeddings of the i-th translation pair (s, t)i by the ith column vectors of X(n) , Y(n) ∈ Rd×n respectively. Xing et al. (2015) show that by enforcing an orthogonality constraint on W ∈ Od (R), we can obtain a closed-form solution from a singular T value decomposition (SVD) of Y(n) X(n) : W∗ = arg max W∈Od (R) kY(n) − WX(n) kF = UVT T UΣVT = SVD(Y(n) X(n) ). (1) In a domain adaptation setting we have parallel out-of-domain data Dparallel-out , which can be used to extract a seed lexicon. Algorithm 1 shows the procedure of extracting this lexicon. We use the word alignment toolkit GIZA++ (Och and Ney, 2003) to extract word translation probabilities P (t|s) and P (s|t) in both forward and backward directions from Dparallel-out , and extract lexicons Lfw = {(s, t), ∀P (t|s) > 0} and Lbw = 2990 Algorithm 1 Supervised lexicon extraction Input: Parallel out-of-domain data Dparallel-out Output: Seed lexicon L = {(s, t)}ni=1 1: Run GIZA++ on Dparallel-out to get Lfw , Lbw 2: Lg = Lfw ∪ Lbw 3: Remove pairs with punctuation only in either s and t from Lg 4: Initialize a counter C[(s, t)] = 0 ∀(s, t) ∈ Lg 5: for (src, tgt) ∈ Dparallel-out do 6: for (s, t) ∈ Lg do 7: if s ∈ src and t ∈ tgt then 8: C[(s, t)"
P19-1286,D17-1039,0,0.0530467,"Missing"
P19-1286,P16-1162,0,0.716803,"changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017). However, while these methods have potential to strengthen the target-language decoder through addition of in-domain target data, they do not explicitly provide direct supervision of domainspec"
P19-1286,P17-2089,0,0.0459512,"rrectly translated in-domain keywords of the sentence. 4 Related Work There is much work on supervised domain adaptation setting where we have large out-of-domain parallel data and much smaller in-domain parallel data. Luong and Manning (2015) propose training a model on an out-of-domain corpus and do finetuning with small sized in-domain parallel data 2996 to mitigate the domain shift problem. Instead of naively mixing out-of-domain and in-domain data, Britz et al. (2017) circumvent the domain shift problem by jointly learning domain discrimination and the translation. Joty et al. (2015) and Wang et al. (2017) address the domain adaptation problem by assigning higher weight to out-ofdomain parallel sentences that are close to the indomain corpus. Our proposed method focuses on solving the adaptation problem with no in-domain parallel sentences, a strict unsupervised setting. Prior work on using monolingual data to do data augmentation could be easily adapted to the domain adaptation setting. Early studies on databased methods such as self-enhancing (Schwenk, 2008; Lambert et al., 2011) translate monolingual source sentences by a statistical machine translation system, and continue training the syst"
P19-1286,N15-1104,0,0.0431559,"müdigkeit: tiredness … !∗ Induction &apos; $ Pseudo-in-domain Source Corpus Figure 1: Work flow of domain adaptation by lexicon induction (DALI). data-based method for unsupervised adaptation that specifically focuses the unknown word problem: domain adaptation by lexicon induction (DALI). Our proposed method leverages large amounts of monolingual data to find translations of in-domain unseen words, and constructs a pseudo-parallel in-domain corpus via word-forword back-translation of monolingual in-domain target sentences into source sentences. More specifically, we leverage existing supervised (Xing et al., 2015) and unsupervised (Conneau et al., 2018) lexicon induction methods that project source word embeddings to the target embedding space, and find translations of unseen words by their nearest neighbors. For supervised lexicon induction, we learn such a mapping function under the supervision of a seed lexicon extracted from out-of-domain parallel sentences using word alignment. For unsupervised lexicon induction, we follow Conneau et al. (2018) to infer a lexicon by adversarial training and iterative refinement. In the experiments on German-to-English translation across five domains (Medical, IT,"
P19-1286,P18-1005,0,0.0225506,"of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, model-based methods change model architectures to leverage monolingual corpus by introducing an extra learning objective, such as auto-encoder objective (Cheng et al., 2016) and language modeling objective (Ramachandran et al., 2017). Another line of research on using monolingual data is unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018b,a; Yang et al., 2018). These methods use word-for-word translation as a component, but require a careful design of model architectures, and do not explicitly tackle the domain adaptation problem. Our proposed data-based method does not depend on model architectures, which makes it orthogonal to these model-based methods. Our work shows that apart from strengthening the target-side decoder, direct supervision over the in-domain unseen words is essential for domain adaptation. Similar to this, a variety of methods focus on solving OOV problems in translation. Daum´e III and Jagarlamudi (2011) induce lexicons for uns"
P19-1286,D16-1160,0,0.0287716,"s on databased methods such as self-enhancing (Schwenk, 2008; Lambert et al., 2011) translate monolingual source sentences by a statistical machine translation system, and continue training the system on the synthetic parallel data. Recent databased methods such as back-translation (Sennrich et al., 2016a) and copy-based methods (Currey et al., 2017) mainly focus on improving fluency of the output sentences and translation of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, model-based methods change model architectures to leverage monolingual corpus by introducing an extra learning objective, such as auto-encoder objective (Cheng et al., 2016) and language modeling objective (Ramachandran et al., 2017). Another line of research on using monolingual data is unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018b,a; Yang et al., 2018). These methods use word-for-word translation as a component, but require a careful design of model architectures, and do not explicitly tackle the domain adaptation problem. O"
P19-1286,D18-1036,0,0.019901,"a lexicon to bias the probability of the NMT system and show promising improvements. Luong and Manning (2015) propose to emit OOV target words by their corresponding source words and do post-translation for those OOV words with a dictionary. Fadaee et al. (2017) propose an effective data augmentation method that generates sentence pairs containing rare words in synthetically created contexts, but this requires parallel training data not available in the fully unsupervised adaptation setting. Arcan and Buitelaar (2017) leverage a domainspecific lexicon to replace unknown words after decoding. Zhao et al. (2018) design a contextual memory module in an NMT system to memorize translations of rare words. Kothur et al. (2018) treats an annotated lexicon as parallel sentences and continues training the NMT system on the lexicon. Though all these works leverage a lexicon to address the problem of OOV words, none specifically target translating in-domain OOV words under a domain adaptation setting. 5 Conclusion In this paper, we propose a data-based, unsupervised adaptation method that focuses on domain adaption by lexicon induction (DALI) for mitigating unknown word problems in NMT. We conduct extensive ex"
P19-1286,2008.iwslt-papers.6,0,0.0298477,"al. (2017) circumvent the domain shift problem by jointly learning domain discrimination and the translation. Joty et al. (2015) and Wang et al. (2017) address the domain adaptation problem by assigning higher weight to out-ofdomain parallel sentences that are close to the indomain corpus. Our proposed method focuses on solving the adaptation problem with no in-domain parallel sentences, a strict unsupervised setting. Prior work on using monolingual data to do data augmentation could be easily adapted to the domain adaptation setting. Early studies on databased methods such as self-enhancing (Schwenk, 2008; Lambert et al., 2011) translate monolingual source sentences by a statistical machine translation system, and continue training the system on the synthetic parallel data. Recent databased methods such as back-translation (Sennrich et al., 2016a) and copy-based methods (Currey et al., 2017) mainly focus on improving fluency of the output sentences and translation of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, mod"
P19-1286,P16-1009,0,0.690315,"changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017). However, while these methods have potential to strengthen the target-language decoder through addition of in-domain target data, they do not explicitly provide direct supervision of domainspec"
P19-1286,D18-1549,0,\N,Missing
P19-1286,W18-6401,0,\N,Missing
P19-1301,Q16-1031,0,0.30865,"question. It has been demonstrated that through cross-lingual transfer, it is possible to leverage one or more similar high-resource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al.,"
P19-1301,D17-1078,0,0.291043,"ource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augment"
P19-1301,N18-2085,0,0.030601,"vided a (non-exhaustive) list of examples that employ cross-lingual transfer across several tasks. Other work has performed large-scale studies on the importance of appropriately selecting a transfer language, such as Paul et al. (2009), which performed an extensive search for a “pivot language” in statistical MT, but without attempting to actually learn or predict which pivot language is best. Typologically-informed models are another vein of research that is relevant to our work. The relationship between linguistic typology and statistical modeling has been studied by Gerz et al. (2018) and Cotterell et al. (2018), with a focus on language modeling. Tsvetkov et al. (2016b) used typological information in the target language as additional input to their model for phonetic representation learning. Ammar et al. (2016) and Ahmad et al. (2018) used similar ideas for output: 2 no stf stk yes &gt; 1.61 output: 3 no output: 1 Figure 4: An example of the decision tree learned in the machine translation task for Galician as task language. dependency parsing, incorporating linguisticallyinformed vectors into their models. O’Horan et al. (2016) survey typological resources available and their utility in NLP tasks. Al"
P19-1301,P15-1166,0,0.228228,"018; Rijhwani et al., 2019). The common thread is that data in a high-resource transfer language is used to improve performance on a low-resource task language. However, determining the best transfer language for any particular task language remains an open question – the choice of transfer language has traditionally been done in a heuristic manner, often based on the intuition of the experimenter. A common method of choosing transfer languages involves selecting one that belongs to the same language family or has a small phylogenetic distance in the language family tree to the task language (Dong et al., 2015; Johnson et al., 2017; Cotterell and Heigold, 2017). However, it is not always true that all languages in a single language family share the same linguistic properties (Ahmad et al., 2018). Therefore, another strategy is to select transfer languages based on the typological properties that are relevant to the specific NLP task, such as word ordering for parsing tasks (Ammar et al., 2016; Ahmad et al., 2018). With several heuristics available for selecting a transfer language, it is unclear a priori if any single attribute of a language will be the most reliable criterion in determining whethe"
P19-1301,P81-1022,0,0.264147,"Missing"
P19-1301,D18-1029,0,0.0367306,"Missing"
P19-1301,Q17-1024,0,0.0480571,"Missing"
P19-1301,P18-1007,0,0.0241831,"Missing"
P19-1301,E17-2002,1,0.85781,"lap, and the word overlap is simply the count of the named entities that have exactly the same representations in both transfer and task languages. We also omit subword overlap in the POS and DEP tasks, as some low-resource languages do not have enough data for properly extracting subwords. 3.2 Dataset-independent Features Dataset-independent features are measures of the similarity between a pair of languages based on phylogenetic or typological properties established by linguistic study. Specifically, we leverage six different linguistic distances queried from the URIEL Typological Database (Littell et al., 2017): Geographic distance (dgeo ): The orthodromic distance between the languages on the surface of the earth, divided by the antipodal distance, based primarily on language location descriptions in Glottolog (Hammarstr¨om et al., 2018). Genetic distance (dgen ): The genealogical distance of the languages, derived from the hypothesized tree of language descent in Glottolog. Inventory distance (dinv ): The cosine distance between the phonological feature vectors derived from the PHOIBLE database (Moran et al., 2014), a collection of seven phonological databases. Syntactic distance (dsyn ): The cosi"
P19-1301,P09-5005,0,0.0897376,"Missing"
P19-1301,P16-1101,1,0.690355,"TM encoders, which are trained to maximize the cosine similarity between parallel (i.e., linked) entities (Rijhwani et al., 2019). We use the same dataset as Rijhwani et al. (2019), which contains language-linked Wikipedia article titles from 9 low-resource task languages and 53 potential transfer languages, resulting in 477 task/transfer pairs. We perform training in a zero-shot setting, where we train on corpora only in the transfer language, and test entity linking accuracy on the task language without joint training or fine-tuning. POS Tagging We train a bi-directional LSTMCNNs-CRF model (Ma and Hovy, 2016) on word 3128 sequences without using pre-trained word embeddings. The implementation is based on the NCRF++ toolkit (Yang and Zhang, 2018). We perform training on the Universal Dependencies v2.2 dataset (Nivre et al., 2018), using 26 languages that have the least training data as task languages, and 60 transfer languages,2 resulting in 1,545 pairs of transfer-task languages. Transfer is performed by joint training over the concatenated task and transfer corpora if the task language has training data, and training only with transfer corpora otherwise. The performance is measured by POS tagging"
P19-1301,N09-2056,0,0.0659497,"Missing"
P19-1301,P18-1247,1,0.846408,"e performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augmentation (Mayhew et al., 2"
P19-1301,D17-1269,0,0.0598619,"Missing"
P19-1301,P10-2041,0,0.0447077,"We consider the following baseline methods: N −1 cess with each language in all N languages as the test language `(tst) , and collect N learned models. We use Normalized Discounted Cumulative 2 For each language, we choose the treebank that has the least number of training instances, which results in 60 languages with training data and 11 without training data. 3129 • Using a single dataset-dependent feature: While dataset-dependent features have not typically been used as criteria for selecting transfer languages, they are a common feature in data selection methods for crossdomain transfer (Moore and Lewis, 2010). In EL POS DEP dataset word overlap ow subword overlap osw size ratio stf /stk type-token ratio dttr 28.6 29.2 3.7 2.5 30.7 – 0.3 – 13.4 – 9.5 7.4 52.3 – 24.8 6.4 genetic dgen syntactic dsyn featural df ea phonological dpho inventory dinv geographic dgeo 24.2 14.8 10.1 3.0 8.5 15.1 50.9 46.4 47.5 4.0 41.3 49.5 14.8 4.1 5.7 9.8 2.4 15.7 32.0 22.9 13.9 43.4 23.5 46.4 L ANG R ANK (all) L ANG R ANK (dataset) L ANG R ANK (URIEL) 51.1 53.7 32.6 63.0 17.0 58.1 28.9 26.5 16.6 65.0 65.0 59.6 1.00 0.95 Max evaluation score MT ling. distance Method view of this, we include selecting the transfer languag"
P19-1301,D18-1103,1,0.843378,"nguages is the lack of training data in the languages in question. It has been demonstrated that through cross-lingual transfer, it is possible to leverage one or more similar high-resource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation pro"
P19-1301,W18-1818,1,0.738931,"of our research goals is to understand what linguistic or statistical features of a dataset play important roles in transfer learning, so the interpretable nature of the treebased model can provide valuable insights, which we elaborate further in §6.2. 5 5.1 Experimental Settings Testbed Tasks We investigate the performance of L ANG R ANK on four common NLP tasks: MT, EL, POS tagging, and DEPendency parsing. We briefly outline the settings for all four NLP tasks. Machine Translation We train a standard attention-based sequence-to-sequence model (Bahdanau et al., 2015), using the XNMT toolkit (Neubig et al., 2018). We perform training on the multilingual TED talk corpus of Qi et al. (2018), using 54 task and 54 transfer languages, always translating into English, which results in 2,862 task/transfer pairs and 54 single-source training settings. Transfer is performed by joint training over the concatenated task and transfer corpora. Entity Linking The cross-lingual EL task involves linking a named entity mention in the task language to an English knowledge base. We train two character-level LSTM encoders, which are trained to maximize the cosine similarity between parallel (i.e., linked) entities (Rijhw"
P19-1301,I17-2050,0,0.0186119,"niques to low-resource languages is the lack of training data in the languages in question. It has been demonstrated that through cross-lingual transfer, it is possible to leverage one or more similar high-resource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al.,"
P19-1301,C16-1123,0,0.056038,"Missing"
P19-1301,D18-1061,0,0.0314953,"Missing"
P19-1301,P11-1157,0,0.0656756,"Missing"
P19-1301,N18-2084,1,0.828574,"a dataset play important roles in transfer learning, so the interpretable nature of the treebased model can provide valuable insights, which we elaborate further in §6.2. 5 5.1 Experimental Settings Testbed Tasks We investigate the performance of L ANG R ANK on four common NLP tasks: MT, EL, POS tagging, and DEPendency parsing. We briefly outline the settings for all four NLP tasks. Machine Translation We train a standard attention-based sequence-to-sequence model (Bahdanau et al., 2015), using the XNMT toolkit (Neubig et al., 2018). We perform training on the multilingual TED talk corpus of Qi et al. (2018), using 54 task and 54 transfer languages, always translating into English, which results in 2,862 task/transfer pairs and 54 single-source training settings. Transfer is performed by joint training over the concatenated task and transfer corpora. Entity Linking The cross-lingual EL task involves linking a named entity mention in the task language to an English knowledge base. We train two character-level LSTM encoders, which are trained to maximize the cosine similarity between parallel (i.e., linked) entities (Rijhwani et al., 2019). We use the same dataset as Rijhwani et al. (2019), which c"
P19-1301,D17-1038,0,0.0282302,"to their model for phonetic representation learning. Ammar et al. (2016) and Ahmad et al. (2018) used similar ideas for output: 2 no stf stk yes &gt; 1.61 output: 3 no output: 1 Figure 4: An example of the decision tree learned in the machine translation task for Galician as task language. dependency parsing, incorporating linguisticallyinformed vectors into their models. O’Horan et al. (2016) survey typological resources available and their utility in NLP tasks. Although not for cross-lingual transfer, there has been prior work on data selection for training models. Tsvetkov et al. (2016a) and Ruder and Plank (2017) use Bayesian optimization for data selection. van der Wees et al. (2017) study the effect of data selection of neural machine translation, as well as propose a dynamic method to select relevant training data that improves translation performance. Plank and van Noord (2011) design a method to automatically select domain-relevant training data for parsing in English and Dutch. 8 7 output: 0 Conclusion We formulate the task of selecting the optimal transfer languages for an NLP task as a ranking problem. For machine translation, entity linking, part-of-speech tagging, and dependency parsing, we"
P19-1301,P16-1162,0,0.00734751,"k consists only of named entities, so the TTR is typically close to 1 for all languages. Therefore, we do not include TTR related features for the EL task. Word overlap and subword overlap: We measure the similarity between the vocabularies of task- and transfer-language corpora by word overlap ow , and subword overlap osw : ow = |Ttf ∩ Ttk | , |Ttf |+ |Ttk | osw = |Stf ∩ Stk | , |Stf |+ |Stk | where Ttf and Ttk are the sets of types in the transfer- and task-language corpora, and Stf and Stk are their sets of subwords. The subwords are obtained by an unsupervised word segmentation algorithm (Sennrich et al., 2016; Kudo, 3127 2018). Note that for EL, we do not consider subword overlap, and the word overlap is simply the count of the named entities that have exactly the same representations in both transfer and task languages. We also omit subword overlap in the POS and DEP tasks, as some low-resource languages do not have enough data for properly extracting subwords. 3.2 Dataset-independent Features Dataset-independent features are measures of the similarity between a pair of languages based on phylogenetic or typological properties established by linguistic study. Specifically, we leverage six differe"
P19-1301,Q13-1001,0,0.0477631,"Missing"
P19-1301,N12-1052,0,0.124907,"Missing"
P19-1301,D18-1034,1,0.865535,"and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augmentation (Mayhew et al., 2017), or zero-shot transfer (Ahmad et al., 2018; Xie et al., 2018; Neubig and Hu, 3125 Proceedings of the 57th"
P19-1301,P18-4013,0,0.0189533,"e the same dataset as Rijhwani et al. (2019), which contains language-linked Wikipedia article titles from 9 low-resource task languages and 53 potential transfer languages, resulting in 477 task/transfer pairs. We perform training in a zero-shot setting, where we train on corpora only in the transfer language, and test entity linking accuracy on the task language without joint training or fine-tuning. POS Tagging We train a bi-directional LSTMCNNs-CRF model (Ma and Hovy, 2016) on word 3128 sequences without using pre-trained word embeddings. The implementation is based on the NCRF++ toolkit (Yang and Zhang, 2018). We perform training on the Universal Dependencies v2.2 dataset (Nivre et al., 2018), using 26 languages that have the least training data as task languages, and 60 transfer languages,2 resulting in 1,545 pairs of transfer-task languages. Transfer is performed by joint training over the concatenated task and transfer corpora if the task language has training data, and training only with transfer corpora otherwise. The performance is measured by POS tagging accuracy on the task language. Dependency Parsing For the dependency parsing task, we follow the settings of (Ahmad et al., 2018) and util"
P19-1301,C16-1045,0,0.016972,"ng machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augmentation (Mayhew et al., 2017), or zero-shot transfer (Ahmad et al., 2018; Xie et al., 2018; Neu"
P19-1301,D16-1163,0,0.0489424,"ng natural language processing (NLP) techniques to low-resource languages is the lack of training data in the languages in question. It has been demonstrated that through cross-lingual transfer, it is possible to leverage one or more similar high-resource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigol"
P19-1301,N16-1072,0,0.0284372,"t https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augmentation (Mayhew et al., 2017), or zero-shot transfer (Ahmad et al., 2018; Xie et al., 2018; Neubig and Hu, 3125 Proceedings of the 57th Annual Meeting of the Association for Co"
P19-1301,P16-1013,0,0.0196907,"lingual transfer across several tasks. Other work has performed large-scale studies on the importance of appropriately selecting a transfer language, such as Paul et al. (2009), which performed an extensive search for a “pivot language” in statistical MT, but without attempting to actually learn or predict which pivot language is best. Typologically-informed models are another vein of research that is relevant to our work. The relationship between linguistic typology and statistical modeling has been studied by Gerz et al. (2018) and Cotterell et al. (2018), with a focus on language modeling. Tsvetkov et al. (2016b) used typological information in the target language as additional input to their model for phonetic representation learning. Ammar et al. (2016) and Ahmad et al. (2018) used similar ideas for output: 2 no stf stk yes &gt; 1.61 output: 3 no output: 1 Figure 4: An example of the decision tree learned in the machine translation task for Galician as task language. dependency parsing, incorporating linguisticallyinformed vectors into their models. O’Horan et al. (2016) survey typological resources available and their utility in NLP tasks. Although not for cross-lingual transfer, there has been prio"
P19-1301,N16-1161,1,0.883501,"Missing"
P19-1301,D17-1147,0,0.0412759,"Missing"
P19-1301,D17-1302,0,\N,Missing
P19-1301,P18-1142,0,\N,Missing
P19-1301,N19-1253,1,\N,Missing
P19-1311,D14-1187,0,\N,Missing
P19-1311,D09-1086,0,\N,Missing
P19-1311,P12-1066,0,\N,Missing
P19-1311,W14-4203,0,\N,Missing
P19-1311,petrov-etal-2012-universal,0,\N,Missing
P19-1311,D11-1006,0,\N,Missing
P19-1311,N13-1126,0,\N,Missing
P19-1311,Q13-1001,0,\N,Missing
P19-1311,D15-1213,0,\N,Missing
P19-1311,P15-1119,0,\N,Missing
P19-1311,W15-2137,0,\N,Missing
P19-1311,N16-1156,0,\N,Missing
P19-1311,Q17-1010,0,\N,Missing
P19-1311,D16-1073,0,\N,Missing
P19-1311,C16-1012,0,\N,Missing
P19-1311,E17-2002,0,\N,Missing
P19-1311,D17-1302,0,\N,Missing
P19-1311,D18-1160,1,\N,Missing
P19-1311,D18-1163,0,\N,Missing
P19-1311,N19-1162,0,\N,Missing
P19-1311,Q18-1046,0,\N,Missing
P19-1311,D19-1077,0,\N,Missing
P19-1311,N19-1253,1,\N,Missing
P19-1311,P18-4013,0,\N,Missing
P19-1311,N19-1423,0,\N,Missing
P19-1427,S14-2010,0,0.0425502,"Missing"
P19-1427,S16-1081,0,0.075813,"Missing"
P19-1427,S13-1004,0,0.0543439,"Missing"
P19-1427,S12-1051,0,0.0491652,"entum of 0.99. Gradients are renormalized to norm 0.1 (Pascanu et al., 2012). We train the LTokLS objective for 200 epochs and the combined objective, LWeighted , for 10. Model selection is done by selecting the model with the lowest validation loss on the validation set. Then, depending on the evaluation being considered, we select models with the highest performance on the validation set. 4 Experiments 4.1 where u is a candidate hypothesis, U(x) is a set of candidate hypotheses, and t is the reference. 7 Evaluation is on the SemEval Semantic Textual Similarity (STS) datasets from 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). In the SemEval STS competitions, teams create models that need to work well on domains both represented in the training data and hidden domains revealed at test time. Our model and those of Wieting and Gimpel (2018), in contrast to the best performing STS systems, do not use any manually-labeled training examples nor any other linguistic resources beyond the ParaNMT corpus (Wieting and Gimpel, 2018). 8 Available at https://github.com/pytorch/ fairseq. Data Training models with minimum risk is expensive, but we wanted to evaluate in a difficult, realistic setting usin"
P19-1427,2014.iwslt-evaluation.4,0,0.0200873,"hat nearly all metrics performed similarly to one another. The 2015 and 2016 results showed more variation among metrics, but also found that BLEU was a strong choice overall, echoing the results of Cer et al. (2010). We have shown that our metric stabilizes training for NMT more than BLEU, which is a promising result given the limited success of the broad spectrum of previous attempts to discover easily tunable metrics in the context of SMT. Some researchers have found success in terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the best performance even when"
P19-1427,W11-2103,0,0.0721467,"Missing"
P19-1427,N10-1080,0,0.0160871,"scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015) and 2016 (Jawaid et al., 2016). In these tasks, participants submitted metrics to optimize during training or combinations of metrics and optimizers, given a fixed SMT system. The 2011 results showed that nearly all metrics p"
P19-1427,D17-1070,0,0.0180748,"the output of S IMI L E is continuous, it provides more informative gradients during the optimization process by distinguishing between candidates that would be similarly scored under matching-based metrics like BLEU. Lastly, we show in our analysis that S IMI L E has an additional benefit over BLEU by translating words with heavier semantic content more accurately. To define an exact metric, we reference the burgeoning field of research aimed at measuring semantic textual similarity (STS) between two sentences (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Hill et al., 2016; Conneau et al., 2017; Pagliardini et al., 2017). Specifically, we start with the method of Wieting and Gimpel (2018), which learns paraphrastic sentence representations using a contrastive loss and a parallel corpus induced by backtranslating bitext. Wieting and Gimpel showed that simple models that average word or character trigram embeddings can be highly effective for semantic similarity. The strong performance, domain robustness, and computationally efficiency of these models make them highly attractive. For the purpose of discriminative NMT training, we augment these basic models with two modifications: we a"
P19-1427,W14-3348,0,0.0229286,"s evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU.1 1 Introduction In neural machine translation (NMT) and other natural language generation tasks, it is common practice to improve likelihood-trained models by further tuning their parameters to explicitly maximize an automatic metric of system accuracy – for example, BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2014). Directly optimizing accuracy metrics involves backpropagating through discrete decoding decisions, and thus is typically accomplished with structured prediction techniques like reinforcement learning (Ranzato et al., 2016), minimum risk training (Shen 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. et al., 2015), and other specialized methods (Wiseman and Rush, 2016). Generally, these methods work by repeatedly generating a translation under the current parameters (via decoding, sampling, or loss-augmented decoding), comparing the generated translation"
P19-1427,N18-1033,0,0.107196,"sentence is about 20 times faster than METEOR when code is executed on GPU (NVIDIA GeForce GTX 1080). 6 We used the segment level data from newstest2015 and newstest2016 available at http://statmt.org/ wmt18/metrics-task.html. The former contains 7 language pairs and the latter 5. 4346 close, but the semantic similarity correlations7 in Table 1 are not, suggest that the difference between METEOR and SIM largely lies in fluency. However, not capturing fluency is something that can be ameliorated by adding a down-weighted maximum-likelihood (MLE) loss to the minimum risk loss. This was done by Edunov et al. (2018) and we use this in our experiments as well. 3 Lang. cs-en de-en ru-en tr-en Objective Functions. Following (Edunov et al., 2018), we first train models with maximum-likelihood with label-smoothing (LTokLS ) (Szegedy et al., 2016; Pereyra et al., 2017). We set the confidence penalty of label smoothing to be 0.1. Next, we fine-tune the model with a weighted average of minimum risk training (LRisk ) (Shen et al., 2015) and (LTokLS ), where the expected risk is defined as: u∈U (x) Test 2,983 2,998 3,000 3,000 Therefore, our fine-tuning objective becomes: LWeighted = γLTokLS + (1 − γ)LRisk Archite"
P19-1427,N16-1162,0,0.0688546,"Missing"
P19-1427,D11-1125,0,0.0338379,"ain - it is really unique. -39.1 -2.1 Table 8: The top two rows show examples where the generated sentences have similar BLEU scores but quite different SIM scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015) and 2016 (Jawaid et al., 2016). In these tasks, participants submitted met"
P19-1427,W16-2303,0,0.0303766,"Missing"
P19-1427,C04-1072,0,0.0918735,"mber of sentence pairs in the training/validation/test sets for all four languages. Machine Translation Preliminaries X Train 218,384 284,286 235,159 207,678 p(u|x) cost(t, u) P 0 u0 ∈U (x) p(u |x) We tune γ from the set {0.2, 0.3} in our experiments. In minimum risk training, we aim to minimized the expected cost. In our case that is 1 − BLEU(t, h) or 1 − S IMI L E(t, h) where t is the target and h is the generated hypothesis. As is commonly done, we use a smoothed version of BLEU by adding 1 to all n-gram counts except unigram counts. This is to prevent BLEU scores from being overly sparse (Lin and Och, 2004). We generate candidates for minimum risk training from n-best lists with 8 hypotheses without and do not include the reference in the candidates. Optimization. We optimize our models using Nesterov’s accelerated gradient method (Sutskever et al., 2013) using a learning rate of 0.25 and momentum of 0.99. Gradients are renormalized to norm 0.1 (Pascanu et al., 2012). We train the LTokLS objective for 200 epochs and the combined objective, LWeighted , for 10. Model selection is done by selecting the model with the lowest validation loss on the validation set. Then, depending on the evaluation be"
P19-1427,D11-1035,0,0.0238807,"mong metrics, but also found that BLEU was a strong choice overall, echoing the results of Cer et al. (2010). We have shown that our metric stabilizes training for NMT more than BLEU, which is a promising result given the limited success of the broad spectrum of previous attempts to discover easily tunable metrics in the context of SMT. Some researchers have found success in terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the best performance even when evaluated under other metrics. These results differ from the usual results obtained for SMT systems, in"
P19-1427,P13-2067,0,0.0266082,"2011 results showed that nearly all metrics performed similarly to one another. The 2015 and 2016 results showed more variation among metrics, but also found that BLEU was a strong choice overall, echoing the results of Cer et al. (2010). We have shown that our metric stabilizes training for NMT more than BLEU, which is a promising result given the limited success of the broad spectrum of previous attempts to discover easily tunable metrics in the context of SMT. Some researchers have found success in terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the"
P19-1427,P16-1159,0,0.0403668,"n terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the best performance even when evaluated under other metrics. These results differ from the usual results obtained for SMT systems, in which tuning to optimize a metric leads to the best performance on that metric (Och, 2003). Edunov et al. (2018) compared structured losses for NMT, also using sentence-level BLEU. They found risk to be an effective and robust choice, so we use risk as well in this paper. 9 Conclusion We have proposed S IMI L E, an alternative to BLEU for use as a reward in minimum risk tra"
P19-1427,N19-4007,1,0.804774,"0.50 0.55 0.45 0.15 0.08 0.03 0.33 0.24 0.27 0.34 1.48 2.50 0.66 0.13 0.25 0.34 0.63 0.65 Table 6: F1 score for various buckets of words. The values in the table are the difference between F1 for that specific language type and bucket between training using S IMI L E and BLEU (positive values means S IM I L E had a higher F1). The first part of the table shows F1 scores across bins defined by word frequency on the test set. So words appearing only 1 time are in the first row, between 2-5 times are in the second row, etc. The next part of the table buckets words by coarse POS tags. compare-mt (Neubig et al., 2019)12 to compute the F1 scores for target word types based on their frequency and their coarse part-of-speech-tag (as labeled by SpaCy13 ) and show the results in Table 6. From the table, we see that training with S IM I L E helps produce low frequency words more accurately, a fact that is consistent with the POS tag analysis in the second part of the table. Wieting and Gimpel (2017) noted that highly discriminative parts-of-speech, such as nouns, proper nouns, and numbers, made the most contribution to the sentence embeddings. Other works (Pham et al., 2015; Wieting et al., 2016) have also found"
P19-1427,P06-2101,0,0.06992,"do not know how to explain it - it is really unique. I don’t know how to explain - it is really unique. -39.1 -2.1 Table 8: The top two rows show examples where the generated sentences have similar BLEU scores but quite different SIM scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015"
P19-1427,P03-1021,0,0.454799,"he case and it’s even possible for less accurate translations to have higher scores than more accurate ones. The bottom half of the table shows examples where the difference in BLEU scores is large, but the difference in SIM scores is small. From thexe examples we can see that when BLEU scores are very different, the semantics of the sentence can still be preserved. However, we observe that often in these cases, the SIM scores of the sentences tend to be similar. 8 Related Work The seminal work on training machine translation systems to optimize particular evaluation measures was performed by Och (2003), who intro4351 Reference BLEU system SIM system ∆BLEU ∆SIM Reference BLEU system SIM system ∆BLEU ∆SIM Workers are beginning to clean up workers . Workers have begun to clean up in Rszke. In Rszke, workers are beginning to clean up. 3.2 -26.3 All that stuff sure does take a toll. None of this takes a toll . All of this is certain to take its toll . 7.1 -22.7 Reference BLEU system SIM system ∆BLEU ∆SIM Reference BLEU system SIM system ∆BLEU ∆SIM Another advantage is that they have fewer enemies. Another benefit: they have less enemies. Another advantage: they have fewer enemies. -33.8 -9.6 I d"
P19-1427,W15-3032,0,0.0355517,"Missing"
P19-1427,P02-1040,0,0.10718,"c results in better translations as evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU.1 1 Introduction In neural machine translation (NMT) and other natural language generation tasks, it is common practice to improve likelihood-trained models by further tuning their parameters to explicitly maximize an automatic metric of system accuracy – for example, BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2014). Directly optimizing accuracy metrics involves backpropagating through discrete decoding decisions, and thus is typically accomplished with structured prediction techniques like reinforcement learning (Ranzato et al., 2016), minimum risk training (Shen 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. et al., 2015), and other specialized methods (Wiseman and Rush, 2016). Generally, these methods work by repeatedly generating a translation under the current parameters (via decoding, sampling, or loss-augmented decoding"
P19-1427,P15-1094,0,0.112474,"rrect but lexically different translations. Moreover, since the output of S IMI L E is continuous, it provides more informative gradients during the optimization process by distinguishing between candidates that would be similarly scored under matching-based metrics like BLEU. Lastly, we show in our analysis that S IMI L E has an additional benefit over BLEU by translating words with heavier semantic content more accurately. To define an exact metric, we reference the burgeoning field of research aimed at measuring semantic textual similarity (STS) between two sentences (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Hill et al., 2016; Conneau et al., 2017; Pagliardini et al., 2017). Specifically, we start with the method of Wieting and Gimpel (2018), which learns paraphrastic sentence representations using a contrastive loss and a parallel corpus induced by backtranslating bitext. Wieting and Gimpel showed that simple models that average word or character trigram embeddings can be highly effective for semantic similarity. The strong performance, domain robustness, and computationally efficiency of these models make them highly attractive. For the purpose of discriminative NMT train"
P19-1427,D07-1080,0,0.0565854,"lly unique. I don’t know how to explain - it is really unique. -39.1 -2.1 Table 8: The top two rows show examples where the generated sentences have similar BLEU scores but quite different SIM scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015) and 2016 (Jawaid et al., 2016). In the"
P19-1427,P17-1190,1,0.867686,"s bins defined by word frequency on the test set. So words appearing only 1 time are in the first row, between 2-5 times are in the second row, etc. The next part of the table buckets words by coarse POS tags. compare-mt (Neubig et al., 2019)12 to compute the F1 scores for target word types based on their frequency and their coarse part-of-speech-tag (as labeled by SpaCy13 ) and show the results in Table 6. From the table, we see that training with S IM I L E helps produce low frequency words more accurately, a fact that is consistent with the POS tag analysis in the second part of the table. Wieting and Gimpel (2017) noted that highly discriminative parts-of-speech, such as nouns, proper nouns, and numbers, made the most contribution to the sentence embeddings. Other works (Pham et al., 2015; Wieting et al., 2016) have also found that when training semantic embeddings using an averaging function, embeddings that bear the most information regarding the meaning have larger norms. We also see that these same parts-of-speech (nouns, proper nouns, numbers) have the largest difference in F1 scores between S IMI L E and BLEU. Other parts-of-speech like SYM and INTJ have high F1 scores as well, and words belongin"
P19-1427,P18-1042,1,0.892666,"timization process by distinguishing between candidates that would be similarly scored under matching-based metrics like BLEU. Lastly, we show in our analysis that S IMI L E has an additional benefit over BLEU by translating words with heavier semantic content more accurately. To define an exact metric, we reference the burgeoning field of research aimed at measuring semantic textual similarity (STS) between two sentences (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Hill et al., 2016; Conneau et al., 2017; Pagliardini et al., 2017). Specifically, we start with the method of Wieting and Gimpel (2018), which learns paraphrastic sentence representations using a contrastive loss and a parallel corpus induced by backtranslating bitext. Wieting and Gimpel showed that simple models that average word or character trigram embeddings can be highly effective for semantic similarity. The strong performance, domain robustness, and computationally efficiency of these models make them highly attractive. For the purpose of discriminative NMT training, we augment these basic models with two modifications: we add a length penalty to avoid short translations, and compose the embeddings of subword units, ra"
P19-1427,P19-1453,1,0.81423,"s, s0 i and we use a margin-based loss: `(s, s0 ) = max(0, δ − cos(g(s), g(s0 )) + cos(g(s), g(t))) 2 In semantic textual similarity the goal is to produce scores that correlate with human judgments on the degree to which two sentences have the same semantics. In embedding based models, including the models used in this paper, the score is produced by the cosine of the two sentence embeddings. 3 We use SentencePiece which is available at https:// github.com/google/sentencepiece. 4 We use 16.77 million paraphrase pairs extracted from the ParaNMT corpus (Wieting and Gimpel, 2018). Recently, in (Wieting et al., 2019) it has been shown that strong performance on semantic similarity tasks can also be achieved using bitext directly without the need for backtranslation. 4345 Model SIM S IMI L E Wieting and Gimpel (2018) BLEU METEOR STS 1st Place STS 2nd Place STS 3rd Place 2012 69.3 70.1 67.8 39.2 53.4 64.8 63.4 64.1 2013 64.1 59.8 62.8 29.5 47.6 62.0 59.1 58.3 2014 77.2 74.7 76.9 42.8 63.7 74.3 74.2 74.3 2015 80.3 79.4 79.8 49.8 68.8 79.0 78.0 77.8 2016 78.6 77.8 76.8 47.4 61.8 77.7 75.7 75.7 tion). However, we found that this favored short sentences. We instead penalize a generated sentence if its length di"
P19-1427,D16-1137,0,0.0755019,"Missing"
P19-1447,K18-1035,0,0.019087,"onstruction model and a matching model. Generative Reconstruction Feature Our reconstruction feature log p(z 7→ x) is a generative model that scores the coherence and adequacy of an MR z using the probability of reproducing the original input utterance x from z. Intuitively, a good candidate MR should adequately encode the semantics of x, leading to high reconstruction score. The idea of using reconstruction as a quality metric is closely related to reconstruction models in auto-encoders (Vincent et al., 2008), and its applications in semi-supervised (Yin et al., 2018b) and weakly supervised (Cheng and Lapata, 2018) semantic parsing, where p(z7→ x) is used to score the quality of sampled MRs in optimization. Similar models have also been applied for pragmatic inference in instruction-following agents for modeling the likelihood of causing the speaker to produce the utterance given an inferred action (Fried et al., 2018), while we use p(z7→x) as one qualitymeasuring feature in our reranker. Specifically, we implement p(z 7→ x) using an attentional sequence-to-sequence network (Luong et al., 2015), which takes as input a tokenized MR z. The network is augmented with a copy mechanism (Gu et al., 2016), allo"
P19-1447,W14-4012,0,0.167035,"Missing"
P19-1447,P16-1004,0,0.0891973,"eration task (Yin et al., 2018a) with reconstruction (z 7→ x) and discriminative matching (x ↔ z) scores. Introduction Semantic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement. A pilot analysis of incorrect predictions from a competitive neural semantic parser, T RAN X (Yin and Neubig, 2018) indicates an obvious issue of incoherence. In the real example in Figure 1, top prediction"
P19-1447,P18-1068,0,0.0460058,"ntic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement. A pilot analysis of incorrect predictions from a competitive neural semantic parser, T RAN X (Yin and Neubig, 2018) indicates an obvious issue of incoherence. In the real example in Figure 1, top prediction z1 is semantically incoherent with the intent expressed in the utterance. Perhaps a more interesting issue is inadequacy —"
P19-1447,N18-1177,0,0.0217596,"encode the semantics of x, leading to high reconstruction score. The idea of using reconstruction as a quality metric is closely related to reconstruction models in auto-encoders (Vincent et al., 2008), and its applications in semi-supervised (Yin et al., 2018b) and weakly supervised (Cheng and Lapata, 2018) semantic parsing, where p(z7→ x) is used to score the quality of sampled MRs in optimization. Similar models have also been applied for pragmatic inference in instruction-following agents for modeling the likelihood of causing the speaker to produce the utterance given an inferred action (Fried et al., 2018), while we use p(z7→x) as one qualitymeasuring feature in our reranker. Specifically, we implement p(z 7→ x) using an attentional sequence-to-sequence network (Luong et al., 2015), which takes as input a tokenized MR z. The network is augmented with a copy mechanism (Gu et al., 2016), allowing out-of-vocabulary variable names (e.g., file name in Figure 1) in z to be directly copied to the utterance x. Discriminative Matching Feature We use a matching model to measure the probability of the input utterance x and a candidate MR z being semantically coherent to each other. Intuitively, for a sema"
P19-1447,P16-1154,0,0.0210992,"heng and Lapata, 2018) semantic parsing, where p(z7→ x) is used to score the quality of sampled MRs in optimization. Similar models have also been applied for pragmatic inference in instruction-following agents for modeling the likelihood of causing the speaker to produce the utterance given an inferred action (Fried et al., 2018), while we use p(z7→x) as one qualitymeasuring feature in our reranker. Specifically, we implement p(z 7→ x) using an attentional sequence-to-sequence network (Luong et al., 2015), which takes as input a tokenized MR z. The network is augmented with a copy mechanism (Gu et al., 2016), allowing out-of-vocabulary variable names (e.g., file name in Figure 1) in z to be directly copied to the utterance x. Discriminative Matching Feature We use a matching model to measure the probability of the input utterance x and a candidate MR z being semantically coherent to each other. Intuitively, for a semantically coherent parse z (e.g., z3 in Figure 1), each sub-piece in z (e.g., 4554 urllib.request.urlretrieve) could coarsely match with a span (e.g., download the file) in the utterance, and vice versa. Motivated by this observation, we implement p(x↔z) as a decomposable attention mo"
P19-1447,D18-1192,0,0.0142994,"of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement. A pilot analysis of incorrect predictions from a competitive neural semantic parser, T RAN X (Yin and Neubig, 2018) indicates an obvious issue of incoherence. In the real example in Figure 1, top prediction z1 is semantically incoherent with the intent expressed in the utterance. Perhaps a more interesting issue is inadequacy — while the predicted"
P19-1447,D16-1244,0,0.107753,"Missing"
P19-1447,P17-1105,0,0.110452,"ction (z 7→ x) and discriminative matching (x ↔ z) scores. Introduction Semantic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement. A pilot analysis of incorrect predictions from a competitive neural semantic parser, T RAN X (Yin and Neubig, 2018) indicates an obvious issue of incoherence. In the real example in Figure 1, top prediction z1 is semantically incoherent with the intent ex"
P19-1447,P06-2034,0,0.252622,"Missing"
P19-1447,P06-2101,0,0.133739,"Missing"
P19-1447,N18-1203,0,0.199822,"Output z3 urllib.request.urlretrieve('url', 'file_name') Figure 1: Illustration of the reranker with a real example from the C O NA L A code generation task (Yin et al., 2018a) with reconstruction (z 7→ x) and discriminative matching (x ↔ z) scores. Introduction Semantic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement. A pilot analysis of incorrect predictions from a competit"
P19-1447,P16-1002,0,0.0267585,"b.request.urlretrieve(str_0) z 7! x : 49.8 z$x: 5.8 Reranker Output z3 urllib.request.urlretrieve('url', 'file_name') Figure 1: Illustration of the reranker with a real example from the C O NA L A code generation task (Yin et al., 2018a) with reconstruction (z 7→ x) and discriminative matching (x ↔ z) scores. Introduction Semantic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement"
P19-1447,D17-1160,0,0.0392226,"inative matching (x ↔ z) scores. Introduction Semantic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement. A pilot analysis of incorrect predictions from a competitive neural semantic parser, T RAN X (Yin and Neubig, 2018) indicates an obvious issue of incoherence. In the real example in Figure 1, top prediction z1 is semantically incoherent with the intent expressed in the utterance. Per"
P19-1447,D14-1135,0,0.0723865,"Missing"
P19-1447,P17-1003,0,0.0527159,".8 z$x: 5.8 Reranker Output z3 urllib.request.urlretrieve('url', 'file_name') Figure 1: Illustration of the reranker with a real example from the C O NA L A code generation task (Yin et al., 2018a) with reconstruction (z 7→ x) and discriminative matching (x ↔ z) scores. Introduction Semantic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement. A pilot analysis of incorrect predict"
P19-1447,P16-1127,0,0.139237,"C O NA L A code generation task (Yin et al., 2018a) with reconstruction (z 7→ x) and discriminative matching (x ↔ z) scores. Introduction Semantic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement. A pilot analysis of incorrect predictions from a competitive neural semantic parser, T RAN X (Yin and Neubig, 2018) indicates an obvious issue of incoherence. In the real example in"
P19-1447,P16-1057,0,0.240155,"Missing"
P19-1447,D15-1166,0,0.0428396,"(Vincent et al., 2008), and its applications in semi-supervised (Yin et al., 2018b) and weakly supervised (Cheng and Lapata, 2018) semantic parsing, where p(z7→ x) is used to score the quality of sampled MRs in optimization. Similar models have also been applied for pragmatic inference in instruction-following agents for modeling the likelihood of causing the speaker to produce the utterance given an inferred action (Fried et al., 2018), while we use p(z7→x) as one qualitymeasuring feature in our reranker. Specifically, we implement p(z 7→ x) using an attentional sequence-to-sequence network (Luong et al., 2015), which takes as input a tokenized MR z. The network is augmented with a copy mechanism (Gu et al., 2016), allowing out-of-vocabulary variable names (e.g., file name in Figure 1) in z to be directly copied to the utterance x. Discriminative Matching Feature We use a matching model to measure the probability of the input utterance x and a candidate MR z being semantically coherent to each other. Intuitively, for a semantically coherent parse z (e.g., z3 in Figure 1), each sub-piece in z (e.g., 4554 urllib.request.urlretrieve) could coarsely match with a span (e.g., download the file) in the utt"
P19-1447,P13-4016,1,0.887222,"Missing"
P19-1447,P17-1041,1,0.797856,", 2018a) with reconstruction (z 7→ x) and discriminative matching (x ↔ z) scores. Introduction Semantic parsing is the task of mapping a natural language utterance into machine executable meaning representations (e.g., Python code). Recent years have witnessed a burgeoning of applying neural network architectures for semantic parsing, from sequence-to-sequence models (Jia and Liang, 2016; Ling et al., 2016; Liang et al., 2017; Suhr et al., 2018), to more complex parsing paradigms guided by the structured topologies of target meaning representations (Xiao et al. (2016); Dong and Lapata (2016); Yin and Neubig (2017); Rabinovich et al. (2017); Krishnamurthy et al. (2017); Zhong et al. (2017); Dong and Lapata (2018); Iyer et al. (2018), inter alia). 1 json.loads(['url', 'file_name', 'file_name']) z 7! x : 34.7 z $ x : 0.8 … 1 System Predictions (n-best list of MRs) z1 While neural network-based semantic parsers have achieved impressive results, there is still room for improvement. A pilot analysis of incorrect predictions from a competitive neural semantic parser, T RAN X (Yin and Neubig, 2018) indicates an obvious issue of incoherence. In the real example in Figure 1, top prediction z1 is semantically inc"
P19-1447,D18-2002,1,0.882152,"Missing"
P19-1447,P02-1038,0,0.0185417,"s between each pair of tokens in x and z are computed using attention; (2) a comparison step, where a set of representations are produced from embeddings of pairwise aligned tokens, capturing their semantic similarities; and (3) an aggregation step, where all pairwise comparisons results are combined to compute the semantic coherence score. Token Count Feature Besides the two primary features introduced above, we also include an auxiliary token count feature |z |of an MR, which has been shown useful in preventing a machine translation model from favoring shorter predictions (Cho et al., 2014; Och and Ney, 2002), while we test them for reranking MRs, especially when the target metric is BLEU (§ 3). 3 Experiment We test on four semantic parsing and code generation benchmarks: G EO (Zelle and Mooney, 1996) and ATIS (Deborah A. Dahl and Shriber) are two closed-domain semantic parsing datasets. The NL utterances are geographical (G EO) and flight booking (ATIS) inquiries (e.g., What is the latest flight to Boston?). The corresponding MRs are defined in λ-calculus logical forms (e.g., argmax x (and (flight x) (to x boston)) (departure time x))). D JANGO (Oda et al., 2015) is a popular Python code generati"
P19-1447,P18-1070,1,0.869915,"es are given by two external models: a reconstruction model and a matching model. Generative Reconstruction Feature Our reconstruction feature log p(z 7→ x) is a generative model that scores the coherence and adequacy of an MR z using the probability of reproducing the original input utterance x from z. Intuitively, a good candidate MR should adequately encode the semantics of x, leading to high reconstruction score. The idea of using reconstruction as a quality metric is closely related to reconstruction models in auto-encoders (Vincent et al., 2008), and its applications in semi-supervised (Yin et al., 2018b) and weakly supervised (Cheng and Lapata, 2018) semantic parsing, where p(z7→ x) is used to score the quality of sampled MRs in optimization. Similar models have also been applied for pragmatic inference in instruction-following agents for modeling the likelihood of causing the speaker to produce the utterance given an inferred action (Fried et al., 2018), while we use p(z7→x) as one qualitymeasuring feature in our reranker. Specifically, we implement p(z 7→ x) using an attentional sequence-to-sequence network (Luong et al., 2015), which takes as input a tokenized MR z. The network is augmen"
P19-1453,S14-2010,0,0.192289,"Missing"
P19-1453,S16-1081,0,0.10707,"Missing"
P19-1453,S13-1004,0,0.309609,"Missing"
P19-1453,S12-1051,0,0.146732,"overlap share more parameters. We utilize several regularization methods (Wieting and Gimpel, 2017) including dropout (Srivastava et al., 2014) and shuffling the words in the sentence when training the LSTMSP. Additionally, we find that annealing the mega-batch size by increasing it during training improved performance by a significant margin for LSTM-SP. 3 Experiments Experiments are split into two groups. First, we compare training on parallel data to training on back-translated parallel data. We evaluate these models on the 2012-2016 SemEval Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), which predict the degree to which sentences have the same meaning as measured by human judges. The evaluation metric is Pearson’s r with the gold labels. We use the small STS English-English dataset from Cer et al. (2017) for model selection. Second, we compare our best model, SP, on two semantic crosslingual tasks: the 2017 SemEval STS task (Cer et al., 2017) which consists of monolingual and cross-lingual datasets and the 2018 Building and Using Parallel Corpora (BUCC) shared bitext mining task (Zweigenbaum et al., 2018). 3.1 Hyperparameters and Optimization Unless"
P19-1453,N06-1003,0,0.0530907,"semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at http"
P19-1453,S17-2001,0,0.187252,"e additional benefit of creating cross-lingual representations that are useful for tasks such as mining or filtering parallel data and cross-lingual retrieval. We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the timeconsuming intermediate step of creating paraphrase corpora. Further, we show that the resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006;"
P19-1453,D17-1070,0,0.039622,"ets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly be"
P19-1453,P11-2117,0,0.0379629,"he resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence"
P19-1453,C04-1051,0,0.257736,"data and cross-lingual retrieval. We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the timeconsuming intermediate step of creating paraphrase corpora. Further, we show that the resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018"
P19-1453,N13-1092,0,0.0967382,"Missing"
P19-1453,C18-1122,0,0.06052,"Missing"
P19-1453,W18-6317,0,0.237718,"In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when applying these representations for mining or filtering large amounts of bitext. Our approach forms the simplest me"
P19-1453,N16-1162,0,0.095929,"Missing"
P19-1453,D18-2012,0,0.0362043,"current pair. However, in the bilingual case, negative examples are only selected from the sentences in the batch from the opposing language. To select difficult negative examples that aid training, we use the mega-batching procedure of Wieting and Gimpel (2018), which aggregates M mini-batches to create one mega-batch and selects negative examples therefrom. Once each pair in the megabatch has a negative example, the mega-batch is split back up into M mini-batches for training. Encoders. Our primary sentence encoder simply averages the embeddings of subword units generated by sentencepiece (Kudo and Richardson, 2018); we refer to it as SP. This means that the sentence piece embeddings themselves are the only learned parameters of this model. As baselines we explore averaging character trigrams (T RIGRAM) (Wieting et al., 2016a) and words (W ORD). SP provides a compromise between averaging words and character trigrams, combining the more distinct semantic units of words with the coverage of character trigrams. We also use a bidirectional LSTM encoder (Hochreiter and Schmidhuber, 1997), with LSTM parameters fully shared between languages , as well as LSTM-SP, which uses sentence pieces instead of words as t"
P19-1453,D17-1126,0,0.0250789,"ross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015"
P19-1453,L16-1147,0,0.0418628,"ell et al. (2017). We included correlations for all languages as well as those with low and high SP overlap with English. 0.50 0.45 0.1 0.2 0.3 0.4 SP Overlap 0.5 0.6 Figure 1: Plot of average performance on the 20122016 STS tasks compared to SP overlap and language distance as defined by Littell et al. (2017). 4.2 SP Ovl. 71.5 23.6 18.5 Does Language Choice Matter? We next investigate the impact of the non-English language in the bitext when training English paraphrastic sentence embeddings. We took all 46 languages with at least 100k parallel sentence pairs in the 2016 OpenSubtitles Corpus (Lison and Tiedemann, 2016) and made a plot of their average STS performance on the 2012-2016 English datasets compared to their SP overlap6 and language distance.7 We segmented the languages separately and trained the models for 10 epochs using the 2017 STS task for model selection. The plot, shown in Figure 1, shows that SentencePieces (SP) overlap is highly correlated with STS score. There are also two clusters in the plot, languages that have a similar alphabet to English and those that do not. In each cluster we find that performance is negatively correlated with language distance. Therefore, languages similar to E"
P19-1453,E17-2002,0,0.141406,"ng that SP is hundreds of times faster. 4605 Model All Lang. Lang. (SP Ovl. ≤ 0.3) Lang. (SP Ovl. &gt; 0.3) Language Similarity Vs. Performance 0.75 indmsa Avg. STS Pearson&apos;s r 0.64 0.63 0.62 0.61 0.60 0.59 tha kor zho 0.70 hun glgron deu swedan ita spa boscat vie turpolnldpor fra norhrv srp ces est slk fin slv sqi lit eus isl lav bul ell heb rus ukr ara fas sinjpn mkd mal kat 0.65 Language Distance 0.65 0.60 0.55 Lang. Distance -22.8 -63.8 -34.2 Table 5: Spearman’s ρ × 100 between average performance on the 2012-2016 STS tasks compared to SP overlap (SP Ovl.) and language distance as defined by Littell et al. (2017). We included correlations for all languages as well as those with low and high SP overlap with English. 0.50 0.45 0.1 0.2 0.3 0.4 SP Overlap 0.5 0.6 Figure 1: Plot of average performance on the 20122016 STS tasks compared to SP overlap and language distance as defined by Littell et al. (2017). 4.2 SP Ovl. 71.5 23.6 18.5 Does Language Choice Matter? We next investigate the impact of the non-English language in the bitext when training English paraphrastic sentence embeddings. We took all 46 languages with at least 100k parallel sentence pairs in the 2016 OpenSubtitles Corpus (Lison and Tiedema"
P19-1453,P15-1094,0,0.0479643,"Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature ha"
P19-1453,W17-2619,0,0.0335664,"and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when app"
P19-1453,P18-2035,0,0.017455,"s. 2 In fact, we show that for monolingual similarity, we can devise random encoders that outperform some of this work. 4602 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4602–4608 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Learning Sentence Embeddings We first describe our objective function and then describe our encoder, in addition to several baseline encoders. The methodology proposed here borrows much from past work (Wieting and Gimpel, 2018; Guo et al., 2018; Gr´egoire and Langlais, 2018; Singla et al., 2018), but this specific combination has not been explored and, as we show in experiments, is surprisingly effective. Training. The training data consists of a sequence of parallel sentence pairs (si , ti ) in source and target languages respectively. For each sentence pair, we randomly choose a negative target sentence t0i during training that is not a translation of si . Our objective is to have source and target sentences be more similar than source and negative target examples by a margin δ: i Xh min δ−fθ (si , ti ) + fθ (s, t0i )) . θsrc ,θtgt i + The similarity function is defined as:   fθ"
P19-1453,D16-1157,1,0.904892,"ese intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when applying these representations for mining or filtering large amounts of bitext. Our approach forms the simplest method to date that is able to achieve state-of-the-art results on multiple monolingual and cross-lingual semantic textual similarity (STS) and parallel corpora mining tasks.2 Lastly, since bitext is available for so many language"
P19-1453,P17-1190,1,0.866482,"words and character trigrams, combining the more distinct semantic units of words with the coverage of character trigrams. We also use a bidirectional LSTM encoder (Hochreiter and Schmidhuber, 1997), with LSTM parameters fully shared between languages , as well as LSTM-SP, which uses sentence pieces instead of words as the input tokens. For all encoders, when the vocabularies of source and target languages overlap, the corresponding encoder embedding parameters are shared. As a result, languages pairs with more lexical overlap share more parameters. We utilize several regularization methods (Wieting and Gimpel, 2017) including dropout (Srivastava et al., 2014) and shuffling the words in the sentence when training the LSTMSP. Additionally, we find that annealing the mega-batch size by increasing it during training improved performance by a significant margin for LSTM-SP. 3 Experiments Experiments are split into two groups. First, we compare training on parallel data to training on back-translated parallel data. We evaluate these models on the 2012-2016 SemEval Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), which predict the degree to which sentences have the s"
P19-1453,P18-1042,1,0.918867,"irs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from n"
P19-1453,D17-1026,1,0.872931,"asets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on model"
P19-1453,P18-2037,0,0.407958,"their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when applying these repr"
P19-1523,P18-2065,0,0.457549,"ary model t+1 classification loss extractions merge extractions up to t+1 Figure 1: Iterative rank-aware learning. Introduction Open information extraction (IE, Sekine (2006); Banko et al. (2007)) aims to extract open-domain assertions represented in the form of n-tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rulebased (Fader et al., 2011) and syntax-driven systems (Mausam et al., 2012; Corro and Gemulla, 2013), and recently has used neural networks for supervised learning (Stanovsky et al., 2018; Cui et al., 2018; Sun et al., 2018; Duh et al., 2017; Jia et al., 2018). A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on tradeoffs between the precision and recall of extracted 1 Code and data are available at https://github. com/jzbjyb/oie_rank assertions. For instance, an open IE-powered medical question answering (QA) system may require its assertions in higher precision (and consequently lower recall) than QA systems for other domains. For supervised open IE systems, the confid"
P19-1523,E17-2011,0,0.177624,"Missing"
P19-1523,D11-1142,0,0.442276,"ed as training samples to help the model learn from trial and error. Experiments on OIE2016 demonstrate the effectiveness of our method.1 1 generate extractions up to t minimize binary model t+1 classification loss extractions merge extractions up to t+1 Figure 1: Iterative rank-aware learning. Introduction Open information extraction (IE, Sekine (2006); Banko et al. (2007)) aims to extract open-domain assertions represented in the form of n-tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rulebased (Fader et al., 2011) and syntax-driven systems (Mausam et al., 2012; Corro and Gemulla, 2013), and recently has used neural networks for supervised learning (Stanovsky et al., 2018; Cui et al., 2018; Sun et al., 2018; Duh et al., 2017; Jia et al., 2018). A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on tradeoffs between the precision and recall of extracted 1 Code and data are available at https://github. com/jzbjyb/oie_rank assertions. For instance, an open IE-powered medical question"
P19-1523,W18-2501,0,0.0326713,"Missing"
P19-1523,P17-1044,0,0.0174974,"del given a sentence associated with a predicate of interest (s, v). At test time, we first identify verbs in the sentence as candidate predicates. Each sentence/predicate pair is fed to the model and extractions are generated from the label sequence. 2.2 Model Architecture and Decoding Our training method in § 3 could potentially be used with any probabilistic open IE model, since we make no assumptions about the model and only the likelihood of the extraction is required for iterative rank-aware learning. As a concrete instantiation in our experiments, we use RnnOIE (Stanovsky et al., 2018; He et al., 2017), a stacked BiLSTM with highway connections (Zhang et al., 2016; Srivastava et al., P (yt |s, v) ∝ exp(Wlabel ht + blabel ), where ht is the hidden state of the last layer. At decoding time, we use the Viterbi algorithm to reject invalid label transitions (He et al., 2017), such as Ba2 followed by Ia1 .2 We use average log probability of the label sequence (Sun et al., 2018) as its confidence:3 P|s| log P (yˆt |s, v) ˆ = t=1 c(s, v, y) . (1) |s| The probability is trained with maximum likelihood estimation (MLE) of the gold extractions. This formulation lacks an explicit concept of cross-sente"
P19-1523,D15-1076,0,0.0940466,"that extractions of higher quality can be added as training samples to continue this training process iteratively. The resulting iterative learning procedure (Alg. 1) incrementally includes extractions generated by the current model as training samples to optimize the binary classification loss to obtain a better model, and this procedure is continued until convergence. 4 4.1 Experiments Experimental Settings Dataset We use the OIE2016 dataset (Stanovsky and Dagan, 2016) to evaluate our method, which only contains verbal predicates. OIE2016 is automatically generated from the QA-SRL dataset (He et al., 2015), and to remove noise, we remove Implementation Details Our implementation is based on AllenNLP (Gardner et al., 2018) by adding binary classification loss function on the implementation of RnnOIE.6 The network consists of 4 BiLSTM layers (2 forward and 2 backward) with 64-dimensional hidden units. ELMo (Peters et al., 2018) is used to map words into contextualized embeddings, which are concatenated with a 100-dimensional predicate indicator embedding. The recurrent dropout probability is set to 0.1. Adadelta (Zeiler, 2012) with  = 10−6 and ρ = 0.95 and mini-batches of size 80 are used to opt"
P19-1523,D12-1048,0,0.399483,"from trial and error. Experiments on OIE2016 demonstrate the effectiveness of our method.1 1 generate extractions up to t minimize binary model t+1 classification loss extractions merge extractions up to t+1 Figure 1: Iterative rank-aware learning. Introduction Open information extraction (IE, Sekine (2006); Banko et al. (2007)) aims to extract open-domain assertions represented in the form of n-tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rulebased (Fader et al., 2011) and syntax-driven systems (Mausam et al., 2012; Corro and Gemulla, 2013), and recently has used neural networks for supervised learning (Stanovsky et al., 2018; Cui et al., 2018; Sun et al., 2018; Duh et al., 2017; Jia et al., 2018). A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on tradeoffs between the precision and recall of extracted 1 Code and data are available at https://github. com/jzbjyb/oie_rank assertions. For instance, an open IE-powered medical question answering (QA) system may require its assertion"
P19-1523,N18-1202,0,0.0099083,"and this procedure is continued until convergence. 4 4.1 Experiments Experimental Settings Dataset We use the OIE2016 dataset (Stanovsky and Dagan, 2016) to evaluate our method, which only contains verbal predicates. OIE2016 is automatically generated from the QA-SRL dataset (He et al., 2015), and to remove noise, we remove Implementation Details Our implementation is based on AllenNLP (Gardner et al., 2018) by adding binary classification loss function on the implementation of RnnOIE.6 The network consists of 4 BiLSTM layers (2 forward and 2 backward) with 64-dimensional hidden units. ELMo (Peters et al., 2018) is used to map words into contextualized embeddings, which are concatenated with a 100-dimensional predicate indicator embedding. The recurrent dropout probability is set to 0.1. Adadelta (Zeiler, 2012) with  = 10−6 and ρ = 0.95 and mini-batches of size 80 are used to optimize the parameters. Beam search size is 5. 4.2 Evaluation Results Tab. 4 lists the evaluation results. Our base model (RnnOIE, § 2) performs better than non-neural systems, confirming the advantage of supervised training under the sequence labeling setting. To test if the binary classification loss (E.q. 2, § 3) could yiel"
P19-1523,P06-2094,0,0.187648,"racted from different sentences. We propose an additional binary classification loss to calibrate the likelihood to make it more globally comparable, and an iterative learning process, where extractions generated by the open IE model are incrementally included as training samples to help the model learn from trial and error. Experiments on OIE2016 demonstrate the effectiveness of our method.1 1 generate extractions up to t minimize binary model t+1 classification loss extractions merge extractions up to t+1 Figure 1: Iterative rank-aware learning. Introduction Open information extraction (IE, Sekine (2006); Banko et al. (2007)) aims to extract open-domain assertions represented in the form of n-tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rulebased (Fader et al., 2011) and syntax-driven systems (Mausam et al., 2012; Corro and Gemulla, 2013), and recently has used neural networks for supervised learning (Stanovsky et al., 2018; Cui et al., 2018; Sun et al., 2018; Duh et al., 2017; Jia et al., 2018). A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on th"
P19-1523,D16-1252,0,0.736311,"actions and decrease those of incorrect ones. Without adding additional model components, this training paradigm naturally leads to a better open IE model, whose extractions can be further included as training samples. We further propose an iter5295 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5295–5300 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ative learning procedure that gradually improves the model by incrementally adding extractions to the training data. Experiments on the OIE2016 dataset (Stanovsky and Dagan, 2016) indicate that our method significantly outperforms both neural and non-neural models. 2 2015) and recurrent dropout (Gal and Ghahramani, 2016). Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: xt = [Wemb (wt ), Wmask (wt = v)]. The probability of the label at each position is calculated independently using a softmax function: Neural Models for Open IE We briefly revisit the formulation of open IE and the neural network model used in our paper. 2.1 Problem Formulation Given sentence s = (w1 , w2 , ..., wn ), the goal of"
P19-1523,N18-1081,0,0.592225,"ons up to t minimize binary model t+1 classification loss extractions merge extractions up to t+1 Figure 1: Iterative rank-aware learning. Introduction Open information extraction (IE, Sekine (2006); Banko et al. (2007)) aims to extract open-domain assertions represented in the form of n-tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rulebased (Fader et al., 2011) and syntax-driven systems (Mausam et al., 2012; Corro and Gemulla, 2013), and recently has used neural networks for supervised learning (Stanovsky et al., 2018; Cui et al., 2018; Sun et al., 2018; Duh et al., 2017; Jia et al., 2018). A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on tradeoffs between the precision and recall of extracted 1 Code and data are available at https://github. com/jzbjyb/oie_rank assertions. For instance, an open IE-powered medical question answering (QA) system may require its assertions in higher precision (and consequently lower recall) than QA systems for other domains. For supervised open IE s"
P19-1579,N19-1388,0,0.0613493,"790 4 BLEU for X )ENG Training Data AZE (TUR) BEL GLG SLK (RUS) (POR) (CES) 12.89 12.78 18.71 21.73 31.16 30.65 29.16 29.54 11.83 0.47 16.34 0.18 29.51 1.15 28.12 0.75 11.84 12.46 15.72 16.40 29.19 30.07 29.79 30.60 (supervised MT) (unsupervised MT) (word subst.) (modified UMT) 11.92 11.86 14.87 14.72 15.24 15.79 13.83 23.56 23.31 24.25 29.91 29.80 32.02 32.27 32.30 28.52 28.69 29.60 29.55 30.00 (word subst.) (modified UMT) 14.18 13.71 21.74 19.94 31.72 31.39 30.90 30.22 (word subst.) 15.74 24.51 33.16 32.07 15.91 23.69 32.55 31.58 Results from Literature SDE (Wang et al., 2019) many-to-many (Aharoni et al., 2019) Standard NMT 1 {SLE SHE , TLE THE } 2 {ML , ME } (supervised MT) (unsupervised MT) Standard Supervised Back-translation 3 + {SˆEs )L , ME } 4 + {SˆEs )H , ME } Augmentation from HRL-ENG 5 + {SˆHs )L , THE } 6 + {SˆHu )L , THE } 7 + {SˆHw)L , THE } 8 + {SˆHm)L , THE } 9 + {SˆHw)L SˆHm)L , THE THE } Augmention from ENG by pivoting 10 + {SˆEw)H )L , ME } 11 + {SˆEm)H )L , ME } Combinations 12 + {SˆHw)L SˆEw)H )L , THE ME } + {SˆHw)L SˆHm)L , THE THE } 13 + {SˆEw)H )L SˆEm)H )L , ME ME } Table 2: Evaluation of translation performance over four language pairs. Rows 1 and 2 show pre-training BLEU s"
P19-1579,D18-1549,0,0.393947,", THE } and {SˆEw)H )L , ME } where w denotes augmentation with word substitution. 3.2 Augmentation with Unsupervised MT Although we assume LRL and HRL to be similar with regards to word morphology and word order, the simple word-by-word augmentation process will almost certainly be insufficient to completely replicate actual LRL data. A natural next step is to further convert the pseudo-LRL data into a version closer to the real LRL. In order to achieve this in our limited-resource setting, we propose to use unsupervised machine translation (UMT). UMT Unsupervised Neural Machine Translation (Artetxe et al., 2018; Lample et al., 2018a,c) makes it possible to translate between languages without parallel data. This is done by coupling denoising auto-encoding, iterative back-translation, and shared representations of both encoders and decoders, making it possible for the model to extend the initial naive word-to-word mapping into learning to translate longer sentences. Initial studies of UMT have focused on data-rich, morphologically simple languages like English and French. Applying the UMT framework to lowresource and morphologically rich languages is largely unexplored, with the exception of Neubig an"
P19-1579,D19-1632,0,0.0482938,"2018a,c) makes it possible to translate between languages without parallel data. This is done by coupling denoising auto-encoding, iterative back-translation, and shared representations of both encoders and decoders, making it possible for the model to extend the initial naive word-to-word mapping into learning to translate longer sentences. Initial studies of UMT have focused on data-rich, morphologically simple languages like English and French. Applying the UMT framework to lowresource and morphologically rich languages is largely unexplored, with the exception of Neubig and Hu (2018) and Guzmán et al. (2019), showing that UMT performs exceptionally poorly between dissimilar language pairs with BLEU scores lower than 1. The problem is naturally harder for morphologically rich LRLs due to two reasons. First, morphologically rich languages have a higher proportions of infrequent words (Chahuneau et al., 2013). Second, even though still larger than the respective parallel datasets, the size of monolingual datasets in these languages is much smaller compared to HRLs. Modified Initialization As pointed out in Lample et al. (2018c), a good initialization plays a critical role in training NMT in an unsup"
P19-1579,D13-1174,0,0.0307241,"nto learning to translate longer sentences. Initial studies of UMT have focused on data-rich, morphologically simple languages like English and French. Applying the UMT framework to lowresource and morphologically rich languages is largely unexplored, with the exception of Neubig and Hu (2018) and Guzmán et al. (2019), showing that UMT performs exceptionally poorly between dissimilar language pairs with BLEU scores lower than 1. The problem is naturally harder for morphologically rich LRLs due to two reasons. First, morphologically rich languages have a higher proportions of infrequent words (Chahuneau et al., 2013). Second, even though still larger than the respective parallel datasets, the size of monolingual datasets in these languages is much smaller compared to HRLs. Modified Initialization As pointed out in Lample et al. (2018c), a good initialization plays a critical role in training NMT in an unsupervised fashion. Previously explored initialization methods include: 1) word-for-word translation with an induced dictionary to create synthetic sentence pairs for initial training (Lample et al., 2018a; Artetxe et al., 2018); 2) joint Byte-Pair-Encoding (BPE) for both the source and target corpus sides"
P19-1579,P17-1176,0,0.0305798,"ed entity recognition (Xie et al., 2018) or information retrieval (Litschko et al., 2018). Hassan et al. (2017) synthesized data with word embeddings for spoken dialect translation, with a process that requires a LRL - ENG as well as a HRL - LRL dictionary, while our work only uses a HRL - LRL dictionary. Bridging source and target languages through a pivot language was originally proposed for phrasebased MT (De Gispert and Marino, 2006; Cohn and Lapata, 2007). It was later adapted for Neural MT (Levinboim and Chiang, 2015), and Cheng et al. (2017) proposed joint training for pivot-based NMT. Chen et al. (2017) proposed to use an existing pivottarget NMT model to guide the training of sourcetarget model. Lakew et al. (2018) proposed an iterative procedure to realize zero-shot translation by pivoting on a third language. 7 Conclusion We propose a generalized data augmentation framework for low-resource translation, making best use of all available resources. We propose an effective two-step pivoting augmentation method to convert HRL parallel data to LRL . In future work, we will explore methods for controlling the induced dictionary quality to improve word substitution as well as M - UMT . We will a"
P19-1579,P17-4012,0,0.028526,"ntation model for English is trained on English monolingual data only. We set the vocabulary size for each model to 20K. All data are then segmented by their respective segmentation model. We use FastText4 to train word embeddings using ML and MH with a dimension of 256 (used for the dictionary induction step). We also pre-train subword level embeddings on the segmented ML , ˆ and MH with the same dimension. M L 4.3 Model Architecture Supervised NMT We use the self-attention Transformer model (Vaswani et al., 2017). We adapt the implementation from the open-source translation toolkit OpenNMT (Klein et al., 2017). Both encoder and decoder consist of 4 layers, with the word embedding and hidden unit dimensions set to 256. 5 We use a batch size of 8096 tokens. Unsupervised NMT We train unsupervised Transformer models with the UnsupervisedMT toolkit.6 Layer sizes and dimensions are the same as in the supervised NMT model. The parameters of the first three layers of the encoder and the decoder are shared. The embedding layers are initialized with the pre-trained subword embeddings from monolingual data. We set the weight parameters for autodenoising language modeling and iterative back translation as λ1 ="
P19-1579,P17-2061,0,0.0196634,"Hm)L , THE THE } Augmention from ENG by pivoting 10 + {SˆEw)H )L , ME } 11 + {SˆEm)H )L , ME } Combinations 12 + {SˆHw)L SˆEw)H )L , THE ME } + {SˆHw)L SˆHm)L , THE THE } 13 + {SˆEw)H )L SˆEm)H )L , ME ME } Table 2: Evaluation of translation performance over four language pairs. Rows 1 and 2 show pre-training BLEU scores. Rows 3–13 show scores after fine tuning. Statistically significantly best scores are highlighted (p &lt; 0.05). 2016; Nguyen and Chiang, 2017). We first train a base NMT model on the concatenation of {SLE , TLE } and {SHE , THE }. Then we adopt the mixed fine-tuning strategy of Chu et al. (2017), fine-tuning the base model on the concatenation of the base and augmented datasets. For each setting, we perform a sufficient number of updates to reach convergence in terms of development perplexity. We use the performance on the development sets (as provided by the TED corpus) as our criterion for selecting the best model, both for augmentation and final model training. 5 Results and Analysis A collection of our results with the baseline and our proposed methods is shown in Table 2. 5.1 Baselines The performance of the base supervised model (row 1) varies from 11.8 to 29.5 BLEU points. Gen"
P19-1579,P07-1092,0,0.0580969,"018; Zhang et al., 2018). Bilingual dictionaries learned in both supervised and unsupervised ways have been used in lowresource settings for tasks such as named entity recognition (Xie et al., 2018) or information retrieval (Litschko et al., 2018). Hassan et al. (2017) synthesized data with word embeddings for spoken dialect translation, with a process that requires a LRL - ENG as well as a HRL - LRL dictionary, while our work only uses a HRL - LRL dictionary. Bridging source and target languages through a pivot language was originally proposed for phrasebased MT (De Gispert and Marino, 2006; Cohn and Lapata, 2007). It was later adapted for Neural MT (Levinboim and Chiang, 2015), and Cheng et al. (2017) proposed joint training for pivot-based NMT. Chen et al. (2017) proposed to use an existing pivottarget NMT model to guide the training of sourcetarget model. Lakew et al. (2018) proposed an iterative procedure to realize zero-shot translation by pivoting on a third language. 7 Conclusion We propose a generalized data augmentation framework for low-resource translation, making best use of all available resources. We propose an effective two-step pivoting augmentation method to convert HRL parallel data t"
P19-1579,W17-3204,0,0.0280161,"esource language (LRL) and a related high-resource language (HRL), typical data augmentation scenarios use any available parallel data [b] and [c] to back-translate English monolingual data [a] and generate parallel resources ([1] and [2]). We additionally propose scenarios [3] and [4], where we pivot through HRL in order to generate a LRL–ENG resource. Introduction The task of Machine Translation (MT) for low resource languages (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large impr"
P19-1579,W17-4715,0,0.0482957,"glish. We propose methods to create pseudo-parallel LRL data in this setting. As illustrated in Figure 1, we augment parallel data via two main methods: 1) back-translating from ENG to LRL or HRL; 2) converting the HRL-ENG dataset to a pseudo LRL-ENG dataset. In the first thread, we focus on creating new parallel sentences through back-translation. Backtranslating from the target language to the source (Sennrich et al., 2016) is a common practice in data augmentation, but has also been shown to be less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017). As a way to ameliorate this problem, we examine methods to instead translate from the target language to a highly-related HRL, which remains unexplored in the context of low-resource NMT. This pseudo-HRL-ENG dataset can then be used for joint training with the LRL-ENG dataset. In the second thread, we focus on converting an HRL - ENG dataset to a pseudo- LRL -to- ENG dataset that better approximates the true LRL data. Converting between HRLs and LRLs also suffers from lack of resources, but because the LRL and HRL are related, this is an easier task that we argue can be done to some extent b"
P19-1579,D15-1126,0,0.0239984,"both supervised and unsupervised ways have been used in lowresource settings for tasks such as named entity recognition (Xie et al., 2018) or information retrieval (Litschko et al., 2018). Hassan et al. (2017) synthesized data with word embeddings for spoken dialect translation, with a process that requires a LRL - ENG as well as a HRL - LRL dictionary, while our work only uses a HRL - LRL dictionary. Bridging source and target languages through a pivot language was originally proposed for phrasebased MT (De Gispert and Marino, 2006; Cohn and Lapata, 2007). It was later adapted for Neural MT (Levinboim and Chiang, 2015), and Cheng et al. (2017) proposed joint training for pivot-based NMT. Chen et al. (2017) proposed to use an existing pivottarget NMT model to guide the training of sourcetarget model. Lakew et al. (2018) proposed an iterative procedure to realize zero-shot translation by pivoting on a third language. 7 Conclusion We propose a generalized data augmentation framework for low-resource translation, making best use of all available resources. We propose an effective two-step pivoting augmentation method to convert HRL parallel data to LRL . In future work, we will explore methods for controlling t"
P19-1579,N18-1032,0,0.151454,"[3] and [4], where we pivot through HRL in order to generate a LRL–ENG resource. Introduction The task of Machine Translation (MT) for low resource languages (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large improvements can be expected when the HRL shares vocabulary or is syntactically similar with the LRL (Lin et al., 2019). Simple joint training is still not ideal, though, considering that there will still be many words and possibly even syntactic structures that will no"
P19-1579,D18-1103,1,0.939235,"s (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large improvements can be expected when the HRL shares vocabulary or is syntactically similar with the LRL (Lin et al., 2019). Simple joint training is still not ideal, though, considering that there will still be many words and possibly even syntactic structures that will not be shared between the most highly related languages. There are model-based methods that ameliorate the problem through more expressive source-side representati"
P19-1579,I17-2050,0,0.184273,"ionally propose scenarios [3] and [4], where we pivot through HRL in order to generate a LRL–ENG resource. Introduction The task of Machine Translation (MT) for low resource languages (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large improvements can be expected when the HRL shares vocabulary or is syntactically similar with the LRL (Lin et al., 2019). Simple joint training is still not ideal, though, considering that there will still be many words and possibly even syntactic struc"
P19-1579,W18-2711,1,0.81174,"Work Our work is related to multilingual and unsupervised translation, bilingual dictionary induction, as well as approaches for triangulation (pivoting). In a low-resource MT scenario, multilingual training that aims at sharing parameters by leveraging parallel datasets of multiple languages is a common practice. Some works target learning a universal representation for all languages either by leveraging semantic sharing between mapped word embeddings (Gu et al., 2018) or by using character n-gram embeddings (Wang et al., 2019) optimizing subword sharing. More related with data augmentation, Nishimura et al. (2018) fill in missing data with a multi-source setting to boost multilingual translation. Unsupervised machine translation enables training NMT models without parallel data (Artetxe et al., 2018; Lample et al., 2018a,c). Recently, multiple methods have been proposed to further improve the framework. By incorporating a statistical MT system as posterior regularization, Ren et al. (2019) achieved state-of-the-art for en-fr and en-de MT. Besides MT, the framework has also been applied to other unsupervised tasks like nonparallel style transfer (Subramanian et al., 2018; Zhang et al., 2018). Bilingual"
P19-1579,N18-2084,1,0.81657,"sier because the two languages are related. A good example is the agglutinative language of Azerbaijiani, where each word may consist of several morphemes and each morpheme could possibly map to an English word itself. Correspondences to (also agglutinative) Turkish, however, are easier to uncover. To give a concrete example, the Azerbijiani word “dü¸süncәlәrim” can be fairly easily aligned to the Turkish word “dü¸süncelerim” while in English it corresponds to the phrase “my thoughts”, which is unlikely to be perfectly aligned. 4 4.1 Experimental Setup Data We use the multilingual TED corpus (Qi et al., 2018) as a test-bed for evaluating the efficacy of each augmentation method. We conduct extensive experiments over four low-resource languages: Azerbaijani (AZE), Belarusian (BEL), Galician (GLG), and Slovak (SLK), along with their highly related languages Turkish (TUR), Russian (RUS), Portuguese (POR), and Czech (CES) respec4.2 Pre-processing We train a joint sentencepiece3 model for each LRL-HRL pair by concatenating the monolingual corpora of the two languages. The segmentation model for English is trained on English monolingual data only. We set the vocabulary size for each model to 20K. All da"
P19-1579,D18-1034,1,0.843215,"without parallel data (Artetxe et al., 2018; Lample et al., 2018a,c). Recently, multiple methods have been proposed to further improve the framework. By incorporating a statistical MT system as posterior regularization, Ren et al. (2019) achieved state-of-the-art for en-fr and en-de MT. Besides MT, the framework has also been applied to other unsupervised tasks like nonparallel style transfer (Subramanian et al., 2018; Zhang et al., 2018). Bilingual dictionaries learned in both supervised and unsupervised ways have been used in lowresource settings for tasks such as named entity recognition (Xie et al., 2018) or information retrieval (Litschko et al., 2018). Hassan et al. (2017) synthesized data with word embeddings for spoken dialect translation, with a process that requires a LRL - ENG as well as a HRL - LRL dictionary, while our work only uses a HRL - LRL dictionary. Bridging source and target languages through a pivot language was originally proposed for phrasebased MT (De Gispert and Marino, 2006; Cohn and Lapata, 2007). It was later adapted for Neural MT (Levinboim and Chiang, 2015), and Cheng et al. (2017) proposed joint training for pivot-based NMT. Chen et al. (2017) proposed to use an ex"
P19-1579,N15-1104,0,0.0828626,"Missing"
P19-1579,D17-1207,0,0.0197792,"that an LRL and its corresponding HRL can be similar in morphology and word order, in the following sections, we propose methods to convert HRL to LRL for data augmentation in a more reliable way. 3 LRL-HRL Translation Methods In this section, we introduce two methods for converting HRL to LRL for data augmentation. 3.1 Augmentation with Word Substitution Mikolov et al. (2013) show that the word embedding spaces share similar innate structure over different languages, making it possible to induce bilingual dictionaries with a limited amount of or even without parallel data (Xing et al., 2015; Zhang et al., 2017; Lample et al., 2018b). Although the capacity of these methods is naturally constrained by the intrinsic properties of the two mapped languages, it’s more likely to create a high-quality bilingual dictionary for two highly-related languages. Given the induced dictionary, we can substitute HRL words with LRL ones and construct a word-by-word translated pseudo-LRL corpus. Dictionary Induction We use a supervised method to obtain a bilingual dictionary between the two highly-related languages. Following Xing et al. (2015), we formulate the task of finding the optimal mapping between the source a"
P19-1579,D16-1163,0,0.0675711,"and [2]). We additionally propose scenarios [3] and [4], where we pivot through HRL in order to generate a LRL–ENG resource. Introduction The task of Machine Translation (MT) for low resource languages (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large improvements can be expected when the HRL shares vocabulary or is syntactically similar with the LRL (Lin et al., 2019). Simple joint training is still not ideal, though, considering that there will still be many words and poss"
P19-1579,P16-1009,0,0.519111,"ly 28 - August 2, 2019. 2019 Association for Computational Linguistics access to parallel or monolingual data of an LRL of interest, its HRL, and the target language, which we will assume is English. We propose methods to create pseudo-parallel LRL data in this setting. As illustrated in Figure 1, we augment parallel data via two main methods: 1) back-translating from ENG to LRL or HRL; 2) converting the HRL-ENG dataset to a pseudo LRL-ENG dataset. In the first thread, we focus on creating new parallel sentences through back-translation. Backtranslating from the target language to the source (Sennrich et al., 2016) is a common practice in data augmentation, but has also been shown to be less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017). As a way to ameliorate this problem, we examine methods to instead translate from the target language to a highly-related HRL, which remains unexplored in the context of low-resource NMT. This pseudo-HRL-ENG dataset can then be used for joint training with the LRL-ENG dataset. In the second thread, we focus on converting an HRL - ENG dataset to a pseudo- LRL -to- ENG dataset that better approximates the"
P19-1579,Q17-1024,0,\N,Missing
P19-1583,D11-1033,0,0.0627611,"ycles (Neubig and Hu, 2018). In this paper, we go a step further and ask the question: can we design an intelligent data selection strategy that allows us to choose the most relevant multilingual data to further boost NMT performance and training speed for LRLs? Prior work has examined data selection from the view of domain adaptation, selecting good training data from out-of-domain text to improve indomain performance. In general, these methods select data that score above a preset threshold according to some metric, such as the difference between in-domain and out-of-domain language models (Axelrod et al., 2011; Moore and Lewis, 2010) or sentence embedding similarity (Wang et al., 2017). Other works use all the data but weight training instances by domain similarity (Chen et al., 2017), or sample subsets of training data at each epoch (van der Wees et al., 2017). However, none of these methods are trivially applicable to multilingual parallel datasets, which usually contain many different languages from the same domain. Moreover, most of these methods need to pretrain language models or NMT models with a reasonable amount of data, and accuracy can suffer in low-resource settings like those encounter"
P19-1583,W17-3205,0,0.0202212,"nt multilingual data to further boost NMT performance and training speed for LRLs? Prior work has examined data selection from the view of domain adaptation, selecting good training data from out-of-domain text to improve indomain performance. In general, these methods select data that score above a preset threshold according to some metric, such as the difference between in-domain and out-of-domain language models (Axelrod et al., 2011; Moore and Lewis, 2010) or sentence embedding similarity (Wang et al., 2017). Other works use all the data but weight training instances by domain similarity (Chen et al., 2017), or sample subsets of training data at each epoch (van der Wees et al., 2017). However, none of these methods are trivially applicable to multilingual parallel datasets, which usually contain many different languages from the same domain. Moreover, most of these methods need to pretrain language models or NMT models with a reasonable amount of data, and accuracy can suffer in low-resource settings like those encountered for LRLs (Duh et al., 2013). In this paper, we create a mathematical framework for data selection in multilingual MT that selects data from all languages, such that minimizing"
P19-1583,P11-2031,0,0.115038,"Missing"
P19-1583,P13-2119,1,0.655335,"ewis, 2010) or sentence embedding similarity (Wang et al., 2017). Other works use all the data but weight training instances by domain similarity (Chen et al., 2017), or sample subsets of training data at each epoch (van der Wees et al., 2017). However, none of these methods are trivially applicable to multilingual parallel datasets, which usually contain many different languages from the same domain. Moreover, most of these methods need to pretrain language models or NMT models with a reasonable amount of data, and accuracy can suffer in low-resource settings like those encountered for LRLs (Duh et al., 2013). In this paper, we create a mathematical framework for data selection in multilingual MT that selects data from all languages, such that minimizing the training objective over the sampled data approximately minimizes the loss of the LRL MT model. The formulation leads to an simple, efficient, and effective algorithm that first samples a target sentence and then conditionally samples which of several source sentences to use for training. We name the method Target Conditioned Sampling (TCS). We also propose and experiment with several design choices for TCS, which are especially effective for L"
P19-1583,N16-1101,0,0.0322392,"seek to construct a sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose an efficient algorithm, Target Conditioned Sampling (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show that TCS brings significant gains of up to 2 BLEU on three of four languages we test, with minimal training overhead1 . 1 Introduction Multilingual NMT has led to impressive gains in translation accuracy of low-resource languages (LRL) (Zoph et al., 2016; Firat et al., 2016; Gu et al., 2018; Neubig and Hu, 2018; Nguyen and Chiang, 2018). Many real world datasets provide sentences that are multi-parallel, with the same content in a variety of languages. Examples include TED (Qi et al., 2018), Europarl (Koehn, 2005), and many others (Tiedemann, 2012). These datasets open up the tantalizing prospect of training a system on many different languages to improve accuracy, but previous work has found methods that use only a single related (HRL) often out-perform systems trained on all available data (Neubig and Hu, 2018). In addition, because the resulting training corp"
P19-1583,N18-1032,0,0.0173476,"sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose an efficient algorithm, Target Conditioned Sampling (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show that TCS brings significant gains of up to 2 BLEU on three of four languages we test, with minimal training overhead1 . 1 Introduction Multilingual NMT has led to impressive gains in translation accuracy of low-resource languages (LRL) (Zoph et al., 2016; Firat et al., 2016; Gu et al., 2018; Neubig and Hu, 2018; Nguyen and Chiang, 2018). Many real world datasets provide sentences that are multi-parallel, with the same content in a variety of languages. Examples include TED (Qi et al., 2018), Europarl (Koehn, 2005), and many others (Tiedemann, 2012). These datasets open up the tantalizing prospect of training a system on many different languages to improve accuracy, but previous work has found methods that use only a single related (HRL) often out-perform systems trained on all available data (Neubig and Hu, 2018). In addition, because the resulting training corpus is smaller, us"
P19-1583,2005.mtsummit-papers.11,0,0.160052,"ples a target sentence, and then conditionally samples its source sentence. Experiments show that TCS brings significant gains of up to 2 BLEU on three of four languages we test, with minimal training overhead1 . 1 Introduction Multilingual NMT has led to impressive gains in translation accuracy of low-resource languages (LRL) (Zoph et al., 2016; Firat et al., 2016; Gu et al., 2018; Neubig and Hu, 2018; Nguyen and Chiang, 2018). Many real world datasets provide sentences that are multi-parallel, with the same content in a variety of languages. Examples include TED (Qi et al., 2018), Europarl (Koehn, 2005), and many others (Tiedemann, 2012). These datasets open up the tantalizing prospect of training a system on many different languages to improve accuracy, but previous work has found methods that use only a single related (HRL) often out-perform systems trained on all available data (Neubig and Hu, 2018). In addition, because the resulting training corpus is smaller, using a single language is also substantially faster to 1 The code can be found at https://github.com/ cindyxinyiwang/TCS. train, speeding experimental cycles (Neubig and Hu, 2018). In this paper, we go a step further and ask the"
P19-1583,D18-2012,0,0.0396842,"-lang LM-lang TCS-D TCS-S 10.76 11.47∗ 14.97 17.61 27.92 28.53† 28.40 28.56∗ Vocab-sent Vocab-sent TCS-D TCS-S 10.68 11.09† 16.13 16.30 27.29 28.36† 27.03 27.01 Vocab-lang Vocab-lang TCS-D TCS-S 10.58 11.46∗ 16.32 17.79 28.17 29.57∗ 28.27∗ 28.45∗ Table 2: BLEU scores on four languages. Statistical significance (Clark et al., 2011) is indicated with ∗ (p &lt; 0.001) and † (p &lt; 0.05), compared with the best baseline. 3.2 Experiment Settings A standard sequence-to-sequence (Sutskever et al., 2014) NMT model with attention is used for all experiments. Byte Pair Encoding (BPE) (Sennrich et al., 2016; Kudo and Richardson, 2018) with vocabulary size of 8000 is applied for each language individually. Details of other hyperparameters can be found in Appendix A.1. 3.3 Results We test both the Deterministic (TCS-D) and Stochastic (TCS-S) algorithms described in Section 2.4. For each algorithm, we experiment with the similarity measures introduced in Section 2.5. The results are listed in Table 2. Of all the baselines, Bi in general has the best performance, while All, which uses all the data and takes much longer to train, generally hurts the performance. This is consistent with findings in prior work (Neubig and Hu, 201"
P19-1583,P10-2041,0,0.042941,"2018). In this paper, we go a step further and ask the question: can we design an intelligent data selection strategy that allows us to choose the most relevant multilingual data to further boost NMT performance and training speed for LRLs? Prior work has examined data selection from the view of domain adaptation, selecting good training data from out-of-domain text to improve indomain performance. In general, these methods select data that score above a preset threshold according to some metric, such as the difference between in-domain and out-of-domain language models (Axelrod et al., 2011; Moore and Lewis, 2010) or sentence embedding similarity (Wang et al., 2017). Other works use all the data but weight training instances by domain similarity (Chen et al., 2017), or sample subsets of training data at each epoch (van der Wees et al., 2017). However, none of these methods are trivially applicable to multilingual parallel datasets, which usually contain many different languages from the same domain. Moreover, most of these methods need to pretrain language models or NMT models with a reasonable amount of data, and accuracy can suffer in low-resource settings like those encountered for LRLs (Duh et al.,"
P19-1583,D18-1103,1,0.951018,"tion over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose an efficient algorithm, Target Conditioned Sampling (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show that TCS brings significant gains of up to 2 BLEU on three of four languages we test, with minimal training overhead1 . 1 Introduction Multilingual NMT has led to impressive gains in translation accuracy of low-resource languages (LRL) (Zoph et al., 2016; Firat et al., 2016; Gu et al., 2018; Neubig and Hu, 2018; Nguyen and Chiang, 2018). Many real world datasets provide sentences that are multi-parallel, with the same content in a variety of languages. Examples include TED (Qi et al., 2018), Europarl (Koehn, 2005), and many others (Tiedemann, 2012). These datasets open up the tantalizing prospect of training a system on many different languages to improve accuracy, but previous work has found methods that use only a single related (HRL) often out-perform systems trained on all available data (Neubig and Hu, 2018). In addition, because the resulting training corpus is smaller, using a single language"
P19-1583,N18-2084,1,0.918807,"pling (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show that TCS brings significant gains of up to 2 BLEU on three of four languages we test, with minimal training overhead1 . 1 Introduction Multilingual NMT has led to impressive gains in translation accuracy of low-resource languages (LRL) (Zoph et al., 2016; Firat et al., 2016; Gu et al., 2018; Neubig and Hu, 2018; Nguyen and Chiang, 2018). Many real world datasets provide sentences that are multi-parallel, with the same content in a variety of languages. Examples include TED (Qi et al., 2018), Europarl (Koehn, 2005), and many others (Tiedemann, 2012). These datasets open up the tantalizing prospect of training a system on many different languages to improve accuracy, but previous work has found methods that use only a single related (HRL) often out-perform systems trained on all available data (Neubig and Hu, 2018). In addition, because the resulting training corpus is smaller, using a single language is also substantially faster to 1 The code can be found at https://github.com/ cindyxinyiwang/TCS. train, speeding experimental cycles (Neubig and Hu, 2018). In this paper, we go a s"
P19-1583,P16-1162,0,0.0772962,"30.94† 29.35∗ 29.00∗ LM-lang LM-lang TCS-D TCS-S 10.76 11.47∗ 14.97 17.61 27.92 28.53† 28.40 28.56∗ Vocab-sent Vocab-sent TCS-D TCS-S 10.68 11.09† 16.13 16.30 27.29 28.36† 27.03 27.01 Vocab-lang Vocab-lang TCS-D TCS-S 10.58 11.46∗ 16.32 17.79 28.17 29.57∗ 28.27∗ 28.45∗ Table 2: BLEU scores on four languages. Statistical significance (Clark et al., 2011) is indicated with ∗ (p &lt; 0.001) and † (p &lt; 0.05), compared with the best baseline. 3.2 Experiment Settings A standard sequence-to-sequence (Sutskever et al., 2014) NMT model with attention is used for all experiments. Byte Pair Encoding (BPE) (Sennrich et al., 2016; Kudo and Richardson, 2018) with vocabulary size of 8000 is applied for each language individually. Details of other hyperparameters can be found in Appendix A.1. 3.3 Results We test both the Deterministic (TCS-D) and Stochastic (TCS-S) algorithms described in Section 2.4. For each algorithm, we experiment with the similarity measures introduced in Section 2.5. The results are listed in Table 2. Of all the baselines, Bi in general has the best performance, while All, which uses all the data and takes much longer to train, generally hurts the performance. This is consistent with findings in pr"
P19-1583,tiedemann-2012-parallel,0,0.0607413,"n conditionally samples its source sentence. Experiments show that TCS brings significant gains of up to 2 BLEU on three of four languages we test, with minimal training overhead1 . 1 Introduction Multilingual NMT has led to impressive gains in translation accuracy of low-resource languages (LRL) (Zoph et al., 2016; Firat et al., 2016; Gu et al., 2018; Neubig and Hu, 2018; Nguyen and Chiang, 2018). Many real world datasets provide sentences that are multi-parallel, with the same content in a variety of languages. Examples include TED (Qi et al., 2018), Europarl (Koehn, 2005), and many others (Tiedemann, 2012). These datasets open up the tantalizing prospect of training a system on many different languages to improve accuracy, but previous work has found methods that use only a single related (HRL) often out-perform systems trained on all available data (Neubig and Hu, 2018). In addition, because the resulting training corpus is smaller, using a single language is also substantially faster to 1 The code can be found at https://github.com/ cindyxinyiwang/TCS. train, speeding experimental cycles (Neubig and Hu, 2018). In this paper, we go a step further and ask the question: can we design an intellig"
P19-1583,P17-2089,0,0.0198988,"estion: can we design an intelligent data selection strategy that allows us to choose the most relevant multilingual data to further boost NMT performance and training speed for LRLs? Prior work has examined data selection from the view of domain adaptation, selecting good training data from out-of-domain text to improve indomain performance. In general, these methods select data that score above a preset threshold according to some metric, such as the difference between in-domain and out-of-domain language models (Axelrod et al., 2011; Moore and Lewis, 2010) or sentence embedding similarity (Wang et al., 2017). Other works use all the data but weight training instances by domain similarity (Chen et al., 2017), or sample subsets of training data at each epoch (van der Wees et al., 2017). However, none of these methods are trivially applicable to multilingual parallel datasets, which usually contain many different languages from the same domain. Moreover, most of these methods need to pretrain language models or NMT models with a reasonable amount of data, and accuracy can suffer in low-resource settings like those encountered for LRLs (Duh et al., 2013). In this paper, we create a mathematical frame"
P19-1583,D18-1100,1,0.784824,"approximate Ps (Y ). We thus only need to sample y uniformly from the union of all extra data. Choosing Q(X|y). Choosing Q(X|y) to approximate Ps (X|y) is more difficult, and there are a number of methods could be used to do so. To do so, we note that conditioning on the same target y and restricting the support of Ps (X|y) to the 5824 sentences that translate into y in at least one of si t, Ps (X = x|y) simply measures how likely x is in s. We thus define a heuristic function sim(x, s) that approximates the probability that x is a sentence in s, and follow the data augmentation objective in Wang et al. (2018) in defining this probability according to exp (sim(x, s)/τ ) 0 x0 exp (sim(x , s)/τ ) Q∗ (x|y) = P (8) Algorithms The formulation of Q(X, Y ) allows one to sample multilingual data with the following algorithm: 1. Select the target y based on Q(y). In our case we can simply use the uniform distribution. 2. Given the target y, gather all data (xi , y) ∈ s1 , s2 , ...sn -t and calculate sim(xi , s) 3. Sample (xi , y) based on Q(X|y) The algorithm requires calculating Q(X|y) repeatedly during training. To reduce this overhead, we propose two strategies for implementation: 1) Stochastic: compute"
P19-1583,D17-1147,0,0.0517142,"Missing"
P19-1583,D16-1163,0,0.0641141,"In this paper, we seek to construct a sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose an efficient algorithm, Target Conditioned Sampling (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show that TCS brings significant gains of up to 2 BLEU on three of four languages we test, with minimal training overhead1 . 1 Introduction Multilingual NMT has led to impressive gains in translation accuracy of low-resource languages (LRL) (Zoph et al., 2016; Firat et al., 2016; Gu et al., 2018; Neubig and Hu, 2018; Nguyen and Chiang, 2018). Many real world datasets provide sentences that are multi-parallel, with the same content in a variety of languages. Examples include TED (Qi et al., 2018), Europarl (Koehn, 2005), and many others (Tiedemann, 2012). These datasets open up the tantalizing prospect of training a system on many different languages to improve accuracy, but previous work has found methods that use only a single related (HRL) often out-perform systems trained on all available data (Neubig and Hu, 2018). In addition, because the res"
P19-1583,I17-2050,0,\N,Missing
P19-1583,W17-4715,0,\N,Missing
Q14-1014,P13-1004,0,0.0206363,"l., 2011) and many other NLP tasks (Olsson, 2009), to name but a few studies. It has also been recognized by the active learning community that correcting the most useful parts first is often not optimal in terms of efficiency, since these parts tend to be the most difficult to manually annotate (Settles et al., 2008). The authors advocate the use of a user model to predict the supervision effort, and select the instances with best “bang-for-thebuck.” This prediction of supervision effort was successful, and was further refined in other NLP-related studies (Tomanek et al., 2010; Specia, 2011; Cohn and Specia, 2013). Our approach to user modeling using GP regression is inspired by the latter. Most studies on user models consider only supervision effort, while neglecting the accuracy of human annotations. The view on humans as a perfect oracle has been criticized (Donmez and Carbonell, 2008), since human errors are common and can negatively affect supervision utility. Research on human-computer-interaction has identified the modeling of human errors as very difficult (Olson and Olson, 1990), depending on factors such as user experience, cognitive load, user interface design, and 178 fatigue. Nevertheless,"
Q14-1014,P10-2032,0,0.0484789,"Missing"
Q14-1014,N09-1047,0,0.0787448,"t to the number of segmented tokens. We feel that this is acceptable, considering that the time needed for human supervision will likely dominate the computation time, and reasonable approximations can be made as noted in Section 3.2. 6 Relation to Prior Work Efficient supervision strategies have been studied across a variety of NLP-related research areas, and received increasing attention in recent years. Examples include post editing for speech recognition (Sanchez-Cortina et al., 2012), interactive machine translation (Gonz´alez-Rubio et al., 2010), active learning for machine translation (Haffari et al., 2009; Gonz´alez-Rubio et al., 2011) and many other NLP tasks (Olsson, 2009), to name but a few studies. It has also been recognized by the active learning community that correcting the most useful parts first is often not optimal in terms of efficiency, since these parts tend to be the most difficult to manually annotate (Settles et al., 2008). The authors advocate the use of a user model to predict the supervision effort, and select the instances with best “bang-for-thebuck.” This prediction of supervision effort was successful, and was further refined in other NLP-related studies (Tomanek et al."
Q14-1014,I08-7018,0,0.0306138,"be grouped into segments are positions between adjacent characters. 5.2.1 Experimental Setup Neubig et al. (2011) have proposed a pointwise method for Japanese word segmentation that can be trained using partially annotated sentences, which makes it attractive in combination with active learning, as well as our segmentation method. The authors released their method as a software package “KyTea” that we employed in this user study. We used KyTea’s active learning domain adaptation toolkit8 as a baseline. For data, we used the Balanced Corpus of Contemporary Written Japanese (BCCWJ), created by Maekawa (2008), with the internet Q&A subcorpus as in-domain data, and the whitepaper subcorpus as background data, a domain adaptation scenario. Sentences were drawn from the in-domain corpus, and the manually annotated data was then used to train KyTea, along with the pre-annotated background data. The goal (objective function) was to improve KyTea’s classification accuracy on an indomain test set, given a constrained time budget of 30 minutes. There were again 2 supervision modes: ANNOTATE and SKIP . Note that this is essentially a batch active learning setup with only one iteration. We conducted experim"
Q14-1014,2006.iwslt-papers.1,0,0.0655285,"Missing"
Q14-1014,P11-2093,1,0.908647,"Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek and Hahn, 2009; Neubig et al., 2011). In these cases, it is possible to achieve much higher benefit per annotated word by annotating sub-sentential units (Figure 1 (b)). However, as Settles et al. (2008) point out, simply maximizing the benefit per annotated instance is not enough, as the real supervision effort varies 169 Transactions of the Association for Computational Linguistics, 2 (2014) 169–180. Action Editor: Eric Fosler-Lussier. c Submitted 11/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. Avg. time / instance [sec] 6 Transcription task Word segmentation task 4 2 0 1 3 5 7 9 11 1"
Q14-1014,P10-1037,0,0.020503,"n terms of the same monetary unit. Vijayanarasimhan et al. (2010) and Donmez and Carbonell (2008) use a more practical approach that specifies a constrained optimization problem by allowing only a limited time budget for supervision. Our approach is a generalization thereof and allows either specifying an upper bound on the predicted cost, or a lower bound on the predicted utility. The main novelty of our presented approach is the explicit modeling and selection of segments of various sizes, such that annotation efficiency is optimized according to the specified constraints. While some works (Sassano and Kurohashi, 2010; Neubig et al., 2011) have proposed using subsentential segments, we are not aware of any previous work that explicitly optimizes that segmentation. 7 Conclusion We presented a method that can effectively choose a segmentation of a language corpus that optimizes supervision efficiency, considering not only the actual usefulness of each segment, but also the annotation cost. We reported noticeable improvements over strong baselines in two user studies. Future user experiments with more participants would be desirable to verify our observations, and allow further analysis of different factors s"
Q14-1014,D08-1112,0,0.840039,"rors in parentheses. The underlined parts are to be corrected by a human for (a) sentences, (b) words, or (c) the proposed segmentation. Introduction Many natural language processing (NLP) tasks require human supervision to be useful in practice, be it to collect suitable training material or to meet some desired output quality. Given the high cost of human intervention, how to minimize the supervision effort is an important research problem. Previous works in areas such as active learning, post editing, and interactive pattern recognition have investigated this question with notable success (Settles, 2008; Specia, 2011; Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek"
Q14-1014,2011.eamt-1.12,0,0.438874,"eses. The underlined parts are to be corrected by a human for (a) sentences, (b) words, or (c) the proposed segmentation. Introduction Many natural language processing (NLP) tasks require human supervision to be useful in practice, be it to collect suitable training material or to meet some desired output quality. Given the high cost of human intervention, how to minimize the supervision effort is an important research problem. Previous works in areas such as active learning, post editing, and interactive pattern recognition have investigated this question with notable success (Settles, 2008; Specia, 2011; Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek and Hahn, 2009"
Q14-1014,P09-1117,0,0.143235,"es, 2008; Specia, 2011; Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek and Hahn, 2009; Neubig et al., 2011). In these cases, it is possible to achieve much higher benefit per annotated word by annotating sub-sentential units (Figure 1 (b)). However, as Settles et al. (2008) point out, simply maximizing the benefit per annotated instance is not enough, as the real supervision effort varies 169 Transactions of the Association for Computational Linguistics, 2 (2014) 169–180. Action Editor: Eric Fosler-Lussier. c Submitted 11/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. Avg. time / instance [sec] 6 Transcription task Word segmentation tas"
Q14-1014,P10-1118,0,0.0154553,"i et al., 2009; Gonz´alez-Rubio et al., 2011) and many other NLP tasks (Olsson, 2009), to name but a few studies. It has also been recognized by the active learning community that correcting the most useful parts first is often not optimal in terms of efficiency, since these parts tend to be the most difficult to manually annotate (Settles et al., 2008). The authors advocate the use of a user model to predict the supervision effort, and select the instances with best “bang-for-thebuck.” This prediction of supervision effort was successful, and was further refined in other NLP-related studies (Tomanek et al., 2010; Specia, 2011; Cohn and Specia, 2013). Our approach to user modeling using GP regression is inspired by the latter. Most studies on user models consider only supervision effort, while neglecting the accuracy of human annotations. The view on humans as a perfect oracle has been criticized (Donmez and Carbonell, 2008), since human errors are common and can negatively affect supervision utility. Research on human-computer-interaction has identified the modeling of human errors as very difficult (Olson and Olson, 1990), depending on factors such as user experience, cognitive load, user interface"
Q15-1041,D11-1039,0,0.0373913,"Missing"
Q15-1041,P14-1133,0,0.0472303,"semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more canonical forms of these questions, with the canonical NL questions being trivially convertible to an MR. Berant and Liang (2014) extract entities from a full-text question, map these entities into a set of candidate MRs, and generate canonical utterances accordingly. Then the canonical utterance that best paraphrases the input is chosen, thereby outputting the corresponding MR. Our approach is the similar but orthogonal to these works in that we focus on situations where the original user input is underspecified, and try to generate a natural language paraphrase that more explicitly states the user intention for disambiguation purposes. A second difference is that we do not use separate model to do paraphrasing, instea"
Q15-1041,W11-2103,0,0.0148728,"ig, 2013) decoder. Unless otherwise specified, the default settings of the decoder are used. Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-Burch et al., 2011) (News). This data represents standard English text unrelated to questions. Second, we use a part of the question paraphrase data gathered by Fader et al. (2013) (Questions).4 This data consists entirely of questions, and thus is a better representative of the latent questions behind the input queries. Finally, we used the full questions from Geoquery sentences to build the language model, building a different language model for each fold, completely separate from the test set. Table 2 gives the details of each dataset. Data News Questions Geoquery Sent. 44.0M 20.2M 792 Tok. 891M 174M ∼1.6K LM"
Q15-1041,J07-2003,0,0.754042,"rse the ambiguous input with significantly better accuracy. 2 Semantic Parsing using Context Free Grammars As a baseline SP formalism, we follow Wong and Mooney (2006) in casting SP as a problem of translation from a natural language query into its MR. This translation is done using synchronous context free grammars, which we describe in detail in the following sections. 2.1 Synchronous Context Free Grammars Synchronous context free grammars are a generalization of context-free grammars (CFGs) that generate pairs of related strings instead of single strings. Slightly modifying the notation of Chiang (2007), we can formalize SCFG rules as: X → ⟨γs , γt ⟩ (1) where X is a non-terminal and γs and γt are strings of terminals and indexed non-terminals on the source and target side of the grammar. SCFGs have recently come into favor as a tool for statistical machine translation (SMT). In SMT, a synchronous rule could, for example, take the form of: X → ⟨X0 eats X1 , X0 wa X1 wo taberu⟩ (2) where γs is an English string and γt is a Japanese string. Each non-terminal on the right side is indexed, with non-terminals with identical indices corresponding to each-other. Given the SCFG grammar, we can addit"
Q15-1041,P13-1158,0,0.217481,"nguage model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-Burch et al., 2011) (News). This data represents standard English text unrelated to questions. Second, we use a part of the question paraphrase data gathered by Fader et al. (2013) (Questions).4 This data consists entirely of questions, and thus is a better representative of the latent questions behind the input queries. Finally, we used the full questions from Geoquery sentences to build the language model, building a different language model for each fold, completely separate from the test set. Table 2 gives the details of each dataset. Data News Questions Geoquery Sent. 44.0M 20.2M 792 Tok. 891M 174M ∼1.6K LM Size 5.5G 1.5G ∼96K Table 2: Details of the data used to build LMs. In addition, because the Geoquery data is useful but small, for all 3-SCFG systems, we perfo"
Q15-1041,N04-1035,0,0.0247175,"as the natural language query and γt as an MR based on λ calculus. SCFG rules are automatically learned from pairs of sentences with input text and the corresponding MR, where the MR is expressed as a parse tree whose internal nodes are predicates, operators, or quantifiers. In this paper, we follow Li et al. (2013)’s approach 573 to extract a grammar from this parallel data. In this approach, for each pair, statistical word alignment aligns natural language tokens with the corresponding elements in the MR, then according to the alignment, minimal rules are extracted with the GHKM algorithm (Galley et al., 2004; Li et al., 2013). Then, up to k minimal rules are composed to form longer rules (Galley et al., 2006), while considering the relationship between logical variables. Finally, unaligned NL tokens are aligned by attaching them to the highest node in the tree that does not break the consistencies of alignment, as specified in Galley et al. (2006). 2.3 Additional Rules While basic rules extracted above are quite effective in parsing the training data,2 we found several problems when we attempt to parse unseen queries. To make our parser more robust, we add two additional varieties of rules. First"
Q15-1041,P06-1121,0,0.0351764,"ed from pairs of sentences with input text and the corresponding MR, where the MR is expressed as a parse tree whose internal nodes are predicates, operators, or quantifiers. In this paper, we follow Li et al. (2013)’s approach 573 to extract a grammar from this parallel data. In this approach, for each pair, statistical word alignment aligns natural language tokens with the corresponding elements in the MR, then according to the alignment, minimal rules are extracted with the GHKM algorithm (Galley et al., 2004; Li et al., 2013). Then, up to k minimal rules are composed to form longer rules (Galley et al., 2006), while considering the relationship between logical variables. Finally, unaligned NL tokens are aligned by attaching them to the highest node in the tree that does not break the consistencies of alignment, as specified in Galley et al. (2006). 2.3 Additional Rules While basic rules extracted above are quite effective in parsing the training data,2 we found several problems when we attempt to parse unseen queries. To make our parser more robust, we add two additional varieties of rules. First, we add a deletion rule which allows us to delete any arbitrary word w with any head symbol X, formall"
Q15-1041,D11-1108,0,0.0328147,"Missing"
Q15-1041,N13-1092,0,0.0757262,"Missing"
Q15-1041,P09-1069,0,0.299465,"ng et al. (2013) showed that doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2009) and Miller et al. (1994) have also used the guidance of natural language syntax for semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more canonical forms of the"
Q15-1041,N13-1116,0,0.0241653,"Missing"
Q15-1041,W11-2123,0,0.0140013,"the input query into an MR including λ calculus expressions, performing β-reduction to remove the λ function, then firing the query against the database. Before querying the database, we also apply Wong and Mooney (2007)’s type-checking to ensure that all MRs are logically valid. For parsing, we implemented CKY-based parsing of tri-synchronous grammars on top of the Travatar (Neubig, 2013) decoder. Unless otherwise specified, the default settings of the decoder are used. Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-Burch et al., 2011) (News). This data represents standard English text unrelated to questions. Second, we use a part of the question paraphrase data gathered by Fader et al. (2013) (Questions).4 This data consists entirely"
Q15-1041,W04-3250,0,0.158833,"Missing"
Q15-1041,D13-1161,0,0.0432772,"Paraphrasing and Verification Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology, Japan {philip.arthur.om0, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp Abstract player?). Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the original input. This paraphra"
Q15-1041,P11-1060,0,0.0494075,"iguous Input through Paraphrasing and Verification Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology, Japan {philip.arthur.om0, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp Abstract player?). Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the or"
Q15-1041,W07-0716,0,0.0308294,"te model to do paraphrasing, instead using the same model to do paraphrasing and semantic parsing synchronously. This has the advantage of being able to scale more easily to complicated and highly compositional questions such as the ones found in Geoquery. In addition to being useful for semantic parsing, SCFGs have also been used for paraphrasing. A variety of research has used SCFG-based paraphrases for text-to-text generation tasks like sentence compression (Cohn and Lapata, 2009; Ganitkevitch et al., 2011), or expanding the set of reference translations for machine translation evaluation (Madnani et al., 2007). In this paper we have introduced a novel use of 3-way SCFGs that allows us to simultaneously do semantic parsing and text-to-text generation. To our knowledge, this is the first method to parse an underspecified input by trying to reconstruct a more explicit paraphrase of the input and validate 582 the naturalness of the paraphrase to disambiguate the meaning of the original input. 8 Conclusion and Future Work In this paper we introduced a method for constructing a semantic parser for ambiguous input that paraphrases the ambiguous input into a more explicit form, and verifies the correctness"
Q15-1041,J83-1001,0,0.767026,"In addition, Wang et al. (2013) showed that doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2009) and Miller et al. (1994) have also used the guidance of natural language syntax for semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more"
Q15-1041,P94-1004,0,0.166442,"hat doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2009) and Miller et al. (1994) have also used the guidance of natural language syntax for semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more canonical forms of these questions, with the ca"
Q15-1041,P11-1064,1,0.832921,"880 sentences representing natural language questions about U.S. Geography, and their corresponding MRs. 6.1 Setup Data: We use the full Geoquery dataset using the same 10 folds of 792 and 88 test data used by Wong and Mooney (2007). We created keyword queries according to the process described in Section 3. We follow standard procedure of removing punctuation for all natural language text, regardless of whether it is a keyword or full question. We also perform stemming on all natural language text, both in the keyword and question queries. Rule Extraction: Alignment is performed by pialign (Neubig et al., 2011) with the setting forcing one-to-many alignments. The algorithm to extract the tri-synchronous grammar is as discussed in Section 4.2 and maximum size of the rules for composition is 4. Decoding: To query the database, we use prolog queries fired against the Geoquery database. The parsing problem can thus be considered the task of decoding from underspecified natural language 3 We also tried gradient-based optimization methods and large feature sets as in Wong and Mooney (2007) and Li et al. (2013), but the dense feature set and MERT achieved similar results with shorter training time. 577 que"
Q15-1041,N15-1033,1,0.821749,"rd inputs and question paraphrases were available, it is theoretically possible for our proposed method to learn from this data as well. 4 Joint Semantic Parsing and Paraphrasing using Tri-Synchronous Grammars In this section we describe our proposed method to parse underspecified and ungrammatical input while jointly generating a paraphrase that can be used to disambiguate the meaning of the original query. 4.1 Generalized Synchronous Context Free Grammars Before defining the actual parsing framework, we first present a generalization of SCFGs, the nsynchronous context free grammar (n-SCFG) (Neubig et al., 2015). In an n-SCFG, the elementary structures are rewrite rules of n − 1 target sides: X → ⟨γ1 , γ2 , ..., γn ⟩ (4) Grammar r0 QUERY → ⟨CONJ0 , give me the CONJ0 , answer(x1 , CONJ0 )⟩ r1 CONJ → ⟨FORM0 STATE1 , FORM0 in STATE1 , (FORM0 , loc(x1 , x2 ), const(x2 , stateid(STATE1 )))⟩ r2 FORM → ⟨cities, cities, city(x1 )⟩ r3 STATE → ⟨virginia, virginia, virginia⟩ Derivations ⟨QUERY0 , QUERY0 , QUERY0 ⟩ r0 ⇒ ⟨CONJ0 , give me the CONJ0 , answer(x1 , CONJ0 )⟩ r1 ⇒ ⟨FORM2 STATE3 , give me the FORM2 in STATE3 , answer(x1 , (FORM2 , loc(x1 , x2 ), const(x2 , stateid(STATE3 ))) ⟩ r2 ⇒ ⟨cities STATE3 , give"
Q15-1041,P13-4016,1,0.853123,"Li et al. (2013), but the dense feature set and MERT achieved similar results with shorter training time. 577 queries into prolog queries. This is done by performing decoding of the SCFG-based parsing model to translate the input query into an MR including λ calculus expressions, performing β-reduction to remove the λ function, then firing the query against the database. Before querying the database, we also apply Wong and Mooney (2007)’s type-checking to ensure that all MRs are logically valid. For parsing, we implemented CKY-based parsing of tri-synchronous grammars on top of the Travatar (Neubig, 2013) decoder. Unless otherwise specified, the default settings of the decoder are used. Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-"
Q15-1041,J03-1002,0,0.00411021,"the generated paraphrase. • Language Model: Counts the log language model probability of the paraphrase. • Unknown: Counts the number of tokens in the paraphrase that are unknown in the language model. • Paraphrase Length: Counts the number of words in the paraphrase, and can be calculated for each rule as the number of terminals in the paraphrase. This feature helps compensate for the fact that language models prefer shorter sentences. 5.3 Learning Feature Weights Now that we have defined the feature space, we need to optimize the weights. For this we use minimum error rate training (MERT) (Och and Ney, 2003), maximizing the number of correct answers over the entire corpus.3 6 Experiment and Analysis We evaluate our system using the Geoquery corpus (Zelle and Mooney, 1996), which contains 880 sentences representing natural language questions about U.S. Geography, and their corresponding MRs. 6.1 Setup Data: We use the full Geoquery dataset using the same 10 folds of 792 and 88 test data used by Wong and Mooney (2007). We created keyword queries according to the process described in Section 3. We follow standard procedure of removing punctuation for all natural language text, regardless of whether"
Q15-1041,D09-1001,0,0.242932,"Missing"
Q15-1041,C12-1143,0,0.293503,"aning representation via verification using a language model that calculates the probability of each paraphrase.1 Much previous work on SP has focused on the case of answering natural language queries to a database of facts, where the queries generally take the form of full sentences such as “What is the height of Kobe Bryant?” While answering these questions provides an excellent first step to natural language information access, in many cases the input is not a full sentence, but something more underspecified and ungrammatical. For example, this is the case for keyword-based search queries (Sajjad et al., 2012) or short dialogue utterances (Zettlemoyer and Collins, 2007). 1 Introduction Semantic parsing (SP) is the problem of parsing a given natural language (NL) sentence into a meaning representation (MR) conducive to further processing by applications. One of the major challenges in SP stems from the fact that NL is rife with ambiguities. For example, even the simple sentence “Where can we eat a steak in Kobe?” contains syntactic ambiguities (“eat in Kobe” or “steak in Kobe”?), quantifier scope ambiguities (do we all eat one steak, or each eat one steak?), and word sense ambiguities (is Kobe a cit"
Q15-1041,P13-2008,0,0.0256986,"s. Underspecified queries are commonly entered into search engines, leading to large result sets that are difficult for users to navigate (Sajjad et al., 2012). Studies have shown that there are several ways to deal with this problem, including query reformulation, which can fall in the categories of query expansion or query substitution (Shokouhi et al., 2014; Xue and Croft, 2013). Leveling (2010) proposed a paraphrasing method that tries to reconstruct original questions given keyword inputs in the IR context, but did not model this reformulation together with semantic parsing. In addition, Wang et al. (2013) showed that doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2"
Q15-1041,N06-1056,0,0.145141,"uery-logic pairs. First we note that baseline SCFG parser achieves reasonable accuracy on regular questions but when the same method is used with underspecified input, the system accuracy decreases significantly. On the other hand, when incorporating the proposed tri-synchronous 572 grammar to generate paraphrases and verify them with a language model, we find that it is possible to recover the loss of accuracy, resulting in a model that is able to parse the ambiguous input with significantly better accuracy. 2 Semantic Parsing using Context Free Grammars As a baseline SP formalism, we follow Wong and Mooney (2006) in casting SP as a problem of translation from a natural language query into its MR. This translation is done using synchronous context free grammars, which we describe in detail in the following sections. 2.1 Synchronous Context Free Grammars Synchronous context free grammars are a generalization of context-free grammars (CFGs) that generate pairs of related strings instead of single strings. Slightly modifying the notation of Chiang (2007), we can formalize SCFG rules as: X → ⟨γs , γt ⟩ (1) where X is a non-terminal and γs and γt are strings of terminals and indexed non-terminals on the sou"
Q15-1041,P07-1121,0,0.424701,"Semantic Parsing of Ambiguous Input through Paraphrasing and Verification Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology, Japan {philip.arthur.om0, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp Abstract player?). Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less a"
Q15-1041,D07-1071,0,0.183538,"age model that calculates the probability of each paraphrase.1 Much previous work on SP has focused on the case of answering natural language queries to a database of facts, where the queries generally take the form of full sentences such as “What is the height of Kobe Bryant?” While answering these questions provides an excellent first step to natural language information access, in many cases the input is not a full sentence, but something more underspecified and ungrammatical. For example, this is the case for keyword-based search queries (Sajjad et al., 2012) or short dialogue utterances (Zettlemoyer and Collins, 2007). 1 Introduction Semantic parsing (SP) is the problem of parsing a given natural language (NL) sentence into a meaning representation (MR) conducive to further processing by applications. One of the major challenges in SP stems from the fact that NL is rife with ambiguities. For example, even the simple sentence “Where can we eat a steak in Kobe?” contains syntactic ambiguities (“eat in Kobe” or “steak in Kobe”?), quantifier scope ambiguities (do we all eat one steak, or each eat one steak?), and word sense ambiguities (is Kobe a city in Japan; or an NBA basketball 1 Tools to replicate our exp"
Q18-1036,E17-1088,1,0.837355,"chine translation (Bahdanau et al., 2016), abstractive summarization (Chopra et al., 2016), and speech processing (Graves et al., 2013). Similarly, state-of-the-art language models are almost universally based on RNNs, particularly long short-term memory (LSTM) networks (Jozefowicz et al., 2016; Inan et al., 2017; Merity et al., 2016). While powerful, LSTM language models usually do not explicitly model many commonly-accepted linguistic phenomena. As a result, standard models lack linguistically informed inductive biases, potentially limiting their accuracy, particularly in lowdata scenarios (Adams et al., 2017; Koehn and Knowles, 2017). In this work, we present a novel modification to the standard LSTM language modeling framework that allows us to incorporate some varieties of these linguistic intuitions seamlessly: neural lattice language models (§3.1). Neural lattice language models define a lattice over possible paths through a sentence, and maximize the marginal probability over all paths that lead to generating the reference sentence, as shown in Fig. 1. Depending on how we define these paths, we can incorporate different assumptions about how language should be modeled. In the particular inst"
Q18-1036,P17-1151,0,0.0137392,"sub ). The final formula for calculating the probability mass assigned to a specific chunk C is: p(C |ht ; θ) =pmain (C |ht ; θ)+ pmain (<s&gt; |ht ; θ)psub (C |ht ; θsub ). 4.2 Incorporating Polysemous Tokens 4.2.1 Motivation A second shortcoming of current language modeling approaches is that each word is associated with only one embedding. For highly polysemous words, a single embedding may be unable to represent all meanings effectively. There has been past work in word embeddings which has shown that using multiple embeddings for each word is helpful in constructing a useful representation. Athiwaratkun and Wilson (2017) represented each word with a multimodal Gaussian distribution and demonstrated that embeddings of this form were able to outperform more standard skipgram embeddings on word similarity and entailment tasks. Similarly, Chen et al. (2015) incorporate standard skip-gram training into a Gaussian mixture framework and show that this improves performance on several word similarity benchmarks. When a polysemous word is represented using only a single embedding in a language modeling task, the multimodal nature of the true embedding distribution may causes the resulting embedding to be both high-vari"
Q18-1036,W17-3204,0,0.0346021,"ahdanau et al., 2016), abstractive summarization (Chopra et al., 2016), and speech processing (Graves et al., 2013). Similarly, state-of-the-art language models are almost universally based on RNNs, particularly long short-term memory (LSTM) networks (Jozefowicz et al., 2016; Inan et al., 2017; Merity et al., 2016). While powerful, LSTM language models usually do not explicitly model many commonly-accepted linguistic phenomena. As a result, standard models lack linguistically informed inductive biases, potentially limiting their accuracy, particularly in lowdata scenarios (Adams et al., 2017; Koehn and Knowles, 2017). In this work, we present a novel modification to the standard LSTM language modeling framework that allows us to incorporate some varieties of these linguistic intuitions seamlessly: neural lattice language models (§3.1). Neural lattice language models define a lattice over possible paths through a sentence, and maximize the marginal probability over all paths that lead to generating the reference sentence, as shown in Fig. 1. Depending on how we define these paths, we can incorporate different assumptions about how language should be modeled. In the particular instantiations of neural latti"
Q18-1036,D14-1113,0,0.0354269,"y For our polysemy experiments, the underlying lattices are multi-lattices: lattices which are also multigraphs, and can have any number of edges between any given pair of nodes (Fig. 2, d). Lattices set up in this manner allow us to incorporate multiple embeddings for each word. Within a single sentence, any pair of nodes corresponds to the start and end of a particular subsequence of the full sentence, and is thus associated with a specific token. Each edge between them is a unique embedding for that token. While many strategies for choosing the number of embeddings exist in the literature (Neelakantan et al., 2014), in this work, we choose a number of embeddings E and assign that many embeddings to each word. This ensures that the maximum indegree of any node in the lattice D, is no greater than E, giving us the time bound O(E|X|). In this work, we do not explore models that include both chunk vocabularies and multiple embeddings. However, combining these two techniques, as well as exploring other, more complex lattice structures, is an interesting avenue for future work. 5 Experiments 5.1 Data We perform experiments on two languages: English and Chinese, which provide an interesting contrast in linguis"
Q18-1036,E17-2025,0,0.182163,"on is exact only in the case where the segmentation is unique. In characterlevel models, it is easy to see that this property is maintained, because each token is unique and nonoverlapping. In word-level models, this also holds, because tokens are delimited by spaces, and no word contains a space. 2.2 Recurrent Neural Networks Recurrent neural networks have emerged as the state-of-the-art approach to approximating p(X). In particular, the LSTM cell (Hochreiter and Schmidhuber, 1997) is a specific RNN architecture which has been shown to be effective on many tasks, including language modeling (Press and Wolf, 2017; Jozefowicz et al., 2016; Merity et al., 2016; Inan et al., 2017).1 LSTM language models recursively cal1 In this work, we utilize an LSTM with linked input and forget gates, as proposed by Greff et al. (2016). 530 (2) (3) Neural Lattice Language Models Language Models with Ambiguous Segmentations To reiterate, the standard formulation of language modeling in the previous section requires splitting sentence X into a unique set of tokens x1 , . . . , x|X |. Our proposed method generalizes the previous formulation to remove the requirement of uniqueness of segmentation, similar to that used in"
Q18-1036,D17-1145,1,0.91956,"multiple predecessors by simply summing the individual hidden and cell state vectors of each of them. For each predecessor location ˜ i ∈ Aj , we first calculate the local hidden state h and local cell state c˜ by combining the embedding eji with the hidden state of the LSTM at x<i using the standard LSTM update function as in Eq. (2): ˜ i , c˜i = LSTM(hi , ci , ej , θ) for i ∈ Aj . h i We then sum the local hidden and cell states: X X ˜i hj = h cj = c˜i . i∈Aj i∈Aj 3 This framework has been used before for calculating neural sentence representations involving lattices by Su et al. (2016) and Sperber et al. (2017), but not for the language models that are the target of this paper. 532 M (x<i p(x<i |θ)p(xji |x<i ; θ) |θ) = . p(x<j |θ) (8) Therefore, one way to update the LSTM is to sample one predecessor x<i from the distribution M and ˜ i and cj = c˜i . However, sampling simply set hj = h is unstable and difficult to train: we found that the model tended to over-sample short tokens early on during training, and thus segmented every sentence into unigrams. This is similar to the outcome reported by Chan et al. (2017), who accounted for it by incorporating an  encouraging exploration. 3.3.3 Marginal App"
Q18-1036,P15-1150,0,0.0491336,"2 barked2 . . Figure 2: Example of (a) a single-path lattice, (b) a sparse lattice, (c) a dense lattice with D = 2, and (d) a multilattice with D = 2, for sentence “the dog barked .” models depend on the entire context, causing them to lack this ability. Our primary technical contribution is therefore to describe several techniques for incorporating lattices into a neural framework with infinite context, by providing ways to approximate the hidden state of the recurrent neural net. 3.3.1 Direct Approximation One approach to approximating the hidden state is the TreeLSTM framework described by Tai et al. (2015).3 In the TreeLSTM formulation, new states are derived from multiple predecessors by simply summing the individual hidden and cell state vectors of each of them. For each predecessor location ˜ i ∈ Aj , we first calculate the local hidden state h and local cell state c˜ by combining the embedding eji with the hidden state of the LSTM at x<i using the standard LSTM update function as in Eq. (2): ˜ i , c˜i = LSTM(hi , ci , ej , θ) for i ∈ Aj . h i We then sum the local hidden and cell states: X X ˜i hj = h cj = c˜i . i∈Aj i∈Aj 3 This framework has been used before for calculating neural sentence"
Q19-1020,N16-1109,0,0.132041,"ntermediate representation for the speech translation task, corresponding to the second stage output. Toshniwal et al. (2017) explore a different way of lower-level supervision during training of an attentional speech recognizer by jointly training an auxiliary phoneme recognizer based on a lower layer in the acoustic encoder. Similarly to the discussed multi-task direct model, this approach discards many of the learned parameters when used on the main task and consequently may also suffer from data efficiency issues. Direct end-to-end speech translation models were first used by Duong et al. (2016), although the authors did not actually evaluate translation performance. Weiss et al. (2017) extended this model into a multi-task model and report excellent translation results. Our baselines do not match their results, despite considerable efforts. We note that other research groups have encountered similar replicability issues (Bansal et al., 2018), explanations include the lack of a large GPU cluster to perform ASGD training, as well as to explore an ideal number of training schedules and other hyper-parameter settings. B´erard et al. (2018) explored the translation of audio books with di"
Q19-1020,N18-1008,0,0.368969,"the speech recognizer passes an erroneous source text to the machine translation component, potentially leading to compounding follow-up errors. Another advantage is the ability to train all model parameters jointly. Despite these obvious advantages, two problems persist: (1) Reports on whether direct models outperform cascaded models (Fig. 1a,d) are inconclusive, with some work in favor of direct models (Weiss et al., 2017), some work in favor of cascaded models (Kano et al., 2017; B´erard et al., 2018), and one work in favor of direct models for two out of the three examined language pairs (Anastasopoulos and Chiang, 2018). (2) Cascaded and direct models have been compared under identical data situations, but this is an unrealistic assumption: In practice, cascaded models can be trained on much more abundant independent ASR and MT corpora, whereas end-to-end models require hard-to-acquire end-to-end corpora of speech utterances paired with textual translations. Our first contribution is a closer investigation of these two issues. Regarding the question of whether direct models or cascaded models are generally stronger, we hypothesize that direct models require more data to work well, due to the more complex map"
Q19-1020,N19-1006,0,0.2679,"which are both attentional sequence-to-sequence models according to equations 1–4, trained on the appropriate data. The ASR component uses the acoustic encoder of §2.1, and the MT model uses a bidirectional LSTM with 2 layers as encoder. 3 Multi-Task Training for the Direct Model ST: Combines source speech encoder, generalpurpose-attention, target text decoder. This is our main task and requires end-to-end data for training. 3 We also experimented with a final fine-tuning phase on only the main task (Niehues and Cho, 2017), but discarded this strategy for lack of consistent gains. 4 Note that Bansal et al. (2019) do experiment with additional speech recognition data, although, differently from our work, for purposes of cross-lingual transfer learning. Incorporating Auxiliary Data The models described in §2.2 and §2.3 are trained only on speech utterances paired with translations 316 Note that somewhat related to our multi-task strategy, Kano et al. (2017) have decomposed their two-stage model in a similar way to perform pretraining for the individual stages, although not with the goal of incorporating additional auxiliary data. Figure 3: Direct multi-task model. 4 Auto-encoder (AE): Combines source te"
Q19-1020,L18-1001,0,0.109917,"h a model does not rely on intermediate ASR output and is therefore not subject to error propagation. However, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well. To make matters precise, given L audio encoder states e1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy. 2 As a case in point, the largest available speech translation corpora (Post et al., 2013; Kocabiyikoglu et al., 2018) are an order of magnitude smaller than the largest speech recognition corpora (Cieri et al., 2004; Panayotov et al., 2015) (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). 315 (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters with the main speech translation"
Q19-1020,L16-1147,0,0.0282839,"This indicates that access to ASR labels in some form contributes to favorable data efficiency of speech translation models. Adding External Data Our approach for evaluating data efficiency so far has been to assume that end-to-end data are available for only a subset of the available auxiliary data. In practice, we can often train ASR and MT tasks on abundant external data. We therefore run experiments in which we use the full Fisher training data for all tasks as before, and add OpenSubtitle11 data for the auxiliary MT task. We clean and normalize the Spanish–English OpenSubtitle 2018 data (Lison and Tiedemann, 2016) to be consistent with the employed Fisher training data by lowercasing and removing punctuation. We apply a basic length filter and obtain 61 million sentences. During training, we include the same number of sentences from in-domain and out-of-domain MT tasks in each minibatch in order to prevent degradation due to domain mismatch. 11 Fisher Table 3: Adding auxiliary OpenSubtitles MT data to the training. The two-stage models benefit much more strongly than the direct model, with our proposed model yielding the strongest overall results. Figure 6: Data efficiency across model types. All model"
Q19-1020,cieri-etal-2004-fisher,0,0.135104,"er, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well. To make matters precise, given L audio encoder states e1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy. 2 As a case in point, the largest available speech translation corpora (Post et al., 2013; Kocabiyikoglu et al., 2018) are an order of magnitude smaller than the largest speech recognition corpora (Cieri et al., 2004; Panayotov et al., 2015) (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). 315 (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters with the main speech translation model. We implement multi-task training by drawing several minibatches, one minibatch for each ta"
Q19-1020,N18-1031,0,0.0337135,"6.59 35.30 24.68 14.91 6.08 Table 1: BLEU scores (4 references) on the Fisher/ Test for various amounts of training data. The direct (multi-task) model performs best in the full data condition, but the cascaded model is best in all reduced conditions. attention MLP, 64 for target character embeddings, 256 for the encoder LSTMs in each direction, and 512 elsewhere. The model uses variational recurrent dropout with probability 0.3 and target character dropout with probability 0.1 (Gal and Ghahramani, 2016). We apply label smoothing (Szegedy et al., 2016) and fix the target embedding norm to 1 (Nguyen and Chiang, 2018). We use beam search with beam size 15 and polynomial length normalization with exponent 1.5.8 All BLEU scores are computed on Fisher/Test against 4 references. 5.1 Cascaded vs. Direct Models We first wish to shed light on the question of whether cascaded or direct models can be expected to perform better. This question has been investigated previously (Weiss et al., 2017; Kano et al., 2017; B´erard et al., 2018; Anastasopoulos and Chiang, 2018), but with contradictory findings. We hypothesize that the increased complexity of the direct mapping from speech to translation increases the data req"
Q19-1020,W17-4708,1,0.82047,"ilitate meaningful comparisons. The cascade consists of an ASR component and an MT component, which are both attentional sequence-to-sequence models according to equations 1–4, trained on the appropriate data. The ASR component uses the acoustic encoder of §2.1, and the MT model uses a bidirectional LSTM with 2 layers as encoder. 3 Multi-Task Training for the Direct Model ST: Combines source speech encoder, generalpurpose-attention, target text decoder. This is our main task and requires end-to-end data for training. 3 We also experimented with a final fine-tuning phase on only the main task (Niehues and Cho, 2017), but discarded this strategy for lack of consistent gains. 4 Note that Bansal et al. (2019) do experiment with additional speech recognition data, although, differently from our work, for purposes of cross-lingual transfer learning. Incorporating Auxiliary Data The models described in §2.2 and §2.3 are trained only on speech utterances paired with translations 316 Note that somewhat related to our multi-task strategy, Kano et al. (2017) have decomposed their two-stage model in a similar way to perform pretraining for the individual stages, although not with the goal of incorporating additiona"
Q19-1020,2013.iwslt-papers.14,0,0.50882,"ons as outputs. Such a model does not rely on intermediate ASR output and is therefore not subject to error propagation. However, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well. To make matters precise, given L audio encoder states e1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy. 2 As a case in point, the largest available speech translation corpora (Post et al., 2013; Kocabiyikoglu et al., 2018) are an order of magnitude smaller than the largest speech recognition corpora (Cieri et al., 2004; Panayotov et al., 2015) (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). 315 (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters wit"
sakti-etal-2014-towards,W13-4604,1,\N,Missing
sakti-etal-2014-towards,maekawa-etal-2000-spontaneous,0,\N,Missing
sakti-etal-2014-towards,P11-2093,1,\N,Missing
shimizu-etal-2014-collection,N12-1048,0,\N,Missing
shimizu-etal-2014-collection,E09-1040,0,\N,Missing
shimizu-etal-2014-collection,N13-1023,0,\N,Missing
U17-1006,C16-1328,0,0.0671518,"Missing"
W13-4504,corvey-etal-2012-foundations,0,0.139573,"risis-related information (Munro, 2010), gather survivor lists from evacuation sites and enter them into a central database (Google Japan, 2011), or even annotate data for the creation of specialized information extraction systems (Neubig et al., 2011). Given the large amount of work required in these collaborative efforts, it is common for as many as hundreds of volunteers to be involved in any single task. On the other hand, examinations of the types of information provided on social networks after crises have shown that the number of possible information extraction tasks is large (20-30 by Corvey et al. (2012)’s classification). Information requirements also vary greatly from situation to situation, with the direction of the wind being important during the Oklahoma wildfires, and radiation measurements being important after the nuclear meltdown following the Great East Japan Earthquake (Vieweg et al., 2010; Doan et al., 2012). While a large number of volunteers may be mobilized for a single task, scaling this approach to tens or hundreds of disparate tasks has not proven This research proposes a framework for efficient information extraction and filtering in situations where 1) extreme reliability"
W13-4504,2010.amta-workshop.1,0,0.0374699,"rbird et al., 2012). However, distinguishing useful information (e.g. “there is water at the evacuation center in Sendai high school”) from unreliable or non-actionable information (e.g. “just arrived at the evacuation center, so tired...”) takes a large amount of human effort. Luckily, however, the effort of good-willed internet users is one thing that is often plentiful in times of crisis. There have been many success stories where volunteers have banded together to turn natural language data into machine-readable format (Starbird and Stamberger, 2010), translate crisis-related information (Munro, 2010), gather survivor lists from evacuation sites and enter them into a central database (Google Japan, 2011), or even annotate data for the creation of specialized information extraction systems (Neubig et al., 2011). Given the large amount of work required in these collaborative efforts, it is common for as many as hundreds of volunteers to be involved in any single task. On the other hand, examinations of the types of information provided on social networks after crises have shown that the number of possible information extraction tasks is large (20-30 by Corvey et al. (2012)’s classification)."
W13-4504,I11-1108,1,0.90347,"he evacuation center, so tired...”) takes a large amount of human effort. Luckily, however, the effort of good-willed internet users is one thing that is often plentiful in times of crisis. There have been many success stories where volunteers have banded together to turn natural language data into machine-readable format (Starbird and Stamberger, 2010), translate crisis-related information (Munro, 2010), gather survivor lists from evacuation sites and enter them into a central database (Google Japan, 2011), or even annotate data for the creation of specialized information extraction systems (Neubig et al., 2011). Given the large amount of work required in these collaborative efforts, it is common for as many as hundreds of volunteers to be involved in any single task. On the other hand, examinations of the types of information provided on social networks after crises have shown that the number of possible information extraction tasks is large (20-30 by Corvey et al. (2012)’s classification). Information requirements also vary greatly from situation to situation, with the direction of the wind being important during the Oklahoma wildfires, and radiation measurements being important after the nuclear m"
W13-4504,D11-1136,0,0.0239071,"s(Di ) = log P (ui = 1|Di ) − log P (ui = 0|Di ) which allows us to define each weight λn as λn = log c(ϕn , u∗ = 1) − log c(ϕn , u∗ = 0). However, zero counts for either positive or negative labels will cause the log odds to be negative or positive infinity, so in many cases, the counts are augmented with a pseudo-count α for smoothing (Mackay and Petoy, 1995): λn = log(c(ϕn , u∗ = 1) + α) − log(c(ϕn , u∗ = 0) + α). (4) It should also be noted that while standard classifiers are trained using the document labels U, it is also possible to directly label the features ϕn (Melville et al., 2009; Settles, 2011). In this case, let l(ϕn , u∗ = 1) be a function that is 1 if feature ϕn is labeled positive, and 0 otherwise. We further augment Equation (4) with a pseudo-count β in the case of labeled features (3) n=1 In the case of s(Di ) ≥ 0, Di is classified as a positive example, and in the case of s(Di ) &lt; 0, Di is classified as a negative example. In order to learn the weights λ = (λ1 , λ2 , . . . , λK ), a corpus of documents D is annotated with labels U ∗ , and a classifier such as support vector machines (SVMs) or naive Bayes classifiers is used to train the weight values (Joachims, 1998). In this"
W13-4604,P11-2093,1,0.605576,"Missing"
W13-4604,2012.eamt-1.60,0,0.0168747,"in the previous section. For training, 4,000 sentences is not enough to build an accurate MT system, so we add several additional corpora for each language pair. For Japanese-English parallel training data, we add the Eijiro dictionary1 and its accompanying sample sentences, the BTEC corpus(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or info"
W13-4604,J07-2003,0,0.120881,"Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or informal. On the other hand, using syntactic information has been shown to improve translation, particularly between language pairs with different syntactic structures such as those we are handling in our experiments. Thus it will be interesting to see which methodology can produce better results, and also if any difference in the effectivenes"
W13-4604,C04-1114,0,0.0639568,"Missing"
W13-4604,P13-4016,1,0.882079,"Missing"
W13-4604,I11-1087,1,0.830624,"Missing"
W13-4604,J03-1002,0,0.0085147,"Missing"
W13-4604,D10-1092,0,0.0662551,"Missing"
W13-4604,P02-1040,0,0.0879458,"Missing"
W13-4604,N03-1017,0,0.0066382,"us(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or informal. On the other hand, using syntactic information has been shown to improve translation, particularly between language pairs with different syntactic structures such as those we are handling in our experiments. Thus it will be interesting to see which methodology can produce better r"
W13-4604,P07-2045,0,0.0126736,"Missing"
W13-4604,P10-1017,0,0.13341,"Missing"
W13-4604,takezawa-etal-2002-toward,0,0.0605347,"edical communication as well. As a result, it is likely that adapting to medical terminology of the domain is somewhat less important than adapting to the conversational speaking style of the speech. 4 4.1 Experimental Setup For the tuning and test data for our translation system, we use the data described in the previous section. For training, 4,000 sentences is not enough to build an accurate MT system, so we add several additional corpora for each language pair. For Japanese-English parallel training data, we add the Eijiro dictionary1 and its accompanying sample sentences, the BTEC corpus(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hie"
W13-4604,I05-3027,0,0.0659246,"Missing"
W13-4604,A00-2028,0,0.0297292,"challenging for a number of reasons. The first reason is that communication of incomplete or incorrect information could lead to a mistaken diagnosis with severe consequences, and thus extremely high levels of accuracy and reliability are 22 International Joint Conference on Natural Language Processing Workshop on Natural Language Processing for Medical and Healthcare Fields, pages 22–29, Nagoya, Japan, 14-18 October 2013. man interpreters, either based on a manual request of one of the users, or through automatic detection of when the dialogue is going poorly, such as the method described by Walker et al. (2000). Even with this fall-back to human interpreters, it is still desirable that the automatic translation system is effective as possible. In order to ensure this, we must be certain that the ASR, MT, and TTS models are all tuned to work as well as possible in medical situations. Some potential problems that we have identified so far based on our analysis of data are as follows: Figure 1: An overview of the use scenario for the medical translation system. Specialized Vocabulary: Perhaps the most obvious problem is that the ASR, MT, and TTS systems must all be able to handle the specialized vocabu"
W13-4604,C04-1168,0,0.0303816,"r. However, as the cost of hiring and maintaining medical interpreters is quite high, we would also like to reduce our reliance on human effort as much as possible. Thus, each device will use automatic translation by default, but also have functionality to connect to huTranslation/Synthesis of Erroneous Input: As we can expect ASR not to be perfect, it will be necessary to be able to translate input that contains errors. This problem can potentially be ameliorated by passing multiple speech recognition hypotheses to translation (Ney, 1999), and jointly optimizing the parameters of ASR and MT (Zhang et al., 2004; Ohgushi et al., 2013). In addition, it will also be necessary to resolve difficulties in TTS due to grammatical errors, lack of punctuation, and unknown words (Parlikar et al., 2010). While all of these problems need to be solved to provide high-reliability speech translation systems, in this paper as a first step we focus mainly on the MT system, and relegate the last problem of integration with ASR to future work. 23 3 Medical Translation Corpus Construction and Analysis In this section, we describe our collection of a tri-lingual (Japanese, English, Chinese) corpus to serve as an initial"
W13-4604,W12-4213,0,\N,Missing
W13-4604,P08-1023,0,\N,Missing
W14-3211,N13-1084,0,0.0485056,"Missing"
W14-3211,W10-4346,0,0.245775,"Missing"
W14-4004,P08-1087,0,0.0211581,"phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label"
W14-4004,P06-1077,0,0.155526,"e syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalization also contains some additional preprocessing steps such as determiner elimination, particle insertion and singularization to generate a sentence that is closer to Japanese grammatical structure. In addition to PBMT, there has also recently been interest in syntax-based SMT (Yamada and Knight, 2001; Liu et al., 2006), which translates using syntactic information. However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excepSeveral preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output. On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper,"
W14-4004,D12-1079,0,0.0187559,"of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute"
W14-4004,P11-2031,0,0.0194023,"(test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. Table 5: Optimized weight of HF-feature in each condition HF-feature Reordering + + + + + + Word Processing + + Weight of HF-feature -0.00707078 0.00524676 0.156724 -0.121326 Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexic"
W14-4004,P14-2024,1,0.800601,"be automatically learned in a well-trained T2S model. Therefore, preordering is not expected to cause large gains, unlike in the case of PBMT. However, it can also be thought that preordering can still have a positive influence on the translation model training process, particularly by increasing alignment accuracy. For example, training methods for word alignment such as the IBM or HMM models (Och and Ney, 2003) are affected by word order, and word alignment may be improved by moving word order closer between the two languages. As alignment accuracy plays a important role in T2S translation (Neubig and Duh, 2014), it is reasonable to hypothesize that reordering may also have a positive effect on T2S. In terms of the actual incorporation with the T2S system, we simply follow the process in Figure 1, but output the reordered tree instead of only the reordered terminal nodes as is done for PBMT. 4.2 Reordering Information as Soft Constraints As described in section 4.1.1, T2S work well on language pairs that have very different word order, but is sensitive to alignment accuracy. On the other hand, we know that in most cases Japanese word order tends to be head final, and thus any rules that do not obey h"
W14-4004,P05-1066,0,0.118261,"Missing"
W14-4004,P11-2093,1,0.796984,"of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation"
W14-4004,P13-4016,1,0.791971,"eriment, we examined how much each of the preprocessing steps (Reordering, Lexical Processing) contribute to improve the translation quality of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluatio"
W14-4004,C00-2162,0,0.0855186,"translation probabilities between phrases consisting of multiple words are calculated, and translated phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), method"
W14-4004,W11-1011,0,0.0190313,"tic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute anything to syntax-based machine translation. Specifically, we examine whether the reordering and lexical processing of Head Finalization contributes to the improvement of syntax-based mach"
W14-4004,J03-1002,0,0.0134154,", because translation patterns of T2S are expressed by using source sentence subtrees, the effect of reordering problems are relatively small, and the majority of reordering rules specified by hand can be automatically learned in a well-trained T2S model. Therefore, preordering is not expected to cause large gains, unlike in the case of PBMT. However, it can also be thought that preordering can still have a positive influence on the translation model training process, particularly by increasing alignment accuracy. For example, training methods for word alignment such as the IBM or HMM models (Och and Ney, 2003) are affected by word order, and word alignment may be improved by moving word order closer between the two languages. As alignment accuracy plays a important role in T2S translation (Neubig and Duh, 2014), it is reasonable to hypothesize that reordering may also have a positive effect on T2S. In terms of the actual incorporation with the T2S system, we simply follow the process in Figure 1, but output the reordered tree instead of only the reordered terminal nodes as is done for PBMT. 4.2 Reordering Information as Soft Constraints As described in section 4.1.1, T2S work well on language pairs"
W14-4004,D10-1092,0,0.046309,"Missing"
W14-4004,P03-1021,0,0.140083,"(such as sentences that contain the determiner 37 “no,” or situations where non-literal translations are necessary) and a hard constraint to obey headfinal word order could be detrimental. In order to incorporate this intuition, we add a feature (HF-feature) to translation patterns that conform to the reordering rules of Head Finalization. This gives the decoder ability to discern translation patterns that follow the canonical reordering patterns in English-Japanese translation, and has the potential to improve translation quality in the T2S translation model. We use the log-linear approach (Och, 2003) to add the Head Finalization feature (HF-feature). As in the standard log-linear model, a source sentence f is translated into a target language sentence e, by searching for the sentence maximizing the score: ˆ = arg max wT · h(f , e). e e VP Source side of translation pattern VBD NP hit x0:NP Word alignment Target side of translation pattern x0 wo utta 1. Apply Reordering to source translation pattern VP Reordered translation pattern NP VBD x0:NP hit Target side of translation pattern (1) where h(f , e) is a feature function vector. w is a weight vector that scales the contribution from each"
W14-4004,W10-1736,0,0.239358,"PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language. It’s often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word ordering problems faced by PBMT (Collins et al., 2005; Xu et al., 2009; Isozaki et al., 2010b). One particularly successful example of rulebased syntactic preprocessing is Head Finalization (Isozaki et al., 2010b), a method of syntactic preprocessing for English to Japanese translation that has significantly improved translation quality of English-Japanese PBMT using simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalizati"
W14-4004,P02-1040,0,0.0902419,"s, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. Table 5: Optimized weight of HF-feature in each condition HF-feature Reordering + + + + + + Word Processing + + Weight of HF-feature -0.00707078 0.00524676 0.156724 -0.121326 Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indi"
W14-4004,N03-1017,0,0.0192216,"ttle work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper, we examine whether the sort of rule-based syntactic preprocessing approaches that have proved beneficial for PBMT can contribute to syntax-based SMT. Specifically, we tailor a highly successful preprocessing method for EnglishJapanese PBMT to syntax-based SMT, and find that while the gains achievable are smaller than those for PBMT, significant improvements in accuracy can be realized. 1 Introduction In the widely-studied framework of phrase-based machine translation (PBMT) (Koehn et al., 2003), translation probabilities between phrases consisting of multiple words are calculated, and translated phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs"
W14-4004,P07-2045,0,0.0091836,"2.11 37.99 5 Experiment In our experiment, we examined how much each of the preprocessing steps (Reordering, Lexical Processing) contribute to improve the translation quality of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki"
W14-4004,W04-3250,0,0.0397255,"ing information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. Table 5: Optimized weight of HF-feature in each condition HF-feature Reordering + + + + + + Word Processing + + Weight of HF-feature -0.00707078 0.00524676 0.156724 -0.121326 Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indicate no significant difference in comparison with the condition that has highest translation quality using the bootstrap resampling method (Koehn, 2004) (p &lt; 0.05). For PBMT, we can see that reordering plays an extremely important role, with the highest BLEU and RIBES scores being achieved when using Reordering preprocessing (line 3, 4). Lexical Processing also provided a slight performance gain for PBMT. When we applied Lexical Processing to PBMT, BLEU and RIBES scores were improved (line 1 vs 2), although this gain was not significant when Reordering was performed as well. Overall T2S without any preprocessing achieved better translation quality than all conditions of PBMT (line 1 of T2S vs line 1-4 of PBMT). In addition, BLEU and RIBES sco"
W14-4004,C04-1073,0,0.0946405,"Missing"
W14-4004,N09-1028,0,0.0214648,"ethods for PBMT. PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language. It’s often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word ordering problems faced by PBMT (Collins et al., 2005; Xu et al., 2009; Isozaki et al., 2010b). One particularly successful example of rulebased syntactic preprocessing is Head Finalization (Isozaki et al., 2010b), a method of syntactic preprocessing for English to Japanese translation that has significantly improved translation quality of English-Japanese PBMT using simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sent"
W14-4004,P01-1067,0,0.415385,"simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalization also contains some additional preprocessing steps such as determiner elimination, particle insertion and singularization to generate a sentence that is closer to Japanese grammatical structure. In addition to PBMT, there has also recently been interest in syntax-based SMT (Yamada and Knight, 2001; Liu et al., 2006), which translates using syntactic information. However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excepSeveral preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output. On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based S"
W14-4004,N06-1033,0,0.0175714,"n and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute anything to syntax-based machine translation. Specifically, we examine whether th"
W14-4004,P13-1083,0,\N,Missing
W14-4004,J08-3004,0,\N,Missing
W14-7002,C14-1106,1,0.798451,"e tree, but a myriad of parse 1 http://phontron.com/travatar 20 Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 20‒25, Tokyo, Japan, 4th October 2014. 2014 Copyright is held by the author(s). fix to this problem, we re-segmented all words that appear in the dev or test set but not the training set using the compound segmentation method of (Koehn and Knight, 2003), which splits words into two, resolving ambiguities such that the newly split words have the highest unigram probability. Finally, in a preliminary analysis of our ja-en system using the error analysis method of Akabe et al. (2014),6 we discovered a peculiarity in the development data: prolific use of the word “標題” (which can be translated into “the mentioned,” or “the XX in the title”). This word appeared prolifically in the dev set (as well as devtest and test), but not once in the training corpus. In order to solve this problem, we normalized “標題” into the lexically different but semantically largely equivalent “表題,” which appeared many times in the training corpus. largely reproduce our experiments will be released open source.2 2 Data and Data Processing 2.1 Data Used For the majority of our systems, we simply used"
W14-7002,P06-1121,0,0.0408608,"d minimum error rate training (MERT; Och (2003)). As the two official evaluation measures of the contest are BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010), we submitted two systems, one optimized for BLEU, and one optimized for BLEU+RIBES. We also attempted optimizing for RIBES only, which did result in higher RIBES scores, but also extremely short translations and extremely low scores, so we decided against submitting this system. Translation Model Training For training our translation model, we extract a synchronous tree substitution grammar (STSG) according to the method of Galley et al. (2006). We used composed rules including up to 5 minimal rules, and attached null-aligned words to the highest possible point in the parse tree. For the translation model features, we used a standard set of 5 features including forward and backward translation probabilities, forward and backward lexical probabilities, and the phrase penalty. When calculating the translation probabilities, we first applied Kneser-Ney smoothing to the phrases counts (Kneser and Ney, 1995). 3.3 4 Issues for Context-aware Machine Translation We did not make any particular attempt to consider super-sentential context in"
W14-7002,P14-2024,1,0.868137,"pair we submitted one system that used additional dictionaries to reduce the number of unknown words. Specifically, we used the EDICT3 , and Eijiro4 dictionaries, as well as the Japanese-English links between Wikipedia pages. There are a number of ways to incorporate these dictionaries, but in the submitted system, we simply added a rule to the translation table for all unknown words that existed in the dictionary. 2.2 2.3 Syntactic Parsing As we are performing translation using syntactic parsing, it is essential that we have an accurate syntactic parser. Based on the experiments presented in Neubig and Duh (2014) we opt to use the Egret parser,7 which implements the latent variable parsing model of (Petrov et al., 2006). For the parsing models in English and Chinese, we use models trained on the English and Chinese Penn Treebanks respectively (Marcus et al., 1993; Xue et al., 2005). For the Japanese model, we train our own model on the Japanese Word Dependency Treebank (Mori et al., 2014). As this is a dependency treebank, we use head rules contained the Travatar toolkit to transform the dependency trees into phrase structure trees.8 For training, we simply use 1-best parses, but at test time we use a"
W14-7002,P11-2093,1,0.639419,"nese model, we train our own model on the Japanese Word Dependency Treebank (Mori et al., 2014). As this is a dependency treebank, we use head rules contained the Travatar toolkit to transform the dependency trees into phrase structure trees.8 For training, we simply use 1-best parses, but at test time we use a forest of parse trees, specifically using forests with all tree edges that exist in at least one of the 100-best parses. Tokenization and Preprocessing For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. We also performed case normalization for English, by changing the first word in English sentences to its most common capitalization before training models and translation, and capitalizing the first letter of the sentence after translation.5 For zh-ja translation, in order to make unknown words more comprehensible, we converted simplified Chinese characters to their Japanese equivalents, and vice-versa for ja-zh translation (using the Kanconvit.pm Perl script). In addition to our standard tokenization, for Japanese, while KyTea is"
W14-7002,P13-2121,0,0.0684795,"Missing"
W14-7002,D10-1092,0,0.101634,"ively expensive, we used only the first 500,000 sentences from the parallel data to train models for each respective task. As RNNLMs cannot be trivially incorporated into decoding due to their continuous-space state representation, we instead use the RNNLM score as an additional feature in 10,000-best rescoring of the output of the baseline model. 3.4 Parameter Optimization In order to optimize the parameters of the loglinear model, we use standard minimum error rate training (MERT; Och (2003)). As the two official evaluation measures of the contest are BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010), we submitted two systems, one optimized for BLEU, and one optimized for BLEU+RIBES. We also attempted optimizing for RIBES only, which did result in higher RIBES scores, but also extremely short translations and extremely low scores, so we decided against submitting this system. Translation Model Training For training our translation model, we extract a synchronous tree substitution grammar (STSG) according to the method of Galley et al. (2006). We used composed rules including up to 5 minimal rules, and attached null-aligned words to the highest possible point in the parse tree. For the tra"
W14-7002,P13-4016,1,0.866688,"sian Language Translation: NAIST at WAT 2014 Graham Neubig Graduate School of Information Science Nara Institute of Science and Technology 8916-5 Takayama-cho, Ikoma-shi, Nara, Japan neubig@is.naist.jp Abstract candidates stored efficiently in a packed-forest data structure. In our previous work (Neubig and Duh, 2014), we have shown that F2S translation is effective for en-ja and ja-en translation, and can outperform alternative methods such as preor post-ordering. Thus, in our WAT submission, we choose this formalism, and specifically it’s implementation in the open-source Travatar decoder1 (Neubig, 2013) as the base of our system. Another promising development over the past couple years is the use of continuous-space representations of language combined with neuralnetwork-based probabilistic models. These have been incorporated into translation as either language models (LMs) (Vaswani et al., 2013) or translation models (TMs) (Le et al., 2012), allowing for large increases in translation accuracy. In our submission, we incorporate this continuousspace representation by training a recurrent neural network language model (RNNLM; Mikolov et al. (2010)) and using its scores as a feature in n-best"
W14-7002,P03-1054,0,0.0193612,"et al., 1993; Xue et al., 2005). For the Japanese model, we train our own model on the Japanese Word Dependency Treebank (Mori et al., 2014). As this is a dependency treebank, we use head rules contained the Travatar toolkit to transform the dependency trees into phrase structure trees.8 For training, we simply use 1-best parses, but at test time we use a forest of parse trees, specifically using forests with all tree edges that exist in at least one of the 100-best parses. Tokenization and Preprocessing For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. We also performed case normalization for English, by changing the first word in English sentences to its most common capitalization before training models and translation, and capitalizing the first letter of the sentence after translation.5 For zh-ja translation, in order to make unknown words more comprehensible, we converted simplified Chinese characters to their Japanese equivalents, and vice-versa for ja-zh translation (using the Kanconvit.pm Perl script). In addition to our standard t"
W14-7002,J03-1002,0,0.00347438,"pl 3 http://www.edrdg.org/jmdict/edict.html 4 http://www.eijiro.jp 5 This is often referred to as “truecasing.” 21 Thus, for en-ja and ja-en translation, we use Nile9 (Riesa and Marcu, 2010), a supervised syntaxbased aligner that can improve alignment accuracy by incorporating information about parse trees and learn from manually created alignments. For our manual alignments, we use the alignments provided by the Kyoto Free Translation Task (Neubig, 2011). Unfortunately, for zh-ja and ja-zh translation, we do not have any hand-aligned data available, so we use the GIZA++ unsupervised aligner (Och and Ney, 2003). 3.2 layers and 300 classes. Because training RNNLM on large data sets is prohibitively expensive, we used only the first 500,000 sentences from the parallel data to train models for each respective task. As RNNLMs cannot be trivially incorporated into decoding due to their continuous-space state representation, we instead use the RNNLM score as an additional feature in 10,000-best rescoring of the output of the baseline model. 3.4 Parameter Optimization In order to optimize the parameters of the loglinear model, we use standard minimum error rate training (MERT; Och (2003)). As the two offic"
W14-7002,P03-1021,0,0.00684997,"d aligner (Och and Ney, 2003). 3.2 layers and 300 classes. Because training RNNLM on large data sets is prohibitively expensive, we used only the first 500,000 sentences from the parallel data to train models for each respective task. As RNNLMs cannot be trivially incorporated into decoding due to their continuous-space state representation, we instead use the RNNLM score as an additional feature in 10,000-best rescoring of the output of the baseline model. 3.4 Parameter Optimization In order to optimize the parameters of the loglinear model, we use standard minimum error rate training (MERT; Och (2003)). As the two official evaluation measures of the contest are BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010), we submitted two systems, one optimized for BLEU, and one optimized for BLEU+RIBES. We also attempted optimizing for RIBES only, which did result in higher RIBES scores, but also extremely short translations and extremely low scores, so we decided against submitting this system. Translation Model Training For training our translation model, we extract a synchronous tree substitution grammar (STSG) according to the method of Galley et al. (2006). We used composed rules in"
W14-7002,E03-1076,0,0.405786,"stic models. These have been incorporated into translation as either language models (LMs) (Vaswani et al., 2013) or translation models (TMs) (Le et al., 2012), allowing for large increases in translation accuracy. In our submission, we incorporate this continuousspace representation by training a recurrent neural network language model (RNNLM; Mikolov et al. (2010)) and using its scores as a feature in n-best hypothesis rescoring. We also made a few small improvements to our ja-en system, mainly in an attempt to reduce the number of unknown words. Specifically, we perform compound splitting (Koehn and Knight, 2003) of unknown words to help reduce the effects of under-segmentation, perform one small word substitution to regularize for the peculiarities of the development/test data, and add large external dictionaries. As a result of the incorporation of F2S translation and RNNLMs, we see a large gain in accuracy over a baseline phrase-based machine translation model. Specifically, we see a gain in BLEU of 8.21 for en-ja, 5.44 for ja-en, 4.71 for zh-ja, and 2.47 for ja-zh. In addition, according to the official automatic evaluation, our system outperformed all other submitted systems in all tracks. Script"
W14-7002,P02-1040,0,0.0926259,"NLM on large data sets is prohibitively expensive, we used only the first 500,000 sentences from the parallel data to train models for each respective task. As RNNLMs cannot be trivially incorporated into decoding due to their continuous-space state representation, we instead use the RNNLM score as an additional feature in 10,000-best rescoring of the output of the baseline model. 3.4 Parameter Optimization In order to optimize the parameters of the loglinear model, we use standard minimum error rate training (MERT; Och (2003)). As the two official evaluation measures of the contest are BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010), we submitted two systems, one optimized for BLEU, and one optimized for BLEU+RIBES. We also attempted optimizing for RIBES only, which did result in higher RIBES scores, but also extremely short translations and extremely low scores, so we decided against submitting this system. Translation Model Training For training our translation model, we extract a synchronous tree substitution grammar (STSG) according to the method of Galley et al. (2006). We used composed rules including up to 5 minimal rules, and attached null-aligned words to the highest possible poi"
W14-7002,P07-2045,0,0.0107982,"ment set. In addition to the n-gram language model, we incorporated a recurrent neural network language model (RNNLM) (Mikolov et al., 2010). This, as mentioned in the introduction, will allow us to incorporate recent advances in continuous-space language modeling, improving robustness to unknown or low-frequency linguistic phenomena. We used the RNNLM toolkit,10 with 500 hidden 5 Experimental Results In Table 1 we show the results for our systems with and without the RNNLM, and tuning with BLEU or BLEU+RIBES. In addition, we show the results for a PBMT system trained using the Moses toolkit (Koehn et al., 2007), with the same data as the F2S system and the default settings except for a reordering limit of 18, which gave better results on all language pairs than the default of 6. From this table we can first see that the F2S translation greatly outperforms PBMT. The trend is more prominent in the translation to or from English, a result of the fact that the amount of reordering is greater between English and Japanese than between Chinese and Japanese. In addition, we can see that the gain over PBMT is smaller 9 https://code.google.com/p/nile/ 10 http://rnnlm.org 22 System PBMT F2S RNN No No Yes F2S+D"
W14-7002,P06-1055,0,0.0455107,"ally, we used the EDICT3 , and Eijiro4 dictionaries, as well as the Japanese-English links between Wikipedia pages. There are a number of ways to incorporate these dictionaries, but in the submitted system, we simply added a rule to the translation table for all unknown words that existed in the dictionary. 2.2 2.3 Syntactic Parsing As we are performing translation using syntactic parsing, it is essential that we have an accurate syntactic parser. Based on the experiments presented in Neubig and Duh (2014) we opt to use the Egret parser,7 which implements the latent variable parsing model of (Petrov et al., 2006). For the parsing models in English and Chinese, we use models trained on the English and Chinese Penn Treebanks respectively (Marcus et al., 1993; Xue et al., 2005). For the Japanese model, we train our own model on the Japanese Word Dependency Treebank (Mori et al., 2014). As this is a dependency treebank, we use head rules contained the Travatar toolkit to transform the dependency trees into phrase structure trees.8 For training, we simply use 1-best parses, but at test time we use a forest of parse trees, specifically using forests with all tree edges that exist in at least one of the 100-"
W14-7002,P10-1017,0,0.0420328,"is necessary to have an accurate word alignment model, which allows for the extraction of more rules that match the parse tree, and the estimation of more accurate reordering probabilities (Neubig and Duh, 2014). 2 http://phontron.com/project/wat2014 6 In fact we slightly modified the method to use the translation reference and a smoothed naive Bayes classifier. 7 https://github.com/neubig/egret 8 ja-depadjust.pl and ja-dep2cfg.pl 3 http://www.edrdg.org/jmdict/edict.html 4 http://www.eijiro.jp 5 This is often referred to as “truecasing.” 21 Thus, for en-ja and ja-en translation, we use Nile9 (Riesa and Marcu, 2010), a supervised syntaxbased aligner that can improve alignment accuracy by incorporating information about parse trees and learn from manually created alignments. For our manual alignments, we use the alignments provided by the Kyoto Free Translation Task (Neubig, 2011). Unfortunately, for zh-ja and ja-zh translation, we do not have any hand-aligned data available, so we use the GIZA++ unsupervised aligner (Och and Ney, 2003). 3.2 layers and 300 classes. Because training RNNLM on large data sets is prohibitively expensive, we used only the first 500,000 sentences from the parallel data to train"
W14-7002,W04-3250,0,0.38312,"Missing"
W14-7002,N12-1005,0,0.0166721,"translation is effective for en-ja and ja-en translation, and can outperform alternative methods such as preor post-ordering. Thus, in our WAT submission, we choose this formalism, and specifically it’s implementation in the open-source Travatar decoder1 (Neubig, 2013) as the base of our system. Another promising development over the past couple years is the use of continuous-space representations of language combined with neuralnetwork-based probabilistic models. These have been incorporated into translation as either language models (LMs) (Vaswani et al., 2013) or translation models (TMs) (Le et al., 2012), allowing for large increases in translation accuracy. In our submission, we incorporate this continuousspace representation by training a recurrent neural network language model (RNNLM; Mikolov et al. (2010)) and using its scores as a feature in n-best hypothesis rescoring. We also made a few small improvements to our ja-en system, mainly in an attempt to reduce the number of unknown words. Specifically, we perform compound splitting (Koehn and Knight, 2003) of unknown words to help reduce the effects of under-segmentation, perform one small word substitution to regularize for the peculiarit"
W14-7002,I05-3027,0,0.0234157,"Word Dependency Treebank (Mori et al., 2014). As this is a dependency treebank, we use head rules contained the Travatar toolkit to transform the dependency trees into phrase structure trees.8 For training, we simply use 1-best parses, but at test time we use a forest of parse trees, specifically using forests with all tree edges that exist in at least one of the 100-best parses. Tokenization and Preprocessing For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. We also performed case normalization for English, by changing the first word in English sentences to its most common capitalization before training models and translation, and capitalizing the first letter of the sentence after translation.5 For zh-ja translation, in order to make unknown words more comprehensible, we converted simplified Chinese characters to their Japanese equivalents, and vice-versa for ja-zh translation (using the Kanconvit.pm Perl script). In addition to our standard tokenization, for Japanese, while KyTea is on average more robust to unknown words than oth"
W14-7002,P06-1077,0,0.100619,"lving translating Japanese (ja), a language with SOV word order, to/from English (en) or Chinese (zh), languages with SVO word order. Because of this, it can be expected that one of the major challenges facing translation systems in this task is the proper reordering of the words between the source and target languages. One promising way to tackle the reordering problem is through the use of tree-to-string (T2S) translation, a translation formalism where the source sentence is first parsed using a syntactic parser, then sub-structures of the parse tree are translated into target-side strings (Liu et al., 2006). Mi et al. (2008) have also demonstrated that forest-to-string (F2S) translation allows for more robust use of source-side syntax by not considering a 1-best parse tree, but a myriad of parse 1 http://phontron.com/travatar 20 Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 20‒25, Tokyo, Japan, 4th October 2014. 2014 Copyright is held by the author(s). fix to this problem, we re-segmented all words that appear in the dev or test set but not the training set using the compound segmentation method of (Koehn and Knight, 2003), which splits words into two, resolving ambiguiti"
W14-7002,D13-1140,0,0.0176101,"work (Neubig and Duh, 2014), we have shown that F2S translation is effective for en-ja and ja-en translation, and can outperform alternative methods such as preor post-ordering. Thus, in our WAT submission, we choose this formalism, and specifically it’s implementation in the open-source Travatar decoder1 (Neubig, 2013) as the base of our system. Another promising development over the past couple years is the use of continuous-space representations of language combined with neuralnetwork-based probabilistic models. These have been incorporated into translation as either language models (LMs) (Vaswani et al., 2013) or translation models (TMs) (Le et al., 2012), allowing for large increases in translation accuracy. In our submission, we incorporate this continuousspace representation by training a recurrent neural network language model (RNNLM; Mikolov et al. (2010)) and using its scores as a feature in n-best hypothesis rescoring. We also made a few small improvements to our ja-en system, mainly in an attempt to reduce the number of unknown words. Specifically, we perform compound splitting (Koehn and Knight, 2003) of unknown words to help reduce the effects of under-segmentation, perform one small word"
W14-7002,J93-2004,0,0.0507107,"ncorporate these dictionaries, but in the submitted system, we simply added a rule to the translation table for all unknown words that existed in the dictionary. 2.2 2.3 Syntactic Parsing As we are performing translation using syntactic parsing, it is essential that we have an accurate syntactic parser. Based on the experiments presented in Neubig and Duh (2014) we opt to use the Egret parser,7 which implements the latent variable parsing model of (Petrov et al., 2006). For the parsing models in English and Chinese, we use models trained on the English and Chinese Penn Treebanks respectively (Marcus et al., 1993; Xue et al., 2005). For the Japanese model, we train our own model on the Japanese Word Dependency Treebank (Mori et al., 2014). As this is a dependency treebank, we use head rules contained the Travatar toolkit to transform the dependency trees into phrase structure trees.8 For training, we simply use 1-best parses, but at test time we use a forest of parse trees, specifically using forests with all tree edges that exist in at least one of the 100-best parses. Tokenization and Preprocessing For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Ma"
W14-7002,P08-1023,0,0.0686049,"apanese (ja), a language with SOV word order, to/from English (en) or Chinese (zh), languages with SVO word order. Because of this, it can be expected that one of the major challenges facing translation systems in this task is the proper reordering of the words between the source and target languages. One promising way to tackle the reordering problem is through the use of tree-to-string (T2S) translation, a translation formalism where the source sentence is first parsed using a syntactic parser, then sub-structures of the parse tree are translated into target-side strings (Liu et al., 2006). Mi et al. (2008) have also demonstrated that forest-to-string (F2S) translation allows for more robust use of source-side syntax by not considering a 1-best parse tree, but a myriad of parse 1 http://phontron.com/travatar 20 Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 20‒25, Tokyo, Japan, 4th October 2014. 2014 Copyright is held by the author(s). fix to this problem, we re-segmented all words that appear in the dev or test set but not the training set using the compound segmentation method of (Koehn and Knight, 2003), which splits words into two, resolving ambiguities such that the n"
W14-7002,mori-etal-2014-japanese,0,0.0128778,"that existed in the dictionary. 2.2 2.3 Syntactic Parsing As we are performing translation using syntactic parsing, it is essential that we have an accurate syntactic parser. Based on the experiments presented in Neubig and Duh (2014) we opt to use the Egret parser,7 which implements the latent variable parsing model of (Petrov et al., 2006). For the parsing models in English and Chinese, we use models trained on the English and Chinese Penn Treebanks respectively (Marcus et al., 1993; Xue et al., 2005). For the Japanese model, we train our own model on the Japanese Word Dependency Treebank (Mori et al., 2014). As this is a dependency treebank, we use head rules contained the Travatar toolkit to transform the dependency trees into phrase structure trees.8 For training, we simply use 1-best parses, but at test time we use a forest of parse trees, specifically using forests with all tree edges that exist in at least one of the 100-best parses. Tokenization and Preprocessing For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. We also per"
W14-7002,W14-7001,0,\N,Missing
W15-3057,P07-2045,0,0.00583319,"rds has an important role in CLQA tasks using knowledge bases. In addition, as a result of fine-grained manual analysis, we identify a number of factors of translation results that affect CLQA. 2 GT and YT The questions are translated using Google Translate3 (GT) and Yahoo Translate4 (YT) systems, these commercial systems can be used via web pages. While the details of these systems are not open to the public, it is likely that Google takes a largely statistical MT approach, while the Yahoo engine is rule-based. Moses The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set). A total of 277 million sentences from various genres are used in training. Travatar The questions are translated using Travatar (Neubig, 2013) (the Tra set), a tool for forest-to-string MT that has achieved competitive results on the Japanese-English language pair. The training data is the same as Moses. Data sets To create data that allows us to investigate the influence of translation on QA, we started with a standard QA data set, and created automatic and manual translations. In this section, we describe the data construction in detail. As our seed data, we used a data set ca"
W15-3057,P13-2009,0,0.0233688,"anslation results that affect CLQA accuracy. 1 Introduction Question answering (QA) is the task of searching for an answer to question sentences using some variety of information resource. Generally, documents, web pages, or knowledge bases are used as these information resources. When the language of the question differs from the language of the information resource, the task is called cross-lingual question answering (CLQA) (Magnini et al., 2004; 2 MT is also used in mono-lingual QA tasks when question sentences are translated into the formal language used to query the information resource (Andreas et al., 2013). 1 All data used in the experiments will be released upon publishing of the paper. 442 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 442–449, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. questions from Japanese to English (the HT set). languages. Correspondingly, it is of interest to investigate which factors of translation output affect CLQA accuracy, which is the first step towards designing MT systems that achieve better accuracy on the task. In this paper, to investigate the influence of translation on CLQA using k"
W15-3057,P14-1133,0,0.0141859,"ld be noted ric putting a weight an content words should be that these words are frequent, and thus even NIST used. 2) References that are actually answerable score will not be able to perform adequate evaluaby the QA system should be used. tion, indicating that other measures may be necesWe should qualify this result, however, noting sary. the fact that the results are based on the use solely of the SEMPRE parsing system. While SEMPRE has shown highly competitive results on standard Table 4: Examples of translations with mistaken QA tasks, we also plan to examine other methods syntax such as Berant and Liang (2014)’s semantic pars◦ OR what library system is the sunset branch library in - JA サンセット・ブランチ図書館はどの図書館システムに所属しますか ing through paraphrasing, which may be less sen◦ HT to what library system does sunset branch library belong ◦ GT sunset branch library do you belong to any library system sitive to superficial differences in surface forms of the translation results. We also plan to to optimize ◦ YT which library system does the sunset branch library belong to ◦ Mo sunset branch library, which belongs to the library system machine translation systems using this analysis, ◦ Tra sunset branch library, bel"
W15-3057,2003.mtsummit-papers.32,0,0.0579481,"Ě KďĂŵĂ ŐŽ ƚŽ  Figure 1: Framework of the SEMPRE semantic parsing system used to perform QA corpus, which is linked to the knowledge base through the use of named entity prediction. By default, SEMPRE uses ClueWeb096 (Callan et al., 2009) as the large text corpus and Freebase as the knowledge base. During the QA process itself, this lexicon is used to convert entities into logical forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, and the score is from 0 to 1"
W15-3057,D13-1160,0,0.0102613,"for forest-to-string MT that has achieved competitive results on the Japanese-English language pair. The training data is the same as Moses. Data sets To create data that allows us to investigate the influence of translation on QA, we started with a standard QA data set, and created automatic and manual translations. In this section, we describe the data construction in detail. As our seed data, we used a data set called Free917 (Cai and Yates, 2013). Free917 is a question set made for QA using the large-scale knowledge base “Freebase,” and is widely used in QA research (Cai and Yates, 2013; Berant et al., 2013). It consists of 917 pairs of question sentences and “logical forms” which are computer-processable expressions of the meaning of the question that can be fired against the Freebase database to return the correct answer. Following Cai and Yates (2013), we divide this data into a training set (512 pairs), dev set (129 pairs) and test set (276 pairs). In the remainder of the paper, we refer to the questions in the test set before translation as the original (OR) set. Next, to investigate the influence of translation quality on the accuracy of QA, we created a question set with five different var"
W15-3057,C04-1072,0,0.0495265,"e QA system. ĂůŝŐŶŵĞŶƚ ĚŝĚ KďĂŵĂ ŐŽ ƚŽ  Figure 1: Framework of the SEMPRE semantic parsing system used to perform QA corpus, which is linked to the knowledge base through the use of named entity prediction. By default, SEMPRE uses ClueWeb096 (Callan et al., 2009) as the large text corpus and Freebase as the knowledge base. During the QA process itself, this lexicon is used to convert entities into logical forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, a"
W15-3057,W14-3336,0,0.0532295,"Missing"
W15-3057,P13-1042,0,0.0393717,"a Graduate School of Information Science Nara Institute of Science and Technology Takayamacho 8916-5, Ikoma, Nara {sugiyama.kyoshiro.sc7, neubig}@is.naist.jp Abstract Sasaki et al., 2007). Machine translation (MT) is one of the most widely used tools to achieve CLQA (Mori and Kawagishi, 2005; Fujii et al., 2009; Kettunen, 2009).2 In the realm of monolingual question answering, recent years have seen a large increase in the use of structured knowledge bases such as Freebase (Bollacker et al., 2008), as they allow for accurate answering of questions over a variety of topics (Frank et al., 2007; Cai and Yates, 2013). However, knowledge bases are limited to only a few major languages. Thus, CLQA is particularly important for QA using knowledge bases. In contrast to the CLQA situation, where an MT system is performing translation for a downstream system to consume, in standard translation tasks the consumer of results is a human (Matsuzaki et al., 2015). In this case, it is important to define an evaluation measure which has high correlation with human evaluation, and the field of MT metrics has widely studied which features of MT results are correlated with human evaluation, and how to reflect these featu"
W15-3057,P15-2024,0,0.0128817,"realm of monolingual question answering, recent years have seen a large increase in the use of structured knowledge bases such as Freebase (Bollacker et al., 2008), as they allow for accurate answering of questions over a variety of topics (Frank et al., 2007; Cai and Yates, 2013). However, knowledge bases are limited to only a few major languages. Thus, CLQA is particularly important for QA using knowledge bases. In contrast to the CLQA situation, where an MT system is performing translation for a downstream system to consume, in standard translation tasks the consumer of results is a human (Matsuzaki et al., 2015). In this case, it is important to define an evaluation measure which has high correlation with human evaluation, and the field of MT metrics has widely studied which features of MT results are correlated with human evaluation, and how to reflect these features in automatic evaluation (Mach´acek and Bojar, 2014). However, translations which are good for humans may not be suitable for question answering. For example, according to the work of Hyodo and Akiba (2009), a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full"
W15-3057,P13-4016,1,0.836387,"tion results that affect CLQA. 2 GT and YT The questions are translated using Google Translate3 (GT) and Yahoo Translate4 (YT) systems, these commercial systems can be used via web pages. While the details of these systems are not open to the public, it is likely that Google takes a largely statistical MT approach, while the Yahoo engine is rule-based. Moses The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set). A total of 277 million sentences from various genres are used in training. Travatar The questions are translated using Travatar (Neubig, 2013) (the Tra set), a tool for forest-to-string MT that has achieved competitive results on the Japanese-English language pair. The training data is the same as Moses. Data sets To create data that allows us to investigate the influence of translation on QA, we started with a standard QA data set, and created automatic and manual translations. In this section, we describe the data construction in detail. As our seed data, we used a data set called Free917 (Cai and Yates, 2013). Free917 is a question set made for QA using the large-scale knowledge base “Freebase,” and is widely used in QA research"
W15-3057,P02-1040,0,0.105224,"l forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, and the score is from 0 to 1, where 0 is the worst and 1 is the best. Bridging To create the query for the knowledge base, SEMPRE merges neighboring logical forms in a binary tree structure. Bridging is an operation that generates predicates compatible with neighboring predicates. WER Word error rate (WER) is the edit distance between the translation and reference normalized by the sentence length. The formula"
W15-3057,N15-1149,0,0.0167969,"t these features in automatic evaluation (Mach´acek and Bojar, 2014). However, translations which are good for humans may not be suitable for question answering. For example, according to the work of Hyodo and Akiba (2009), a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full sentences on CLQA using documents or web pages, although it is not clear whether these results will apply to more structured QA using knowledge bases. There is also work on optimizing translation to improve CLQA accuracy (Riezler et al., 2014; Haas and Riezler, 2015), but these methods require a large set of translated questionanswer pairs, which may not be available in many Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accur"
W15-3057,P14-1083,0,0.0412693,"ion, and how to reflect these features in automatic evaluation (Mach´acek and Bojar, 2014). However, translations which are good for humans may not be suitable for question answering. For example, according to the work of Hyodo and Akiba (2009), a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full sentences on CLQA using documents or web pages, although it is not clear whether these results will apply to more structured QA using knowledge bases. There is also work on optimizing translation to improve CLQA accuracy (Riezler et al., 2014; Haas and Riezler, 2015), but these methods require a large set of translated questionanswer pairs, which may not be available in many Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT"
W15-3057,D10-1092,0,0.175745,"c parsing system used to perform QA corpus, which is linked to the knowledge base through the use of named entity prediction. By default, SEMPRE uses ClueWeb096 (Callan et al., 2009) as the large text corpus and Freebase as the knowledge base. During the QA process itself, this lexicon is used to convert entities into logical forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, and the score is from 0 to 1, where 0 is the worst and 1 is the best. Bridging To crea"
W15-5001,W15-5008,0,0.0448879,"Missing"
W15-5001,W15-5013,0,0.0370801,"Missing"
W15-5001,W15-5002,0,0.0289439,"Missing"
W15-5001,W14-7001,1,0.205867,"ve been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 • Evaluation method Evaluation is done both automatically and manually. For human evaluation, WAT uses crowdsourcing, which is low cost and allows multiple evaluations, as the first-stage evaluation. Also, JPO adequacy evaluation is conducted for the selected submissions according to the crowdsourcing evaluation results. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshop WAT2014(Nakazawa et al., 2014), WAT2015 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 2nd WAT, we adopt new translation subtasks “Chinese-to-Japanese and Koreanto-Japanese patent translation” in addition to the subtasks that were conducted in WAT2014. WAT is unique for the following reasons: 2 Dataset WAT uses the Asian Scientific Paper Excerpt Corpus (ASPEC)1 and JPO Patent Corpus (JPC) 2 as the dataset. 2.1 ASPEC ASPEC is constructed by t"
W15-5001,P11-2093,1,0.853461,"on results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005) 8 9 • Subtask: – Scientific papers subtask (J ↔ E, J ↔ C); – Patents subtask (C → J, K → J); • Method (SMT, RBMT, SMT and RBMT, EBMT, Other); 10 http://code.google.com/p/mecab/downloads/detail? name=mecab-ipadic-2.7.0-20070801.tar.gz 11 http://nlp.stanford.edu/software/segmenter.shtml 12 https://bitbucket.org/eunjeon/mecab-ko/ 13 https://github.com/moses-smt/mosesdecoder/tree/ RELEASE-2.1.1/scripts/tokenizer/tokenizer.perl 14 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html http://w"
W15-5001,P13-2121,0,0.0884737,"Missing"
W15-5001,2009.iwslt-papers.4,0,0.108842,"em as that at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To obtain word alignment"
W15-5001,W15-5003,1,0.888058,"Missing"
W15-5001,D10-1092,0,0.23015,"2 Automatic Evaluation System The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results. Figure 1 shows the submission interface for participants. The system requires participants to provide the following information when they upload translation results: Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005) 8 9 • Subtask: – Scientific paper"
W15-5001,P02-1040,0,0.109103,"the other system parameters. 4 4.2 Automatic Evaluation System The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results. Figure 1 shows the submission interface for participants. The system requires participants to provide the following information when they upload translation results: Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005)"
W15-5001,P06-1055,0,0.112959,"we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To obtain word alignments, GIZA++ and growdiag-final-and heuristics were used. We used 5gram language models with mod"
W15-5001,P07-2045,0,0.0208005,"ich is the same system as that at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To"
W15-5001,W15-5006,1,0.742862,"Missing"
W15-5001,W04-3250,0,0.568022,"Missing"
W15-5001,W15-5010,0,0.0468531,"Missing"
W15-5001,W15-5005,0,0.0362327,"Missing"
W15-5001,W15-5012,0,0.0480478,"Missing"
W15-5001,W15-5009,0,0.0622163,"Missing"
W15-5001,2007.mtsummit-papers.63,0,0.08182,"2015. 2015 Copyright is held by the author(s). LangPair ASPEC-JE ASPEC-JC Train 3,008,500 672,315 Dev 1,790 2,090 DevTest 1,784 2,148 Test 1,812 2,107 LangPair JPC-CJ JPC-KJ Table 1: Statistics for ASPEC. Train 1,000,000 1,000,000 Dev 2,000 2,000 DevTest 2,000 2,000 Test 2,000 2,000 Table 2: Statistics for JPC. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by the NICT from approximately 2 million Japanese-English scientific paper abstracts owned by the JST. Because the abstracts are comparable corpora, the sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score and the field symbol. The similarity scores are calculated by the method from (Utiyama and Isahara, 2007). The field symbols are single letters A-Z and show the scientific field for each document3 . The correspondence between the symbols and field names, along with the frequency and occurrence ratios for the training data, are given in the README file from ASPECJE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts owned by JST that are not contained in the tr"
W15-5001,W15-5011,0,0.0304094,"Missing"
W15-5001,W15-5007,0,0.153198,"Missing"
W15-5003,P06-1077,0,0.0142234,"that achieved the highest accuracy on all four tracks in the last year’s eval1 Particularly when comparing similar systems, such as the case of when neural MT is used for reranking existing system results. 35 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35‒41, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT."
W15-5003,P15-1002,0,0.110434,"Missing"
W15-5003,W15-3034,0,0.0197064,"s conclusive. ing points. First, we can see that the improvement in scores is very slightly sub-linear in the log number of hypotheses in the n-best list. In other words, every time we double the n-best list size we will see an improvement in accuracy that is slightly smaller than the last time we doubled the size. Second, we can note that in most cases this trend continues all the way up to our limit of 1000best lists, indicating that gains are not saturating, and we can likely expect even more improvements from using larger lists, or perhaps directly performing decoding using neural models (Alkhouli et al., 2015). The en-ja results, however, are an exception to this rule, with BLEU gains more or less saturating around the 50-best list point. 5 Effect of n-best Size on Reranking In the previous sections, we confirmed the effectiveness of n-best list reranking using neural MT models. However, reranking using n-best lists (like other search methods for MT) is an approximate search method, and its effectiveness is limited by the size of the n-best list used. In order to quantify the effect of this inexact search, we performed experiments to examine the post-reranking automatic evaluation scores of the MT"
W15-5003,D15-1166,0,0.106833,"the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. 1 Introduction Neural network models for machine translation (MT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), while still in a nascent stage, have shown impressive results in a number of translation tasks. Specifically, a number of works have demonstrated gains in BLEU score (Papineni et al., 2002) over state-of-the-art non-neural systems, both when using the neural MT model standalone (Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there has been little examination of the effect that these gains have on the subjective impressions of human users. Because BLEU generally has 2 Generation of Translation Results 2.1 Baseline System All experiments are performed on WAT2015 translation task from Japanese (ja) to/from English (en) and Chinese (zh). As a baseline, we used the NAIST syste"
W15-5003,N06-1020,0,0.0102649,"first encodes the source sentence f using bidirectional 3 Experimental Results 2 First, we calculate overall numerical results for our systems with and without the neural MT reranking model. As automatic evaluation we use the standard BLEU (Papineni et al., 2002) and reorderingoriented RIBES (Isozaki et al., 2010) metrics. In Scripts to reproduce the system are available at http: //phontron.com/project/wat2014. 3 https://github.com/neubig/egret 4 In addition, for ja-en translation, we make one modification to the parser used in the previous year’s submission, performing parser self-training (McClosky et al., 2006) using sentences from the training data that had a BLEU score greater than 0.8, and selecting the tree corresponding to the 500-best hypothesis that had the best score according to BLEU+1 (Lin and Och, 2004). 5 http://github.com/neubig/lamtram More standard log-linear interpolation resulted in similar, or slightly inferior results. 6 36 System Base Rerank B 36.6 38.2 en-ja R H 79.6 49.8 81.4 62.3 B 22.6 25.4 ja-en R H 72.3 11.8 75.0 35.5 B 40.5 43.0 zh-ja R H 83.4 25.8 84.8 35.8 B 30.1 31.6 ja-zh R 81.5 83.3 H 2.8 7.0 Table 1: Overall BLEU, RIBES, and HUMAN scores for our baseline system and s"
W15-5003,P96-1041,0,0.1011,"n-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector hj for each word fj in f . The model then proceeds to generate the target translation eˆ one word at a time, at each time step calculating soft alignments ai that are used to generate a context vector gi , which is referenced when generating the target word gi = |f | ∑ ai,j hj . (1) j=1 Attentional"
W15-5003,P08-1023,0,0.0137546,"t here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tse"
W15-5003,N04-1014,0,0.0201838,"state-of-the-art system that achieved the highest accuracy on all four tracks in the last year’s eval1 Particularly when comparing similar systems, such as the case of when neural MT is used for reranking existing system results. 35 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35‒41, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpu"
W15-5003,P13-2121,0,0.0527266,"Missing"
W15-5003,P14-2024,1,0.579915,"held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stan"
W15-5003,D10-1092,0,0.0799846,"scarce. In this paper, we attempt to close this gap by examining the gains provided by using neural MT models to rerank the hypotheses a state-of-the-art non-neural MT system, both from the objective and subjective perspectives. Specifically, as part of the Nara Institute of Science and Technology (NAIST) submission to the Workshop on Asian Translation (WAT) 2015 (Nakazawa et al., 2015), we generate reranked and non-reranked translation results in four language pairs (Section 2). Based on these translation results, we calculate scores according to automatic evaluation measures BLEU and RIBES (Isozaki et al., 2010), and a manual evaluation that involves comparing hypotheses to a baseline system (Section 3). Next, we perform a detailed analysis of the cases in which subjective impressions improved or degraded due to neural MT reranking, and identify major areas in which neural reranking improves results, and areas in which reranking is less helpful (Section 4). Finally, as an auxiliary result, we also examine the effect that the size of the n-best list used in reranking has on the improvement of translation results (Section 5). Abstract This year, the Nara Institute of Science and Technology (NAIST)’s su"
W15-5003,P11-2093,1,0.825294,"d encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector"
W15-5003,P15-1001,0,0.0419024,"ns of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. 1 Introduction Neural network models for machine translation (MT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), while still in a nascent stage, have shown impressive results in a number of translation tasks. Specifically, a number of works have demonstrated gains in BLEU score (Papineni et al., 2002) over state-of-the-art non-neural systems, both when using the neural MT model standalone (Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there has been little examination of the effect that these gains have on the subjective impressions of human users. Because BLEU generally has 2 Generation of Translation Results 2.1 Baseline System All experiments are performed on WAT2015 translation task from Japanese (ja) to/from English (en) and Chinese (zh). As a baseline, we used the NAIST system for WAT 2014 (Neub"
W15-5003,P13-4016,1,0.860103,"used the NAIST system for WAT 2014 (Neubig, 2014), a state-of-the-art system that achieved the highest accuracy on all four tracks in the last year’s eval1 Particularly when comparing similar systems, such as the case of when neural MT is used for reranking existing system results. 35 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35‒41, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa a"
W15-5003,D13-1176,0,0.0518765,"l machine translation models. Experiments re-confirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. 1 Introduction Neural network models for machine translation (MT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), while still in a nascent stage, have shown impressive results in a number of translation tasks. Specifically, a number of works have demonstrated gains in BLEU score (Papineni et al., 2002) over state-of-the-art non-neural systems, both when using the neural MT model standalone (Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there"
W15-5003,W14-7002,1,0.820443,"2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there has been little examination of the effect that these gains have on the subjective impressions of human users. Because BLEU generally has 2 Generation of Translation Results 2.1 Baseline System All experiments are performed on WAT2015 translation task from Japanese (ja) to/from English (en) and Chinese (zh). As a baseline, we used the NAIST system for WAT 2014 (Neubig, 2014), a state-of-the-art system that achieved the highest accuracy on all four tracks in the last year’s eval1 Particularly when comparing similar systems, such as the case of when neural MT is used for reranking existing system results. 35 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35‒41, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical M"
W15-5003,P03-1054,0,0.0231386,"the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent netw"
W15-5003,P03-1021,0,0.0398538,"ls. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector hj for each word fj in f . The model then proceeds to generate the target translation eˆ one word at a time, at each time step calculating soft alignments ai that are used to generate a context vector gi , which is referenced when generating the target word gi = |f | ∑ ai,j hj . (1) j=1 Attentional models have a number of appealing properties, such as being theoretically able to encode variable length sequences without worrying about memory constr"
W15-5003,W04-3250,0,0.0309746,"Missing"
W15-5003,P02-1040,0,0.103382,"arry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. 1 Introduction Neural network models for machine translation (MT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), while still in a nascent stage, have shown impressive results in a number of translation tasks. Specifically, a number of works have demonstrated gains in BLEU score (Papineni et al., 2002) over state-of-the-art non-neural systems, both when using the neural MT model standalone (Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there has been little examination of the effect that these gains have on the subjective impressions of human users. Because BLEU generally has 2 Generation of Translation Results 2.1 Baseline System All experiments are performed on WAT2015 tran"
W15-5003,D12-1096,0,0.016682,"tained by Y,” as shown in the example. The baseline system does not include any explicit features to make this distinction between whether a verb is part of a relative clause or not, and thus made a number of mistakes of this variety. However, it is evident that the neural MT model has learned to make this distinction, greatly reducing the number of these errors. The third subcategory is similar to the first, but explicitly involves the correct interpretation of coordinate structures. It is well known that syntactic parsers often make mistakes in their interpretation of coordinate structures (Kummerfeld et al., 2012). Of course, the parser used in our syntaxbased MT system is no exception to this rule, and parse errors often cause coordinate phrases to be broken apart on the target side, as is the case in the example’s “local heating and ablation.” The fact that the neural MT models were able to correct a large number of errors related to these structures suggests that they are able to successfully determine whether two phrases are coordinated or not, and keep them together on the target side. The final sub-category of the top four is related to verb conjugation agreement. Many of the examples related to"
W15-5003,P06-1055,0,0.0147746,"orpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector hj for each word fj in f . The model then proceeds to generate the target translation eˆ one word at a time, at each time step calculating soft alignments ai that are used to gen"
W15-5003,P10-1017,0,0.0237546,", 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implem"
W15-5003,I05-3027,0,0.017369,"08), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector hj for each word fj in f . The model then proceed"
W15-5003,vilar-etal-2006-error,0,0.0653908,"Missing"
W15-5003,C04-1072,0,\N,Missing
W16-3640,J12-1001,0,0.058228,"Missing"
W16-3640,N13-2012,0,0.0206591,"prevalent for utterances of particular dialogue acts? Does the level of entrainment increase as dialogue progresses? 310 Proceedings of the SIGDIAL 2016 Conference, pages 310–318, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics 2 Related Works In detail, we can express this formula with word count CS1 (w) and CS2 (w), and all of words W as, 2.1 Varieties of entrainment En(V ) = ∑ CS2 (w) CS1 (w) − −∑ ∑ . wi ∈W CS1 (wi ) C (w ) i wi ∈W S2 As mentioned in the introduction, entrainment has been shown to occur at almost every level of human communication (Levitan, 2013), including both human-human and human-system conversation. In human-human conversation, Kawahara et al. (2015) showed the synchrony of backchannels to the preceding utterances in attentive listening, and they investigated the relationship between morphological patterns of backchannels and the syntactic complexities of preceding utterances. Levitan et al. (2015) showed the entrainment of latency in turn taking. In human-system conversation, Campbell and Scherer (2010) tried to predict user’s turn taking behavior by considering entrainment. Fandrianto and Eskenazi (2012) modeled a dialogue stra"
W16-3640,P08-2043,0,0.82587,"ical patterns of backchannels and the syntactic complexities of preceding utterances. Levitan et al. (2015) showed the entrainment of latency in turn taking. In human-system conversation, Campbell and Scherer (2010) tried to predict user’s turn taking behavior by considering entrainment. Fandrianto and Eskenazi (2012) modeled a dialogue strategy to increase the accuracy of speech recognition by using entrainment intentionally. Levitan (2013) unified these two works. One of the most important questions about entrainment with respect to dialogue systems is its association with dialogue quality. Nenkova et al. (2008) proposed a score to evaluate the lexical entrainment in highly frequent words, and found that the score has high correlation with task success and engagement. This indicates that lexical entrainment has an important role in dialogue. In addition, it suggests that entrainment of lexical choice is probably affected by more detailed dialogue information, such as dialogue act. w∈V (2) Nenkova et al. (2008) used following word classes as V . 25MFC: 25 Most frequent words in the corpus. The idea of using only frequent words is based on the fact that we would like to avoid the score being affected b"
W16-3640,P07-1102,0,0.0411561,"t, structural level. In this paper, we investigate the effect of entrainment on dialogue acts and on lexical choice given dialogue acts, as well as how entrainment changes during a dialogue. We also define a novel measure of entrainment to measure these various types of entrainment. These results may serve as guidelines for dialogue systems that would like to entrain with users in a similar manner. 1 Introduction Entrainment is a conversational phenomenon in which dialogue participants synchronize to each other with regards to various factors: lexical choice (Brennan and Clark, 1996), syntax (Reitter and Moore, 2007; Ward and Litman, 2007), style (Niederhoffer and Pennebaker, 2002; DanescuNiculescu-Mizil et al., 2011), acoustic prosody (Natale, 1975; Coulston et al., 2002; Ward and Litman, 2007; Kawahara et al., 2015), pronunciation (Pardo, 2006) and turn taking (Campbell and Scherer, 2010; Beˇnuˇs et al., 2014). Previous works have reported that entrainment is correlated with dialogue success, naturalness and engagement. However, there is much that is still unclear with regards to how entrainment affects the overall flow of the dialogue. For example, can entrainment also be observed in choice of dialog"
W16-4601,W16-4616,1,0.921374,"n and Communication Technology Waseda University Ehara NLP Research Laboratory NTT Communication Science Laboratories Weblio, Inc. Indian Institute of Technology Bombay Japan Patent Information Organization Indian Institute of Technology Patna University of Tokyo University of Tokyo ASPEC JPC BPPT IITBC pivot JE EJ JC CJ JE EJ JC CJ JK KJ IE EI HE EH HJ JH ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 7: List of participants who submitted translation results to WAT2016 and their participation in each subtasks. Team ID NAIST (Neubig, 2016) Kyoto-U (Cromieres et al., 2016) TMU (Yamagishi et al., 2016) bjtu nlp (Li et al., 2016) Sense (Tan, 2016) NICT-2 (Imamura and Sumita, 2016) WASUIPS (Yang and Lepage, 2016) EHR (Ehara, 2016) ntt (Sudoh and Nagata, 2016) TOKYOMT (Shu and Miura, 2016) IITB-EN-ID (Singh et al., 2016) JAPIO (Kinoshita et al., 2016) IITP-MT (Sen et al., 2016) UT-KAY (Hashimoto et al., 2016) UT-AKY (Eriguchi et al., 2016) 7 Evaluation Results In this section, the evaluation results for WAT2016 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website24 . 7.1 Offi"
W16-4601,W16-4609,0,0.0501384,"Missing"
W16-4601,W16-4617,0,0.0323933,"Missing"
W16-4601,P13-2121,0,0.0165266,"evelopment data. 3.2 Common Settings for Baseline SMT We used the following tools for tokenization. • Juman version 7.08 for Japanese segmentation. • Stanford Word Segmenter version 2014-01-049 (Chinese Penn Treebank (CTB) model) for Chinese segmentation. • The Moses toolkit for English and Indonesian tokenization. • Mecab-ko10 for Korean segmentation. • Indic NLP Library11 for Hindi segmentation. To obtain word alignments, GIZA++ and grow-diag-final-and heuristics were used. We used 5-gram language models with modified Kneser-Ney smoothing, which were built using a tool in the Moses toolkit (Heafield et al., 2013). 3.3 Phrase-based SMT We used the following Moses configuration for the phrase-based SMT system. • distortion-limit – 20 for JE, EJ, JC, and CJ – 0 for JK, KJ, HE, and EH – 6 for IE and EI • msd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuring The default values were used for the other system parameters. 3.4 Hierarchical Phrase-based SMT We used the following Moses configuration for the hierarchical phrase-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-"
W16-4601,2009.iwslt-papers.4,0,0.0367694,"tp://lotus.kuee.kyoto-u.ac.jp/WAT/Hindi-corpus/WAT2016-Ja-Hi.zip 3 LangPair IITB-EH IITB-JH Train 1,492,827 152,692 Dev 520 1,566 Test 2,507 2,000 Monolingual Corpus (Hindi) 45,075,279 - Table 4: Statistics for IITB Corpus. SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page7 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 5. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 7 http://lotus.kuee.kyot"
W16-4601,W16-4611,0,0.032371,"Missing"
W16-4601,D10-1092,0,0.148885,"rser to obtain source language syntax. We used the following Moses configuration for the baseline tree-to-string syntax-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. The default values were used for the other system parameters. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016. All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994"
W16-4601,W16-4612,0,0.0719596,"kyo ASPEC JPC BPPT IITBC pivot JE EJ JC CJ JE EJ JC CJ JK KJ IE EI HE EH HJ JH ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 7: List of participants who submitted translation results to WAT2016 and their participation in each subtasks. Team ID NAIST (Neubig, 2016) Kyoto-U (Cromieres et al., 2016) TMU (Yamagishi et al., 2016) bjtu nlp (Li et al., 2016) Sense (Tan, 2016) NICT-2 (Imamura and Sumita, 2016) WASUIPS (Yang and Lepage, 2016) EHR (Ehara, 2016) ntt (Sudoh and Nagata, 2016) TOKYOMT (Shu and Miura, 2016) IITB-EN-ID (Singh et al., 2016) JAPIO (Kinoshita et al., 2016) IITP-MT (Sen et al., 2016) UT-KAY (Hashimoto et al., 2016) UT-AKY (Eriguchi et al., 2016) 7 Evaluation Results In this section, the evaluation results for WAT2016 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website24 . 7.1 Official Evaluation Results Figures 2, 3, 4 and 5 show the official evaluation results of ASPEC subtasks, Figures 6, 7, 8, 9 and 10 show those of JPC subtasks, Figures 11 and 12 show those of BPPT subtasks and Figures 13 and 14 show those of IITB subtasks. Each figure contains automa"
W16-4601,P07-2045,0,0.0173325,"ee syntax-based 6 http://lotus.kuee.kyoto-u.ac.jp/WAT/Hindi-corpus/WAT2016-Ja-Hi.zip 3 LangPair IITB-EH IITB-JH Train 1,492,827 152,692 Dev 520 1,566 Test 2,507 2,000 Monolingual Corpus (Hindi) 45,075,279 - Table 4: Statistics for IITB Corpus. SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page7 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 5. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 7 h"
W16-4601,W04-3250,0,0.322367,"Missing"
W16-4601,W15-5008,0,0.0203255,"eement between the workers, we calculated the Fleiss’ κ (Fleiss and others, 1971) values. The results are shown in Table 20. We can see that the κ values are larger for X → J translations than for J → X translations. This may be because the majority of the workers are Japanese, and the evaluation of one’s mother tongue is much easier than for other languages in general. 7.3 Chronological Evaluation Figure 15 shows the chronological evaluation results of 4 subtasks of ASPEC and 2 subtasks of JPC. The Kyoto-U (2016) (Cromieres et al., 2016), ntt (2016) (Sudoh and Nagata, 2016) and naver (2015) (Lee et al., 2015) are NMT systems, the NAIST (2015) (Neubig et al., 2015) is a forest-to-string SMT system, Kyoto-U (2015) (Richardson et al., 2015) is a dependency tree-to-tree EBMT system and JAPIO (2016) (Kinoshita et al., 2016) system is a phrase-based SMT system. What we can see is that in ASPEC-JE and EJ, the overall quality is improved from the last year, but the ratio of grade 5 is decreased. This is because the NMT systems can output much fluent translations 24 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html 12 but the adequacy is worse. As for ASPEC-JC and CJ, the quality is very much impro"
W16-4601,W16-4608,0,0.0513487,"Missing"
W16-4601,W14-7001,1,0.885529,"red tasks from the 3rd workshop on Asian translation (WAT2016) including J↔E, J↔C scientific paper translation subtasks, C↔J, K↔J, E↔J patent translation subtasks, I↔E newswire subtasks and H↔E, H↔J mixed domain subtasks. For the WAT2016, 15 institutions participated in the shared tasks. About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014) and WAT2015 (Nakazawa et al., 2015), WAT2016 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 3rd WAT, we adopt new translation subtasks with English-Japanese patent description, Indonesian-English news description and Hindi-English and Hindi-Japanese mixed domain corpus in addition to the subtasks that were conducted in WAT2015. Furthermore, we invited research papers on topics related to the machine translation"
W16-4601,W15-5001,1,0.853738,"sian translation (WAT2016) including J↔E, J↔C scientific paper translation subtasks, C↔J, K↔J, E↔J patent translation subtasks, I↔E newswire subtasks and H↔E, H↔J mixed domain subtasks. For the WAT2016, 15 institutions participated in the shared tasks. About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014) and WAT2015 (Nakazawa et al., 2015), WAT2016 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 3rd WAT, we adopt new translation subtasks with English-Japanese patent description, Indonesian-English news description and Hindi-English and Hindi-Japanese mixed domain corpus in addition to the subtasks that were conducted in WAT2015. Furthermore, we invited research papers on topics related to the machine translation, especially for Asian languages. Th"
W16-4601,P11-2093,1,0.785718,"s et al., 2015). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016. All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 13 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0 14 . For Chinese segmentation we used two different tools: KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 15 (Tseng, 2005). For Korean segmentation we used mecabko 16 . For English and Indonesian segmentations we used tokenizer.perl 17 in the Moses toolkit. For Hindi segmentation we used Indic NLP Library 18 . Detailed procedures for the automatic evaluation are shown on the WAT2016 evaluation web page 19"
W16-4601,W16-4610,1,0.870902,"Missing"
W16-4601,P02-1040,0,0.0994357,"sed SMT We used the Berkeley parser to obtain source language syntax. We used the following Moses configuration for the baseline tree-to-string syntax-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. The default values were used for the other system parameters. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016. All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman versi"
W16-4601,P06-1055,0,0.0157429,"Train 1,492,827 152,692 Dev 520 1,566 Test 2,507 2,000 Monolingual Corpus (Hindi) 45,075,279 - Table 4: Statistics for IITB Corpus. SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page7 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 5. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 7 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 4 5 System ID SMT Phrase SMT Hiero SMT S2T SMT T2S RBMT X RBMT X RBMT X RBMT X"
W16-4601,W15-5006,1,0.884753,"Missing"
W16-4601,W16-4622,0,0.0346093,"Missing"
W16-4601,W16-4623,0,0.0353036,"Missing"
W16-4601,W16-4604,0,0.0409014,"Missing"
W16-4601,W16-4621,0,0.131935,"mation Organization Indian Institute of Technology Patna University of Tokyo University of Tokyo ASPEC JPC BPPT IITBC pivot JE EJ JC CJ JE EJ JC CJ JK KJ IE EI HE EH HJ JH ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 7: List of participants who submitted translation results to WAT2016 and their participation in each subtasks. Team ID NAIST (Neubig, 2016) Kyoto-U (Cromieres et al., 2016) TMU (Yamagishi et al., 2016) bjtu nlp (Li et al., 2016) Sense (Tan, 2016) NICT-2 (Imamura and Sumita, 2016) WASUIPS (Yang and Lepage, 2016) EHR (Ehara, 2016) ntt (Sudoh and Nagata, 2016) TOKYOMT (Shu and Miura, 2016) IITB-EN-ID (Singh et al., 2016) JAPIO (Kinoshita et al., 2016) IITP-MT (Sen et al., 2016) UT-KAY (Hashimoto et al., 2016) UT-AKY (Eriguchi et al., 2016) 7 Evaluation Results In this section, the evaluation results for WAT2016 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website24 . 7.1 Official Evaluation Results Figures 2, 3, 4 and 5 show the official evaluation results of ASPEC subtasks, Figures 6, 7, 8, 9 and 10 show those of JPC subtasks, Figures 11 and 12 show those of"
W16-4601,W16-4618,0,0.0231168,"Missing"
W16-4601,2007.mtsummit-papers.63,0,0.716723,"Information and Communications Technology (NICT). It consists of a JapaneseEnglish scientific paper abstract corpus (ASPEC-JE), which is used for J↔E subtasks, and a JapaneseChinese scientific paper excerpt corpus (ASPEC-JC), which is used for J↔C subtasks. The statistics for each corpus are described in Table1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by the NICT from approximately 2 million JapaneseEnglish scientific paper abstracts owned by the JST. Because the abstracts are comparable corpora, the sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score and the field symbol. The similarity scores are calculated by the method from (Utiyama and Isahara, 2007). The field symbols are single letters A-Z and show the scientific field for each document5 . The correspondence between the symbols and field names, along with the frequency and occurrence ratios for the training data, are given in the README file from ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts owned by JST that are not contained in the t"
W16-4601,W16-4620,0,0.0195884,"Missing"
W16-4601,W16-4619,0,0.038307,"Missing"
W16-4610,D16-1162,1,0.86575,"lunsom, 2013; Sutskever et al., 2014)), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks (Luong and Manning, 2015; Sennrich et al., 2016a). In this paper, we describe NMT systems for the Japanese-English scientific paper translation task of the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016a). The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (§2). In particular we focus on two. First, we follow the recent work of Arthur et al. (2016) in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model (§3). Second, we incorporate minimum-risk training (Shen et al., 2016) to optimize the parameters of the model to improve translation accuracy (§4). In experiments (§5), we examine the effect of each of these improvements, and find that they both contribute to overall translation accuracy, leading to state-of-the-art results on the Japanese-English translation task. 2 Baseline Neural Machine Translation Model Our baseline translation model is the attentional model implemented in"
W16-4610,N13-1073,0,0.0410134,"crete Lexicons The first modification that we make to the base model is incorporating discrete lexicons to improve translation probabilities, according to the method of Arthur et al. (2016). The motivation behind this method is twofold: Handling low-frequency words: Neural machine translation systems tend to have trouble translating low-frequency words (Sutskever et al., 2014), so incorporating translation lexicons with good coverage of content words could improve translation accuracy of these words. Training speed: Training the alignments needed for discrete lexicons can be done efficiently (Dyer et al., 2013), and by seeding the neural MT system with these efficiently trained alignments it is easier to learn models that achieve good results more quickly. The model starts with lexical translation probabilities pl (e|f ) for individual words, which have been obtained through traditional word alignment methods. These probabilities must first be converted to a form that can be used together with pm (ei |ei−1 1 , F ). Given input sentence F , we can construct a matrix in which each column corresponds to a word in the input sentence, each row corresponds to a word in the VE , and the entry corresponds t"
W16-4610,D13-1176,0,0.0563174,"ence and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task. 1 Introduction Neural machine translation (NMT; (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014)), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks (Luong and Manning, 2015; Sennrich et al., 2016a). In this paper, we describe NMT systems for the Japanese-English scientific paper translation task of the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016a). The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (§2). In particular we focus on two. First, we follow the recent work of Arthur et al"
W16-4610,C04-1072,0,0.0572997,"n accuracy (such as BLEU (Papineni et al., 2002)) and this maximum likelihood objective. To remove this disconnect, we use the method of Shen et al. (2016) to optimize our systems directly using BLEU score. Specifically, we define the following loss function over the model parameters θ for a single training sentence pair ⟨F, E⟩ ∑ LF,E (θ) = err(E, E ′ )P (E ′ |F ; θ), E′ which is summed over all potential translations E ′ in the target language. Here err(·) can be an arbitrary error function, which we define as 1 − SBLEU(E, E ′ ), where SBLEU(·) is the smoothed BLEU score (BLEU+1) proposed by Lin and Och (2004). As the number of target-language translations E ′ is infinite, the sum above is intractable, so we approximate the sum by randomly sampling a subset of translations S according to P (E|F ; θ), then enumerating over this sample:2 LF,E (θ) = ∑ P (E ′ |F ; θ) . ′′ E ′′ ∈S P (E |F ; θ) err(E, E ′ ) ∑ E ′ ∈S This objective function is then modified by introducing a scaling factor α, which makes it possible to adjust the smoothness of the distribution being optimized, which in turn results in adjusting the strength with which the model will try to push good translations to have high probabilities."
W16-4610,2015.iwslt-evaluation.11,0,0.084922,"ine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task. 1 Introduction Neural machine translation (NMT; (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014)), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks (Luong and Manning, 2015; Sennrich et al., 2016a). In this paper, we describe NMT systems for the Japanese-English scientific paper translation task of the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016a). The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (§2). In particular we focus on two. First, we follow the recent work of Arthur et al. (2016) in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model (§3). Second, we incorporate minimum-risk training (Shen et"
W16-4610,D15-1166,0,0.377349,"the highest translation evaluation scores for the task. 1 Introduction Neural machine translation (NMT; (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014)), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks (Luong and Manning, 2015; Sennrich et al., 2016a). In this paper, we describe NMT systems for the Japanese-English scientific paper translation task of the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016a). The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (§2). In particular we focus on two. First, we follow the recent work of Arthur et al. (2016) in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model (§3). Second, we incorporate minimum-risk training (Shen et al., 2016) to optimize the parameters of the model to improve translation accuracy (§4). In experiments (§5), we examine the effect of each of these improvements, and find that they both contribute to overall translation accuracy, leading to state-of-the-art results on the Japanese-English transla"
W16-4610,P11-2093,1,0.766156,"3 99.2 25.9 75.5 98.0 26.2 76.0 98.6 26.9 76.3 98.8 26.4 75.9 97.7 26.3 75.7 97.3 29.3 77.3 97.9 Table 1: Overall BLEU, RIBES, and length ratio for systems with various types of attention (dot product or multi-layer perceptron), lexicon (yes/no and which value of λ), training algorithm (maximum likelihood or minimum risk), and word penalty value. 5 Experiments 5.1 Experimental Setup To create data to train the model, we use the top 2M sentences of the ASPEC Japanese-English training corpus (Nakazawa et al., 2016b) provided by the task. The Japanese size of the corpus is tokenized using KyTea (Neubig et al., 2011), and the English side is tokenized with the tokenizer provided with the Travatar toolkit (Neubig, 2013). Japanese is further normalized so all full-width roman characters and digits are normalized to half-width. The words are further broken into subword units using joint byte pair encoding (Sennrich et al., 2016b) with 100,000 merge operations. 5.2 Experimental Results In Figure 1 we show results for various settings regarding attention, the use of lexicons, training criterion, and word penalty. In addition, we calculate the ensemble of 6 models, where the average probability assigned by each"
W16-4610,P13-4016,1,0.859147,"ll BLEU, RIBES, and length ratio for systems with various types of attention (dot product or multi-layer perceptron), lexicon (yes/no and which value of λ), training algorithm (maximum likelihood or minimum risk), and word penalty value. 5 Experiments 5.1 Experimental Setup To create data to train the model, we use the top 2M sentences of the ASPEC Japanese-English training corpus (Nakazawa et al., 2016b) provided by the task. The Japanese size of the corpus is tokenized using KyTea (Neubig et al., 2011), and the English side is tokenized with the tokenizer provided with the Travatar toolkit (Neubig, 2013). Japanese is further normalized so all full-width roman characters and digits are normalized to half-width. The words are further broken into subword units using joint byte pair encoding (Sennrich et al., 2016b) with 100,000 merge operations. 5.2 Experimental Results In Figure 1 we show results for various settings regarding attention, the use of lexicons, training criterion, and word penalty. In addition, we calculate the ensemble of 6 models, where the average probability assigned by each of the models is used to determine the probability of the next word at test time. From the results in t"
W16-4610,P02-1040,0,0.0950007,"still be in the probability domain after the softmax is calculated, and add the hyper-parameter ϵ to prevent zero probabilities from becoming −∞ after taking the log. We test various values including ϵ = {10−4 , 10−5 , 10−6 } in experiments. 4 Minimum Risk Training The second improvement that we make to our model is the use of minimum risk training. As mentioned in Section 2.2 our baseline model optimizes the model parameters according to maximize the likelihood of the training data. However, there is a disconnect between the evaluation of our systems using translation accuracy (such as BLEU (Papineni et al., 2002)) and this maximum likelihood objective. To remove this disconnect, we use the method of Shen et al. (2016) to optimize our systems directly using BLEU score. Specifically, we define the following loss function over the model parameters θ for a single training sentence pair ⟨F, E⟩ ∑ LF,E (θ) = err(E, E ′ )P (E ′ |F ; θ), E′ which is summed over all potential translations E ′ in the target language. Here err(·) can be an arbitrary error function, which we define as 1 − SBLEU(E, E ′ ), where SBLEU(·) is the smoothed BLEU score (BLEU+1) proposed by Lin and Och (2004). As the number of target-lang"
W16-4610,W16-2309,0,0.0390702,"els. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task. 1 Introduction Neural machine translation (NMT; (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014)), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks (Luong and Manning, 2015; Sennrich et al., 2016a). In this paper, we describe NMT systems for the Japanese-English scientific paper translation task of the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016a). The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (§2). In particular we focus on two. First, we follow the recent work of Arthur et al. (2016) in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model (§3). Second, we incorporate minimum-risk training (Shen et al., 2016) to optimize"
W16-4610,P16-1162,0,0.355378,"els. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task. 1 Introduction Neural machine translation (NMT; (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014)), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks (Luong and Manning, 2015; Sennrich et al., 2016a). In this paper, we describe NMT systems for the Japanese-English scientific paper translation task of the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016a). The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (§2). In particular we focus on two. First, we follow the recent work of Arthur et al. (2016) in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model (§3). Second, we incorporate minimum-risk training (Shen et al., 2016) to optimize"
W16-4610,P16-1159,0,0.129464,"ng, 2015; Sennrich et al., 2016a). In this paper, we describe NMT systems for the Japanese-English scientific paper translation task of the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016a). The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (§2). In particular we focus on two. First, we follow the recent work of Arthur et al. (2016) in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model (§3). Second, we incorporate minimum-risk training (Shen et al., 2016) to optimize the parameters of the model to improve translation accuracy (§4). In experiments (§5), we examine the effect of each of these improvements, and find that they both contribute to overall translation accuracy, leading to state-of-the-art results on the Japanese-English translation task. 2 Baseline Neural Machine Translation Model Our baseline translation model is the attentional model implemented in the lamtram toolkit (Neubig, 2015), which is a combination of the models of Bahdanau et al. (2015) and Luong et al. (2015) that we found to be effective. We describe the model briefly he"
W16-4610,W16-2323,0,\N,Missing
W17-3203,D16-1162,1,0.941561,"n Table 1, are drawn from shared translation tasks at the 2016 ACL Conference on Machine Translation (WMT16)4 and the 2016 International Workshop on Spoken Language Translation (IWSLT16)5 . 1 https://github.com/neubig/lamtram https://github.com/clab/dynet 3 Translation dictionaries are learned from the system’s training data using fast align (Dyer et al., 2013). 4 http://statmt.org/wmt16 (Bojar et al., 2016) 5 https://workshop2016.iwslt.org, https: //wit3.fbk.eu (Cettolo et al., 2012) 2 19 of generally being “good enough” have made it a popular choice for researchers and NMT toolkit authors6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017). While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness. In the following section, we explore the relative quality of the optima found by these training algorithms. 3.2 set perplexity every 50K training sentences for the first training run and every 25K sentences for subsequent runs. For IWSLT systems, we evaluate every 25K sentences and then every 6,250 sentences. Training stops when no improvement in perplexity has b"
W17-3203,N13-1073,0,0.0409993,"compared to annealing SGD (Wu et al., 2016). However, Adam’s speed and reputation Data Sets We evaluate systems on a selection of public data sets covering a range of data sizes, language directions, and morphological complexities. These sets, described in Table 1, are drawn from shared translation tasks at the 2016 ACL Conference on Machine Translation (WMT16)4 and the 2016 International Workshop on Spoken Language Translation (IWSLT16)5 . 1 https://github.com/neubig/lamtram https://github.com/clab/dynet 3 Translation dictionaries are learned from the system’s training data using fast align (Dyer et al., 2013). 4 http://statmt.org/wmt16 (Bojar et al., 2016) 5 https://workshop2016.iwslt.org, https: //wit3.fbk.eu (Cettolo et al., 2012) 2 19 of generally being “good enough” have made it a popular choice for researchers and NMT toolkit authors6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017). While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness. In the following section, we explore the relative quality of the optima found by these training algorithms. 3"
W17-3203,P84-1044,0,0.34221,"Missing"
W17-3203,D17-1151,0,0.0381184,"ation tasks at the 2016 ACL Conference on Machine Translation (WMT16)4 and the 2016 International Workshop on Spoken Language Translation (IWSLT16)5 . 1 https://github.com/neubig/lamtram https://github.com/clab/dynet 3 Translation dictionaries are learned from the system’s training data using fast align (Dyer et al., 2013). 4 http://statmt.org/wmt16 (Bojar et al., 2016) 5 https://workshop2016.iwslt.org, https: //wit3.fbk.eu (Cettolo et al., 2012) 2 19 of generally being “good enough” have made it a popular choice for researchers and NMT toolkit authors6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017). While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness. In the following section, we explore the relative quality of the optima found by these training algorithms. 3.2 set perplexity every 50K training sentences for the first training run and every 25K sentences for subsequent runs. For IWSLT systems, we evaluate every 25K sentences and then every 6,250 sentences. Training stops when no improvement in perplexity has been seen in 20 evaluations. For each e"
W17-3203,P15-1001,0,0.0330033,"and our recommended systems (byte pair encoding and annealing Adam, with and without ensembling). Scores for single models are averaged over 3 independent optimizer runs while scores for ensembles are the result of combining 3 runs. CS-EN Adam +Annealing +Ensemble Word BPE Word BPE BPE Baseline 20.2 22.1 21.0 23.0 25.5 Dropout 20.7 22.7 21.4 23.6 26.1 Lexicon Bias 20.7 22.5 20.6 22.7 25.2 Pre-Translation – 23.1 – 23.8 25.8 Bootstrapping 20.7 23.2 21.6 23.6 26.2 duce training time, some work ensembles different training checkpoints of the same model rather than using fully independent models (Jean et al., 2015; Sennrich et al., 2016a). While checkpoint ensembling is shown to be effective for improving BLEU scores under resource constraints, it does so with less diverse models. As discussed in recent work and demonstrated in our experiments in §6, model diversity is a key component in building strong NMT ensembles (Jean et al., 2015; Sennrich et al., 2016a; Farajian et al., 2016). For these reasons, we recommend evaluating new techniques on systems that ensemble multiple independently trained models for the most reliable results. Results showing both the effectiveness of ensembles and the importance"
W17-3203,2012.eamt-1.60,0,0.00782967,"ion of public data sets covering a range of data sizes, language directions, and morphological complexities. These sets, described in Table 1, are drawn from shared translation tasks at the 2016 ACL Conference on Machine Translation (WMT16)4 and the 2016 International Workshop on Spoken Language Translation (IWSLT16)5 . 1 https://github.com/neubig/lamtram https://github.com/clab/dynet 3 Translation dictionaries are learned from the system’s training data using fast align (Dyer et al., 2013). 4 http://statmt.org/wmt16 (Bojar et al., 2016) 5 https://workshop2016.iwslt.org, https: //wit3.fbk.eu (Cettolo et al., 2012) 2 19 of generally being “good enough” have made it a popular choice for researchers and NMT toolkit authors6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017). While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness. In the following section, we explore the relative quality of the optima found by these training algorithms. 3.2 set perplexity every 50K training sentences for the first training run and every 25K sentences for subsequent runs. For IWS"
W17-3203,D16-1139,0,0.0141278,"matic improvements in BLEU scores for model ensembles (Sutskever et al., 2014; Sennrich et al., 2016a). While this technique is conceptually simple, it requires training and decoding with multiple translation models, often at significant resource costs. However, these costs are either mitigated or justified when building real-world systems or evaluating techniques that should be applicable to those systems. Decoding costs can be reduced by using knowledge distillation techniques to train a single, compact model to replicate the output of an ensemble (Hinton et al., 2015; Kuncoro et al., 2016; Kim and Rush, 2016). Researchers can skip this timeconsuming step, evaluating the ensemble directly, while real-world system engineers can rely on it to make deployment of ensembles practical. To reThe final two categories evaluate handling of true out-of-vocabulary items. For OOVs that should be translated, the full-word system will always score zero, lacking any mechanism for producing words not in its vocabulary or dictionary. The more interesting result is in the relatively low scores for OOVs that should simply be copied from source to target. While phrase-based systems can reliably pass OOVs through 1:1, f"
W17-3203,2016.amta-researchers.10,0,0.0149068,"-of-the-art systems and nuanced interactions between techniques that we have reported. Based on these results, we highly recommend evaluating new techniques on systems that are at least this strong and representative of those deployed for real-world use. suggested translation from the phrase-based system. As pre-translation doubles source vocabulary size and input length, we only apply it to sub-word systems to keep complexity reasonable. Data bootstrapping: Expand training data by extracting phrase pairs (sub-sentence translation examples) and including them as additional training instances (Chen et al., 2016). We apply a novel extension where we train a phrase-based system and use it to re-translate the training data, providing a near-optimal phrase segmentation as a byproduct. We use these phrases in place of the heuristically chosen phrases in the original work, improving coverage and leading to more fine-grained translation examples. 6.2 Experimental Results The immediately noticeable trend from Table 4 is that while all techniques improve basic systems, only a single technique, data bootstrapping, improves the fully strengthened system for both data sets (and barely so). This can be attributed"
W17-3203,P11-2031,0,0.0190447,"bulary is limited to the top 50K source words and 50K target words by frequency, with all others mapped to an unk token. A post-processing step replaces any unk tokens in system output by attempting a dictionary lookup3 of the corresponding source word (highest attention score) and backing off to copying the source word directly (Luong et al., 2015). Experiments in each section evaluate this system against incremental extensions such as improved model vocabulary or training algorithm. Evaluation is conducted by average BLEU score over multiple independent training runs (Papineni et al., 2002; Clark et al., 2011). 2.2 Scenario DE-EN EN-FI RO-EN EN-FR CS-EN Validation (Dev) Set News test 2015 News test 2015 News dev 2016 TED test 2013+2014 TED test 2012+2013 Test Set News test 2016 News test 2016 News test 2016 TED test 2015+2016 TED test 2015+2016 Table 1: Top: parallel training data available for all scenarios. Bottom: validation and test sets. 3 3.1 Training Algorithms Background The first neural translation models were optimized with stochastic gradient descent (Sutskever et al., 2014). After training for several epochs with a fixed learning rate, the rate is halved at prespecified intervals. This"
W17-3203,D16-1180,0,0.0148191,"scriptions reports dramatic improvements in BLEU scores for model ensembles (Sutskever et al., 2014; Sennrich et al., 2016a). While this technique is conceptually simple, it requires training and decoding with multiple translation models, often at significant resource costs. However, these costs are either mitigated or justified when building real-world systems or evaluating techniques that should be applicable to those systems. Decoding costs can be reduced by using knowledge distillation techniques to train a single, compact model to replicate the output of an ensemble (Hinton et al., 2015; Kuncoro et al., 2016; Kim and Rush, 2016). Researchers can skip this timeconsuming step, evaluating the ensemble directly, while real-world system engineers can rely on it to make deployment of ensembles practical. To reThe final two categories evaluate handling of true out-of-vocabulary items. For OOVs that should be translated, the full-word system will always score zero, lacking any mechanism for producing words not in its vocabulary or dictionary. The more interesting result is in the relatively low scores for OOVs that should simply be copied from source to target. While phrase-based systems can reliably pas"
W17-3203,P16-1160,0,0.0203069,"from shared translation tasks at the 2016 ACL Conference on Machine Translation (WMT16)4 and the 2016 International Workshop on Spoken Language Translation (IWSLT16)5 . 1 https://github.com/neubig/lamtram https://github.com/clab/dynet 3 Translation dictionaries are learned from the system’s training data using fast align (Dyer et al., 2013). 4 http://statmt.org/wmt16 (Bojar et al., 2016) 5 https://workshop2016.iwslt.org, https: //wit3.fbk.eu (Cettolo et al., 2012) 2 19 of generally being “good enough” have made it a popular choice for researchers and NMT toolkit authors6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017). While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness. In the following section, we explore the relative quality of the optima found by these training algorithms. 3.2 set perplexity every 50K training sentences for the first training run and every 25K sentences for subsequent runs. For IWSLT systems, we evaluate every 25K sentences and then every 6,250 sentences. Training stops when no improvement in perplexity has been seen in 20 eva"
W17-3203,C16-1172,0,0.0492294,"Missing"
W17-3203,P02-1040,0,0.101554,"o 100 words. Model vocabulary is limited to the top 50K source words and 50K target words by frequency, with all others mapped to an unk token. A post-processing step replaces any unk tokens in system output by attempting a dictionary lookup3 of the corresponding source word (highest attention score) and backing off to copying the source word directly (Luong et al., 2015). Experiments in each section evaluate this system against incremental extensions such as improved model vocabulary or training algorithm. Evaluation is conducted by average BLEU score over multiple independent training runs (Papineni et al., 2002; Clark et al., 2011). 2.2 Scenario DE-EN EN-FI RO-EN EN-FR CS-EN Validation (Dev) Set News test 2015 News test 2015 News dev 2016 TED test 2013+2014 TED test 2012+2013 Test Set News test 2016 News test 2016 News test 2016 TED test 2015+2016 TED test 2015+2016 Table 1: Top: parallel training data available for all scenarios. Bottom: validation and test sets. 3 3.1 Training Algorithms Background The first neural translation models were optimized with stochastic gradient descent (Sutskever et al., 2014). After training for several epochs with a fixed learning rate, the rate is halved at prespeci"
W17-3203,E17-3017,0,0.0598143,"Missing"
W17-3203,W16-2323,0,0.232831,"ller steps to explore that part of the space for a good local optimum. While effective, this approach can be time consuming and relies on hand-crafted learning schedules that may not generalize to different models and data sets. To eliminate the need for schedules, subsequent NMT work trained models using the Adadelta algorithm, which automatically and continuously adapts learning rates for individual parameters during training (Zeiler, 2012). Model performance is reported to be equivalent to SGD with annealing, though training still takes a considerable amount of time (Bahdanau et al., 2015; Sennrich et al., 2016b). More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015). While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD (Wu et al., 2016). However, Adam’s speed and reputation Data Sets We evaluate systems on a selection of public data sets covering a range of data sizes, language directions, and morphological complexities. These sets, described in Table 1, are drawn fro"
W17-3203,P16-1162,0,0.558737,"ller steps to explore that part of the space for a good local optimum. While effective, this approach can be time consuming and relies on hand-crafted learning schedules that may not generalize to different models and data sets. To eliminate the need for schedules, subsequent NMT work trained models using the Adadelta algorithm, which automatically and continuously adapts learning rates for individual parameters during training (Zeiler, 2012). Model performance is reported to be equivalent to SGD with annealing, though training still takes a considerable amount of time (Bahdanau et al., 2015; Sennrich et al., 2016b). More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015). While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD (Wu et al., 2016). However, Adam’s speed and reputation Data Sets We evaluate systems on a selection of public data sets covering a range of data sizes, language directions, and morphological complexities. These sets, described in Table 1, are drawn fro"
W17-3203,P15-1002,0,\N,Missing
W17-3203,W16-2301,0,\N,Missing
W17-3208,P11-2093,1,0.641669,"Missing"
W17-3208,P02-1040,0,0.100862,"Missing"
W17-3208,D17-1151,0,0.053157,"conjectured that this is an effect of gathering the similar sentences in a mini-batch as we mentioned in Section 4.2.3. These results indicate that in the case of SGD it is acceptable to TRG SRC , which is the fastest method to process the whole corpus (see Table 3), for SGD. Recently, Wu et al. (2016) proposed a new learning paradigm, which uses Adam for the initial training, then switches to SGD after several iterations. If we use this learning algorithm, we may be able to train the model more effectively by using SHUFFLE or SRC sorting method for Adam, and TRG SRC for SGD. 4.3 5 Recently, Britz et al. (2017) have released a paper about exploring the hyper-parameters of NMT. This work is similar to our paper in the terms of finding the better hyper-parameters by doing a large number of experiments and deriving empirical conclusions. However, notably this paper fixed the mini-batch size to 128 sentences and did not treat mini-batch creation strategy as one of the hyper-parameters of the model. With our experimental results, we argue that the mini-batch creation strategies also have an impact on the NMT training, and thus having solid recommendations for how to adjust this hyper-parameter are also o"
W17-3208,C16-2064,0,0.0371884,"Missing"
W17-3208,P17-4012,0,0.0977649,"Missing"
W17-3208,W16-2301,0,\N,Missing
W17-4709,J93-2003,0,0.0967937,"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 90 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 90–98 c Copenhagen, Denmark, Septem"
W17-4709,J07-2003,0,0.827158,"(de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include E"
W17-4709,P07-1092,0,0.754293,"[X1] 记录 [X1] dossier [X2] (a) Standard triangulation method matching phrases VP VP [X1] enregistrer [X2] TO [X1] 记录 [X2] VB NP record [X2] [X1] NP NP [X1] dossier [X2] DT [X1] [X2] [X1] 记录 NN NP record [X2] (b) Proposed triangulation method matching subtrees Figure 1: Example of disambiguation by parse subtree matching (Fr-En-Zh), [X1] and [X2] are non-terminals for sub-phrases. lel data with the source and target languages exists (de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and targ"
W17-4709,W08-0333,0,0.0164523,"n confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 90 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 90–98 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics trained on direct parallel corpora. (Aho and Ullman, 1969; Chian"
W17-4709,N04-1014,0,0.0774748,"ssible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998). When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007). 2.2 rules, it has the tendency to extract very large translation rule tables and also tends to be less syntactically faithful in its derivations. 2.3 Explicitly Syntactic Rules An alternative to Hiero rules is the use of synchronous context-free grammar or synchronous tree-substitution grammar (Graehl and Knight, 2004) rules that explicitly take into account the syntax of the source side (tree-to-string rules), target side (string-to-tree rules), or both (tree-to-tree rules). Taking the example of tree-to-string (T2S) rules, these use parse trees on the source language side, and the head symbols of the synchronous rules are not limited to S or X, but instead use non-terminal symbols corresponding to the phrase structure tags of a given parse tree. For example, T2S rules could take the form of: Hierarchical Rules In this section, we specifically cover the rules used in Hiero. Hierarchical rules are composed"
W17-4709,P15-2094,1,0.752216,"d Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English. One effective"
W17-4709,W11-2123,0,0.0138784,"trees, we used the Ckylark PCFG-LA parser (Oda et al., 2015), and filtered out lines of length over 60 tokens from all the parallel data to ensure accuracy of parsing and alignment. About 7.6M lines remained. Since Hiero requires a large amount of computational resources for training and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT"
W17-4709,P13-4016,1,0.850627,"ext with the trained model. We used English raw text without tokenization for phrase structure analysis and for training Hiero and T2S TMs on the pivot side. To generate parse trees, we used the Ckylark PCFG-LA parser (Oda et al., 2015), and filtered out lines of length over 60 tokens from all the parallel data to ensure accuracy of parsing and alignment. About 7.6M lines remained. Since Hiero requires a large amount of computational resources for training and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using"
W17-4709,Q17-1024,0,0.0416509,"Missing"
W17-4709,P11-2093,1,0.727838,"s a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT ) (22) ˆ SP p, pˆT ) p∈T p∈T ˆ SP w(ˆ  0 (f lat(pˆS ) = f lat(pˆT )) w(pˆS , pˆT ) = exp (−d (pˆS , pˆT )) (otherwise) d(pˆS , pˆT ) = T reeEditDistance(pˆS , pˆT ) (23) (24) where f lat(ˆ p) returns the symbol string of pˆ keeping non-terminals, and T reeEditDistance(pˆS , pˆT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) n"
W17-4709,N15-3009,1,0.821989,"zer, that is although designed mainly for neural MT, we confirmed that it also helps to reduce training time and even improves translation accuracy in our Hiero model as well. We first trained a single shared tokenization model by feeding a total of 10M sentences from the data of all the 6 languages, set the maximum shared vocabulary size to be 16k, and tokenized all available text with the trained model. We used English raw text without tokenization for phrase structure analysis and for training Hiero and T2S TMs on the pivot side. To generate parse trees, we used the Ckylark PCFG-LA parser (Oda et al., 2015), and filtered out lines of length over 60 tokens from all the parallel data to ensure accuracy of parsing and alignment. About 7.6M lines remained. Since Hiero requires a large amount of computational resources for training and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used t"
W17-4709,P02-1040,0,0.0978157,"T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT ) (22) ˆ SP p, pˆT ) p∈T p∈T ˆ SP w(ˆ  0 (f lat(pˆS ) = f lat(pˆT )) w(pˆS , pˆT ) = exp (−d (pˆS , pˆT )) (otherwise) d(pˆS , pˆT ) = T reeEditDistance(pˆS , pˆT ) (23) (24) where f lat(ˆ p) returns the symbol string of pˆ keeping non-terminals, and T reeEditDistance(pˆS , pˆT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pˆS into pˆT (Klein, 1998). According to equatio"
W17-4709,P07-2045,0,0.008798,"ng and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT ) (22) ˆ SP p, pˆT ) p∈T p∈T ˆ SP w(ˆ  0 (f lat(pˆS ) = f lat(pˆT )) w(pˆS , pˆT ) = exp (−d (pˆS , pˆT )) (otherwise) d(pˆS , pˆT ) = T reeEditDistance(pˆS , pˆT ) (23) (24) where f lat(ˆ p) returns the symbol string of pˆ keeping non-terminals, and T reeEditDistance(pˆS , pˆT )"
W17-4709,N07-1061,0,0.283363,"o 2.3 BLEU points.1 1 [X2] [X1] 记录 [X1] dossier [X2] (a) Standard triangulation method matching phrases VP VP [X1] enregistrer [X2] TO [X1] 记录 [X2] VB NP record [X2] [X1] NP NP [X1] dossier [X2] DT [X1] [X2] [X1] 记录 NN NP record [X2] (b) Proposed triangulation method matching subtrees Figure 1: Example of disambiguation by parse subtree matching (Fr-En-Zh), [X1] and [X2] are non-terminals for sub-phrases. lel data with the source and target languages exists (de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences"
W17-4709,N03-1017,0,0.272216,"[X2] are non-terminals for sub-phrases. lel data with the source and target languages exists (de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are"
W17-4709,L16-1561,0,0.20481,"used in tree-based machine translation frameworks (§2). After describing the baseline triangulation method (§3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching (§4). The first places a hard restriction on exact matching of parse trees (§4.1) included in translation rules, while the second places a softer restriction allowing partial matches (§4.2). To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language (§5). In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario. In the triangulation method, source-pivot and pivot-target phrase pairs are connected as a sourcetarget phrase pair when a common pivot-side phrase e"
W17-4753,W16-2358,0,0.056134,"Missing"
W17-4753,W16-2359,0,0.0171527,"Zhang1,2 , Masao Utiyama1 , Eiichro Sumita1 Graham Neubig2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp neubig/s-nakamura@is.naist.jp Abstract We also explored ways to improve the NMT model with image information. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use"
W17-4753,E17-2101,0,0.0123746,"a1 Graham Neubig2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp neubig/s-nakamura@is.naist.jp Abstract We also explored ways to improve the NMT model with image information. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use these target descriptions as additional in"
W17-4753,P05-1033,0,0.0896966,"ing alone. We also present a multimodal NMT model that integrates the target language descriptions of images that are similar to the image described by the source sentence as additional inputs of the neural networks to help the translation of the source sentence. We give detailed analysis for the results of the multimodal NMT model. Our system obtained the first place for the English-to-French task according to human evaluation. 1 2 Text-only MT We compared three text-only approaches for this translation task. 2.1 Introduction Hierarchical Phrase-based SMT The hierarchical phrase-based model (Chiang, 2005) extracts hierarchical phrase-based translation rules from parallel sentence pairs with word alignments. The word alignments can be learned by IBM models. Each translation rule contains several feature scores. The decoder of hierarchical phrase-based model implements a bottom-up CKY+ algorithm. The weights for different features can be tuned on the development set. We participated in the WMT 2017 shared multimodal machine translation task 1, which translates a source language description of an image into a target language description. We built systems for both English-to-German and English-toF"
W17-4753,W16-3210,0,0.06354,"Missing"
W17-4753,P16-1227,0,0.0232453,"ion. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use these target descriptions as additional inputs for the NMT model. This paper describes the NICT-NAIST system for the WMT 2017 shared multimodal machine translation task for both language pairs, English-to-German and English-to-French. We built a hierarchical phrase-based (Hiero) translation system and trained an attentional encoder-decoder neural machine translation (NMT) model to rerank the n-best output of the Hiero system, which obtained significant gains over both the Hiero"
W17-4753,W16-2360,0,0.0386153,"ama1 , Eiichro Sumita1 Graham Neubig2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp neubig/s-nakamura@is.naist.jp Abstract We also explored ways to improve the NMT model with image information. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use these target descri"
W17-4753,P07-2045,0,0.0214797,"between image vectors. Method Hiero NMT Reranking Flickr en-de 27.86 30.52 31.98 en-fr 50.38 50.46 55.25 COCO en-de 24.57 24.27 28.05 en-fr 41.88 41.26 45.17 λ=0 λ = 0.2 en-de 52.17 52.93 en-fr 65.60 66.31 Table 3: 1-gram BLEU score of selected target words on the development set. Table 2: Results of text-only approaches (BLEU). coding, we find the most probable source word for each UNK and replace the UNK using a lexicon extracted from the word-aligned training set. et al., 2017). We lowercase, normalise punctuation and tokenise all sentences. The Hiero translation system was based on Moses (Koehn et al., 2007). We used GIZA++ (Och and Ney, 2003) and growdiag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments. For decoding, we used standard features: direct/inverse phrase translation probability, direct/inverse lexical translation probability and a 5-gram language model, which was trained on the target side of the training corpus by IRSTLM Toolkit3 with improved Kneser-Ney smoothing. Attentional encoder-decoder networks were trained with Lamtram4 . Word embedding size and hidden layer size are both 512. Training data was reshuffled between epochs. Validation was done after"
W17-4753,N03-1017,0,0.0127221,"38 50.46 55.25 COCO en-de 24.57 24.27 28.05 en-fr 41.88 41.26 45.17 λ=0 λ = 0.2 en-de 52.17 52.93 en-fr 65.60 66.31 Table 3: 1-gram BLEU score of selected target words on the development set. Table 2: Results of text-only approaches (BLEU). coding, we find the most probable source word for each UNK and replace the UNK using a lexicon extracted from the word-aligned training set. et al., 2017). We lowercase, normalise punctuation and tokenise all sentences. The Hiero translation system was based on Moses (Koehn et al., 2007). We used GIZA++ (Och and Ney, 2003) and growdiag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments. For decoding, we used standard features: direct/inverse phrase translation probability, direct/inverse lexical translation probability and a 5-gram language model, which was trained on the target side of the training corpus by IRSTLM Toolkit3 with improved Kneser-Ney smoothing. Attentional encoder-decoder networks were trained with Lamtram4 . Word embedding size and hidden layer size are both 512. Training data was reshuffled between epochs. Validation was done after each epoch. We used the Adam optimization algorithm (Kingma and Ba, 2014). Because the tra"
W17-4753,P03-1021,0,0.0340914,"Validation was done after each epoch. We used the Adam optimization algorithm (Kingma and Ba, 2014). Because the training set is only 29K sentence pairs, we used dropout (0.5) and a small learning rate (0.0001) to reduce overfitting, which yielded improvements of 3 − 4 BLEU on the development set. For training the NMT model, we replace words that occur less than twice in the training set as UNK. When de3 4 We used the NMT model to rerank the unique 10, 000-best output of the Hiero system. The NMT score was used as an additional feature for the Hiero system. Feature weights were tuned by MERT (Och, 2003). Table 2 shows results of the Hiero system, the NMT system and using the NMT model to rerank the Hiero outputs. The reranking system had the best performance on both language pairs. It is straightforward that using the NMT feature to rerank the Hiero outputs can achieve improvements over the pure Hiero system. The reason why the reranking method outperformed the NMT system should be that the training corpus is relatively small and the NMT system did not outperform the Hiero system largely. Therefore, the reranking method that takes advantages of both the Hiero and NMT systems worked the best"
W17-4753,J03-1002,0,0.0124739,"MT Reranking Flickr en-de 27.86 30.52 31.98 en-fr 50.38 50.46 55.25 COCO en-de 24.57 24.27 28.05 en-fr 41.88 41.26 45.17 λ=0 λ = 0.2 en-de 52.17 52.93 en-fr 65.60 66.31 Table 3: 1-gram BLEU score of selected target words on the development set. Table 2: Results of text-only approaches (BLEU). coding, we find the most probable source word for each UNK and replace the UNK using a lexicon extracted from the word-aligned training set. et al., 2017). We lowercase, normalise punctuation and tokenise all sentences. The Hiero translation system was based on Moses (Koehn et al., 2007). We used GIZA++ (Och and Ney, 2003) and growdiag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments. For decoding, we used standard features: direct/inverse phrase translation probability, direct/inverse lexical translation probability and a 5-gram language model, which was trained on the target side of the training corpus by IRSTLM Toolkit3 with improved Kneser-Ney smoothing. Attentional encoder-decoder networks were trained with Lamtram4 . Word embedding size and hidden layer size are both 512. Training data was reshuffled between epochs. Validation was done after each epoch. We used the Adam optimiz"
W17-5545,D13-1160,0,0.0657811,"a seed lexicon and a generative grammar, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named S MART H OME, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets. 1 Figure 1: Crowdsourcing pipeline for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog data"
W17-5545,P09-1010,0,0.0240886,"generative grammar, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named S MART H OME, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets. 1 Figure 1: Crowdsourcing pipeline for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog datasets for specific domai"
W17-5545,W10-2903,0,0.0864785,"Missing"
W17-5545,P16-1004,0,0.023426,"1: Crowdsourcing pipeline for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog datasets for specific domains becomes especially important, and is often the bottleneck for applying semantic parsers to new tasks. Introduction Semantic parsing is the task of mapping natural language utterances to their underlying meaning representations. This is an essential component for many tasks that require understanding natural language dialogue (Woods, 1977; Zelle and Wang et al. (2015) propose a methodology for efficient creation of semantic parsing data that starts with the set of target logical forms,"
W17-5545,P16-1002,0,0.0946799,"ne for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog datasets for specific domains becomes especially important, and is often the bottleneck for applying semantic parsers to new tasks. Introduction Semantic parsing is the task of mapping natural language utterances to their underlying meaning representations. This is an essential component for many tasks that require understanding natural language dialogue (Woods, 1977; Zelle and Wang et al. (2015) propose a methodology for efficient creation of semantic parsing data that starts with the set of target logical forms, and ∗ *The indicated a"
W17-5545,Q13-1016,0,0.0830323,"entations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named S MART H OME, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets. 1 Figure 1: Crowdsourcing pipeline for building semantic parsers for new domains Mooney, 1996; Berant et al., 2013; Branavan et al., 2009; Azaria et al., 2016; Gulwani and Marron, 2014; Krishnamurthy and Kollar, 2013). Orienting a dialogue-capable intelligent system is accomplished by training its semantic parser with utterances that capture the nuances of the domain. An inherent challenge lies in building datasets that have enough lexical diversity for granting the system robustness against natural language variation in query-based dialogue. With the advent of datadriven methods for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016), constructing such realistic and sufficient-sized dialog datasets for specific domains becomes especially important, and is often the bottleneck for applying semant"
W17-5545,P15-1129,0,0.385032,"Abhilasha Ravichander1∗, Thomas Manzini1∗, Matthias Grabmair1 Graham Neubig1 , Jonathan Francis12 , Eric Nyberg1 1 Language Technologies Institute, Carnegie Mellon University 2 Robert Bosch LLC, Corporate Sector Research and Advanced Engineering {aravicha, tmanzini, mgrabmai, gneubig, ehn}@cs.cmu.edu jon.francis@us.bosch.com Abstract Building dialogue interfaces for realworld scenarios often entails training semantic parsers starting from zero examples. How can we build datasets that better capture the variety of ways users might phrase their queries, and what queries are actually realistic? Wang et al. (2015) proposed a method to build semantic parsing datasets by generating canonical utterances using a grammar and having crowdworkers paraphrase them into natural wording. A limitation of this approach is that it induces bias towards using similar language as the canonical utterances. In this work, we present a methodology that elicits meaningful and lexically diverse queries from users for semantic parsing tasks. Starting from a seed lexicon and a generative grammar, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the q"
W17-5545,N13-1103,0,0.177964,"Missing"
W17-5545,P11-1060,0,0.187332,"Missing"
W17-5545,W16-6644,0,0.0547291,"gested cannot be used to generate all the queries we may want to support in a new domain, and (3) there is no check on the correctness or naturalness of the canonical utterances themselves, which may not be logically plausible. This is problematic as even unlikely canonical utterances can be paraphrased fluently. 2 In this paper, we propose and evaluate a new approach for creating lexically diverse and plausible utterances for semantic parsing (Figure 1.). Firstly, inspired by the use of images in the creation of datasets for paraphrasing (Lin et al., 2014) or for natural language generation (Novikova et al., 2016), we seek to reduce this linguistic bias by using a lexicon consisting of images. Secondly, a generative grammar, which is tailored to the domain, combines these images to form mixed text-image representations. Using these two approaches, we retain many of the advantages of existing approaches such as ease of supervision and completeness of the dataset, with the added bonus of promoting lexical diversity in the natural language utterances, and supporting queries relevant to our domain. Finally, we add a simple step within the crowdsourcing experiment where crowd-workers evaluate the plausibili"
W17-5545,E17-1052,0,0.0456458,"Missing"
W17-5545,P15-1085,0,0.0354576,"Missing"
W17-5701,W17-5715,0,0.0507339,"Missing"
W17-5701,W04-3250,0,0.243434,"Missing"
W17-5701,W17-5714,1,0.815327,"Missing"
W17-5701,P07-2045,0,0.0182137,"f a hierarchical phrase-based SMT system, a string-to-tree syntax-based SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. We also experimentally produced results for the baseline systems that consisted of an neural machine translation system using the implementation of (Vaswani et al., 2017). The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page8 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 6. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 8 h"
W17-5701,W17-5710,0,0.0950012,"Missing"
W17-5701,P13-2121,0,0.0263037,"Missing"
W17-5701,2009.iwslt-papers.4,0,0.017237,"ase-based SMT system, a string-to-tree syntax-based SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. We also experimentally produced results for the baseline systems that consisted of an neural machine translation system using the implementation of (Vaswani et al., 2017). The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page8 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 6. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 8 http://lotus.kuee.kyot"
W17-5701,W17-5709,0,0.0346313,"Missing"
W17-5701,W17-5711,0,0.0304192,"Missing"
W17-5701,W17-5716,0,0.0397813,"Missing"
W17-5701,D10-1092,0,0.0485922,"GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. Phrase-based SMT We used the following Moses configuration for the phrase-based SMT system. The default values were used for the other system parameters. • distortion-limit – 20 for JE, EJ, JC, and CJ – 0 for JK, KJ, HE, and EH – 6 for IE and EI • msd-bidirectional-fe lexicalized reordering 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl which was distributed 9 http://nlp.ist.i.kyotou.ac.jp/EN/index.php?JUMAN 10 http://nlp.stanford.edu/software/segmenter.shtml 11 https://bitbucket.org/eunjeon/mecab-ko/ 12 https://bitbucket.org/anoopk/indic_nlp_library 6 with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4 13 . AMFM scores were calculated using scripts created by the technical collaborators of WAT2017. All scores for each task were calculated using the corresponding reference. Before the calculat"
W17-5701,W17-5706,0,0.0506466,"Missing"
W17-5701,W17-5713,0,0.03618,"Missing"
W17-5701,W14-7001,1,0.898058,"ic evaluation server, and selected submissions were manually evaluated. 1 Sadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. There is no deadline of translation result submission with respect to automatic evaluation of translation quality and WAT receives submissions at any time. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014), WAT2015 (Nakazawa et al., 2015) and WAT2016 (Nakazawa et al., 2016), WAT2017 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 4th WAT, we adopted new translation subtasks with English-Japanese news corpus and English-Japanese recipe corpus in addition to the subtasks at WAT2016 1 . Fur• Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Ko"
W17-5701,2007.mtsummit-papers.63,0,0.586487,"nstitute of Information and Communications Technology (NICT). The corpus consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for J↔E subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for J↔C subtasks. The statistics for each corpus are shown in Table 1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million Japanese-English scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence 2.2 JPC JPC was constructed by the Japan Patent Office (JPO). The corpus consists of ChineseJapanese patent description corpus (JPC-CJ), Korean-Japanese patent description corpus (JPC-KJ) and English-Japanese patent description corpus (JPC-EJ) with the sections of Chemistry, Electricity, Mechanical engineering, and Physics on the basis of International Patent Classification (IPC). Each corpus is partitioned into training, development, development-test and test data. This corpus is used for patent subtasks C↔J, K↔J and E↔J. The statistics for each corpus are shown 2 http://lotus"
W17-5701,W15-5001,1,0.885412,"d submissions were manually evaluated. 1 Sadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. There is no deadline of translation result submission with respect to automatic evaluation of translation quality and WAT receives submissions at any time. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014), WAT2015 (Nakazawa et al., 2015) and WAT2016 (Nakazawa et al., 2016), WAT2017 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 4th WAT, we adopted new translation subtasks with English-Japanese news corpus and English-Japanese recipe corpus in addition to the subtasks at WAT2016 1 . Fur• Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Korean↔Japanese language pairs. In"
W17-5701,W17-5707,0,0.110884,"Missing"
W17-5701,W17-5708,0,0.0494718,"Missing"
W17-5701,P11-2093,1,0.766986,"ecab-ko/ 12 https://bitbucket.org/anoopk/indic_nlp_library 6 with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4 13 . AMFM scores were calculated using scripts created by the technical collaborators of WAT2017. All scores for each task were calculated using the corresponding reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 14 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0 15 . For Chinese segmentation, we used two different tools: KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 16 . For Korean segmentation we used mecab-ko 17 . For English segmentation, we used tokenizer.perl 18 in the Moses toolkit. For Hindi segmentation, we used Indic NLP Library 19 . The detailed procedures for the automatic evaluation are shown on the WAT2017 evaluation web page 20 . 4.2 N"
W17-5701,W17-5712,1,0.882844,"Missing"
W17-5701,P02-1040,0,0.105207,"n = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. Phrase-based SMT We used the following Moses configuration for the phrase-based SMT system. The default values were used for the other system parameters. • distortion-limit – 20 for JE, EJ, JC, and CJ – 0 for JK, KJ, HE, and EH – 6 for IE and EI • msd-bidirectional-fe lexicalized reordering 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl which was distributed 9 http://nlp.ist.i.kyotou.ac.jp/EN/index.php?JUMAN 10 http://nlp.stanford.edu/software/segmenter.shtml 11 https://bitbucket.org/eunjeon/mecab-ko/ 12 https://bitbucket.org/anoopk/indic_nlp_library 6 with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4 13 . AMFM scores were calculated using scripts created by the technical collaborators of WAT2017. All scores for each task were calculated using the corresponding"
W17-5701,P06-1055,0,0.0160054,"d SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. We also experimentally produced results for the baseline systems that consisted of an neural machine translation system using the implementation of (Vaswani et al., 2017). The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page8 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 6. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 8 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 4 5 ✓ ✓ ✓ ✓ ✓ ✓ ✓ JPC IITB JIJI RECIPE EJ JC CJ JK KJHE EH JE EJ JE EJ ✓ ✓ ✓ ✓"
W18-1818,P16-5005,0,0.0182733,"uction Due to the effectiveness and relative ease of implementation, there is now a proliferation of toolkits for neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), as many as 51 according to the tally by nmt-list.1 The common requirements for such toolkits are speed, memory efﬁciency, and translation accuracy, which are essential for the use of such systems in practical translation settings. Many open source toolkits do an excellent job at this to the point where they can be used in production systems (e.g. OpenNMT2 is used by Systran (Crego et al., 2016)). This paper describes XNMT, the eXtensible Neural Machine Translation toolkit, a toolkit that optimizes not for efﬁciency, but instead for ease of use in practical research settings. In other words, instead of only optimizing time for training or inference, XNMT aims to reduce the time it takes for a researcher to turn their idea into a practical experimental setting, test with a large number of parameters, and produce valid and trustable research results. Of course, this necessitates a certain level of training efﬁciency and accuracy, but XNMT also takes into account a number of considerati"
W18-1818,D13-1176,0,0.0308801,"tself from other open-source NMT toolkits by its focus on modular code design, with the purpose of enabling fast iteration in research and replicable, reliable results. In this paper we describe the design of XNMT and its experiment conﬁguration system, and demonstrate its utility on the tasks of machine translation, speech recognition, and multi-tasked machine translation/parsing. XNMT is available open-source at https://github.com/neulab/xnmt. 1 Introduction Due to the effectiveness and relative ease of implementation, there is now a proliferation of toolkits for neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), as many as 51 according to the tally by nmt-list.1 The common requirements for such toolkits are speed, memory efﬁciency, and translation accuracy, which are essential for the use of such systems in practical translation settings. Many open source toolkits do an excellent job at this to the point where they can be used in production systems (e.g. OpenNMT2 is used by Systran (Crego et al., 2016)). This paper describes XNMT, the eXtensible Neural Machine Translation toolkit, a toolkit that optimizes not for efﬁciency, but instead for ease of use"
W18-1818,D15-1166,0,0.269704,"Missing"
W18-1818,H92-1073,0,0.558988,"nput to the model are Mel-ﬁlterbank features with 40 coefﬁcients. For regularization, we apply variational dropout of rate 0.3 in all LSTMs, and word dropout of rate 0.1 on the target side (Gal and Ghahramani, 2016). For training, we use Adam (Kingma and Ba, 2014) with initial learning rate of 0.0003, which is decayed by factor 0.5 if no improved in WER is observed. To further facilitate training, label smoothing (Szegedy et al., 2016) is applied. For the search, we use beam size 20 and length normalization with the exponent set to 1.5. We test this model on both the Wall Street Journal (WSJ; Paul and Baker (1992)) corpus which contains read speech, and the TEDLIUM corpus (Rousseau et al., 2014) which contains recorded TED talks. Numbers are shown in Table 4.2. Comparison to results from the literature shows that our results are competitive. 4.3 Multi-task MT + Parsing We performed multi-task training of a sequence-to-sequence model for parsing and machine translation. The main task is the parsing task, and we followed the general setup in (Vinyals et al., 2015), but we only used the standard WSJ training data. It is jointly trained with an English-German translation system. Compared to a single sequen"
W18-1818,rousseau-etal-2014-enhancing,0,0.0453766,"Missing"
W18-1818,P16-1162,0,0.0942711,"ase studies of using XNMT to perform various experiments: a standard machine translation experiment (§4.1), a speech recognition experiment (§4.2), and a multi-task learning experiment where we train a parser along with an MT model (§4.3). 4.1 Machine Translation We trained a machine translation model on the WMT English-German benchmark, using the preprocessed data by Stanford.5 Our model was a basic 1-layer model with bidirectional LSTM encoder and 256 units per direction, LSTM decoder output projections and MLP attention mechanism all with 512 hidden units. We applied joint BPE of size 32k (Sennrich et al., 2016). We also applied input feeding, as well as variational dropout of rate 0.3 to encoder and decoder LSTMs. Decoding was performed with a beam of size 1. Overall, results were similar, with our model achieving a BLEU of 18.26 and Luong et al. (2015) achieving a BLEU of 18.1. Note that the model by Luong et al. (2015) is simpler because it does not use BPE and only a unidirectional encoder. 5 https://nlp.stanford.edu/projects/nmt/ Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 189 Model XNMT Zhang et al. (2017) Rousseau et al. (2014) WSJ dev93 16.65 — — WSJ"
W18-1818,P16-1159,0,0.0220965,"earning framework. DyNet uses dynamic computation graphs, which makes it possible to write code in a very natural way, and beneﬁt from additional ﬂexibility to implement complex networks with dynamic structure, as are often beneﬁcial in natural language processing. Further beneﬁts include transparent handling of batching operations, or even removing explicit batch handling and relying on autobatching for speed-up instead. • XNMT of course contains standard NMT models, but also includes functionality for optimization using reinforcement learning (Ranzato et al., 2015) or minimum risk training (Shen et al., 2016), ﬂexible multi-task learning (Dai and Le, 2015), encoders for speech (Chan et al., 2016), and training and testing of retrieval-based models (Huang et al., 2013). In the remainder of the paper, we provide some concrete examples of the design principles behind XNMT, and a few examples of how it can be used to implement standard models. 2 Model Structure and Speciﬁcation 2.1 NMT Design Dimensions: Model, Training, and Inference When training an NMT system there are a number of high-level design decisions that we need to make: what kind of model do we use? how do we test this model? at test time"
W18-2701,W18-2713,0,0.0259017,"LSTMbased encoder-decoders with attention (Bahdanau et al., 2015). 3.4 Submitted Systems Four teams, Team Amun, Team Marian, Team OpenNMT, and Team NICT submitted to the shared task, and we will summarize each below. Before stepping in to the details of each system, we first note general trends that all or many systems attempted. The first general trend was a fast C++ decoder, with Teams Amun, Marian, and NICT using the Amun or Marian decoders included in the Marian toolkit,4 and team OpenNMT 4 5 https://marian-nmt.github.io 3 http://opennmt.net 3.4.1 3.4.4 Team Amun Team NICT’s contribution (Imamura and Sumita, 2018) to the shared task was centered around using self-training as a way to improve NMT accuracy without changing the architecture. Specifically, they used a method of randomly sampling pseudo-source sentences from a back-translation model (Imamura et al., 2018) and used this to augment the data set to increase coverage. They tested two basic architectures for the actual translation model, a recurrent neural network-based model trained using OpenNMT, and a self-attentional model trained using Marian, finally submitting the self-attentional model using Marian as their sole contribution to the share"
W18-2701,W18-2716,0,0.0254636,"xamination for future shared tasks. Next, considering memory usage, we can see again that the submissions from the Marian team tend to be the most efficient. One exception is the extremely small memory system OpenNMT-Tiny, which achieves significantly lower translation accuracies, but fits in a mere 220MB of memory on the CPU. In this first iteration of the task, we attempted to establish best practices and strong baselines upon which to build efficient test-time methods for NMT. One characteristic of the first iteration of the task was that the basic model architectures Team Marian’s system (Junczys-Dowmunt et al., 2018) used the Marian C++ decoder, and concentrated on new optimizations for the CPU. The team distilled a large self-attentional model into two types of “student” models: a smaller self-attentional model using average attention networks (Zhang et al., 2018), a new higherspeed version of the original Transformer model (Vaswani et al., 2017), and a standard RNN-based decoder. They also introduced an auto-tuning approach that chooses which of multiple matrix multiplication implementations is most efficient in the current context, then uses this implementation going forward. This resulted in the Maria"
W18-2701,W18-2708,0,0.0238604,"ization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018) Accuracy Measures: As a measure of translation accuracy, we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores. Data augmentation: A number of the contributed papers examined ways to augment data for more efficient training. These include methods for considering multiple back translations (Imamura et al., 2018), iterative back translation (Hoang et al., 2018b), bidirectional multilingual training (Niu et al., 2018), and document level adaptation (Kothur et al., 2018) Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the test set on CPU or GPU. Time for loading models was measured by having the model translate an empty file, then subtracting this from the total time to translate the test set file. Inadequate resources: Several contributions involved settings in which resources were insufficient, such as investigating the impact of noise (Khayrallah and Koehn, 2018), missing data in multi-source settings (Nishimura et al., 2018) and one-shot learning (Pham et al., 2018). Memory Efficiency Measures: We me"
W18-2701,N18-2078,0,0.0233178,"r Computational Linguistics Translation (Nakazawa et al., 2017)) was focused on creating systems for NMT that are not only accurate, but also efficient. Efficiency can include a number of concepts, including memory efficiency and computational efficiency. This task concerns itself with both, and we cover the detail of the evaluation below. model research, with the contributions being concentrated on the following topics: Linguistic structure: How can we incorporate linguistic structure in neural MT or generation models? Contributions examined the effect of considering semantic role structure (Marcheggiani et al., 2018), latent structure (Bastings et al., 2018), and structured self-attention (Bisk and Tran, 2018). 3.1 The first step to the evaluation was deciding what we want to measure. In the case of the shared task, we used metrics to measure several different aspects connected to how good the system is. These were measured for systems that were run on CPU, and also systems that were run on GPU. Domain adaptation: Some contributions examined regularization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018) Accuracy Measures: As a meas"
W18-2701,D15-1199,0,0.0577417,"Missing"
W18-2701,P18-2050,1,0.824817,"g semantic role structure (Marcheggiani et al., 2018), latent structure (Bastings et al., 2018), and structured self-attention (Bisk and Tran, 2018). 3.1 The first step to the evaluation was deciding what we want to measure. In the case of the shared task, we used metrics to measure several different aspects connected to how good the system is. These were measured for systems that were run on CPU, and also systems that were run on GPU. Domain adaptation: Some contributions examined regularization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018) Accuracy Measures: As a measure of translation accuracy, we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores. Data augmentation: A number of the contributed papers examined ways to augment data for more efficient training. These include methods for considering multiple back translations (Imamura et al., 2018), iterative back translation (Hoang et al., 2018b), bidirectional multilingual training (Niu et al., 2018), and document level adaptation (Kothur et al., 2018) Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the t"
W18-2701,P18-1166,0,0.0179704,"lation accuracies, but fits in a mere 220MB of memory on the CPU. In this first iteration of the task, we attempted to establish best practices and strong baselines upon which to build efficient test-time methods for NMT. One characteristic of the first iteration of the task was that the basic model architectures Team Marian’s system (Junczys-Dowmunt et al., 2018) used the Marian C++ decoder, and concentrated on new optimizations for the CPU. The team distilled a large self-attentional model into two types of “student” models: a smaller self-attentional model using average attention networks (Zhang et al., 2018), a new higherspeed version of the original Transformer model (Vaswani et al., 2017), and a standard RNN-based decoder. They also introduced an auto-tuning approach that chooses which of multiple matrix multiplication implementations is most efficient in the current context, then uses this implementation going forward. This resulted in the MarianTinyRNN system using an RNN-based model, and the Marian-Trans-Small-AAN, MarianTrans-Base-AAN, Marian-Trans-Big, MarianTrans-Big-int8 systems, which use different varieties and sizes of self-attentional models. 3.4.3 Team NICT Team OpenNMT Team OpenNMT"
W18-2701,W18-2712,0,0.0187161,", and document level adaptation (Kothur et al., 2018) Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the test set on CPU or GPU. Time for loading models was measured by having the model translate an empty file, then subtracting this from the total time to translate the test set file. Inadequate resources: Several contributions involved settings in which resources were insufficient, such as investigating the impact of noise (Khayrallah and Koehn, 2018), missing data in multi-source settings (Nishimura et al., 2018) and one-shot learning (Pham et al., 2018). Memory Efficiency Measures: We measured: (1) the size on disk of the model, (2) the number of parameters in the model, and (3) the peak consumption of the host memory and GPU memory. These metrics were measured by having participants submit a container for the virtualization environment Docker1 , then measuring from outside the container the usage of computation time and memory. All evaluations were performed on dedicated instances on Amazon Web Services2 , specifically of type m5.large for CPU evaluation, and p3.2xlarge (with a NVIDIA Tesla V100 GPU). Model analysis: There were also many me"
W18-2701,D15-1044,0,0.0437311,"cipants were tasked with creating NMT systems that are both accurate and efficient. 1 Introduction Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 2nd Workshop on Neural Machine Translation and Generation (WNMT 2018) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (Rush et al., 2015), NLG from structured data (Wen et al., 2015), dialog response generation (Vinyals and Le, 2015), among others). Overall, the workshop was held with two goals: First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: This year we will continue to encourage submissions that not only advance the state of 2 Summary of Research Contributions We published a call for long papers, extended abstracts for preliminary work, and crosssubmissions of papers submitted to other venues. The goal was to encourage discussion and interaction with researchers from"
W18-2701,W18-2715,0,0.0477416,"Missing"
W18-2701,P17-2091,0,0.117559,"most efficient in the current context, then uses this implementation going forward. This resulted in the MarianTinyRNN system using an RNN-based model, and the Marian-Trans-Small-AAN, MarianTrans-Base-AAN, Marian-Trans-Big, MarianTrans-Big-int8 systems, which use different varieties and sizes of self-attentional models. 3.4.3 Team NICT Team OpenNMT Team OpenNMT (Senellart et al., 2018) built a system based on the OpenNMT toolkit. The model was based on a large self-attentional teacher model distilled into a smaller, fast RNN-based model. The system also used a version of vocabulary selection (Shi and Knight, 2017), and a method to increase the size of the encoder but decrease the size of the decoder to improve the efficiency of beam search. They submitted two systems, OpenNMT-Small and OpenNMT-Tiny, which were two variously-sized implementations of this model. 4 used relatively standard, with the valuable contributions lying in solid engineering work and best practices in neural network optimization such as low-precision calculation and model distillation. With these contributions, we now believe we have very strong baselines upon which future iterations of the task can build, examining novel architect"
W18-2701,W18-2702,0,0.0225666,"ntainer for the virtualization environment Docker1 , then measuring from outside the container the usage of computation time and memory. All evaluations were performed on dedicated instances on Amazon Web Services2 , specifically of type m5.large for CPU evaluation, and p3.2xlarge (with a NVIDIA Tesla V100 GPU). Model analysis: There were also many methods that analyzed modeling and design decisions, including investigations of individual neuron contributions (Bau et al., 2018), parameter sharing (Jean et al., 2018), controlling output characteristics (Fan et al., 2018), and shared attention (Unanue et al., 2018) 3 Evaluation Measures Shared Task 3.2 Many shared tasks, such as the ones run by the Conference on Machine Translation (Bojar et al., 2017), aim to improve the state of the art for MT with respect to accuracy: finding the most accurate MT system regardless of computational cost. However, in production settings, the efficiency of the implementation is also extremely important. The shared task for WNMT (inspired by the “small NMT” task at the Workshop on Asian Data The data used was from the WMT 2014 EnglishGerman task (Bojar et al., 2014), using the preprocessed corpus provided by the Stanford"
W18-2701,D13-1176,0,\N,Missing
W18-2701,P02-1040,0,\N,Missing
W18-2701,W14-3302,0,\N,Missing
W18-2701,W17-4717,0,\N,Missing
W18-2701,W18-2704,0,\N,Missing
W18-2701,W18-2711,1,\N,Missing
W18-2701,W18-2707,0,\N,Missing
W18-2701,W18-2706,0,\N,Missing
W18-2701,W18-2709,0,\N,Missing
W18-2701,W18-2705,0,\N,Missing
W18-2711,D15-1166,0,0.434437,"g multi-source NMT implementations without no special modifications. Experimental results with real incomplete multilingual corpora of TED Talks show that it is effective in allowing for multi-source NMT in situations where full multilingual corpora are not available, resulting in BLEU score gains of up to 2 points compared to standard bi-lingual NMT. (3) The method we base our work upon is largely similar to Zoph and Knight (2016), with the exception of a few details. Most notably, they used local-p attention, which focuses only on a small subset of the source positions for each target word (Luong et al., 2015). In this work, we used global attention, which attends to all words on the source side for each target word, as this is the standard method used in the great majority of recent NMT work. 93 Gating Network Es Encoder Decoder Fr Encoder Decoder Ar Encoder Decoder En Es Eso es verdad Fr C&apos;est vrai Ar &lt;NULL&gt; That is true En Figure 4: Multi-encoder NMT with a missing input sentence pre-train Figure 3: Mixture of NMT Experts 2.2 Specifically, we attempt to extend the methods in the previous section to use an incomplete multilingual corpus in this work. Mixture of NMT Experts Garmash and Monz (2016)"
W18-2711,2012.eamt-1.60,0,0.0155388,"vous remercie (a) A standard bilingual corpus English Hello Thank you French Bonjour Je vous remercie Spanish Hola Gracias (b) A complete multi-source corpus English Hello Thank you French Spanish Hola × × Bonjour Je vous remercie (c) An incomplete multi-source corpus with missing data Figure 1: Example of type of corpora. languages involved in the translation process. For example, we may have an original document in English, that we want to translate into several languages such as French, Spanish, and Portuguese. Some examples of these scenarios are the creation of video captions for talks (Cettolo et al., 2012) or Movies (Tiedemann, 2009), or translation of official documents into all the languages of a governing body, such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet hav"
W18-2711,2001.mtsummit-papers.46,0,0.675579,", such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-source translation takes in multiple inputs, and references all of them when deciding which sentence to output. Specifically, in the context of neural machine translation (NMT), there are several methods proposed to do so. For example, Zoph and Knight (2016) propose a method where multiple sentences are each encoded separately, then all referenced during the decoding process (the “multi-encoder” method). In addition, Garmash and Monz (2016) propose a method where NMT systems over multiple inputs are ensembled together to make a final pred"
W18-2711,P15-1166,0,0.0351747,"c a estudiar el modelo empresarial. Luego empec a mirar el modelo empresarial. Luego empec a ver el modelo de negocios. 0.266 0.266 0.726 Sometimes they agree. &lt;NULL&gt; &lt;NULL&gt; A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. 1.000 1.000 1.000 Table 6: Translation examples in {English, French, Brazilian Portuguese}-to-Spanish translation. 5 6 Related Work In this paper, we examined strategies for multisource NMT. On the other hand, there are there are other strategies for multilingual NMT that do not use multiple source sentences as their input. Dong et al. (2015) proposed a method for multitarget NMT. Their method is using one sharing encoder and decoders corresponding to the number of target languages. Firat et al. (2016) proposed a method for multi-source multi-target NMT using multiple encoders and decoders with a shared attention mechanism. Johonson et al. (2017) and Ha et al. (2016) proposed multi-source and multitarget NMT using one encoder and one decoder, and sharing all parameters with all languages. Notably, these methods use multilingual data to better train one-to-one NMT systems. However, our motivation of this study is to improve NMT fur"
W18-2711,P02-1040,0,0.100903,"of NMT experts, and one-to-one NMT. We used global attention and attention feeding (Luong et al., 2015) for the NMT models and used a bidirectional encoder (Bahdanau et al., 2015) in their encoders. The number of units was 512 for the hidden and embedding layers. Vocabulary size was the most frequent 30,000 words in the training data for each source and target languages. The parameter optimization algorithm was Adam (Kingma and Ba, 2015) and gradient clipping was set to 5. The number of hidden state units in the gating network for the mixture of NMT experts experiments was 256. We used BLEU (Papineni et al., 2002) as the evaluation metric. We performed early stopping, saving parameter values that had the smallest log perplexities on the validation data and used them when decoding test data. 4.2 Es x Data We used UN6WAY (Ziemski et al., 2016) as the complete multilingual corpus. We chose Spanish (Es), French (Fr), and Arabic (Ar) as source languages and English (En) as a target language The training data in the experiments were the one million sentences from the UN6WAY corpus whose sentence lengths were less than or equal to 40 words. We excluded 200,000 sentences for each language for the pseudo-incomp"
W18-2711,N16-1101,0,0.0320253,"ULL&gt; &lt;NULL&gt; A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. 1.000 1.000 1.000 Table 6: Translation examples in {English, French, Brazilian Portuguese}-to-Spanish translation. 5 6 Related Work In this paper, we examined strategies for multisource NMT. On the other hand, there are there are other strategies for multilingual NMT that do not use multiple source sentences as their input. Dong et al. (2015) proposed a method for multitarget NMT. Their method is using one sharing encoder and decoders corresponding to the number of target languages. Firat et al. (2016) proposed a method for multi-source multi-target NMT using multiple encoders and decoders with a shared attention mechanism. Johonson et al. (2017) and Ha et al. (2016) proposed multi-source and multitarget NMT using one encoder and one decoder, and sharing all parameters with all languages. Notably, these methods use multilingual data to better train one-to-one NMT systems. However, our motivation of this study is to improve NMT further by the help of other translations that are available on the source side at test time, and thus their approaches are different from ours. Conclusion In this pa"
W18-2711,L16-1561,0,0.142902,"ench Spanish Hola × × Bonjour Je vous remercie (c) An incomplete multi-source corpus with missing data Figure 1: Example of type of corpora. languages involved in the translation process. For example, we may have an original document in English, that we want to translate into several languages such as French, Spanish, and Portuguese. Some examples of these scenarios are the creation of video captions for talks (Cettolo et al., 2012) or Movies (Tiedemann, 2009), or translation of official documents into all the languages of a governing body, such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-sou"
W18-2711,C16-1133,0,0.437525,"2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-source translation takes in multiple inputs, and references all of them when deciding which sentence to output. Specifically, in the context of neural machine translation (NMT), there are several methods proposed to do so. For example, Zoph and Knight (2016) propose a method where multiple sentences are each encoded separately, then all referenced during the decoding process (the “multi-encoder” method). In addition, Garmash and Monz (2016) propose a method where NMT systems over multiple inputs are ensembled together to make a final prediction (the “mixture-of-NMT-experts” method). Es"
W18-2711,N16-1004,0,0.239969,"ean parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-source translation takes in multiple inputs, and references all of them when deciding which sentence to output. Specifically, in the context of neural machine translation (NMT), there are several methods proposed to do so. For example, Zoph and Knight (2016) propose a method where multiple sentences are each encoded separately, then all referenced during the decoding process (the “multi-encoder” method). In addition, Garmash and Monz (2016) propose a method where NMT systems over multiple inputs are ensembled together to make a final prediction (the “mixture-of"
W18-2711,W04-3250,0,0.0543353,"idation and test data for these experiments were also incomplete. This is in contrast to the experiments on UN6WAY where the test and validation data were complete, and thus this setting is arguable of more practical use. Table 4: The percentage of data without missing sentences on TED data. pus, even if just through the simple modification of replacing missing sentences with &lt;NULL&gt;. 4.3.3 Results Table 5 shows the results in BLEU and BLEU gains with respect to the one-to-one results. All the differences are statistically significant (p &lt; 0.01) by significance tests with bootstrap resampling (Koehn, 2004). The multi-source NMTs achieved consistent improvements over the oneto-one baseline as expected, but the BLEU gains were smaller than those in the previous experiments using the UN6WAY data. This is possibly With respect to the difference between the multi-encoder NMT and mixture of NMT experts, the multi-encoder achieved much higher BLEU in Pseudo-incomplete (0.8M) and Complete (1M), but this was not the case in Complete (0.2M). One possible reason here is the model complexity; the multi-encoder NMT uses a large single model while one-to-one sub-models in the mixture of NMT experts can be tr"
W18-2711,2005.mtsummit-papers.11,0,0.129127,"h Hello Thank you French Spanish Hola × × Bonjour Je vous remercie (c) An incomplete multi-source corpus with missing data Figure 1: Example of type of corpora. languages involved in the translation process. For example, we may have an original document in English, that we want to translate into several languages such as French, Spanish, and Portuguese. Some examples of these scenarios are the creation of video captions for talks (Cettolo et al., 2012) or Movies (Tiedemann, 2009), or translation of official documents into all the languages of a governing body, such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garma"
W18-6327,P17-1080,0,0.0610242,"Missing"
W18-6327,D14-1179,0,0.0117089,"Missing"
W18-6327,N16-1101,0,0.322685,"a and code of this paper is available at: https://github.com/DevSinghSachan/multilingual_nmt 261 the NMT research community has been transitioning from RNNs to an alternative method for encoding sentences using self-attention (Vaswani et al., 2017), represented by the so-called “Transformer” model, which both improves the speed of processing sentences on computational hardware such as GPUs due to its lack of recurrence, and achieves impressive results. In parallel to this transition to self-attentional models, there has also been an active interest in the multilingual training of NMT systems (Firat et al., 2016; Johnson et al., 2017; Ha et al., Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 261–271 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64027 2016). In contrast to the standard bilingual models, multilingual models follow the multi-task training paradigm (Caruana, 1997) where models are jointly trained on training data from several language pairs, with some degree of parameter sharing. The objective of this is two-fold: First, compared to individually traini"
W18-6327,N18-1032,0,0.0413689,"y, Johnson et al. (2017) proposed a unified model with full parameter sharing and obtained comparable or better performance compared with bilingual translation scores. During model training and decoding, target language was specified by an additional token at the beginning of the source sentence. Coming to low-resource language translation, Zoph et al. (2016) used a transfer learning approach of fine-tuning the model parameters learned on a high-resource language pair of French→English and were able to significantly increase the translation performance on Turkish and Urdu languages. Recently, Gu et al. (2018) ad269 dresses the many-to-one translation problem for extremely low-resource languages by using a transfer learning approach such that all language pairs share the lexical and sentence-level representations. By performing joint training of the model with high-resource languages, large gains in the BLEU scores were reported for low-resource languages. In this paper, we first experiment with the Transformer model for one-to-many multilingual translation on a variety of language pairs and demonstrate that the approach of Johnson et al. (2017) and Dong et al. (2015) is not optimal for all kinds o"
W18-6327,P82-1020,0,0.787194,"Missing"
W18-6327,P15-1166,0,0.403239,"ing, specifically focusing on the self-attentional Transformer model. We find that the full parameter sharing approach leads to increases in BLEU scores mainly when the target languages are from a similar language family. However, even in the case where target languages are from different families where full parameter sharing leads to a noticeable drop in BLEU scores, our proposed methods for partial sharing of parameters can lead to substantial improvements in translation accuracy.1 1 Shared Encoder Source Language: &quot;En&quot; Decoder 2 Target Language 2: &quot;Nl&quot; (a) Shared encoder, separate decoder (Dong et al., 2015). Target Language 1: &quot;De&quot; Shared Encoder Source Language: &quot;En&quot; Shared Decoder Target Language 2: &quot;Nl&quot; (b) Shared encoder and decoder (Johnson et al., 2017). Target Language 1: &quot;De&quot; Decoder 1 Shared Encoder Source Language: &quot;En&quot; Shareable Parameters Decoder 2 Target Language 2: &quot;Nl&quot; (c) Proposed shared decoder with partial parameter sharing. Figure 1: Examples of MTL frameworks for the translation of one source language (for example “En”) to two target languages (for example “De”, “Nl”). The principle remains the same with more than two target languages. Best viewed in color. Introduction Neura"
W18-6327,D15-1166,0,0.0610453,"languages (for example “De”, “Nl”). The principle remains the same with more than two target languages. Best viewed in color. Introduction Neural machine translation (NMT; Sutskever et al. (2014); Cho et al. (2014)) is now the de-facto standard in MT research due to its relative simplicity of implementation, ability to perform end-to-end training, and high translation accuracy. Early approaches to NMT used recurrent neural networks (RNNs), usually LSTMs (Hochreiter and Schmidhuber, 1997), in their encoder and decoder layers, with the addition of an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) to focus more on specific encoded source words when deciding the next translation target output. Recently, 1 Data and code of this paper is available at: https://github.com/DevSinghSachan/multilingual_nmt 261 the NMT research community has been transitioning from RNNs to an alternative method for encoding sentences using self-attention (Vaswani et al., 2017), represented by the so-called “Transformer” model, which both improves the speed of processing sentences on computational hardware such as GPUs due to its lack of recurrence, and achieves impressive results. In parallel to this transition"
W18-6327,P11-2093,1,0.775319,"airs from the openly available TED talks dataset (Qi et al., 2018) whose statistics are mentioned in Table 1. This dataset already contains predefined splits for training, development, and test sets. Among these languages, Romanian (RO) and French (F R) are Romance languages, German (D E) and Dutch (N L) are Germanic languages while Turkish (T R) and Japanese (JA) are unrelated languages that come from distant language families. For all language pairs, tokenization was carried out using the Moses tokenizer,2 except for Japanese, where word segmentation was performed using the KyTea tokenizer (Neubig et al., 2011). To select training examples, we filter sentences with a maximum length of 70 tokens. For evaluation, we report the model’s performance using the standard BLEU score metric (Papineni et al., 2002). We use the mtevalv14.pl script Training Protocols In this work, we follow the same training process for all the experiments. We jointly encode the source and target language words with subword units by applying byte pair encoding (Gage, 1994) with 32,000 merge operations (Sennrich et al., 2016). These subword units restrict the vocabulary size and prevent the need for explicitly handling out-of-voc"
W18-6327,W17-4708,0,0.0128868,"Ihr Herz beginnt schneller zu schlagen . (1.0) Ihr Herz schlägt schneller . (0.27) Table 4: Sample translations from E N→D E when one-to-many multilingual model was trained on unrelated target language pairs E N→D E+T R. In these examples, the method of partial sharing of decoder parameters obtains a very high BLEU score (mentioned in parentheses). sequence encoder and reported moderate improvements in results. Recently, Luong et al. (2016) investigated MTL for a tasks such as parsing, image captioning, and translation and observed large gains in the translation task. Similarly, for MT tasks, Niehues and Cho (2017) also leverage MTL by using additional linguistic information to improve the translation accuracy of NMT models. They share the encoder representations to perform joint training on translation, POS, and NER tasks. MTL has also been widely applied to multilingual translation that will be discussed next. 5.2 Multilingual Translation On the multilingual translation task, Dong et al. (2015) obtained significant performance gains by sharing the encoder parameters of the source language while having a separate decoder for each target language. Later, Firat et al. (2016) attempted the more challengin"
W18-6327,P02-1040,0,0.102755,"ets. Among these languages, Romanian (RO) and French (F R) are Romance languages, German (D E) and Dutch (N L) are Germanic languages while Turkish (T R) and Japanese (JA) are unrelated languages that come from distant language families. For all language pairs, tokenization was carried out using the Moses tokenizer,2 except for Japanese, where word segmentation was performed using the KyTea tokenizer (Neubig et al., 2011). To select training examples, we filter sentences with a maximum length of 70 tokens. For evaluation, we report the model’s performance using the standard BLEU score metric (Papineni et al., 2002). We use the mtevalv14.pl script Training Protocols In this work, we follow the same training process for all the experiments. We jointly encode the source and target language words with subword units by applying byte pair encoding (Gage, 1994) with 32,000 merge operations (Sennrich et al., 2016). These subword units restrict the vocabulary size and prevent the need for explicitly handling out-of-vocabulary symbols as the vocabulary can be used to represent any word. We use LeCun uniform initialization (LeCun et al., 1998) for all the trainable model parameters. Embedding layer weights are ran"
W18-6327,N18-2084,1,0.812938,"ability, and we expect this method to obtain good translation accuracy mainly when the target languages are related (Johnson et al., 2017). 3 Experimental Setup In this section, first, we describe the datasets used in this work and the evaluation criteria. Then, we describe the training regimen followed in all our experiments. All of our models were implemented in PyTorch framework (Paszke et al., 2017) and were trained on a single GPU. 3.1 Datasets and Evaluation Metric To perform multilingual translation experiments, we select six language pairs from the openly available TED talks dataset (Qi et al., 2018) whose statistics are mentioned in Table 1. This dataset already contains predefined splits for training, development, and test sets. Among these languages, Romanian (RO) and French (F R) are Romance languages, German (D E) and Dutch (N L) are Germanic languages while Turkish (T R) and Japanese (JA) are unrelated languages that come from distant language families. For all language pairs, tokenization was carried out using the Moses tokenizer,2 except for Japanese, where word segmentation was performed using the KyTea tokenizer (Neubig et al., 2011). To select training examples, we filter sente"
W18-6327,P16-1162,0,0.0499589,"the Moses tokenizer,2 except for Japanese, where word segmentation was performed using the KyTea tokenizer (Neubig et al., 2011). To select training examples, we filter sentences with a maximum length of 70 tokens. For evaluation, we report the model’s performance using the standard BLEU score metric (Papineni et al., 2002). We use the mtevalv14.pl script Training Protocols In this work, we follow the same training process for all the experiments. We jointly encode the source and target language words with subword units by applying byte pair encoding (Gage, 1994) with 32,000 merge operations (Sennrich et al., 2016). These subword units restrict the vocabulary size and prevent the need for explicitly handling out-of-vocabulary symbols as the vocabulary can be used to represent any word. We use LeCun uniform initialization (LeCun et al., 1998) for all the trainable model parameters. Embedding layer weights are randomly initialized according to truncated Gaussian distribution WE ∼ N (0, dm −1/2 ). In all the experiments, we use Transformer base model configuration (Vaswani et al., 2017) that consists of six encoder-decoder layers, dm = 512, dh = 2, 048, and ` = 8. For optimization, we use SGD with Adam opt"
W18-6327,D16-1163,0,0.0607408,"consisted of one shared encoder and decoder per language and a shared attention layer that was common to all languages. This approach obtained competitive BLEU scores on ten European language pairs while substantially reducing the total parameters. Recently, Johnson et al. (2017) proposed a unified model with full parameter sharing and obtained comparable or better performance compared with bilingual translation scores. During model training and decoding, target language was specified by an additional token at the beginning of the source sentence. Coming to low-resource language translation, Zoph et al. (2016) used a transfer learning approach of fine-tuning the model parameters learned on a high-resource language pair of French→English and were able to significantly increase the translation performance on Turkish and Urdu languages. Recently, Gu et al. (2018) ad269 dresses the many-to-one translation problem for extremely low-resource languages by using a transfer learning approach such that all language pairs share the lexical and sentence-level representations. By performing joint training of the model with high-resource languages, large gains in the BLEU scores were reported for low-resource la"
W18-6327,P18-2104,0,\N,Missing
W18-6462,W13-2242,0,0.291852,"Missing"
W18-6462,C04-1046,0,0.080733,"e second part leverages a one-dimensional convolution layer to integrate local context information for each target word. The third part applies a stack of feed-forward and recurrent neural networks to further encode the global context in the sentence before making the predictions. This model was submitted as the CMU entry to the WMT2018 shared task on QE, and achieves strong results, ranking first in three of the six tracks.1 1 Introduction Quality estimation (QE) refers to the task of measuring the quality of machine translation (MT) system outputs without reference to the gold translations (Blatz et al., 2004; Specia et al., 2013). QE research has grown increasingly popular due to the improved quality of MT systems, and potential for reductions in post-editing time and the corresponding savings in labor costs (Specia, 2011; Turchi et al., 2014). QE can be performed on multiple granularities, including at word level, sentence level, or document level. In this paper, we focus on quality estimation at word level, which is framed as the task of performing binary classification of translated tokens, assigning “OK” or “BAD” labels. 2 Model The QE module receives as input a tuple hs, t, Ai, where s = s1"
W18-6462,D14-1181,0,0.0026165,"s://doi.org/10.18653/v1/W18-64089 overall architecture is shown in Figure 1 CEQE consists of three major components: (1) embedding layers for words and part-of-speech (POS) tags in both languages, (2) convolution encoding of the local context for each target word, and (3) encoding the global context by the recurrent neural network. 2.1 After embedding each word in the target sentence {t1 , . . . , tj , . . . , tN }, we obtain a matrix of embeddings for the target sequence, x1:N = x1 ⊕ x2 . . . ⊕ xN , where ⊕ is the column-wise concatenation operator. We then apply one-dimensional convolution (Kim, 2014; Liu et al., 2017) on x1:N along the target sequence to extract the local context of each target word. Specifically, a one-dimensional convolution involves a filter w ∈ Rhk , which is applied to a window of h words in target sequence to produce new features. Embedding Layer Inspired by (Martins et al., 2017), the first embedding layer is a vector representing each target word tj obtained by concatenating the embedding of that word with those of the aligned words sA(:,tj ) in the source. If a target word is aligned to multiple source words, we average the embedding of all the source words, and"
W18-6462,P14-1067,0,0.0280095,"Missing"
W18-6462,J07-1003,0,0.107496,"Missing"
W18-6462,Q17-1015,0,0.0597686,"xt by the recurrent neural network. 2.1 After embedding each word in the target sentence {t1 , . . . , tj , . . . , tN }, we obtain a matrix of embeddings for the target sequence, x1:N = x1 ⊕ x2 . . . ⊕ xN , where ⊕ is the column-wise concatenation operator. We then apply one-dimensional convolution (Kim, 2014; Liu et al., 2017) on x1:N along the target sequence to extract the local context of each target word. Specifically, a one-dimensional convolution involves a filter w ∈ Rhk , which is applied to a window of h words in target sequence to produce new features. Embedding Layer Inspired by (Martins et al., 2017), the first embedding layer is a vector representing each target word tj obtained by concatenating the embedding of that word with those of the aligned words sA(:,tj ) in the source. If a target word is aligned to multiple source words, we average the embedding of all the source words, and concatenate the target word embedding with its average source embedding. The immediate left and right contexts for source and target words are also concatenated, enriching the local context information of the embedding of target word tj . Thus, the embedding of target word tj , denoted as xj , is a 6d dimens"
W18-6462,2011.eamt-1.12,0,0.0259235,"al context in the sentence before making the predictions. This model was submitted as the CMU entry to the WMT2018 shared task on QE, and achieves strong results, ranking first in three of the six tracks.1 1 Introduction Quality estimation (QE) refers to the task of measuring the quality of machine translation (MT) system outputs without reference to the gold translations (Blatz et al., 2004; Specia et al., 2013). QE research has grown increasingly popular due to the improved quality of MT systems, and potential for reductions in post-editing time and the corresponding savings in labor costs (Specia, 2011; Turchi et al., 2014). QE can be performed on multiple granularities, including at word level, sentence level, or document level. In this paper, we focus on quality estimation at word level, which is framed as the task of performing binary classification of translated tokens, assigning “OK” or “BAD” labels. 2 Model The QE module receives as input a tuple hs, t, Ai, where s = s1 , . . . , sM is the source sentence, t = t1 , . . . , tN is the translated sentence, and A ⊆ {(m, n)|1 ≤ m ≤ M, 1 ≤ n ≤ N } is a set of word alignments. It predicts as output a sequence yˆ = y1 , . . . , yN , with each"
W18-6462,P13-4014,0,0.0149647,"ges a one-dimensional convolution layer to integrate local context information for each target word. The third part applies a stack of feed-forward and recurrent neural networks to further encode the global context in the sentence before making the predictions. This model was submitted as the CMU entry to the WMT2018 shared task on QE, and achieves strong results, ranking first in three of the six tracks.1 1 Introduction Quality estimation (QE) refers to the task of measuring the quality of machine translation (MT) system outputs without reference to the gold translations (Blatz et al., 2004; Specia et al., 2013). QE research has grown increasingly popular due to the improved quality of MT systems, and potential for reductions in post-editing time and the corresponding savings in labor costs (Specia, 2011; Turchi et al., 2014). QE can be performed on multiple granularities, including at word level, sentence level, or document level. In this paper, we focus on quality estimation at word level, which is framed as the task of performing binary classification of translated tokens, assigning “OK” or “BAD” labels. 2 Model The QE module receives as input a tuple hs, t, Ai, where s = s1 , . . . , sM is the so"
W19-5303,2012.eamt-1.60,0,0.0358,"indicate the data domain. For data augmentation, they back-translate from a target language to its noisy source. The intuition, also observed by Michel and Neubig (2018), is that the source sentences are noisier than their target translations. They include out-ofdomain clean data during this step and differentiate data types with a special symbol on the target side. In addition, they also run a model ensemble. Training Data In the constrained setting, participants were allowed to use the WMT15 training data3 for Eng↔Fra and any of the KFTT (Neubig, 2011), JESC (Pryzant et al.) and TED talks (Cettolo et al., 2012) corpora for Jpn↔Eng. Additionally, the use of the MTNT corpus (Michel and Neubig, 2018) was allowed in order to adapt models on limited in-domain data. 3.3 Evaluation protocol Test Data The test sets were collected following the same protocol as the MTNT dataset, i.e. collected from 3 http://www.statmt.org/wmt15/ translation-task.html 93 Figure 1: Annotation interface for human evaluations. 94 Eng-Fra Fra-Eng Eng-Jpn Jpn-Eng # samples 1,401 1,233 1,392 1,111 # source tokens # target tokens 20.0k 22.8k 19.8k 19.2k 20.0k 33.6k 18.7k 13.4k Table 1: Statistics of the test sets. model which was al"
W19-5303,P19-1425,0,0.113812,"inkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve robustness. A specific challenge was the small size of the in-domain noisy parallel dataset. We summarize th"
W19-5303,P17-4012,0,0.0548466,"data, improve existing semisupervised approach such as backtranslation. We provide both in-domain (MTNT) and outof-domain (News Commentary, News Crawl, etc) monolingual data. 3.2 Participants and System Descriptions We received 23 submissions from 11 teams. Except two submissions on the Eng-Fra language pair, all systems used the constrained setup. Below we briefly describe the systems from the 8 teams which submitted corresponding system description papers: Baidu & Oregon State University’s submission (Zheng et al., 2019): Their system is based on the Transformer implementation in OpenNMTpy (Klein et al., 2017). The main methods applied in their submission are: domain-sensitive data mixing and data augmentation with backtranslation. For data mixing, they used a special symbol on the source side to indicate the data domain. For data augmentation, they back-translate from a target language to its noisy source. The intuition, also observed by Michel and Neubig (2018), is that the source sentences are noisier than their target translations. They include out-ofdomain clean data during this step and differentiate data types with a special symbol on the target side. In addition, they also run a model ensem"
W19-5303,P18-1163,0,0.189473,"Missing"
W19-5303,W17-3204,1,0.848798,"re efforts from the community in building robust MT models. 2 Related Work The fragility of neural networks (Szegedy et al., 2013) has been shown to extend to neural machine translation models (Belinkov and Bisk, 2018; Heigold et al., 2017) and recent work focused on various aspects of the problem. From the identification of the causes of this brittleness, to the induction of (adversarial) inputs that trigger the unwanted behavior (attacks) and making such models robust against various types of noisy inputs (defenses); improving robustness has been receiving increasing attention in NMT. While Koehn and Knowles (2017) mentioned domain mismatch as a challenge for neural machine translation, Khayrallah and Koehn (2018) addressed noisy training data and focus on the types of noise occurring in web-crawled corpora. Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet and demonstrated that these challenges cannot be overcome by simple domain adaptation techniques alone. Belinkov and Bisk (2018) and Heigold et al. (2017) showed that NMT systems are very sensitive to slightly perturbed input forms, and hinted at the importance of"
W19-5303,W19-5362,0,0.149193,"this campaign. Unlike other participants, the winning team Naver Labs B´erard et al. (2019) and NTT (Murakami et al., 2019) applied data cleaning techniques in order to filter noisy parallel sentences. They filtered i) identical sentences on source and target side, ii) sentences that belonged to a language other than the source and target language, iii) sentences with length mismatch, and iv) also applied attention-based filtering. Data cleaning gave an improvement of more than 5 BLEU points with substantial reduction in the hallucination of the model for the winning team. NICT’s submission (Dabre and Sumita, 2019): The authors used Transformer models to train their systems and employed two strategies namely: i) mixed fine-tuning and ii) multilingual models for making the systems robust. The former helps as the in-domain data is available in a very small quantity. Using a mix of in-domain and outdomain data for fine-tuning helps overcome the problem of adjusting learning rate, applying better regularization and other complicated strategies. It is not clear how these two methods contributed towards making the models more robust. According to the authors, mixed fine-tuning and multilingual training (bidir"
W19-5303,D18-2012,0,0.020004,"mer-Big architecture, whereas improvements were substantially larger when the base models were RNN-Based MTNT baselines, about 8+ BLEU points. Participants emphasized the importance of their strong Transformer-Big base JHU’s submission (Post and Duh, 2019): This submission participated in the Fra→Eng and Jpn↔Eng tasks. The participants used data dual cross-entropy filtering for reducing the monolingual data, then back-translate these, and train their Transformer models (Vaswani et al., 2017). They compared Moses tokenization+Byte Pair Encoding (BPE) (Sennrich et al., 2016), and sentencepiece (Kudo and Richardson, 2018) (without any pre-processing) and found the two comparable, and that using larger sentence-piece models improved over smaller ones. For Jpn↔Eng (both di4 http://www.statmt.org/wmt19/biomedical-translationtask.html 95 sis, they found that their system performs poorly in translating emojis. The segmentation errors generated by KyTea resulted in further errors in the translation. rections) they first used both in-domain (MTNT) and out-of-domain data (other constrained), and then continued training (fine-tune) using MTNT only. They also reported many results from their hyper-parameter search (albe"
W19-5303,N19-1154,1,0.767588,"ise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve robustness. A specific challenge was the small size of the in-domain noisy parallel dataset. We summarize the participating systems in Section 4 and the notable methods in Section 5. The contributions were evaluated both automatically and via a huma"
W19-5303,W18-6459,0,0.0413765,"ish (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized th"
W19-5303,N19-1314,1,0.8427,"ernals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve ro"
W19-5303,C18-1055,0,0.064971,"n this first iteration, the shared-task used the MTNT dataset (Michel and Neubig, 2018) that contains noisy social media texts and their translations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase"
W19-5303,D18-1050,1,0.625161,"erstand the overall challenges in translating social media text and identify major themes of efforts which needs more research from the community. In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial 1 2 In this first iteration, the shared-task used the MTNT dataset (Michel and Neubig, 2018) that contains noisy social media texts and their translations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where ex"
W19-5303,W19-5363,0,0.0447228,"e of tied multitask learning, where the noisy source sentences are first decoded by a same-language denoising decoder, and both information is passed on to the translation decoder. This approach requires data triples of noisy source, clean source, translation, which they created by data augmentation over the provided data, using tag-informed translation systems trained on either noisy (MTNT) or clean (Europarl) data. As the participants point out though, their performance improvements seems to be attributed to data augmentation and not to the intermediate denoising decoder. FOKUS’ submission (Grozea, 2019): This team participated in three directions: Eng→Fra, Fra→Eng and Jpn→Eng. For the Eng→Fra and Fra→Eng language pairs, the submissions are unconstrained systems, where the model was trained on the medical domain corpus provided by the WMT biomedical shared task 4 . Despite the training data being out-of-domain, removing “lowquality” parallel data such as “Subtitles” as the author hypothesized helped to bring 2 to 4 BLEU points improvement over the baseline models. Their Jpn→Eng submission is a constrained system, using the same model architecture as the Eng→Fra language pair. To improve robus"
W19-5303,N19-4007,1,0.827549,"ype tags (real or backtranslated) for further categorization of the training data. Compared to fine-tuning, adding tags provides them additional flexibility, resulting in a generalized system, robust towards a variety of input data. Human Evaluation The results of human evaluation following the evaluation protocol described in Section 3.4 are outlined in Table 2. Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in Table 3. 6.2 Qualitative Analysis In order to discover salient differences between the methods, we performed analysis using compare-mt (Neubig et al., 2019), and present a few of the salient findings below. Fine-tuning Along with the noisy in-domain MTNT data, general domain data typically made available for WMT campaign was also allowed for this task. Most participants (Murakami et al., 2019; Dabre and Sumita, 2019; Helcl et al., 2019) trained on general domain data and fine-tuned the models towards the task. Murakami et al. (2019) did not see a consistent improvement with finetuning. Due to the small size of the in-domain data, Dabre and Sumita (2019) fine-tuned on a mix of in-domain and a subset of the out-of-domain data. Stronger Submissions"
W19-5303,W19-5364,0,0.144816,"Missing"
W19-5303,P02-1040,0,0.110507,"he translators were presented the original source sentence, the reference and the system output side by side. The order between the reference and the system output was randomized by the user interface. The translators rated both the reference and the translation on a scale from 1 to 100. For both the original source sentence and the reference, the original text was presented except for Eng-Jpn where the Japanese reference tokenized with KyTea was presented in order to be consistent with the systems’ outputs. The user interface for annotation is illustrated in Figure 1. We also evaluated BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except Eng-Jpn, we used the original reference and SacreBLEU with the default options. In the case of Eng-Jpn, we used the reference tokenized with KyTea and the option --tokenize none. Task Setup The task includes two tracks, constrained and unconstrained depending on whether the system is trained on a predefined training datasets or not. The two tracks are evaluated by the same automatic and human evaluation protocol, however, they are compared separately. For the constrained system track, the task specifies two types of t"
W19-5303,D19-5506,0,0.114843,"thout accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of"
W19-5303,W18-2709,1,0.860168,"networks (Szegedy et al., 2013) has been shown to extend to neural machine translation models (Belinkov and Bisk, 2018; Heigold et al., 2017) and recent work focused on various aspects of the problem. From the identification of the causes of this brittleness, to the induction of (adversarial) inputs that trigger the unwanted behavior (attacks) and making such models robust against various types of noisy inputs (defenses); improving robustness has been receiving increasing attention in NMT. While Koehn and Knowles (2017) mentioned domain mismatch as a challenge for neural machine translation, Khayrallah and Koehn (2018) addressed noisy training data and focus on the types of noise occurring in web-crawled corpora. Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet and demonstrated that these challenges cannot be overcome by simple domain adaptation techniques alone. Belinkov and Bisk (2018) and Heigold et al. (2017) showed that NMT systems are very sensitive to slightly perturbed input forms, and hinted at the importance of injecting noisy examples during training, also known as adversarial examples. Further research propo"
W19-5303,W18-6319,0,0.0373317,"ce, the reference and the system output side by side. The order between the reference and the system output was randomized by the user interface. The translators rated both the reference and the translation on a scale from 1 to 100. For both the original source sentence and the reference, the original text was presented except for Eng-Jpn where the Japanese reference tokenized with KyTea was presented in order to be consistent with the systems’ outputs. The user interface for annotation is illustrated in Figure 1. We also evaluated BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except Eng-Jpn, we used the original reference and SacreBLEU with the default options. In the case of Eng-Jpn, we used the reference tokenized with KyTea and the option --tokenize none. Task Setup The task includes two tracks, constrained and unconstrained depending on whether the system is trained on a predefined training datasets or not. The two tracks are evaluated by the same automatic and human evaluation protocol, however, they are compared separately. For the constrained system track, the task specifies two types of training data in addition to MTNT train set: •"
W19-5303,P16-1162,0,0.0719206,"on top of the base models with the Transformer-Big architecture, whereas improvements were substantially larger when the base models were RNN-Based MTNT baselines, about 8+ BLEU points. Participants emphasized the importance of their strong Transformer-Big base JHU’s submission (Post and Duh, 2019): This submission participated in the Fra→Eng and Jpn↔Eng tasks. The participants used data dual cross-entropy filtering for reducing the monolingual data, then back-translate these, and train their Transformer models (Vaswani et al., 2017). They compared Moses tokenization+Byte Pair Encoding (BPE) (Sennrich et al., 2016), and sentencepiece (Kudo and Richardson, 2018) (without any pre-processing) and found the two comparable, and that using larger sentence-piece models improved over smaller ones. For Jpn↔Eng (both di4 http://www.statmt.org/wmt19/biomedical-translationtask.html 95 sis, they found that their system performs poorly in translating emojis. The segmentation errors generated by KyTea resulted in further errors in the translation. rections) they first used both in-domain (MTNT) and out-of-domain data (other constrained), and then continued training (fine-tune) using MTNT only. They also reported many"
W19-5303,N19-1190,1,0.747855,"e (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and al"
W19-5303,D18-1316,0,0.0279519,"ations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) rece"
W19-5303,W19-5368,1,0.878409,"not experimented. Finally, participants point out one peculiarity they’ve noticed in the train/validation partitioning of the original MTNT dataset; validation source sentences being started with the letter “Y” followed by alphabetically sorted sentences (test partition not effected). The team experimented with the Fra→Eng and Eng→Fra translation directions, obtaining 43.6 and 36.4 BLEU-cased, respectively (3rd place in both). Their ablations show significant benefit from domain-sensitive training (+3 BLEU), with additional improvements from back-translation and ensembling. CMU’s submission (Zhou et al., 2019): This submission only participated in the Fra→Eng direction. They proposed the use of tied multitask learning, where the noisy source sentences are first decoded by a same-language denoising decoder, and both information is passed on to the translation decoder. This approach requires data triples of noisy source, clean source, translation, which they created by data augmentation over the provided data, using tag-informed translation systems trained on either noisy (MTNT) or clean (Europarl) data. As the participants point out though, their performance improvements seems to be attributed to da"
W19-5303,Q19-1004,1,\N,Missing
W19-5303,N19-1311,1,\N,Missing
W19-5303,W19-5366,0,\N,Missing
W19-5368,D18-1050,1,0.760721,"proposed strategy is flexible and it could be used as long as we have at least one element of the T triple. 566 Figure 2: Training data synthesis. Blocks rounded by dash rectangle are synthetic while others are real. lated French sentences are of high quality and thus treat them as clean French text. Depending on which part of triple is available, we select the proper NMT model and synthesize the missing ones. In Figure 2, we show 3 ways that we did this in this work. Note that because we focus on the translation from French to English where the French text mostly consists of MTNTstyle noise (Michel and Neubig, 2018), we specify the source language as fr, the target language as en and the noise style as MTNT; however, our approach could be used for all other language pairs with different noise distributions. Clean fr: To make our back-translation strategy more generalized to settings where the above parallel data is not enough to train the model, we also design a pipeline to utilize monolingual data which is likely to be available most of the time. In this case, we first translate these sentences to English and then translate them back to French. Both NMT models are trained with TED and MTNT data as we de"
W19-5368,W19-4822,1,0.842506,"sing source text and domain adaptation, both of which are popular approaches for designing robust NMT systems. Compared to the baseline vanilla transformer that is trained on clean data only, our proposed model with fine tuning enjoys 7.1 BLEU points improvement on the WMT Robustness shared task French to English dataset. However, this improvement is most likely attributed to the noisy text we add to the training process (hence, due to better domain adaptation), and not due to the denoising multi-task strategy. kinds of noise. Similar findings were outlined in Anastasopoulos et al. (2019) and Anastasopoulos (2019), which evaluated MT systems on natural and natural-like grammatical noise, specifically on English produced by non-native speakers. Natural noise appears to be richer and more complex compared to synthetic noise, making it challenging to manually design a comprehensive set of noise to approximate real world settings. In our work, we follow (Vaibhav et al., 2019) and synthesize the noisy text through back-translation. There is no need to manually control the distribution of noise. In terms of multi-task learning for machine translation, Tu et al. (2017) proposes to add a reconstructor on top o"
W19-5368,N18-1008,1,0.884763,"al. (2015); Vaswani et al. (2017)) systems are known to degrade drastically when confronted with noisy data (Belinkov and Bisk, 2017; Khayrallah and Koehn, 2018; Anastasopoulos et al., 2019). Thus, there is increasing need to build robust NMT systems that are resilient to naturally occurring noise. In this work, we attempt to enhance the robustness of the NMT system through multi-task learning. Our model is a transformer-based model (Vaswani et al., 2017) augmented with two decoders, with each decoder bound to different learning objectives. It has a cascade architecture (Niehues et al., 2016; Anastasopoulos and Chiang, 2018) where the first decoder reads in the output of the encoder and the second decoder reads in the 2 Multi-task Transformer In this section, we describe in detail the architecture of our proposed multi-task transformer. It is a transformer-based (Vaswani et al., 2017) cascade multi-task framework (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). 2.1 Detailed Architecture As illustrated in Figure 1, the model consists of one transformer encoder and two transformer de1 The code is available at https://github.com/ shuyanzhou/multitask_transformer 565 Proceedings of the Fourth Conference on Ma"
W19-5368,C16-1172,0,0.0592808,"Missing"
W19-5368,N19-1311,1,0.775947,"Missing"
W19-5368,N19-1043,0,0.0282892,". In our work, we follow (Vaibhav et al., 2019) and synthesize the noisy text through back-translation. There is no need to manually control the distribution of noise. In terms of multi-task learning for machine translation, Tu et al. (2017) proposes to add a reconstructor on top of the decoder. The auxiliary objective is to reconstruct the source sentence from the hidden layers of the translation decoder. This encourages the decoder to embed complete source information, which helps improve the translation performance. This approach was found to be helpful in low-resource MT scenarios also by Niu et al. (2019). Anastasopoulos and Chiang (2018) proposes a tied multitask learning model architecture to improve the speech translation task. The intuition is that, speech transcription as an intermediate task, should improve the performance of speech translation if the speech translation is based on both the input speech and its transcription. 6 Acknowledgements We thank AWS Educate program for donating computational GPU resources used in this work. We also thank Daniel Clothiaux and Junxian He for their insightful comments. This material is based upon work supported in part by the Defense Advanced Resear"
W19-5368,N19-4009,0,0.169687,"lean English parallel data). Finally, we carry out a case study by comparing the output of our model with the baseline model. 4.1 Data Pre-processing Because of time limitations, we did not use all three kinds of training triples. We only used the first two triples introduced in Section 3. Noisy fr & Clean en: This kind of parallel text can be found in the MTNT training data. Note that even though the manually translated English sentences contain some level of “noise” (e.g. emoji), we treat them as clean English text. In this scenario, we leverage a pre-trained NMT system provided by fairseq (Ott et al., 2019) to translate English sentences back to French. Considering its good performance over other benchmarks (e.g. WMT newstest datasets) we assume that the transClean fr & Clean en: The clean data consists of europarl-v73 and news-commentary-v10 copora.4 We filter out sentences whose length is greater 2 We did not attempt this due to time restrictions. http://www.statmt.org/europarl/v7/ fr-en.tgz 4 http://www.statmt.org/wmt15/ training-parallel-nc-v10.tgz 3 567 than 50. We apply a pretrained Byte Pair Encoding (BPE, Gage (1994)) model with 16k subword units to both source and target sentences. The"
W19-5368,W18-1807,0,0.130833,"is that the first (denoising) decoder does not really properly deal with noise in the desired way, and the translation decoder generally 5 Related Work Here, we discuss how the MT community handles the noise problem. In general, there are mainly two kinds of approaches: the first attempts to denoise text, and the second proposes training with noisy texts. Denoising text: Sakaguchi et al. (2017) proposes semi-character level recurrent neural network (scRNN) to correct words with scrambling characters. Each word is represented as a vector with elements corresponding to the characters’ position. Heigold et al. (2018) investigates the robustness of character-based word embeddings in machine translation against word scrambling and random noise. The experiments show that the noise has a larger influence on character-based models than BPE-based models. To minimize the influence of word structure, Belinkov and Bisk (2017) proposes to represent word as its average character embeddings, which is invariant to these kinds of noise. The proposed method enables the MT system to be more robust to scrambling noise even training the model with clean text. Instead of handling noise at the word level, we try to recover t"
W19-5368,W18-2709,0,0.0189376,"e of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text1 . 1 Introduction Real world data, especially in the realm of social media, often contains noise such as mis-spellings, grammar errors, or lexical variations. Even though humans do not have much difficulty in recognizing and translating noisy or ungrammatical sentences, neural machine translation (NMT; Bahdanau et al. (2015); Vaswani et al. (2017)) systems are known to degrade drastically when confronted with noisy data (Belinkov and Bisk, 2017; Khayrallah and Koehn, 2018; Anastasopoulos et al., 2019). Thus, there is increasing need to build robust NMT systems that are resilient to naturally occurring noise. In this work, we attempt to enhance the robustness of the NMT system through multi-task learning. Our model is a transformer-based model (Vaswani et al., 2017) augmented with two decoders, with each decoder bound to different learning objectives. It has a cascade architecture (Niehues et al., 2016; Anastasopoulos and Chiang, 2018) where the first decoder reads in the output of the encoder and the second decoder reads in the 2 Multi-task Transformer In this"
W19-5368,N19-1190,1,0.885147,", the objective of the second decoder, namely the translation decoder, is to correctly translate the sentence to the target language. This framework should be beneficial in two ways: 1) Since the model is trained with noisy text, it should inherently better generalize to noisy text. 2) The translation decoder could potentially take advantage of the recovered clean sentence while maintaining specific varieties of noise (e.g. emoji) by referring to the original noisy sentence. This framework requires triplets of clean and noisy source sentences, along with target translations, so we also follow Vaibhav et al. (2019) and design a back-translation strategy that synthesizes noisy data. Our proposed model outperforms the baseline vanilla transformer trained with clean text by 4.6 BLEU points on the WMT 2019 Robustness shared task (Li et al., 2019) French to English dataset. The fine-tuning process brings an additional 2.5 points improvement. According to our analysis, however, the improvements can mainly be attributed to introducing noisy data during training rather than the multi-task learning objective. While neural machine translation (NMT) achieves remarkable performance on clean, indomain text, performa"
W19-5368,kobus-etal-2017-domain,0,0.0312925,"T data as we describe above. Similarly, in both directions, we add the MTNT tag in the beginning of the sentences. Note that alternatively one could use an off-the-shelf NMT model to generate clean English text.2 Clean fr & Clean en: This is the most common parallel corpus that could be obtained from many existing resources. The only missing text is the noisy French text. In this case, we synthesize the noisy text with the help of the NMT model trained with both TED and MTNT training data. During training, we add a tag showing the source of this pair at the beginning of each English sentence (Kobus et al., 2017; Vaibhav et al., 2019). By adding this tag, the model could potentially better distinguish TED data and MTNT data. To generate the noisy French text, we add an MTNT tag at the beginning of each sentence and feed them to this NMT model. Ideally, besides the inherent noise as a result of imperfect translations, the translated French sentences could also possess a similar noise distribution as MTNT. 4 Experiments In this section, we first describe in detail our data pre-processing scheme, as well as the choice of hyperparameters. Then we compare our system with the baseline model (a vanilla tran"
W19-5368,W19-5303,1,0.878835,"ld inherently better generalize to noisy text. 2) The translation decoder could potentially take advantage of the recovered clean sentence while maintaining specific varieties of noise (e.g. emoji) by referring to the original noisy sentence. This framework requires triplets of clean and noisy source sentences, along with target translations, so we also follow Vaibhav et al. (2019) and design a back-translation strategy that synthesizes noisy data. Our proposed model outperforms the baseline vanilla transformer trained with clean text by 4.6 BLEU points on the WMT 2019 Robustness shared task (Li et al., 2019) French to English dataset. The fine-tuning process brings an additional 2.5 points improvement. According to our analysis, however, the improvements can mainly be attributed to introducing noisy data during training rather than the multi-task learning objective. While neural machine translation (NMT) achieves remarkable performance on clean, indomain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multitask learning algorithm for transformer-based MT systems that is more resi"
W19-5368,E17-2004,0,0.0311683,"ed models. To minimize the influence of word structure, Belinkov and Bisk (2017) proposes to represent word as its average character embeddings, which is invariant to these kinds of noise. The proposed method enables the MT system to be more robust to scrambling noise even training the model with clean text. Instead of handling noise at the word level, we try to recover the clean text from the noisy one at the sentence level. Besides noise like word scrambling, the sentence level denoising could potentially better deal with more complex noise like grammatical errors. Training with noisy data: Li et al. (2017) designs methods to generate noise in the text, mainly focusing on syntactic noise and semantic noise. (Sperber et al., 2017) proposes a noise model based on automatic speech recognizer (ASR) error types, which consists of substitutions, deletions and insertions. Their noise model samples the positions of words that should be altered in the source sentence. Even training with synthetic noise data brings a large improvement in translating noisy data, Belinkov and Bisk (2017) shows that models mainly perform well on the same kind of noise that is introduced at training time, and they mostly fail"
W19-5909,L18-1307,0,0.0220459,"s to annotate efficiently and accurately. Thus, curating large corpora is labor-intensive, and we are always faced with a paucity of data in new domains and labeling paradigms of interest. Moreover, the label assigned to an utterance depends on the current state of the dialogue (Stone, 2005) and prediction of an utterance’s label benefits from referring to other utterances in context and their labels (Jaiswal et al., 2019). Deep learning models like RNNs and CNNs have proven effective tools to encode neighbouring utterances (Chen et al., 2018; Liu et al., 2017; Blunsom and Kalchbrenner, 2013; Bothe et al., 2018; Kumar et al., 2017). However such models rely on large annotated corpora that are prohibitively expensive to procure, especially for niche domains. One recently popular method to overcome the dearth of supervised data in NLP is unsupervised pretraining over large unlabeled corpora. For exIntroduction Spontaneous human conversations have been collected in different domains to support research in data-driven dialogue systems (Serban et al., 2015), affective computing (Zadeh et al., 2018; Busso et al., 2008; Park et al., 2014), clinical psychology (Althoff et al., 2016) and tutoring systems (Si"
W19-5909,Q16-1033,0,0.0261721,"som and Kalchbrenner, 2013; Bothe et al., 2018; Kumar et al., 2017). However such models rely on large annotated corpora that are prohibitively expensive to procure, especially for niche domains. One recently popular method to overcome the dearth of supervised data in NLP is unsupervised pretraining over large unlabeled corpora. For exIntroduction Spontaneous human conversations have been collected in different domains to support research in data-driven dialogue systems (Serban et al., 2015), affective computing (Zadeh et al., 2018; Busso et al., 2008; Park et al., 2014), clinical psychology (Althoff et al., 2016) and tutoring systems (Sinha et al., 2015). These conversations are analyzed by segmenting transcriptions into each speaker’s utterances (Traum and Heeman, 1996), which are often labeled with different types of information. The exact type of label to be used depends on the downstream task or research questions to be answered, and thus the tagging paradigms are varied and numerous. For example, the speaker’s intention can be specified using a dialogue acts (DAs) or speech acts (Searle and Searle, 1969), which capture the pragmatic or semantic function of the utterance. 68 Proceedings of the SIG"
W19-5909,J97-1002,0,0.432489,"ic improvisations or scripted sceX 1 LBOW (C) = LBOW (ui ). (2) narios, with eight categorical emotion labels (Park N i=0 et al., 2014) (10K utterances). LEGO, a subset (14K utterances) of the Lets 2.2 Utterance Tagging Go bus-information dialogue system corpus (Raux Once we have learned contextualized utterance rep- et al., 2006) annotated with the ISO 24617-2 stanresentations, we can use them to predict the se- dard for conversation functions of task by (Ribeiro quence of labels Y = {y1 , y2 , ..., yN }, such as et al., 2016). dialogue acts, for utterances in the conversation. In Map Task, (Carletta et al., 1997; Anderson et al., this work we use a linear-chain conditional random 1991) is 18 hrs of dialogue where speakers collabofield (Lafferty et al., 2001) as used in previous state- rate to complete a map (5K utterances). of-the-art models for DA tagging (Kumar et al., To simulate low-resource settings for the larger 2017; Chen et al., 2018) to predict one of the |T | datasets like SWDA and MRDA, we experiment tags for each ui , where the utterance is represented with different sizes of the training datasets and evalas the concatenation of the forward and backward uate on the standard test set for"
W19-5909,N18-1202,0,0.301503,"ften labeled with different types of information. The exact type of label to be used depends on the downstream task or research questions to be answered, and thus the tagging paradigms are varied and numerous. For example, the speaker’s intention can be specified using a dialogue acts (DAs) or speech acts (Searle and Searle, 1969), which capture the pragmatic or semantic function of the utterance. 68 Proceedings of the SIGDial 2019 Conference, pages 68–74 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics {""Hey"", ""man"", ""!""} ample, Melamud et al. (2016); Peters et al. (2018); Devlin et al. (2018) use language modeling as an unsupervised task to learn word embeddings in context, and demonstrate remarkable improvements on a number of downstream NLP tasks. However, these methods learn representations for individual words, whereas for dialog analysis tasks, we need representations for utterances in the context of the entire dialog. In this paper, we adapt the technique of learning contextualized representations using unsupervised pretraining to learn representations for utterances in the context of the dialogue. We first introduce a general model architecture consist"
W19-5909,P17-1081,0,0.0347523,"to train contextualized utterance embeddings using a bag-of-word reconstruction loss is beneficial for utterance-level tagging in the low-resource setting, indicating that these embeddings learn useful and generalizeable properties of conversational discourse. Future work involves incorporating speaker identity, utterance duration and speech/prosody features. Result and Discussion We observe that using pretrained utterance representations shows improved performance over random initialization and is competitive with existing state-of-the-art works by Kumar et al. (2017) for SwDA and MRDA, and Poria et al. (2017) for IEMOCAP that use similar hierarchical architec71 Utterance B: where are you going to move to? A: Uh, Maryland. B: Oh, are you? A: Uh-huh. B: Do you have friends there? B: or, A: My fiancee is down there hlaughteri. B: Oh, I see. B: So, does he work for a company down there? A: Yeah, A: he works for the government. B: Oh, I see. B: Oh, the big company. A: Yeah A: and I said no, I’m just twenty-three, B: Uh-huh. A: you know, because I don’t think of myself as needing to have children A: but the first thing he says is, well, don’t you miss that part of your life. A: And I just, A: my, my min"
W19-5909,D17-1231,0,0.0278089,"pendent (Table 1), making it challenging for humans to annotate efficiently and accurately. Thus, curating large corpora is labor-intensive, and we are always faced with a paucity of data in new domains and labeling paradigms of interest. Moreover, the label assigned to an utterance depends on the current state of the dialogue (Stone, 2005) and prediction of an utterance’s label benefits from referring to other utterances in context and their labels (Jaiswal et al., 2019). Deep learning models like RNNs and CNNs have proven effective tools to encode neighbouring utterances (Chen et al., 2018; Liu et al., 2017; Blunsom and Kalchbrenner, 2013; Bothe et al., 2018; Kumar et al., 2017). However such models rely on large annotated corpora that are prohibitively expensive to procure, especially for niche domains. One recently popular method to overcome the dearth of supervised data in NLP is unsupervised pretraining over large unlabeled corpora. For exIntroduction Spontaneous human conversations have been collected in different domains to support research in data-driven dialogue systems (Serban et al., 2015), affective computing (Zadeh et al., 2018; Busso et al., 2008; Park et al., 2014), clinical psycho"
W19-5909,K16-1006,0,0.0318586,"man, 1996), which are often labeled with different types of information. The exact type of label to be used depends on the downstream task or research questions to be answered, and thus the tagging paradigms are varied and numerous. For example, the speaker’s intention can be specified using a dialogue acts (DAs) or speech acts (Searle and Searle, 1969), which capture the pragmatic or semantic function of the utterance. 68 Proceedings of the SIGDial 2019 Conference, pages 68–74 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics {""Hey"", ""man"", ""!""} ample, Melamud et al. (2016); Peters et al. (2018); Devlin et al. (2018) use language modeling as an unsupervised task to learn word embeddings in context, and demonstrate remarkable improvements on a number of downstream NLP tasks. However, these methods learn representations for individual words, whereas for dialog analysis tasks, we need representations for utterances in the context of the entire dialog. In this paper, we adapt the technique of learning contextualized representations using unsupervised pretraining to learn representations for utterances in the context of the dialogue. We first introduce a general mode"
W19-5909,P18-1208,0,0.0496338,"ogueact recognition and emotion classification, especially in low-resource settings encountered when analyzing conversations in new domains. A: Hi B: Hi, How are you? A: Are you done with your homework? B: Yeah B: How about you? A: I’m having trouble with Q4 B: Yeah A: so it’s going to take some time Greeting Greeting Question Yes Answer Question Statement Backchannel Statement Table 1: Snippets of conversation with dialogue act tags. “Yeah” is tagged differently in different contexts. Utterances may also be tagged with traits such as sentiment, emotion and valence labels (Busso et al., 2008; Zadeh et al., 2018), speaker persuasiveness (Park et al., 2014), speaker dominance(Busso et al., 2008) and other characteristics at the utterance and conversational level. While these labels vary greatly, one constant is that they are often ambiguous and contextdependent (Table 1), making it challenging for humans to annotate efficiently and accurately. Thus, curating large corpora is labor-intensive, and we are always faced with a paucity of data in new domains and labeling paradigms of interest. Moreover, the label assigned to an utterance depends on the current state of the dialogue (Stone, 2005) and predicti"
