2006.iwslt-papers.8,takezawa-kikui-2004-comparative,0,0.146526,"Missing"
2006.iwslt-papers.8,2006.iwslt-evaluation.12,1,\N,Missing
2012.iwslt-evaluation.10,2012.iwslt-evaluation.1,0,0.0411309,"two stages, where the systems of the second stage are adapted in an unsupervised manner on the combination of the ﬁrst stage outputs using VTLN, MLLR, and cMLLR. Index Terms: speech recognition, IWSLT, TED talks, evaluation system, system development 1. Introduction The International Workshop on Spoken Language Translation (IWSLT) offers a comprehensive evaluation campaign on spoken language translation. One part of the campaign focuses on the translation of TED Talks1 , short 5-25min presentations by people from various ﬁelds related in some way to Technology, Entertainment, and Design (TED) [1]. In order to evaluate different aspects of this task IWSLT organizes several evaluation tracks on this data covering the aspects of automatic speech recognition (ASR), machine translation (MT), and the full-ﬂedged combination of the two of them into speech translation systems. The goal of the TED ASR track is the automatic transcription of TED lectures on a given segmentation, in order to interface with the machine translation components in the speech-translation track. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our English ASR"
2012.iwslt-evaluation.10,2011.iwslt-evaluation.12,1,0.446759,"nslation (MT), and the full-ﬂedged combination of the two of them into speech translation systems. The goal of the TED ASR track is the automatic transcription of TED lectures on a given segmentation, in order to interface with the machine translation components in the speech-translation track. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our English ASR systems with which we participated in the TED ASR track of the 2012 IWSLT evaluation campaign. This year, our system is a further development of our last year’s evaluation system [2] and makes use of system combination and cross-adaptation, by utilising acoustic models which are trained with different acoustic front-ends and employ two different phoneme sets. In addition to last year, we also included TED talks available via TED’s website by training on them in a slightly supervised manner. We submitted two primary systems. One was solely developed by KIT, the other one was developed in cooperation with NAIST in Japan. A description of the additional work done by NAIST on the KIT-NAIST (contrastive) submission can be found in [3]. On the 2011 evaluations set, which serves"
2012.iwslt-evaluation.10,2012.iwslt-evaluation.11,1,0.864393,"pment of our last year’s evaluation system [2] and makes use of system combination and cross-adaptation, by utilising acoustic models which are trained with different acoustic front-ends and employ two different phoneme sets. In addition to last year, we also included TED talks available via TED’s website by training on them in a slightly supervised manner. We submitted two primary systems. One was solely developed by KIT, the other one was developed in cooperation with NAIST in Japan. A description of the additional work done by NAIST on the KIT-NAIST (contrastive) submission can be found in [3]. On the 2011 evaluations set, which serves as a progress test set, we were able to reduce the word error rate of our transcription 1 http://www.ted.com/talks Text corpus IWSLT training data transcripts News (+news commentary) Parallel Giga Corpus LDC English Gigaword 4 UN + Europarl documents Google Books Ngrams (subset) total Word Count 3 million 2114 million 523 million 1800 million 376 million 1000 million ngrams 4816 million sources 2 4 1 6 1 1 15 Table 1: Language Model training data word count per corpus after cleaning and data selection and number of text sources included in corpus. Th"
2012.iwslt-evaluation.10,2011.iwslt-papers.2,1,0.705542,"of the ﬁnal feature vectors was empirically proven to work well and coincides with the dimensionality of a 14 dimensional static feature vector augmented with ﬁrst and second order dynamic features. In recent years neural network based features have been shown to improve ASR systems [6]. A typical setup involves training a neural network to recognize phones (or phone-states) from a window of ordinary (e.g. MFCC) feature vectors. With the help a hidden bottleneck layer the trained network can be used to project the input features onto a feature vector with an arbitrarily chosen dimensionality [7]. The input vector is derived from a 15 frame context window with each frame containing 20 MFCC or MVDR coefﬁcients. So far, we used LDA to reduce the dimensionality of this input vector, which limits the resulting LDA-features to linear combinations of the input features. A multi layer perceptron (MLP) with the bottleneck in the 2nd hidden layer can make use of nonlinear information. For our IWSLT systems we used bottleneck features for both our MVDR and MFCC front ends. 4. Acoustic Modeling 4.1. Data Preprocessing For the TED data only subtitles were available so the data had to be split int"
2012.iwslt-evaluation.11,2011.iwslt-evaluation.1,1,0.866959,"Missing"
2012.iwslt-evaluation.11,P07-1085,0,0.0270438,"Missing"
2012.iwslt-evaluation.11,N10-1103,0,0.0548812,"Missing"
2012.iwslt-evaluation.15,D12-1037,0,0.0218568,"Missing"
2012.iwslt-evaluation.15,P07-2045,0,0.0268509,"Missing"
2012.iwslt-evaluation.15,P02-1040,0,\N,Missing
2012.iwslt-evaluation.15,N04-1022,0,\N,Missing
2012.iwslt-evaluation.15,P03-1021,0,\N,Missing
2012.iwslt-evaluation.15,J03-1002,0,\N,Missing
2012.iwslt-evaluation.15,2009.iwslt-evaluation.12,1,\N,Missing
2012.iwslt-evaluation.15,I05-3027,0,\N,Missing
2012.iwslt-evaluation.5,2012.iwslt-evaluation.1,0,0.0315298,"D task, exploring issues such as out-of-domain data ﬁltering, minimum Bayes risk decoding, MERT vs. PRO tuning, word alignment combination, and morphology. Decoding Baseline NAIST Submission dev2010 26.02 27.05 tst2010 29.75 31.81 Table 1: The scores for systems with and without the proposed improvements. ogy processing and large language models, which resulted in an average gain of 1.18 BLEU points over all languages. Section 4 describes these results in further detail. 2. English-French System 1. Introduction This paper describes the NAIST participation in the IWSLT 2012 evaluation campaign [1]. We participated in all 11 TED tasks, dividing our efforts in half between the ofﬁcial English-French track and the 10 other unofﬁcial ForeignEnglish tracks. For all tracks we used the Moses decoder [2] and its experiment management system to run a large number of experiments with different settings over many language pairs. For the English-French system we experimented with a number of techniques, settling on a combination that provided signiﬁcant accuracy improvements without introducing unnecessary complexity into the system. In the end, we chose a four-pronged approach consisting of using"
2012.iwslt-evaluation.5,P07-2045,0,0.0162664,"2 27.05 tst2010 29.75 31.81 Table 1: The scores for systems with and without the proposed improvements. ogy processing and large language models, which resulted in an average gain of 1.18 BLEU points over all languages. Section 4 describes these results in further detail. 2. English-French System 1. Introduction This paper describes the NAIST participation in the IWSLT 2012 evaluation campaign [1]. We participated in all 11 TED tasks, dividing our efforts in half between the ofﬁcial English-French track and the 10 other unofﬁcial ForeignEnglish tracks. For all tracks we used the Moses decoder [2] and its experiment management system to run a large number of experiments with different settings over many language pairs. For the English-French system we experimented with a number of techniques, settling on a combination that provided signiﬁcant accuracy improvements without introducing unnecessary complexity into the system. In the end, we chose a four-pronged approach consisting of using the web data with ﬁltering to remove noisy sentences, phrase table smoothing, language model interpolation, and minimum Bayes risk decoding. This led to a score of 31.81 BLEU on the tst2010 data set, a"
2012.iwslt-evaluation.5,N03-1017,0,0.111297,"urposes, in Section 3, we also present additional experiments that gave negative results, which were not included in our ofﬁcial submission. For the 10 translation tasks into English, we focused on techniques that could be used widely across all languages. In particular, we experimented with unsupervised approaches to handling source-side morphology, minimum Bayes risk decoding, and large language models. In the end, most of our systems used a combination of unsupervised morpholThe NAIST English-French translation system for IWSLT 2012 was based on phrase-based statistical machine translation [3] using the Moses decoder [2] and its corresponding training regimen. Overall, we made four enhancements over the standard Moses setup to improve the translation accuracy: Large-scale Data with Filtering: In order to use the large, but noisy parallel training data in the English-French Giga Corpus, we implemented a technique to ﬁlter out noisy translated text. Phrase Table Smoothing: We performed phrase table smoothing to improve the probability estimates of low-frequency phrases. Language Model Interpolation: In order to adapt to the domain of the task, we interpolated language models trained"
2012.iwslt-evaluation.5,J05-4003,0,0.0924584,"Missing"
2012.iwslt-evaluation.5,2011.iwslt-evaluation.9,0,0.0502184,"Missing"
2012.iwslt-evaluation.5,J93-2003,0,0.0294894,"Missing"
2012.iwslt-evaluation.5,W99-0604,0,0.275234,"Missing"
2012.iwslt-evaluation.5,P02-1038,0,0.0218946,"s comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smoothing of the phrase table improves results by a signiﬁcant amount. In initial research on MBR, the space of possible hypotheses E was deﬁned as the n-best list output by the decoder. This was further expanded by [14], who deﬁned MBR over lattices. We tested both of these approaches (as implemented in the Moses decoder). Finally, one ﬁne point about MBR is that it requires a good estimate of the probability P (E  |F ) of hypotheses. In the discriminative training framework of [15], which is used in most modern SMT systems, scores of machine translation hypotheses are generally deﬁned as a log-linear combination of feature functions such as language model or translation model probabilities P (E  |F ) = 2.3. Language Model Interpolation One of the characteristics of the IWSLT TED task is that, as shown in Table 2, we have several heterogeneous corpora. In addition, the in-domain TED data is relatively small, so it can be expected that we will beneﬁt from using data outside of the TED domain. In order to effectively utilize out-ofdomain data in language modeling, we buil"
2012.iwslt-evaluation.5,P03-1021,0,0.0174134,"ns. 1 i wi φi (E  ,F ) e Z (4) where φi indicates feature functions such as the language model, translation model, and reordering model log probabilities, wi is the weight measuring the relative importance of this feature, and Z is a partition function that ensures that the probabilities add to 1. Choosing the weights wi for each feature such that the answer with highest probability ˆ = argmax P (E|F ) E E (5) is the best possible translation is a process called “tuning,” and essential to modern SMT systems. However, in most tuning methods, including the standard minimum error rate training [16] that was used in the proposed system, while the relative weight of each feature wi is adjusted, the overall sum of the weights i wi is generally set ﬁxed at 1. While this is not a problem when ﬁnding the highest probability hypothesis in 5, it will affect the probability estimates P (E  |F ), with 56 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Decoding Viterbi MBR (λ = 1) Lattice MBR (λ = 1) Lattice MBR (λ = 5) dev2010 27.59 27.29 26.70 27.05 tst2010 31.01 31.24 31.25 31.81 Table 6: BLEU Results using Minimum Bayes Risk decoding. larger s"
2012.iwslt-evaluation.5,2010.iwslt-papers.5,1,0.897262,"Missing"
2012.iwslt-evaluation.5,W06-1607,0,0.01659,"pothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we use one minus sentence-wise BLEU+1 score [13] as our loss function L(E  , E) = 1 − BLEU+1(E  , E). (3) 2.2. Phrase Table Smoothing We also performed experiments that used smoothing of the statistics used in calculating translation model probabilities [9]. The motivation behind this method is that the statistics used to train the phrase table are generally sparse, and tend to over-estimate the probabilities of rare events. In the submitted system we used Good-Turing smoothing for the phrase table probabilities. Results comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smoothing of the phrase table improves results by a signiﬁcant amount. In initial research on MBR, the space of possible hypotheses E was deﬁned as the n-best list output by the decoder. This was further expanded by ["
2012.iwslt-evaluation.5,P12-1018,1,0.830308,"ch TED task. 3.2. Word Alignment & Phrase Table Combination We investigated different alignment tools and ways to combine them, as shown in Table 8. Observations are as follows: • GIZA++ and BerkeleyAligner achieve similar BLEU on this task. • Concatenating GIZA++ and BerkeleyAligner word alignment results, prior to phrase extraction, achieves a small boost (29.57 to 29.89 BLEU). We experimented with the simplest approach to exploiting out-of-domain bitext in translation models: data concatenation. This can be seen as adaptation at the earliest stage of the • We also experimented with pilaign [19], a Bayesian phrasal alignment toolkit. This tool directly extracts phrases without resorting to the preliminary step of word alignments, and achieves extremely compact phrase table sizes (0.8M entries) without signiﬁcantly sacriﬁcing BLEU (29.24). 3 It should be noted that due to constraints in the available data for these MBR experiments we are both tuning on testing on tst2010, but the tuning of λ also demonstrated gains in accuracy on the ofﬁcial blind test on tst2011 and tst2012 (37.33→37.90 and 38.92→39.47 respectively). • Combining the GIZA++ and pialign phrase tables by Moses’ multiple"
2012.iwslt-evaluation.5,D11-1125,0,0.0529106,", the number of random restarts was set to 20. 3.3. Lexical Reordering Models Several reordering models available in the Moses decoder were tried. In general, we found the full “msd-bidir-fe” option to perform best, despite the small number of word order differences between English and French. Results are shown in Table 9. Reordering model msd-bidir-fe msd-bidir-f monotonicity-bidir-fe msd-backward-fe distance msd-bidir-fe-collapse BLEU 29.57 29.43 29.29 29.22 28.99 28.86 Table 9: Comparison of Reordering models on tst2010. 3.4. MERT vs. PRO tuning We compared two tuning methods: MERT and PRO [20]. We used the implementations distributed with Moses. For both MERT and PRO, we set the size of k-best list to k = 100, used 14 standard features, and removed duplicates in k-best lists when merging previously generated k-best lists. We ran MERT in multi-threaded setting until convergence. Since the number of random restarts in MERT greatly affects on the translation accuracy [21], we tried various number of random restarts for 1, 10, 20, and 50.4 For PRO, we used MegaM5 as a binary classiﬁer with the default setting. We ran PRO for 25 iterations. We tried two kinds of PRO: [20] interpolated t"
2012.iwslt-evaluation.5,C08-1074,0,0.0166288,"ir-fe msd-backward-fe distance msd-bidir-fe-collapse BLEU 29.57 29.43 29.29 29.22 28.99 28.86 Table 9: Comparison of Reordering models on tst2010. 3.4. MERT vs. PRO tuning We compared two tuning methods: MERT and PRO [20]. We used the implementations distributed with Moses. For both MERT and PRO, we set the size of k-best list to k = 100, used 14 standard features, and removed duplicates in k-best lists when merging previously generated k-best lists. We ran MERT in multi-threaded setting until convergence. Since the number of random restarts in MERT greatly affects on the translation accuracy [21], we tried various number of random restarts for 1, 10, 20, and 50.4 For PRO, we used MegaM5 as a binary classiﬁer with the default setting. We ran PRO for 25 iterations. We tried two kinds of PRO: [20] interpolated the weights with previously learned weights to improve the stability (henceforth “PRO-interpolated”)6 , and 4 Currently, Moses’s default setting is 20. ˜hal/megam/ 6 We set the same interpolation coefﬁcient value of 0.1 as [20] noted. 5 http://www.cs.utah.edu/ the version that do not use such a interpolation (henceforth “PRO-basic”). We ﬁrst investigate the effect of the number of"
2012.iwslt-evaluation.5,N04-1022,0,0.0390465,"ing. LM TED Only Without Interp. With Interp. dev2010 24.80 26.30 27.05 tst2010 29.44 31.15 31.81 Table 5: Results training the language model on only TED data, and when other data is used without and with language model interpolation. results are shown in Table 3. As a result, we can see that using the data from the Giga corpus has a positive effect on the results, but ﬁltering does not have a clear signiﬁcant effect on the results. 2.4. Minimum Bayes Risk Decoding Finally, we experimented with improved decoding strategies for translation, particularly using minimum Bayes risk decoding (MBR, [12]). In normal translation, the decoder attempts to simply ﬁnd the answer with the highest probability among the translation candidates ˆ = argmax P (E|F ) E E (1) in a process called Viterbi decoding. As an alternative to this, MBR attempts to ﬁnd the hypothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we"
2012.iwslt-evaluation.5,C04-1072,0,0.0264753,"empts to simply ﬁnd the answer with the highest probability among the translation candidates ˆ = argmax P (E|F ) E E (1) in a process called Viterbi decoding. As an alternative to this, MBR attempts to ﬁnd the hypothesis that minimizes risk  ˆ = argmin P (E  |F )L(E  , E) (2) E E E  ∈E considering the posterior probability P (E  |F ) of hypotheses E  in the space of all possible hypotheses E, as well as a loss L(E  , E) which determines how bad a translation E is if the true translation is E  . In this work (as with most others on MBR in MT) we use one minus sentence-wise BLEU+1 score [13] as our loss function L(E  , E) = 1 − BLEU+1(E  , E). (3) 2.2. Phrase Table Smoothing We also performed experiments that used smoothing of the statistics used in calculating translation model probabilities [9]. The motivation behind this method is that the statistics used to train the phrase table are generally sparse, and tend to over-estimate the probabilities of rare events. In the submitted system we used Good-Turing smoothing for the phrase table probabilities. Results comparing a system with smoothing and without smoothing can be found in Figure 4. It can be seen that GoodTuring smooth"
2012.iwslt-evaluation.5,E03-1076,0,0.149639,"e baseline results. First, adding additional out-of-domain data (nc=News Commentary, ep=Europarl, un=UN Multitext) to the language model increased results uniformly for all language pairs (line (b) of Table 12). We used an interpolated language model, trained in the same fashion as in our English-French system. Next, we tried two strategies for handling rich morphology in the input. The “CompoundSplit” program in the Moses package was developed for languages with extensive noun compounding, e.g. German, and breaks apart words if sub-parts are seen in the training data over a certain frequency [22]. The alternate “Morfessor” program [23] is an unsupervised morphological analyzer based on the Minimum Description Length principle – it tries to ﬁnd the the smallest set of morphemes that parsimoniously cover the training set. Morfessor is expected to segment more aggressively than CompoundSplit, especially because it can ﬁnd both bound and free morphemes. However, we empirically found that Morfessor segments too aggressively for unknown words (i.e. each character becomes a morpheme), so we do not segment OOV words in dev/test.8 The results in line (c) of Table 12 shows that German beneﬁt mo"
2012.iwslt-evaluation.5,W02-0603,0,0.0311216,"onal out-of-domain data (nc=News Commentary, ep=Europarl, un=UN Multitext) to the language model increased results uniformly for all language pairs (line (b) of Table 12). We used an interpolated language model, trained in the same fashion as in our English-French system. Next, we tried two strategies for handling rich morphology in the input. The “CompoundSplit” program in the Moses package was developed for languages with extensive noun compounding, e.g. German, and breaks apart words if sub-parts are seen in the training data over a certain frequency [22]. The alternate “Morfessor” program [23] is an unsupervised morphological analyzer based on the Minimum Description Length principle – it tries to ﬁnd the the smallest set of morphemes that parsimoniously cover the training set. Morfessor is expected to segment more aggressively than CompoundSplit, especially because it can ﬁnd both bound and free morphemes. However, we empirically found that Morfessor segments too aggressively for unknown words (i.e. each character becomes a morpheme), so we do not segment OOV words in dev/test.8 The results in line (c) of Table 12 shows that German beneﬁt most from CompoundSplit, while Arabic, Rus"
2012.iwslt-evaluation.5,D08-1065,0,\N,Missing
2012.iwslt-papers.2,2011.mtsummit-papers.7,0,0.0422007,"etween duration translation and duration and power translation. Particularly, the former method was often labeled with a score of 2 indicating that the duration is not sufﬁcient to represent emphasis clearly. However, duration+power almost always scored 3 and can be recognized as the position of emphasis. This means that in English-Japanese speech translation, speech’s power is an important factor to convey emphasis. 5. Related Works There have been several studies demonstrating improved speech translation performance by utilizing paralinguistic information of source side speech. For example, [8] focuses on using the input speech’s acoustic information to improve translation accuracy. They try to explore a tight coupling of ASR and MT for speech translation, sharing information on the phone level to boost translation accuracy as measured by BLEU score. Other related works focus on using speech intonation to reduce translation ambiguity on the target side [9, 10]. While the above methods consider paralinguistic information to boost translation accuracy, as we mentioned before, there is more to speech translation than just the accuracy of the target sentence. It is also necessary to con"
2012.iwslt-papers.2,P07-2045,0,0.00260194,"vely, and speech synthesis with TTS. While this is the same general architecture as traditional speech translation systems, we add an additional model to translate not only lexical information but also two types of paralinguistic information: duration and power. In this paper, in order to focus speciﬁcally on paralinguistic translation we chose a simple, small-vocabulary lexical MT task: number-to-number translation. (6) where J indicates the target language sentence and E indicates the recognized source language sentence. Generally we can use a statistical machine translation tool like Moses [4], to obtain this translation in standard translation tasks. However in this paper we have chosen a simple number-tonumber translation task so we can simply write one-to-one lexical translation rules with no loss in accuracy. 3.3. Paralinguistic Translation Paralinguistic translation converts the source-side duration and mean power vector X into the target-side duration and mean power vector Y according to the following equation 159 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 ˆ = arg max P (Y|X). Y Y (7) training data by minimize root mean squ"
2013.iwslt-evaluation.23,N10-1103,0,0.0377182,"Missing"
2013.iwslt-papers.3,D10-1092,0,0.0188586,"nslators. The underlined score is BLEU and the plain score is RIBES generated and checked by voluntary translators. For the two varieties of interpretation data, I1 and I2, we used the transcriptions of the interpretations performed by the S rank and A rank interpreter respectively. The first motivation for collecting this data is that it may allow us to quantitatively measure the similarity or difference between interpretations and translations automatically. In order to calculate the similarity between each of these pieces of data, we use the automatic similarity measures BLEU [9] and RIBES [10]. As BLEU and RIBES are not symmetric, we average BLEU or RIBES in both directions. For example, we calculate for BLEU using 1 {BLEU(R, H) + BLEU(H, R)} 2 (1) where R and H are the reference and the hypothesis. Based on this data, if the similarities of T1-T2 and I1-I2 are higher than T1-I1, T2-I1, T1-I2 and T2-I2, we can find that there are real differences between the output produced by translators and interpreters, more so than the superficial differences produced by varying expressions. 3.2. Result The result of the similarity is shown in Figure 3. First, we focus on the relationship betwe"
2013.iwslt-papers.3,2011.iwslt-evaluation.18,0,0.0461329,"Missing"
2013.iwslt-papers.3,P11-2093,1,0.744327,"Missing"
2013.iwslt-papers.3,N12-1048,0,0.119056,"hen looking at the source and the translation, the word order is quite different, reversing two long clauses: A and B. In contrast, when looking at the source and the simultaneous interpretation, the word order is similar. If a simultaneous ST system attempts to reproduce the first word order, it will only be able to start translation after it has received the full “A because B.” On the other hand, if the system is able to choose the word order closer to human interpreters, it can begin translation after “A,” resulting in a lower delay. There are several related works about simultaneous ST [2][3][4] that automatically divide longer sentences up into a number of shorter ones similarly to the salami technique employed by simultaneous interpreters. While these related works aim to segment sentences in a similar fashion to simultaneous interpreters, all previous works concerned with sentence segmentation have used translation data (made by translators) for learning of the machine translation system. In addition, while there are other related works about collecting simultaneous interpretation data [5][6][7], all previous works did not compare simultaneous interpreters of multiple experienc"
2013.iwslt-papers.3,shimizu-etal-2014-collection,1,0.823939,"ranslation results closer to a highly experienced simultaneous interpreter than when translation data alone is used in training. We also find that according to automatic evaluation metrics, our system achieves performance similar to that of a simultaneous interpreter that has 1 year of experience. 2. Simultaneous interpretation data As the first step to performing our research, we first must collect simultaneous interpretation data. In this section, we describe how we did so with the cooperation of professional simultaneous interpreters. A fuller description of the corpus will be published in [8]. 2.1. Materials As materials for the simultaneous interpreters to translate, we used TED1 talks, and had the interpreters translate in real time from English to Japanese while watching and listening to the TED videos. We have several reasons for using TED talks. The first is that for many of the TED talks there are already Japanese subtitles available. This makes it possible to compare data created by translators (i.e. the subtitles) with simultaneous interpretation data. TED is also an attractive testbed for machine translation systems, as it covers a wide variety of topics of interest to a"
2013.iwslt-papers.3,P02-1040,0,0.0899235,"reters and translators. The underlined score is BLEU and the plain score is RIBES generated and checked by voluntary translators. For the two varieties of interpretation data, I1 and I2, we used the transcriptions of the interpretations performed by the S rank and A rank interpreter respectively. The first motivation for collecting this data is that it may allow us to quantitatively measure the similarity or difference between interpretations and translations automatically. In order to calculate the similarity between each of these pieces of data, we use the automatic similarity measures BLEU [9] and RIBES [10]. As BLEU and RIBES are not symmetric, we average BLEU or RIBES in both directions. For example, we calculate for BLEU using 1 {BLEU(R, H) + BLEU(H, R)} 2 (1) where R and H are the reference and the hypothesis. Based on this data, if the similarities of T1-T2 and I1-I2 are higher than T1-I1, T2-I1, T1-I2 and T2-I2, we can find that there are real differences between the output produced by translators and interpreters, more so than the superficial differences produced by varying expressions. 3.2. Result The result of the similarity is shown in Figure 3. First, we focus on the rel"
2013.iwslt-papers.3,ma-2006-champollion,0,0.0318601,"Missing"
2013.iwslt-papers.3,P07-2045,0,0.0252123,"Missing"
2013.iwslt-papers.3,J03-1002,0,0.00584698,"Missing"
2013.iwslt-papers.3,P03-1021,0,0.0154201,"Missing"
2013.iwslt-papers.8,stuker-etal-2012-kit,1,0.825719,"where new data for retraining comes from the same speaker, channel and related conversation topics. Following the implications of [8] we add low confidence score data to the training, but unlike in other work we apply wordbased weighting in order to compensate for errors, as it was done by [9] for acoustic model adaptation. The assumption is that erroneous data is helpful to improve system generalization. Unlike other work, e.g. [10], we refrained from a lattice-based approach. 2. Data The experiments in this paper were conducted with the help of the KIT Lecture Corpus for Speech Translation [11]. The corpus consists of recorded scientific lectures that were held at the Karlsruhe Institute of Technology (KIT). Currently the corpus mainly contains computer science lectures, and a small amount of lectures from other departments and ceremonial talks. 2.1. Training Data The speaker-independent system that we used in our experiments was trained on about 94 hours of speech from the lecture corpus. Our experiments were constrained to two distinct speakers. As training data we had 7.4 hours for speaker A and 8.3 hours for speaker B respectively, which had not been used for training the speake"
2015.iwslt-evaluation.17,H92-1073,0,\N,Missing
2015.iwslt-evaluation.17,rousseau-etal-2014-enhancing,0,\N,Missing
2020.acl-main.327,W05-0909,0,0.440604,"stitute human evaluation in machine translation development because it is low-cost, handy, and stable to use. Popular automatic MTE metrics such as BLEU (Papineni et al., 2002) calculate the evaluation score based on a surface-level similarity of a paired 1-to-1 reference and translated hypothesis sentences. BLEU particularly evaluates the sentence similarity with the ngram word matching rate between a reference and hypothesis. However, the evaluation score drops when a reference and hypothesis are dissimilar in the surface even if they share the same meaning. To counter this problem, METEOR (Banerjee and Lavie, 2005) is proposed to mitigate the word matching of synonyms with a synonym dictionary. Yet still, with mitigation of word matching, surfacelevel similarity cannot fully compensate for semantics, thus word representation instead of word symbols is used in Word Mover’s Distance (Kusner et al., 2015) and bleu2vec (T¨attar and Fishel, 2017). Besides, sentence representation is known to be an efficient feature instead of word representation because sentence vectors can represent more global meanings. RUSE (Shimanaka et al., 2018) and BERT (Devlin et al., 2019) based MTE, BERT regressor (Shimanaka et al."
2020.acl-main.327,W17-4755,0,0.173914,"l, with mitigation of word matching, surfacelevel similarity cannot fully compensate for semantics, thus word representation instead of word symbols is used in Word Mover’s Distance (Kusner et al., 2015) and bleu2vec (T¨attar and Fishel, 2017). Besides, sentence representation is known to be an efficient feature instead of word representation because sentence vectors can represent more global meanings. RUSE (Shimanaka et al., 2018) and BERT (Devlin et al., 2019) based MTE, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations as argued by Dreyer and Marcu (2012) and Qin and Specia (2015), but preparing such multiple references is costly. Hereby, we propose a method to incorporate source sentence into MTE as"
2020.acl-main.327,D18-2029,0,0.0753115,"Missing"
2020.acl-main.327,D17-1070,0,0.0508202,"et al. (2018) proposed an MTE framework called RUSE (Regressor Using Sentence Embeddings), which uses sentence3553 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3553–3558 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Table 1: Available corpus size annotated with human judgments in WMT-2017 Metrics Shared Task (to-English) WMT-2015 WMT-2016 WMT-2017 ALL cs-en 500 560 560 1620 de-en 500 560 560 1620 fi-en 500 560 560 1620 lv-en 560 560 ro-en 560 560 level embeddings obtained by a large-scale pretrained model like InferSent (Conneau et al., 2017), Quick Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018). Its regressor takes sentence vectors for a reference and translation hypothesis as inputs and returns a score, which is trained to correlate well with human evaluation (Graham et al., 2015). RUSE achieved the best correlation score with human judgments in the WMT-2017 Metrics Shared Task (Bojar et al., 2017). BERT regressor (Shimanaka et al., 2019) is a simple MTE metric based on BERT (Devlin et al., 2019) encoder. It is composed of BERT encoder and a multi-layer perceptron (MLP) regressor attached t"
2020.acl-main.327,P02-1040,0,0.111256,"at our proposed method using Crosslingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance. 1 Introduction Automatic machine translation evaluation (MTE) has been studied to substitute human evaluation in machine translation development because it is low-cost, handy, and stable to use. Popular automatic MTE metrics such as BLEU (Papineni et al., 2002) calculate the evaluation score based on a surface-level similarity of a paired 1-to-1 reference and translated hypothesis sentences. BLEU particularly evaluates the sentence similarity with the ngram word matching rate between a reference and hypothesis. However, the evaluation score drops when a reference and hypothesis are dissimilar in the surface even if they share the same meaning. To counter this problem, METEOR (Banerjee and Lavie, 2005) is proposed to mitigate the word matching of synonyms with a synonym dictionary. Yet still, with mitigation of word matching, surfacelevel similarity"
2020.acl-main.327,W15-4915,0,0.35177,"E, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations as argued by Dreyer and Marcu (2012) and Qin and Specia (2015), but preparing such multiple references is costly. Hereby, we propose a method to incorporate source sentence into MTE as another pseudo reference, since the source and reference sentences should be semantically equivalent. The proposed method uses Cross-lingual Language Model (XLM) (Lample and Conneau, 2019) to handle source and target languages in a shared sentence embedding space. The proposed method with XLM trained with a translation language modeling (TLM) objective showed a higher correlation with human judgments than a baseline method using hypothesis and reference sentences. 2 Relate"
2020.acl-main.327,D18-1269,0,0.022276,"o encode both source and target language sentences into an embedding vector. XLM has three additional techniques to BERT: language independent subword based on Byte Pair Encoding (Sennrich et al., 2016), a language embedding layer, and a translation language modeling (TLM) objective that predicts masked words from surrounding words or a paired translation. The brief architecture of XLM is shown in Figure 1. (Lample and Conneau, 2019) reported that XLM trained with TLM objective obtains better performance than multilingual BERT(Devlin et al., 2019) on the XNLI cross-lingual classification task(Conneau et al., 2018). The experiments were conducted with a corpus of all language pairs to English translation from segment-level WMT2017 Metrics Shared Task (Bojar et al., 2017). We split sentences in WMT15 and WMT16 to training and development data with the ratio of 9:1 and whole sentences in WMT17 are used for evaluation of MTE methods. The corpus size for each language pair is shown in Table 1. We used two different models from all available XLM family models2 : XLM15 pretrained by MLM and TLM, and XLM100 pretrained only by MLM. XLM15 is expected to perform better by the paired bilingual training of TLM, but"
2020.acl-main.327,P16-1162,0,0.0127871,"arned to represent the quality of the translation hypothesis given two correct sentences aligned aside. 4 Experiments We conducted experiments to evaluate the performance of the proposed method in MTE by comparing with some existing methods. 4.1 Setting We propose an MTE method using source language sentences as additional pseudo references. We use cross-lingual language models called XLM (Lample and Conneau, 2019) to encode both source and target language sentences into an embedding vector. XLM has three additional techniques to BERT: language independent subword based on Byte Pair Encoding (Sennrich et al., 2016), a language embedding layer, and a translation language modeling (TLM) objective that predicts masked words from surrounding words or a paired translation. The brief architecture of XLM is shown in Figure 1. (Lample and Conneau, 2019) reported that XLM trained with TLM objective obtains better performance than multilingual BERT(Devlin et al., 2019) on the XNLI cross-lingual classification task(Conneau et al., 2018). The experiments were conducted with a corpus of all language pairs to English translation from segment-level WMT2017 Metrics Shared Task (Bojar et al., 2017). We split sentences i"
2020.acl-main.327,N19-1423,0,0.532221,"ning. To counter this problem, METEOR (Banerjee and Lavie, 2005) is proposed to mitigate the word matching of synonyms with a synonym dictionary. Yet still, with mitigation of word matching, surfacelevel similarity cannot fully compensate for semantics, thus word representation instead of word symbols is used in Word Mover’s Distance (Kusner et al., 2015) and bleu2vec (T¨attar and Fishel, 2017). Besides, sentence representation is known to be an efficient feature instead of word representation because sentence vectors can represent more global meanings. RUSE (Shimanaka et al., 2018) and BERT (Devlin et al., 2019) based MTE, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations as argued by Dreyer and Marcu (2"
2020.acl-main.327,W18-6456,0,0.350446,"ce even if they share the same meaning. To counter this problem, METEOR (Banerjee and Lavie, 2005) is proposed to mitigate the word matching of synonyms with a synonym dictionary. Yet still, with mitigation of word matching, surfacelevel similarity cannot fully compensate for semantics, thus word representation instead of word symbols is used in Word Mover’s Distance (Kusner et al., 2015) and bleu2vec (T¨attar and Fishel, 2017). Besides, sentence representation is known to be an efficient feature instead of word representation because sentence vectors can represent more global meanings. RUSE (Shimanaka et al., 2018) and BERT (Devlin et al., 2019) based MTE, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations a"
2020.acl-main.327,N12-1017,0,0.0185447,"evlin et al., 2019) based MTE, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations as argued by Dreyer and Marcu (2012) and Qin and Specia (2015), but preparing such multiple references is costly. Hereby, we propose a method to incorporate source sentence into MTE as another pseudo reference, since the source and reference sentences should be semantically equivalent. The proposed method uses Cross-lingual Language Model (XLM) (Lample and Conneau, 2019) to handle source and target languages in a shared sentence embedding space. The proposed method with XLM trained with a translation language modeling (TLM) objective showed a higher correlation with human judgments than a baseline method using hypothesis and ref"
2020.acl-main.327,N15-1124,0,0.0273411,"ics Table 1: Available corpus size annotated with human judgments in WMT-2017 Metrics Shared Task (to-English) WMT-2015 WMT-2016 WMT-2017 ALL cs-en 500 560 560 1620 de-en 500 560 560 1620 fi-en 500 560 560 1620 lv-en 560 560 ro-en 560 560 level embeddings obtained by a large-scale pretrained model like InferSent (Conneau et al., 2017), Quick Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018). Its regressor takes sentence vectors for a reference and translation hypothesis as inputs and returns a score, which is trained to correlate well with human evaluation (Graham et al., 2015). RUSE achieved the best correlation score with human judgments in the WMT-2017 Metrics Shared Task (Bojar et al., 2017). BERT regressor (Shimanaka et al., 2019) is a simple MTE metric based on BERT (Devlin et al., 2019) encoder. It is composed of BERT encoder and a multi-layer perceptron (MLP) regressor attached to the last layer of BERT. This BERT encoder is a 12 layers bi-directional language model, referring to BERTbase (uncased)1 , trained with masked language model (MLM) and next sentence prediction (NSP). BERT regressor surpassed RUSE on the WMT-2017 data. 3 Proposed method: Automatic e"
2020.acl-main.327,W17-4771,0,0.0354707,"Missing"
2020.acl-srw.8,Q17-1010,0,0.0505016,"explicit knowledge of the given words in its prediction. The contribution of this work is two-fold: (1) We propose a word attribute transfer method that obtains a vector with an inverted binary attribute without explicit knowledge. (2) The proposed method demonstrates more accurate word attribute transfer for words that have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can be used to transfer a certain attribute of a word, such as changing king into quee"
2020.acl-srw.8,P19-1160,0,0.0274677,"Missing"
2020.acl-srw.8,P19-1601,0,0.0149893,"6.9 29.5 0.0 26.7 29.7 0.0 33.5 36.7 0.0 25.7 36.6 100.0 100.0 0.1 100.0 100.0 0.5 100.0 100.0 1.2 100.0 100.0 4.6 5.5 The theory of analogic relations in word embeddings has been widely discussed (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019; Linzen, 2016). In our work, we focus on the analogic relations in a word embedding space and propose a novel framework to obtain a word vector with inverted attributes. The style transfer task (Niu et al., 2018; Prabhumoye et al., 2018; Logeswaran et al., 2018; Jain et al., 2019; Dai et al., 2019; Lample et al., 2019) resembles ours. In style transfer, the text style of the input sentences is changed. For instance, Jain et al. (2019) transferred from formal to informal sentences. These style transfer tasks use sentence pairs; our word attribute transfer task uses word pairs. Style transfer changes sentence styles, but our task changes the word attributes. Soricut and Och (2015) studied morphological transformation based on character information. Our work aims for more general attribute transfer, such as gender transfer and antonym, and is not limited to morphological transformation. S"
2020.acl-srw.8,P19-1315,0,0.0699055,"hat have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can be used to transfer a certain attribute of a word, such as changing king into queen by transferring its gender. This transfer can be applied to perform data augmentation; for example, rewriting He is a boy to She is a girl. It can be used to generate negative examples for natural language inference, for example. We tackle a novel 2 Word Attribute Transfer Task In this task, we focus on modeling t"
2020.acl-srw.8,W14-1618,0,0.437667,"ethod demonstrates more accurate word attribute transfer for words that have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can be used to transfer a certain attribute of a word, such as changing king into queen by transferring its gender. This transfer can be applied to perform data augmentation; for example, rewriting He is a boy to She is a girl. It can be used to generate negative examples for natural language inference, for example. We tackle a novel"
2020.acl-srw.8,P17-1007,0,0.0651237,"e transfer for words that have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can be used to transfer a certain attribute of a word, such as changing king into queen by transferring its gender. This transfer can be applied to perform data augmentation; for example, rewriting He is a boy to She is a girl. It can be used to generate negative examples for natural language inference, for example. We tackle a novel 2 Word Attribute Transfer Task In this tas"
2020.acl-srw.8,N16-2002,0,0.0129596,"r datasets (in number of word pairs) Val X where V is the vocabulary of the word embedding model and cos(vy , vk ) is the cosine similarity meav ·v sure, defined as: cos(vy , vk ) = kvyykkvkk k . We evaluated the performance of the word attribute transfer using data with four different attributes. We used 300-dimensional word2vec and GloVe as the pre-trained word embedding. We used four different datasets of word pairs with four binary attributes: Male-Female, Singular-Plural, CapitalCountry, and Antonym (Table 1). These word pairs were collected from analogy test sets (Mikolov et al., 2013a; Gladkova et al., 2016) and the Internet. Noun antonyms were taken from the literature (Nguyen et al., 2017). For non-attribute dataset N , we sampled words from the vocabulary of word embedding. We sampled from 4 to 50 words for training and 1000 for the test (|Ntest |= 1000). Train |Atest | Stability = Experiment Dataset A 1 M EAN D IFF Analogy-based word attribute trans¯ d ¯ = fer withPa mean difference vector d: 1 (mi ,wi ,z)∈Atrain (vmi −vwi ). We de|Atrain | ¯ to vx termined whether to add or subtract d based on the explicit knowledge (Eq. 4). For proposed methods, we used the Adam optimizer (Kingma and Ba, 20"
2020.acl-srw.8,W16-2503,0,0.0720254,"ain the following formulas: (1) ∀(m, w, z) ∈ A, vm = fz ( fz (vm ) ), (8) ∀(m, w, z) ∈ A, vw = fz ( fz (vw ) ), (9) ∀(u, z) ∈ N , vu = fz ( fz (vu ) ). (10) Hence, the ideal transfer function is a mapping that becomes an identity mapping when we apply it twice for any v. Such a mapping is called involution in geometry. For example, f : v 7→ −v is one example of an involution. 4.2 Analogy is a general idea that can be used for word attribute transfer. PMI-based word embedding, such as SGNS and GloVe, captures analogic relations, including Eq. 2 (Mikolov et al., 2013c; Levy and Goldberg, 2014a; Linzen, 2016). By rearranging Eq. 2, Eq. 3 is obtained: Reflection Reflection Ref a,c is an ideal function because this mapping is an involution: ∀v ∈ Rn , v = Ref a,c ( Ref a,c (v) ). (11) Reflection reverses the location between two vectors in a Euclidean space through an hyperplane called a mirror. Reflection is different from inverse mapping. When m and w are paired words, reflection can transfer vm and vw each other with identical reflection mapping as in Eqs. 5 and 6, but an inverse mapping cannot. Given vector v in Euclidean space Rn , the formula for the reflection in the mirror is given: (2) ≈ vki"
2020.acl-srw.8,N13-1090,0,0.742739,"work for a word attribute transfer based on reflection that does not require explicit knowledge of the given words in its prediction. The contribution of this work is two-fold: (1) We propose a word attribute transfer method that obtains a vector with an inverted binary attribute without explicit knowledge. (2) The proposed method demonstrates more accurate word attribute transfer for words that have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can b"
2020.acl-srw.8,D18-1521,0,0.0198579,"= MLPθ1 ([z; vx ]), actress heroine Figure 3: Reflection with parameterized mirrors Reflection with a mirror by Eqs. 13 and 14 assumes a single mirror that only depends on z. Previous discussion assumed pairs that share a stable pair, such as king and queen. However, since gendered words often do not come in pairs, gender is not stable enough to be modeled by a single mirror. For example, although actress is exclusively feminine, actor is clearly neutral in many cases. Thus, actor is not obviously a masculine counterpart like king. In fact, bias exists in gender words in the embedding space (Zhao et al., 2018; Kaneko and Bollegala, 2019). This phenomenon can occur not only with gender attributes but also with other attributes. The single mirror assumption forces the The following property must be satisfied in word attribute transfer: (1) words with attribute z are transferred and (2) words without it are not transferred. Thus, loss L(θ1 , θ2 ) is defined: L(θ1 , θ2 ) = 1 |A| 1 + |N | 53 X (vy − vt )2 (18) (vy − vx )2 , (19) (x,t,z)∈A X (x,z)∈N where Eq. 18 is a term that draws target word vector vti closer to corresponding transferred vector vyi and Eq. 19 is a term that prevents words without a t"
2020.acl-srw.8,C18-1086,0,0.0392076,"Missing"
2020.acl-srw.8,D14-1162,0,0.0779148,"Missing"
2020.acl-srw.8,P18-1080,0,0.0292195,"0.0 100.0 0.4 100.0 99.9 1.0 100.0 99.9 5.2 AN R EF R EF+PM MLP 0.0 26.9 29.5 0.0 26.7 29.7 0.0 33.5 36.7 0.0 25.7 36.6 100.0 100.0 0.1 100.0 100.0 0.5 100.0 100.0 1.2 100.0 100.0 4.6 5.5 The theory of analogic relations in word embeddings has been widely discussed (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019; Linzen, 2016). In our work, we focus on the analogic relations in a word embedding space and propose a novel framework to obtain a word vector with inverted attributes. The style transfer task (Niu et al., 2018; Prabhumoye et al., 2018; Logeswaran et al., 2018; Jain et al., 2019; Dai et al., 2019; Lample et al., 2019) resembles ours. In style transfer, the text style of the input sentences is changed. For instance, Jain et al. (2019) transferred from formal to informal sentences. These style transfer tasks use sentence pairs; our word attribute transfer task uses word pairs. Style transfer changes sentence styles, but our task changes the word attributes. Soricut and Och (2015) studied morphological transformation based on character information. Our work aims for more general attribute transfer, such as gender transfer and"
2020.acl-srw.8,N15-1186,0,0.0479293,"Missing"
2020.coling-main.234,N03-2003,0,0.0401264,"been applied to solve this problem, which generates pseudo training samples (Hou et al., 2018; Yoo et al., 2019). However, such methods often generate unnatural training samples that will decrease SLU accuracy. Another problem is the ambiguity of user utterances; it is difficult to generate such ambiguous examples with generative approaches. Some existing work tackled this problem by using paraphrasing models (Saha et al., 2018; Ray et al., 2018). On the other hand, collecting text data from the Web is a widely used approach to building language models of automatic speech recognition systems (Bulyko et al., 2003; Sarikaya et al., 2005; Ng et al., 2005; Tsiartas et al., 2010). Web texts are expected to be more natural than generated pseudo sentences because users handcraft most of them. However, Web texts contain diverse domain texts; thus, we need some criteria to select appropriate texts to be used for the augmented training data. Test-set perplexity (Misu and Kawahara, 2006) or semantic similarity (Hakkani-Tur and Rahim, 2006; Yoshino et al., 2013) were widely used as criteria to select appropriate sentences for the training data augmentation. Such a selective approach using large-scale Web data ha"
2020.coling-main.234,P16-2006,0,0.0261972,"dsourcing (J is the number of queries assigned to intent fi ). We calculate similarities between each qi,j and ck , which is a question sentence extracted from the knowledge community website, for finding similar sentence cˆk to qi,j . cˆk which will be assigned as an agumented training sample of fi . Converting sentences to vector representations is an common approach to calculate similarities: vector space model (Salton et al., 1975), means of distributed representation of words (Mikolov et al., 2013; Le and Mikolov, 2014) and bi-directional long short-term memory neural networks (Bi-LSTM) (Cross and Huang, 2016; Yang et al., 2019). Recently, Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is known as a better sentence encoder, which is based on masked word prediction in surrounding sentences. We used the BERT model trained using Japanese Wikipedia (Sakata et al., 2019) on the task of masked word prediction because we would like to extract semantically similar sentences to seed queries. The task of masked word prediction is based on the distributional hypothesis (Harris, 1954); thus, the resultant model trained in the task can embed semantically similar sentences"
2020.coling-main.234,H94-1010,0,0.686349,"ebsite. We investigated the effects of the proposed data augmentation method in SLU task, even with small seed data. In particular, the proposed architecture augmented more than 120,000 samples to improve SLU accuracies. 1 Introduction Recent advances in speech applications running on smartphones and smart speakers increase the importance of spoken language understanding (SLU). SLU is a task to predict an appropriate system function with its arguments, given a user request written or spoken in natural language. Various SLU benchmarks have been proposed: Air Travel Information Services (ATIS) (Dahl et al., 1994), restaurant information navigation (Williams et al., 2014), and other speech applications (Hori et al., 2019). Adaptation of SLU to newly defined tasks is an important problem (Henderson et al., 2014). The number of training data directly affects the SLU accuracy because most of the recent SLU systems are based on statistical machine learning approaches. Some existing work tackled this problem based on transfer learning (Wu et al., 2019), which uses a pre-trained model on different domain data. However, it is still challenging to make the SLU accurate with no or fewer data. Data augmentation"
2020.coling-main.234,N19-1423,0,0.0173428,"hich is a question sentence extracted from the knowledge community website, for finding similar sentence cˆk to qi,j . cˆk which will be assigned as an agumented training sample of fi . Converting sentences to vector representations is an common approach to calculate similarities: vector space model (Salton et al., 1975), means of distributed representation of words (Mikolov et al., 2013; Le and Mikolov, 2014) and bi-directional long short-term memory neural networks (Bi-LSTM) (Cross and Huang, 2016; Yang et al., 2019). Recently, Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is known as a better sentence encoder, which is based on masked word prediction in surrounding sentences. We used the BERT model trained using Japanese Wikipedia (Sakata et al., 2019) on the task of masked word prediction because we would like to extract semantically similar sentences to seed queries. The task of masked word prediction is based on the distributional hypothesis (Harris, 1954); thus, the resultant model trained in the task can embed semantically similar sentences into close points on the latent space. We note the vector of sentence qi,j as qi,j . Because both vectors qi,j and c"
2020.coling-main.234,W18-5708,0,0.0276546,"10). Web texts are expected to be more natural than generated pseudo sentences because users handcraft most of them. However, Web texts contain diverse domain texts; thus, we need some criteria to select appropriate texts to be used for the augmented training data. Test-set perplexity (Misu and Kawahara, 2006) or semantic similarity (Hakkani-Tur and Rahim, 2006; Yoshino et al., 2013) were widely used as criteria to select appropriate sentences for the training data augmentation. Such a selective approach using large-scale Web data has been applied to the data augmentation of dialogue systems (Du and Black, 2018; Henderson et al., 2019). This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/. Licence details: http: 2606 Proceedings of the 28th International Conference on Computational Linguistics, pages 2606–2612 Barcelona, Spain (Online), December 8-13, 2020 Crowdsourcing is a common way to collect human-annotated data at low-cost (Zhao et al., 2011; Mozafari et al., 2014). However, accurate SLU systems based on neural networks require a large-scale dataset. It is not easy to collect sufficient amount of training data only using c"
2020.coling-main.234,C18-1105,0,0.0209126,"lications (Hori et al., 2019). Adaptation of SLU to newly defined tasks is an important problem (Henderson et al., 2014). The number of training data directly affects the SLU accuracy because most of the recent SLU systems are based on statistical machine learning approaches. Some existing work tackled this problem based on transfer learning (Wu et al., 2019), which uses a pre-trained model on different domain data. However, it is still challenging to make the SLU accurate with no or fewer data. Data augmentation has been applied to solve this problem, which generates pseudo training samples (Hou et al., 2018; Yoo et al., 2019). However, such methods often generate unnatural training samples that will decrease SLU accuracy. Another problem is the ambiguity of user utterances; it is difficult to generate such ambiguous examples with generative approaches. Some existing work tackled this problem by using paraphrasing models (Saha et al., 2018; Ray et al., 2018). On the other hand, collecting text data from the Web is a widely used approach to building language models of automatic speech recognition systems (Bulyko et al., 2003; Sarikaya et al., 2005; Ng et al., 2005; Tsiartas et al., 2010). Web text"
2020.coling-main.234,P18-1156,0,0.0128664,"arning (Wu et al., 2019), which uses a pre-trained model on different domain data. However, it is still challenging to make the SLU accurate with no or fewer data. Data augmentation has been applied to solve this problem, which generates pseudo training samples (Hou et al., 2018; Yoo et al., 2019). However, such methods often generate unnatural training samples that will decrease SLU accuracy. Another problem is the ambiguity of user utterances; it is difficult to generate such ambiguous examples with generative approaches. Some existing work tackled this problem by using paraphrasing models (Saha et al., 2018; Ray et al., 2018). On the other hand, collecting text data from the Web is a widely used approach to building language models of automatic speech recognition systems (Bulyko et al., 2003; Sarikaya et al., 2005; Ng et al., 2005; Tsiartas et al., 2010). Web texts are expected to be more natural than generated pseudo sentences because users handcraft most of them. However, Web texts contain diverse domain texts; thus, we need some criteria to select appropriate texts to be used for the augmented training data. Test-set perplexity (Misu and Kawahara, 2006) or semantic similarity (Hakkani-Tur and"
2020.coling-main.234,P19-1078,0,0.0171223,"s arguments, given a user request written or spoken in natural language. Various SLU benchmarks have been proposed: Air Travel Information Services (ATIS) (Dahl et al., 1994), restaurant information navigation (Williams et al., 2014), and other speech applications (Hori et al., 2019). Adaptation of SLU to newly defined tasks is an important problem (Henderson et al., 2014). The number of training data directly affects the SLU accuracy because most of the recent SLU systems are based on statistical machine learning approaches. Some existing work tackled this problem based on transfer learning (Wu et al., 2019), which uses a pre-trained model on different domain data. However, it is still challenging to make the SLU accurate with no or fewer data. Data augmentation has been applied to solve this problem, which generates pseudo training samples (Hou et al., 2018; Yoo et al., 2019). However, such methods often generate unnatural training samples that will decrease SLU accuracy. Another problem is the ambiguity of user utterances; it is difficult to generate such ambiguous examples with generative approaches. Some existing work tackled this problem by using paraphrasing models (Saha et al., 2018; Ray e"
2020.coling-main.319,P17-4012,0,0.0407213,"ediction We need length estimates when we use LRPE and LDPE in inferences. Instead of using the input lengths like Lakew et al. (2019), we propose using an output length prediction based on a pre-trained BERT model in the source language. We used the [CLS] vector in the last layer of the BERT encoder to predict the output length through an output layer as a regression problem. 4 Experiments To investigate the performance of our proposed method, we conducted English-to-Japanese translation experiments between a vanilla Transformer and its variants with LRPE and LDPE, implemented using OpenNMT (Klein et al., 2017). 4.1 Setup Datasets We used the Japanese-English portion of the ASPEC corpus (Nakazawa et al., 2016), which consists of 3 million parallel sentences for training, 1,790 sentences for development, 1,784 sentences for the devtest, and 1,812 sentences for the test. All the sentences were tokenized into subwords using a SentencePiece model (Kudo and Richardson, 2018) with a shared subword vocabulary of 16,000 entries, which were trained with 2M English and Japanese sentences that included the first set of the training sentences pairs (train-1). Throughout the experiments, we used subword-based le"
2020.coling-main.319,D18-2012,0,0.01921,"regression problem. 4 Experiments To investigate the performance of our proposed method, we conducted English-to-Japanese translation experiments between a vanilla Transformer and its variants with LRPE and LDPE, implemented using OpenNMT (Klein et al., 2017). 4.1 Setup Datasets We used the Japanese-English portion of the ASPEC corpus (Nakazawa et al., 2016), which consists of 3 million parallel sentences for training, 1,790 sentences for development, 1,784 sentences for the devtest, and 1,812 sentences for the test. All the sentences were tokenized into subwords using a SentencePiece model (Kudo and Richardson, 2018) with a shared subword vocabulary of 16,000 entries, which were trained with 2M English and Japanese sentences that included the first set of the training sentences pairs (train-1). Throughout the experiments, we used subword-based lengths. 3581 Hyperparameters Our hyperparameter settings came from OpenNMT-py FAQ1 and are used commonly for all the compared methods described later in this section. We conducted five independent training runs with different random seeds and chose the best runs and training epochs in the devtest set to determine the models for the final evaluation. Evaluation We u"
2020.coling-main.319,P02-1040,0,0.115462,"ared subword vocabulary of 16,000 entries, which were trained with 2M English and Japanese sentences that included the first set of the training sentences pairs (train-1). Throughout the experiments, we used subword-based lengths. 3581 Hyperparameters Our hyperparameter settings came from OpenNMT-py FAQ1 and are used commonly for all the compared methods described later in this section. We conducted five independent training runs with different random seeds and chose the best runs and training epochs in the devtest set to determine the models for the final evaluation. Evaluation We used BLEU (Papineni et al., 2002) for our evaluation metric given by multi-bleu.perl and also investigated the length ratio (LR) of the input and output sentences (LR = tgt len/ref len). BLEU was calculated on translation results re-tokenized by MeCab (Kudo, 2005) after merging the subwords. We also calculated the variance of the length difference between the translation results and the references (VAR) to investigate the effects of the output length constraints, following Takase and Okazaki (2019). The variance of the length differences on a test set consisting of n sentences is given by: V AR = 4.2 n 1X |li − ref leni |2 n"
2020.coling-main.319,N19-1401,0,0.220731,"slation (NMT), a decoder generates one token at a time, and each output token depends on the output tokens generated so far. The decoder’s prediction of the end of the sentence determines the length of the output sentence. This prediction is sometimes made too early– before all of the input information is translated–causing a so-called under-translation. Transformer has sinusoidal positional encoding to incorporate the token position information in the sequence into its encoder and decoder (Vaswani et al., 2017). There are some previous studies for controlling an output length in Transformer. Takase and Okazaki (2019) proposed two variants of length-aware positional encodings called length-ratio positional encoding (LRPE) and length-difference positional encoding (LDPE) to control the output length based on the given length constraints in automatic summarization. Lakew et al. (2019) applied LDPE and LRPE to NMT. They trained an NMT model using output length constraints based on LDPE and LRPE along with special tokens representing length ratio classes between input and output sentences, while they used the input sentence length at the inference time. However, the length of an input sentence is not a reliabl"
2020.iwslt-1.21,D19-1166,0,0.0202838,"res for Disfluent Spanish to Fluent English. NMT models used Fisher’s disfluent references for training. System Fisher UNCorpus + Fisher Fisher-like UNCorpus + Fisher 3.3.2 As shown above, some generated sentences lost the meaning of the sentence due to missing phrases. As a result, the quality of the parallel data decreased and the final translation performance was also degraded. One of the causes of this problem is style transfer constraints are too strong. Thus, it may be mitigated by a model that could control the tradeoff between style transfer and content preservation (Niu et al., 2017; Agrawal and Carpuat, 2019; Lample et al., 2019). Further improvement can be expected by preventing changes in the meaning of sentences and converting only the style. Fisher/Test 11.6 15.2 15.6 Results Tables 3 and 4 show the BLEU scores of the systems evaluated with single fluent references. In Table 3, “Fisher”, “UNCorpus” and “Fisher-like UNCorpus” are models trained on a single training data. “UNCorpus + Fisher” and “Fisher-like UNCorpus + Fisher” are models that were pre-trained on UNCorpus and Fisher-like UNCorpus and then fine-tuned on Fisher/Train, respectively. The models in Table 4 did not use Fisher’s fluent"
2020.iwslt-1.21,D18-1549,0,0.0125757,"m disfluent Spanish to fluent English, as illustrated in Figure 1. First, we transferred fluent Spanish in out-of-domain data into disfluent Spanish (Section 2.1). Then we trained the NMT model leveraging both out-of-domain parallel data as well as in-domain parallel data (Section 2.2). then fine-tuned on true in-domain data. 2.1 3 Unsupervised Style Transfer We employed an unsupervised learning method for the style transfer of Spanish of out-of-domain data. This is because there is no parallel corpus of fluent and disfluent Spanish and it is not possible to adapt supervised learning methods. Artetxe et al. (2018); Lample et al. (2018a,b) proposed Unsupervised Neural Machine Translation (UNMT) that learns the translation using monolingual corpora of two languages. In this system, we built a fluent-todisfluent style transfer model based on UNMT with out-of-domain fluent data and in-domain disfluent data. 2.2 Fisher/Train Dev Test UNCorpus/Train Dev Test # sentences 138,720 3,977 3,641 1,000,000 4,000 4,000 Results 3.1 Datasets We used the LDC Fisher Spanish speech (disfluent) with new English translations (fluent) (Post et al., 2013; Salesky et al., 2018) as parallel in-domain data and the United Nation"
2020.iwslt-1.21,D11-1033,0,0.0517251,"d the decoder. We chose Adam (Kingma and Ba, 2014) with a learning rate of 0.0001, β1 = 0.9, β2 = 0.999 as the optimizer. Each mini-batch contained 16 sentences. In order to gain robustness to the content of the sentence, we first pre-trained the model using only UNCorpus/Train. During pre-training, early stopping was applied on the BLEU score between source sentences and back-translated sentences of the UNCorpus/Dev with a patience of 10 iterations, and the model with the highest score was stored. After that, additional training of 1 iteration using the Fisher/Train was performed. Evaluation Axelrod et al. (2011) used a language model of in-domain data for out-of-domain data selection in domain adaptation. Following this study, we estimated the similarity between domains by measuring the perplexity (P P L) of the training set W of the out-of-domain data using a 3-gram language model M made from the in-domain data (Equation 1). P P L = 10H(W |M ) (1) H(W |M ) is the entropy, defined as the average of the negative log-likelihood per token, as shown in the following equation: H(W |M ) = 1 1 X − log10 P (s|M ) |W |s∈W Table 2: Perplexity and the number of unknown words (# UNK) for Fisher/train in the 3-gr"
2020.iwslt-1.21,P17-2061,0,0.0491093,"Missing"
2020.iwslt-1.21,C18-1111,0,0.0174526,"tween in-domain and out-of-domain data affects the translation accuracy significantly (Koehn and Knowles, 2017). A domain can be defined by any property of the training data such as topic and style. We expect that the domain similarity comes from these properties. Introduction Neural Machine Translation (NMT) has significantly improved the quality of Machine Translation (MT) (Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). However, domain-specific translation is still difficult in low-resource scenarios, although high performance can be achieved in resource-rich scenarios (Chu and Wang, 2018). Another major problem is the difficulty in translating noisy input sentences including filler, hesitation, etc. Belinkov and Bisk (2017) suggests the difficulty in learning to translate noisy sentences compared to clean ones. The translation of noisy sentences is very important for spoken language translation. In the IWSLT 2020 Conversational Speech Translation Task, we are going to tackle these two problems. The task includes speech-to-text and textto-text translation from disfluent Spanish speeches/transcripts to fluent English text. We chose the text-to-text subtask for our challenge task"
2020.iwslt-1.21,W17-3204,0,0.0919082,"onal speech translation task. We focus on the translation disfluent speech transcripts that include ASR errors and non-grammatical utterances. We tried a domain adaptation method by transferring the styles of out-of-domain data (United Nations Parallel Corpus) to be like in-domain data (Fisher transcripts). Our system results showed that the NMT model with domain adaptation outperformed a baseline. In addition, slight improvement by the style transfer was observed. 1 In domain adaptation, the “similarity” between in-domain and out-of-domain data affects the translation accuracy significantly (Koehn and Knowles, 2017). A domain can be defined by any property of the training data such as topic and style. We expect that the domain similarity comes from these properties. Introduction Neural Machine Translation (NMT) has significantly improved the quality of Machine Translation (MT) (Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). However, domain-specific translation is still difficult in low-resource scenarios, although high performance can be achieved in resource-rich scenarios (Chu and Wang, 2018). Another major problem is the difficulty in translating noisy input sentences including fil"
2020.iwslt-1.21,D15-1166,0,0.14652,"Missing"
2020.iwslt-1.21,D17-1299,0,0.0623377,"Missing"
2020.iwslt-1.21,P02-1040,0,0.106503,"ng and punctuation removal were applied only to the source language. Model We used OpenNMT-py3 . The NMT model was based on Transformer. The hyperparameters of the model almost follow the transformer base settings (Vaswani et al., 2017). Note that in the Fisher-only experiment without domain adaptation, the batch size was halved to 2048 tokens. The model was trained for 20,000 iterations using out-of-domain data, and then fine-tuned for 1,000 iterations using in-domain data. The model parameters saved every 100 iterations. Evaluation To evaluate the performance, we calculated the BLEU scores (Papineni et al., 2002) with sacreBLEU4 . (2) 2 http://www.speech.sri.com/projects/srilm/ https://github.com/OpenNMT/OpenNMT-py 4 https://github.com/mjpost/sacreBLEU 3 https://github.com/facebookresearch/UnsupervisedMT 174 Table 3: BLEU scores of trained NMT models for Disfluent Spanish to Fluent English. System Fisher UNCorpus Fisher-like UNCorpus UNCorpus + Fisher Fisher-like UNCorpus + Fisher Fisher/Test 14.8 7.8 6.7 18.3 18.5 Table 4: BLEU scores for Disfluent Spanish to Fluent English. NMT models used Fisher’s disfluent references for training. System Fisher UNCorpus + Fisher Fisher-like UNCorpus + Fisher 3.3.2"
2020.iwslt-1.21,2013.iwslt-papers.14,0,0.0279375,"panish and it is not possible to adapt supervised learning methods. Artetxe et al. (2018); Lample et al. (2018a,b) proposed Unsupervised Neural Machine Translation (UNMT) that learns the translation using monolingual corpora of two languages. In this system, we built a fluent-todisfluent style transfer model based on UNMT with out-of-domain fluent data and in-domain disfluent data. 2.2 Fisher/Train Dev Test UNCorpus/Train Dev Test # sentences 138,720 3,977 3,641 1,000,000 4,000 4,000 Results 3.1 Datasets We used the LDC Fisher Spanish speech (disfluent) with new English translations (fluent) (Post et al., 2013; Salesky et al., 2018) as parallel in-domain data and the United Nations Parallel Corpus (UNCorpus) (Ziemski et al., 2016) as parallel out-ofdomain data. Fisher has the following multi-way parallel data distributed by the task organizer: 1. Spanish disfluent speech 2. Spanish disfluent transcripts (gold) Domain Adaptation 3. Spanish disfluent transcripts (ASR output) For the challenge task, we apply fine-tuning, which is one of the conventional domain adaptation methods of MT (Sennrich et al., 2016a). The fine-tuning can result in significant improvements compared to both only in-domain train"
2020.iwslt-1.21,P16-1009,0,0.0177075,"Datasets We used the LDC Fisher Spanish speech (disfluent) with new English translations (fluent) (Post et al., 2013; Salesky et al., 2018) as parallel in-domain data and the United Nations Parallel Corpus (UNCorpus) (Ziemski et al., 2016) as parallel out-ofdomain data. Fisher has the following multi-way parallel data distributed by the task organizer: 1. Spanish disfluent speech 2. Spanish disfluent transcripts (gold) Domain Adaptation 3. Spanish disfluent transcripts (ASR output) For the challenge task, we apply fine-tuning, which is one of the conventional domain adaptation methods of MT (Sennrich et al., 2016a). The fine-tuning can result in significant improvements compared to both only in-domain training or only out-ofdomain training (Dakwale and Monz, 2017). In this method, an NMT is pre-trained on a resource rich out-of-domain data until convergence, and then its parameters are fine-tuned on a low-resource indomain data. In this study, we pre-trained the NMT model on the pseudo in-domain data generated in 2.1, and 4. English disfluent translations 5. English fluent translations When training, we used (3) as input and (4) or (5) as output. UNCorpus consists of manually translated UN documents o"
2020.iwslt-1.21,P16-1162,0,0.0253932,"Missing"
2020.iwslt-1.21,C16-1295,0,0.0524319,"Missing"
2020.iwslt-1.21,L16-1561,0,0.0134775,"osed Unsupervised Neural Machine Translation (UNMT) that learns the translation using monolingual corpora of two languages. In this system, we built a fluent-todisfluent style transfer model based on UNMT with out-of-domain fluent data and in-domain disfluent data. 2.2 Fisher/Train Dev Test UNCorpus/Train Dev Test # sentences 138,720 3,977 3,641 1,000,000 4,000 4,000 Results 3.1 Datasets We used the LDC Fisher Spanish speech (disfluent) with new English translations (fluent) (Post et al., 2013; Salesky et al., 2018) as parallel in-domain data and the United Nations Parallel Corpus (UNCorpus) (Ziemski et al., 2016) as parallel out-ofdomain data. Fisher has the following multi-way parallel data distributed by the task organizer: 1. Spanish disfluent speech 2. Spanish disfluent transcripts (gold) Domain Adaptation 3. Spanish disfluent transcripts (ASR output) For the challenge task, we apply fine-tuning, which is one of the conventional domain adaptation methods of MT (Sennrich et al., 2016a). The fine-tuning can result in significant improvements compared to both only in-domain training or only out-ofdomain training (Dakwale and Monz, 2017). In this method, an NMT is pre-trained on a resource rich out-of"
2020.lrec-1.62,N19-1423,0,0.00517172,"xpressiveness of the emotion by the system. We recorded speeches from a student of a voice actor school, who is training to be a professional voice actor. It is challenging to record the whole of the collected system’s responses (7,356 responses); thus, we ranked each dialogue context. We used BERT to convert a dialogue context to a vector in a latent space and used K-means clustering to select points in the latent space. We selected a variety of dialogue contexts for building a robust dialogue system. 3.1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT (Devlin et al., 2019) is a language representation model, which is trained from large-scale text corpus with its transformer architecture (Vaswani et al., 2017). The pretrained model of BERT is trained to predict masked next or previous sentence in language modeling task for making representation of sentences. It is reported that the model well represents a sentence meaning and outperformed existing word or sentence representation methods in several language understanding benchmark tasks. We used a pretrained model that is trained from Japanese texts on social network services (Sakaki et al., 2019). We input a sen"
2020.lrec-1.62,W13-4016,0,0.0199182,"combinations of broad dialogue context and a variety of emotional states by crowd-sourcing. Then, we recorded emotional speech consisting of collected emotional expressions spoken by a voice actor. The experimental results indicate that the collected emotional expressions with their speeches have higher emotional expressiveness for expressing the system’s emotion to users. Keywords: dialogue corpus, emotional expression, speech corpus, persuasive dialogue 1. Introduction Persuasion or negotiation is an important dialogue style, which has been widely researched recently (Mazzotta et al., 2007; Georgila, 2013; Hiraoka et al., 2016; Wang et al., 2019). Emotional expressions have an important role in various dialogue situations and contexts (Keltner and Haidt, 1999; Morris and Keltner, 2000; Adler et al., 2016). It is well known that emotional expressions are useful in persuasion and negotiation (Fogg, 1999): building a cooperative relationship with positive expressions (Forgas, 1998) or pressing the dialogue partner to accept a proposal with negative expressions (Sinaceur and Tiedens, 2006). We built dialogue corpora in a persuasive scenario annotated with emotion labels to build persuasive dialogu"
2020.lrec-1.62,P17-1059,0,0.0664608,"Missing"
2020.lrec-1.62,P17-2017,0,0.0125703,"ollected in this work, which have the same meaning as the original “target response” in different emotional expressions. The original corpus was collected in Japanese; thus, the English is a translation. sultant utterances that are classified as “target responses” is 1,839, including 774 neutral, 320 anger, 392 sadness, and 353 happy labeled utterances. We used crowd-sourcing to collect response variations in different emotions. Crowdsourcing is a widely used approach for collecting paraphrasing expressions in existing works (Burrows et al., 2013) to cover lexical divergence (Xu et al., 2014; Jiang et al., 2017). In this work, we focus on collecting emotional variations of system utterances. We showed the dialogue context, target response, and a new emotion label, and requested the crowd-sourcing participants to paraphrase the target response with the given new emotion label. In the example in Table 2, the dialogue context and target response are “system-1”, “user-1”, and “system-2”, and the target emotion label is one of the emotions except “neutral”: “angry”, “sad”, and “happy”. During the annotation, we showed the participants a figure of Russell ’s circumplex model (Figure 1) and the following in"
2020.lrec-1.62,P19-1566,0,0.0284738,"and a variety of emotional states by crowd-sourcing. Then, we recorded emotional speech consisting of collected emotional expressions spoken by a voice actor. The experimental results indicate that the collected emotional expressions with their speeches have higher emotional expressiveness for expressing the system’s emotion to users. Keywords: dialogue corpus, emotional expression, speech corpus, persuasive dialogue 1. Introduction Persuasion or negotiation is an important dialogue style, which has been widely researched recently (Mazzotta et al., 2007; Georgila, 2013; Hiraoka et al., 2016; Wang et al., 2019). Emotional expressions have an important role in various dialogue situations and contexts (Keltner and Haidt, 1999; Morris and Keltner, 2000; Adler et al., 2016). It is well known that emotional expressions are useful in persuasion and negotiation (Fogg, 1999): building a cooperative relationship with positive expressions (Forgas, 1998) or pressing the dialogue partner to accept a proposal with negative expressions (Sinaceur and Tiedens, 2006). We built dialogue corpora in a persuasive scenario annotated with emotion labels to build persuasive dialogue systems that can use emotional expressio"
2020.lrec-1.62,Q14-1034,0,0.0220673,"onse variations collected in this work, which have the same meaning as the original “target response” in different emotional expressions. The original corpus was collected in Japanese; thus, the English is a translation. sultant utterances that are classified as “target responses” is 1,839, including 774 neutral, 320 anger, 392 sadness, and 353 happy labeled utterances. We used crowd-sourcing to collect response variations in different emotions. Crowdsourcing is a widely used approach for collecting paraphrasing expressions in existing works (Burrows et al., 2013) to cover lexical divergence (Xu et al., 2014; Jiang et al., 2017). In this work, we focus on collecting emotional variations of system utterances. We showed the dialogue context, target response, and a new emotion label, and requested the crowd-sourcing participants to paraphrase the target response with the given new emotion label. In the example in Table 2, the dialogue context and target response are “system-1”, “user-1”, and “system-2”, and the target emotion label is one of the emotions except “neutral”: “angry”, “sad”, and “happy”. During the annotation, we showed the participants a figure of Russell ’s circumplex model (Figure 1)"
2020.lrec-1.62,L18-1194,1,0.88301,"important role in various dialogue situations and contexts (Keltner and Haidt, 1999; Morris and Keltner, 2000; Adler et al., 2016). It is well known that emotional expressions are useful in persuasion and negotiation (Fogg, 1999): building a cooperative relationship with positive expressions (Forgas, 1998) or pressing the dialogue partner to accept a proposal with negative expressions (Sinaceur and Tiedens, 2006). We built dialogue corpora in a persuasive scenario annotated with emotion labels to build persuasive dialogue systems that can use emotional expressions to improve its success rate (Yoshino et al., 2018). When the system uses emotional expression in a dialogue, it is important to correctly express the emotion that the system intended. Expressing actual emotion to the users with only textual information is sometimes difficult because textual information has limited expressiveness. In contrast, emotional speech or gesture has the potential to improve the expressiveness for expressing the intended emotional state to the user. In this paper, we collected emotional expressions for the persuasive scenario and recorded their audio by expressing the emotional state to be indicated to the dialogue par"
2020.sltu-1.18,I08-3018,0,0.083296,"Missing"
2020.sltu-1.18,H92-1073,0,0.831266,"Missing"
2021.humeval-1.5,J96-2004,0,0.787758,"diction &lt; Serious &lt; Incomprehensible &lt; Unrelated &lt; Fair &lt; Good &lt; Excellent for adequacy14 . Tables 9 and 10 show the label statistics on the training, development, and test sets after applying the heuristics. 5.1.2 Automatic Evaluation Method We used a simple sentence-level automatic MT evaluation framework, which takes hypothesis and reference sentences as the input and predicts the label. Since the task in the experiments was classification, the evaluation model was trained with Inter-annotator Agreement We also measured pairwise agreement among the three annotators using the κ coefficient (Carletta, 1996) and label concordance rate. The results are shown in Table 8. The inter-annotator agreement was not high enough but κ values are also com13 Note that we had three annotators who evaluated all the sentences. 14 We used this heuristic order because of the importance of content errors suggested by Contradiction and Serious. 51 Fluency Incomprehensible Poor Fair Good Excellent A -0.644 (0.371) -0.421 (0.408) -0.079 (0.478) 0.165 (0.479) 0.428 (0.524) B -0.692 (0.356) -0.420 (0.399) 0.019 (0.474) 0.408 (0.485) 0.644 (0.465) C -0.649 (0.378) -0.400 (0.418) -0.129 (0.449) 0.122 (0.467) 0.427 (0.521)"
2021.humeval-1.5,2010.amta-papers.20,0,0.204684,"comprehension results in the early 1990s. The Quality Panel approach presented in their paper was motivated by the evaluation of human translations, but it was finally abandoned due to human evaluation difficulties. CallisonBurch et al. (2007) presented meta-evaluation of the MT evaluation in WMT shared tasks. According to the findings there, the WMT shared tasks had employed ranking-based human evaluation for a while. Snover et al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables ex"
2021.humeval-1.5,N19-1423,0,0.00475106,"on corpus will be publicly available at https://github.com/ ksudoh/wmt15-17-humaneval. 1 Introduction Most machine translation (MT) studies still evaluate their results using BLEU (Papineni et al., 2002) because of its simple, language-agnostic, and model-free methodology. Recent remarkable advances in neural MT (NMT) have cast an important challenge in its evaluation; NMT usually generates a fluent translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shimanaka et al., 2018). Zhang et al. (2020) proposed BERTScore based on hard token-level 1 The correlation got worse in the newer WMT datasets (Ma et al., 2018, 2019) due to noise in human judgement (Sellam et al., 2020). 2 This example is taken from the Metrics dataset of WMT 46 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 46–55 Online, April 19, 2021. ©2021 Association for Comput"
2021.humeval-1.5,C16-1294,0,0.123701,"al., 2015). Sellam et al. (2020) proposed BLEURT that incorporates auxiliary task signals into the pre-training of a BERT-based sentence-level regression model. These methods aim to evaluate a translation hypothesis using the corresponding reference with a high correlation to human judgment. The evaluation of this kind of MT evaluation, often called meta-evaluation, is usually based on some benchmarks. The meta-evaluation in the recent studies uses the WMT Metrics task dataset consisting of human judgment on MT results. The human judgment is given in the form of Human Direct Assessment (DA) (Graham et al., 2016), a 100-point rating scale. The Human DA results are standardized into z-scores (human DA scores, hereinafter) and used as the evaluation and optimization objective of regression-based MT evaluation methods. Recent MT evaluation methods achieved more than 0.8 in Pearson correlation on WMT 2017 test set1 . However, Takahashi et al. (2020) reported a weaker correlation in low human DA score ranges. Such a finding suggests the difficulty of the MT evaluation on low-quality results. In this work, we focus on the problem in the evaluation of low-quality translations that cause serious misunderstand"
2021.humeval-1.5,D16-1134,0,0.0223488,"al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables explainable evaluation instead. However, the meaning of the sentence can be changed by small changes, as discussed later in section 3. Looking at sub-structures and using their coverage in the MT evaluation may suffer from this problem. One recent approach has been proposed by Popovi`c (Popovic, 2020; Popovi´c, 2020). Her work analyzed the differences between comprehensibility and adequacy in machine translation outputs. The"
2021.humeval-1.5,U12-1010,0,0.0239653,"arly 1990s. The Quality Panel approach presented in their paper was motivated by the evaluation of human translations, but it was finally abandoned due to human evaluation difficulties. CallisonBurch et al. (2007) presented meta-evaluation of the MT evaluation in WMT shared tasks. According to the findings there, the WMT shared tasks had employed ranking-based human evaluation for a while. Snover et al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables explainable evaluation inst"
2021.humeval-1.5,W15-3044,0,0.0454957,"Missing"
2021.humeval-1.5,2020.eamt-1.39,0,0.0281184,"focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables explainable evaluation instead. However, the meaning of the sentence can be changed by small changes, as discussed later in section 3. Looking at sub-structures and using their coverage in the MT evaluation may suffer from this problem. One recent approach has been proposed by Popovi`c (Popovic, 2020; Popovi´c, 2020). Her work analyzed the differences between comprehensibility and adequacy in machine translation outputs. The human annotations in her work are major and minor errors in comprehensibility and adequacy on words and phrases. These fine-grained annotations are helpful for detailed translation error detection. The focus of our work is different; we are going to develop sentence-level MT evaluation through simpler human and automatic evaluation schemes. In this work, we suggest revisiting the classification-based evaluation with fluency and 2015. 3 http://producthelp.sdl.com/SDL_"
2021.humeval-1.5,2020.acl-main.704,0,0.163006,"t translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shimanaka et al., 2018). Zhang et al. (2020) proposed BERTScore based on hard token-level 1 The correlation got worse in the newer WMT datasets (Ma et al., 2018, 2019) due to noise in human judgement (Sellam et al., 2020). 2 This example is taken from the Metrics dataset of WMT 46 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 46–55 Online, April 19, 2021. ©2021 Association for Computational Linguistics difference in this example is just in one missing word not in the hypothesis, but it may cause a serious misunderstanding. Such translation errors are considered as critical ones by professional translators. There are several metrics for translation quality assessment (QA) proposed in the translators’ community, such as LISA QA Metric3 and Multidimentional Quality Metrics (MQM)4"
2021.humeval-1.5,W18-6456,0,0.0229894,"02) because of its simple, language-agnostic, and model-free methodology. Recent remarkable advances in neural MT (NMT) have cast an important challenge in its evaluation; NMT usually generates a fluent translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shimanaka et al., 2018). Zhang et al. (2020) proposed BERTScore based on hard token-level 1 The correlation got worse in the newer WMT datasets (Ma et al., 2018, 2019) due to noise in human judgement (Sellam et al., 2020). 2 This example is taken from the Metrics dataset of WMT 46 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 46–55 Online, April 19, 2021. ©2021 Association for Computational Linguistics difference in this example is just in one missing word not in the hypothesis, but it may cause a serious misunderstanding. Such translation errors are considered as critical ones by p"
2021.humeval-1.5,2021.ccl-1.108,0,0.0313368,"Missing"
2021.humeval-1.5,H93-1040,0,0.712365,"Missing"
2021.humeval-1.5,P11-1023,0,0.0383942,"gs there, the WMT shared tasks had employed ranking-based human evaluation for a while. Snover et al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables explainable evaluation instead. However, the meaning of the sentence can be changed by small changes, as discussed later in section 3. Looking at sub-structures and using their coverage in the MT evaluation may suffer from this problem. One recent approach has been proposed by Popovi`c (Popovic, 2020; Popovi´c, 2020). Her work"
2021.humeval-1.5,2006.amta-papers.25,0,0.204382,"ed Work MT evaluation has evolved along with the advance of MT technologies. White et al. (1994) reviewed some attempts of human evaluation and presented adequacy, fluency, and comprehension results in the early 1990s. The Quality Panel approach presented in their paper was motivated by the evaluation of human translations, but it was finally abandoned due to human evaluation difficulties. CallisonBurch et al. (2007) presented meta-evaluation of the MT evaluation in WMT shared tasks. According to the findings there, the WMT shared tasks had employed ranking-based human evaluation for a while. Snover et al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et a"
2021.humeval-1.5,2020.acl-main.327,1,0.753728,"on, often called meta-evaluation, is usually based on some benchmarks. The meta-evaluation in the recent studies uses the WMT Metrics task dataset consisting of human judgment on MT results. The human judgment is given in the form of Human Direct Assessment (DA) (Graham et al., 2016), a 100-point rating scale. The Human DA results are standardized into z-scores (human DA scores, hereinafter) and used as the evaluation and optimization objective of regression-based MT evaluation methods. Recent MT evaluation methods achieved more than 0.8 in Pearson correlation on WMT 2017 test set1 . However, Takahashi et al. (2020) reported a weaker correlation in low human DA score ranges. Such a finding suggests the difficulty of the MT evaluation on low-quality results. In this work, we focus on the problem in the evaluation of low-quality translations that cause serious misunderstanding. Judging erroneous translations in the 100-point rating scale would be very difficult and unstable, because the extent of errors cannot be mapped easily into a one-dimensional space. Suppose we are evaluating a translation hypothesis, (1) It is our duty to remain at his sides with its reference, It is not our duty to remain at his si"
2021.humeval-1.5,W18-6450,0,0.0150766,"allenge in its evaluation; NMT usually generates a fluent translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shimanaka et al., 2018). Zhang et al. (2020) proposed BERTScore based on hard token-level 1 The correlation got worse in the newer WMT datasets (Ma et al., 2018, 2019) due to noise in human judgement (Sellam et al., 2020). 2 This example is taken from the Metrics dataset of WMT 46 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 46–55 Online, April 19, 2021. ©2021 Association for Computational Linguistics difference in this example is just in one missing word not in the hypothesis, but it may cause a serious misunderstanding. Such translation errors are considered as critical ones by professional translators. There are several metrics for translation quality assessment (QA) proposed in the translators’ community, such a"
2021.humeval-1.5,1994.amta-1.25,0,0.699241,"Missing"
2021.humeval-1.5,W19-5302,0,0.0420376,"Missing"
2021.humeval-1.5,P02-1040,0,0.116701,"tical machine translation evaluation in an age of neural machine translation. We have made additional annotations on the WMT 2015-2017 Metrics datasets with fluency and adequacy labels to distinguish different types of translation errors from syntactic and semantic viewpoints. We present our human evaluation criteria for the corpus development and automatic evaluation experiments using the corpus. The human evaluation corpus will be publicly available at https://github.com/ ksudoh/wmt15-17-humaneval. 1 Introduction Most machine translation (MT) studies still evaluate their results using BLEU (Papineni et al., 2002) because of its simple, language-agnostic, and model-free methodology. Recent remarkable advances in neural MT (NMT) have cast an important challenge in its evaluation; NMT usually generates a fluent translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shi"
2021.humeval-1.5,D19-1053,0,0.0399997,"Missing"
2021.humeval-1.5,2020.coling-main.444,0,0.0779522,"Missing"
2021.iwslt-1.1,2020.lrec-1.520,0,0.0759372,"Missing"
2021.iwslt-1.1,2020.iwslt-1.3,0,0.0592828,"Missing"
2021.iwslt-1.1,2021.iwslt-1.5,0,0.0628415,"Missing"
2021.iwslt-1.1,N19-1202,1,0.897776,"Missing"
2021.iwslt-1.1,2021.acl-long.224,1,0.769761,"sh-German section of the MuST-C V2 corpus7 and include training, dev, and test (Test Common), in the same structure of the MuST-C V1 corpus (Cattoni et al., 2021) used last year. Since the 2021 test set was processed using the same pipeline applied to create MuST-C V2, the use of the new training resource was strongly recommended. The main differences with respect to MuST-C v1 are: gap with respect to the traditional cascade approach (integrating ASR and MT components in a pipelined architecture). In light of last year’s IWSLT results (Ansari et al., 2020) and of the findings of recent works (Bentivogli et al., 2021) attesting that the gap between the two paradigms has substantially closed, also this year a key element of the evaluation was to set up a shared framework for their comparison. For this reason, and to reliably measure progress with respect to the past rounds, the general evaluation setting was kept unchanged. This stability mainly concerns two aspects: the allowed architectures and the test set provision. On the architecture side, participation was allowed both with cascade and end-to-end (also known as direct) systems. In the latter case, valid submissions had to be obtained by models that:"
2021.iwslt-1.1,2021.iwslt-1.22,0,0.082468,"Missing"
2021.iwslt-1.1,2021.iwslt-1.27,1,0.721453,"to use the same training and development data as in the Offline Speech Translation track. More details are available in §3.2. For the English-Japanese text-to-text track, participants could use the parallel data and monolingual data available for the English-Japanese WMT20 news task (Barrault et al., 2020). For development, participants could use the IWSLT 2017 development sets,2 the IWSLT 2021 development set3 and the simultaneous interpretation transcripts for the IWSLT 2021 development set.4 The simultaneous interpretation was recorded as a part of NAIST Simultaneous Interpretation Corpus (Doi et al., 2021). Systems were evaluated with respect to quality and latency. Quality was evaluated with the standard BLEU metric (Papineni et al., 2002a). Latency was evaluated with metrics developed for simultaneous machine translation, including average proportion (AP), average lagging (AL) and differentiable average lagging (DAL, Cherry and Foster 2019), and later extended to the task of simultaneous speech translation (Ma et al., 2020b). The evaluation was run with the S IMUL E VAL toolkit (Ma et al., 2020a). For the latency measurement of speech input systems, we contrasted computation-aware and non com"
2021.iwslt-1.1,2012.eamt-1.60,1,0.698226,"rently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun,"
2021.iwslt-1.1,2020.iwslt-1.8,1,0.838546,"Missing"
2021.iwslt-1.1,2021.iwslt-1.12,0,0.0248123,"a et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two S"
2021.iwslt-1.1,L18-1001,0,0.0652124,"Missing"
2021.iwslt-1.1,2021.acl-long.68,1,0.787992,"Missing"
2021.iwslt-1.1,L18-1275,0,0.0286278,"ng two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guidelines.19 This makes them less literal compared to standard, unconstrained translations; • OpenSubtitles 2018 (Lison et al., 2018); • Augmented LibriSpeech et al., 2018)16 (Kocabiyikoglu • Mozilla Common Voice17 ; • Unconstrained translations. These references were created from scratch20 by adhering to the usual translation guidelines. They are hence exact (more literal) translations, without paraphrasing and with proper punctuation. • LibriSpeech ASR corpus (Panayotov et al., 2015). The list of allowed development data includes the dev set from IWSLT 2010, as well as the test sets used for the 2010, 2013, 2014, 2015 and 2018 IWSLT campaigns. Using other training/development resources was allowed but, in this case, parti"
2021.iwslt-1.1,2020.iwslt-1.9,0,0.0594925,"Missing"
2021.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0742394,"Missing"
2021.iwslt-1.1,2021.iwslt-1.4,0,0.134226,"essler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and acces"
2021.iwslt-1.1,N18-2074,0,0.0499123,"Missing"
2021.iwslt-1.1,2006.amta-papers.25,0,0.370434,"Missing"
2021.iwslt-1.1,W14-3354,0,0.0745687,"Missing"
2021.iwslt-1.1,W18-6319,0,0.0144137,"cipants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guideli"
2021.iwslt-1.1,2021.iwslt-1.19,0,0.0280723,", 2021) Fondazione Bruno Kessler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; comm"
2021.iwslt-1.1,2020.findings-emnlp.230,0,0.0691772,"Missing"
2021.iwslt-1.1,2021.iwslt-1.7,0,0.0794497,"Missing"
2021.iwslt-1.1,2021.iwslt-1.16,0,0.0332503,"nyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two Swahili varieties (Congolese and Co"
2021.iwslt-1.1,2020.lrec-1.517,1,0.846095,"s were finally converted from two (stereo) to one (mono) channel and downsampled from 48 to 16 kHz, using FFmpeg.10 Upon inspection of the spectrograms of the same talks in the two versions of MuST-C, it clearly emerges that the upper limit band in the audios used in MuST-C V1 is 5 kHz, while it is at 8 kHz in the latest version, coherently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 thi"
2021.iwslt-1.1,2021.iwslt-1.6,0,0.156379,"2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource spe"
2021.iwslt-1.24,N18-1008,0,0.0413439,"Missing"
2021.iwslt-1.24,W19-6612,0,0.0356742,"Missing"
2021.iwslt-1.24,N19-1202,0,0.0304475,"Missing"
2021.iwslt-1.24,P16-5005,0,0.029923,"Missing"
2021.iwslt-1.24,P84-1044,0,0.731782,"Missing"
2021.iwslt-1.24,2020.acl-demos.34,0,0.0278197,"Missing"
2021.iwslt-1.24,D16-1139,0,0.0291103,"perber et al. (2017b) and Xue et al. (2020) showed that translation accuracy against erroneous speech transcriptions can be improved by introducing pseudo ASR errors in the training data of MT. Knowledge distillation (KD) (Buciluˇa et al., 2006; Hinton et al., 2015) is a method of transferring knowledge from a teacher to a student model. Typically, the student model is trained by minimizing the KL-divergence (Kullback and Leibler, 1951) loss between the output probability distributions of the teacher and student models (wordlevel KD). Sequence-level knowledge distillation (sequence-level KD) (Kim and Rush, 2016a) targets the token-sequence generated by the teacher model using beam search. In our experiments, sequence-level KD outperformed word-level one, and Kim and Rush (2016b) showed similar trends. Therefore, in our experiments, we call it KD. The KD technique is prevalent in many applications of machine learning, including MT (non-autoregressive machine translation (Gu et al., 2017), simultaneous translation (Ren et al., 2020), etc.). Typically, it is used to distill knowledge from a larger teacher model to a smaller or faster student model. Recent works (Furlanello et al., 2018; Yang et al., 20"
2021.iwslt-1.24,2013.iwslt-papers.14,0,0.0343778,"rained by FT. Specifically, (1) the student model is trained ˆ and LM T , inheriting the parameters with X of the teacher model. Then (2) fine-tune the ˆ and LKD . model with X 5 we used MuST-C (Di Gangi et al., 2019a), a multilingual ST corpus built from TED talks. It contains triplets of about 250K segments of English speeches, transcripts, and Italian translations. We used audio and transcript pairs to train the ASR. To train the MT model, we used transcripts as clean input and ASR outputs as noisy input. For Spanish-English, we used LDC Fisher Spanish speech with new English translations (Post et al., 2013; Salesky et al., 2018). It has the following roughly 140K segments of multi-way parallel data: Experiments Dataset We conducted experiments for English to Italian and Spanish to English NMT. For English-Italian, 200 1 2 https://github.com/mjpost/sacreBLEU https://github.com/pytorch/fairseq ST Type End-to-end Cascade System ST + ASR-PT (Di Gangi et al., 2019b)1 ST + ASR-PT (ESPnet)2 ST ST + ASR-PT MTclean (Di Gangi et al., 2019b)1 MTclean MTasr MTasr + FT MTasr + KD MTasr + FT + KD MTasr + KD → FT MTasr + FT → KD ASR-based input Clean input 16.8 21.5 17.0 21.4 18.9 22.4 29.7 22.1 27.2 23.2 29."
2021.iwslt-1.24,2020.acl-main.350,0,0.0217861,"bler, 1951) loss between the output probability distributions of the teacher and student models (wordlevel KD). Sequence-level knowledge distillation (sequence-level KD) (Kim and Rush, 2016a) targets the token-sequence generated by the teacher model using beam search. In our experiments, sequence-level KD outperformed word-level one, and Kim and Rush (2016b) showed similar trends. Therefore, in our experiments, we call it KD. The KD technique is prevalent in many applications of machine learning, including MT (non-autoregressive machine translation (Gu et al., 2017), simultaneous translation (Ren et al., 2020), etc.). Typically, it is used to distill knowledge from a larger teacher model to a smaller or faster student model. Recent works (Furlanello et al., 2018; Yang et al., 2018) have shown that the student model’s accuracy exceeds that of the teacher model, even if its size is identical as the student model. KD has also been applied to ST. Gaido et al. (2020) applied KD to an end-to-end ST using an MT model based on clean transcriptions as the teacher of the end-toend ST model. Our work focuses on the application of KD to a cascade ST using a teacher model based on clean transcripts for the stud"
2021.iwslt-1.24,2020.acl-main.217,0,0.0298296,"ST consist of two components: automatic speech recognition (ASR) and machine translation (MT). In the cascade ST, the error propagation from ASR to MT seriously degrades the ST performance. On the other hand, a new ST system called end-to-end or direct ST uses a single model to directly translate the source language speech into target language text (B´erard et al., 2016). Such an end-to-end approach is a new paradigm in ST and is attracting much research attention. However, a naive end-toend ST without additional training, such as ASR tasks, remains inferior to a cascade ST (Liu et al., 2018; Salesky and Black, 2020). Additionally, it requires parallel data of the source language speech and the target language text, which cannot be obtained easily in practice. Recent ST studies have incorporated the techniques of cascade ST to end-to-end STs. Multitask training with an ASR subtask has been used During the training of an MT model for a cascade ST, we can use clean human transcripts for the source language speech as input. However, since the MT in a cascade ST always receives ASR output during inferences, ASR errors should be propagated to the MT model to cause translation errors. What if we use erroneous s"
2021.iwslt-1.24,P16-1009,0,0.237125,"int use of KD and FT To most effectively exploit both clean input X and ˆ we introduce two training ASR-based input X, techniques: KD and fine-tuning. In KD, the student ˆ by minimizing loss LKD . model is trained using X As shown in Fig. 1, LKD is the loss between Y 0 = 0 0 (y1 , ..., yM ) and Yˆ = (yˆ1 , ..., yˆN ), where Y 0 (1 ≤ m ≤ M ) and Yˆ (1 ≤ n ≤ N ) are the outputs of the teacher and student models. We use the sequence-level KD so that LKD is calculated by replacing L with M and l with m in Eq. 1. On the other hand, fine-tuning (FT) has been widely used for domain adaptation in MT (Sennrich et al., 2016a). Di Gangi et al. (2019c) showed that a model fine-tuned with ASR-based input becomes robust to erroneous ASR input while maintaining high performance for clean input. Following this finding, we employ FT for MT training. In FT, the ˆ which inherits the paramstudent model with X, eters of the teacher model with X, is trained by minimizing LM T (Fig. 1). In addition to the independent use of KD and FT, we examined their possible combinations: • FT+KD. Apply these techniques at the same time. Unlike regular FT, we use loss LKD instead of LM T . Specifically, (1) the teacher model is trained wi"
2021.iwslt-1.24,P16-1162,0,0.263516,"int use of KD and FT To most effectively exploit both clean input X and ˆ we introduce two training ASR-based input X, techniques: KD and fine-tuning. In KD, the student ˆ by minimizing loss LKD . model is trained using X As shown in Fig. 1, LKD is the loss between Y 0 = 0 0 (y1 , ..., yM ) and Yˆ = (yˆ1 , ..., yˆN ), where Y 0 (1 ≤ m ≤ M ) and Yˆ (1 ≤ n ≤ N ) are the outputs of the teacher and student models. We use the sequence-level KD so that LKD is calculated by replacing L with M and l with m in Eq. 1. On the other hand, fine-tuning (FT) has been widely used for domain adaptation in MT (Sennrich et al., 2016a). Di Gangi et al. (2019c) showed that a model fine-tuned with ASR-based input becomes robust to erroneous ASR input while maintaining high performance for clean input. Following this finding, we employ FT for MT training. In FT, the ˆ which inherits the paramstudent model with X, eters of the teacher model with X, is trained by minimizing LM T (Fig. 1). In addition to the independent use of KD and FT, we examined their possible combinations: • FT+KD. Apply these techniques at the same time. Unlike regular FT, we use loss LKD instead of LM T . Specifically, (1) the teacher model is trained wi"
2021.iwslt-1.24,D17-1145,0,0.0211509,"ion improved the robustness against ASR errors and that the knowledge distillation after the fine-tuning provided more significant improvement. 198 Proceedings of the 18th International Conference on Spoken Language Translation, pages 198–205 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics 2 Related work (a) MT89:;< Some ST studies have tackled the problem of ASR error propagation. N-best hypotheses (Zhang et al., 2004; Quan et al., 2005), confusion networks (Bertoldi and Federico, 2005; Bertoldi et al., 2007), and lattices (Matusov and Ney, 2010; Sperber et al., 2017a) were used to include ASR ambiguity in the ST process. Osamura et al. (2018) used the weighted sum of embedding vectors for ASR word hypotheses based on their posterior probabilities. Sperber et al. (2017b) and Xue et al. (2020) showed that translation accuracy against erroneous speech transcriptions can be improved by introducing pseudo ASR errors in the training data of MT. Knowledge distillation (KD) (Buciluˇa et al., 2006; Hinton et al., 2015) is a method of transferring knowledge from a teacher to a student model. Typically, the student model is trained by minimizing the KL-divergence ("
2021.iwslt-1.24,Q19-1020,0,0.0339238,"Missing"
2021.iwslt-1.24,2020.aacl-demo.6,0,0.0861949,"Missing"
2021.iwslt-1.24,2020.autosimtrans-1.3,0,0.0350951,"s 198–205 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics 2 Related work (a) MT89:;< Some ST studies have tackled the problem of ASR error propagation. N-best hypotheses (Zhang et al., 2004; Quan et al., 2005), confusion networks (Bertoldi and Federico, 2005; Bertoldi et al., 2007), and lattices (Matusov and Ney, 2010; Sperber et al., 2017a) were used to include ASR ambiguity in the ST process. Osamura et al. (2018) used the weighted sum of embedding vectors for ASR word hypotheses based on their posterior probabilities. Sperber et al. (2017b) and Xue et al. (2020) showed that translation accuracy against erroneous speech transcriptions can be improved by introducing pseudo ASR errors in the training data of MT. Knowledge distillation (KD) (Buciluˇa et al., 2006; Hinton et al., 2015) is a method of transferring knowledge from a teacher to a student model. Typically, the student model is trained by minimizing the KL-divergence (Kullback and Leibler, 1951) loss between the output probability distributions of the teacher and student models (wordlevel KD). Sequence-level knowledge distillation (sequence-level KD) (Kim and Rush, 2016a) targets the token-sequ"
2021.iwslt-1.24,C04-1168,0,0.116584,"ranscriptions. We also investigate the joint use of knowledge distillation and fine-tuning. Experimental results revealed that the knowledge distillation improved the robustness against ASR errors and that the knowledge distillation after the fine-tuning provided more significant improvement. 198 Proceedings of the 18th International Conference on Spoken Language Translation, pages 198–205 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics 2 Related work (a) MT89:;< Some ST studies have tackled the problem of ASR error propagation. N-best hypotheses (Zhang et al., 2004; Quan et al., 2005), confusion networks (Bertoldi and Federico, 2005; Bertoldi et al., 2007), and lattices (Matusov and Ney, 2010; Sperber et al., 2017a) were used to include ASR ambiguity in the ST process. Osamura et al. (2018) used the weighted sum of embedding vectors for ASR word hypotheses based on their posterior probabilities. Sperber et al. (2017b) and Xue et al. (2020) showed that translation accuracy against erroneous speech transcriptions can be improved by introducing pseudo ASR errors in the training data of MT. Knowledge distillation (KD) (Buciluˇa et al., 2006; Hinton et al.,"
2021.iwslt-1.27,2021.iwslt-1.29,0,0.0229024,"foreign guest speakers from politicians to business representatives. The press conferences are video-recorded and available online3 . For some of them, transcripts are provided on its website. Translation Studies In translation studies, SI characteristics have typically been investigated from the aspects of latency, quality, and word order. For evaluating latency by human interpreters, Ear-Voice Span (EVS) is commonly used as a metric. EVS denotes the lag between the original utterances and the corresponding SIs. The analysis of quality often relies on a manual evaluation of the corpus data (Fantinuoli and Prandi, 2021). Ino and Kawahara (2008), for example, investigated SI faithfulness based on manual annotation of the data. SI aims to translate a source speech with low latency and high quality, where the two factors are in a trade-off relationship. However, previous studies (e.g., Lee, 2002) argued that a longer latency negatively affects SI quality. Word order has also been intensively studied in the field. Recent research by Cai et al. (2020) demonstrated a statistical study based on SIDB and compared word order between translation and SI. Material Our corpus consists of the SIs of four kinds of material"
2021.iwslt-1.27,2021.naacl-main.150,0,0.0126031,"amounts of experience, as in Shimizu et al. (2014), which enables comparisons of SI differences based on experience. Introduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translation"
2021.iwslt-1.27,P19-1126,0,0.0259571,"Missing"
2021.iwslt-1.27,D15-1276,0,0.0178831,"Missing"
2021.iwslt-1.27,2020.acl-main.253,0,0.015926,"egments that correspond to multiple English sentences: divide where it corresponds to the boundary of English sentences. Mark XXXXX for end/start times of Japanese segments. • English segments that consist of multiple sentences: divide at sentence boundary. Mark XXXXX for end/start times of segments. An example of the data aligned at the sentence level is shown in Fig. 2. Each sentence is delimited by one blank line. 4.3 Quality: To evaluate the SI quality, we calculated two metrics8 . The first one was BERTScore (Zhang et al., 2019), which is also used to evaluate machine translations (e.g., Edunov et al., 2020). It is based on contextualized subword embeddings and is expected to capture meanings rather than surface forms like BLEU (Papineni et al., 2002). It would be appropriate for evaluating the aspects of SIs used by interpreters, including anticipation, summarization, and generalization. BERTScores were calculated between SIs (candidates) and offline translations (references) for each sentence. The other quality metric was the bunsetsu-level semantic preservation score (BSPS), which evaluated the faithfulness of the SIs against the translations. An example is shown in Fig. 3. Similar to Ino and"
2021.iwslt-1.27,P14-2090,1,0.790449,"h enables comparisons of SI differences based on experience. Introduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translations are generated is different. For Speech Translation"
2021.iwslt-1.27,P02-1040,0,0.111383,"t times of Japanese segments. • English segments that consist of multiple sentences: divide at sentence boundary. Mark XXXXX for end/start times of segments. An example of the data aligned at the sentence level is shown in Fig. 2. Each sentence is delimited by one blank line. 4.3 Quality: To evaluate the SI quality, we calculated two metrics8 . The first one was BERTScore (Zhang et al., 2019), which is also used to evaluate machine translations (e.g., Edunov et al., 2020). It is based on contextualized subword embeddings and is expected to capture meanings rather than surface forms like BLEU (Papineni et al., 2002). It would be appropriate for evaluating the aspects of SIs used by interpreters, including anticipation, summarization, and generalization. BERTScores were calculated between SIs (candidates) and offline translations (references) for each sentence. The other quality metric was the bunsetsu-level semantic preservation score (BSPS), which evaluated the faithfulness of the SIs against the translations. An example is shown in Fig. 3. Similar to Ino and Kawahara (2008), each bunsetsu that appeared in the translation was considered a unit of ideas. Then we counted the number of bunsetsus in the SI"
2021.iwslt-1.27,shimizu-etal-2014-collection,1,0.825935,"quality. 1 Language En↔Jp En↔Es En↔Jp Zh→En En↔Jp Hours 182 217 22 68 304.5 Table 1: Existing SI corpora and ours or transcripts; for SI corpora, human interpreters actually do SI. SI corpora are useful not only for the construction of automatic SI systems but also for translation studies. To facilitate research in the field of SI, we are constructing a new large-scale English↔Japanese SI corpus1 . We recorded the SIs of lectures and press conferences and amassed over 300 hours of such data. Some lectures have SI data generated by three interpreters with different amounts of experience, as in Shimizu et al. (2014), which enables comparisons of SI differences based on experience. Introduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), includin"
2021.iwslt-1.27,D18-2010,0,0.0128757,"by the first author to fairly compare the data of the interpreters of each rank. Since the segments in the SI transcripts were based on the interpreters’ utterances, they did not necessarily match among the interpreters. Thus, we gave sentence alignments based on the sentences of the English transcripts segmented using the following rules: 228 4 A bunsetsu is a basic unit of dependency in Japanese that consists of one or more content words and the following zero or more function words (Kawahara and Kurohashi, 2006). 5 We used Juman++ ver.1.02 rather than the development version of Juman++ V2 (Tolmachev et al., 2018). in our data, we separately calculated EVS at the beginning and the end of a sentence7 : EN_0177 469789 471829 I&apos;ve got two questions for you. JA_0116 XXXXXX 473315 二つの質問がありますよ。 EN_0178 471829 473469 (Laughter) JA_0000 XXXXXX XXXXXX __null__ EV Sstart = start timeJP − start timeEN EN_0179 473469 476069 You know what&apos;s coming now, right? JA_0117 474778 476197 質問分かってるんですね。 EV Send = end timeJP − end timeEN . Figure 2: Example of sentence-level alignment • segments ending with a period (.) or a period + a double quotation mark (.”) • segments ending with a question mark (?) or a question mark +"
2021.iwslt-1.27,2021.autosimtrans-1.5,0,0.0767654,"e after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translations are generated is different. For Speech Translation corpora, a translation is based on complete audio data In this paper, we describe the construction of a new corpus and present the results of its analysis. Its design follows the framework of Shimizu et al. (2014). The analysis was conducted on a subset of lectu"
2021.iwslt-1.27,2020.emnlp-main.178,0,0.0147252,"roduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translations are generated is different. For Speech Translation corpora, a translation is based on complete audio data In this pap"
2021.iwslt-1.27,P19-1582,0,0.0131896,"ons of SI differences based on experience. Introduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translations are generated is different. For Speech Translation corpora, a translati"
2021.iwslt-1.3,P19-1126,0,0.0284687,"Missing"
2021.iwslt-1.3,N12-1048,0,0.010074,"pon partial input observations of X. Suppose an output prefix subsequence Y1j = y1 , y2 , ..., yj has already been predicted from prefix observations of the input X1i = x1 , x2 , ..., xi . When we predict the next Introduction Automatic simultaneous translation is an attractive research field that aims to translate an input before observing its end for real-time translation similar to human simultaneous interpretation. Starting from early attempts using rule-based machine translation (Matsubara and Inagaki, 1997; Ryu et al., 2006) and statistical methods using statistical machine translation (Bangalore et al., 2012; Fujita et al., 2013), recent studies successfully applied neural machine translation (NMT) into this task (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019). The simultaneous translation shared task in the IWSLT evaluation campaign started on 2020 with English-to-German (Ansari et al., 2020) speechto-text and text-to-text tasks, and a new language pair of English-to-Japanese has been included on 2021 only in text-to-text task. English-to-Japanese is much more challenging than English-to-German due to the large language difference in addition to data scarsity. We developed an automa"
2021.iwslt-1.3,N19-1423,0,0.0161391,"が 起き て も 何 が 起き て も 何 が 起き て も 何 が 起き て 人間 は 何 が 間違っ て いる の か を 考える の が 得意 です 新しい こと を 試し て み て も いい です よ ね 昇給 を 求める という よう な 何 か 新しい こと を 試みよ う という とき 人 は どう まずい こと に なり 得る か 考える こと に 長け て い ます Table 3: Translation examples by wait-k baseline and wait-k with chunk shuffling (pr = 0.02). System wait-10 + CShuflow wait-20 + SKDmedium wait-30high BLEU 14.41 16.20 16.19 train 2,762,408 AL 7.21 11.54 13.83 translation for partially-observed input, using a multi-label classifier based on linear SVMs (Fan et al., 2008). Motivated by this study, we used a neural network-based classifier using BERT (Devlin et al., 2019) for NCLP. The problem of NCLP is defined as the label prediction of a syntactic constituent coming next to a given word subsequence in the pre-order tree traversal. In this work, we used 1-lookahead prediction, so the problem was relaxed into the prediction of a label of a syntactic constituent given its preceding words and the first word composing it. A predicted constituent label was inserted at the corresponding position in the input word sequence, immediately after its preceding word. That doubled the length of input sequences. For subword-based NMT, we applied BPE only onto words in the"
2021.iwslt-1.3,2020.emnlp-demos.19,0,0.0245443,"mimic its knowledge (Kim and Rush, 2016). The teacher distribution q(Y |X) is approximated by its mode q(Y |X) ≈ 1{Y = argmax q(Y |X)}, X∈T and the loss objectives as follows: LSKD = −Ex∼data X Target-side chunk shuffling q(Y |X) log p(Y |X) Y ∈T ≈ −EX∼data,Yb =argmax q(Y |X) [log p(y = Yb |X)] (4) 5 Primary system Y ∈T 5.1 Implementation where p(Y |X) is the sequence-level distribution, Our system implementation was based on the ofand Y ∈ T is the space of possible target sentences. ficial baseline1 using fairseq (Ott et al., 2019) and SKD can be implemented simply by training the SimulEval (Ma et al., 2020). student model using (X, Yb ), where Yb is derived from the teacher model outputs for the source lan- 5.2 Setup guage portion of the training corpus. Data All of the models were based on TransWe use SKD for reduction of colloquial expres- former, trained using 17.9 million Englishsions in the spoken language corpus. Such col- Japanese parallel sentences from WMT20 news loquial expressions are highly dependent on lan- task and fine-tuned using 223 thousand parallel guages and difficult to translate by NMT, which sentences from IWSLT 2017. During fine-tuning, usually generates literal translati"
2021.iwslt-1.3,J93-2004,0,0.0770268,"on of a label of a syntactic constituent given its preceding words and the first word composing it. A predicted constituent label was inserted at the corresponding position in the input word sequence, immediately after its preceding word. That doubled the length of input sequences. For subword-based NMT, we applied BPE only onto words in the input sequences and put dummy labels after subwords other than end-of-word ones, to order the input in an alternating way. We used Huggingface transformers (Wolf et al., 2020) for our implementation of NCLP with bert-base-uncased. We used Penn Treebank 3 (Marcus et al., 1993) for the NCLP training and development sets, and NAIST-NTT TED Talk Treebank (Neubig et al., 2014) for the NCLP evaluation set. Table 5 shows the number of training, development, and evaluation instances extracted from the datasets. Note that we can extract many instances from a single parse tree. Table 6 shows the results of the 5 most frequent labels in the NCLP training data. NP and VP are chunk shuffling may work as a perturbation, and we need further investigation. Official results on the test set Table 4 shows BLEU and AL results on the test set. The system with the medium latency regime"
2021.iwslt-1.3,E17-1099,0,0.0202073,"refix observations of the input X1i = x1 , x2 , ..., xi . When we predict the next Introduction Automatic simultaneous translation is an attractive research field that aims to translate an input before observing its end for real-time translation similar to human simultaneous interpretation. Starting from early attempts using rule-based machine translation (Matsubara and Inagaki, 1997; Ryu et al., 2006) and statistical methods using statistical machine translation (Bangalore et al., 2012; Fujita et al., 2013), recent studies successfully applied neural machine translation (NMT) into this task (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019). The simultaneous translation shared task in the IWSLT evaluation campaign started on 2020 with English-to-German (Ansari et al., 2020) speechto-text and text-to-text tasks, and a new language pair of English-to-Japanese has been included on 2021 only in text-to-text task. English-to-Japanese is much more challenging than English-to-German due to the large language difference in addition to data scarsity. We developed an automatic text-to-text simultaneous translation system for this shared task. We applied some extensions to a standard wait-k NMT i"
2021.iwslt-1.3,2014.iwslt-papers.16,0,0.0132413,". A predicted constituent label was inserted at the corresponding position in the input word sequence, immediately after its preceding word. That doubled the length of input sequences. For subword-based NMT, we applied BPE only onto words in the input sequences and put dummy labels after subwords other than end-of-word ones, to order the input in an alternating way. We used Huggingface transformers (Wolf et al., 2020) for our implementation of NCLP with bert-base-uncased. We used Penn Treebank 3 (Marcus et al., 1993) for the NCLP training and development sets, and NAIST-NTT TED Talk Treebank (Neubig et al., 2014) for the NCLP evaluation set. Table 5 shows the number of training, development, and evaluation instances extracted from the datasets. Note that we can extract many instances from a single parse tree. Table 6 shows the results of the 5 most frequent labels in the NCLP training data. NP and VP are chunk shuffling may work as a perturbation, and we need further investigation. Official results on the test set Table 4 shows BLEU and AL results on the test set. The system with the medium latency regime (wait-20 + SKD) worked relatively well; it achived a comparable BLEU result with wait-30. However"
2021.iwslt-1.3,P15-1020,1,0.837341,"tempt: Incremental Next Constituent Label Prediction We tried another technique described below in the shared task, but it was not included in our primary submission because it did not outperform the baseline. Here, we also describe this for further investigation in future. For simultaneous machine translation, deciding how long to wait for input before translation is important. Predicting what kind of phrase comes next is a part of useful information in determining the timing. In this study, we tried incremental Next Constituent Label Prediction (NCLP). In SMT-based simultaneous translation, Oda et al. (2015) proposed a method to predict unseen syntactic constituents to determine when to start 42 natural than baseline like these examples. However, many sentences are not informative and missing details compared to the baseline. We’ll investigate a more effective way to use NCLP in our future work. 18 16 wait-k wait-k+NCLP BLEU 14 32 12 k=10 56 52 40 60 64 48 28 44 7 In this paper, we described our English-to-Japanese text-to-text simultaneous translation system. We extended the baseline wait-k with the knowledge distillation to encourage literal translation and targetside chunk shuffling to relax t"
2021.iwslt-1.3,D16-1139,0,0.0183701,"o the length of T . Then, we choose to shuffle or keep the chunks in T¯ with a probability pr , defined as a hyperparameter. We tried only the random shuffling with the fixed chunk size of k in this time; More linguistically-motivated chunk reordering would be worth trying as future work. q(y = k|x; θT ) × k=1 log p(y = k|x; θ) (3) where θT parameterizes the teacher distribution. Sequence-level Knowledge Distillation (SKD), which gives the student model the output of the teacher model as knowledge, propagates a wide range of knowledge to the student model and trains it to mimic its knowledge (Kim and Rush, 2016). The teacher distribution q(Y |X) is approximated by its mode q(Y |X) ≈ 1{Y = argmax q(Y |X)}, X∈T and the loss objectives as follows: LSKD = −Ex∼data X Target-side chunk shuffling q(Y |X) log p(Y |X) Y ∈T ≈ −EX∼data,Yb =argmax q(Y |X) [log p(y = Yb |X)] (4) 5 Primary system Y ∈T 5.1 Implementation where p(Y |X) is the sequence-level distribution, Our system implementation was based on the ofand Y ∈ T is the space of possible target sentences. ficial baseline1 using fairseq (Ott et al., 2019) and SKD can be implemented simply by training the SimulEval (Ma et al., 2020). student model using (X"
2021.iwslt-1.3,N19-4009,0,0.0238387,"ge, propagates a wide range of knowledge to the student model and trains it to mimic its knowledge (Kim and Rush, 2016). The teacher distribution q(Y |X) is approximated by its mode q(Y |X) ≈ 1{Y = argmax q(Y |X)}, X∈T and the loss objectives as follows: LSKD = −Ex∼data X Target-side chunk shuffling q(Y |X) log p(Y |X) Y ∈T ≈ −EX∼data,Yb =argmax q(Y |X) [log p(y = Yb |X)] (4) 5 Primary system Y ∈T 5.1 Implementation where p(Y |X) is the sequence-level distribution, Our system implementation was based on the ofand Y ∈ T is the space of possible target sentences. ficial baseline1 using fairseq (Ott et al., 2019) and SKD can be implemented simply by training the SimulEval (Ma et al., 2020). student model using (X, Yb ), where Yb is derived from the teacher model outputs for the source lan- 5.2 Setup guage portion of the training corpus. Data All of the models were based on TransWe use SKD for reduction of colloquial expres- former, trained using 17.9 million Englishsions in the spoken language corpus. Such col- Japanese parallel sentences from WMT20 news loquial expressions are highly dependent on lan- task and fine-tuned using 223 thousand parallel guages and difficult to translate by NMT, which sent"
2021.iwslt-1.3,P06-2088,0,0.0507834,"slate X to Y incrementally. In other words, each output prediction of Y is made upon partial input observations of X. Suppose an output prefix subsequence Y1j = y1 , y2 , ..., yj has already been predicted from prefix observations of the input X1i = x1 , x2 , ..., xi . When we predict the next Introduction Automatic simultaneous translation is an attractive research field that aims to translate an input before observing its end for real-time translation similar to human simultaneous interpretation. Starting from early attempts using rule-based machine translation (Matsubara and Inagaki, 1997; Ryu et al., 2006) and statistical methods using statistical machine translation (Bangalore et al., 2012; Fujita et al., 2013), recent studies successfully applied neural machine translation (NMT) into this task (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019). The simultaneous translation shared task in the IWSLT evaluation campaign started on 2020 with English-to-German (Ansari et al., 2020) speechto-text and text-to-text tasks, and a new language pair of English-to-Japanese has been included on 2021 only in text-to-text task. English-to-Japanese is much more challenging than English-to-German due"
2021.iwslt-1.3,P16-1162,0,0.0452263,"IWSLT 2017. During fine-tuning, usually generates literal translations. Here, we we examined the effectiveness of knowledge distilfirstly train a teacher, Transformer-based offline lation and chunk shuffling with several hyperparamNMT model using the training corpus and use it to eter settings and reported the results by the models obtain pseudo-reference translations in the target that resulted in the higher BLEU on IWSLT 2021 language. Then, we train a student, Transformer- development set. The text was preprocessed by based simultaneous NMT model using the pseudo- Byte Pair Encoding (BPE) (Sennrich et al., 2016) parallel corpus with the original source language 1 https://github.com/pytorch/fairseq/ sentences and the corresponding translation re- blob/master/examples/simultaneous_ sults by the teacher model. The pseudo-references translation/docs/enja-waitk.md 40 BLEU 16.8 AL - 18 16 18 11.8 14.69 15.57 7.27 11.47 13.7 14 BLEU System offline Baseline wait-10 wait-20 wait-30high Proposed wait-10 + CShuflow wait-10 + SKD wait-20 + SKDmedium wait-30 + SKD 16 12 12 k=10 12 18 14 14 20 24 22 26 28 30 32 28 30 24 22 32 26 16 10 13.77 13.5 15.22 15.21 7.29 7.28 11.48 13.71 wait-k wait-k+SKD 8 6 6 8 10 12 14"
2021.sigdial-1.9,N19-1423,0,0.00516984,"raining data. Cx+i and Cx−i are, respectively, the set of the positive example action categories associated with the user request xi and the set of the action categories without any annotation. rj is the rank predicted by the model for the positive category j and L(rj ) is the weight function satisfying 81 bels (categories) as follows: ( ) d(xi , xj ) 70 · . sij = exp − 69 d¯ 4.1 Model Configuration PyTorch (Paszke et al., 2019) is used to implement the models. We used the Japanese BERT model (Shibata et al., 2019), which was pre-trained on Wikipedia articles. Both BASE and LARGE model sizes (Devlin et al., 2019) were used for the experiments. We used Adam (Kingma and Ba, 2015) to optimize the model parameters and set the learning rate to 1e−5. For m in Eq. (3) and κ in Eq. (1), we set m = −0.8, κ = 5 according to the literature (Cevikalp et al., 2020). We used the distributed representations output by BERT as the vector xi in the label propagation. Since the parameters of BERT are also optimized during the training, we reran the label propagation every five epochs. We pre-trained the model by PN learning before we applied PU learning. Similarity score sij of (PU, nearest) is also scaled by Eq. (6) as"
2021.sigdial-1.9,J13-3008,0,0.0716535,"Missing"
2021.sigdial-1.9,W17-5526,0,0.0430423,"Missing"
2021.sigdial-1.9,W17-5514,0,0.0497338,"Missing"
2021.sigdial-1.9,W17-5506,0,0.0139152,"stem assumes that the user’s intention is not clear. In the corpus collected by Cohen and Lane (2012), which assumes a car navigation dialogue agent, the agent responds to user requests classified as Q1, such as suggesting a stop at a gas station when the user is running out of gasoline. Our study collected a variation of ambiguous user utterances to cover several situations in sightseeing. Table 12: Ratios of false positive in label propagation 5.1 Task-Oriented Dialogue Corpus Many dialogue corpora for task-oriented dialogue have been proposed, such as Frames (El Asri et al., 2017), In-Car (Eric et al., 2017), bAbI dialog (Bordes and Weston, 2016), and MultiWOZ (Budzianowski et al., 2018). These corpora assume that the user requests are clear, as in Q3 in Table 1 defined by Taylor (1962, 1968), and do not assume that user requests are ambiguous, as is the case in our study. The corpus collected in our study assumes cases where the user requests are ambiguous, such as Q1 and Q2 in Table 1. Ohtake et al. (2009); Yoshino et al. (2017) tackled sightseeing dialogue domains. The corpus collected by Ohtake et al. (2009) consisted of dialogues by a tourist and guide for making a oneday plan to sightsee in"
2021.sigdial-1.9,D18-1547,0,0.0349231,"Missing"
2021.sigdial-1.9,W12-1635,0,0.0462243,"Missing"
2021.sigdial-1.9,C02-1086,0,0.195053,"Missing"
2021.sigdial-1.9,J98-1005,0,0.370326,"Missing"
2021.sigdial-1.9,W19-5931,0,0.0132541,"king them to these categories. There are 70 categories in total. The functions and categories are defined heuristically acThoughtful System Action to Ambiguous User Request Existing task-oriented dialogue systems assume that user intentions are clarified and uttered in an explicit manner; however, users often do not know what they want to request. User requests in such cases are ambiguous. Taylor (1962, 1968) categorizes user states in information search into four levels according to their clarity, as shown in Table 1. Most of the existing task-oriented dialogue systems (Madotto et al., 2018; Vanzo et al., 2019) convert explicit user requests (Q3) into machine readable expressions (Q4). Future dialogue systems need to take appropriate actions even in situations such as Q1 and Q2, where the users are not able to clearly verbalize their requests 1 The dataset is available at https://github.com/ahclab/arta_corpus. 78 Function spot search cording to Web sites for Kyoto sightseeing. “Spot search” is a function to search for specific spots and is presented to the user in the form of an action such as “Shall I search for an art museum around here?” “Restaurant search” is a function to search for specific re"
2021.sigdial-1.9,N10-1055,0,0.0451359,"Missing"
2021.sigdial-1.9,P18-1136,0,0.0133131,"s are generated by linking them to these categories. There are 70 categories in total. The functions and categories are defined heuristically acThoughtful System Action to Ambiguous User Request Existing task-oriented dialogue systems assume that user intentions are clarified and uttered in an explicit manner; however, users often do not know what they want to request. User requests in such cases are ambiguous. Taylor (1962, 1968) categorizes user states in information search into four levels according to their clarity, as shown in Table 1. Most of the existing task-oriented dialogue systems (Madotto et al., 2018; Vanzo et al., 2019) convert explicit user requests (Q3) into machine readable expressions (Q4). Future dialogue systems need to take appropriate actions even in situations such as Q1 and Q2, where the users are not able to clearly verbalize their requests 1 The dataset is available at https://github.com/ahclab/arta_corpus. 78 Function spot search cording to Web sites for Kyoto sightseeing. “Spot search” is a function to search for specific spots and is presented to the user in the form of an action such as “Shall I search for an art museum around here?” “Restaurant search” is a function to s"
2021.sigdial-1.9,P19-1078,0,0.017233,"mes that only a part of the data is labeled with positive examples. The experimental results show that the PU learning method achieved better performance than the general positive/negative (PN) learning method to classify thoughtful actions given an ambiguous user request. 1 Introduction Task-oriented dialogue systems satisfy user requests by using pre-defined system functions (Application Programming Interface (API) calls). Natural language understanding, a module to bridge user requests and system API calls, is an important technology for spoken language applications such as smart speakers (Wu et al., 2019). Although existing spoken dialogue systems assume that users give explicit requests to the system (Young et al., 2010), users may not always be able to define and verbalize the content and conditions of their own requests clearly (Yoshino et al., 77 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 77–88 July 29–31, 2021. ©2021 Association for Computational Linguistics Level Q1 Q2 Q3 Q4 Definition The actual, but unexpressed request The conscious, within-brain description of the request The formal statement of the request The request as pres"
2021.sigdial-1.9,P19-1081,0,0.0164307,"logues by a tourist and guide for making a oneday plan to sightsee in Kyoto. However, it was difficult for the developed system to make particular recommendations for conversational utterances or monologues. Yoshino et al. (2017) developed a dialogue agent that presented information with a proactive dialogue strategy. Although the situation is similar to our task, their agent does not have clear natural language understanding (NLU) systems to bridge the user requests to a particular system action. Some dialogue corpora are proposed to treat user requests that are not always clear: OpenDialKG (Moon et al., 2019), ReDial (Li et al., 2018), and RCG (Kang et al., 2019). They assume that the system makes recommendations even if the user does not have a specific request, in particular, dialogue domains such as movies or music. In our study, we focus on conversational utterance and monologue during sightseeing, which can be a trigger of thoughtful actions from the system. 84 6 Conclusion straints. In 3rd International Conference on Computer Vision Theory and Applications (VISAPP ’ 08), pages 489–496. We collected a dialogue corpus that bridges ambiguous user requests to thoughtful system actions while focu"
2021.sigdial-1.9,W17-5542,1,0.848867,"d system actions. We defined a problem to train a model on the incompletely annotated data and tested on the completely annotated data1 . In order to train the model on the incomplete training data, we applied the positive/unlabeled (PU) learning method (Elkan and Noto, 2008; Cevikalp et al., 2020), which assumes that some of the data are annotated as positive and the rest are not. The experimental results show that the proposed classifier based on PU learning has higher classification performances than the conventional classifier, which is based on general positive/negative (PN) learning. 2 (Yoshino et al., 2017). We used crowdsourcing to collect ambiguous user requests and link them to appropriate system actions. This section describes the data collection. 2.1 Corpus Collection We assume a dialogue between a user and a dialogue agent on a smartphone application in the domain of tourist information. The user can make ambiguous requests or monologues, and the agent responds with thoughtful actions. Figure 1 shows an example dialogue between a user and a dialogue agent. The user utterance “I love the view here!” is not verbalized as a request for a specific function. The dialogue agent responds with a t"
2021.sigdial-1.9,W09-3405,1,0.687342,"tios of false positive in label propagation 5.1 Task-Oriented Dialogue Corpus Many dialogue corpora for task-oriented dialogue have been proposed, such as Frames (El Asri et al., 2017), In-Car (Eric et al., 2017), bAbI dialog (Bordes and Weston, 2016), and MultiWOZ (Budzianowski et al., 2018). These corpora assume that the user requests are clear, as in Q3 in Table 1 defined by Taylor (1962, 1968), and do not assume that user requests are ambiguous, as is the case in our study. The corpus collected in our study assumes cases where the user requests are ambiguous, such as Q1 and Q2 in Table 1. Ohtake et al. (2009); Yoshino et al. (2017) tackled sightseeing dialogue domains. The corpus collected by Ohtake et al. (2009) consisted of dialogues by a tourist and guide for making a oneday plan to sightsee in Kyoto. However, it was difficult for the developed system to make particular recommendations for conversational utterances or monologues. Yoshino et al. (2017) developed a dialogue agent that presented information with a proactive dialogue strategy. Although the situation is similar to our task, their agent does not have clear natural language understanding (NLU) systems to bridge the user requests to a"
C08-3006,W05-0909,0,0.0284034,"70.70 – 89.73 67.33 72.19 69.14 68.32 64.55 62.91 70.81 77.62 languages like Danish, German, English, Spanish, etc. does not differ much. Moreover, languages with phrasal segments and/or rich morphology like Arabic, Malay, Russian or Vietnamese have a high total entropy and thus can be expected to be more difficult to translate. This is confirmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) where scores range between 0 (worst) and 1 (best). Besides Korean (single references only), all languages were evaluated using 16 reference translations. The evaluation results in Table 3 show that closely related language pairs like Japanese-Korean or Portuguese-Brazilian can be translated very accurately, whereas translations into languages with high total entropy are of lower quality. 3 Multilingual Speech Translation Service (MSTS) The speech translation service1 can be accessed via ‘http://www.atr-trek.co.jp/contents html’ or using the QR code in Figure 4 that also illustrates the graphi"
C08-3006,2007.iwslt-1.15,1,0.895955,"Missing"
C08-3006,P02-1040,0,0.0823612,"65.83 69.61 75.39 72.17 72.82 69.00 70.70 – 89.73 67.33 72.19 69.14 68.32 64.55 62.91 70.81 77.62 languages like Danish, German, English, Spanish, etc. does not differ much. Moreover, languages with phrasal segments and/or rich morphology like Arabic, Malay, Russian or Vietnamese have a high total entropy and thus can be expected to be more difficult to translate. This is confirmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) where scores range between 0 (worst) and 1 (best). Besides Korean (single references only), all languages were evaluated using 16 reference translations. The evaluation results in Table 3 show that closely related language pairs like Japanese-Korean or Portuguese-Brazilian can be translated very accurately, whereas translations into languages with high total entropy are of lower quality. 3 Multilingual Speech Translation Service (MSTS) The speech translation service1 can be accessed via ‘http://www.atr-trek.co.jp/contents html’ or using the QR code in Fig"
C08-3006,2008.iwslt-evaluation.11,1,\N,Missing
C14-1106,D10-1092,0,0.0603452,"big, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-bests from each MT system and features described in Section 2.3. We use 1-grams to 3-grams as n-gram features. 2 3 http://code.google.com/p/nile/ http://code.google.com/p/egret-parser/ 1128 System pbmt hiero f2s BLEU(dev) Original LM applied 0"
C14-1106,P07-2045,0,0.0192459,"effectiveness of our method by performing a manual evaluation over three translation systems, two translation directions, and two evaluation measures. 4.1 Experiment Setup For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as descri"
C14-1106,C04-1072,0,0.0264719,"2 Discriminative Language Models In this section, we first introduce the discriminative LM used in our method. As a target for our analysis, ˆ1, . . . , E ˆ K } of an MT system, and we have input sentences F = {F1 , . . . , FK }, n-best outputs Eˆ = {E reference translations R = {R1 , . . . , RK }. Discriminative LMs define feature vectors ϕ(Ei ) for each ˆ k = {E1 , E2 , . . . , EI }, and calculate inner products w · ϕ(Ei ) as scores. candidate in E To train the weight vector w, we first calculate evaluation scores of all candidates using a sentencelevel evaluation measure EV such as BLEU+1 (Lin and Och, 2004) given the reference sentence Rk . 1125 We choose the sentence with the highest evaluation EV as an oracle Ek∗ . Oracles are chosen for each n-best, and we train w so that the oracle’s score becomes higher than the other candidates. 2.1 Structured Perceptron While there are a number of methods for training discriminative LMs, we follow Roark et al. (2007) in using the structured perceptron as a simple and effective method for LM training. The structured perceptron is a widely used on-line learning method that examines one training instance and updates the weight vector using the difference bet"
C14-1106,P13-4016,1,0.814008,"tion systems, and choose representative n-grams using the proposed method. Then we examine the selected n-grams in context and then compare the result of this analysis. 4 Experiments We evaluate the effectiveness of our method by performing a manual evaluation over three translation systems, two translation directions, and two evaluation measures. 4.1 Experiment Setup For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et"
C14-1106,J03-1002,0,0.00792232,"Setup For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-b"
C14-1106,P03-1021,0,0.0624679,", 2011) as a corpus. The size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-bests from each MT system and features described in Section 2.3. We us"
C14-1106,P02-1040,0,0.091993,"he size of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system built using Moses (Koehn et al., 2007). The f2s system is built using Nile2 for making word alignments, and syntax trees generated with Egret3 . pbmt and hiero are built using GIZA++ (Och and Ney, 2003) for word alignments. Each system is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al., 2010) as additional metric for training the discriminative LM. For training discriminative LMs, our method uses the structured perceptron with 100 iterations and FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the range 10−6 -10−2 to give the highest performance on the KFTT test data. LMs are trained using 500-bests from each MT system and features described in Section 2.3. We use 1-grams to 3-grams as n-gram fea"
C14-1106,J11-4002,0,0.124693,"analyze a large number of translations to get an overall grasp of the system’s error trends. In addition, many sentences will contain no errors, or only errors from the long tail that are not representative of the system as a whole. On the other hand, if we are able to detect and rank important errors automatically, we will likely be able to find representative errors of the SMT system more efficiently. Previous work has proposed methods for automatic error analysis of MT systems based on automatically separating errors into classes and sorting these classes by frequency (Vilar et al., 2006; Popovic and Ney, 2011). These classes cover common mistakes of MT systems, e.g. conjugation, reordering, word deletion, and insertion. This makes it possible to view overall error trends, but when the goal of analysis is to identify errors to make some concrete improvement to the system, it is often necessary to perform a more focused analysis, looking at actual errors made by a particular language pair or system. We show examples of errors types that are informative, but are language- or task-specific, and not covered by previous methods in Figure 1. In this example, the type given by more standard error typologie"
C14-1106,P09-1054,0,0.0258248,"ge or we reach a fixed iteration limit N . We show the above procedure in Algorithm 1. Algorithm 1 Structured perceptron training of the discriminative LM for n = 1 to N do ˆ ∈ Eˆ do for all E ∗ E ← arg max EV (E) ˆ E∈E ˆ ← arg max w · ϕ(E) E ˆ E∈E ˆ w ← w + ϕ(E ∗ ) − ϕ(E) end for end for 2.2 Learning Sparse Discriminative LMs While the structured perceptron is a simple and effective method for learning discriminative LMs, it also has no bias towards reducing the number of features used in the model. However, if we add a bias towards learning smaller models, we can keep only salient features (Tsuruoka et al., 2009). In our work, we use L1 regularization to add this bias. L1 regularization gives a penalty to w pro∑ portional to the L1 norm ∥w∥1 = i |wi |, pushing a large number of elements in w to 0, so ineffective features are removed from the model. To train L1 regularized discriminative LMs, we use the forward-backward splitting (FOBOS) algorithm proposed by Duchi and Singer (2009). FOBOS splits update and regularization, and lazily calculates the regularization upon using the weight to improve efficiency. 2.3 Features of Discriminative LMs In the LM, we used the following three features: 1. System sc"
C14-1106,vilar-etal-2006-error,0,0.0742926,"Missing"
C14-1161,W13-4016,0,0.324973,"f framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that applying reinforcement learning is effective for construction of cooperative persuasive dialogue systems which use framing. 1 Introduction With the basic technology supporting dialogue systems maturing, there has been more interest in recent years about dialogue systems that move beyond the traditional task-based or chatter bot frameworks. In particular there has been increasing interest in dialogue systems that engage in persuasion or negotiation (Georgila and Traum, 2011; Georgila, 2013; Paruchuri et al., 2009; Heeman, 2009; Mazzotta and de Rosis, 2006; Mazzotta et al., 2007; Nguyen et al., 2007; Guerini et al., 2003). We concern ourselves with cooperative persuasive dialogue systems (Hiraoka et al., 2013), which try to satisfy both the user and system goals. For these types of systems, creating a system policy that both has persuasive power and is able to ensure that the user is satisfied is the key to the system’s success. In recent years, reinforcement learning has gained much attention in the dialogue research community as an approach for automatically learning optimal d"
C14-1161,C10-1086,0,0.0268802,"a et al., 2014). In this paper, dialogue features for the predictive model are calculated at each turn. In addition, persuasion success and user satisfaction are successively calculated at each turn. In contrast, in previous research, the predictive model was constructed with dialogue features calculated at end of the dialogue. Therefore, it is not guaranteed that the predictive model estimates appropriate persuasion success and user satisfaction at each turn. Another reason is that the simulator is not sufficiently accurate to use for reflecting real user’s behavior. Compared to other works (Meguro et al., 2010; Misu et al., 2012), we are using a relatively small sized corpus for training the user simulator. Therefore, the user simulator cannot be trained to accurately imitate real user behavior. Improving the user simulator is an important challenge for future work. 7 Related work There are a number of related works that apply reinforcement learning to persuasion and negotiation dialogue. Georgila and Traum (2011) apply reinforcement learning to negotiation dialogue using user simulators divided into three types representing individualist, collectivist, and altruist. Dialogue between a florist and"
C14-1161,W12-1611,0,0.0952795,"his paper, dialogue features for the predictive model are calculated at each turn. In addition, persuasion success and user satisfaction are successively calculated at each turn. In contrast, in previous research, the predictive model was constructed with dialogue features calculated at end of the dialogue. Therefore, it is not guaranteed that the predictive model estimates appropriate persuasion success and user satisfaction at each turn. Another reason is that the simulator is not sufficiently accurate to use for reflecting real user’s behavior. Compared to other works (Meguro et al., 2010; Misu et al., 2012), we are using a relatively small sized corpus for training the user simulator. Therefore, the user simulator cannot be trained to accurately imitate real user behavior. Improving the user simulator is an important challenge for future work. 7 Related work There are a number of related works that apply reinforcement learning to persuasion and negotiation dialogue. Georgila and Traum (2011) apply reinforcement learning to negotiation dialogue using user simulators divided into three types representing individualist, collectivist, and altruist. Dialogue between a florist and a grocer are assumed"
C14-1161,W12-1639,0,0.024996,"dialogue acts is added at each turn. 3 Preliminary experiments indicated that the user behaved differently depending on the first selection of the camera, thus we introduce this variable to the user simulator. 4 We also optimized the policy in the case where the reward (Equation (3)) is given only when dialogue is closed. However, the convergence of the learning was much longer, and the performance was relatively bad. 5 Originally, there are more dialogue features for the predictive model. However as in previous research, we choose significant dialogue features by step-wise feature selection (Terrell and Bilge, 2012). 1710 Table 4: Features for calculating reward. These features are also used as the system belief state. Satuser P Ssys N Table 5: System framing. Pos represents positive framing and Neg represents negative framing. A, B, C, D, E represent camera names. Pos A Neg A Frequency of system commisive Frequency of system question Total time Calt (for each 6 cameras) Salt (for each 6 cameras) System and user current GPF System and user previous GPF System framing Pos B Neg B Pos C Neg C Pos D Neg D Pos E Neg E None Table 6: System action. <None, ReleaseTurn&gt; <Pos A, Inform&gt; <Neg A, Inform&gt; <Pos B, An"
D15-1250,N15-1027,0,0.0153685,"sh training data from noise by maximize the conditional likelihood, L = log P (v = 1|C, ti ) + k P j=1 log P (v = 0|C, tik ). The normalization cost can be avoided by using p (ti |C) as an approximation of P (ti |C).2 1 If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word. 2 The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)’s method, are investigated by Andreas and Klein (2015). 3 Binarized NNJM In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classification problem by adding a variable v ∈ {0, 1} that stands for whether the current target word ti is correctly/wrongly produced in terms of source cona +(m−1)/2 text words saii −(m−1)/2 and target history words i−1 ti−n+1 ,   a +(m−1)/2 P v|saii −(m−1)/2 , ti−1 i−n+1 , ti . The BNNJM is learned by a feedforward neural network o with m + n inputs n ai +(m−1)/2 i−1 sai −(m−1)/2 , ti−n+1 ,"
D15-1250,D13-1106,0,0.0273384,"classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost dur"
D15-1250,P14-1129,0,0.0837573,"Missing"
D15-1250,E14-1003,0,0.0168384,"native to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to"
D15-1250,P07-2045,0,0.00974293,"9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples"
D15-1250,P02-1040,0,0.0928294,"tion task, as it did for the other translation tasks. We found that using the BNNJM instead of the NNJM on the JE task did improve translation quality significantly for infrequent words, but not for frequent words. First, we describe how we estimate translation quality for infrequent words. Suppose we have a test set S, a reference set R and a translation set T with I sentences, Si (1 ≤ i ≤ I) , Ri (1 ≤ i ≤ I) , Ti (1 ≤ i ≤ I) Ti contains J individual words, To (Wij ) is how many times Wij occurs in Ti and Ro (Wij ) is how many times Wij occurs in Ri . The general 1-gram translation accuracy (Papineni et al., 2002) is calculated as, I P J P Pg = i=1 j=1 min(To (Wij ),Ro (Wij )) I P J P To (Wij ) i=1 j=1 This general 1-gram translation accuracy does not distinguish word frequency. We use a modified 1-gram translation accuracy that weights infrequent words more heavily, I P J P Pc = i=1 j=1 min(To (Wij ),Ro (Wij ))· I P J P i=1 j=1 1 Occur Wij ( ) To (Wij ) where Occur (Wij ) is how many times Wij occurs in the whole reference set. Note Pc will not be 1 even in the case of completely accurate translations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word t"
D15-1250,C12-2104,0,0.0869208,"kes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. H"
D15-1250,D14-1003,0,0.0152779,"JM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive nor"
D15-1250,D13-1140,0,0.562022,"t of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. However, they also note that this self-normalization technique sacrifices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE). To remedy the problem of long training times in the context of NNLMs, Vaswani et al. (2013) used a method called noise contrastive estimation (NCE). Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise. This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is correct or not, as shown in Figure 1b. Because the BNNJM uses the current target word"
D15-1250,D11-1104,0,0.0257476,"g 68.2 68.4 0.29 JE Pc 4.15 4.30 3.6 Pg 61.2 61.7 0.81 FE Pc 6.70 6.86 2.4 Table 5: 1-gram precisions and improvements. grammatical features of Japanese and English are quite different.8 Wrong function word alignments will make noise sampling less effective and therefore lower the BNNJM performance for function word translations. Although wrong word alignments will also make noise sampling less effective for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM. 6 Wij ∈ W ords (Ti ) Pg 70.3 70.9 0.85 Related Work Xu et al. (2011) proposed a method to use binary classifiers to learn NNLMs. But they also used the current target word in the output, similarly to NCE. The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word information and processed in hidden layers. Mauser et al. (2009) presented discriminative lexicon models to predict target words. They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share features. In contrast, the BNNJM uses rea"
D15-1250,W06-0127,0,0.144311,"P align(sai ,ti 0 ) ti 00 ∈U (sai ) align(sai ,ti Experiments We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE), Japanese-toEnglish (JE) and French-to-English (FE) translation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The wo"
D15-1250,W04-3250,0,0.0992848,"(E) and time (T) in minutes per epoch for each task. Base NNJM BNNJM UPD TPD UPD TPD CE 32.95 34.36+ 34.60+ 32.89 35.05+* JE 30.13 31.30+ 31.50+ 30.04 31.42+ FE 24.56 24.68 24.80 24.50 25.84+* Table 3: Translation examples. Here, S: source; R: reference; T1 uses NNJM; T2 uses BNNJM. 该− &gt;the 移动− &gt;mobile 持续− &gt;continues 到− &gt;to SUM 该− &gt;this 移动− &gt;movement null− &gt;is 持续− &gt;continued 到− &gt;until SUM Table 2: Translation results. The symbol + and * represent significant differences at the p &lt; 0.01 level against Base and NNJM+UPD, respectively. Significance tests were conducted using bootstrap resampling (Koehn, 2004). the whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive. However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples. 5.2 Results and Discussion Table 1 shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine.7 Translation results are shown in Table 2. We can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the t"
D15-1250,D09-1022,0,0.0791321,"Missing"
D15-1250,P03-1021,0,0.105131,"Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples for NCE was set to be 100. For the BNNJM, we used only one negative example for each positive example in each training epoch, as the BNNJM needs to calculate 3 00 ) where align (sai , ti 0 ) is how many times ti 0 is aligned to sai in the parallel corpus. Note that ti could be unaligned, in"
D16-1162,2011.iwslt-evaluation.18,0,0.0073927,"4.3 Hybrid Lexicons Handmade lexicons have broad coverage of words but their probabilities might not be as accurate as the Data Train Dev Test Corpus Sentence BTEC KFTT BTEC KFTT BTEC KFTT 464K 377K 510 1160 508 1169 Tokens En Ja 3.60M 4.97M 7.77M 8.04M 3.8K 5.3K 24.3K 26.8K 3.8K 5.5K 26.0K 28.4K Table 1: Corpus details. learned ones, particularly if the automatic lexicon is constructed on in-domain data. Thus, we also test a hybrid method where we use the handmade lexicons to complement the automatically learned lexicon.2 3 Specifically, inspired by phrase table fill-up used in PBMT systems (Bisazza et al., 2011), we use the probability of the automatically learned lexicons pl,a by default, and fall back to the handmade lexicons pl,m only for uncovered words: { pl,a (e|f ) if f is covered (6) pl,h (e|f ) = pl,m (e|f ) otherwise 5 Experiment & Result In this section, we describe experiments we use to evaluate our proposed methods. 5.1 Settings Dataset: We perform experiments on two widelyused tasks for the English-to-Japanese language pair: KFTT (Neubig, 2011) and BTEC (Kikui et al., 2003). KFTT is a collection of Wikipedia article about city of Kyoto and BTEC is a travel conversation corpus. BTEC is a"
D16-1162,J93-2003,0,0.140989,"ty with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.1 1 I come from Tunisia. チュニジア の 出身です。 Chunisia no shusshindesu. System: (I’m from Tunisia.) ノルウェー の 出身です。 Noruue- no shusshindesu. (I’m from Norway.) Figure 1: An example of a mistake made by NMT on low-frequency content words. Introduction Neural machine translation (NMT, §2; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016). One feature of NMT systems is that they treat each word in the vocabulary as a vector of 1 Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html continuous-valued numbers. This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; Koehn"
D16-1162,J07-2003,0,0.149615,"Missing"
D16-1162,W14-4012,0,0.085105,"Missing"
D16-1162,P16-1160,0,0.0188031,"the original HMM alignments gathered from the training corpus. Basically, this method is a specific version of our bias method that gives some of the vocabulary a bias of negative infinity and all other vocabulary a uniform distribution. Our method improves over this by considering actual translation probabilities, and also considering the attention vector when deciding how to combine these probabilities. Finally, there have been a number of recent works that improve accuracy of low-frequency words using character-based translation models (Ling et al., 2015; Costa-Juss`a and Fonollosa, 2016; Chung et al., 2016). However, Luong and Manning (2016) have found that even when using character-based models, incorporating information about words allows for gains in translation accuracy, and it is likely that our lexicon-based method could result in improvements in these hybrid systems as well. 8 Conclusion & Future Work In this paper, we have proposed a method to incorporate discrete probabilistic lexicons into NMT systems to solve the difficulties that NMT systems have demonstrated with low-frequency words. As a result, we achieved substantial increases in BLEU (2.0-2.3) and NIST (0.13-0.44) scores, and ob"
D16-1162,P16-2058,0,0.0238357,"Missing"
D16-1162,N13-1073,0,0.110911,"strap the learning of the NMT system, allowing it to approach an appropriate answer in a more timely fashion.8 It is also interesting to examine the alignment vectors produced by the baseline and proposed meth8 Note that these gains are despite the fact that one iteration of the proposed method takes a longer (167 minutes for attn vs. 275 minutes for auto-bias) due to the necessity to calculate and use the lexical probability matrix for each sentence. It also takes an additional 297 minutes to train the lexicon with GIZA++, but this can be greatly reduced with more efficient training methods (Dyer et al., 2013). (a) BTEC Lexicon auto man hyb BLEU bias linear 48.31 49.74∗ 47.97 49.08 51.04† † 50.34 49.27 NIST linear 5.98 6.11 5.90 ∗ 6.03 6.14† ∗ 6.10 5.94 bias (b) KFTT Lexicon auto man hyb BLEU bias linear 20.86 † 23.20 18.19 20.78 20.88 † 22.80 20.33 NIST linear 5.15 5.59† 4.61 5.12 5.11 † 5.55 5.03 bias Table 4: A comparison of the bias and linear lexicon integration methods on the automatic, manual, and hybrid lexicons. The first line without lexicon is the traditional attentional NMT. ods, a visualization of which we show in Figure 3. For this sentence, the outputs of both methods were both ident"
D16-1162,P16-1154,0,0.140543,"(·) and the lexicon probability pl (·). We will call this the linear method, and define it as follows:  po (ei |F, ei−1 1 )=  pl (ei = 1|F, ei−1 pm (e = 1|F, ei−1 [ ] 1 ) 1 ) λ   .. .. ,   . . 1−λ i−1 i−1 pl (ei = |Ve ||F, e1 ) pm (e = |Ve ||F, e1 ) where λ is an interpolation coefficient that is the result of the sigmoid function λ = sig(x) = 1+e1−x . x is a learnable parameter, and the sigmoid function ensures that the final interpolation level falls between 0 and 1. We choose x = 0 (λ = 0.5) at the beginning of training. This notation is partly inspired by Allamanis et al. (2016) and Gu et al. (2016) who use linear interpolation to merge a standard attentional model with a “copy” operator that copies a source word as-is into the target sentence. The main difference is that they use this to copy words into the output while our method uses it to influence the probabilities of all target words. 4 Constructing Lexicon Probabilities In the previous section, we have defined some ways to use predictive probabilities pl (ei |F, ei−1 1 ) based on word-to-word lexical probabilities pl (e|f ). Next, we define three ways to construct these lexical probabilities using automatically learned lexicons, h"
D16-1162,P16-1014,0,0.023798,"Missing"
D16-1162,P15-1001,0,0.192362,"inement, with Model 1 relying solely on these lexical probabilities, and latter IBM models (Models 2, 3, 4, 5) introducing more sophisticated models of fertility and relative alignment. Even though IBM models also occasionally have problems when dealing with the rare words (e.g. “garbage collecting” effects (Liang et al., 2006)), traditional SMT systems generally achieve better translation accuracies of lowfrequency words than NMT systems (Sutskever et al., 2014), indicating that these problems are less prominent than they are in NMT. Note that in many cases, NMT limits the target vocabulary (Jean et al., 2015) for training speed or memory constraints, resulting in rare words not being covered by the NMT vocabulary VE . Accordingly, we allocate the remaining probability assigned by the lexicon to the unknown word symbol ⟨unk⟩: ∑ pl,a (e = ⟨unk⟩|f ) = 1 − pl,a (e = i|f ). (5) i∈Ve 4.2 Manual Lexicons In addition, for many language pairs, broadcoverage handmade dictionaries exist, and it is desirable that we be able to use the information included in them as well. Unlike automatically learned lexicons, however, handmade dictionaries generally do not contain translation probabilities. To construct the"
D16-1162,D13-1176,0,0.0542733,"which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.1 1 I come from Tunisia. チュニジア の 出身です。 Chunisia no shusshindesu. System: (I’m from Tunisia.) ノルウェー の 出身です。 Noruue- no shusshindesu. (I’m from Norway.) Figure 1: An example of a mistake made by NMT on low-frequency content words. Introduction Neural machine translation (NMT, §2; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016). One feature of NMT systems is that they treat each word in the vocabulary as a vector of 1 Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html continuous-valued numbers. This i"
D16-1162,N03-1017,0,0.278912,"Missing"
D16-1162,W04-3250,0,0.0368593,"iro contains 104K distinct word-to-word translation entries. The third lexicon (hyb) is built by combining the first and second lexicon with the hybrid method of §4.3. Evaluation: We use standard single reference BLEU-4 (Papineni et al., 2002) to evaluate the translation performance. Additionally, we also use NIST (Doddington, 2002), which is a measure that puts a particular focus on low-frequency word strings, and thus is sensitive to the low-frequency words we are focusing on in this paper. We measure the statistical significant differences between systems using paired bootstrap resampling (Koehn, 2004) with 10,000 iterations and measure statistical significance at the p &lt; 0.05 and p &lt; 0.10 levels. Additionally, we also calculate the recall of rare words from the references. We define “rare words” as words that appear less than eight times in the target training corpus or references, and measure the percentage of time they are recovered by each translation system. 10 5 attn auto-bias hyb-bias 0 1000 2000 3000 time (minutes) 4000 Figure 2: Training curves for the baseline attn and the proposed bias method. with the auto or hyb lexicons, which empirically gave the best results, and perform a c"
D16-1162,N06-1014,0,0.0168164,"lgorithm. First in the expectation step, the algorithm estimates the expected count c(e|f ). In the maximiza1560 tion step, lexical probabilities are calculated by dividing the expected count by all possible counts: c(f, e) pl,a (e|f ) = ∑ , ˜) e˜ c(f, e The IBM models vary in level of refinement, with Model 1 relying solely on these lexical probabilities, and latter IBM models (Models 2, 3, 4, 5) introducing more sophisticated models of fertility and relative alignment. Even though IBM models also occasionally have problems when dealing with the rare words (e.g. “garbage collecting” effects (Liang et al., 2006)), traditional SMT systems generally achieve better translation accuracies of lowfrequency words than NMT systems (Sutskever et al., 2014), indicating that these problems are less prominent than they are in NMT. Note that in many cases, NMT limits the target vocabulary (Jean et al., 2015) for training speed or memory constraints, resulting in rare words not being covered by the NMT vocabulary VE . Accordingly, we allocate the remaining probability assigned by the lexicon to the unknown word symbol ⟨unk⟩: ∑ pl,a (e = ⟨unk⟩|f ) = 1 − pl,a (e = i|f ). (5) i∈Ve 4.2 Manual Lexicons In addition, for"
D16-1162,P16-1100,0,0.022818,"Missing"
D16-1162,D15-1166,0,0.0679155,"Missing"
D16-1162,P15-1002,0,0.73991,"Chunisia no shusshindesu. System: (I’m from Tunisia.) ノルウェー の 出身です。 Noruue- no shusshindesu. (I’m from Norway.) Figure 1: An example of a mistake made by NMT on low-frequency content words. Introduction Neural machine translation (NMT, §2; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016). One feature of NMT systems is that they treat each word in the vocabulary as a vector of 1 Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html continuous-valued numbers. This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; Koehn et al. (2003)), which represent translations as discrete pairs of word strings in the source and target languages. The use of continuous representations is a major advantage, allowing NMT to share statistical power between similar words (e.g. “do"
D16-1162,J93-2004,0,0.0712609,"Missing"
D16-1162,P16-2021,0,0.0327462,"the linear approach of §3.2.2, but are only applicable to words that can be copied asis into the target language. In fact, these models can be thought of as a subclass of the proposed approach that use a lexicon that assigns a all its probability to target words that are the same as the source. On the other hand, while we are simply using a static interpolation coefficient λ, these works generally have a more sophisticated method for choosing the interpolation between the standard and “copy” models. Incorporating these into our linear method is a promising avenue for future work. In addition Mi et al. (2016) have also recently proposed a similar approach by limiting the number of vocabulary being predicted by each batch or sentence. This vocabulary is made by considering the original HMM alignments gathered from the training corpus. Basically, this method is a specific version of our bias method that gives some of the vocabulary a bias of negative infinity and all other vocabulary a uniform distribution. Our method improves over this by considering actual translation probabilities, and also considering the attention vector when deciding how to combine these probabilities. Finally, there have been"
D16-1162,P11-2093,1,0.34484,"Missing"
D16-1162,P13-4016,1,0.790627,"Missing"
D16-1162,J03-1002,0,0.0763353,"of all target words. 4 Constructing Lexicon Probabilities In the previous section, we have defined some ways to use predictive probabilities pl (ei |F, ei−1 1 ) based on word-to-word lexical probabilities pl (e|f ). Next, we define three ways to construct these lexical probabilities using automatically learned lexicons, handmade lexicons, or a combination of both. 4.1 Automatically Learned Lexicons In traditional SMT systems, lexical translation probabilities are generally learned directly from parallel data in an unsupervised fashion using a model such as the IBM models (Brown et al., 1993; Och and Ney, 2003). These models can be used to estimate the alignments and lexical translation probabilities pl (e|f ) between the tokens of the two languages using the expectation maximization (EM) algorithm. First in the expectation step, the algorithm estimates the expected count c(e|f ). In the maximiza1560 tion step, lexical probabilities are calculated by dividing the expected count by all possible counts: c(f, e) pl,a (e|f ) = ∑ , ˜) e˜ c(f, e The IBM models vary in level of refinement, with Model 1 relying solely on these lexical probabilities, and latter IBM models (Models 2, 3, 4, 5) introducing more"
D16-1162,P02-1040,0,0.096508,"ling 6 experiments. The first lexicon (auto) is built on the training data using the automatically learned lexicon method of §4.1 separately for both the BTEC and KFTT experiments. Automatic alignment is performed using GIZA++ (Och and Ney, 2003). The second lexicon (man) is built using the popular English-Japanese dictionary Eijiro7 with the manual lexicon method of §4.2. Eijiro contains 104K distinct word-to-word translation entries. The third lexicon (hyb) is built by combining the first and second lexicon with the hybrid method of §4.3. Evaluation: We use standard single reference BLEU-4 (Papineni et al., 2002) to evaluate the translation performance. Additionally, we also use NIST (Doddington, 2002), which is a measure that puts a particular focus on low-frequency word strings, and thus is sensitive to the low-frequency words we are focusing on in this paper. We measure the statistical significant differences between systems using paired bootstrap resampling (Koehn, 2004) with 10,000 iterations and measure statistical significance at the p &lt; 0.05 and p &lt; 0.10 levels. Additionally, we also calculate the recall of rare words from the references. We define “rare words” as words that appear less than e"
D16-1162,P16-1009,0,0.0439705,"desu. System: (I’m from Tunisia.) ノルウェー の 出身です。 Noruue- no shusshindesu. (I’m from Norway.) Figure 1: An example of a mistake made by NMT on low-frequency content words. Introduction Neural machine translation (NMT, §2; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016). One feature of NMT systems is that they treat each word in the vocabulary as a vector of 1 Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html continuous-valued numbers. This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; Koehn et al. (2003)), which represent translations as discrete pairs of word strings in the source and target languages. The use of continuous representations is a major advantage, allowing NMT to share statistical power between similar words (e.g. “dog” and “cat”) or contexts"
D16-1162,P07-2045,0,\N,Missing
D16-1263,W08-0336,0,0.0470765,"Missing"
D16-1263,N16-1109,1,0.622951,"pressed with a weighted finite-state transducer (WFST) framework represents the joint distribution of source acoustic features, phonemes and latent source words given the target words. Sampling of alignments is used to learn source words and their target translations, which are then used to improve transcription of the source audio they were learnt from. Importantly, the model assumes no prior lexicon or translation model. This method builds on work on phoneme translation modeling (Besacier et al., 2006; St¨uker et al., 2009; Stahlberg et al., 2012; Stahlberg et al., 2014; Adams et al., 2015; Duong et al., 2016), speech translation (Casacuberta et al., 2004; Matusov et al., 2005), computer-aided translation, (Brown et al., 1994; Vidal et al., 2006; Khadivi and Ney, 2008; Reddy and Rose, 2010; Pelemans et al., 2015), translation modeling from automatically transcribed 1 Code is available at https://github.com/oadams/latticetm. 2377 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2377–2382, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics speech (Paulik and Waibel, 2013), word segmentation and translation modeling (Chang e"
D16-1263,N09-1046,0,0.0684154,"Missing"
D16-1263,W08-0704,0,0.0500543,"Missing"
D16-1263,P11-2093,1,0.626336,"y their ability to improve phoneme recognition, measuring phoneme error rate (PER). Experimental setup We used less than 10 hours of English–Japanese data from the BTEC corpus (Takezawa et al., 2002), comprised of spoken utterances paired with textual translations. This allows us to assess the approach assuming quality acoustic models. We used acoustic models similar to Heck et al. (2015) to obtain source phoneme lattices. Gold phoneme transcriptions were obtained by transforming the text with pronunciation lexicons and, in the Japanese case, first segmenting the text into tokens using KyTea (Neubig et al., 2011). We run experiments in both directions: English– Japanese and Japanese–English (en–ja and ja–en), while comparing against three settings: the ASR 1best path uninformed by the model (ASR); a monolingual version of our model that is identical except without conditioning on the target side (Mono); and the model applied using the source language sentence as the target (Oracle). We tuned on the first 1,000 utterences (about 1 hour) of speech and trained on up to 9 hours of the English (en) Mono –ja Oracle ASR 22.1 Vague 17.7 18.5 17.2 Shifted 17.4 16.9 16.6 Poisson 17.3 17.2 16.8 Japanese (ja) Mon"
D16-1263,C10-1092,0,0.0680955,"Missing"
D16-1263,takezawa-etal-2002-toward,0,0.0482154,"ce constituting one block. To sample from WFSTs, we use forwardfiltering/backward-sampling (Scott, 2002; Neubig et al., 2012), creating forward probabilities using the forward algorithm for hidden Markov models before backward-sampling edges proportionally to the product of the forward probability and the edge weight.3 3 No Metropolis-Hastings rejection step was used. 2379 We evaluate the lexicon and translation model by their ability to improve phoneme recognition, measuring phoneme error rate (PER). Experimental setup We used less than 10 hours of English–Japanese data from the BTEC corpus (Takezawa et al., 2002), comprised of spoken utterances paired with textual translations. This allows us to assess the approach assuming quality acoustic models. We used acoustic models similar to Heck et al. (2015) to obtain source phoneme lattices. Gold phoneme transcriptions were obtained by transforming the text with pronunciation lexicons and, in the Japanese case, first segmenting the text into tokens using KyTea (Neubig et al., 2011). We run experiments in both directions: English– Japanese and Japanese–English (en–ja and ja–en), while comparing against three settings: the ASR 1best path uninformed by the mod"
D16-1263,1983.tc-1.13,0,0.746681,"Missing"
D16-1263,D15-1142,0,\N,Missing
D16-1263,W14-2201,1,\N,Missing
E14-4025,strapparava-valitutti-2004-wordnet,0,0.156175,"s that answer this very question, or more formally “given a particular emotion, what are the most prevalent events (or situations, contexts) that provoke it?”1 Information about these emotion-provoking events is potentially useful for emotion recognition (recognizing emotion based on events mentioned in a dialogue), response generation (providing an answer to emotion-related questions), and answering social-science related questions (discovering events that affect the emotion of a particular segment of the population). 1 This is in contrast to existing sentiment lexicons (Riloff et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Velikovich et al., 2010; Mohammad and Turney, 2013), which only record the sentiment orientation of particular words (such as “meet” or “friend”), which, while useful, are less directly connected to the emotions than the events themselves. 2 Manual Creation of Events In order to create a small but clean set of goldstandard data for each emotion, we first performed 128 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128–132, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational"
E14-4025,N10-1119,0,0.0291115,"mally “given a particular emotion, what are the most prevalent events (or situations, contexts) that provoke it?”1 Information about these emotion-provoking events is potentially useful for emotion recognition (recognizing emotion based on events mentioned in a dialogue), response generation (providing an answer to emotion-related questions), and answering social-science related questions (discovering events that affect the emotion of a particular segment of the population). 1 This is in contrast to existing sentiment lexicons (Riloff et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Velikovich et al., 2010; Mohammad and Turney, 2013), which only record the sentiment orientation of particular words (such as “meet” or “friend”), which, while useful, are less directly connected to the emotions than the events themselves. 2 Manual Creation of Events In order to create a small but clean set of goldstandard data for each emotion, we first performed 128 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128–132, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Emotions happiness sadness anger fear sur"
E14-4025,esuli-sebastiani-2006-sentiwordnet,0,0.0164046,"s very question, or more formally “given a particular emotion, what are the most prevalent events (or situations, contexts) that provoke it?”1 Information about these emotion-provoking events is potentially useful for emotion recognition (recognizing emotion based on events mentioned in a dialogue), response generation (providing an answer to emotion-related questions), and answering social-science related questions (discovering events that affect the emotion of a particular segment of the population). 1 This is in contrast to existing sentiment lexicons (Riloff et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Velikovich et al., 2010; Mohammad and Turney, 2013), which only record the sentiment orientation of particular words (such as “meet” or “friend”), which, while useful, are less directly connected to the emotions than the events themselves. 2 Manual Creation of Events In order to create a small but clean set of goldstandard data for each emotion, we first performed 128 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128–132, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Emotions happine"
E14-4025,P11-1038,0,0.0114375,"his section, we describe an experimental evaluation of the accuracy of automatic extraction of emotion-provoking events. 5.1 Experimental Setup We use Twitter3 as a source of data, as it is it provides a massive amount of information, and also because users tend to write about what they are doing as well as their thoughts, feelings and emotions. We use a data set that contains more than 30M English tweets posted during the course of six weeks in June and July of 2012. To remove noise, we perform a variety of preprocessing, removing emoticons and tags, normalizing using the scripts provided by Han and Baldwin (2011), and Han et al. (2012). CoreNLP4 was used to get the information about part-of-speech, syntactic parses, and lemmas. We prepared four systems for comparison. As a baseline, we use a method that only uses the original seed pattern mentioned in Section 3 to acquire emotion-provoking events. We also evaluate expansions to this method with clustering, with pattern expansion, and with both. We set a 10 iteration limit on the Espresso algorithm and after each iteration, we add the 20 3 2 http://www.twitter.com http://nlp.stanford.edu/software/ corenlp.shtml In the current work we did not allow anno"
E14-4025,D12-1039,0,0.0389339,"Missing"
E14-4025,P06-1015,0,0.0314114,"e shared by more than one person. It should be noted that this will not come anywhere close to covering the entirety of human emotion, but as each event is shared by at least two people in a relatively small sample, any attempt to create a comprehensive dictionary of emotion-provoking events should at least be able to cover the pairs in this collection. We show the most common three events for each emotion in Table 1. 3 3.1 Pattern Expansion Pattern expansion, or bootstrapping algorithms are widely used in the information extraction field (Ravichandran and Hovy, 2002). In particular Espresso (Pantel and Pennacchiotti, 2006) is known as a state-of-the-art pattern expansion algorithm widely used in acquiring relationships between entities. We omit the details of the algorithm for space concerns, but note that applying the algorithm to our proposed task is relatively straightforward, and allows us to acquire additional patterns that may be matched to improve the coverage over the single seed pattern. We do, however, make two changes to the algorithm. The first is that, as we are interested in extracting events instead of entities, we impose the previously mentioned restriction of one verb phrase and one noun phrase"
E14-4025,P02-1006,0,0.0415296,"ly, for each emotion we extract all the events that are shared by more than one person. It should be noted that this will not come anywhere close to covering the entirety of human emotion, but as each event is shared by at least two people in a relatively small sample, any attempt to create a comprehensive dictionary of emotion-provoking events should at least be able to cover the pairs in this collection. We show the most common three events for each emotion in Table 1. 3 3.1 Pattern Expansion Pattern expansion, or bootstrapping algorithms are widely used in the information extraction field (Ravichandran and Hovy, 2002). In particular Espresso (Pantel and Pennacchiotti, 2006) is known as a state-of-the-art pattern expansion algorithm widely used in acquiring relationships between entities. We omit the details of the algorithm for space concerns, but note that applying the algorithm to our proposed task is relatively straightforward, and allows us to acquire additional patterns that may be matched to improve the coverage over the single seed pattern. We do, however, make two changes to the algorithm. The first is that, as we are interested in extracting events instead of entities, we impose the previously men"
E14-4025,W03-0404,0,0.109919,"Missing"
E14-4025,C08-1111,0,0.219614,"ecognition. In this paper, we describe work on creating prevalence-ranked dictionaries of emotionprovoking events through both manual labor and automatic information extraction. To create a manual dictionary of events, we perform a survey asking 30 participants to describe events that caused them to feel a particular emotion, and manually cleaned and aggregated the results into a ranked list. Next, we propose several methods for extracting events automatically from large data from the Web, which will allow us to increase the coverage over the smaller manually created dictionary. We start with Tokuhisa et al. (2008)’s patterns as a baseline, and examine methods for improving precision and coverage through the use of seed expansion and clustering. Finally, we discuss evaluation measures for the proposed task, and perform an evaluation of the automatically extracted emotion-provoking events. The acquired events will be provided publicly upon acceptance of the paper. This paper is concerned with the discovery and aggregation of events that provoke a particular emotion in the person who experiences them, or emotion-provoking events. We first describe the creation of a small manually-constructed dictionary of"
I17-1016,W16-2206,0,0.0245607,"er- translation under-translation Reduce over-translation unrelated translation No difference 2 11 10 4 73 Table 10: Human evaluation results. Reranking PBMT Outputs with NMT We also did experiments that use the NMT score as an additional feature to rerank PBMT outputs (unique 1, 000-best list). The results are shown 159 reason why NMT is more likely to produce completely unrelated translations, over-translation or under-translation compared to traditional SMT. To relieve NMT of these problems, there are methods that modify the NMT neural network structure (Tu et al., 2016; Meng et al., 2016; Alkhouli et al., 2016) while we rerank NMT outputs by exploiting knowledge from traditional SMT. There are also existing methods that rerank NMT outputs by using target-bidirectional NMT models (Liu et al., 2016; Sennrich et al., 2016a). Their reranking method aims to overcome the issue of unbalanced accuracy in NMT outputs while our reranking method aims to solve the inadequacy problem of NMT. in Table 11. We also copy results of baseline PBMT and NMT from Table 4 for direct comparison. As we can see, using NMT to rerank PBMT outputs achieved improvements over the baseline PBMT system. However, when the baseline N"
I17-1016,D16-1162,1,0.935856,"ct There are a number of methods that combine the two paradigms to address their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT outputs, but do not impose hard alignment constraints like traditional SMT systems and therefore cannot effectively solve all over-translation or under-translation problems. In this paper, we propose a method that exploits an existing phrase-based translation model to compute the phrase-based decoding cost for a given NMT translation.1 That is, we force a phrase-based translation s"
I17-1016,J93-2003,0,0.0533469,"er all source words and does not provides exact mutually-exclusive word or phrase level alignments. As a result, it is known that attentional NMT systems make mistakes in over- or undertranslation (Cohn et al., 2016; Mi et al., 2016). 3 3.1 Phrase-based SMT In phrase-based SMT (Koehn et al., 2003), a phrase-based translation rule r includes a source phrase, a target phrase and a translation score S (r). Phrase-based translation rules can be extracted from the word-aligned training set and then used to translate new sentences. Word alignments for the training set can be obtained by IBM models (Brown et al., 1993). Phrase-based decoding uses a list of translation rules to translate source phrases in the input sentence and generate target phrases from left to right. A basic concept in phrase-based decoding is hypotheses. As shown in Figure 1, the hypothesis H1 consists of two rules r1 and r2 . The score of a hypothesis S (H) can be calculated as the product of the scores of all applied rules.3 An existing hypothesis can be expanded into a new hypothesis by applying a new rule. As shown in Figure 1, H1 can be expanded into H2 , H3 and H4 . H2 cannot be further expanded, because it covers all source words"
I17-1016,W15-5003,1,0.831286,"Eiichro Sumita1 Graham Neubig3,2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Japan 3 Language Technologies Institute, Carnegie Mellon University, USA jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp gneubig@cs.cmu.edu, s-nakamura@is.naist.jp Abstract There are a number of methods that combine the two paradigms to address their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT out"
I17-1016,J03-1002,0,0.0122746,"#Words #Sents #Words #Sents #Words #Vocab #Sents #Words #Sents #Words SOURCE TARGET 1.90M 52.2M 49.7M 113K 376K 3,003 67.6K 63.0K 2,169 46.8K 44.0K 1.99M 54.4M 60.4M 114K 137K 3,003 71.1K 81.1K 1.5K 27.1K 29.8K 954K 40.4M 37.2M 504K 288K 2K 77.5K 75.4K 2K 58.1K 55.5K 3.14M 104M 118M 273K 150K 2K 66.5K 74.6K 2K 70.6K 78.5K Table 3: Data sets. are both 512. We used Byte-pair encoding (BPE) (Sennrich et al., 2016b) and set the vocabulary size to be 50K. We used the Adam algorithm for optimization. To obtain a phrase-based translation rule table for our forced decoding algorithm, we used GIZA++ (Och and Ney, 2003) and grow-diagfinal-and heuristic to obtain symmetric word alignments for the training set. Then we extracted the rule table using Moses (Koehn et al., 2007). Experiments Settings We evaluated the proposed approach for Englishto-Chinese (en-zh), English-to-Japanese (en-ja), English-to-German (en-de) and English-to-French (en-fr) translation tasks. For the en-zh and enja tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 For the en-de and en-fr tasks, we used version 7 of the Europarl corpus as training data, WMT 2014 test sets as our devel"
I17-1016,W16-2323,0,0.404803,"opose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP rule table. To solve this problem, we propose"
I17-1016,P16-1162,0,0.763028,"opose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP rule table. To solve this problem, we propose"
I17-1016,W04-3250,0,0.0605453,"anese. We built attentional NMT systems with Lamtram7 . Word embedding size and hidden layer size 5.2 Results and Analysis Table 4 shows results of the phrase-based SMT system8 , the baseline NMT system, the lexicon integration method (Arthur et al., 2016) and the proposed reranking method. We tested three features for reranking: the NMT score Pn , the forced decoding score Sd and a word penalty (WP) feature, which is the length of the translation. The best NMT system and the systems that have no significant difference from the best NMT system at the p &lt; 0.05 level using bootstrap resampling (Koehn, 2004) are shown in bold font. As we can see, integrating lexical translation probabilities improved the baseline NMT system 5 Note that NTCIR-9 only contained a Chinese-to-English translation task, we used English as the source language in our experiments. In NTCIR-9, the development and test sets were both provided for the zh-en task while only the test set was provided for the en-ja task. We used the sentences from the NTCIR-8 en-ja and ja-en test sets as the development set in our experiments. 6 http://sourceforge.net/projects/mecab/files/ 7 https://github.com/neubig/lamtram 8 We used the defaul"
I17-1016,P16-1159,0,0.0418421,"Missing"
I17-1016,P07-2045,0,0.0120866,"4.4M 60.4M 114K 137K 3,003 71.1K 81.1K 1.5K 27.1K 29.8K 954K 40.4M 37.2M 504K 288K 2K 77.5K 75.4K 2K 58.1K 55.5K 3.14M 104M 118M 273K 150K 2K 66.5K 74.6K 2K 70.6K 78.5K Table 3: Data sets. are both 512. We used Byte-pair encoding (BPE) (Sennrich et al., 2016b) and set the vocabulary size to be 50K. We used the Adam algorithm for optimization. To obtain a phrase-based translation rule table for our forced decoding algorithm, we used GIZA++ (Och and Ney, 2003) and grow-diagfinal-and heuristic to obtain symmetric word alignments for the training set. Then we extracted the rule table using Moses (Koehn et al., 2007). Experiments Settings We evaluated the proposed approach for Englishto-Chinese (en-zh), English-to-Japanese (en-ja), English-to-German (en-de) and English-to-French (en-fr) translation tasks. For the en-zh and enja tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 For the en-de and en-fr tasks, we used version 7 of the Europarl corpus as training data, WMT 2014 test sets as our development sets and WMT 2015 test sets as our test sets. The detailed statistics for training, development and test sets are given in Table 3. The word segmentat"
I17-1016,E17-2058,0,0.093465,"Missing"
I17-1016,W17-3204,0,0.0413309,"nguage pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP rule table. To solve this problem, we propose a soft forced decoding algorithm, which is based on the standard phrase-based decoding algorithm and integrates new types of translation rules (deleting a source word or inserting a target word). The proposed forced decoding algorithm can always su"
I17-1016,P16-2049,0,0.0730172,"am Neubig3,2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Japan 3 Language Technologies Institute, Carnegie Mellon University, USA jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp gneubig@cs.cmu.edu, s-nakamura@is.naist.jp Abstract There are a number of methods that combine the two paradigms to address their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT outputs, but do not impose h"
I17-1016,N03-1017,0,0.218879,"phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP"
I17-1016,N16-1046,1,0.898579,"Missing"
I17-1016,P16-1008,0,0.0263944,"nder translation. both under- and over- translation under-translation Reduce over-translation unrelated translation No difference 2 11 10 4 73 Table 10: Human evaluation results. Reranking PBMT Outputs with NMT We also did experiments that use the NMT score as an additional feature to rerank PBMT outputs (unique 1, 000-best list). The results are shown 159 reason why NMT is more likely to produce completely unrelated translations, over-translation or under-translation compared to traditional SMT. To relieve NMT of these problems, there are methods that modify the NMT neural network structure (Tu et al., 2016; Meng et al., 2016; Alkhouli et al., 2016) while we rerank NMT outputs by exploiting knowledge from traditional SMT. There are also existing methods that rerank NMT outputs by using target-bidirectional NMT models (Liu et al., 2016; Sennrich et al., 2016a). Their reranking method aims to overcome the issue of unbalanced accuracy in NMT outputs while our reranking method aims to solve the inadequacy problem of NMT. in Table 11. We also copy results of baseline PBMT and NMT from Table 4 for direct comparison. As we can see, using NMT to rerank PBMT outputs achieved improvements over the baselin"
I17-1016,C16-1205,0,0.0455265,"both under- and over- translation under-translation Reduce over-translation unrelated translation No difference 2 11 10 4 73 Table 10: Human evaluation results. Reranking PBMT Outputs with NMT We also did experiments that use the NMT score as an additional feature to rerank PBMT outputs (unique 1, 000-best list). The results are shown 159 reason why NMT is more likely to produce completely unrelated translations, over-translation or under-translation compared to traditional SMT. To relieve NMT of these problems, there are methods that modify the NMT neural network structure (Tu et al., 2016; Meng et al., 2016; Alkhouli et al., 2016) while we rerank NMT outputs by exploiting knowledge from traditional SMT. There are also existing methods that rerank NMT outputs by using target-bidirectional NMT models (Liu et al., 2016; Sennrich et al., 2016a). Their reranking method aims to overcome the issue of unbalanced accuracy in NMT outputs while our reranking method aims to solve the inadequacy problem of NMT. in Table 11. We also copy results of baseline PBMT and NMT from Table 4 for direct comparison. As we can see, using NMT to rerank PBMT outputs achieved improvements over the baseline PBMT system. Howe"
I17-1016,W12-3158,0,0.0325086,"Missing"
I17-1016,D16-1096,0,0.0382651,"ling method to obtain a more diverse n-best list. We test the proposed method on English-toChinese, English-to-Japanese, English-to-German and English-to-French translation tasks, obtaining large improvements over a strong NMT baseline that already incorporates discrete lexicon features. 2 As we can see, NMT only learns an attention (alignment) distribution for each target word over all source words and does not provides exact mutually-exclusive word or phrase level alignments. As a result, it is known that attentional NMT systems make mistakes in over- or undertranslation (Cohn et al., 2016; Mi et al., 2016). 3 3.1 Phrase-based SMT In phrase-based SMT (Koehn et al., 2003), a phrase-based translation rule r includes a source phrase, a target phrase and a translation score S (r). Phrase-based translation rules can be extracted from the word-aligned training set and then used to translate new sentences. Word alignments for the training set can be obtained by IBM models (Brown et al., 1993). Phrase-based decoding uses a list of translation rules to translate source phrases in the input sentence and generate target phrases from left to right. A basic concept in phrase-based decoding is hypotheses. As"
I17-1016,P10-1049,0,0.157145,"to generate new hypotheses with phrase-based SMT, but instead use the phrase-based model to calculate scores for NMT output. In order to do so, we can perform forced decoding, which is very similar to the algorithm in the previous section but discards all partial hypotheses that do not match the NMT output. However, the NMT output is not limited by the phrase-based rule table, so there may be no decoding path that completely matches the NMT output when using only the phrase-based rules. To remedy this problem, inspired by previous work in forced decoding for training phrase-based SMT systems (Wuebker et al., 2010, 2012) we propose a soft forced decoding algorithm that can always successfully find a decoding path for a source sentence F and an NMT translation E. First, we introduce two new types of rules R1 and R2 . s (null → e) = unalign (e) |T | (6) where unalign (e) is how many times e is unaligned in T . One motivation for Equations 5 and 6 is that function words usually have high frequencies, but do not have as clear a correspondence with a word in the other language as content words. As a result, in the training set function words are more often unaligned than content words. As an example, Table"
I17-1016,D13-1112,0,0.0602895,"Missing"
I17-1016,P17-1139,0,0.0346417,"s their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT outputs, but do not impose hard alignment constraints like traditional SMT systems and therefore cannot effectively solve all over-translation or under-translation problems. In this paper, we propose a method that exploits an existing phrase-based translation model to compute the phrase-based decoding cost for a given NMT translation.1 That is, we force a phrase-based translation system to take in the source sentence and generate an NMT translation. The"
I17-1016,W06-0127,0,0.0264745,"evaluated the proposed approach for Englishto-Chinese (en-zh), English-to-Japanese (en-ja), English-to-German (en-de) and English-to-French (en-fr) translation tasks. For the en-zh and enja tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 For the en-de and en-fr tasks, we used version 7 of the Europarl corpus as training data, WMT 2014 test sets as our development sets and WMT 2015 test sets as our test sets. The detailed statistics for training, development and test sets are given in Table 3. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. We built attentional NMT systems with Lamtram7 . Word embedding size and hidden layer size 5.2 Results and Analysis Table 4 shows results of the phrase-based SMT system8 , the baseline NMT system, the lexicon integration method (Arthur et al., 2016) and the proposed reranking method. We tested three features for reranking: the NMT score Pn , the forced decoding score Sd and a word penalty (WP) feature, which is the length of the translation. The best NMT system and the systems that have no significant difference from the best NMT system at the p &lt; 0.05 lev"
I17-1044,P07-2045,0,0.0041814,"015) Proposed Att Enc-Dec + Unconst (exp) 40.9 (2σ = 4) Att Enc-Dec + Unconst (exp) 41.8 (2σ = 6) generation process usually follows the source sentence structure without many reordering process. 7.1 Dataset We used BTEC dataset (Kikui et al., 2003) and chose English-to-France and Indonesian-toEnglish parallel corpus. From BTEC dataset, we extracted 162318 sentences for training and 510 sentences for test data. Because there are no default development set, we randomly sampled 1000 sentences from training data for validation set. For all language pairs, we preprocessed our dataset using Moses (Koehn et al., 2007) tokenizer. For training, we replaced any word that appear less then twice with unknown (unk) symbol. In details, we keep 10105 words for French corpus, 8265 words for English corpus and 9577 words for Indonesian corpus. We only used sentence pairs where the source is no longer than 60 words in training phase. 7.2 7.3 Result Discussion Table 3 summarizes our experiment on proposed local attention models compared to baseline global attention model and local-m attention model (Luong et al., 2015). Generally, local monotonic attention had better result compared to global attention on both English"
I17-1044,D15-1166,0,0.143094,"chieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture. 1 Introduction End-to-end training is a newly emerging approach to sequence-to-sequence mapping tasks, that allows the model to directly learn the mapping between variable-length representation of different 431 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 431–440, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP used today has a “global” property (Bahdanau et al., 2014; Luong et al., 2015). Every time the decoder needs to predict the output given the previous output, it must compute a weighted summarization of the whole input sequence generated by the encoder states. This global property allows the decoder to address any parts of the source sequence at each step of the output generation and provides advantages in some cases like machine translation tasks. Specifically, when the source and the target languages have different sentence structures and the last part of the target sequence may depend on the first part of the source sequence. However, although the global attention mec"
I17-1044,D14-1179,0,0.0296084,"Missing"
I17-1044,1983.tc-1.13,0,0.568509,"Missing"
I17-1092,W03-1601,0,0.0435491,"lection of System Actions User Action Elaborateness/Indirectness Generator Core Statement Knowledge Integration Subject/Predicate/Object Related RDF Triplets Knowledge Base Figure 1: Partial architecture of the KRISTINA system, enhanced by the proposed algorithm. tences from a more structured representation. With regard to surface realisation, one characteristic of good generators is their ability to provide variation in the generated sentences, which has been explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this research area certainly provides important insights to the generation of elaborateness, they need to be considered wi"
I17-1092,P98-1116,0,0.0767059,"s User Action Elaborateness/Indirectness Generator Core Statement Knowledge Integration Subject/Predicate/Object Related RDF Triplets Knowledge Base Figure 1: Partial architecture of the KRISTINA system, enhanced by the proposed algorithm. tences from a more structured representation. With regard to surface realisation, one characteristic of good generators is their ability to provide variation in the generated sentences, which has been explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this research area certainly provides important insights to the generation of elaborateness, they need to be considered with respect to the peculiariti"
I17-1092,H05-1042,0,0.0533774,"explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this research area certainly provides important insights to the generation of elaborateness, they need to be considered with respect to the peculiarities of dialogue. Instead of providing an overview over the most important information in a larger amount of data, the goal of our work is to augment an already determined piece of information with relevant further information. Hence, content selection is more concerned with filtering information, while our approach focuses on adding information. Related Work Adaptive DMs can be beneficial to the user experience (Ultes et al., 2015; Bertr"
I17-1092,W16-3610,1,0.828813,"their preferences in a conversation is demanding work. Approaches to the automatic generation of system actions, such as (Kadlec et al., 2015), have been presented to facilitate that process. However, those approaches often consider only system actions that are necessary from a functional point of view. There is no variety of system actions produced that would enable the DM to adapt to specific users characteristics or preferences. However, automatically generating variants of system actions can greatly increase the adaptability of a DM and thereby improve the user experience. Studies (e.g. (Miehle et al., 2016; Pragst et al., 2017)) have shown that elaborateness and indirectness can be useful in adaptive DM. Here, elaborateness refers to the amount of additional information provided to the user and the level of indirectness describes how concretely information is addressed by a speaker. We have proposed the automatic generation of elaborateness and indirectness in (Pragst et al., 2016). In this work, we introduce an algorithm that, given a core statement on a semantic level, automatically creates more elaborated or indirect versions of that statement by retrieving semantic content from a knowledge"
I17-1092,W03-1016,0,0.0849349,"sentences, which has been explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this research area certainly provides important insights to the generation of elaborateness, they need to be considered with respect to the peculiarities of dialogue. Instead of providing an overview over the most important information in a larger amount of data, the goal of our work is to augment an already determined piece of information with relevant further information. Hence, content selection is more concerned with filtering information, while our approach focuses on adding information. Related Work Adaptive DMs can be beneficial to the user experienc"
I17-1092,W16-5506,1,0.794544,"dapt to specific users characteristics or preferences. However, automatically generating variants of system actions can greatly increase the adaptability of a DM and thereby improve the user experience. Studies (e.g. (Miehle et al., 2016; Pragst et al., 2017)) have shown that elaborateness and indirectness can be useful in adaptive DM. Here, elaborateness refers to the amount of additional information provided to the user and the level of indirectness describes how concretely information is addressed by a speaker. We have proposed the automatic generation of elaborateness and indirectness in (Pragst et al., 2016). In this work, we introduce an algorithm that, given a core statement on a semantic level, automatically creates more elaborated or indirect versions of that statement by retrieving semantic content from a knowledge base (KB) and assessing its relevance to the original In a dialogue system, the dialogue manager selects one of several system actions and thereby determines the system’s behaviour. Defining all possible system actions in a dialogue system by hand is a tedious work. While efforts have been made to automatically generate such system actions, those approaches are mostly focused on p"
I17-1092,W15-4649,1,0.912882,"show that the results of our algorithm are mostly perceived similarly to human generated elaborateness and indirectness and can be used to adapt a conversation to the current user and situation. We also discuss where the results of our algorithm are still lacking and how this could be improved: Taking into account the conversation topic as well as the culture of the user is likely to have beneficial effect on the user’s perception. 1 Introduction In a dialogue system (DS), the dialogue manager (DM) is responsible for choosing the system’s contribution to a conversation. Several studies (e.g. (Ultes et al., 2015; Bertrand et al., 2011; 915 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 915–925, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP statement. Additionally, we ascertain that elaborateness and indirectness are suitable options for providing adaptability to the DM, and that our automatically generated system actions are mostly perceived similarly compared to human instances of elaborated and indirect statements. We further examine the circumstances under which the perception of automatically generated system actions deviates from hum"
I17-1092,D15-1199,0,0.0128131,"ction 5. Finally, we draw a conclusion in Section 6. 2 Dialogue Manager System Action Language Generation Selection of System Actions User Action Elaborateness/Indirectness Generator Core Statement Knowledge Integration Subject/Predicate/Object Related RDF Triplets Knowledge Base Figure 1: Partial architecture of the KRISTINA system, enhanced by the proposed algorithm. tences from a more structured representation. With regard to surface realisation, one characteristic of good generators is their ability to provide variation in the generated sentences, which has been explored, among others, by Wen et al. (2015). With a similar goal, efforts towards the paraphrasing of sentences have been made (e.g. (Kozlowski et al., 2003; Langkilde and Knight, 1998)). Those approaches provide variation at the word level and preserve the semantic content of a sentence. They are complemented by our approach that focuses on variations of the semantic content of a system action. The content selection task is concerned with choosing relevant information that is to be communicated in the generated text, often with the goal of creating summaries (e.g. (Duboue and McKeown, 2003; Barzilay and Lapata, 2005)). While this rese"
I17-1092,P14-1004,0,0.0151015,"13) or emotion (Andr´e et al., 2004; Gnjatovi´c and R¨osner, 2008; Pittermann and Pittermann, 2007). Komatani et al. (2005) use the amount of information presented as adaptation mechanism to the user’s knowledge and the degree of urgency. Such architectures provide the decision making process necessary for choosing the best suited system action. However, they depend on the availability of suitable system actions to perform optimally. To facilitate the process of defining system actions, efforts have been made to model dialogues automatically, e.g (Beveridge and Fox, 2006; Kadlec et al., 2015; Zhai and Williams, 2014; Niraula et al., 2014). Those approaches are mostly focused on functional system behaviour. Only system actions that are necessary to solve a task are defined, limiting the possibilities for adaptation. Our goal is to generate variants of system actions that address the same functionality, and thereby increase the adaptability. Our efforts to generate variants of system actions is paralleled by a number of tasks in the area of natural language generation. Natural language generators produce human-readable sen3 System Architecture We embed our approach to the generation of elaborateness and in"
I17-1092,C98-1112,0,\N,Missing
itahashi-etal-2006-oriental,campbell-2000-cocosda,0,\N,Missing
kuwabara-etal-2002-present,maekawa-etal-2000-spontaneous,0,\N,Missing
kuwabara-etal-2002-present,nakamura-etal-2000-acoustical,1,\N,Missing
L16-1314,bazillon-etal-2008-manual,0,0.0834192,"Missing"
L16-1314,D14-1172,0,0.0283067,"ther differences are caused by transcriber characteristics or by experimental settings. More generally, our experiments involve “random” factors that are difficult to control for, and that potentially have a significant influence on our observations. In fact, this is a common problem in user studies. Recently, linear mixed-effects models 2 www.msperber.com/research/lrec-iterative-gui (short: mixed models) have become popular as a convenient way of dealing with such situations. For instance, mixed models have been used for error analysis in ASR (Goldwater et al., 2010) and machine translation (Federico et al., 2014), and for analysis of post-editing for translation (Green et al., 2013). Mixed models are specified by the following components: • Response variable: The central quantity for which we wish to determine how it is influenced by other measured covariates. In our experiments, this will be the post-correction error rate or the transcription time. • Fixed effects: Numerical or categorical attributes that influence the response variable in a meaningful way. In this paper, we assume a linear relationship. In the case of categorical variables, the assumption is that the observations include all values"
L16-1314,2012.iwslt-papers.10,1,0.873966,"Missing"
L16-1314,N10-1024,0,0.169441,"Missing"
L16-1314,Q14-1014,1,0.861183,"Our goal in this paper is to design a computer-assisted transcription user interface such that the outcome quality is optimized while avoiding unnecessary effort. The key interface feature we investigate is support for iterative transcription. This term is borrowed from iterative human computation processes (Little et al., 2010), in which humans solve tasks by improving upon a previously obtained solution. We consider computer-assisted transcription performed in an efficient segment-by-segment fashion, where only lowconfidence segments are selected for manual transcription (Roy and Roy, 2009; Sperber et al., 2014b). Our iterative interfaces then provide the initial transcription as created by the ASR as a starting point for each segment, upon which the transcriber improves (cf. Figure 1). The benefit of the iterative interfaces is that the transcriber can simply use the initially correct parts from the ASR as-is, and focus attention on the problematic parts. Ideally, words that were recognized correctly by the ASR will not be changed, reducing the chance of correction errors. In addition, the iterative approach can assist transcription of parts that are difficult for the transcriber to understand by p"
L18-1194,P12-3007,0,0.0305924,"ogue systems investigated efficient dialogue strategy on dialogue management to persuade users. Hiraoka et al., (2016) introduced actions of framing and logical explanations of advantages and disadvantages of products for The major problem of implementing a dialogue system is data, in any domains or tasks of systems, because most methods of dialogue modeling are based on statistical methods that require large-scale data-sets. However, collecting new dialogue data in accordance with a defined new task is costly. Some approaches enable easy data collection in a new domain by utilizing Web data (Banchs and Li, 2012) or by extracting dialogue parts from chat-like conversations (Nio et al., 2014). However, collecting large-scale dialogue data that contain emotional expressions are still difficult. Emotional expressions tend to be observed in communications between people who have close relationships. However, it is hard to record dialogues in such closed situations. It is also difficult to extract such conversations from Web because expressing emotions in public space is somewhat suppressed. Crowdsourcing has attracted attention as an efficient way for collecting or expanding dialogue data (Yu et al., 2016"
L18-1194,W04-3230,0,0.0318912,"Vector Regression (SVR). SVR is an expansion of Support Vector Machine (SVM) for the re#annotations Fleiss’ Kappa Mean square error emotion 22,008 0.411 — acceptance 10,980 0.370 0.850 gression problem. SVR has high generalizing capability because the learning of SVR minimizes the upper bound of generalization error. We used the corpus that is annotated with the degree of user’s acceptance as described in Section 3.4.. To make feature vectors from the user’s utterances for the regression, we extracted words as linguistic features from the user utterance by using morphological analyzer Mecab (Kudo et al., 2004). Synonyms of words in the user utterance are extracted by using WordNet (Bond et al., 2012) to extend the word feature vector. We also used positive/negative score by using pre-defined dictionary of positive/negative words (Takamura et al., 2005). Each extracted vector is concatenated as a single vector to be used as the input of SVR. The regression learns the annotated degree of user’s acceptance for each utterance. The system calculates the cosine similarity cos(ut , qj ) for each pair of the user utterance ut and a user-query in the 1 example database qj (Figure 2- ). The example database"
L18-1194,P05-1017,0,0.08217,"cause the learning of SVR minimizes the upper bound of generalization error. We used the corpus that is annotated with the degree of user’s acceptance as described in Section 3.4.. To make feature vectors from the user’s utterances for the regression, we extracted words as linguistic features from the user utterance by using morphological analyzer Mecab (Kudo et al., 2004). Synonyms of words in the user utterance are extracted by using WordNet (Bond et al., 2012) to extend the word feature vector. We also used positive/negative score by using pre-defined dictionary of positive/negative words (Takamura et al., 2005). Each extracted vector is concatenated as a single vector to be used as the input of SVR. The regression learns the annotated degree of user’s acceptance for each utterance. The system calculates the cosine similarity cos(ut , qj ) for each pair of the user utterance ut and a user-query in the 1 example database qj (Figure 2- ). The example database consists of pairs of a user query qj and its response rj with annotations of the user’s acceptance of the query bj and the emotional state of the response ej . The database consist of query-response pairs that are extracted from the collected corp"
L18-1468,W16-3210,0,0.066685,"Missing"
L18-1468,O04-3001,0,0.0630407,"based only on monolingual transcription. In multilingual corpora, the ATR basic travel expression corpus (BTEC) has served as the primary source for developing broad-coverage speech translation systems (Kikui et al., 2006). Its sentences were collected by bilingual travel experts from Japanese/English sentence pairs in travel domain phrasebooks. The ATR-BTEC has been translated into 18 languages, including French, German, Italian, Chinese, Korean, and Indonesian. Each language is comprised of 160,000 sentences. This corpus contains only text-based data. The Formosa Speech Database (Formosa) (Lyu et al., 2004), a multilingual corpus for TaiwaneseHakka-Mandarin, was created by recording 49 hours of speech. Its corpus construction project took over one year to collect recordings from thousands of speakers. The constructed corpus consists of speech and text data. Recently, the Multi30K Database (Elliott et al., 2016), which is from Multilingual English-German Image Descriptions, was created for a WMT Shared Task of Multimodal Machine Translation. It is based on the Flickr30K Entities dataset (Plummer et al., 2015) that was selected and manually translated into German and French by human translators. H"
N09-2056,J03-1002,0,0.00229055,"tence pairs was used to train the source-to-pivot translation models (80k sp ) and the second subset of sentence pairs was used to train the pivot-to-target translation models (80k pt ). Table 1 summarizes the characteristics of the BTEC corpus data sets used for the training (train) of the SMT models, the tuning of model weights (dev), and the evaluation of translation quality (eval). Besides the number of sentences (sen) and the vocabulary (voc), the sentence length (len) is also given, as the average number of words per sentence. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters, and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, an in-house multi-stack phrase-based decoder comparable to MOSES was used. For the evaluation of translation quality, we applied standard automatic evaluation metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). For the experimental results in this paper, the given scores are calculated as the average of the respective BLEU and MET"
N09-2056,P02-1040,0,0.0971744,"vocabulary (voc), the sentence length (len) is also given, as the average number of words per sentence. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters, and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, an in-house multi-stack phrase-based decoder comparable to MOSES was used. For the evaluation of translation quality, we applied standard automatic evaluation metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). For the experimental results in this paper, the given scores are calculated as the average of the respective BLEU and METEOR scores obtained for each system output and are listed as percent figures. 222 Table 1: Language Resources BTEC Corpus # of sen en voc len de voc len es voc len fr voc len hi voc len id voc len ja voc len ko voc len ms voc len th voc len vi voc len zh voc len train 80ksp 80kpt 80,000 80,000 12,264 11,047 7.8 7.2 19,593 17,324 7.4 6.8 16,317 14,807 7.6 7.1 15,319 13,663 7.8 7.3 26,096 19,906 8.1 7.6 14,585 13,224 7.0 6.5 13,868 12,51"
N09-2056,N07-1061,0,\N,Missing
N09-2056,W05-0909,0,\N,Missing
N09-2056,D08-1078,0,\N,Missing
N15-3009,J93-2004,0,0.0526534,"e of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, (Matsuzaki et al., 2005; Petrov et al., 2006)) framework is among the most popular for several reasons. The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank (Marcus et al., 1993), and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above). The second is the availablity of easy-to-use tools, most notably the Berkeley Parser,2 but also including Egret,3 and BUBS Parser.4 However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our knowledge, all existing tools produce a certain number of failed parses when run on large data sets. In this paper, we introduce Ckylark, a new PCFG-LA parser specifically designed for robustness. Specifically, Ckylark"
N15-3009,P05-1010,0,0.0533528,"ortant from the view of downstream applications is parser robustness, or the ability to return at least some parse regardless of the input. Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, (Matsuzaki et al., 2005; Petrov et al., 2006)) framework is among the most popular for several reasons. The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank (Marcus et al., 1993), and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above). The second is the availablity of easy-to-use tools, most notably the Berkeley Parser,2 but also including Egret,3 and BUBS Parser.4 However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our"
N15-3009,P08-1023,0,0.0266681,"id underflow without other expensive operations: Calculating P (X) is not trivial, but we can retrieve these values using the graph propagation algorithm proposed by Petrov and Klein (2007). 4 Experiments We evaluated parsing accuracies of our parser Ckylark and conventional PCFG-LA parsers: Berkeley Parser and Egret. Berkeley Parser is a conventional PCFG-LA parser written in Java with some additional optimization techniques. Egret is also a conventional PCFG-LA parser in C++ which can generate a parsing forest that can be used in downstream application such forest based machine translation (Mi et al., 2008). Q(X → w) ≡ P ′ (X → w)/sl (w), (2) 4.1 Dataset and Tools Q(X → Y ) ≡ P (X → Y ), (3) Table 1 shows summaries of each dataset. We used GrammarTrainer in the Berkeley Parser to train a PCFG-LA grammar with the Penn Treebank WSJ dataset section 2 to 22 (WSJ-train/dev). Egret and Ckylark can use the same model as the Berkeley Parser so we can evaluate only the performance of the parsers using the same grammar. Each parser is run on a Debian 7.1 machine with an Intel Core i7 CPU (3.40GHz, 4 cores, 8MB caches) and 4GB RAM. We chose 2 datasets to evaluate the performances of each parser. First, WSJ"
N15-3009,P14-2024,1,0.692936,"failure: outputting intermediate results when coarse-to-fine analysis fails, smoothing lexicon probabilities, and scaling probabilities to avoid underflow. An experiment shows that this allows millions of sentences can be parsed without any failures, in contrast to other publicly available PCFG-LA parsers. Ckylark is implemented in C++, and is available opensource under the LGPL license.1 1 Introduction Parsing accuracy is important. Parsing accuracy has been shown to have a significant effect on downstream applications such as textual entailment (Yuret et al., 2010) and machine translation (Neubig and Duh, 2014), and most work on parsing evaluates accuracy to some extent. However, one element that is equally, or perhaps even more, important from the view of downstream applications is parser robustness, or the ability to return at least some parse regardless of the input. Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among"
N15-3009,N07-1051,0,0.0743748,",Z (1) where X is any pre-terminal (part-of-speech) symbol in the grammar, w is any word, and wunk is the unknown word. λ is an interpolation factor between w and wunk , and should be small enough to cause no effect when the parser can generate the result without interpolation. Our implementation uses λ = 10−10 . 3.3 Probability Scaling To solve the problem of underflow, we modify model probabilities as Equations (2) to (4) to avoid underflow without other expensive operations: Calculating P (X) is not trivial, but we can retrieve these values using the graph propagation algorithm proposed by Petrov and Klein (2007). 4 Experiments We evaluated parsing accuracies of our parser Ckylark and conventional PCFG-LA parsers: Berkeley Parser and Egret. Berkeley Parser is a conventional PCFG-LA parser written in Java with some additional optimization techniques. Egret is also a conventional PCFG-LA parser in C++ which can generate a parsing forest that can be used in downstream application such forest based machine translation (Mi et al., 2008). Q(X → w) ≡ P ′ (X → w)/sl (w), (2) 4.1 Dataset and Tools Q(X → Y ) ≡ P (X → Y ), (3) Table 1 shows summaries of each dataset. We used GrammarTrainer in the Berkeley Parser"
N15-3009,P06-1055,0,0.47607,"downstream applications is parser robustness, or the ability to return at least some parse regardless of the input. Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform 1 http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fall back to some alternative processing scheme. Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, (Matsuzaki et al., 2005; Petrov et al., 2006)) framework is among the most popular for several reasons. The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank (Marcus et al., 1993), and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above). The second is the availablity of easy-to-use tools, most notably the Berkeley Parser,2 but also including Egret,3 and BUBS Parser.4 However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our knowledge, all existin"
N15-3009,S10-1009,0,0.0488826,"Missing"
N16-1003,W10-2916,0,0.0187418,"replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences that contain phrases that are fre"
N16-1003,P10-1088,0,0.121589,"be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences that contain phrases that are frequent in monolingual data but not in"
N16-1003,P11-2071,0,0.0432275,"Missing"
N16-1003,2005.iwslt-1.7,0,0.645248,"s large corpora can be collected, for example by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a numbe"
N16-1003,2014.amta-workshop.3,0,0.0133335,"filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Select sentences randomly. 4gram-rand: Select n-gram strings of length of up to 4 in random order. sent-by-4gram-freq: Select the sentence including the most frequent uncovered phrase with length of up to 4 words (baseline 1, §3.1). 4gram-freq: Select the most frequent uncovered phrase wi"
N16-1003,E12-1025,0,0.512627,"Missing"
N16-1003,D14-1130,0,0.0163888,"many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences that contain phrases that are frequent in monolingual data but not in bilingual data (Eck et al., 2005), have low confid"
N16-1003,P09-1021,0,0.381297,"web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences tha"
N16-1003,N09-1047,0,0.0215798,"xample by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are metho"
N16-1003,W11-2123,0,0.0105239,"after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Select sentences randomly. 4gram-rand: Select n-gram strings of length of up to 4 in random order. sent-by-4gram-freq: Select the sentence including the most frequent uncovered phrase with length of up to 4 words (baseline 1, §3.1). 4gram-freq: Select the most frequent uncovered phrase with length of up to 4 words (baseline 2, §3.2). maxsubst-freq: Select the most frequent uncovered maximal phrase (proposed, §4.1) reduced-maxsubst-freq: Select the most"
N16-1003,N03-1017,0,0.0128806,"the English-Japanese translation task, we adopted the broad-coverage example sentence corpus provided with the Eijiro dictionary4 as general domain data, and the ASPEC5 scientific paper abstract corpus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github"
N16-1003,P07-2045,0,0.0114659,"dopted the broad-coverage example sentence corpus provided with the Eijiro dictionary4 as general domain data, and the ASPEC5 scientific paper abstract corpus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Sel"
N16-1003,P11-2093,1,0.756098,". En: 46.4M Words Ja: 57.6M Words 1790 Sent. 1790 Sent. Table 1: Details of parallel data from WMT20143 as a base parallel data source and EMEA (Tiedemann, 2009), PatTR (W¨aschle and Riezler, 2012), and Wikipedia titles, used in the medical translation task, as the target domain data. For the English-Japanese translation task, we adopted the broad-coverage example sentence corpus provided with the Eijiro dictionary4 as general domain data, and the ASPEC5 scientific paper abstract corpus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Mos"
N16-1003,J03-1002,0,0.00479721,"pus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Select sentences randomly. 4gram-rand: Select n-gram strings of length of up to 4 in random order. sent-by-4gram-freq: Select the sentence including the most"
N16-1003,P03-1021,0,0.0212038,"To simulate a realistic active learning scenario, we started from given parallel data in the general domain and sequentially added additional source language data in a specific target domain. For the English-French translation task, we adopted the Europarl corpus 2 The method does not distinguish between equivalent word sequences even if they have different tree structures Lang Pair En-Fr En-Ja Domain Dataset General (Base) Train Medical Train (Target) Test Dev General (Base) Train Scientific Train (Target) Test Dev For the tuning of decoding parameters, since it is not realistic to run MERT (Och, 2003) at each retraining step, we tuned the parameters to maximize the BLEU score (Papineni et al., 2002) for the baseline system, and re-used the parameters thereafter. We compare the following 8 segment selection methods, including 2 random selection methods, 2 conventional methods and 4 proposed methods: Amount 1.89M Sent. En: 47.6M Words Fr: 49.4M Words 15.5M Sent. En: 393M Words Fr: 418M Words 1000 Sent. 500 Sent. 414k Sent. En: 6.72M Words Ja: 9.69M Words 1.87M Sent. En: 46.4M Words Ja: 57.6M Words 1790 Sent. 1790 Sent. Table 1: Details of parallel data from WMT20143 as a base parallel data s"
N16-1003,N15-3009,1,0.751844,", §3.2). maxsubst-freq: Select the most frequent uncovered maximal phrase (proposed, §4.1) reduced-maxsubst-freq: Select the most frequent uncovered semi-maximal phrase (proposed, §4.1) struct-freq: Select the most frequent uncovered phrase extracted from the subtrees (proposed, §4.2). reduced-struct-freq: Select the most frequent uncovered semi-maximal phrase extracted from the subtrees (proposed, §4.1 and §4.2). To generate oracle translations, we used an SMT system trained on all of the data in both the general and target-domain corpora. To generate parse trees, we used the Ckylark parser (Oda et al., 2015). 5.2 Results and Discussion Comparison of efficiency: In Figure 3, we show the evaluation score results by the number of additional source words up to 100k and 1M words. We can see that in English-French translation, the accuracy of the selection methods using parse trees grows more rapidly than other methods and was significantly better even at the point of 1M additional words. In the case of English-Japanese translation, the gains over 4-gram frequency are much smaller, but the proposed methods still consistently perform as well or better than the other methods. Besides, in all the graphs w"
N16-1003,P02-1040,0,0.0981299,"the general domain and sequentially added additional source language data in a specific target domain. For the English-French translation task, we adopted the Europarl corpus 2 The method does not distinguish between equivalent word sequences even if they have different tree structures Lang Pair En-Fr En-Ja Domain Dataset General (Base) Train Medical Train (Target) Test Dev General (Base) Train Scientific Train (Target) Test Dev For the tuning of decoding parameters, since it is not realistic to run MERT (Och, 2003) at each retraining step, we tuned the parameters to maximize the BLEU score (Papineni et al., 2002) for the baseline system, and re-used the parameters thereafter. We compare the following 8 segment selection methods, including 2 random selection methods, 2 conventional methods and 4 proposed methods: Amount 1.89M Sent. En: 47.6M Words Fr: 49.4M Words 15.5M Sent. En: 393M Words Fr: 418M Words 1000 Sent. 500 Sent. 414k Sent. En: 6.72M Words Ja: 9.69M Words 1.87M Sent. En: 46.4M Words Ja: 57.6M Words 1790 Sent. 1790 Sent. Table 1: Details of parallel data from WMT20143 as a base parallel data source and EMEA (Tiedemann, 2009), PatTR (W¨aschle and Riezler, 2012), and Wikipedia titles, used in"
N16-1003,J03-3002,0,0.0863344,"Missing"
N16-1003,D08-1112,0,0.0420495,"gual data (Eck et al., 2005), have low confidence according to the MT system (Haffari et al., 2009), or are predicted to be poor translations by an MT quality estimation system (Ananthakrishnan et al., 2010). However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate. To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks (Settles and Craven, 2008; Bloodgood and Callison-Burch, 2010; Tomanek and Hahn, 2009; Sperber et al., 2014). For MT in particular, Bloodgood and Callison-Burch (2010) have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words (§3). Nevertheless, our experiments identified two major practical problems with this method. First, as shown in Figure 1 (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency. Second, it is common to see fragments o"
N16-1003,Q14-1014,1,0.851551,"et al., 2009), or are predicted to be poor translations by an MT quality estimation system (Ananthakrishnan et al., 2010). However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate. To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks (Settles and Craven, 2008; Bloodgood and Callison-Burch, 2010; Tomanek and Hahn, 2009; Sperber et al., 2014). For MT in particular, Bloodgood and Callison-Burch (2010) have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words (§3). Nevertheless, our experiments identified two major practical problems with this method. First, as shown in Figure 1 (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency. Second, it is common to see fragments of complex phrases such as “one of the preceding,” which may be difficult for worker"
N16-1003,P09-1117,0,0.0296401,"the MT system (Haffari et al., 2009), or are predicted to be poor translations by an MT quality estimation system (Ananthakrishnan et al., 2010). However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate. To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks (Settles and Craven, 2008; Bloodgood and Callison-Burch, 2010; Tomanek and Hahn, 2009; Sperber et al., 2014). For MT in particular, Bloodgood and Callison-Burch (2010) have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words (§3). Nevertheless, our experiments identified two major practical problems with this method. First, as shown in Figure 1 (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency. Second, it is common to see fragments of complex phrases such as “one of the preceding,” which may"
N16-1003,W08-0305,0,0.0285635,"Missing"
N16-1003,P11-1122,0,0.0204067,"sed parse subtree selection method Figure 1: Conventional and proposed data selection methods Introduction In statistical machine translation (SMT) (Brown et al., 1993), large quantities of high-quality bilingual data are essential to achieve high translation accuracy. While in many cases large corpora can be collected, for example by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators."
N16-1003,J07-2003,0,\N,Missing
N18-1120,D16-1162,1,0.868289,"based method with respect to accuracy, speed, and simplicity of implementation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translatio"
N18-1120,W17-4716,0,0.0252546,"l corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of transla"
N18-1120,D17-1148,0,0.0329483,"ety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Gre"
N18-1120,W17-4713,0,0.397612,"-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen1 Note that there are existing retrieval-based methods for phrase-based and hierarchical phrase-based translation (Lopez, 2007; Germann, 2015). However, these methods do not improve translation quality but rather aim to improve the efficiency of the translation models. 1325 Proceedings of NAACL-HLT 2018, pages 1325–1335 c New Orleans, Loui"
N18-1120,1999.tc-1.8,0,0.191708,"17), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen1 Note that there are existing"
N18-1120,P07-2045,0,0.00831454,"43.76 50.15 METEOR 36.69 39.50 36.57 39.18 en-fr BLEU 57.26 62.60 57.67 63.27 METEOR 43.51 45.83 43.66 46.24 en-es BLEU 55.76 60.51 55.78 60.54 METEOR 42.53 44.58 42.55 44.64 Table 2: Translation results. TRAIN DEV TEST Average Length en-de 674K 1,636 1,689 31 en-fr 665K 1,733 1,710 29 en-es 663K 1,662 1,696 29 dev test Table 3: Data sets. The last line is the average length of English sentences. directions: English-to-German (en-de), Englishto-French (en-fr) and English-to-Spanish (en-es). We cleaned the data by removing repeated sentences and used the train-truecaser.perl script from Moses (Koehn et al., 2007) to truecase the corpus. Then we selected 2000 sentence pairs as development and test sets, respectively. The rest was used as the training set. We removed sentences longer than 80 and 100 from the training and development/test sets respectively. The final numbers of sentence pairs contained in the training, development and test sets are shown in Table 3.5 We applied byte pair encoding (Sennrich et al., 2016b) and set the vocabulary size to be 20K. For translation piece collection, we use GIZA++ (Och and Ney, 2003) and the grow-diag-final-and heuristic (Koehn et al., 2003) to obtain symmetric"
N18-1120,W17-3204,0,0.0504375,"Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT usi"
N18-1120,N03-1017,0,0.0776975,"translation time, and compares favorably to another alternative retrievalbased method with respect to accuracy, speed, and simplicity of implementation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information abou"
N18-1120,L18-1146,0,0.239139,"Missing"
N18-1120,D07-1104,0,0.0403502,"translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen1 Note that there are existing retrieval-based methods for phrase-based and hierarchical phrase-based translation (Lopez, 2007; Germann, 2015). However, these methods do not improve translation quality but rather aim to improve the efficiency of the translation models. 1325 Proceedings of NAACL-HLT 2018, pages 1325–1335 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Input: requirements Retrieved: requirements Vorschriften in relation in für to the relation die operational to Eignung the von suitability suitability Um@@ of of bulk carriers terminals schlags@@ anlagen Figure 1: A word-aligned sentence pair retrieved for an input sentence. Red words are unedited words obtained"
N18-1120,N18-1031,0,0.0217635,"Missing"
N18-1120,J03-1002,0,0.00854324,"moving repeated sentences and used the train-truecaser.perl script from Moses (Koehn et al., 2007) to truecase the corpus. Then we selected 2000 sentence pairs as development and test sets, respectively. The rest was used as the training set. We removed sentences longer than 80 and 100 from the training and development/test sets respectively. The final numbers of sentence pairs contained in the training, development and test sets are shown in Table 3.5 We applied byte pair encoding (Sennrich et al., 2016b) and set the vocabulary size to be 20K. For translation piece collection, we use GIZA++ (Och and Ney, 2003) and the grow-diag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments for the training set. We trained an attentional NMT model as our baseline system. The settings for NMT are shown in Table 4. We also compared our method with the search engine guided NMT model (SGNMT, Gu et al. (2017)) in Section 4.5. Word embedding GRU dimension Optimizer Initial learning rate Beam size 512 1024 adam 0.0001 5 Table 4: NMT settings. 5 We put the datasets used in our experiments on Github https://github.com/jingyiz/Data-sampled-preprocessed NMT Ours NMT Ours en-de 1.000 1.005 0.995 1"
N18-1120,P16-1162,0,0.472084,", an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrievalbased method with respect to accuracy, speed, and simplicity of implementation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problem"
N18-1120,E17-2058,0,0.0499781,"Missing"
N18-1120,2011.mtsummit-papers.37,1,0.902992,"ables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help"
N18-1120,P17-2089,1,0.897275,"Missing"
N18-1120,W17-4742,0,0.0321322,"Missing"
N18-1120,P17-1139,0,0.0211901,"functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or"
N18-1120,C16-1170,0,0.0180518,"s or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target tech"
N18-1120,W16-2323,0,\N,Missing
ohtake-etal-2010-dialogue,W04-2319,0,\N,Missing
ohtake-etal-2010-dialogue,W07-1524,0,\N,Missing
ohtake-etal-2010-dialogue,P01-1066,0,\N,Missing
ohtake-etal-2010-dialogue,P06-1026,0,\N,Missing
ohtake-etal-2010-dialogue,J02-3001,0,\N,Missing
ohtake-etal-2010-dialogue,W02-0708,0,\N,Missing
ohtake-etal-2010-dialogue,2007.sigdial-1.45,0,\N,Missing
ohtake-etal-2010-dialogue,maekawa-etal-2000-spontaneous,0,\N,Missing
P07-2007,N06-2049,1,\N,Missing
P07-2007,takezawa-kikui-2004-comparative,0,\N,Missing
P07-2007,2006.iwslt-evaluation.12,1,\N,Missing
P07-2007,2006.iwslt-papers.8,1,\N,Missing
P14-2090,N03-1033,0,0.00952551,"introduce regularization into the Greedy+DP algorithm, with the evaluation function ω rewrit553 Algorithm 2 Greedy+DP segmentation search Φ0 ← ∅ for k = 1 to K do for j = 0 to k − 1 do Φ′ ← {ϕ : c(ϕ; F) = k − j ∧ ϕ  ∈ Φj } #words f e Train M T 21.8M 20.3M En-De Train Seg. 424k 390k Test 27.6k 25.4k Train M T 13.7M 19.7M En-Ja Train Seg. 401k 550k Test 8.20k 11.9k Table 1: Size of M T training, segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy sear"
P14-2090,2012.eamt-1.60,0,0.00455234,"completely described in Algorithms 1 and 2. However, these algorithms require a large amount of computation and simple implementations of them are too slow to finish in realistic time. Because the heaviest parts of the algorithm are the calculation of M T and EV , we can greatly improve efficiency by memoizing the results of these functions, only recalculating on new input. 3 Experiments 3.1 Experimental Settings 3.2 Results and Discussion We evaluated the performance of our segmentation strategies by applying them to English-German and English-Japanese TED speech translation data from WIT3 (Cettolo et al., 2012). For EnglishGerman, we used the TED data and splits from the IWSLT2013 evaluation campaign (Cettolo et al., 2013), as well as 1M sentences selected from the out-of-domain training data using the method of Duh et al. (2013). For English-Japanese, we used TED data and the dictionary entries and sentences from EIJIRO.4 Table 1 shows summaries of the datasets we used. 4 Type Figures 4 and 5 show the results of evaluation for each segmentation strategy measured by BLEU and RIBES respectively. The horizontal axis is the mean number of words in the generated translation units. This value is proporti"
P14-2090,2013.iwslt-evaluation.1,0,0.0607953,"Missing"
P14-2090,P13-2119,1,0.641725,"hm are the calculation of M T and EV , we can greatly improve efficiency by memoizing the results of these functions, only recalculating on new input. 3 Experiments 3.1 Experimental Settings 3.2 Results and Discussion We evaluated the performance of our segmentation strategies by applying them to English-German and English-Japanese TED speech translation data from WIT3 (Cettolo et al., 2012). For EnglishGerman, we used the TED data and splits from the IWSLT2013 evaluation campaign (Cettolo et al., 2013), as well as 1M sentences selected from the out-of-domain training data using the method of Duh et al. (2013). For English-Japanese, we used TED data and the dictionary entries and sentences from EIJIRO.4 Table 1 shows summaries of the datasets we used. 4 Type Figures 4 and 5 show the results of evaluation for each segmentation strategy measured by BLEU and RIBES respectively. The horizontal axis is the mean number of words in the generated translation units. This value is proportional to the delay experienced during simultaneous speech translation (Rangarajan Sridhar et al., 2013) and thus a smaller value is desirable. RP, Greedy, and Greedy+DP methods have multiple results in these graphs because t"
P14-2090,D10-1092,0,0.0248496,"segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy search and a linear SVM (using surrounding word/POS 1, 2 and 3-grams as features) to learn the segmentation model. Greedy+DP is the algorithm that introduces grouping the positions in the source sentence by POS bigrams. Punct-Predict is the method using predicted positions of punctuation (Rangarajan Sridhar et al., 2013). RP is the method using right probability (Fujita et al., 2013). ten as belo"
P14-2090,P07-2045,0,0.00281304,"j ∧ ϕ  ∈ Φj } #words f e Train M T 21.8M 20.3M En-De Train Seg. 424k 390k Test 27.6k 25.4k Train M T 13.7M 19.7M En-Ja Train Seg. 401k 550k Test 8.20k 11.9k Table 1: Size of M T training, segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy search and a linear SVM (using surrounding word/POS 1, 2 and 3-grams as features) to learn the segmentation model. Greedy+DP is the algorithm that introduces grouping the positions in the source sentence by P"
P14-2090,C04-1072,0,0.0577816,"n this work, we define ω as the sum of the evaluation measure for each parallel sentence pair ⟨fj , ej ⟩: ω(S) := N ∑ EV (M T (fj , S), ej ), (3) j=1 where M T (f , S) represents the concatenation of all partial translations {M T (f (n) )} given the segments S as shown in Figure 1. Equation (3) indicates that we assume all parallel sentences to be independent of each other, and the evaluation measure is calculated for each sentence separately. This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). 1. Decide the mean number of words µ and the machine translation evaluation measure EV as parameters of algorithm. We can use an automatic evaluation measure such as BLEU (Papineni et al., 2002) as EV . Then, we calculate the number of sub-sentential segmentation boundaries K that we will need to insert into F to achieve an average segment length µ: ⌋ ) ( ⌊∑ f ∈F |f | − N . (1) K := max 0, µ 3. Make a segmentation model MS ∗ by treating the obtained segmentation boundaries S ∗ as positive labels, all other positions as negative labels, and training a classifier to distinguish between them. T"
P14-2090,2006.iwslt-papers.1,0,0.0144169,"is one example of such an application. When translating dialogue, the length of each utterance will usually be short, so the system can simply start the translation process when it detects the end of an utterance. However, in the case of lectures, for example, there is often no obvious boundary between utterances. Thus, translation systems require a method of deciding the timing at which to start the translation process. Using estimated ends of sentences as the timing with which to start translation, in the same way as a normal text translation, is a straightforward solution to this problem (Matusov et al., 2006). However, this approach 2 The method using RP can decide relative frequency of segmentation by changing a parameter, but guessing the length of a translation unit from this parameter is not trivial. 1 The implementation is available at http://odaemon.com/docs/codes/greedyseg.html. 551 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 551–556, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics lation accuracy as measured by BLEU or another evaluation measure. We evaluate our methods on a speech"
P14-2090,P11-2093,1,0.45417,"rit553 Algorithm 2 Greedy+DP segmentation search Φ0 ← ∅ for k = 1 to K do for j = 0 to k − 1 do Φ′ ← {ϕ : c(ϕ; F) = k − j ∧ ϕ  ∈ Φj } #words f e Train M T 21.8M 20.3M En-De Train Seg. 424k 390k Test 27.6k 25.4k Train M T 13.7M 19.7M En-Ja Train Seg. 401k 550k Test 8.20k 11.9k Table 1: Size of M T training, segmentation training and testing datasets. f -e { } Φk,j ← Φj ∪ arg max ω(S(F , Φj ∪ {ϕ})) ϕ∈Φ′ end for Φk ← arg max Φ∈{Φk,j :0≤j<k} ω(S(F, Φ)) end for return S(F, ΦK ) We use the Stanford POS Tagger (Toutanova et al., 2003) to tokenize and POS tag English and German sentences, and KyTea (Neubig et al., 2011) to tokenize Japanese sentences. A phrasebased machine translation (PBMT) system learned by Moses (Koehn et al., 2007) is used as the translation system M T . We use BLEU+1 as the evaluation measure EV in the proposed method. The results on the test data are evaluated by BLEU and RIBES (Isozaki et al., 2010), which is an evaluation measure more sensitive to global reordering than BLEU. We evaluated our algorithm and two conventional methods listed below: Greedy is our first method that uses simple greedy search and a linear SVM (using surrounding word/POS 1, 2 and 3-grams as features) to learn"
P14-2090,P02-1040,0,0.0982654,"tion of all partial translations {M T (f (n) )} given the segments S as shown in Figure 1. Equation (3) indicates that we assume all parallel sentences to be independent of each other, and the evaluation measure is calculated for each sentence separately. This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). 1. Decide the mean number of words µ and the machine translation evaluation measure EV as parameters of algorithm. We can use an automatic evaluation measure such as BLEU (Papineni et al., 2002) as EV . Then, we calculate the number of sub-sentential segmentation boundaries K that we will need to insert into F to achieve an average segment length µ: ⌋ ) ( ⌊∑ f ∈F |f | − N . (1) K := max 0, µ 3. Make a segmentation model MS ∗ by treating the obtained segmentation boundaries S ∗ as positive labels, all other positions as negative labels, and training a classifier to distinguish between them. This classifier is used to detect segmentation boundaries at test time. Steps 1. and 3. of the above procedure are trivial. In contrast, choosing a good segmentation according to Equation (2) is di"
P14-2090,N13-1023,0,0.679115,"s reason, segmentation strategies, which separate the input at appropriate positions other than end of the sentence, have been studied. A number of segmentation strategies for simultaneous speech translation have been proposed in recent years. F¨ugen et al. (2007) and Bangalore et al. (2012) propose using prosodic pauses in speech recognition to denote segmentation boundaries, but this method strongly depends on characteristics of the speech, such as the speed of speaking. There is also research on methods that depend on linguistic or non-linguistic heuristics over recognized text (Rangarajan Sridhar et al., 2013), and it was found that a method that predicts the location of commas or periods achieves the highest performance. Methods have also been proposed using the phrase table (Yarmohammadi et al., 2013) or the right probability (RP) of phrases (Fujita et al., 2013), which indicates whether a phrase reordering occurs or not. However, each of the previously mentioned methods decides the segmentation on the basis of heuristics, so the impact of each segmentation strategy on translation performance is not directly considered. In addition, the mean number of words in the translation unit, which strongly"
P14-2090,N12-1048,0,\N,Missing
P14-2090,I13-1141,0,\N,Missing
P14-2090,federico-etal-2012-iwslt,0,\N,Missing
P15-1020,P13-2121,0,0.0185406,"and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved h"
P15-1020,2006.amta-papers.8,0,0.0443221,"II et al. (2014), who describe a method that predicts sentence-final verbs using reinforcement learning (e.g. Figure 1 (b)). This approach has the potential to greatly decrease the delay in translation from verb-final languages to verbinitial languages (such as German-English), but is also limited to only this particular case. In this paper, we propose a more general method that focuses on a different variety of information: unseen syntactic constituents. This method is motivated by our desire to apply translation models that use source-side parsing, such as tree-to-string (T2S) translation (Huang et al., 2006) or syntactic pre-ordering (Xia and McCord, 2004), which have been shown to greatly improve translation accuracy over syntactically divergent language pairs. However, conventional methods for parsing are not directly applicable to the partial sentences that arise in simultaneous MT. The reason for this, as explained in detail in Section 3, is that parsing methods generally assume that they are given input that forms a complete syntactic phrase. Looking at the example in Figure 1, after the speaker has spoken the words “I think” we have a partial sentence that will only be complete once we obse"
P15-1020,D10-1092,0,0.0151267,"y, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syntactic and semantic units in the original sentence, allowing us to evaluate the robustness of translation against poor segmentation boundaries. The second method is the state-of-the-art segmentation strategy proposed by Oda et al. (2014), which"
P15-1020,P07-2045,0,0.00788058,"I et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syn"
P15-1020,C04-1072,0,0.0585426,"Missing"
P15-1020,J93-2004,0,0.0498969,"tisfy these conditions. As shown in the Figure 3, there is ambiguity regarding syntactic constituents to be predicted (e.g. we can choose either [ NP ] or [ DT , NN ] as R for w = [ “this”, “is” ]). These conditions avoid ambiguity of which syntactic constituents should predicted for partial sentences in the training data. Looking at the example, Figures 3(d1) and 3(e1) satisfy these conditions, but 3(d2) and 3(e2) do not. Figure 4 shows the statistics of the lengths of L and R sequences extracted according to these criteria for all substrings of the WSJ datasets 2 to 23 of the Penn Treebank (Marcus et al., 1993), a standard training set for English syntactic parsers. From the figure we can see that lengths of up to 2 constituents cover the majority of cases for both L and R, but a significant number of cases require longer strings. Thus methods that predict a fixed number of constituents are not appropriate here. In Algorithm 1, we show the method we propose to Algorithmically, parsing with predicted syntactic constituents can be achieved by simply treating each syntactic constituent as another word in the input sequence and using a standard parsing algorithm such as the CKY algorithm. In this proces"
P15-1020,2006.iwslt-papers.1,0,0.0292458,"mation needed 1 Introduction Speech translation is an application of machine translation (MT) that converts utterances from the speaker’s language into the listener’s language. One of the most identifying features of speech translation is the fact that it must be performed in real time while the speaker is speaking, and thus it is necessary to split a constant stream of words into translatable segments before starting the translation process. Traditionally, speech translation assumes that each segment corresponds to a sentence, and thus performs sentence boundary detection before translation (Matusov et al., 2006). However, full sentences can be long, particularly in formal speech such as lectures, and if translation does not start until explicit ends of 198 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 198–207, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Process of English-Japanese simultaneous translation with sentence segmentation. tic prediction to MT, including the proposal of a heuristic method that examines whether a future co"
P15-1020,P11-2093,1,0.840277,"these tags to future work. 6.1.2 Simultaneous Translation Next, we evaluate the performance of T2S simultaneous translation adopting the two proposed methods. We use data of TED talks from the English-Japanese section of WIT3 (Cettolo et al., 2012), and also append dictionary entries and examples in Eijiro3 to the training data to increase the vocabulary of the translation model. The total number of sentences/entries is 2.49M (WIT3, Eijiro), 998 (WIT3), and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to"
P15-1020,2012.eamt-1.60,0,0.0111743,"uents. Creating a language model that contains probabilities for these tags in the appropriate places is not trivial, so for simplicity, we simply assume that every syntactic constituent tag is an unknown word, and that the output of translation consists of both translated normal words and non-translated tags as shown in Figure 5. We relegate a more complete handling of these tags to future work. 6.1.2 Simultaneous Translation Next, we evaluate the performance of T2S simultaneous translation adopting the two proposed methods. We use data of TED talks from the English-Japanese section of WIT3 (Cettolo et al., 2012), and also append dictionary entries and examples in Eijiro3 to the training data to increase the vocabulary of the translation model. The total number of sentences/entries is 2.49M (WIT3, Eijiro), 998 (WIT3), and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we"
P15-1020,P13-4016,1,0.829485,"redictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al."
P15-1020,J03-1002,0,0.00408816,"of sentences/entries is 2.49M (WIT3, Eijiro), 998 (WIT3), and 468 (WIT3) sentences for training, development, and testing respectively. We use the Stanford Tokenizer4 for English tokenization, KyTea (Neubig et al., 2011) for 2 It is also potentially possible to create a predictive model for the actual content of the PP as done for sentence-final verbs by Grissom II et al. (2014), but the space of potential prepositional phrases is huge, and we leave this non-trivial task for future work. 3 4 203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki"
P15-1020,2014.iwslt-papers.8,0,0.0146061,"are given an incoming stream of words f , which we are expected to translate. As the f is long, we would like to begin translating before we reach the end of the stream. Previous methods to do so can generally be categorized into incremental decoding methods, and sentence segmentation methods. In incremental decoding, each incoming word is fed into the decoder one-by-one, and the decoder updates the search graph with the new words and decides whether it should begin translation. Incremental decoding methods have been proposed for phrase-based (Sankaran et al., 2010; Yarmohammadi et al., 2013; Finch et al., 2014) and hierarchical phrase-based (Siahbani et al., 2014) SMT Specifically the method consists of two parts: First, we propose a method that trains a statistical model to predict future syntactic constituents based on features of the input segment (Section 4). Second, we demonstrate how to apply this syntac199 models.1 Incremental decoding has the advantage of using information about the decoding graph in the choice of translation timing, but also requires significant changes to the internal workings of the decoder, precluding the use of standard decoding tools or techniques. Sentence segmentatio"
P15-1020,P03-1021,0,0.0381705,"203 http://eijiro.jp/ http://nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syntactic and semantic units in the original sentence, allowing us to evaluate the robustness of translation against poor seg"
P15-1020,P14-2090,1,0.854392,"rovide a simpler alternative by first dividing f into subsequences of 1 or more words [f (1) , . . . , f (N ) ]. These segments are then translated with a traditional decoder into output sequences [e(1) , . . . , e(N ) ], which each are output as soon as translation finishes. Many methods have been proposed to perform segmentation, including the use of prosodic boundaries (F¨ugen et al., 2007; Bangalore et al., 2012), predicting punctuation marks (Rangarajan Sridhar et al., 2013), reordering probabilities of phrases (Fujita et al., 2013), or models to explicitly optimize translation accuracy (Oda et al., 2014). Previous work often assumes that f is a single sentence, and focus on sub-sentential segmentation, an approach we follow in this work. Sentence segmentation methods have the obvious advantage of allowing for translation as soon as a segment is decided. However, the use of the shorter segments also makes it necessary to translate while part of the utterance is still unknown. As a result, segmenting sentences more aggressively often results in a decrease translation accuracy. This is a problem in phrase-based MT, the framework used in the majority of previous research on simultaneous translati"
P15-1020,N15-3009,1,0.857129,"Missing"
P15-1020,D14-1140,0,0.315517,"Missing"
P15-1020,P02-1040,0,0.0940091,"nlp.stanford.edu/software/tokenizer.shtml Japanese tokenization, GIZA++ (Och and Ney, 2003) to construct word alignment, and KenLM (Heafield et al., 2013) to generate a 5-gram target language model. We use the Ckylark parser, which we modified to implement the parsing method of Section 3.2, to generate T ∗ from L∗ , w and R∗ . We use Travatar (Neubig, 2013) to train the T2S translation model used in the proposed method, and also Moses (Koehn et al., 2007) to train phrase-based translation models that serve as a baseline. Each translation model is tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002). We evaluate translation accuracies by BLEU and also RIBES (Isozaki et al., 2010), a reordering-focused metric which has achieved high correlation with human evaluation on English-Japanese translation tasks. We perform tests using two different sentence segmentation methods. The first is n-words segmentation (Rangarajan Sridhar et al., 2013), a simple heuristic that simply segments the input every n words. This method disregards syntactic and semantic units in the original sentence, allowing us to evaluate the robustness of translation against poor segmentation boundaries. The second method i"
P15-1020,N13-1023,0,0.362073,"internal workings of the decoder, precluding the use of standard decoding tools or techniques. Sentence segmentation methods (Figure 2) provide a simpler alternative by first dividing f into subsequences of 1 or more words [f (1) , . . . , f (N ) ]. These segments are then translated with a traditional decoder into output sequences [e(1) , . . . , e(N ) ], which each are output as soon as translation finishes. Many methods have been proposed to perform segmentation, including the use of prosodic boundaries (F¨ugen et al., 2007; Bangalore et al., 2012), predicting punctuation marks (Rangarajan Sridhar et al., 2013), reordering probabilities of phrases (Fujita et al., 2013), or models to explicitly optimize translation accuracy (Oda et al., 2014). Previous work often assumes that f is a single sentence, and focus on sub-sentential segmentation, an approach we follow in this work. Sentence segmentation methods have the obvious advantage of allowing for translation as soon as a segment is decided. However, the use of the shorter segments also makes it necessary to translate while part of the utterance is still unknown. As a result, segmenting sentences more aggressively often results in a decrease translat"
P15-1020,P06-2088,0,0.113368,"Figure 3(e2) by appending only NN after the unit. 3 Parsing Incomplete Sentences 3.1 Difficulties in Incomplete Parsing In standard phrase structure parsing, the parser assumes that each input string is a complete sentence, or at least a complete phrase. For example, Figure 3 (a) shows the phrase structure of the complete sentence “this is a pen.” However, in the case of simultaneous translation, each translation unit 3.2 Formulation of Incomplete Parsing 1 There is also one previous rule-based system that uses syntax in incremental translation, but it is language specific and limited domain (Ryu et al., 2006), and thus difficult to compare with our SMT-based system. It also does not predict unseen constituents, relying only on the observed segment. A typical model for phrase structure parsing is the probabilistic context-free grammar (PCFG). Parsing is performed by finding the parse tree T that 200 maximizes the PCFG probability given a sequence of words w ≡ [w1 , w2 , · · · , wn ] as shown by Eq. (2): T ∗ ≡ arg max Pr(T |w) T It should be noted that here L refers to syntactic constituents that have already been seen in the past. Thus, it is theoretically possible to store past parse trees as hist"
P15-1020,W10-1733,0,0.0784773,"on In simultaneous translation, we assume that we are given an incoming stream of words f , which we are expected to translate. As the f is long, we would like to begin translating before we reach the end of the stream. Previous methods to do so can generally be categorized into incremental decoding methods, and sentence segmentation methods. In incremental decoding, each incoming word is fed into the decoder one-by-one, and the decoder updates the search graph with the new words and decides whether it should begin translation. Incremental decoding methods have been proposed for phrase-based (Sankaran et al., 2010; Yarmohammadi et al., 2013; Finch et al., 2014) and hierarchical phrase-based (Siahbani et al., 2014) SMT Specifically the method consists of two parts: First, we propose a method that trains a statistical model to predict future syntactic constituents based on features of the input segment (Section 4). Second, we demonstrate how to apply this syntac199 models.1 Incremental decoding has the advantage of using information about the decoding graph in the choice of translation timing, but also requires significant changes to the internal workings of the decoder, precluding the use of standard de"
P15-1020,C04-1073,0,0.0449431,"redicts sentence-final verbs using reinforcement learning (e.g. Figure 1 (b)). This approach has the potential to greatly decrease the delay in translation from verb-final languages to verbinitial languages (such as German-English), but is also limited to only this particular case. In this paper, we propose a more general method that focuses on a different variety of information: unseen syntactic constituents. This method is motivated by our desire to apply translation models that use source-side parsing, such as tree-to-string (T2S) translation (Huang et al., 2006) or syntactic pre-ordering (Xia and McCord, 2004), which have been shown to greatly improve translation accuracy over syntactically divergent language pairs. However, conventional methods for parsing are not directly applicable to the partial sentences that arise in simultaneous MT. The reason for this, as explained in detail in Section 3, is that parsing methods generally assume that they are given input that forms a complete syntactic phrase. Looking at the example in Figure 1, after the speaker has spoken the words “I think” we have a partial sentence that will only be complete once we observe the following SBAR. Our method attempts to pr"
P15-1020,N12-1048,0,\N,Missing
P15-1020,I13-1141,0,\N,Missing
P15-2094,N15-1033,1,0.878686,"Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich monolingual resources in pivot languages such as English, and SMT can utilize this additional information to improve the translation quality. To utilize information about the pivot language at translation time, we train a Multi-Synchronous Context-free Grammar (MSCFG) (Neubig et al., 2015), a generalized extension of synchronous CFGs (SCFGs) (Chiang, 2007), that can generate strings in multiple languages at the same time. To create the MSCFG, we triangulate source-pivot and pivot-target SCFG rule tables not into a single source-target SCFG, but into a source-target-pivot MSCFG rule table that remembers the pivot. During decoding, we use language models over both the target and the pivot to assess the naturalness of the derivation. We perform experiments on pivot translation of Europarl proceedings, which show that our method indeed provide significant gains in accuracy (of up t"
P15-2094,P13-4016,1,0.853226,"MSCFG +PivotLM 2M † 25.75 † 24.58 ‡ 22.29 † 19.40 † 29.95 ‡ 25.64 † 19.19 ‡ 31.00 ‡ 26.22 ‡ 18.52 † 29.31 † 29.02 Table 1: Results for each method. Bold indicates the highest BLEU score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG († : p < 0.05, ‡ : p < 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: 4.2 Experimental Results The result of experiments using all combinations of pivot translation tasks for 4 languages via English is shown in Table 1. From the results, we can see that the proposed triangulation method con"
P15-2094,P03-1021,0,0.0312701,"score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG († : p < 0.05, ‡ : p < 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: 4.2 Experimental Results The result of experiments using all combinations of pivot translation tasks for 4 languages via English is shown in Table 1. From the results, we can see that the proposed triangulation method considering pivot LMs outperforms the traditional triangulation method for all language pairs, and translation with larger pivot LMs improves the BLEU scores. For all languages, the"
P15-2094,P02-1040,0,0.0927681,"29.02 Table 1: Results for each method. Bold indicates the highest BLEU score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG († : p < 0.05, ‡ : p < 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: 4.2 Experimental Results The result of experiments using all combinations of pivot translation tasks for 4 languages via English is shown in Table 1. From the results, we can see that the proposed triangulation method considering pivot LMs outperforms the traditional triangulation method for all language pairs, and translation with large"
P15-2094,N07-1061,0,0.884546,"e translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 573 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally w"
P15-2094,D14-1174,0,0.0648612,"et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 573 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich m"
P15-2094,J07-2003,0,0.870535,"Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich monolingual resources in pivot languages such as English, and SMT can utilize this additional information to improve the translation quality. To utilize information about the pivot language at translation time, we train a Multi-Synchronous Context-free Grammar (MSCFG) (Neubig et al., 2015), a generalized extension of synchronous CFGs (SCFGs) (Chiang, 2007), that can generate strings in multiple languages at the same time. To create the MSCFG, we triangulate source-pivot and pivot-target SCFG rule tables not into a single source-target SCFG, but into a source-target-pivot MSCFG rule table that remembers the pivot. During decoding, we use language models over both the target and the pivot to assess the naturalness of the derivation. We perform experiments on pivot translation of Europarl proceedings, which show that our method indeed provide significant gains in accuracy (of up to 1.2 BLEU points), in all combinations of 4 languages with English"
P15-2094,P07-1092,0,0.805068,"n In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 573 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this a"
P15-2094,W08-0333,0,0.349773,"ional triangulation method, information about pivot phrases that behave as bridges between source and target phrases is lost after learning phrase pairs, as shown in Figure 1 (b). To overcome these problems, we propose a novel triangulation method that remembers the pivot phrase connecting source and target in the records of phrase/rule table, and estimates a joint translation probability from the source to target Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1 Code to replicate"
P15-2094,J93-1004,0,\N,Missing
P15-2094,J93-2003,0,\N,Missing
P15-2094,2005.mtsummit-papers.11,0,\N,Missing
P16-1130,P10-2002,0,0.0185124,"ver, the information contained in Rule4 will be considered as context features for Rule1. Therefore, this is no longer an advantage for the CSRS model as long as we use rich enough context features, which could be the reason why using both the CSRS and CSRS-MINI models cannot further improve the translation quality compared to using only the CSRS-MINI model. 5 Related Work The rule selection problem for syntax-based SMT has received much attention. He et al. (2008) proposed a lexicalized rule selection model to perform context-sensitive rule selection for hierarchical phrase-base translation. Cui et al. (2010) introduced a joint rule selection model for hierarchical phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach. However, these two models adopted linear classifiers similar to those used in the MERS model (Liu et al., 2008), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabili"
P16-1130,P14-1129,0,0.0707017,"Missing"
P16-1130,N13-1001,0,0.0201378,"Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on paral"
P16-1130,P16-1078,0,0.0171099,"tion models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syntactic NMT model (Eriguchi et al., 2016), which extends the original sequence-to-sequence NMT model with the source-side phrase structure. Although this model takes source-side syntax into consideration, it still produces target words one by one as a sequence. In contrast, the tree-based translation rules used in our model can take advantage of the hierarchical structures of both source and target languages. 6 Conclusion In this paper, we propose a CSRS model for syntax-based SMT, which is learned by a feedforward neural network on a continuous space. Compared with the previous MERS model that used discrete representations of words"
P16-1130,N04-1035,0,0.0816271,"n rules used in translations down into minimal rules and multiply all probabilities to calculate the necessary features. PP NP IN DT NN on the table 在 桌子 上 extract Rule1 Source tree NN Target string table NP 桌子 DT Rule2 NN the DT Rule3 桌子 table NP 4 Experiments 4.1 NN the x0 PP x0 NP IN DT NN Rule4 on the PP table 在 桌子 上 IN NP 在 x0 上 on x0 PP Rule5 NP IN DT NN Rule6 on the x0 在 x0 上 Figure 3: Rules. and many may only appear a few times in the corpus. To reduce these problems of sparsity, we propose another improvement to the model, specifically through the use of minimal rules. Minimal rules (Galley et al., 2004) are translation rules that cannot be split into two smaller rules. For example, in Figure 3, Rule2 is not a minimal rule, since Rule2 can be split into Rule1 and Rule3. In the same way, Rule4 and Rule6 are not minimal while Rule1, Rule3 and Rule5 are minimal. Minimal rules are more frequent than nonminimal rules and have richer training data. Hence, we can expect that a rule selection model trained on minimal rules will suffer less from data sparsity problems. Besides, without non-minimal rules, the rule selection model will need less memSetting We evaluated the proposed approach for Englisht"
P16-1130,P06-1121,0,0.0645963,".1K 1.5K 27.1K 29.8K 954K 40.4M 37.2M 504K 288K 2K 77.5K 75.4K 2K 58.1K 55.5K 3.14M 104M 118M 273K 150K 2K 66.5K 74.6K 2K 70.6K 78.5K Base MERS CSRS MERS-MINI CSRS-MINI CSRS vs. MERS CSRS-MINI vs. MERS-MINI MERS-MINI vs. MERS CSRS-MINI vs. CSRS EC 29.42 29.75 30.12 30.53 31.63 EJ 37.10 37.76 37.83 38.14 38.32 ED &gt;&gt; &gt;&gt; − &gt; EF &gt;&gt; − &gt;&gt; − EC &gt; &gt;&gt; &gt;&gt; &gt;&gt; EJ − − &gt;&gt; &gt;&gt; Table 3: Significance test results. The symbol &gt;&gt; (&gt;) represents a significant difference at the p < 0.01 (p < 0.05) level and the symbol - represents no significant difference at the p < 0.05 level. performed using the GHKM algorithm (Galley et al., 2006) and the maximum numbers of nonterminals and terminals contained in one rule were set to 2 and 10 respectively. Note that when extracting minimal rules, we release this limit. The decoding algorithm is the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). For English parsing, we used Egret8 , which is able to output packed forests for decoding. We trained the CSRS models (CSRS and CSRSMINI) on translation rules extracted from the training set. Translation rules extracted from the development set were used as validation data for model training to avoid over-fitting. For differe"
P16-1130,P14-1066,0,0.0278847,"8), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus. Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2"
P16-1130,N04-1014,0,0.0657252,"ules. We tested our model on different translation tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 1 S NP VP NP PRP VBD DT NN I caught a thief 我 抓 了 一个 贼 NP NP PRP VBD DT NN I caught a cold 我 得 了 感冒 Rule Extraction S S VP VP NP VBD NP NP VBD NP x0 caught x1 x0 caught x1 x0 抓 了 x1 x0 得 了 x1 Figure 1: An ambiguous source subtree with different translations (English-to-Chinese). Introduction In syntax-based statistical machine translation (SMT), especially tree-to-string (Liu et al., 2006; Graehl and Knight, 2004) and forest-to-string (Mi et al., 2008) SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence. A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals. One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correct rules during decoding is"
P16-1130,C08-1041,0,0.018049,"tion than minimal rules. For example, in Figure 3, Rule4 contains more information than Rule1, which could be an advantage for rule selection. However, the information contained in Rule4 will be considered as context features for Rule1. Therefore, this is no longer an advantage for the CSRS model as long as we use rich enough context features, which could be the reason why using both the CSRS and CSRS-MINI models cannot further improve the translation quality compared to using only the CSRS-MINI model. 5 Related Work The rule selection problem for syntax-based SMT has received much attention. He et al. (2008) proposed a lexicalized rule selection model to perform context-sensitive rule selection for hierarchical phrase-base translation. Cui et al. (2010) introduced a joint rule selection model for hierarchical phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach. However, these two models adopted linear classifiers similar to those used in the MERS model (Liu et al., 2008), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to lea"
P16-1130,P15-1001,0,0.0379898,"a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on pl"
P16-1130,W04-3250,0,0.140364,"owing their work, the iteration number was set to be 100 and the Gaussian prior was set to be 1. We also compared the original MERS model and the MERS model trained only on minimal rules (MERS-MINI) to test the benefit of using minimal rules for model training. The MERS and CSRS models were both used to calculate features used to rerank unique 1,000best outputs of the baseline system. Tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 4.2 Results Table 2 shows the translation results and Table 3 shows significance test results using bootstrap resampling (Koehn, 2004): “Base” stands for the baseline system without any; “MERS”, “CSRS”, “MERS-MINI” and “CSRS-MINI” means the outputs of the baseline system were reranked using features from the MERS, CSRS, MERS-MINI and CSRS-MINI models respectively. Generally, the CSRS model outperformed the MERS model and the CSRS-MINI model outperformed the MERSMINI model on different translation tasks. In addition, using minimal rules for model training benefitted both the MERS and CSRS models. Table 4 shows translation examples in the EC task to demonstrate the reason why our approach improved accuracy. Among all translati"
P16-1130,P08-1023,0,0.171613,"ion tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 1 S NP VP NP PRP VBD DT NN I caught a thief 我 抓 了 一个 贼 NP NP PRP VBD DT NN I caught a cold 我 得 了 感冒 Rule Extraction S S VP VP NP VBD NP NP VBD NP x0 caught x1 x0 caught x1 x0 抓 了 x1 x0 得 了 x1 Figure 1: An ambiguous source subtree with different translations (English-to-Chinese). Introduction In syntax-based statistical machine translation (SMT), especially tree-to-string (Liu et al., 2006; Graehl and Knight, 2004) and forest-to-string (Mi et al., 2008) SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence. A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals. One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correct rules during decoding is a major challenge for SMT in general,"
P16-1130,P13-4016,1,0.856935,"on tasks. For the ED and EF tasks, the translation systems are trained on Europarl v7 parallel corpus and tested on the WMT 2015 translation task.4 The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 The detailed statistics for training, development and test sets are given in Table 1. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. For each translation task, we used Travatar (Neubig, 2013) to train a forest-to-string translation system. GIZA++ (Och and Ney, 2003) was used for word alignment. A 5-gram language model was trained on the target side of the training corpus using the IRST-LM Toolkit7 with modified Kneser-Ney smoothing. Rule extraction was 4 The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. 5 Note that NTCIR-9 only contained a Chinese-to-English translation task. Because we want to test the proposed approach with a simi"
P16-1130,J03-1002,0,0.00539023,"on Europarl v7 parallel corpus and tested on the WMT 2015 translation task.4 The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 The detailed statistics for training, development and test sets are given in Table 1. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. For each translation task, we used Travatar (Neubig, 2013) to train a forest-to-string translation system. GIZA++ (Och and Ney, 2003) was used for word alignment. A 5-gram language model was trained on the target side of the training corpus using the IRST-LM Toolkit7 with modified Kneser-Ney smoothing. Rule extraction was 4 The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. 5 Note that NTCIR-9 only contained a Chinese-to-English translation task. Because we want to test the proposed approach with a similarly accurate parsing model across our tasks, we used English as the sourc"
P16-1130,P03-1021,0,0.0729483,"stems. Table 1: Data sets. 8 ED 15.00 15.62 16.15 15.77 16.49 ing instances for their model were extracted from the training set. Following their work, the iteration number was set to be 100 and the Gaussian prior was set to be 1. We also compared the original MERS model and the MERS model trained only on minimal rules (MERS-MINI) to test the benefit of using minimal rules for model training. The MERS and CSRS models were both used to calculate features used to rerank unique 1,000best outputs of the baseline system. Tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 4.2 Results Table 2 shows the translation results and Table 3 shows significance test results using bootstrap resampling (Koehn, 2004): “Base” stands for the baseline system without any; “MERS”, “CSRS”, “MERS-MINI” and “CSRS-MINI” means the outputs of the baseline system were reranked using features from the MERS, CSRS, MERS-MINI and CSRS-MINI models respectively. Generally, the CSRS model outperformed the MERS model and the CSRS-MINI model outperformed the MERSMINI model on different translation tasks. In addition, using minimal rules for model training benefitted both the MERS and CSRS mod"
P16-1130,C12-2104,0,0.0248686,"phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach. However, these two models adopted linear classifiers similar to those used in the MERS model (Liu et al., 2008), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus. Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minima"
P16-1130,P15-1004,0,0.0145628,"odel for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syntactic NMT model (Eriguchi et al., 2016), which extends the original sequence-to-sequence NMT model with the source-side phrase structure. Although this model tak"
P16-1130,P11-1086,0,0.0154809,"space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al."
P16-1130,D13-1140,0,0.0199508,"MERS probability feature, and, h2 is a penalty feature counting the number of predictions made by the MERS model. 3 where v ∈ {0, 1} is an indicator of whether t˜ is translated into e˜. This is in contrast to the MERS model, which treated the rule selection problem as a multi-class classification task. If instead we attempted to estimate output probabilities for all different e˜, the cost of estimating the normalization coefficient would be prohibitive, as the number of unique output-side word strings e˜ is large. There are a number of remedies to this, including noise contrastive estimation (Vaswani et al., 2013), but the binary approximation method has been reported to have better performance (Zhang et al., 2015). To learn this model, we use a feed-forward neural network with structure similar to neural network language models (Vaswani et al., 2013). The input of the neural rule selection model is a vector representation for t˜, another vector representation for e˜, and a set of ξ vector representations for both source-side and target-side context words of r: In our model, C (r) is calculated differently depending on the number of nonterminals included in the rule. Specifically, Equation 7 defines Co"
P16-1130,P06-1077,0,0.310903,"d to non-minimal rules. We tested our model on different translation tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 1 S NP VP NP PRP VBD DT NN I caught a thief 我 抓 了 一个 贼 NP NP PRP VBD DT NN I caught a cold 我 得 了 感冒 Rule Extraction S S VP VP NP VBD NP NP VBD NP x0 caught x1 x0 caught x1 x0 抓 了 x1 x0 得 了 x1 Figure 1: An ambiguous source subtree with different translations (English-to-Chinese). Introduction In syntax-based statistical machine translation (SMT), especially tree-to-string (Liu et al., 2006; Graehl and Knight, 2004) and forest-to-string (Mi et al., 2008) SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence. A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals. One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correc"
P16-1130,P14-1011,0,0.01589,"from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus. Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method t"
P16-1130,D08-1010,0,0.241026,"tion rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correct rules during decoding is a major challenge for SMT in general, and syntax-based models are no exception. There have been several methods proposed to resolve this ambiguity. The most simple method, used in the first models of tree-to-string translation (Liu et al., 2006), estimated the probability of a translation rule by relative frequencies. For example, in Figure 1, the rule that occurs more times in the training data will have a higher score. Later, Liu et al. (2008) proposed a maximum entropy based rule selection (MERS, Section 2) model for syntax-based SMT, which used contextual information for rule selection, such as words surrounding a rule and words covered by nonterminals in a rule. For example, to choose the correct rule from the two rules in Figure 1 for decoding a particular input sentence, if the source phrase covered by “x1” is “a thief” and this child phrase 1372 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1372–1381, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Ling"
P16-1130,D15-1250,1,0.828569,"S model. 3 where v ∈ {0, 1} is an indicator of whether t˜ is translated into e˜. This is in contrast to the MERS model, which treated the rule selection problem as a multi-class classification task. If instead we attempted to estimate output probabilities for all different e˜, the cost of estimating the normalization coefficient would be prohibitive, as the number of unique output-side word strings e˜ is large. There are a number of remedies to this, including noise contrastive estimation (Vaswani et al., 2013), but the binary approximation method has been reported to have better performance (Zhang et al., 2015). To learn this model, we use a feed-forward neural network with structure similar to neural network language models (Vaswani et al., 2013). The input of the neural rule selection model is a vector representation for t˜, another vector representation for e˜, and a set of ξ vector representations for both source-side and target-side context words of r: In our model, C (r) is calculated differently depending on the number of nonterminals included in the rule. Specifically, Equation 7 defines Cout (r, n) to be context words (n-grams) around r and Cin (r, n, Xk ) to be boundary words (n-grams) cov"
P16-1130,D15-1166,0,0.012445,"t al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these mode"
P16-1130,W06-0127,0,0.0362305,"o-German (ED), English-to-French (EF), English-to-Chinese (EC) and English-to-Japanese (EJ) translation tasks. For the ED and EF tasks, the translation systems are trained on Europarl v7 parallel corpus and tested on the WMT 2015 translation task.4 The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 The detailed statistics for training, development and test sets are given in Table 1. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. For each translation task, we used Travatar (Neubig, 2013) to train a forest-to-string translation system. GIZA++ (Och and Ney, 2003) was used for word alignment. A 5-gram language model was trained on the target side of the training corpus using the IRST-LM Toolkit7 with modified Kneser-Ney smoothing. Rule extraction was 4 The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. 5 Note that NTCIR-9 only contained"
P16-1130,P15-1002,0,0.0492218,"Missing"
P16-1130,P15-1003,0,0.0142853,"ibuted representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syntactic NMT model (Eriguchi et al., 2016), which extends the original sequence-to-sequence NMT model with the source-si"
P17-1079,J92-4003,0,0.649568,"Missing"
P17-1079,P16-1186,0,0.0411675,"Missing"
P17-1079,D15-1249,0,0.0192774,"2; Mikolov et al., 2013) to the denominator of the softmax have also been proposed to reduce calculation at training. However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax. Vocabulary selection approaches (Mi et al., 2016; L’Hostis et al., 2016) can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality. Other methods using characters (Ling et al., 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency. Problem Description and Prior Work Formulation and Standard Softmax Most of current NMT models use one-hot representations to represent the words in the output vocabulary – each word w is represented by a unique sparse vector eid(w) ∈ RV , in which only one element at the position corresponding to the word ID id(w) ∈ {x ∈ N |1 ≤ x ≤ V } is 1, while others are 0. V represents the vocabulary size of the target language. NMT"
P17-1079,P07-2045,0,0.008746,"Missing"
P17-1079,P11-2093,1,0.742129,"Missing"
P17-1079,D15-1166,0,0.0750918,"train the parameters efficiently, and possible to perform decoding efficiently with choosing the candidate words from the full probability distribution. In particular, the method should be performed fast on general CPUs to suppress physical costs of computational resources for actual production systems. When handling broad or open domains, machine translation systems usually have to handle a large vocabulary as their inputs and outputs. This is particularly a problem in neural machine translation (NMT) models (Sutskever et al., 2014), such as the attention-based models (Bahdanau et al., 2014; Luong et al., 2015) shown in Figure 1. In these models, the output layer is required to generate a specific word from an internal vector, and a large vocabulary size tends to require a large amount of computation to predict each of the candidate word probabilities. Because this is a significant problem for neural language and translation models, there are a number of methods proposed to resolve this problem, which we detail in Section 2.2. However, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in NMT systems: Compatibilit"
P17-1079,P02-1040,0,0.0973991,"Missing"
P17-1079,P16-2021,0,0.0214666,"ion matrix of the output layer sparser than a fully-connected softmax, and can reduce time/space computation amount by ignoring zero part of the matrix. However, this method restricts the usage of hidden layer, and the size of the matrix is still in proportion to V . Sampling-based approximations (Mnih and Teh, 2012; Mikolov et al., 2013) to the denominator of the softmax have also been proposed to reduce calculation at training. However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax. Vocabulary selection approaches (Mi et al., 2016; L’Hostis et al., 2016) can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality. Other methods using characters (Ling et al., 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency. Problem Description and Prior Work Formulation and Standard Softmax Most of current NMT models use"
P17-1079,P16-1162,0,0.0527334,"ions (Mnih and Teh, 2012; Mikolov et al., 2013) to the denominator of the softmax have also been proposed to reduce calculation at training. However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax. Vocabulary selection approaches (Mi et al., 2016; L’Hostis et al., 2016) can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality. Other methods using characters (Ling et al., 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency. Problem Description and Prior Work Formulation and Standard Softmax Most of current NMT models use one-hot representations to represent the words in the output vocabulary – each word w is represented by a unique sparse vector eid(w) ∈ RV , in which only one element at the position corresponding to the word ID id(w) ∈ {x ∈ N |1 ≤ x ≤ V } is 1, while others are 0. V represents the vocabulary size"
Q14-1014,P13-1004,0,0.0206363,"l., 2011) and many other NLP tasks (Olsson, 2009), to name but a few studies. It has also been recognized by the active learning community that correcting the most useful parts first is often not optimal in terms of efficiency, since these parts tend to be the most difficult to manually annotate (Settles et al., 2008). The authors advocate the use of a user model to predict the supervision effort, and select the instances with best “bang-for-thebuck.” This prediction of supervision effort was successful, and was further refined in other NLP-related studies (Tomanek et al., 2010; Specia, 2011; Cohn and Specia, 2013). Our approach to user modeling using GP regression is inspired by the latter. Most studies on user models consider only supervision effort, while neglecting the accuracy of human annotations. The view on humans as a perfect oracle has been criticized (Donmez and Carbonell, 2008), since human errors are common and can negatively affect supervision utility. Research on human-computer-interaction has identified the modeling of human errors as very difficult (Olson and Olson, 1990), depending on factors such as user experience, cognitive load, user interface design, and 178 fatigue. Nevertheless,"
Q14-1014,P10-2032,0,0.0484789,"Missing"
Q14-1014,N09-1047,0,0.0787448,"t to the number of segmented tokens. We feel that this is acceptable, considering that the time needed for human supervision will likely dominate the computation time, and reasonable approximations can be made as noted in Section 3.2. 6 Relation to Prior Work Efficient supervision strategies have been studied across a variety of NLP-related research areas, and received increasing attention in recent years. Examples include post editing for speech recognition (Sanchez-Cortina et al., 2012), interactive machine translation (Gonz´alez-Rubio et al., 2010), active learning for machine translation (Haffari et al., 2009; Gonz´alez-Rubio et al., 2011) and many other NLP tasks (Olsson, 2009), to name but a few studies. It has also been recognized by the active learning community that correcting the most useful parts first is often not optimal in terms of efficiency, since these parts tend to be the most difficult to manually annotate (Settles et al., 2008). The authors advocate the use of a user model to predict the supervision effort, and select the instances with best “bang-for-thebuck.” This prediction of supervision effort was successful, and was further refined in other NLP-related studies (Tomanek et al."
Q14-1014,I08-7018,0,0.0306138,"be grouped into segments are positions between adjacent characters. 5.2.1 Experimental Setup Neubig et al. (2011) have proposed a pointwise method for Japanese word segmentation that can be trained using partially annotated sentences, which makes it attractive in combination with active learning, as well as our segmentation method. The authors released their method as a software package “KyTea” that we employed in this user study. We used KyTea’s active learning domain adaptation toolkit8 as a baseline. For data, we used the Balanced Corpus of Contemporary Written Japanese (BCCWJ), created by Maekawa (2008), with the internet Q&A subcorpus as in-domain data, and the whitepaper subcorpus as background data, a domain adaptation scenario. Sentences were drawn from the in-domain corpus, and the manually annotated data was then used to train KyTea, along with the pre-annotated background data. The goal (objective function) was to improve KyTea’s classification accuracy on an indomain test set, given a constrained time budget of 30 minutes. There were again 2 supervision modes: ANNOTATE and SKIP . Note that this is essentially a batch active learning setup with only one iteration. We conducted experim"
Q14-1014,2006.iwslt-papers.1,0,0.0655285,"Missing"
Q14-1014,P11-2093,1,0.908647,"Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek and Hahn, 2009; Neubig et al., 2011). In these cases, it is possible to achieve much higher benefit per annotated word by annotating sub-sentential units (Figure 1 (b)). However, as Settles et al. (2008) point out, simply maximizing the benefit per annotated instance is not enough, as the real supervision effort varies 169 Transactions of the Association for Computational Linguistics, 2 (2014) 169–180. Action Editor: Eric Fosler-Lussier. c Submitted 11/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. Avg. time / instance [sec] 6 Transcription task Word segmentation task 4 2 0 1 3 5 7 9 11 1"
Q14-1014,P10-1037,0,0.020503,"n terms of the same monetary unit. Vijayanarasimhan et al. (2010) and Donmez and Carbonell (2008) use a more practical approach that specifies a constrained optimization problem by allowing only a limited time budget for supervision. Our approach is a generalization thereof and allows either specifying an upper bound on the predicted cost, or a lower bound on the predicted utility. The main novelty of our presented approach is the explicit modeling and selection of segments of various sizes, such that annotation efficiency is optimized according to the specified constraints. While some works (Sassano and Kurohashi, 2010; Neubig et al., 2011) have proposed using subsentential segments, we are not aware of any previous work that explicitly optimizes that segmentation. 7 Conclusion We presented a method that can effectively choose a segmentation of a language corpus that optimizes supervision efficiency, considering not only the actual usefulness of each segment, but also the annotation cost. We reported noticeable improvements over strong baselines in two user studies. Future user experiments with more participants would be desirable to verify our observations, and allow further analysis of different factors s"
Q14-1014,D08-1112,0,0.840039,"rors in parentheses. The underlined parts are to be corrected by a human for (a) sentences, (b) words, or (c) the proposed segmentation. Introduction Many natural language processing (NLP) tasks require human supervision to be useful in practice, be it to collect suitable training material or to meet some desired output quality. Given the high cost of human intervention, how to minimize the supervision effort is an important research problem. Previous works in areas such as active learning, post editing, and interactive pattern recognition have investigated this question with notable success (Settles, 2008; Specia, 2011; Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek"
Q14-1014,2011.eamt-1.12,0,0.438874,"eses. The underlined parts are to be corrected by a human for (a) sentences, (b) words, or (c) the proposed segmentation. Introduction Many natural language processing (NLP) tasks require human supervision to be useful in practice, be it to collect suitable training material or to meet some desired output quality. Given the high cost of human intervention, how to minimize the supervision effort is an important research problem. Previous works in areas such as active learning, post editing, and interactive pattern recognition have investigated this question with notable success (Settles, 2008; Specia, 2011; Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek and Hahn, 2009"
Q14-1014,P09-1117,0,0.143235,"es, 2008; Specia, 2011; Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek and Hahn, 2009; Neubig et al., 2011). In these cases, it is possible to achieve much higher benefit per annotated word by annotating sub-sentential units (Figure 1 (b)). However, as Settles et al. (2008) point out, simply maximizing the benefit per annotated instance is not enough, as the real supervision effort varies 169 Transactions of the Association for Computational Linguistics, 2 (2014) 169–180. Action Editor: Eric Fosler-Lussier. c Submitted 11/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. Avg. time / instance [sec] 6 Transcription task Word segmentation tas"
Q14-1014,P10-1118,0,0.0154553,"i et al., 2009; Gonz´alez-Rubio et al., 2011) and many other NLP tasks (Olsson, 2009), to name but a few studies. It has also been recognized by the active learning community that correcting the most useful parts first is often not optimal in terms of efficiency, since these parts tend to be the most difficult to manually annotate (Settles et al., 2008). The authors advocate the use of a user model to predict the supervision effort, and select the instances with best “bang-for-thebuck.” This prediction of supervision effort was successful, and was further refined in other NLP-related studies (Tomanek et al., 2010; Specia, 2011; Cohn and Specia, 2013). Our approach to user modeling using GP regression is inspired by the latter. Most studies on user models consider only supervision effort, while neglecting the accuracy of human annotations. The view on humans as a perfect oracle has been criticized (Donmez and Carbonell, 2008), since human errors are common and can negatively affect supervision utility. Research on human-computer-interaction has identified the modeling of human errors as very difficult (Olson and Olson, 1990), depending on factors such as user experience, cognitive load, user interface"
Q15-1041,D11-1039,0,0.0373913,"Missing"
Q15-1041,P14-1133,0,0.0472303,"semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more canonical forms of these questions, with the canonical NL questions being trivially convertible to an MR. Berant and Liang (2014) extract entities from a full-text question, map these entities into a set of candidate MRs, and generate canonical utterances accordingly. Then the canonical utterance that best paraphrases the input is chosen, thereby outputting the corresponding MR. Our approach is the similar but orthogonal to these works in that we focus on situations where the original user input is underspecified, and try to generate a natural language paraphrase that more explicitly states the user intention for disambiguation purposes. A second difference is that we do not use separate model to do paraphrasing, instea"
Q15-1041,W11-2103,0,0.0148728,"ig, 2013) decoder. Unless otherwise specified, the default settings of the decoder are used. Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-Burch et al., 2011) (News). This data represents standard English text unrelated to questions. Second, we use a part of the question paraphrase data gathered by Fader et al. (2013) (Questions).4 This data consists entirely of questions, and thus is a better representative of the latent questions behind the input queries. Finally, we used the full questions from Geoquery sentences to build the language model, building a different language model for each fold, completely separate from the test set. Table 2 gives the details of each dataset. Data News Questions Geoquery Sent. 44.0M 20.2M 792 Tok. 891M 174M ∼1.6K LM"
Q15-1041,J07-2003,0,0.754042,"rse the ambiguous input with significantly better accuracy. 2 Semantic Parsing using Context Free Grammars As a baseline SP formalism, we follow Wong and Mooney (2006) in casting SP as a problem of translation from a natural language query into its MR. This translation is done using synchronous context free grammars, which we describe in detail in the following sections. 2.1 Synchronous Context Free Grammars Synchronous context free grammars are a generalization of context-free grammars (CFGs) that generate pairs of related strings instead of single strings. Slightly modifying the notation of Chiang (2007), we can formalize SCFG rules as: X → ⟨γs , γt ⟩ (1) where X is a non-terminal and γs and γt are strings of terminals and indexed non-terminals on the source and target side of the grammar. SCFGs have recently come into favor as a tool for statistical machine translation (SMT). In SMT, a synchronous rule could, for example, take the form of: X → ⟨X0 eats X1 , X0 wa X1 wo taberu⟩ (2) where γs is an English string and γt is a Japanese string. Each non-terminal on the right side is indexed, with non-terminals with identical indices corresponding to each-other. Given the SCFG grammar, we can addit"
Q15-1041,P13-1158,0,0.217481,"nguage model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-Burch et al., 2011) (News). This data represents standard English text unrelated to questions. Second, we use a part of the question paraphrase data gathered by Fader et al. (2013) (Questions).4 This data consists entirely of questions, and thus is a better representative of the latent questions behind the input queries. Finally, we used the full questions from Geoquery sentences to build the language model, building a different language model for each fold, completely separate from the test set. Table 2 gives the details of each dataset. Data News Questions Geoquery Sent. 44.0M 20.2M 792 Tok. 891M 174M ∼1.6K LM Size 5.5G 1.5G ∼96K Table 2: Details of the data used to build LMs. In addition, because the Geoquery data is useful but small, for all 3-SCFG systems, we perfo"
Q15-1041,N04-1035,0,0.0247175,"as the natural language query and γt as an MR based on λ calculus. SCFG rules are automatically learned from pairs of sentences with input text and the corresponding MR, where the MR is expressed as a parse tree whose internal nodes are predicates, operators, or quantifiers. In this paper, we follow Li et al. (2013)’s approach 573 to extract a grammar from this parallel data. In this approach, for each pair, statistical word alignment aligns natural language tokens with the corresponding elements in the MR, then according to the alignment, minimal rules are extracted with the GHKM algorithm (Galley et al., 2004; Li et al., 2013). Then, up to k minimal rules are composed to form longer rules (Galley et al., 2006), while considering the relationship between logical variables. Finally, unaligned NL tokens are aligned by attaching them to the highest node in the tree that does not break the consistencies of alignment, as specified in Galley et al. (2006). 2.3 Additional Rules While basic rules extracted above are quite effective in parsing the training data,2 we found several problems when we attempt to parse unseen queries. To make our parser more robust, we add two additional varieties of rules. First"
Q15-1041,P06-1121,0,0.0351764,"ed from pairs of sentences with input text and the corresponding MR, where the MR is expressed as a parse tree whose internal nodes are predicates, operators, or quantifiers. In this paper, we follow Li et al. (2013)’s approach 573 to extract a grammar from this parallel data. In this approach, for each pair, statistical word alignment aligns natural language tokens with the corresponding elements in the MR, then according to the alignment, minimal rules are extracted with the GHKM algorithm (Galley et al., 2004; Li et al., 2013). Then, up to k minimal rules are composed to form longer rules (Galley et al., 2006), while considering the relationship between logical variables. Finally, unaligned NL tokens are aligned by attaching them to the highest node in the tree that does not break the consistencies of alignment, as specified in Galley et al. (2006). 2.3 Additional Rules While basic rules extracted above are quite effective in parsing the training data,2 we found several problems when we attempt to parse unseen queries. To make our parser more robust, we add two additional varieties of rules. First, we add a deletion rule which allows us to delete any arbitrary word w with any head symbol X, formall"
Q15-1041,D11-1108,0,0.0328147,"Missing"
Q15-1041,N13-1092,0,0.0757262,"Missing"
Q15-1041,P09-1069,0,0.299465,"ng et al. (2013) showed that doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2009) and Miller et al. (1994) have also used the guidance of natural language syntax for semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more canonical forms of the"
Q15-1041,N13-1116,0,0.0241653,"Missing"
Q15-1041,W11-2123,0,0.0140013,"the input query into an MR including λ calculus expressions, performing β-reduction to remove the λ function, then firing the query against the database. Before querying the database, we also apply Wong and Mooney (2007)’s type-checking to ensure that all MRs are logically valid. For parsing, we implemented CKY-based parsing of tri-synchronous grammars on top of the Travatar (Neubig, 2013) decoder. Unless otherwise specified, the default settings of the decoder are used. Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-Burch et al., 2011) (News). This data represents standard English text unrelated to questions. Second, we use a part of the question paraphrase data gathered by Fader et al. (2013) (Questions).4 This data consists entirely"
Q15-1041,W04-3250,0,0.158833,"Missing"
Q15-1041,D13-1161,0,0.0432772,"Paraphrasing and Verification Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology, Japan {philip.arthur.om0, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp Abstract player?). Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the original input. This paraphra"
Q15-1041,P11-1060,0,0.0494075,"iguous Input through Paraphrasing and Verification Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology, Japan {philip.arthur.om0, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp Abstract player?). Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the or"
Q15-1041,W07-0716,0,0.0308294,"te model to do paraphrasing, instead using the same model to do paraphrasing and semantic parsing synchronously. This has the advantage of being able to scale more easily to complicated and highly compositional questions such as the ones found in Geoquery. In addition to being useful for semantic parsing, SCFGs have also been used for paraphrasing. A variety of research has used SCFG-based paraphrases for text-to-text generation tasks like sentence compression (Cohn and Lapata, 2009; Ganitkevitch et al., 2011), or expanding the set of reference translations for machine translation evaluation (Madnani et al., 2007). In this paper we have introduced a novel use of 3-way SCFGs that allows us to simultaneously do semantic parsing and text-to-text generation. To our knowledge, this is the first method to parse an underspecified input by trying to reconstruct a more explicit paraphrase of the input and validate 582 the naturalness of the paraphrase to disambiguate the meaning of the original input. 8 Conclusion and Future Work In this paper we introduced a method for constructing a semantic parser for ambiguous input that paraphrases the ambiguous input into a more explicit form, and verifies the correctness"
Q15-1041,J83-1001,0,0.767026,"In addition, Wang et al. (2013) showed that doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2009) and Miller et al. (1994) have also used the guidance of natural language syntax for semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more"
Q15-1041,P94-1004,0,0.166442,"hat doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2009) and Miller et al. (1994) have also used the guidance of natural language syntax for semantic parsing. However, the usage of natural language syntax in the semantic parsing on keyword queries are not trivial. For example, the approach using syntax tree of the input side from Ge and Mooney (2009) can not be directly applied to the keyword query as syntax parsing on keyword query itself is not a trivial problem. There have also been a few methods proposed to combine paraphrasing with semantic parsing. Fader et al. (2013) proposed a method to map from full questions to more canonical forms of these questions, with the ca"
Q15-1041,P11-1064,1,0.832921,"880 sentences representing natural language questions about U.S. Geography, and their corresponding MRs. 6.1 Setup Data: We use the full Geoquery dataset using the same 10 folds of 792 and 88 test data used by Wong and Mooney (2007). We created keyword queries according to the process described in Section 3. We follow standard procedure of removing punctuation for all natural language text, regardless of whether it is a keyword or full question. We also perform stemming on all natural language text, both in the keyword and question queries. Rule Extraction: Alignment is performed by pialign (Neubig et al., 2011) with the setting forcing one-to-many alignments. The algorithm to extract the tri-synchronous grammar is as discussed in Section 4.2 and maximum size of the rules for composition is 4. Decoding: To query the database, we use prolog queries fired against the Geoquery database. The parsing problem can thus be considered the task of decoding from underspecified natural language 3 We also tried gradient-based optimization methods and large feature sets as in Wong and Mooney (2007) and Li et al. (2013), but the dense feature set and MERT achieved similar results with shorter training time. 577 que"
Q15-1041,N15-1033,1,0.821749,"rd inputs and question paraphrases were available, it is theoretically possible for our proposed method to learn from this data as well. 4 Joint Semantic Parsing and Paraphrasing using Tri-Synchronous Grammars In this section we describe our proposed method to parse underspecified and ungrammatical input while jointly generating a paraphrase that can be used to disambiguate the meaning of the original query. 4.1 Generalized Synchronous Context Free Grammars Before defining the actual parsing framework, we first present a generalization of SCFGs, the nsynchronous context free grammar (n-SCFG) (Neubig et al., 2015). In an n-SCFG, the elementary structures are rewrite rules of n − 1 target sides: X → ⟨γ1 , γ2 , ..., γn ⟩ (4) Grammar r0 QUERY → ⟨CONJ0 , give me the CONJ0 , answer(x1 , CONJ0 )⟩ r1 CONJ → ⟨FORM0 STATE1 , FORM0 in STATE1 , (FORM0 , loc(x1 , x2 ), const(x2 , stateid(STATE1 )))⟩ r2 FORM → ⟨cities, cities, city(x1 )⟩ r3 STATE → ⟨virginia, virginia, virginia⟩ Derivations ⟨QUERY0 , QUERY0 , QUERY0 ⟩ r0 ⇒ ⟨CONJ0 , give me the CONJ0 , answer(x1 , CONJ0 )⟩ r1 ⇒ ⟨FORM2 STATE3 , give me the FORM2 in STATE3 , answer(x1 , (FORM2 , loc(x1 , x2 ), const(x2 , stateid(STATE3 ))) ⟩ r2 ⇒ ⟨cities STATE3 , give"
Q15-1041,P13-4016,1,0.853123,"Li et al. (2013), but the dense feature set and MERT achieved similar results with shorter training time. 577 queries into prolog queries. This is done by performing decoding of the SCFG-based parsing model to translate the input query into an MR including λ calculus expressions, performing β-reduction to remove the λ function, then firing the query against the database. Before querying the database, we also apply Wong and Mooney (2007)’s type-checking to ensure that all MRs are logically valid. For parsing, we implemented CKY-based parsing of tri-synchronous grammars on top of the Travatar (Neubig, 2013) decoder. Unless otherwise specified, the default settings of the decoder are used. Language Model: For all 3-SCFG systems we use a 4-gram Kneser-Ney smoothed language model trained using the KenLM toolkit (Heafield, 2011). Standard preprocessing such as lowercasing and tokenization is performed before training the models. As it is of interest whether or not the type of data used to train the language model affects the resulting performance, we build language models on several types of data. First, we use a corpus of news data from the Workshop on Machine Translation evaluation data (Callison-"
Q15-1041,J03-1002,0,0.00411021,"the generated paraphrase. • Language Model: Counts the log language model probability of the paraphrase. • Unknown: Counts the number of tokens in the paraphrase that are unknown in the language model. • Paraphrase Length: Counts the number of words in the paraphrase, and can be calculated for each rule as the number of terminals in the paraphrase. This feature helps compensate for the fact that language models prefer shorter sentences. 5.3 Learning Feature Weights Now that we have defined the feature space, we need to optimize the weights. For this we use minimum error rate training (MERT) (Och and Ney, 2003), maximizing the number of correct answers over the entire corpus.3 6 Experiment and Analysis We evaluate our system using the Geoquery corpus (Zelle and Mooney, 1996), which contains 880 sentences representing natural language questions about U.S. Geography, and their corresponding MRs. 6.1 Setup Data: We use the full Geoquery dataset using the same 10 folds of 792 and 88 test data used by Wong and Mooney (2007). We created keyword queries according to the process described in Section 3. We follow standard procedure of removing punctuation for all natural language text, regardless of whether"
Q15-1041,D09-1001,0,0.242932,"Missing"
Q15-1041,C12-1143,0,0.293503,"aning representation via verification using a language model that calculates the probability of each paraphrase.1 Much previous work on SP has focused on the case of answering natural language queries to a database of facts, where the queries generally take the form of full sentences such as “What is the height of Kobe Bryant?” While answering these questions provides an excellent first step to natural language information access, in many cases the input is not a full sentence, but something more underspecified and ungrammatical. For example, this is the case for keyword-based search queries (Sajjad et al., 2012) or short dialogue utterances (Zettlemoyer and Collins, 2007). 1 Introduction Semantic parsing (SP) is the problem of parsing a given natural language (NL) sentence into a meaning representation (MR) conducive to further processing by applications. One of the major challenges in SP stems from the fact that NL is rife with ambiguities. For example, even the simple sentence “Where can we eat a steak in Kobe?” contains syntactic ambiguities (“eat in Kobe” or “steak in Kobe”?), quantifier scope ambiguities (do we all eat one steak, or each eat one steak?), and word sense ambiguities (is Kobe a cit"
Q15-1041,P13-2008,0,0.0256986,"s. Underspecified queries are commonly entered into search engines, leading to large result sets that are difficult for users to navigate (Sajjad et al., 2012). Studies have shown that there are several ways to deal with this problem, including query reformulation, which can fall in the categories of query expansion or query substitution (Shokouhi et al., 2014; Xue and Croft, 2013). Leveling (2010) proposed a paraphrasing method that tries to reconstruct original questions given keyword inputs in the IR context, but did not model this reformulation together with semantic parsing. In addition, Wang et al. (2013) showed that doing paraphrasing on the queries for web search is able to reduce the mismatch between queries and documents, resulting in a gain in search accuracy. Using paraphrasing to resolve ambiguity is not 6 Because the shuffling process is random we could conceivably generate and train with multiple shuffled versions, but because the Questions data is relatively large already, we only train the paraphrasing system with the single permutation of keywords generated by the shuffling. new, as it was used to resolve ambiguity interactively with a user’s input (McKeown, 1983). Ge and Mooney (2"
Q15-1041,N06-1056,0,0.145141,"uery-logic pairs. First we note that baseline SCFG parser achieves reasonable accuracy on regular questions but when the same method is used with underspecified input, the system accuracy decreases significantly. On the other hand, when incorporating the proposed tri-synchronous 572 grammar to generate paraphrases and verify them with a language model, we find that it is possible to recover the loss of accuracy, resulting in a model that is able to parse the ambiguous input with significantly better accuracy. 2 Semantic Parsing using Context Free Grammars As a baseline SP formalism, we follow Wong and Mooney (2006) in casting SP as a problem of translation from a natural language query into its MR. This translation is done using synchronous context free grammars, which we describe in detail in the following sections. 2.1 Synchronous Context Free Grammars Synchronous context free grammars are a generalization of context-free grammars (CFGs) that generate pairs of related strings instead of single strings. Slightly modifying the notation of Chiang (2007), we can formalize SCFG rules as: X → ⟨γs , γt ⟩ (1) where X is a non-terminal and γs and γt are strings of terminals and indexed non-terminals on the sou"
Q15-1041,P07-1121,0,0.424701,"Semantic Parsing of Ambiguous Input through Paraphrasing and Verification Philip Arthur, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology, Japan {philip.arthur.om0, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp Abstract player?). Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less a"
Q15-1041,D07-1071,0,0.183538,"age model that calculates the probability of each paraphrase.1 Much previous work on SP has focused on the case of answering natural language queries to a database of facts, where the queries generally take the form of full sentences such as “What is the height of Kobe Bryant?” While answering these questions provides an excellent first step to natural language information access, in many cases the input is not a full sentence, but something more underspecified and ungrammatical. For example, this is the case for keyword-based search queries (Sajjad et al., 2012) or short dialogue utterances (Zettlemoyer and Collins, 2007). 1 Introduction Semantic parsing (SP) is the problem of parsing a given natural language (NL) sentence into a meaning representation (MR) conducive to further processing by applications. One of the major challenges in SP stems from the fact that NL is rife with ambiguities. For example, even the simple sentence “Where can we eat a steak in Kobe?” contains syntactic ambiguities (“eat in Kobe” or “steak in Kobe”?), quantifier scope ambiguities (do we all eat one steak, or each eat one steak?), and word sense ambiguities (is Kobe a city in Japan; or an NBA basketball 1 Tools to replicate our exp"
sakti-etal-2014-towards,W13-4604,1,\N,Missing
sakti-etal-2014-towards,maekawa-etal-2000-spontaneous,0,\N,Missing
sakti-etal-2014-towards,P11-2093,1,\N,Missing
shimizu-etal-2014-collection,N12-1048,0,\N,Missing
shimizu-etal-2014-collection,E09-1040,0,\N,Missing
shimizu-etal-2014-collection,N13-1023,0,\N,Missing
W09-3405,2007.sigdial-1.45,0,0.0254459,"o express the dialogue act of each utterance. Many studies have focused on developing spoken dialogue systems. Their typical task domains included the retrieval of information from databases or making reservations, such as airline information e.g., DARPA Communicator (Walker et al., 2001) and train information e.g., ARISE (Bouwman et al., 1999) and MASK (Lamel et al., 2002). Most studies assumed a definite and consistent user objective, and the dialogue strategy was usually designed to minimize the cost of information access. Other target tasks include tutoring and trouble-shooting dialogues (Boye, 2007). 32 Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32–39, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP Table 2: Overview of Kyoto corpus dialogue type F2F # of dialogues 114 3 # of guides avg. # of utterance 365.4 / dialogue (guide) avg. # of utterance 301.7 / dialogue (tourist) sional tour guide and a tourist. Three guides, one male and two females, were employed to collect the dialogues. All three guides were involved in almost the same number of dialogues. The guides used maps, guidebooks, and a PC connected to the internet. tour guide dialo"
W09-3405,J02-3001,0,0.0350253,"e t y pe ……. o b j ect Figure 3: A part of the semantic category hierarchy Kinkakuji temple.” is annotated as shown in Figure 2. In this figure, the semantic content tag preference.action indicates that the predicate portion expresses the speaker’s preference for the speaker’s action, while the semantic content tag preference.spot.name indicates the name of the spot as the object of the speaker’s preference. Although we do not define semantic the role (e.g., object (Kinakuji temple) and subject (I)) of each argument item in this case, we can use conventional semantic role labeling techniques (Gildea and Jurafsky, 2002) to estimate them. Therefore, we do not annotate such semantic role labels in the corpus. 5.1 Annotation of semantic contents tags The annotation of semantic contents tags is performed by the following four steps. First, an utterance is analyzed by a morphological analyzer, ChaSen3 . Second, the morphemes are chunked into dependency unit (bunsetsu). Third, dependency analysis is performed using a Japanese dependency parser, CaboCha4 . Finally, we annotate the semantic content tags for each bunsetsu unit by using our annotation tool. An example of an annotation is shown in Table 1. Each row in"
W09-3405,W02-0708,0,0.13753,"collaboratively operate the WOZ system to serve a user (tourist). structure of the transducers and the weight for each state transitions from an annotated corpus. Thus, the corpus must be sufficiently rich in information to describe the consulting dialogue to construct the statistical dialogue manager via such techniques. In addition, a detailed description would be preferable when developing modules that focus on spoken language understanding and generation modules. In this study, we adopt dialogue acts (DAs) (Bunt, 2000; Shriberg et al., 2004; Bangalore et al., 2006; Rodriguez et al., 2007; Levin et al., 2002) for this information and annotate DAs in the corpus. In this paper, we describe the design of the Kyoto tour guide dialogue corpus in Section 2. Our design of the DA annotation is described in Section 3. Sections 4 and 5 respectively describe two types of the tag sets, namely, the speech act tag and the semantic content tag. In the telephone dialogues, two female guides who are the same for the WOZ dialogues were employed. In these dialogues, we used the WOZ system, but we did not need the speech synthesis program. The guide and a tourist shared the same interface in different rooms, and they"
W09-3405,maekawa-etal-2000-spontaneous,0,0.0469089,"ideki Kashioka Satoshi Nakamura MASTAR Project, National Institute of Information and Communications Technology Hikaridai, Keihanna Science City, JAPAN kiyonori.ohtake (at) nict.go.jp Abstract In such tasks, dialogue scenarios or agendas are usually described using a (dynamic) tree structure, and the objective is to satisfy all requirements. In this paper, we introduce our corpus, which is being developed as part of a project to construct consulting dialogue systems, that helps the user in making a decision. So far, several projects have been organized to construct speech corpora such as CSJ (Maekawa et al., 2000) for Japanese. The size of CSJ is very large, and a great part of the corpus consists of monologues. Although, CSJ includes some dialogues, the size of dialogues is not enough to construct a dialogue system via recent statistical techniques. In addition, relatively to consulting dialogues, the existing large dialogue corpora covered very clear tasks in limited domains. However, consulting is a frequently used and very natural form of human interaction. We often consult with a sales clerk while shopping or with staff at a concierge desk in a hotel. Such dialogues usually form part of a series o"
W09-3405,W07-1524,0,0.0612339,"Missing"
W09-3405,W04-2319,0,0.243262,"e guide and operators used own computer connected each other, and they collaboratively operate the WOZ system to serve a user (tourist). structure of the transducers and the weight for each state transitions from an annotated corpus. Thus, the corpus must be sufficiently rich in information to describe the consulting dialogue to construct the statistical dialogue manager via such techniques. In addition, a detailed description would be preferable when developing modules that focus on spoken language understanding and generation modules. In this study, we adopt dialogue acts (DAs) (Bunt, 2000; Shriberg et al., 2004; Bangalore et al., 2006; Rodriguez et al., 2007; Levin et al., 2002) for this information and annotate DAs in the corpus. In this paper, we describe the design of the Kyoto tour guide dialogue corpus in Section 2. Our design of the DA annotation is described in Section 3. Sections 4 and 5 respectively describe two types of the tag sets, namely, the speech act tag and the semantic content tag. In the telephone dialogues, two female guides who are the same for the WOZ dialogues were employed. In these dialogues, we used the WOZ system, but we did not need the speech synthesis program. The guide"
W09-3405,P01-1066,0,0.0305516,"Missing"
W09-3405,P06-1026,0,\N,Missing
W09-3410,P79-1022,0,0.405633,"es and applications. In recent decades, corpus development has seen rapid growth for many languages such as English, Japanese, and Chinese. For Chinese, since there are no plain delimiters among the words, the creation of a segmented and part-of-speech (POS)-tagged corpus is the initial step for most statistical language processes. Several such Chinese corpora have been developed since the 1990s. The two most typical are People’s Daily corpus (referred to as PKU), jointly developed by the Institute of Computational Linguistics of Peking University and the Fujitsu Research & Development Center [1], and 70 Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 70–75, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP constitute a set of parallel corpora. The following shows examples of sentences in the three languages: Japanese and English. Finally, in Section 6, we discuss the performance of the corpora, the problems that remain in the corpora, and give our ideas concerning future work. Chn.: BTEC1jpn067 3870zh\\我想喝浓咖啡。 Eng.: BTEC1jpn067 3870en\\I'd like to have some strong coffee. Jap.:BTEC1jpn067 3870ja\\濃いコーヒーが飲みたい。 2. Current NICT C"
W10-4339,W09-3405,1,0.921607,"ed on the state model through reinforcement learning with a natural policy gradient approach using a user simulator trained on the collected dialogue corpus. 1 p1 Criteria 1. Cherry Blossoms v11 … Alternatives (choices) KinkakujiTemple p2 2. Japanese Garden v12 … RyoanjiTemple p3 3. Easy Access ・・・・・ v13 … NanzenjiTemple ・・・・・ Figure 1: Hierarchy structure for sightseeing guidance dialogue by a car navigation system. In this work, we deal with a sightseeing planning task where the user determines the sightseeing spot to visit, with little prior knowledge about the target domain. The study of (Ohtake et al., 2009), which investigated human-human dialogue in such a task, reported that such consulting usually consists of a sequence of information requests from the user, presentation and elaboration of information about certain spots by the guide followed by the user’s evaluation. We thus focus on these interactions. Several studies have featured decision support systems in the operations research field, and the typical method that has been employed is the Analytic Hierarchy Process (Saaty, 1980) (AHP). In AHP, the problem is modeled as a hierarchy that consists of the decision goal, the alternatives for"
W10-4339,P08-1055,0,0.0295842,"The information about the spot in terms of the criteria is not known to the users, but is obtained only via navigating through the system’s information. In addition, spoken dialogue systems usually handle several candidates and criteria, making pairwise comparison a costly affair. We thus consider a spoken dialogue framework that estimates the weights for the user’s preference (potential preferences) as well as the user’s knowledge Introduction In many situations where spoken dialogue interfaces are used, information access by the user is not a goal in itself, but a means for decision making (Polifroni and Walker, 2008). For example, in a restaurant retrieval system, the user’s goal may not be the extraction of price information but to make a decision on candidate restaurants based on the retrieved information. This work focuses on how to assist a user who is using the system for his/her decision making, when he/she does not have enough knowledge about the target domain. In such a situation, users are often unaware of not only what kind of information the system can provide but also their own preference or factors that they should emphasize. The system, too, has little knowledge about the user, or where his/"
W10-4339,N07-2038,0,0.0283932,"parameter Kuser = (k1 , k2 , . . . , kM ) that shows if the user has the perception that the system is able to handle or he/she is interested in the determinants. km is set to “1” if the user knows (or is listed by system’s recommendations) that the system can handle determinant m and “0” when he/she does not. For example, the state that the determinant m is the potential preference of a user (but he/she is unaware of that) is represented by (km = 0, pm = 1). This idea is in contrast to previous research which assumes some fixed goal observable by the user from the beginning of the dialogue (Schatzmann et al., 2007). A user’s local weight vnm for spot n in terms of determinant m is set to “1”, when the system lets the user know that the evaluation of spots is “1” through recommendation Methods 1, 2 and 6. We constructed a user simulator that is based on the statistics calculated through an experiment with the trial system (Misu et al., 2010) as well as the knowledge and preference of the user. That is, the user’s communicative act catuser and the semantic content sctuser for the system’s recommendation atsys are generated based on the following equation: Knowledge base Our back-end DB consists of 15 sigh"
W11-2028,W09-3936,0,0.0586725,"Missing"
W11-2028,W10-4318,0,0.0309577,"ckchannels under the hypothesis of: People will give more spontaneous backchannels to a spoken dialogue system that makes more spontaneous backchannel-inviting cues than a spoken dialogue system that makes less spontaneous ones. which is derived from the Media Equation (Reeves and Nass, 1996). 2 Related Works A number of studies have aimed at improving the naturalness of TTS. Though most of these have focused on means of realizing a clear and easy-to-listen-to reading-style speech, some attempts have been made at spontaneous conversational speech. Andersson (Andersson et al., 2010) and Marge (Marge et al., 2010) focused on lexi259 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 259–265, c Portland, Oregon, June 17-18, 2011. 2011 Association for Computational Linguistics cal phenomena such as lexical filler and acknowledgments in spontaneous speech, and showed that inserting them improves the naturalness of humancomputer dialogues. In this work, we tackle constructing a natural dialogue-style TTS system focusing on prosodic phenomena such as intonation and phoneme duration. In the field of conversation analysis, many studies analy"
W13-4604,P11-2093,1,0.605576,"Missing"
W13-4604,2012.eamt-1.60,0,0.0168747,"in the previous section. For training, 4,000 sentences is not enough to build an accurate MT system, so we add several additional corpora for each language pair. For Japanese-English parallel training data, we add the Eijiro dictionary1 and its accompanying sample sentences, the BTEC corpus(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or info"
W13-4604,J07-2003,0,0.120881,"Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or informal. On the other hand, using syntactic information has been shown to improve translation, particularly between language pairs with different syntactic structures such as those we are handling in our experiments. Thus it will be interesting to see which methodology can produce better results, and also if any difference in the effectivenes"
W13-4604,C04-1114,0,0.0639568,"Missing"
W13-4604,P13-4016,1,0.882079,"Missing"
W13-4604,I11-1087,1,0.830624,"Missing"
W13-4604,J03-1002,0,0.0085147,"Missing"
W13-4604,D10-1092,0,0.0662551,"Missing"
W13-4604,P02-1040,0,0.0879458,"Missing"
W13-4604,N03-1017,0,0.0066382,"us(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or informal. On the other hand, using syntactic information has been shown to improve translation, particularly between language pairs with different syntactic structures such as those we are handling in our experiments. Thus it will be interesting to see which methodology can produce better r"
W13-4604,P07-2045,0,0.0126736,"Missing"
W13-4604,P10-1017,0,0.13341,"Missing"
W13-4604,takezawa-etal-2002-toward,0,0.0605347,"edical communication as well. As a result, it is likely that adapting to medical terminology of the domain is somewhat less important than adapting to the conversational speaking style of the speech. 4 4.1 Experimental Setup For the tuning and test data for our translation system, we use the data described in the previous section. For training, 4,000 sentences is not enough to build an accurate MT system, so we add several additional corpora for each language pair. For Japanese-English parallel training data, we add the Eijiro dictionary1 and its accompanying sample sentences, the BTEC corpus(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hie"
W13-4604,I05-3027,0,0.0659246,"Missing"
W13-4604,A00-2028,0,0.0297292,"challenging for a number of reasons. The first reason is that communication of incomplete or incorrect information could lead to a mistaken diagnosis with severe consequences, and thus extremely high levels of accuracy and reliability are 22 International Joint Conference on Natural Language Processing Workshop on Natural Language Processing for Medical and Healthcare Fields, pages 22–29, Nagoya, Japan, 14-18 October 2013. man interpreters, either based on a manual request of one of the users, or through automatic detection of when the dialogue is going poorly, such as the method described by Walker et al. (2000). Even with this fall-back to human interpreters, it is still desirable that the automatic translation system is effective as possible. In order to ensure this, we must be certain that the ASR, MT, and TTS models are all tuned to work as well as possible in medical situations. Some potential problems that we have identified so far based on our analysis of data are as follows: Figure 1: An overview of the use scenario for the medical translation system. Specialized Vocabulary: Perhaps the most obvious problem is that the ASR, MT, and TTS systems must all be able to handle the specialized vocabu"
W13-4604,C04-1168,0,0.0303816,"r. However, as the cost of hiring and maintaining medical interpreters is quite high, we would also like to reduce our reliance on human effort as much as possible. Thus, each device will use automatic translation by default, but also have functionality to connect to huTranslation/Synthesis of Erroneous Input: As we can expect ASR not to be perfect, it will be necessary to be able to translate input that contains errors. This problem can potentially be ameliorated by passing multiple speech recognition hypotheses to translation (Ney, 1999), and jointly optimizing the parameters of ASR and MT (Zhang et al., 2004; Ohgushi et al., 2013). In addition, it will also be necessary to resolve difficulties in TTS due to grammatical errors, lack of punctuation, and unknown words (Parlikar et al., 2010). While all of these problems need to be solved to provide high-reliability speech translation systems, in this paper as a first step we focus mainly on the MT system, and relegate the last problem of integration with ASR to future work. 23 3 Medical Translation Corpus Construction and Analysis In this section, we describe our collection of a tri-lingual (Japanese, English, Chinese) corpus to serve as an initial"
W13-4604,W12-4213,0,\N,Missing
W13-4604,P08-1023,0,\N,Missing
W14-3211,N13-1084,0,0.0485056,"Missing"
W14-3211,W10-4346,0,0.245775,"Missing"
W14-4004,P08-1087,0,0.0211581,"phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label"
W14-4004,P06-1077,0,0.155526,"e syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalization also contains some additional preprocessing steps such as determiner elimination, particle insertion and singularization to generate a sentence that is closer to Japanese grammatical structure. In addition to PBMT, there has also recently been interest in syntax-based SMT (Yamada and Knight, 2001; Liu et al., 2006), which translates using syntactic information. However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excepSeveral preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output. On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper,"
W14-4004,D12-1079,0,0.0187559,"of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute"
W14-4004,P11-2031,0,0.0194023,"(test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. Table 5: Optimized weight of HF-feature in each condition HF-feature Reordering + + + + + + Word Processing + + Weight of HF-feature -0.00707078 0.00524676 0.156724 -0.121326 Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexic"
W14-4004,P14-2024,1,0.800601,"be automatically learned in a well-trained T2S model. Therefore, preordering is not expected to cause large gains, unlike in the case of PBMT. However, it can also be thought that preordering can still have a positive influence on the translation model training process, particularly by increasing alignment accuracy. For example, training methods for word alignment such as the IBM or HMM models (Och and Ney, 2003) are affected by word order, and word alignment may be improved by moving word order closer between the two languages. As alignment accuracy plays a important role in T2S translation (Neubig and Duh, 2014), it is reasonable to hypothesize that reordering may also have a positive effect on T2S. In terms of the actual incorporation with the T2S system, we simply follow the process in Figure 1, but output the reordered tree instead of only the reordered terminal nodes as is done for PBMT. 4.2 Reordering Information as Soft Constraints As described in section 4.1.1, T2S work well on language pairs that have very different word order, but is sensitive to alignment accuracy. On the other hand, we know that in most cases Japanese word order tends to be head final, and thus any rules that do not obey h"
W14-4004,P05-1066,0,0.118261,"Missing"
W14-4004,P11-2093,1,0.796984,"of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation"
W14-4004,P13-4016,1,0.791971,"eriment, we examined how much each of the preprocessing steps (Reordering, Lexical Processing) contribute to improve the translation quality of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluatio"
W14-4004,C00-2162,0,0.0855186,"translation probabilities between phrases consisting of multiple words are calculated, and translated phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), method"
W14-4004,W11-1011,0,0.0190313,"tic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute anything to syntax-based machine translation. Specifically, we examine whether the reordering and lexical processing of Head Finalization contributes to the improvement of syntax-based mach"
W14-4004,J03-1002,0,0.0134154,", because translation patterns of T2S are expressed by using source sentence subtrees, the effect of reordering problems are relatively small, and the majority of reordering rules specified by hand can be automatically learned in a well-trained T2S model. Therefore, preordering is not expected to cause large gains, unlike in the case of PBMT. However, it can also be thought that preordering can still have a positive influence on the translation model training process, particularly by increasing alignment accuracy. For example, training methods for word alignment such as the IBM or HMM models (Och and Ney, 2003) are affected by word order, and word alignment may be improved by moving word order closer between the two languages. As alignment accuracy plays a important role in T2S translation (Neubig and Duh, 2014), it is reasonable to hypothesize that reordering may also have a positive effect on T2S. In terms of the actual incorporation with the T2S system, we simply follow the process in Figure 1, but output the reordered tree instead of only the reordered terminal nodes as is done for PBMT. 4.2 Reordering Information as Soft Constraints As described in section 4.1.1, T2S work well on language pairs"
W14-4004,D10-1092,0,0.046309,"Missing"
W14-4004,P03-1021,0,0.140083,"(such as sentences that contain the determiner 37 “no,” or situations where non-literal translations are necessary) and a hard constraint to obey headfinal word order could be detrimental. In order to incorporate this intuition, we add a feature (HF-feature) to translation patterns that conform to the reordering rules of Head Finalization. This gives the decoder ability to discern translation patterns that follow the canonical reordering patterns in English-Japanese translation, and has the potential to improve translation quality in the T2S translation model. We use the log-linear approach (Och, 2003) to add the Head Finalization feature (HF-feature). As in the standard log-linear model, a source sentence f is translated into a target language sentence e, by searching for the sentence maximizing the score: ˆ = arg max wT · h(f , e). e e VP Source side of translation pattern VBD NP hit x0:NP Word alignment Target side of translation pattern x0 wo utta 1. Apply Reordering to source translation pattern VP Reordered translation pattern NP VBD x0:NP hit Target side of translation pattern (1) where h(f , e) is a feature function vector. w is a weight vector that scales the contribution from each"
W14-4004,W10-1736,0,0.239358,"PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language. It’s often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word ordering problems faced by PBMT (Collins et al., 2005; Xu et al., 2009; Isozaki et al., 2010b). One particularly successful example of rulebased syntactic preprocessing is Head Finalization (Isozaki et al., 2010b), a method of syntactic preprocessing for English to Japanese translation that has significantly improved translation quality of English-Japanese PBMT using simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalizati"
W14-4004,P02-1040,0,0.0902419,"s, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. Table 5: Optimized weight of HF-feature in each condition HF-feature Reordering + + + + + + Word Processing + + Weight of HF-feature -0.00707078 0.00524676 0.156724 -0.121326 Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indi"
W14-4004,N03-1017,0,0.0192216,"ttle work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper, we examine whether the sort of rule-based syntactic preprocessing approaches that have proved beneficial for PBMT can contribute to syntax-based SMT. Specifically, we tailor a highly successful preprocessing method for EnglishJapanese PBMT to syntax-based SMT, and find that while the gains achievable are smaller than those for PBMT, significant improvements in accuracy can be realized. 1 Introduction In the widely-studied framework of phrase-based machine translation (PBMT) (Koehn et al., 2003), translation probabilities between phrases consisting of multiple words are calculated, and translated phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs"
W14-4004,P07-2045,0,0.0091836,"2.11 37.99 5 Experiment In our experiment, we examined how much each of the preprocessing steps (Reordering, Lexical Processing) contribute to improve the translation quality of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki"
W14-4004,W04-3250,0,0.0397255,"ing information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. Table 5: Optimized weight of HF-feature in each condition HF-feature Reordering + + + + + + Word Processing + + Weight of HF-feature -0.00707078 0.00524676 0.156724 -0.121326 Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indicate no significant difference in comparison with the condition that has highest translation quality using the bootstrap resampling method (Koehn, 2004) (p &lt; 0.05). For PBMT, we can see that reordering plays an extremely important role, with the highest BLEU and RIBES scores being achieved when using Reordering preprocessing (line 3, 4). Lexical Processing also provided a slight performance gain for PBMT. When we applied Lexical Processing to PBMT, BLEU and RIBES scores were improved (line 1 vs 2), although this gain was not significant when Reordering was performed as well. Overall T2S without any preprocessing achieved better translation quality than all conditions of PBMT (line 1 of T2S vs line 1-4 of PBMT). In addition, BLEU and RIBES sco"
W14-4004,C04-1073,0,0.0946405,"Missing"
W14-4004,N09-1028,0,0.0214648,"ethods for PBMT. PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language. It’s often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word ordering problems faced by PBMT (Collins et al., 2005; Xu et al., 2009; Isozaki et al., 2010b). One particularly successful example of rulebased syntactic preprocessing is Head Finalization (Isozaki et al., 2010b), a method of syntactic preprocessing for English to Japanese translation that has significantly improved translation quality of English-Japanese PBMT using simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sent"
W14-4004,P01-1067,0,0.415385,"simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalization also contains some additional preprocessing steps such as determiner elimination, particle insertion and singularization to generate a sentence that is closer to Japanese grammatical structure. In addition to PBMT, there has also recently been interest in syntax-based SMT (Yamada and Knight, 2001; Liu et al., 2006), which translates using syntactic information. However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excepSeveral preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output. On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based S"
W14-4004,N06-1033,0,0.0175714,"n and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. 34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute anything to syntax-based machine translation. Specifically, we examine whether th"
W14-4004,P13-1083,0,\N,Missing
W14-4004,J08-3004,0,\N,Missing
W15-3057,P07-2045,0,0.00583319,"rds has an important role in CLQA tasks using knowledge bases. In addition, as a result of fine-grained manual analysis, we identify a number of factors of translation results that affect CLQA. 2 GT and YT The questions are translated using Google Translate3 (GT) and Yahoo Translate4 (YT) systems, these commercial systems can be used via web pages. While the details of these systems are not open to the public, it is likely that Google takes a largely statistical MT approach, while the Yahoo engine is rule-based. Moses The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set). A total of 277 million sentences from various genres are used in training. Travatar The questions are translated using Travatar (Neubig, 2013) (the Tra set), a tool for forest-to-string MT that has achieved competitive results on the Japanese-English language pair. The training data is the same as Moses. Data sets To create data that allows us to investigate the influence of translation on QA, we started with a standard QA data set, and created automatic and manual translations. In this section, we describe the data construction in detail. As our seed data, we used a data set ca"
W15-3057,P13-2009,0,0.0233688,"anslation results that affect CLQA accuracy. 1 Introduction Question answering (QA) is the task of searching for an answer to question sentences using some variety of information resource. Generally, documents, web pages, or knowledge bases are used as these information resources. When the language of the question differs from the language of the information resource, the task is called cross-lingual question answering (CLQA) (Magnini et al., 2004; 2 MT is also used in mono-lingual QA tasks when question sentences are translated into the formal language used to query the information resource (Andreas et al., 2013). 1 All data used in the experiments will be released upon publishing of the paper. 442 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 442–449, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. questions from Japanese to English (the HT set). languages. Correspondingly, it is of interest to investigate which factors of translation output affect CLQA accuracy, which is the first step towards designing MT systems that achieve better accuracy on the task. In this paper, to investigate the influence of translation on CLQA using k"
W15-3057,P14-1133,0,0.0141859,"ld be noted ric putting a weight an content words should be that these words are frequent, and thus even NIST used. 2) References that are actually answerable score will not be able to perform adequate evaluaby the QA system should be used. tion, indicating that other measures may be necesWe should qualify this result, however, noting sary. the fact that the results are based on the use solely of the SEMPRE parsing system. While SEMPRE has shown highly competitive results on standard Table 4: Examples of translations with mistaken QA tasks, we also plan to examine other methods syntax such as Berant and Liang (2014)’s semantic pars◦ OR what library system is the sunset branch library in - JA サンセット・ブランチ図書館はどの図書館システムに所属しますか ing through paraphrasing, which may be less sen◦ HT to what library system does sunset branch library belong ◦ GT sunset branch library do you belong to any library system sitive to superficial differences in surface forms of the translation results. We also plan to to optimize ◦ YT which library system does the sunset branch library belong to ◦ Mo sunset branch library, which belongs to the library system machine translation systems using this analysis, ◦ Tra sunset branch library, bel"
W15-3057,2003.mtsummit-papers.32,0,0.0579481,"Ě KďĂŵĂ ŐŽ ƚŽ  Figure 1: Framework of the SEMPRE semantic parsing system used to perform QA corpus, which is linked to the knowledge base through the use of named entity prediction. By default, SEMPRE uses ClueWeb096 (Callan et al., 2009) as the large text corpus and Freebase as the knowledge base. During the QA process itself, this lexicon is used to convert entities into logical forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, and the score is from 0 to 1"
W15-3057,D13-1160,0,0.0102613,"for forest-to-string MT that has achieved competitive results on the Japanese-English language pair. The training data is the same as Moses. Data sets To create data that allows us to investigate the influence of translation on QA, we started with a standard QA data set, and created automatic and manual translations. In this section, we describe the data construction in detail. As our seed data, we used a data set called Free917 (Cai and Yates, 2013). Free917 is a question set made for QA using the large-scale knowledge base “Freebase,” and is widely used in QA research (Cai and Yates, 2013; Berant et al., 2013). It consists of 917 pairs of question sentences and “logical forms” which are computer-processable expressions of the meaning of the question that can be fired against the Freebase database to return the correct answer. Following Cai and Yates (2013), we divide this data into a training set (512 pairs), dev set (129 pairs) and test set (276 pairs). In the remainder of the paper, we refer to the questions in the test set before translation as the original (OR) set. Next, to investigate the influence of translation quality on the accuracy of QA, we created a question set with five different var"
W15-3057,C04-1072,0,0.0495265,"e QA system. ĂůŝŐŶŵĞŶƚ ĚŝĚ KďĂŵĂ ŐŽ ƚŽ  Figure 1: Framework of the SEMPRE semantic parsing system used to perform QA corpus, which is linked to the knowledge base through the use of named entity prediction. By default, SEMPRE uses ClueWeb096 (Callan et al., 2009) as the large text corpus and Freebase as the knowledge base. During the QA process itself, this lexicon is used to convert entities into logical forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, a"
W15-3057,W14-3336,0,0.0532295,"Missing"
W15-3057,P13-1042,0,0.0393717,"a Graduate School of Information Science Nara Institute of Science and Technology Takayamacho 8916-5, Ikoma, Nara {sugiyama.kyoshiro.sc7, neubig}@is.naist.jp Abstract Sasaki et al., 2007). Machine translation (MT) is one of the most widely used tools to achieve CLQA (Mori and Kawagishi, 2005; Fujii et al., 2009; Kettunen, 2009).2 In the realm of monolingual question answering, recent years have seen a large increase in the use of structured knowledge bases such as Freebase (Bollacker et al., 2008), as they allow for accurate answering of questions over a variety of topics (Frank et al., 2007; Cai and Yates, 2013). However, knowledge bases are limited to only a few major languages. Thus, CLQA is particularly important for QA using knowledge bases. In contrast to the CLQA situation, where an MT system is performing translation for a downstream system to consume, in standard translation tasks the consumer of results is a human (Matsuzaki et al., 2015). In this case, it is important to define an evaluation measure which has high correlation with human evaluation, and the field of MT metrics has widely studied which features of MT results are correlated with human evaluation, and how to reflect these featu"
W15-3057,P15-2024,0,0.0128817,"realm of monolingual question answering, recent years have seen a large increase in the use of structured knowledge bases such as Freebase (Bollacker et al., 2008), as they allow for accurate answering of questions over a variety of topics (Frank et al., 2007; Cai and Yates, 2013). However, knowledge bases are limited to only a few major languages. Thus, CLQA is particularly important for QA using knowledge bases. In contrast to the CLQA situation, where an MT system is performing translation for a downstream system to consume, in standard translation tasks the consumer of results is a human (Matsuzaki et al., 2015). In this case, it is important to define an evaluation measure which has high correlation with human evaluation, and the field of MT metrics has widely studied which features of MT results are correlated with human evaluation, and how to reflect these features in automatic evaluation (Mach´acek and Bojar, 2014). However, translations which are good for humans may not be suitable for question answering. For example, according to the work of Hyodo and Akiba (2009), a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full"
W15-3057,P13-4016,1,0.836387,"tion results that affect CLQA. 2 GT and YT The questions are translated using Google Translate3 (GT) and Yahoo Translate4 (YT) systems, these commercial systems can be used via web pages. While the details of these systems are not open to the public, it is likely that Google takes a largely statistical MT approach, while the Yahoo engine is rule-based. Moses The questions are translated using a phrase-based system built using Moses (Koehn et al., 2007) (the Mo set). A total of 277 million sentences from various genres are used in training. Travatar The questions are translated using Travatar (Neubig, 2013) (the Tra set), a tool for forest-to-string MT that has achieved competitive results on the Japanese-English language pair. The training data is the same as Moses. Data sets To create data that allows us to investigate the influence of translation on QA, we started with a standard QA data set, and created automatic and manual translations. In this section, we describe the data construction in detail. As our seed data, we used a data set called Free917 (Cai and Yates, 2013). Free917 is a question set made for QA using the large-scale knowledge base “Freebase,” and is widely used in QA research"
W15-3057,P02-1040,0,0.105224,"l forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, and the score is from 0 to 1, where 0 is the worst and 1 is the best. Bridging To create the query for the knowledge base, SEMPRE merges neighboring logical forms in a binary tree structure. Bridging is an operation that generates predicates compatible with neighboring predicates. WER Word error rate (WER) is the edit distance between the translation and reference normalized by the sentence length. The formula"
W15-3057,N15-1149,0,0.0167969,"t these features in automatic evaluation (Mach´acek and Bojar, 2014). However, translations which are good for humans may not be suitable for question answering. For example, according to the work of Hyodo and Akiba (2009), a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full sentences on CLQA using documents or web pages, although it is not clear whether these results will apply to more structured QA using knowledge bases. There is also work on optimizing translation to improve CLQA accuracy (Riezler et al., 2014; Haas and Riezler, 2015), but these methods require a large set of translated questionanswer pairs, which may not be available in many Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accur"
W15-3057,P14-1083,0,0.0412693,"ion, and how to reflect these features in automatic evaluation (Mach´acek and Bojar, 2014). However, translations which are good for humans may not be suitable for question answering. For example, according to the work of Hyodo and Akiba (2009), a translation model trained using a parallel corpus without function words achieved higher accuracy than a model trained using full sentences on CLQA using documents or web pages, although it is not clear whether these results will apply to more structured QA using knowledge bases. There is also work on optimizing translation to improve CLQA accuracy (Riezler et al., 2014; Haas and Riezler, 2015), but these methods require a large set of translated questionanswer pairs, which may not be available in many Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT"
W15-3057,D10-1092,0,0.175745,"c parsing system used to perform QA corpus, which is linked to the knowledge base through the use of named entity prediction. By default, SEMPRE uses ClueWeb096 (Callan et al., 2009) as the large text corpus and Freebase as the knowledge base. During the QA process itself, this lexicon is used to convert entities into logical forms through a process called alignment. 4.1 Result 1: Evaluation of translation quality First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (Lin and Och, 2004), WER (Leusch et al., 2003), NIST (Doddington, 2002) and RIBES (Isozaki et al., 2010) and manual evaluation of acceptability (Goto et al., 2013). Translation has the potential to affect this part by changing the words in the translation. Because the strings in the sentence are used to look up which logical form to use, a mistranslated word may result in a failure in lookup. BLEU+1 BLEU (Papineni et al., 2002) is the most popular automatic evaluation metric of machine translation quality, and BLEU+1 is a smoothed version that can be used with single sentences. It is based on n-gram precision, and the score is from 0 to 1, where 0 is the worst and 1 is the best. Bridging To crea"
W15-4605,W14-4308,0,0.0697525,"alists, collectivists, and altruists). The domain was negotiation between a florist and a grocer who had to agree on the temperature of a shared retail space. Georgila (2013) used RL to learn the dialog system policy in a two-issue negotiation domain where two participants (the user and the system) organize a party, and need to decide on both the day that the party will take place and the type of food that will be served. Also, Heeman (2009) modeled negotiation dialog for a furniture layout task, and Paruchuri et al. (2009) modeled negotiation dialog between a seller and buyer. More recently, Efstathiou and Lemon (2014) focused on non-cooperative aspects of trading dialog, and Georgila et al. (2014) used multi-agent RL to learn negotiation policies in a resource allocation scenario. Finally, Hiraoka et al. (2014) applied RL to the problem of learning cooperative persuasive policies using framing, and Nouri et al. (2012) learned models for cultural decision-making in a simple negotiation game (the Ultimatum Game). In contrast to typical 2 Reinforcement Learning Reinforcement learning (RL) is a machine learning technique for learning the policy of an agent 1 Note that there is some previous work on using RL to"
W15-4605,W10-4321,1,0.654188,"Missing"
W15-4605,P14-1047,1,0.927285,"Institute for Creative Technologies nouri@ict.usc.edu traum@ict.usc.edu Satoshi Nakamura Nara Institute of Science and Technology s-nakamura@is.naist.jp Abstract party. Trading dialogs can be considered as a kind of negotiation, in which participants use various tactics to try to reach an agreement. It is common to have dialogs that may involve multiple offers or even multiple trades. In this way, trading dialogs are different from other sorts of negotiation in which a single decision (possibly about multiple issues) is considered, for example partitioning a set of items (Nouri et al., 2013; Georgila et al., 2014). Another difference between trading dialogs and partitioning dialogs is what happens when a deal is not made. In partitioning dialogs, if an agreement is not reached, then participants get nothing, so there is a very strong incentive to reach a deal, which allows pressure and can result in a “chicken game”, where people give up value in order to avoid a total loss. By contrast, in trading dialogs, if no deal is made, participants stick with the status quo. Competitive two-party trading dialogs may result in a kind of stasis, where the wealthier party will pass up mutually beneficial deals, in"
W15-4605,W13-4016,1,0.842785,"mine a number of strategies for this game, including random, simple, and complex 32 Proceedings of the SIGDIAL 2015 Conference, pages 32–41, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics hand-crafted strategies, as well as several reinforcement learning (RL) (Sutton and Barto, 1998) algorithms, and examine performance with different numbers and kinds of opponents. slot-filling dialog systems, in these negotiation dialogs, the dialog system is rewarded based on the achievement of its own goals rather than those of its interlocutor. For example, in Georgila (2013), the dialog system gets a higher reward when its party plan is accepted by the other participant. Note that in all of the previous work mentioned above, the focus was on negotiation dialog between two participants only, ignoring cases where negotiation takes place between more than two interlocutors. However, in the real world, multiparty negotiation is quite common. In this paper, as a first study on multi-party negotiation, we apply RL to a multi-party trading scenario where the dialog system (learner) trades with one, two, or three other agents. We experiment with different RL algorithms a"
W15-4605,C14-1161,1,0.625159,"system policy in a two-issue negotiation domain where two participants (the user and the system) organize a party, and need to decide on both the day that the party will take place and the type of food that will be served. Also, Heeman (2009) modeled negotiation dialog for a furniture layout task, and Paruchuri et al. (2009) modeled negotiation dialog between a seller and buyer. More recently, Efstathiou and Lemon (2014) focused on non-cooperative aspects of trading dialog, and Georgila et al. (2014) used multi-agent RL to learn negotiation policies in a resource allocation scenario. Finally, Hiraoka et al. (2014) applied RL to the problem of learning cooperative persuasive policies using framing, and Nouri et al. (2012) learned models for cultural decision-making in a simple negotiation game (the Ultimatum Game). In contrast to typical 2 Reinforcement Learning Reinforcement learning (RL) is a machine learning technique for learning the policy of an agent 1 Note that there is some previous work on using RL to learn negotiation policies among more than two participants. For example, Mayya et al. (2011) and Zou et al. (2014) used multi-agent RL to learn the negotiation policies of sellers and buyers in a"
W15-4605,W11-2001,0,\N,Missing
W15-5003,P06-1077,0,0.0142234,"that achieved the highest accuracy on all four tracks in the last year’s eval1 Particularly when comparing similar systems, such as the case of when neural MT is used for reranking existing system results. 35 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35‒41, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT."
W15-5003,P15-1002,0,0.110434,"Missing"
W15-5003,W15-3034,0,0.0197064,"s conclusive. ing points. First, we can see that the improvement in scores is very slightly sub-linear in the log number of hypotheses in the n-best list. In other words, every time we double the n-best list size we will see an improvement in accuracy that is slightly smaller than the last time we doubled the size. Second, we can note that in most cases this trend continues all the way up to our limit of 1000best lists, indicating that gains are not saturating, and we can likely expect even more improvements from using larger lists, or perhaps directly performing decoding using neural models (Alkhouli et al., 2015). The en-ja results, however, are an exception to this rule, with BLEU gains more or less saturating around the 50-best list point. 5 Effect of n-best Size on Reranking In the previous sections, we confirmed the effectiveness of n-best list reranking using neural MT models. However, reranking using n-best lists (like other search methods for MT) is an approximate search method, and its effectiveness is limited by the size of the n-best list used. In order to quantify the effect of this inexact search, we performed experiments to examine the post-reranking automatic evaluation scores of the MT"
W15-5003,D15-1166,0,0.106833,"the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. 1 Introduction Neural network models for machine translation (MT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), while still in a nascent stage, have shown impressive results in a number of translation tasks. Specifically, a number of works have demonstrated gains in BLEU score (Papineni et al., 2002) over state-of-the-art non-neural systems, both when using the neural MT model standalone (Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there has been little examination of the effect that these gains have on the subjective impressions of human users. Because BLEU generally has 2 Generation of Translation Results 2.1 Baseline System All experiments are performed on WAT2015 translation task from Japanese (ja) to/from English (en) and Chinese (zh). As a baseline, we used the NAIST syste"
W15-5003,N06-1020,0,0.0102649,"first encodes the source sentence f using bidirectional 3 Experimental Results 2 First, we calculate overall numerical results for our systems with and without the neural MT reranking model. As automatic evaluation we use the standard BLEU (Papineni et al., 2002) and reorderingoriented RIBES (Isozaki et al., 2010) metrics. In Scripts to reproduce the system are available at http: //phontron.com/project/wat2014. 3 https://github.com/neubig/egret 4 In addition, for ja-en translation, we make one modification to the parser used in the previous year’s submission, performing parser self-training (McClosky et al., 2006) using sentences from the training data that had a BLEU score greater than 0.8, and selecting the tree corresponding to the 500-best hypothesis that had the best score according to BLEU+1 (Lin and Och, 2004). 5 http://github.com/neubig/lamtram More standard log-linear interpolation resulted in similar, or slightly inferior results. 6 36 System Base Rerank B 36.6 38.2 en-ja R H 79.6 49.8 81.4 62.3 B 22.6 25.4 ja-en R H 72.3 11.8 75.0 35.5 B 40.5 43.0 zh-ja R H 83.4 25.8 84.8 35.8 B 30.1 31.6 ja-zh R 81.5 83.3 H 2.8 7.0 Table 1: Overall BLEU, RIBES, and HUMAN scores for our baseline system and s"
W15-5003,P96-1041,0,0.1011,"n-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector hj for each word fj in f . The model then proceeds to generate the target translation eˆ one word at a time, at each time step calculating soft alignments ai that are used to generate a context vector gi , which is referenced when generating the target word gi = |f | ∑ ai,j hj . (1) j=1 Attentional"
W15-5003,P08-1023,0,0.0137546,"t here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tse"
W15-5003,N04-1014,0,0.0201838,"state-of-the-art system that achieved the highest accuracy on all four tracks in the last year’s eval1 Particularly when comparing similar systems, such as the case of when neural MT is used for reranking existing system results. 35 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35‒41, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpu"
W15-5003,P13-2121,0,0.0527266,"Missing"
W15-5003,P14-2024,1,0.579915,"held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stan"
W15-5003,D10-1092,0,0.0799846,"scarce. In this paper, we attempt to close this gap by examining the gains provided by using neural MT models to rerank the hypotheses a state-of-the-art non-neural MT system, both from the objective and subjective perspectives. Specifically, as part of the Nara Institute of Science and Technology (NAIST) submission to the Workshop on Asian Translation (WAT) 2015 (Nakazawa et al., 2015), we generate reranked and non-reranked translation results in four language pairs (Section 2). Based on these translation results, we calculate scores according to automatic evaluation measures BLEU and RIBES (Isozaki et al., 2010), and a manual evaluation that involves comparing hypotheses to a baseline system (Section 3). Next, we perform a detailed analysis of the cases in which subjective impressions improved or degraded due to neural MT reranking, and identify major areas in which neural reranking improves results, and areas in which reranking is less helpful (Section 4). Finally, as an auxiliary result, we also examine the effect that the size of the n-best list used in reranking has on the improvement of translation results (Section 5). Abstract This year, the Nara Institute of Science and Technology (NAIST)’s su"
W15-5003,P11-2093,1,0.825294,"d encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector"
W15-5003,P15-1001,0,0.0419024,"ns of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. 1 Introduction Neural network models for machine translation (MT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), while still in a nascent stage, have shown impressive results in a number of translation tasks. Specifically, a number of works have demonstrated gains in BLEU score (Papineni et al., 2002) over state-of-the-art non-neural systems, both when using the neural MT model standalone (Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there has been little examination of the effect that these gains have on the subjective impressions of human users. Because BLEU generally has 2 Generation of Translation Results 2.1 Baseline System All experiments are performed on WAT2015 translation task from Japanese (ja) to/from English (en) and Chinese (zh). As a baseline, we used the NAIST system for WAT 2014 (Neub"
W15-5003,P13-4016,1,0.860103,"used the NAIST system for WAT 2014 (Neubig, 2014), a state-of-the-art system that achieved the highest accuracy on all four tracks in the last year’s eval1 Particularly when comparing similar systems, such as the case of when neural MT is used for reranking existing system results. 35 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35‒41, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa a"
W15-5003,D13-1176,0,0.0518765,"l machine translation models. Experiments re-confirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. 1 Introduction Neural network models for machine translation (MT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), while still in a nascent stage, have shown impressive results in a number of translation tasks. Specifically, a number of works have demonstrated gains in BLEU score (Papineni et al., 2002) over state-of-the-art non-neural systems, both when using the neural MT model standalone (Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there"
W15-5003,W14-7002,1,0.820443,"2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there has been little examination of the effect that these gains have on the subjective impressions of human users. Because BLEU generally has 2 Generation of Translation Results 2.1 Baseline System All experiments are performed on WAT2015 translation task from Japanese (ja) to/from English (en) and Chinese (zh). As a baseline, we used the NAIST system for WAT 2014 (Neubig, 2014), a state-of-the-art system that achieved the highest accuracy on all four tracks in the last year’s eval1 Particularly when comparing similar systems, such as the case of when neural MT is used for reranking existing system results. 35 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35‒41, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). uation.2 The details of construction are described in Neubig (2014), but we briefly outline it here for completeness. The system is based on the Travatar toolkit (Neubig, 2013), using tree-to-string statistical M"
W15-5003,P03-1054,0,0.0231386,"the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent netw"
W15-5003,P03-1021,0,0.0398538,"ls. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector hj for each word fj in f . The model then proceeds to generate the target translation eˆ one word at a time, at each time step calculating soft alignments ai that are used to generate a context vector gi , which is referenced when generating the target word gi = |f | ∑ ai,j hj . (1) j=1 Attentional models have a number of appealing properties, such as being theoretically able to encode variable length sequences without worrying about memory constr"
W15-5003,W04-3250,0,0.0309746,"Missing"
W15-5003,P02-1040,0,0.103382,"arry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. 1 Introduction Neural network models for machine translation (MT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), while still in a nascent stage, have shown impressive results in a number of translation tasks. Specifically, a number of works have demonstrated gains in BLEU score (Papineni et al., 2002) over state-of-the-art non-neural systems, both when using the neural MT model standalone (Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b), or to rerank the output of more traditional systems phrase-based MT systems (Sutskever et al., 2014). However, despite these impressive results with regards to automatic measures of translation quality, there has been little examination of the effect that these gains have on the subjective impressions of human users. Because BLEU generally has 2 Generation of Translation Results 2.1 Baseline System All experiments are performed on WAT2015 tran"
W15-5003,D12-1096,0,0.016682,"tained by Y,” as shown in the example. The baseline system does not include any explicit features to make this distinction between whether a verb is part of a relative clause or not, and thus made a number of mistakes of this variety. However, it is evident that the neural MT model has learned to make this distinction, greatly reducing the number of these errors. The third subcategory is similar to the first, but explicitly involves the correct interpretation of coordinate structures. It is well known that syntactic parsers often make mistakes in their interpretation of coordinate structures (Kummerfeld et al., 2012). Of course, the parser used in our syntaxbased MT system is no exception to this rule, and parse errors often cause coordinate phrases to be broken apart on the target side, as is the case in the example’s “local heating and ablation.” The fact that the neural MT models were able to correct a large number of errors related to these structures suggests that they are able to successfully determine whether two phrases are coordinated or not, and keep them together on the target side. The final sub-category of the top four is related to verb conjugation agreement. Many of the examples related to"
W15-5003,P06-1055,0,0.0147746,"orpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector hj for each word fj in f . The model then proceeds to generate the target translation eˆ one word at a time, at each time step calculating soft alignments ai that are used to gen"
W15-5003,P10-1017,0,0.0237546,", 2013), using tree-to-string statistical MT (Graehl and Knight, 2004; Liu et al., 2006), in which the source is first syntactically parsed, then subtrees of the input parse are converted into strings on the target side. This translation paradigm has proven effective for translation between syntactically distant language pairs such as those handled by the WAT tasks. In addition, following our findings in Neubig and Duh (2014), to improve the accuracy of translation we use forestbased encoding of many parse candidates (Mi et al., 2008), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implem"
W15-5003,I05-3027,0,0.017369,"08), and a supervised alignment technique for ja-en and en-ja (Riesa and Marcu, 2010). To train the systems, we used the ASPEC corpus provided by WAT. For the zh-ja and ja-zh systems, we used all of the data, amounting to 672k sentences. For the en-ja and ja-en systems, we used all 3M sentences for training the language models, and the first 2M sentences of the training data for training the translation models. For English, Japanese, and Chinese, tokenization was performed using the Stanford Parser (Klein and Manning, 2003), the KyTea toolkit (Neubig et al., 2011), and the Stanford Segmenter (Tseng et al., 2005) respectively. For parsing, we use the Egret parser,3 which implements the latent variable parsing model of (Petrov et al., 2006).4 For all systems, we trained a 6-gram language model smoothed with modified KneserNey smoothing (Chen and Goodman, 1996) using KenLM (Heafield et al., 2013). To optimize the parameters of the log-linear model, we use standard minimum error rate training (MERT; Och (2003)) with BLEU as an objective. long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) recurrent networks. This results in an encoding vector hj for each word fj in f . The model then proceed"
W15-5003,vilar-etal-2006-error,0,0.0653908,"Missing"
W15-5003,C04-1072,0,\N,Missing
W16-3640,J12-1001,0,0.058228,"Missing"
W16-3640,N13-2012,0,0.0206591,"prevalent for utterances of particular dialogue acts? Does the level of entrainment increase as dialogue progresses? 310 Proceedings of the SIGDIAL 2016 Conference, pages 310–318, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics 2 Related Works In detail, we can express this formula with word count CS1 (w) and CS2 (w), and all of words W as, 2.1 Varieties of entrainment En(V ) = ∑ CS2 (w) CS1 (w) − −∑ ∑ . wi ∈W CS1 (wi ) C (w ) i wi ∈W S2 As mentioned in the introduction, entrainment has been shown to occur at almost every level of human communication (Levitan, 2013), including both human-human and human-system conversation. In human-human conversation, Kawahara et al. (2015) showed the synchrony of backchannels to the preceding utterances in attentive listening, and they investigated the relationship between morphological patterns of backchannels and the syntactic complexities of preceding utterances. Levitan et al. (2015) showed the entrainment of latency in turn taking. In human-system conversation, Campbell and Scherer (2010) tried to predict user’s turn taking behavior by considering entrainment. Fandrianto and Eskenazi (2012) modeled a dialogue stra"
W16-3640,P08-2043,0,0.82587,"ical patterns of backchannels and the syntactic complexities of preceding utterances. Levitan et al. (2015) showed the entrainment of latency in turn taking. In human-system conversation, Campbell and Scherer (2010) tried to predict user’s turn taking behavior by considering entrainment. Fandrianto and Eskenazi (2012) modeled a dialogue strategy to increase the accuracy of speech recognition by using entrainment intentionally. Levitan (2013) unified these two works. One of the most important questions about entrainment with respect to dialogue systems is its association with dialogue quality. Nenkova et al. (2008) proposed a score to evaluate the lexical entrainment in highly frequent words, and found that the score has high correlation with task success and engagement. This indicates that lexical entrainment has an important role in dialogue. In addition, it suggests that entrainment of lexical choice is probably affected by more detailed dialogue information, such as dialogue act. w∈V (2) Nenkova et al. (2008) used following word classes as V . 25MFC: 25 Most frequent words in the corpus. The idea of using only frequent words is based on the fact that we would like to avoid the score being affected b"
W16-3640,P07-1102,0,0.0411561,"t, structural level. In this paper, we investigate the effect of entrainment on dialogue acts and on lexical choice given dialogue acts, as well as how entrainment changes during a dialogue. We also define a novel measure of entrainment to measure these various types of entrainment. These results may serve as guidelines for dialogue systems that would like to entrain with users in a similar manner. 1 Introduction Entrainment is a conversational phenomenon in which dialogue participants synchronize to each other with regards to various factors: lexical choice (Brennan and Clark, 1996), syntax (Reitter and Moore, 2007; Ward and Litman, 2007), style (Niederhoffer and Pennebaker, 2002; DanescuNiculescu-Mizil et al., 2011), acoustic prosody (Natale, 1975; Coulston et al., 2002; Ward and Litman, 2007; Kawahara et al., 2015), pronunciation (Pardo, 2006) and turn taking (Campbell and Scherer, 2010; Beˇnuˇs et al., 2014). Previous works have reported that entrainment is correlated with dialogue success, naturalness and engagement. However, there is much that is still unclear with regards to how entrainment affects the overall flow of the dialogue. For example, can entrainment also be observed in choice of dialog"
W17-3208,P11-2093,1,0.641669,"Missing"
W17-3208,P02-1040,0,0.100862,"Missing"
W17-3208,D17-1151,0,0.053157,"conjectured that this is an effect of gathering the similar sentences in a mini-batch as we mentioned in Section 4.2.3. These results indicate that in the case of SGD it is acceptable to TRG SRC , which is the fastest method to process the whole corpus (see Table 3), for SGD. Recently, Wu et al. (2016) proposed a new learning paradigm, which uses Adam for the initial training, then switches to SGD after several iterations. If we use this learning algorithm, we may be able to train the model more effectively by using SHUFFLE or SRC sorting method for Adam, and TRG SRC for SGD. 4.3 5 Recently, Britz et al. (2017) have released a paper about exploring the hyper-parameters of NMT. This work is similar to our paper in the terms of finding the better hyper-parameters by doing a large number of experiments and deriving empirical conclusions. However, notably this paper fixed the mini-batch size to 128 sentences and did not treat mini-batch creation strategy as one of the hyper-parameters of the model. With our experimental results, we argue that the mini-batch creation strategies also have an impact on the NMT training, and thus having solid recommendations for how to adjust this hyper-parameter are also o"
W17-3208,C16-2064,0,0.0371884,"Missing"
W17-3208,P17-4012,0,0.0977649,"Missing"
W17-3208,W16-2301,0,\N,Missing
W17-4709,J93-2003,0,0.0967937,"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 90 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 90–98 c Copenhagen, Denmark, Septem"
W17-4709,J07-2003,0,0.827158,"(de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include E"
W17-4709,P07-1092,0,0.754293,"[X1] 记录 [X1] dossier [X2] (a) Standard triangulation method matching phrases VP VP [X1] enregistrer [X2] TO [X1] 记录 [X2] VB NP record [X2] [X1] NP NP [X1] dossier [X2] DT [X1] [X2] [X1] 记录 NN NP record [X2] (b) Proposed triangulation method matching subtrees Figure 1: Example of disambiguation by parse subtree matching (Fr-En-Zh), [X1] and [X2] are non-terminals for sub-phrases. lel data with the source and target languages exists (de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and targ"
W17-4709,W08-0333,0,0.0164523,"n confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 90 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 90–98 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics trained on direct parallel corpora. (Aho and Ullman, 1969; Chian"
W17-4709,N04-1014,0,0.0774748,"ssible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998). When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007). 2.2 rules, it has the tendency to extract very large translation rule tables and also tends to be less syntactically faithful in its derivations. 2.3 Explicitly Syntactic Rules An alternative to Hiero rules is the use of synchronous context-free grammar or synchronous tree-substitution grammar (Graehl and Knight, 2004) rules that explicitly take into account the syntax of the source side (tree-to-string rules), target side (string-to-tree rules), or both (tree-to-tree rules). Taking the example of tree-to-string (T2S) rules, these use parse trees on the source language side, and the head symbols of the synchronous rules are not limited to S or X, but instead use non-terminal symbols corresponding to the phrase structure tags of a given parse tree. For example, T2S rules could take the form of: Hierarchical Rules In this section, we specifically cover the rules used in Hiero. Hierarchical rules are composed"
W17-4709,P15-2094,1,0.752216,"d Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English. One effective"
W17-4709,W11-2123,0,0.0138784,"trees, we used the Ckylark PCFG-LA parser (Oda et al., 2015), and filtered out lines of length over 60 tokens from all the parallel data to ensure accuracy of parsing and alignment. About 7.6M lines remained. Since Hiero requires a large amount of computational resources for training and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT"
W17-4709,P13-4016,1,0.850627,"ext with the trained model. We used English raw text without tokenization for phrase structure analysis and for training Hiero and T2S TMs on the pivot side. To generate parse trees, we used the Ckylark PCFG-LA parser (Oda et al., 2015), and filtered out lines of length over 60 tokens from all the parallel data to ensure accuracy of parsing and alignment. About 7.6M lines remained. Since Hiero requires a large amount of computational resources for training and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using"
W17-4709,Q17-1024,0,0.0416509,"Missing"
W17-4709,P11-2093,1,0.727838,"s a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT ) (22) ˆ SP p, pˆT ) p∈T p∈T ˆ SP w(ˆ  0 (f lat(pˆS ) = f lat(pˆT )) w(pˆS , pˆT ) = exp (−d (pˆS , pˆT )) (otherwise) d(pˆS , pˆT ) = T reeEditDistance(pˆS , pˆT ) (23) (24) where f lat(ˆ p) returns the symbol string of pˆ keeping non-terminals, and T reeEditDistance(pˆS , pˆT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) n"
W17-4709,N15-3009,1,0.821989,"zer, that is although designed mainly for neural MT, we confirmed that it also helps to reduce training time and even improves translation accuracy in our Hiero model as well. We first trained a single shared tokenization model by feeding a total of 10M sentences from the data of all the 6 languages, set the maximum shared vocabulary size to be 16k, and tokenized all available text with the trained model. We used English raw text without tokenization for phrase structure analysis and for training Hiero and T2S TMs on the pivot side. To generate parse trees, we used the Ckylark PCFG-LA parser (Oda et al., 2015), and filtered out lines of length over 60 tokens from all the parallel data to ensure accuracy of parsing and alignment. About 7.6M lines remained. Since Hiero requires a large amount of computational resources for training and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used t"
W17-4709,P02-1040,0,0.0978157,"T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT ) (22) ˆ SP p, pˆT ) p∈T p∈T ˆ SP w(ˆ  0 (f lat(pˆS ) = f lat(pˆT )) w(pˆS , pˆT ) = exp (−d (pˆS , pˆT )) (otherwise) d(pˆS , pˆT ) = T reeEditDistance(pˆS , pˆT ) (23) (24) where f lat(ˆ p) returns the symbol string of pˆ keeping non-terminals, and T reeEditDistance(pˆS , pˆT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pˆS into pˆT (Klein, 1998). According to equatio"
W17-4709,P07-2045,0,0.008798,"ng and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT ) (22) ˆ SP p, pˆT ) p∈T p∈T ˆ SP w(ˆ  0 (f lat(pˆS ) = f lat(pˆT )) w(pˆS , pˆT ) = exp (−d (pˆS , pˆT )) (otherwise) d(pˆS , pˆT ) = T reeEditDistance(pˆS , pˆT ) (23) (24) where f lat(ˆ p) returns the symbol string of pˆ keeping non-terminals, and T reeEditDistance(pˆS , pˆT )"
W17-4709,N07-1061,0,0.283363,"o 2.3 BLEU points.1 1 [X2] [X1] 记录 [X1] dossier [X2] (a) Standard triangulation method matching phrases VP VP [X1] enregistrer [X2] TO [X1] 记录 [X2] VB NP record [X2] [X1] NP NP [X1] dossier [X2] DT [X1] [X2] [X1] 记录 NN NP record [X2] (b) Proposed triangulation method matching subtrees Figure 1: Example of disambiguation by parse subtree matching (Fr-En-Zh), [X1] and [X2] are non-terminals for sub-phrases. lel data with the source and target languages exists (de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences"
W17-4709,N03-1017,0,0.272216,"[X2] are non-terminals for sub-phrases. lel data with the source and target languages exists (de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are"
W17-4709,L16-1561,0,0.20481,"used in tree-based machine translation frameworks (§2). After describing the baseline triangulation method (§3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching (§4). The first places a hard restriction on exact matching of parse trees (§4.1) included in translation rules, while the second places a softer restriction allowing partial matches (§4.2). To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language (§5). In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario. In the triangulation method, source-pivot and pivot-target phrase pairs are connected as a sourcetarget phrase pair when a common pivot-side phrase e"
W17-4753,W16-2358,0,0.056134,"Missing"
W17-4753,W16-2359,0,0.0171527,"Zhang1,2 , Masao Utiyama1 , Eiichro Sumita1 Graham Neubig2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp neubig/s-nakamura@is.naist.jp Abstract We also explored ways to improve the NMT model with image information. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use"
W17-4753,E17-2101,0,0.0123746,"a1 Graham Neubig2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp neubig/s-nakamura@is.naist.jp Abstract We also explored ways to improve the NMT model with image information. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use these target descriptions as additional in"
W17-4753,P05-1033,0,0.0896966,"ing alone. We also present a multimodal NMT model that integrates the target language descriptions of images that are similar to the image described by the source sentence as additional inputs of the neural networks to help the translation of the source sentence. We give detailed analysis for the results of the multimodal NMT model. Our system obtained the first place for the English-to-French task according to human evaluation. 1 2 Text-only MT We compared three text-only approaches for this translation task. 2.1 Introduction Hierarchical Phrase-based SMT The hierarchical phrase-based model (Chiang, 2005) extracts hierarchical phrase-based translation rules from parallel sentence pairs with word alignments. The word alignments can be learned by IBM models. Each translation rule contains several feature scores. The decoder of hierarchical phrase-based model implements a bottom-up CKY+ algorithm. The weights for different features can be tuned on the development set. We participated in the WMT 2017 shared multimodal machine translation task 1, which translates a source language description of an image into a target language description. We built systems for both English-to-German and English-toF"
W17-4753,W16-3210,0,0.06354,"Missing"
W17-4753,P16-1227,0,0.0232453,"ion. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use these target descriptions as additional inputs for the NMT model. This paper describes the NICT-NAIST system for the WMT 2017 shared multimodal machine translation task for both language pairs, English-to-German and English-to-French. We built a hierarchical phrase-based (Hiero) translation system and trained an attentional encoder-decoder neural machine translation (NMT) model to rerank the n-best output of the Hiero system, which obtained significant gains over both the Hiero"
W17-4753,W16-2360,0,0.0386153,"ama1 , Eiichro Sumita1 Graham Neubig2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp neubig/s-nakamura@is.naist.jp Abstract We also explored ways to improve the NMT model with image information. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use these target descri"
W17-4753,P07-2045,0,0.0214797,"between image vectors. Method Hiero NMT Reranking Flickr en-de 27.86 30.52 31.98 en-fr 50.38 50.46 55.25 COCO en-de 24.57 24.27 28.05 en-fr 41.88 41.26 45.17 λ=0 λ = 0.2 en-de 52.17 52.93 en-fr 65.60 66.31 Table 3: 1-gram BLEU score of selected target words on the development set. Table 2: Results of text-only approaches (BLEU). coding, we find the most probable source word for each UNK and replace the UNK using a lexicon extracted from the word-aligned training set. et al., 2017). We lowercase, normalise punctuation and tokenise all sentences. The Hiero translation system was based on Moses (Koehn et al., 2007). We used GIZA++ (Och and Ney, 2003) and growdiag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments. For decoding, we used standard features: direct/inverse phrase translation probability, direct/inverse lexical translation probability and a 5-gram language model, which was trained on the target side of the training corpus by IRSTLM Toolkit3 with improved Kneser-Ney smoothing. Attentional encoder-decoder networks were trained with Lamtram4 . Word embedding size and hidden layer size are both 512. Training data was reshuffled between epochs. Validation was done after"
W17-4753,N03-1017,0,0.0127221,"38 50.46 55.25 COCO en-de 24.57 24.27 28.05 en-fr 41.88 41.26 45.17 λ=0 λ = 0.2 en-de 52.17 52.93 en-fr 65.60 66.31 Table 3: 1-gram BLEU score of selected target words on the development set. Table 2: Results of text-only approaches (BLEU). coding, we find the most probable source word for each UNK and replace the UNK using a lexicon extracted from the word-aligned training set. et al., 2017). We lowercase, normalise punctuation and tokenise all sentences. The Hiero translation system was based on Moses (Koehn et al., 2007). We used GIZA++ (Och and Ney, 2003) and growdiag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments. For decoding, we used standard features: direct/inverse phrase translation probability, direct/inverse lexical translation probability and a 5-gram language model, which was trained on the target side of the training corpus by IRSTLM Toolkit3 with improved Kneser-Ney smoothing. Attentional encoder-decoder networks were trained with Lamtram4 . Word embedding size and hidden layer size are both 512. Training data was reshuffled between epochs. Validation was done after each epoch. We used the Adam optimization algorithm (Kingma and Ba, 2014). Because the tra"
W17-4753,P03-1021,0,0.0340914,"Validation was done after each epoch. We used the Adam optimization algorithm (Kingma and Ba, 2014). Because the training set is only 29K sentence pairs, we used dropout (0.5) and a small learning rate (0.0001) to reduce overfitting, which yielded improvements of 3 − 4 BLEU on the development set. For training the NMT model, we replace words that occur less than twice in the training set as UNK. When de3 4 We used the NMT model to rerank the unique 10, 000-best output of the Hiero system. The NMT score was used as an additional feature for the Hiero system. Feature weights were tuned by MERT (Och, 2003). Table 2 shows results of the Hiero system, the NMT system and using the NMT model to rerank the Hiero outputs. The reranking system had the best performance on both language pairs. It is straightforward that using the NMT feature to rerank the Hiero outputs can achieve improvements over the pure Hiero system. The reason why the reranking method outperformed the NMT system should be that the training corpus is relatively small and the NMT system did not outperform the Hiero system largely. Therefore, the reranking method that takes advantages of both the Hiero and NMT systems worked the best"
W17-4753,J03-1002,0,0.0124739,"MT Reranking Flickr en-de 27.86 30.52 31.98 en-fr 50.38 50.46 55.25 COCO en-de 24.57 24.27 28.05 en-fr 41.88 41.26 45.17 λ=0 λ = 0.2 en-de 52.17 52.93 en-fr 65.60 66.31 Table 3: 1-gram BLEU score of selected target words on the development set. Table 2: Results of text-only approaches (BLEU). coding, we find the most probable source word for each UNK and replace the UNK using a lexicon extracted from the word-aligned training set. et al., 2017). We lowercase, normalise punctuation and tokenise all sentences. The Hiero translation system was based on Moses (Koehn et al., 2007). We used GIZA++ (Och and Ney, 2003) and growdiag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments. For decoding, we used standard features: direct/inverse phrase translation probability, direct/inverse lexical translation probability and a 5-gram language model, which was trained on the target side of the training corpus by IRSTLM Toolkit3 with improved Kneser-Ney smoothing. Attentional encoder-decoder networks were trained with Lamtram4 . Word embedding size and hidden layer size are both 512. Training data was reshuffled between epochs. Validation was done after each epoch. We used the Adam optimiz"
W17-5542,W14-4305,1,0.619963,"ation Navigation System with Discovering User Interests Koichiro Yoshino, Yu Suzuki and Satoshi Nakamura Graduate School of Information Science Nara Institute of Science and Technology 8916-5, Takayama-cho, Ikoma, Nara, 6300192, Japan {koichiro,ysuzuki,s-nakamura}@is.naist.jp Abstract user’s question and presents additional information proactively according to the detected user’s interests, even if the user can not find the exact words to express his or her interests. The partially observable Markov decision process (POMDP)-based management architecture of information navigation was proposed (Yoshino and Kawahara, 2014) with several dialogue acts of providing information, however, this study ran the system only on one limited domain. In this demonstration, we used the belief-update function and the policy function trained on the di↵erent domain (news navigation for baseball news) for the proposed system (information navigation for tourist) to investigate the robustness of the defined system architecture. We also introduce a new mechanism to select more related topics when the system selects a topic to be presented in the next sub-dialogue by introducing semantic similarity of dialogue topics (content). The p"
W17-5542,bunt-etal-2012-iso,0,\N,Missing
W17-5712,P11-2027,0,0.0225845,"https://github.com/google/sentencepiece 137 System (this-year) Adjusted (last-year) Table 2: JPO adequacy results. Scores Ensemble 1 2 3 4 8 models 0.25 1.75 8.25 36.50 Single 0.25 1.75 17.50 37.75 3 models 2.00 2.75 19.25 43.50 language pair. In addition, we also tried to use SentencePiece, an unsupervised tokenizer to avoid complicated tokenization problems, and also confirmed that the resulting translation systems can perform with no accuracy reduction. scribed in Section 2.4. Table 1 shows the official evaluation scores of our systems, including BLEU, RIBES (Isozaki et al., 2010), AM-FM (Banchs and Li, 2011), and the human evaluation. The rows labeled last-year shows the best system in all previous WAT campaigns. We can see that our one-best system already achieves higher translation accuracy in all automatic evaluation metrics than last-year systems. In addition, adjusted system achieves further better scores than one-best, which means applying better decoding strategy can improve translation accuracy even using the same model. Table 1 also shows the place of our systems in this year. Because official results do not separate scores of single (no-ensemble) models and ensemble models, we also calc"
W17-5712,D10-1092,1,0.7921,"W P = 0.75. 2 Model Ensembling https://github.com/google/sentencepiece 137 System (this-year) Adjusted (last-year) Table 2: JPO adequacy results. Scores Ensemble 1 2 3 4 8 models 0.25 1.75 8.25 36.50 Single 0.25 1.75 17.50 37.75 3 models 2.00 2.75 19.25 43.50 language pair. In addition, we also tried to use SentencePiece, an unsupervised tokenizer to avoid complicated tokenization problems, and also confirmed that the resulting translation systems can perform with no accuracy reduction. scribed in Section 2.4. Table 1 shows the official evaluation scores of our systems, including BLEU, RIBES (Isozaki et al., 2010), AM-FM (Banchs and Li, 2011), and the human evaluation. The rows labeled last-year shows the best system in all previous WAT campaigns. We can see that our one-best system already achieves higher translation accuracy in all automatic evaluation metrics than last-year systems. In addition, adjusted system achieves further better scores than one-best, which means applying better decoding strategy can improve translation accuracy even using the same model. Table 1 also shows the place of our systems in this year. Because official results do not separate scores of single (no-ensemble) models and"
W17-5712,W16-4601,0,0.192201,"The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture. 1 Introduction Pr(e|f ) = 2.1 Pr(et |e<t , f ), (1) t=1 Neural machine translation (NMT) methods became one of the main-stream techniques in current machine translation studies. Previous WAT campaign showed that NMT methods can achieve higher translation accuracy in spite of simple model configurations (Nakazawa et al., 2016a). In this year, we chose the NMT architecture as our translation systems submitted for WAT2017 English-Japanese Scientific Paper Translation Task (Nakazawa et al., 2017). The main translation model is constructed by an encoder-decoder model (Sutskever et al., 2014) enforced by an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). This paper describes the details of our system, including whole model architecture, training criteria, decoding strategy, and data preparation. Results show that our system achieves higher translation accuracy than any systems submitted in previous WAT"
W17-5712,L16-1350,1,0.888778,"Missing"
W17-5712,P02-1040,0,0.104299,"ystem penalizes shorter sentences, and tends to generate longer sentences. Note that if the beam width is 1, there is no effect from word penalty, because the translation system can generate only 1-best results. Results We trained all translation systems varied by model/training/tokenization hyper-parameters described in previous sections, and performed a grid search to find an optimal set of hyper-parameters for this task. For the training data, we used top 2M sentences in ASPEC corpus (Nakazawa et al., 2016b) provided by the organizer. We chose the optimal model that achieves the best BLEU (Papineni et al., 2002) score over the dev corpus. For the optimal model, we also performed a grid search about decoding-time hyper-parameters. All the optimal hyper-parameters described in previous sections are found as the results of these searches. We submitted two results generated from the same optimal model: one-best results, i.e., the results with fixing BW = 1, and adjusted results, i.e., the results with optimal BW and W P deHyper-parameters In our decoding strategy, We have 2 hyper-parameters: beam width BW and word penalty factor W P . We varied BW from 1 to 128, and W P from 0 to 1.5, and finally chose B"
W18-2711,D15-1166,0,0.434437,"g multi-source NMT implementations without no special modifications. Experimental results with real incomplete multilingual corpora of TED Talks show that it is effective in allowing for multi-source NMT in situations where full multilingual corpora are not available, resulting in BLEU score gains of up to 2 points compared to standard bi-lingual NMT. (3) The method we base our work upon is largely similar to Zoph and Knight (2016), with the exception of a few details. Most notably, they used local-p attention, which focuses only on a small subset of the source positions for each target word (Luong et al., 2015). In this work, we used global attention, which attends to all words on the source side for each target word, as this is the standard method used in the great majority of recent NMT work. 93 Gating Network Es Encoder Decoder Fr Encoder Decoder Ar Encoder Decoder En Es Eso es verdad Fr C&apos;est vrai Ar &lt;NULL&gt; That is true En Figure 4: Multi-encoder NMT with a missing input sentence pre-train Figure 3: Mixture of NMT Experts 2.2 Specifically, we attempt to extend the methods in the previous section to use an incomplete multilingual corpus in this work. Mixture of NMT Experts Garmash and Monz (2016)"
W18-2711,2012.eamt-1.60,0,0.0155388,"vous remercie (a) A standard bilingual corpus English Hello Thank you French Bonjour Je vous remercie Spanish Hola Gracias (b) A complete multi-source corpus English Hello Thank you French Spanish Hola × × Bonjour Je vous remercie (c) An incomplete multi-source corpus with missing data Figure 1: Example of type of corpora. languages involved in the translation process. For example, we may have an original document in English, that we want to translate into several languages such as French, Spanish, and Portuguese. Some examples of these scenarios are the creation of video captions for talks (Cettolo et al., 2012) or Movies (Tiedemann, 2009), or translation of official documents into all the languages of a governing body, such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet hav"
W18-2711,2001.mtsummit-papers.46,0,0.675579,", such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-source translation takes in multiple inputs, and references all of them when deciding which sentence to output. Specifically, in the context of neural machine translation (NMT), there are several methods proposed to do so. For example, Zoph and Knight (2016) propose a method where multiple sentences are each encoded separately, then all referenced during the decoding process (the “multi-encoder” method). In addition, Garmash and Monz (2016) propose a method where NMT systems over multiple inputs are ensembled together to make a final pred"
W18-2711,P15-1166,0,0.0351747,"c a estudiar el modelo empresarial. Luego empec a mirar el modelo empresarial. Luego empec a ver el modelo de negocios. 0.266 0.266 0.726 Sometimes they agree. &lt;NULL&gt; &lt;NULL&gt; A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. 1.000 1.000 1.000 Table 6: Translation examples in {English, French, Brazilian Portuguese}-to-Spanish translation. 5 6 Related Work In this paper, we examined strategies for multisource NMT. On the other hand, there are there are other strategies for multilingual NMT that do not use multiple source sentences as their input. Dong et al. (2015) proposed a method for multitarget NMT. Their method is using one sharing encoder and decoders corresponding to the number of target languages. Firat et al. (2016) proposed a method for multi-source multi-target NMT using multiple encoders and decoders with a shared attention mechanism. Johonson et al. (2017) and Ha et al. (2016) proposed multi-source and multitarget NMT using one encoder and one decoder, and sharing all parameters with all languages. Notably, these methods use multilingual data to better train one-to-one NMT systems. However, our motivation of this study is to improve NMT fur"
W18-2711,P02-1040,0,0.100903,"of NMT experts, and one-to-one NMT. We used global attention and attention feeding (Luong et al., 2015) for the NMT models and used a bidirectional encoder (Bahdanau et al., 2015) in their encoders. The number of units was 512 for the hidden and embedding layers. Vocabulary size was the most frequent 30,000 words in the training data for each source and target languages. The parameter optimization algorithm was Adam (Kingma and Ba, 2015) and gradient clipping was set to 5. The number of hidden state units in the gating network for the mixture of NMT experts experiments was 256. We used BLEU (Papineni et al., 2002) as the evaluation metric. We performed early stopping, saving parameter values that had the smallest log perplexities on the validation data and used them when decoding test data. 4.2 Es x Data We used UN6WAY (Ziemski et al., 2016) as the complete multilingual corpus. We chose Spanish (Es), French (Fr), and Arabic (Ar) as source languages and English (En) as a target language The training data in the experiments were the one million sentences from the UN6WAY corpus whose sentence lengths were less than or equal to 40 words. We excluded 200,000 sentences for each language for the pseudo-incomp"
W18-2711,N16-1101,0,0.0320253,"ULL&gt; &lt;NULL&gt; A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. 1.000 1.000 1.000 Table 6: Translation examples in {English, French, Brazilian Portuguese}-to-Spanish translation. 5 6 Related Work In this paper, we examined strategies for multisource NMT. On the other hand, there are there are other strategies for multilingual NMT that do not use multiple source sentences as their input. Dong et al. (2015) proposed a method for multitarget NMT. Their method is using one sharing encoder and decoders corresponding to the number of target languages. Firat et al. (2016) proposed a method for multi-source multi-target NMT using multiple encoders and decoders with a shared attention mechanism. Johonson et al. (2017) and Ha et al. (2016) proposed multi-source and multitarget NMT using one encoder and one decoder, and sharing all parameters with all languages. Notably, these methods use multilingual data to better train one-to-one NMT systems. However, our motivation of this study is to improve NMT further by the help of other translations that are available on the source side at test time, and thus their approaches are different from ours. Conclusion In this pa"
W18-2711,L16-1561,0,0.142902,"ench Spanish Hola × × Bonjour Je vous remercie (c) An incomplete multi-source corpus with missing data Figure 1: Example of type of corpora. languages involved in the translation process. For example, we may have an original document in English, that we want to translate into several languages such as French, Spanish, and Portuguese. Some examples of these scenarios are the creation of video captions for talks (Cettolo et al., 2012) or Movies (Tiedemann, 2009), or translation of official documents into all the languages of a governing body, such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-sou"
W18-2711,C16-1133,0,0.437525,"2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-source translation takes in multiple inputs, and references all of them when deciding which sentence to output. Specifically, in the context of neural machine translation (NMT), there are several methods proposed to do so. For example, Zoph and Knight (2016) propose a method where multiple sentences are each encoded separately, then all referenced during the decoding process (the “multi-encoder” method). In addition, Garmash and Monz (2016) propose a method where NMT systems over multiple inputs are ensembled together to make a final prediction (the “mixture-of-NMT-experts” method). Es"
W18-2711,N16-1004,0,0.239969,"ean parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-source translation takes in multiple inputs, and references all of them when deciding which sentence to output. Specifically, in the context of neural machine translation (NMT), there are several methods proposed to do so. For example, Zoph and Knight (2016) propose a method where multiple sentences are each encoded separately, then all referenced during the decoding process (the “multi-encoder” method). In addition, Garmash and Monz (2016) propose a method where NMT systems over multiple inputs are ensembled together to make a final prediction (the “mixture-of"
W18-2711,W04-3250,0,0.0543353,"idation and test data for these experiments were also incomplete. This is in contrast to the experiments on UN6WAY where the test and validation data were complete, and thus this setting is arguable of more practical use. Table 4: The percentage of data without missing sentences on TED data. pus, even if just through the simple modification of replacing missing sentences with &lt;NULL&gt;. 4.3.3 Results Table 5 shows the results in BLEU and BLEU gains with respect to the one-to-one results. All the differences are statistically significant (p &lt; 0.01) by significance tests with bootstrap resampling (Koehn, 2004). The multi-source NMTs achieved consistent improvements over the oneto-one baseline as expected, but the BLEU gains were smaller than those in the previous experiments using the UN6WAY data. This is possibly With respect to the difference between the multi-encoder NMT and mixture of NMT experts, the multi-encoder achieved much higher BLEU in Pseudo-incomplete (0.8M) and Complete (1M), but this was not the case in Complete (0.2M). One possible reason here is the model complexity; the multi-encoder NMT uses a large single model while one-to-one sub-models in the mixture of NMT experts can be tr"
W18-2711,2005.mtsummit-papers.11,0,0.129127,"h Hello Thank you French Spanish Hola × × Bonjour Je vous remercie (c) An incomplete multi-source corpus with missing data Figure 1: Example of type of corpora. languages involved in the translation process. For example, we may have an original document in English, that we want to translate into several languages such as French, Spanish, and Portuguese. Some examples of these scenarios are the creation of video captions for talks (Cettolo et al., 2012) or Movies (Tiedemann, 2009), or translation of official documents into all the languages of a governing body, such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garma"
W18-5017,N16-1014,0,0.14454,"ories to emotionally color the response via the internal state of the decoder. However, this study has not yet considered user’s emotion in the response generation process, nor attempted improve emotional experience of user. Towards positive emotion elicitation, Lubis et al. (2018) have recently proposed a model that encodes emotion information from user input and utilizes it in generating response. However, the resulting system is still limited to short and generic responses with positive affect, echoing the long standing lack-of-diversity problem in neural network based response generation (Li et al., 2016). Furthermore, the reported system has not learn about positive emotion elicitation strategies from an expert as the corpus construction relied on crowd-sourcing workers. This points to another problem: the lack of data that shows positive emotion elicitation or emotion recovery in everyday situations. Learning from expert responses and actions are essential in such a scenario as these potentially differ from standard chat-based scenarios. With scarcity of large-scale data, additional knowledge from higher level abstraction, such as dialogue action labels, may be highly beneficial. However, su"
W18-5017,P12-3007,0,0.0347229,"the model, in form of response cluster labels (Section 4.1), to aid its re165 … ?2,?2 ?2,1 ?3,1 … ?3,?3 Pretraining SubTle Counseling data Counselor dialogue clustering Unsupervised action label Finetuning ℎ??? ℎ??? ℎ??? ℎ??? Testing Figure 3: The flow of the experiment. ?1,1 … ?1,?1 ?2,1 … ?2,?2 5.1 Figure 2: MC-HRED architecture. Emotion encoder is shown in dark blue, and action encoder in dark yellow. Blue NNs are relating to input, and yellow NNs to response. Previous works have demonstrated the effectiveness of large scale conversational data in improving the quality of dialogue systems (Banchs and Li, 2012; Ameixa et al., 2014; Serban et al., 2016). In this study, we make use of SubTle (Ameixa et al., 2014), a large scale conversational corpus collected from movie subtitles, to learn the syntactic and semantic knowledge for response generation. The use of movie subtitles is particularly suitable as they are available in large amounts and reflecting natural human communication. In our experiments, we utilize the HRED trained on the SubTle corpus as our starting model. We follow the data pre-processing method in (Serban et al., 2016). The processed SubTle corpus contained 5,503,741 query-answer p"
W18-5017,W12-1630,0,0.0746901,"Missing"
W18-5017,gratch-etal-2014-distress,0,0.300446,"ty and the induced emotion (sadness or anger). Finally, we selected 20 videos, 10 of each emotion with varied intensity level where the two human ratings agree. Corpus Construction: Positive Emotion Elicitation by an Expert 2.2 Data Collection We arrange for the dyad to consist of an expert and a participant, each with a distinct role. The roles are based on the “social sharing of emotion” scenario, which argues that after an emotional event, a person is inclined to initiate an interaction which Even though various affective conversational scenarios have been considered (McKeown et al., 2012; Gratch et al., 2014), there is still a lack of resources that show common emotional problems in everyday social settings. Furthermore, a great ma162 yses of validation experiments have confirmed the reliability and indicated the precision of the FEELtrace system (Cowie et al., 2000). centers on the event and their reactions to it (Rime et al., 1991; Luminet IV et al., 2000). This form of social sharing is argued to be integral in processing the emotional event (Rime et al., 1991). In the interactions, the expert plays the part of the external party who helps facilitate this process following the emotional respons"
W18-5017,P13-1095,0,0.0320912,"04). Acosta and Ward (2011) have attempted to connect the two competences to build rapport, by recognizing user’s emotion and reflecting it in the system response. Although these competences address some of the user’s emotional needs (Picard and Klein, 2002), they are not sufficient to provide emotional support in an interaction. Recently, there has been an increasing interest in eliciting user’s emotional response via dialogue system interaction, i.e. emotion elicitation. Skowron et al. (2013) have studied the impact of different affective personalities in a text-based dialogue system, while Hasegawa et al. (2013) constructed translation-based response generators with various emotion targets. Despite the positive results, these approaches have not yet paid attention to the emotional benefit for the users. Our work aims to draw on an important overlooked potential of emotion elicitation: its application to improve emotional states, similar to that of emotional support between humans. This can be achieved by actively eliciting a more positive emotional valence throughout the interaction, i.e. positive emotion elicitation. This takes form as a chat-oriented dialogue system interaction that is layered with"
W18-5017,N15-1020,0,0.0713517,"Missing"
W19-4106,P14-1092,0,0.021216,"pan Science and Technology Agency {takana.shohei.tj7, koichiro, sudoh, s-nakamura}@is.naist.jp Abstract the relation between dialogue continuity and the coherency of system responses is still an underlying problem. In this paper, we propose a novel method to select an appropriate response from response candidates generated by NCMs. We define a score for re-ranking to select a response that has an event causality relation to a dialogue history. Re-ranking effectively improves response reliability in language generation tasks such as why-question answering and dialogue systems (Oh et al., 2013; Jansen et al., 2014; Bogdanova and Foster, 2016; Ohmura and Eskenazi, 2018). We used event causality pairs extracted from a large-scale corpus (Shibata and Kurohashi, 2011; Shibata et al., 2014). We also use distributed event representation based on the Role Factored Tensor Model (RFTM) (Weber et al., 2018) to realize a robust matching of event causality relations, even if these causalities are not included in the extracted event causality pairs. In human and automatic evaluations, the proposed method outperformed conventional methods in selecting coherent and diverse responses. We propose a novel method for sel"
W19-4106,N06-1023,0,0.011883,"s also reported that a conversational model using event causality relations can generate diverse and coherent responses (Fujita et al., 2011). However, 2 Response Re-ranking Using Event Causality Relations Figure 1 shows an overview of the proposed method. The process consists of four parts. First, N -best response candidates are generated from 1 an NCM given a dialogue history (Figure 1 ; Section 2.1). Then, events (predicate-argument structures) are extracted by an event parser from both the dialogue history and the response candi2 dates (Figure 1 ). We used Kurohashi Nagao Parser (KNP)1 (Kawahara and Kurohashi, 2006; Sasano and Kurohashi, 2011) as the event parser. Next, the extracted events are converted to dis1 http://nlp.ist.i.kyoto-u.ac.jp/?KNP 51 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 51–59 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Figure 1: Neural conversational model+re-ranking using event causality; a response that has an event causality relation (“be exhausted” → “relax”) to the dialogue history is selected by the re-ranking. predicate 1 be stressed out argument 1 - predicate 2 relieve lif t 10.02 argument 2 stress Table 1: Exa"
W19-4106,D18-2012,0,0.0232194,"l., 2018) was 100. We used gated recurrent units (GRUs) (Cho et al., 2014; Chung et al., 2014) whose number of layers was 2 and hidden unit size was 256, for the encoder and decoder of the NCMs. The batch size was 100, the dropout probability was 0.1, and the teacher forcing rate was 1.0. We used Adam (Kingma and Ba, 2015) as the optimizer. The gradient clipping was 50, the learning rate for the encoder and the context RNN of HRED was 1e−4 , and the learning rate for the decoder was 5e−4 . The loss function was inverse token frequency (ITF) loss (Nakamura et al., 2019). We used sentencepiece (Kudo and Richardson, 2018) as the tokenizer, and the vocabulary size was 32,000. These settings were the same in all models. Repetitive suppression (Nakamura et al., 2019) and length normalization (Macherey et al., 2016) were used at the decoding step. Finally, λ of Eq. (1) and Eq. (4) was set to 1.0. esd and ewd are the dth dimensions of es and ew respectively. Additionally, we evaluated dist (Li et al., 2016), Pointwise Mutual Information (PMI) (Newman et al., 2010), and average response length (“length”). Dist and PMI are used to evaluate diversity and coherency respectively. PMI between a response and a dialogue hi"
W19-4106,N16-1154,0,0.0202521,"ology Agency {takana.shohei.tj7, koichiro, sudoh, s-nakamura}@is.naist.jp Abstract the relation between dialogue continuity and the coherency of system responses is still an underlying problem. In this paper, we propose a novel method to select an appropriate response from response candidates generated by NCMs. We define a score for re-ranking to select a response that has an event causality relation to a dialogue history. Re-ranking effectively improves response reliability in language generation tasks such as why-question answering and dialogue systems (Oh et al., 2013; Jansen et al., 2014; Bogdanova and Foster, 2016; Ohmura and Eskenazi, 2018). We used event causality pairs extracted from a large-scale corpus (Shibata and Kurohashi, 2011; Shibata et al., 2014). We also use distributed event representation based on the Role Factored Tensor Model (RFTM) (Weber et al., 2018) to realize a robust matching of event causality relations, even if these causalities are not included in the extracted event causality pairs. In human and automatic evaluations, the proposed method outperformed conventional methods in selecting coherent and diverse responses. We propose a novel method for selecting coherent and diverse"
W19-4106,N16-1014,0,0.0329726,"encoder and the context RNN of HRED was 1e−4 , and the learning rate for the decoder was 5e−4 . The loss function was inverse token frequency (ITF) loss (Nakamura et al., 2019). We used sentencepiece (Kudo and Richardson, 2018) as the tokenizer, and the vocabulary size was 32,000. These settings were the same in all models. Repetitive suppression (Nakamura et al., 2019) and length normalization (Macherey et al., 2016) were used at the decoding step. Finally, λ of Eq. (1) and Eq. (4) was set to 1.0. esd and ewd are the dth dimensions of es and ew respectively. Additionally, we evaluated dist (Li et al., 2016), Pointwise Mutual Information (PMI) (Newman et al., 2010), and average response length (“length”). Dist and PMI are used to evaluate diversity and coherency respectively. PMI between a response and a dialogue history is defined as, 3.2 Diversity of Beam Search We investigated internal diversity of N -best response candidates generated from each dialogue model. It is expected that the higher diversity is, the more effective re-ranking is. Hence, we evaluated diversity on the test data by dist-1, 2 (Li et al., 2016). Beam width was set to 20; it is same in the PMI = 1 |response| |response| ∑ wr"
W19-4106,D16-1230,0,0.0260094,"LEU, NIST, and PMI values than those of EncDec models in all re-ranking methods, we conducted a human evaluation by comparing HRED model-based systems. 4 Discussion We analyzed an adequacy of re-ranking using event causality relations. Here are system response examples of our proposed method. “()” indicates original Japanese sentences, “[]” indicates event causality relations used for re-ranking, and “<&gt;” indicates responses before re-ranking. All examples are translated from Japanese to English. 3.4 Human Evaluation It is difficult to evaluate system performances only with automatic metrics (Liu et al., 2016). Hence, we compared a baseline model and our models in a human evaluation to confirm coherency and dialogue continuity of responses selected by our proposed methods. We compared baseline HRED model with our proposed models, re-ranked without embedding and with embedding using the last Conversation 1: User 1: Because of my fears, I have been stressed out. 55 (Mou fuan-na koto ga oosugite sutoresu ga tamatteku.) User 2 (System): Are you OK? Don’t work too hard. ß (Daijobu desuka muri shinaide kudasaine) [work too hard → be stressed out (muri wo suru → sutoresu ga tamaru)] <Are you OK? (Daijobu"
W19-4106,D15-1166,0,0.0287431,"Missing"
W19-4106,P02-1040,0,0.103893,"the training. The dialogue corpus was split into 2,509,836, 63,308, and 58,970 dialogues as training, validation, and testing data, respectively. following experiments. The result is shown in Table 2: Ave.dists are averages of dist computed internal N -best response candidates. The diversity of EncDec is higher than that of HRED. 3.3 Comparison in Automatic Metrics Table 3 shows the results of our evaluation using automatic metrics. We compared the results by referring to the ratio of responses different from the without re-ranking method (“re-ranked”), bilingual evaluation understudy (BLEU) (Papineni et al., 2002), NIST (Doddington, 2002), and vector extrema (Gabriel et al., 2014) (“extrema”) score. NIST is based on BLEU, but heavily weights less frequent N-grams to focus on content words. Vector extrema computes cosine similarity between sentence vectors of a reference and a generated response from a model. Each sentence vector es is computed by taking extrema of Skip-gram word vectors ew in each dimension d as, { maxw∈s ewd if ewd &gt; |minw′ ∈s ew′ d | esd = . minw∈s ewd otherwise (5) 3.1 Model Settings The hidden unit size of Skip-gram (Mikolov et al., 2013c,a,b), predicate embedding, and RFTM (Weber"
W19-4106,N13-1090,0,0.159864,"alogue with those in the event causality pair pool. Any events are embedded into fixed length vectors to calculate their similarities. 52 Figure 2: Model architecture of predicate embedding Figure 3: Event causality relation matching; the lif t of the event causality relation in which “be exhausted” precedes “relax,” is calculated from the lif t of the most similar event causality relation where “be stressed out” precedes “relieve stress.” We define an event with a single predicate or a pair of a predicate and arguments. Argument a of an event is embedded into vector as va by using Skip-gram (Mikolov et al., 2013c,a,b). Predicate p of an event is embedded into vector as vp by using predicate embedding which is based on case-unit Skip-gram. Figure 2 shows the model architecture of predicate embedding. The model learns predicate vector representations which are good at predicting its arguments. To get an event embedding for the pair of vp and va , we propose to use RFTM, which was proposed by Weber et al. (2018). The RFTM embeds a predicate and its arguments into vector e as, ∑ e= Wa T (vp , va ). (2) Ave.dist-1 0.44 0.33 EncDec HRED Ave.dist-2 0.56 0.42 Table 2: Diversity of N -best Response Candidates"
W19-4106,I11-1085,0,0.0125595,"ational model using event causality relations can generate diverse and coherent responses (Fujita et al., 2011). However, 2 Response Re-ranking Using Event Causality Relations Figure 1 shows an overview of the proposed method. The process consists of four parts. First, N -best response candidates are generated from 1 an NCM given a dialogue history (Figure 1 ; Section 2.1). Then, events (predicate-argument structures) are extracted by an event parser from both the dialogue history and the response candi2 dates (Figure 1 ). We used Kurohashi Nagao Parser (KNP)1 (Kawahara and Kurohashi, 2006; Sasano and Kurohashi, 2011) as the event parser. Next, the extracted events are converted to dis1 http://nlp.ist.i.kyoto-u.ac.jp/?KNP 51 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 51–59 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Figure 1: Neural conversational model+re-ranking using event causality; a response that has an event causality relation (“be exhausted” → “relax”) to the dialogue history is selected by the re-ranking. predicate 1 be stressed out argument 1 - predicate 2 relieve lif t 10.02 argument 2 stress Table 1: Example of event causality relat"
W19-4106,shibata-etal-2014-large,0,0.283568,"responses is still an underlying problem. In this paper, we propose a novel method to select an appropriate response from response candidates generated by NCMs. We define a score for re-ranking to select a response that has an event causality relation to a dialogue history. Re-ranking effectively improves response reliability in language generation tasks such as why-question answering and dialogue systems (Oh et al., 2013; Jansen et al., 2014; Bogdanova and Foster, 2016; Ohmura and Eskenazi, 2018). We used event causality pairs extracted from a large-scale corpus (Shibata and Kurohashi, 2011; Shibata et al., 2014). We also use distributed event representation based on the Role Factored Tensor Model (RFTM) (Weber et al., 2018) to realize a robust matching of event causality relations, even if these causalities are not included in the extracted event causality pairs. In human and automatic evaluations, the proposed method outperformed conventional methods in selecting coherent and diverse responses. We propose a novel method for selecting coherent and diverse responses for a given dialogue context. The proposed method re-ranks response candidates generated from conversational models by using event causal"
W19-4106,N10-1012,0,0.0222227,"e learning rate for the decoder was 5e−4 . The loss function was inverse token frequency (ITF) loss (Nakamura et al., 2019). We used sentencepiece (Kudo and Richardson, 2018) as the tokenizer, and the vocabulary size was 32,000. These settings were the same in all models. Repetitive suppression (Nakamura et al., 2019) and length normalization (Macherey et al., 2016) were used at the decoding step. Finally, λ of Eq. (1) and Eq. (4) was set to 1.0. esd and ewd are the dth dimensions of es and ew respectively. Additionally, we evaluated dist (Li et al., 2016), Pointwise Mutual Information (PMI) (Newman et al., 2010), and average response length (“length”). Dist and PMI are used to evaluate diversity and coherency respectively. PMI between a response and a dialogue history is defined as, 3.2 Diversity of Beam Search We investigated internal diversity of N -best response candidates generated from each dialogue model. It is expected that the higher diversity is, the more effective re-ranking is. Hence, we evaluated diversity on the test data by dist-1, 2 (Li et al., 2016). Beam width was set to 20; it is same in the PMI = 1 |response| |response| ∑ wr max PMI(wr, wh). wh (6) wr and wh are words in the respon"
W19-4106,I11-1115,0,0.409655,"and the coherency of system responses is still an underlying problem. In this paper, we propose a novel method to select an appropriate response from response candidates generated by NCMs. We define a score for re-ranking to select a response that has an event causality relation to a dialogue history. Re-ranking effectively improves response reliability in language generation tasks such as why-question answering and dialogue systems (Oh et al., 2013; Jansen et al., 2014; Bogdanova and Foster, 2016; Ohmura and Eskenazi, 2018). We used event causality pairs extracted from a large-scale corpus (Shibata and Kurohashi, 2011; Shibata et al., 2014). We also use distributed event representation based on the Role Factored Tensor Model (RFTM) (Weber et al., 2018) to realize a robust matching of event causality relations, even if these causalities are not included in the extracted event causality pairs. In human and automatic evaluations, the proposed method outperformed conventional methods in selecting coherent and diverse responses. We propose a novel method for selecting coherent and diverse responses for a given dialogue context. The proposed method re-ranks response candidates generated from conversational model"
W19-4106,P13-1170,0,0.0793544,"Missing"
W19-8627,N18-2008,0,0.012838,"As shown in Figure 1, a response Ri is required to satisfy not only the behavioral characteristic of a given dialogue act but also the appropriateness in the dialogue context (=history). One of the simplest approaches to building such a conditional generation system given a class label in NCM is adding the class label to the input of a decoder (Li et al., 2016; Zhao et al., 2017; Sun et al., 2017). We describe this baseline in the following section. There is another research trend in controlling NCMs with a given condition, such as speaker or emotion labels (Li et al., 2016; Sun et al., 2017; Huang et al., 2018; Zhou et al., 2018). These NCMs are optimized by softmax cross-entropy loss (SCE-loss), which calculates a loss wordby-word. However, such existing training objectives do not necessarily guarantee that a generated response has high discriminability to for a given class label. In other words, SCE-loss is not an appropriate objective that explicitly evaluates whether a generated response reflects the property of the given class label or not. Therefore, the generated response will be biased by majority class labels. 3.2 Conditional NCM with Dialogue Acts We introduce a general conditional NCM th"
W19-8627,D18-1477,0,0.0519525,"ugh LSTM has dominant performance (Lei et al., 2017). This characteristic is critical for adversarial learning, which requires a large number of iterations. We used the policy gradient in this research to update the parameters of the generator, which is based on expected reward calculation by MCTS. However, MCTS requires enormous calculation costs because it requires scanning the discriminator and generator r × w times per one update of generator, where r is the number of rollouts and w is the number of words in a response each time step. Thus, we propose to use a simple recurrent unit (SRU) (Lei et al., 2018) in our generator and discriminator. SRU is known as an extension of RNN, which has comparable performance to LSTM even if it works at significantly higher speed. SRU is defined as follows. Figure 3: Implicit & Explicit-Discriminator. 4.2.2 Multi-class Objective; Explicit-Discriminator We propose an approach extending the classification problem of the discriminator from the binary classification of fake/real to multi-class classification to distinguish target dialogue act classes (Figure 3 upper-right). This discriminator has a multiclass objective for N +1 class classification. Here, N is the"
W19-8627,P16-1094,0,0.0500426,"Missing"
W19-8627,D17-1230,0,0.386726,"response by using statistical methods such as reinforcement learning (Young et al., 2010; Meguro et al., 2010; Yoshino and Kawahara, 2015; Keizer and Rieser, 2017). Response generation modules generate responses according to these dialogue acts or dialogue states on the basis of the rules, templates, agendas or other statistical models (Oh and Rudnicky, 2000; Xu and Rudnicky, 2000). Recently, neural network based generation modules have been widely used. Figure 1: Task of response generation conditioned by dialogue act labels. framework of the generative adversarial network (Yu et al., 2017; Li et al., 2017a; Tuan and Lee, 2019). This framework makes it possible to consider the total quality of generated sequences unlike SCE-loss, which is optimized for each token. We extend adversarial networks to generate qualified and controlled sentences given a condition, especially dialogue act labels. Wen et al. (2015) proposed a conditional language model (Semantically Conditioned Long Short-Term Memory; SC-LSTM) for task-oriented systems, which generates utterances on the basis of any dialogue acts and frames in the domain of restaurant navigation dialogue by using gating mechanism. However, the trainin"
W19-8627,W10-4356,0,0.0377102,"plicitly distinguishes sentences by using labels. This change strongly encourages the generation of label-conditioned sentences. We compared the proposed method with some existing methods for generating conditional responses. The experimental results show that our proposed method has higher controllability for dialogue acts even though it has higher or comparable naturalness to existing methods. 1 Introduction A dialogue act is defined as the intention or the function of an utterance in dialogues. Dialogue act labels are defined as unique classes to distinguish between kinds of dialogue acts (Boyer et al., 2010; Bunt et al., 2012). Some existing studies have exploited the dialogue act as a component in modeling the dialogue strategy of dialogue systems (Meguro et al., 2010; Yoshino and Kawahara, 2015; Shibata et al., 2016; Keizer and Rieser, 2017). Neural conversation models (NCMs), which learn a direct mapping between a dialogue history and a response utterance, are widely researched as a scalable approach to building non-task oriented dialogue systems (Vinyals and Le, 2015; Serban et al., 2016). However, it is difficult to control their responses on the basis of actual constraints such as dialogue"
W19-8627,I17-1099,0,0.265167,"response by using statistical methods such as reinforcement learning (Young et al., 2010; Meguro et al., 2010; Yoshino and Kawahara, 2015; Keizer and Rieser, 2017). Response generation modules generate responses according to these dialogue acts or dialogue states on the basis of the rules, templates, agendas or other statistical models (Oh and Rudnicky, 2000; Xu and Rudnicky, 2000). Recently, neural network based generation modules have been widely used. Figure 1: Task of response generation conditioned by dialogue act labels. framework of the generative adversarial network (Yu et al., 2017; Li et al., 2017a; Tuan and Lee, 2019). This framework makes it possible to consider the total quality of generated sequences unlike SCE-loss, which is optimized for each token. We extend adversarial networks to generate qualified and controlled sentences given a condition, especially dialogue act labels. Wen et al. (2015) proposed a conditional language model (Semantically Conditioned Long Short-Term Memory; SC-LSTM) for task-oriented systems, which generates utterances on the basis of any dialogue acts and frames in the domain of restaurant navigation dialogue by using gating mechanism. However, the trainin"
W19-8627,E17-1041,0,0.018765,"he discriminator in SeqGAN are extended to produce responses according to given dialogue acts. The adversarial framework is extended for jointly optimizing both networks: a generator network to produce response utterances 2 1 T T ∑ · ∇θ log p(wt |w1:t−1 , M, d)] min −ER∼pdata (·|M ) [log Dϕ (R, M )] −ER∼Gθ (·|M ) [log(1 − Dϕ (R, M ))]. G Gθ (wt |w1:t−1 , M, d) · QDθϕ ((w1:t−1 , M, d), wt ) w1:T QDθϕ ((w1:t−1 , M ), wt ) · ∇θ Gθ (wt |w1:t−1 , M ) ∑ Detailed derivation is shown in (Yu et al., 2017). 201 responses. We propose another discriminator to solve this problem in the next section. GANs (Tran et al., 2017). However, the training speed of LSTM is much slower than other types of networks, although LSTM has dominant performance (Lei et al., 2017). This characteristic is critical for adversarial learning, which requires a large number of iterations. We used the policy gradient in this research to update the parameters of the generator, which is based on expected reward calculation by MCTS. However, MCTS requires enormous calculation costs because it requires scanning the discriminator and generator r × w times per one update of generator, where r is the number of rollouts and w is the number of wor"
W19-8627,D16-1230,0,0.0666879,"Missing"
W19-8627,C10-1086,0,0.0392435,"h some existing methods for generating conditional responses. The experimental results show that our proposed method has higher controllability for dialogue acts even though it has higher or comparable naturalness to existing methods. 1 Introduction A dialogue act is defined as the intention or the function of an utterance in dialogues. Dialogue act labels are defined as unique classes to distinguish between kinds of dialogue acts (Boyer et al., 2010; Bunt et al., 2012). Some existing studies have exploited the dialogue act as a component in modeling the dialogue strategy of dialogue systems (Meguro et al., 2010; Yoshino and Kawahara, 2015; Shibata et al., 2016; Keizer and Rieser, 2017). Neural conversation models (NCMs), which learn a direct mapping between a dialogue history and a response utterance, are widely researched as a scalable approach to building non-task oriented dialogue systems (Vinyals and Le, 2015; Serban et al., 2016). However, it is difficult to control their responses on the basis of actual constraints such as dialogue act classes. Some existing studies have tackled this problem to control responses from 2 Related Work Dialogue systems that have dialogue management modules determi"
W19-8627,D15-1199,0,0.138205,"Missing"
W19-8627,W00-0306,0,0.204989,"e management modules determine a dialogue act or dialogue state 198 Proceedings of The 12th International Conference on Natural Language Generation, pages 198–207, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics of a system response by using statistical methods such as reinforcement learning (Young et al., 2010; Meguro et al., 2010; Yoshino and Kawahara, 2015; Keizer and Rieser, 2017). Response generation modules generate responses according to these dialogue acts or dialogue states on the basis of the rules, templates, agendas or other statistical models (Oh and Rudnicky, 2000; Xu and Rudnicky, 2000). Recently, neural network based generation modules have been widely used. Figure 1: Task of response generation conditioned by dialogue act labels. framework of the generative adversarial network (Yu et al., 2017; Li et al., 2017a; Tuan and Lee, 2019). This framework makes it possible to consider the total quality of generated sequences unlike SCE-loss, which is optimized for each token. We extend adversarial networks to generate qualified and controlled sentences given a condition, especially dialogue act labels. Wen et al. (2015) proposed a conditional language model"
W19-8627,W00-0309,0,0.43862,"termine a dialogue act or dialogue state 198 Proceedings of The 12th International Conference on Natural Language Generation, pages 198–207, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics of a system response by using statistical methods such as reinforcement learning (Young et al., 2010; Meguro et al., 2010; Yoshino and Kawahara, 2015; Keizer and Rieser, 2017). Response generation modules generate responses according to these dialogue acts or dialogue states on the basis of the rules, templates, agendas or other statistical models (Oh and Rudnicky, 2000; Xu and Rudnicky, 2000). Recently, neural network based generation modules have been widely used. Figure 1: Task of response generation conditioned by dialogue act labels. framework of the generative adversarial network (Yu et al., 2017; Li et al., 2017a; Tuan and Lee, 2019). This framework makes it possible to consider the total quality of generated sequences unlike SCE-loss, which is optimized for each token. We extend adversarial networks to generate qualified and controlled sentences given a condition, especially dialogue act labels. Wen et al. (2015) proposed a conditional language model (Semantically Condition"
W19-8627,P17-1061,0,0.268604,"fied and controlled sentences given a condition, especially dialogue act labels. Wen et al. (2015) proposed a conditional language model (Semantically Conditioned Long Short-Term Memory; SC-LSTM) for task-oriented systems, which generates utterances on the basis of any dialogue acts and frames in the domain of restaurant navigation dialogue by using gating mechanism. However, the training framework of SC-LSTM requires state frames that express the function and the contents of target utterances entirely. Thus, it is not realistic to apply this method to building an open-domain dialogue system. Zhao et al. (2017) proposed an NCM based on a variation of the conditional variational autoencoder (CVAE), which generates responses that have high diversity in discourse level by using latent variables as dialogue acts. However, this model has no mechanism to guarantee for generating discriminable responses for given dialogue acts. 3 Response Generation Conditioned by Dialogue Act Label 3.1 Task Settings The task we focus on is building a controllable NCM with a given condition, typically dialogue act labels. The problem is defined as generating the ith response word sequence Ri = {w1 , w2 , · · · , wT } given"
W19-8627,P18-1104,0,0.0334419,"Missing"
W19-8627,P17-2036,0,0.023219,"rm memory (LSTM) are generally used to model a sequential generation of responses in NCMs (Hochreiter and Schmidhuber, 1997; Vinyals and Le, 2015; Serban et al., 2016). The encoder receives a word at each time step by using forward RNNs to encode an utterance into a fixed length vector (utterance encoder). Utterance vectors of a dialogue are input to another fixed length vector according to their time sequence to encode the dialogue context (dialogue encoder). The resultant vector is fed into the decoder to generate a response sentence (word sequence). We used the same encoder architecture as Tian et al. (2017). In the decoding steps of the NCMs, the decoder receives a previous hidden vector ht−1 , memory cell ct−1 , and generated word wt−1 to generate a word wt . Here, t is an actual time-step of generation. The model has been changed to receive not only the previous word wt−1 but also the dialogue act label d in conditional generation. The vector representations of d and wt are concatenated and used as the input of the decoder at time-step t.1 The decoder itself also predicts the vector representation of words. This architecture is also the same as those of the models proposed by Zhao et al. (2017"
