2021.sigdial-1.8,Getting to Production with Few-shot Natural Language Generation Models,2021,-1,-1,12,0,1426,peyman heidari,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"In this paper, we study the utilization of pre-trained language models to enable few-shotNatural Language Generation (NLG) in task-oriented dialog systems. We introduce a system consisting of iterative self-training and an extensible mini-template framework that textualizes the structured input data into semi-natural text to fully take advantage of pre-trained language models. We compare var-ious representations of NLG models{'} input and output and show that transforming the input and output to be similar to what the language model has seen before during pre-training improves the model{'}s few-shot performance substantially. We show that neural mod-els can be trained with as few as 300 annotated examples while providing high fidelity, considerably lowering the resource requirements for standing up a new domain or language.This level of data efficiency removes the need for crowd-sourced data collection resulting in higher quality data annotated by expert linguists. In addition, model maintenance and debugging processes will improve in this few-shot setting. Finally, we explore distillation and using a caching system to satisfy latency requirements of real-world systems."
2021.inlg-1.2,Neural Methodius Revisited: Do Discourse Relations Help with Pre-Trained Models Too?,2021,-1,-1,4,1,5908,aleksandre maskharashvili,Proceedings of the 14th International Conference on Natural Language Generation,0,"Recent developments in natural language generation (NLG) have bolstered arguments in favor of re-introducing explicit coding of discourse relations in the input to neural models. In the Methodius corpus, a meaning representation (MR) is hierarchically structured and includes discourse relations. Meanwhile pre-trained language models have been shown to implicitly encode rich linguistic knowledge which provides an excellent resource for NLG. By virtue of synthesizing these lines of research, we conduct extensive experiments on the benefits of using pre-trained models and discourse relation information in MRs, focusing on the improvement of discourse coherence and correctness. We redesign the Methodius corpus; we also construct another Methodius corpus in which MRs are not hierarchically structured but flat. We report experiments on different versions of the corpora, which probe when, where, and how pre-trained models benefit from MRs with discourse relation information in them. We conclude that discourse relations significantly improve NLG when data is limited."
2021.inlg-1.10,Self-Training for Compositional Neural {NLG} in Task-Oriented Dialogue,2021,-1,-1,4,1,5910,xintong li,Proceedings of the 14th International Conference on Natural Language Generation,0,"Neural approaches to natural language generation in task-oriented dialogue have typically required large amounts of annotated training data to achieve satisfactory performance, especially when generating from compositional inputs. To address this issue, we show that self-training enhanced with constrained decoding yields large gains in data efficiency on a conversational weather dataset that employs compositional meaning representations. In particular, our experiments indicate that self-training with constrained decoding can enable sequence-to-sequence models to achieve satisfactory quality using vanilla decoding with five to ten times less data than with ordinary supervised baseline; moreover, by leveraging pretrained models, data efficiency can be increased further to fifty times. We confirm the main automatic results with human evaluations and show that they extend to an enhanced, compositional version of the E2E dataset. The end result is an approach that makes it possible to achieve acceptable performance on compositional NLG tasks using hundreds rather than tens of thousands of training samples."
2021.gem-1.12,"Structure-to-Text Generation with Self-Training, Acceptability Classifiers and Context-Conditioning for the {GEM} Shared Task",2021,-1,-1,6,0,6273,shreyan bakshi,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",0,"We explore the use of self-training and acceptability classifiers with pre-trained models for natural language generation in structure-to-text settings using three GEM datasets (E2E, WebNLG-en, Schema-Guided Dialog). With the Schema-Guided Dialog dataset, we also experiment with including multiple turns of context in the input. We find that self-training with reconstruction matching along with acceptability classifier filtering can improve semantic correctness, though gains are limited in the full-data setting. With context-conditioning, we find that including multiple turns in the context encourages the model to align with the user{'}s word and phrasing choices as well as to generate more self-consistent responses. In future versions of the GEM challenge, we encourage the inclusion of few-shot tracks to encourage research on data efficiency."
2021.emnlp-main.53,Building Adaptive Acceptability Classifiers for Neural {NLG},2021,-1,-1,12,0,1429,soumya batra,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"We propose a novel framework to train models to classify acceptability of responses generated by natural language generation (NLG) models, improving upon existing sentence transformation and model-based approaches. An NLG response is considered acceptable if it is both semantically correct and grammatical. We don{'}t make use of any human references making the classifiers suitable for runtime deployment. Training data for the classifiers is obtained using a 2-stage approach of first generating synthetic data using a combination of existing and new model-based approaches followed by a novel validation framework to filter and sort the synthetic data into acceptable and unacceptable classes. Our 2-stage approach adapts to a wide range of data representations and does not require additional data beyond what the NLG models are trained on. It is also independent of the underlying NLG model architecture, and is able to generate more realistic samples close to the distribution of the NLG model-generated responses. We present results on 5 datasets (WebNLG, Cleaned E2E, ViGGO, Alarm, and Weather) with varying data representations. We compare our framework with existing techniques that involve synthetic data generation using simple sentence transformations and/or model-based techniques, and show that building acceptability classifiers using data that resembles the generation model outputs followed by a validation framework outperforms the existing techniques, achieving state-of-the-art results. We also show that our techniques can be used in few-shot settings using self-training."
2020.webnlg-1.12,Leveraging Large Pretrained Models for {W}eb{NLG} 2020,2020,-1,-1,4,1,5910,xintong li,Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+),0,"In this paper, we report experiments on finetuning large pretrained models to realize resource description framework (RDF) triples to natural language. We provide the details of how to build one of the top-ranked English generation models in WebNLG Challenge 2020. We also show that there appears to be considerable potential for reranking to improve the current state of the art both in terms of statistical metrics and model-based metrics. Our human analyses of the generated texts show that for Russian, pretrained models showed some success, both in terms of lexical and morpho-syntactic choices for generation, as well as for content aggregation. Nevertheless, in a number of cases, the model can be unpredictable, both in terms of failure or success. Omissions of the content and hallucinations, which in many cases occurred at the same time, were major problems. By contrast, the models for English showed near perfect performance on the validation set."
2020.inlg-1.37,Neural {NLG} for Methodius: From {RST} Meaning Representations to Texts,2020,-1,-1,5,1,5909,symon stevensguille,Proceedings of the 13th International Conference on Natural Language Generation,0,"While classic NLG systems typically made use of hierarchically structured content plans that included discourse relations as central components, more recent neural approaches have mostly mapped simple, flat inputs to texts without representing discourse relations explicitly. In this paper, we investigate whether it is beneficial to include discourse relations in the input to neural data-to-text generators for texts where discourse relations play an important role. To do so, we reimplement the sentence planning and realization components of a classic NLG system, Methodius, using LSTM sequence-to-sequence (seq2seq) models. We find that although seq2seq models can learn to generate fluent and grammatical texts remarkably well with sufficiently representative Methodius training data, they cannot learn to correctly express Methodius{'}s similarity and contrast comparisons unless the corresponding RST relations are included in the inputs. Additionally, we experiment with using self-training and reverse model reranking to better handle train/test data mismatches, and find that while these methods help reduce content errors, it remains essential to include discourse relations in the input to obtain optimal performance."
2020.coling-industry.7,Best Practices for Data-Efficient Modeling in {NLG}:How to Train Production-Ready Neural Models with Less Data,2020,-1,-1,12,0,1431,ankit arun,Proceedings of the 28th International Conference on Computational Linguistics: Industry Track,0,"Natural language generation (NLG) is a critical component in conversational systems, owing to its role of formulating a correct and natural text response. Traditionally, NLG components have been deployed using template-based solutions. Although neural network solutions recently developed in the research community have been shown to provide several benefits, deployment of such model-based solutions has been challenging due to high latency, correctness issues, and high data needs. In this paper, we present approaches that have helped us deploy data-efficient neural solutions for NLG in conversational systems to production. We describe a family of sampling and modeling techniques to attain production quality with light-weight neural network models using only a fraction of the data that would be necessary otherwise, and show a thorough comparison between each. Our results show that domain complexity dictates the appropriate approach to achieve high data efficiency. Finally, we distill the lessons from our experimental findings into a list of best practices for production-level NLG model development, and present them in a brief runbook. Importantly, the end products of all of the techniques are small sequence-to-sequence models ({\textasciitilde}2Mb) that we can reliably deploy in production. These models achieve the same quality as large pretrained models ({\textasciitilde}1Gb) as judged by human raters."
W19-8611,A Tree-to-Sequence Model for Neural {NLG} in Task-Oriented Dialog,2019,0,0,4,0,23318,jinfeng rao,Proceedings of the 12th International Conference on Natural Language Generation,0,"Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Sequence-to-sequence models on flat meaning representations (MR) have been dominant in this task, for example in the E2E NLG Challenge. Previous work has shown that a tree-structured MR can improve the model for better discourse-level structuring and sentence-level planning. In this work, we propose a tree-to-sequence model that uses a tree-LSTM encoder to leverage the tree structures in the input MR, and further enhance the decoding by a structure-enhanced attention mechanism. In addition, we explore combining these enhancements with constrained decoding to improve semantic correctness. Our experiments not only show significant improvements over standard seq2seq baselines, but also is more data-efficient and generalizes better to hard scenarios."
W19-5608,Verbs in {E}gyptian {A}rabic: a case for register variation,2019,-1,-1,1,1,1437,michael white,Proceedings of the 3rd Workshop on Arabic Corpus Linguistics,0,None
W19-0123,Evaluation Order Effects in Dynamic Continuized {CCG}: From Negative Polarity Items to Balanced Punctuation,2019,0,0,1,1,1437,michael white,Proceedings of the Society for Computation in Linguistics ({SC}i{L}) 2019,0,None
P19-1080,Constrained Decoding for Neural {NLG} from Compositional Representations in Task-Oriented Dialogue,2019,0,4,4,0,23319,anusha balakrishnan,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset."
D19-6309,The {OSU}/{F}acebook Realizer for {SRST} 2019: {S}eq2{S}eq Inflection and Serialized {T}ree2{T}ree Linearization,2019,0,0,5,0,16611,kartikeya upasani,Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019),0,"We describe our exploratory system for the shallow surface realization task, which combines morphological inflection using character sequence-to-sequence models with a baseline linearizer that implements a tree-to-tree model using sequence-to-sequence models on serialized trees. Results for morphological inflection were competitive across languages. Due to time constraints, we could only submit complete results (including linearization) for English. Preliminary linearization results were decent, with a small benefit from reranking to prefer valid output trees, but inadequate control over the words in the output led to poor quality on longer sentences."
W18-6528,{LSTM} Hypertagging,2018,-1,-1,2,0,27676,reid fu,Proceedings of the 11th International Conference on Natural Language Generation,0,"Hypertagging, or supertagging for surface realization, is the process of assigning lexical categories to nodes in an input semantic graph. Previous work has shown that hypertagging significantly increases realization speed and quality by reducing the search space of the realizer. Building on recent work using LSTMs to improve accuracy on supertagging for parsing, we develop an LSTM hypertagging method for OpenCCG, an open source NLP toolkit for CCG. Our results show significant improvements in both hypertagging accuracy and downstream realization performance."
W18-3605,The {OSU} Realizer for {SRST} {`}18: Neural Sequence-to-Sequence Inflection and Incremental Locality-Based Linearization,2018,-1,-1,2,1,15559,david king,Proceedings of the First Workshop on Multilingual Surface Realisation,0,"Surface realization is a nontrivial task as it involves taking structured data and producing grammatically and semantically correct utterances. Many competing grammar-based and statistical models for realization still struggle with relatively simple sentences. For our submission to the 2018 Surface Realization Shared Task, we tackle the shallow task by first generating inflected wordforms with a neural sequence-to-sequence model before incrementally linearizing them. For linearization, we use a global linear model trained using early update that makes use of features that take into account the dependency structure and dependency locality. Using this pipeline sufficed to produce surprisingly strong results in the shared task. In future work, we intend to pursue joint approaches to linearization and morphological inflection and incorporating a neural language model into the linearization choices."
W18-0502,Using Paraphrasing and Memory-Augmented Models to Combat Data Sparsity in Question Interpretation with a Virtual Patient Dialogue System,2018,0,2,4,0.833333,3602,lifeng jin,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"When interpreting questions in a virtual patient dialogue system one must inevitably tackle the challenge of a long tail of relatively infrequently asked questions. To make progress on this challenge, we investigate the use of paraphrasing for data augmentation and neural memory-based classification, finding that the two methods work best in combination. In particular, we find that the neural memory-based approach not only outperforms a straight CNN classifier on low frequency questions, but also takes better advantage of the augmented data created by paraphrasing, together yielding a nearly 10{\%} absolute improvement in accuracy on the least frequently asked questions."
N18-5011,Madly Ambiguous: A Game for Learning about Structural Ambiguity and Why It{'}s Hard for Computers,2018,0,0,3,1,29280,ajda gokcen,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"Madly Ambiguous is an open source, online game aimed at teaching audiences of all ages about structural ambiguity and why it{'}s hard for computers. After a brief introduction to structural ambiguity, users are challenged to complete a sentence in a way that tricks the computer into guessing an incorrect interpretation. Behind the scenes are two different NLP-based methods for classifying the user{'}s input, one representative of classic rule-based approaches to disambiguation and the other representative of recent neural network approaches. Qualitative feedback from the system{'}s use in online, classroom, and science museum settings indicates that it is engaging and successful in conveying the intended take home messages. A demo of Madly Ambiguous can be played at \url{http://madlyambiguous.osu.edu}."
W17-6208,Parsing with Dynamic Continuized {CCG},2017,18,0,1,1,1437,michael white,Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms,0,None
W17-5405,"Breaking {NLP}: Using Morphosyntax, Semantics, Pragmatics and World Knowledge to Fool Sentiment Analysis Systems",2017,6,6,8,0,31526,taylor mahler,Proceedings of the First Workshop on Building Linguistically Generalizable {NLP} Systems,0,"This paper describes our {``}breaker{''} submission to the 2017 EMNLP {``}Build It Break It{''} shared task on sentiment analysis. In order to cause the {``}builder{''} systems to make incorrect predictions, we edited items in the blind test data according to linguistically interpretable strategies that allow us to assess the ease with which the builder systems learn various components of linguistic structure. On the whole, our submitted pairs break all systems at a high rate (72.6{\%}), indicating that sentiment analysis as an NLP task may still have a lot of ground to cover. Of the breaker strategies that we consider, we find our semantic and pragmatic manipulations to pose the most substantial difficulties for the builder systems."
W17-5002,Combining {CNN}s and Pattern Matching for Question Interpretation in a Virtual Patient Dialogue System,2017,17,9,2,0.833333,3602,lifeng jin,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"For medical students, virtual patient dialogue systems can provide useful training opportunities without the cost of employing actors to portray standardized patients. This work utilizes word- and character-based convolutional neural networks (CNNs) for question identification in a virtual patient dialogue system, outperforming a strong word- and character-based logistic regression baseline. While the CNNs perform well given sufficient training data, the best system performance is ultimately achieved by combining CNNs with a hand-crafted pattern matching system that is robust to label sparsity, providing a 10{\%} boost in system accuracy and an error reduction of 47{\%} as compared to the pattern-matching system alone."
W17-3702,A Simple Method for Clarifying Sentences with Coordination Ambiguities,2017,8,0,1,1,1437,michael white,Proceedings of the 1st Workshop on Explainable Computational Intelligence ({XCI} 2017),0,None
W16-6638,Enhancing {PTB} {U}niversal {D}ependencies for Grammar-Based Surface Realization,2016,12,0,2,1,15559,david king,Proceedings of the 9th International Natural Language Generation conference,0,None
W16-1718,Generating Disambiguating Paraphrases for Structurally Ambiguous Sentences,2016,18,5,3,0.740741,30319,manjuan duan,Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with {ACL} 2016 ({LAW}-X 2016),0,None
L16-1506,A Corpus of Word-Aligned Asked and Anticipated Questions in a Virtual Patient Dialogue System,2016,8,1,4,1,29280,ajda gokcen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a corpus of virtual patient dialogues to which we have added manually annotated gold standard word alignments. Since each question asked by a medical student in the dialogues is mapped to a canonical, anticipated version of the question, the corpus implicitly defines a large set of paraphrase (and non-paraphrase) pairs. We also present a novel process for selecting the most useful data to annotate with word alignments and for ensuring consistent paraphrase status decisions. In support of this process, we have enhanced the earlier Edinburgh alignment tool (Cohn et al., 2008) and revised and extended the Edinburgh guidelines, in particular adding guidance intended to ensure that the word alignments are consistent with the overall paraphrase status decision. The finished corpus and the enhanced alignment tool are made freely available."
W15-4704,Inducing Clause-Combining Rules: A Case Study with the {SP}a{RK}y Restaurant Corpus,2015,24,3,1,1,1437,michael white,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"We describe an algorithm for inducing clause-combining rules for use in a traditional natural language generation architecture. An experiment pairing lexicalized text plans from the SPaRKy Restaurant Corpus with logical forms obtained by parsing the corresponding sentences demonstrates that the approach is able to learn clause-combining operations which have essentially the same coverage as those used in the SPaRKy Restaurant Corpus. This paper fills a gap in the literature, showing that it is possible to learn microplanning rules for both aggregation and discourse connective insertion, an important step towards ameliorating the knowledge acquisition bottleneck for NLG systems that produce texts with rich discourse structures using traditional architectures."
W15-0611,Interpreting Questions with a Log-Linear Ranking Model in a Virtual Patient Dialogue System,2015,23,4,2,1,7108,evan jaffe,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present a log-linear ranking model for interpreting questions in a virtual patient dialogue system and demonstrate that it substantially outperforms a more typical multiclass classifier model using the same information. The full model makes use of weighted and concept-based matching features that together yield a 15% error reduction over a strong lexical overlap baseline. The accuracy of the ranking model approaches that of an extensively handcrafted pattern matching system, promising to reduce the authoring burden and make it possible to use confidence estimation in choosing dialogue acts; at the same time, the effectiveness of the concept-based features indicates that manual development resources can be productively employed with the approach in developing concept hierarchies."
W14-4424,Towards Surface Realization with {CCG}s Induced from Dependencies,2014,8,2,1,1,1437,michael white,Proceedings of the 8th International Natural Language Generation Conference ({INLG}),0,"We present a novel algorithm for inducing Combinatory Categorial Grammars from dependency treebanks, along with initial experiments showing that it can be used to achieve competitive realization results using an enhanced version of the surface realization shared task data."
P14-1039,That{'}s Not What {I} Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text,2014,35,2,2,0.740741,30319,manjuan duan,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving xe2x80x9cviciousxe2x80x9d ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones. Using parse accuracy in a simple reranking strategy for selfmonitoring, we find that with a stateof-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. However, by using an SVM ranker to combine the realizerxe2x80x99s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved. Moreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy."
W13-2104,Enhancing the Expression of Contrast in the {SP}a{RK}y Restaurant Corpus,2013,17,11,3,0,10049,david howcroft,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"We show that Nakatsu & Whitexe2x80x99s (2010) proposed enhancements to the SPaRKy Restaurant Corpus (SRC; Walker et al., 2007) for better expressing contrast do indeed make it possible to generate better texts, including ones that make effective and varied use of contrastive connectives and discourse adverbials. After first presenting a validation experiment for naturalness ratings of SRC texts gathered using Amazonxe2x80x99s Mechanical Turk, we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them. We conclude with a discussion of possible ways of improving the ranker in future work."
W12-1525,The Surface Realisation Task: Recent Developments and Future Plans,2012,13,8,5,0,26421,anja belz,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"The Surface Realisation Shared Task was first run in 2011. Two common-ground input representations were developed and for the first time several independently developed surface realisers produced realisations from the same shared inputs. However, the input representations had several shortcomings which we have been aiming to address in the time since. This paper reports on our work to date on improving the input representations and on our plans for the next edition of the SR Task. We also briefly summarise other related developments in NLG shared tasks and outline how the different ideas may be usefully brought together in the future."
W12-1528,Shared Task Proposal: Syntactic Paraphrase Ranking,2012,5,2,1,1,1437,michael white,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"We describe a new shared task on syntactic paraphrase ranking that is intended to run in conjunction with the main surface realization shared task. Taking advantage of the human judgments collected to evaluate the surface realizations produced by competing systems, the task is to automatically rank these realizations---viewed as syntactic paraphrases---in a way that agrees with the human judgments as often as possible. The task is designed to appeal to developers of surface realization systems as well as machine translation evaluation metrics: for surface realization systems, the task sidesteps the thorny issue of converting inputs to a common representation; for MT evaluation metrics, the task provides a challenging framework for advancing automatic evaluation, as many of the paraphrases are expected to be of high quality, differing only in subtle syntactic choices."
D12-1023,Minimal Dependency Length in Realization Ranking,2012,41,21,1,1,1437,michael white,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices. In this paper, we investigate dependency length minimization in the context of discriminative realization ranking, focusing on its potential to eliminate egregious ordering errors as well as better match the distributional characteristics of sentence orderings in news text. We find that with a state-of-the-art, comprehensive realization ranking model, dependency length minimization yields statistically significant improvements in BLEU scores and significantly reduces the number of heavy/light ordering errors. Through distributional analyses, we also show that with simpler ranking models, dependency length minimization can go overboard, too often sacrificing canonical word order to shorten dependencies, while richer models manage to better counterbalance the dependency length minimization preference against (sometimes) competing canonical word order preferences."
C12-2120,A Joint Phrasal and Dependency Model for Paraphrase Alignment,2012,23,14,3,0,21158,kapil thadani,Proceedings of {COLING} 2012: Posters,0,Monolingual alignment is frequently required for natural language tasks that involve similar or comparable sentences. We present a new model for monolingual alignment in which the score of an alignment decomposes over both the set of aligned phrases as well as a set of aligned dependency arcs. Optimal alignments under this scoring function are decoded using integer linear programming while model parameters are learned using standard structured prediction approaches. We evaluate our joint aligner on the Edinburgh paraphrase corpus and show significant gains over a Meteor baseline and a state-of-the-art phrase-based aligner.
2012.amta-monomt.3,Shallow and Deep Paraphrasing for Improved Machine Translation Parameter Optimization,2012,-1,-1,2,0,40043,dennis mehay,Workshop on Monolingual Machine Translation,0,"String comparison methods such as BLEU (Papineni et al., 2002) are the de facto standard in MT evaluation (MTE) and in MT system parameter tuning (Och, 2003). It is difficult for these metrics to recognize legitimate lexical and grammatical paraphrases, which is important for MT system tuning (Madnani, 2010). We present two methods to address this: a shallow lexical substitution technique and a grammar-driven paraphrasing technique. Grammatically precise paraphrasing is novel in the context of MTE, and demonstrating its usefulness is a key contribution of this paper. We use these techniques to paraphrase a single reference, which, when used for parameter tuning, leads to superior translation performance over baselines that use only human-authored references."
W11-2827,Glue Rules for Robust Chart Realization,2011,25,4,1,1,1437,michael white,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"This paper shows how glue rules can be used to increase the robustness of statistical chart realization in a manner inspired by dependency realization. Unlike the use of glue rules in MT---but like previous work with XLE on improving robustness with hand-crafted grammars---they are invoked here as a fall-back option when no grammatically complete realization can be found. The method works with Combinatory Categorial Grammar (CCG) and has been implemented in OpenCCG. As the techniques are not overly tied to CCG, they are expected to be applicable to other grammar-based chart realizers where robustness is a common problem. Unlike an earlier robustness technique of greedily assembling fragments, glue rules enable n-best outputs and are compatible with disjunctive inputs. Experimental results indicate that glue rules yield improved realizations in comparison to greedy fragment assembly, though a sizeable gap remains between the quality of grammatically complete realizations and fragmentary ones."
W11-2832,The First Surface Realisation Shared Task: Overview and Evaluation Results,2011,17,53,2,0,26421,anja belz,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"The Surface Realisation (SR) Task was a new task at Generation Challenges 2011, and had two tracks: (1) Shallow: mapping from shallow input representations to realisations; and (2) Deep: mapping from deep input representations to realisations. Five teams submitted six systems in total, and we additionally evaluated human toplines. Systems were evaluated automatically using a range of intrinsic metrics. In addition, systems were assessed by human judges in terms of Clarity, Readability and Meaning Similarity. This report presents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report."
W11-2836,The {OSU} System for Surface Realization at Generation Challenges 2011,2011,11,5,3,1,2209,rajakrishnan rajkumar,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"This report documents our efforts to develop a Generation Challenges 2011 surface realization system by converting the shared task deep inputs to ones compatible with OpenCCG. Although difficulties in conversion led us to employ machine learning for relation mapping and to introduce several robustness measures into OpenCCG's grammar-based chart realizer, the percentage of grammatically complete realizations still remained well below results using native OpenCCG inputs on the development set, with a corresponding drop in output quality. We discuss known conversion issues and possible ways to improve performance on shared task inputs."
W11-2706,Linguistically Motivated Complementizer Choice in Surface Realization,2011,-1,-1,2,1,2209,rajakrishnan rajkumar,Proceedings of the {UCNLG}+{E}val: Language Generation and Evaluation Workshop,0,None
W11-1609,Creating Disjunctive Logical Forms from Aligned Sentences for Grammar-Based Paraphrase Generation,2011,41,1,2,1,16609,scott martin,Proceedings of the Workshop on Monolingual Text-To-Text Generation,0,"We present a method of creating disjunctive logical forms (DLFs) from aligned sentences for grammar-based paraphrase generation using the OpenCCG broad coverage surface realizer. The method takes as input word-level alignments of two sentences that are para-phrases and projects these alignments onto the logical forms that result from automatically parsing these sentences. The projected alignments are then converted into phrasal edits for producing DLFs in both directions, where the disjunctions represent alternative choices at the level of semantic dependencies. The resulting DLFs are fed into the OpenCCG realizer for n-best realization, using a pruning strategy that encourages lexical diversity. After merging, the approach yields an n-best list of paraphrases that contain grammatical alternatives to each original sentence, as well as paraphrases that mix and match content from the pair. A preliminary error analysis suggests that the approach could benefit from taking the word order in the original sentences into account. We conclude with a discussion of plans for future work, highlighting the method's potential use in enhancing automatic MT evaluation."
J10-2001,"Generating Tailored, Comparative Descriptions with Contextually Appropriate Intonation",2010,78,23,1,1,1437,michael white,Computational Linguistics,0,"Generating responses that take user preferences into account requires adaptation at all levels of the generation process. This article describes a multi-level approach to presenting user-tailored information in spoken dialogues which brings together for the first time multi-attribute decision models, strategic content planning, surface realization that incorporates prosody prediction, and unit selection synthesis that takes the resulting prosodic structure into account. The system selects the most important options to mention and the attributes that are most relevant to choosing between them, based on the user model. Multiple options are selected when each offers a compelling trade-off. To convey these trade-offs, the system employs a novel presentation strategy which straightforwardly lends itself to the determination of information structure, as well as the contents of referring expressions. During surface realization, the prosodic structure is derived from the information structure using Combinatory Categorial Grammar in a way that allows phrase boundaries to be determined in a flexible, data-driven fashion. This approach to choosing pitch accents and edge tones is shown to yield prosodic structures with significantly higher acceptability than baseline prosody prediction models in an expert evaluation. These prosodic structures are then shown to enable perceptibly more natural synthesis using a unit selection voice that aims to produce the target tunes, in comparison to two baseline synthetic voices. An expert evaluation and f0 analysis confirm the superiority of the generator-driven intonation and its contribution to listeners' ratings."
D10-1055,Further Meta-Evaluation of Broad-Coverage Surface Realization,2010,20,16,3,1,44158,dominic espinosa,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We present the first evaluation of the utility of automatic evaluation metrics on surface realizations of Penn Treebank data. Using outputs of the OpenCCG and XLE realizers, along with ranked WordNet synonym substitutions, we collected a corpus of generated surface realizations. These outputs were then rated and post-edited by human annotators. We evaluated the realizations using seven automatic metrics, and analyzed correlations obtained between the human judgments and the automatic scores. In contrast to previous NLG meta-evaluations, we find that several of the metrics correlate moderately well with human judgments of both adequacy and fluency, with the TER family performing best overall. We also find that all of the metrics correctly predict more than half of the significant systemlevel differences, though none are correct in all cases. We conclude with a discussion of the implications for the utility of such metrics in evaluating generation in the presence of variation. A further result of our research is a corpus of post-edited realizations, which will be made available to the research community."
C10-2119,Designing Agreement Features for Realization Ranking,2010,22,10,2,1,2209,rajakrishnan rajkumar,Coling 2010: Posters,0,"This paper shows that incorporating linguistically motivated features to ensure correct animacy and number agreement in an averaged perceptron ranking model for CCG realization helps improve a state-of-the-art baseline even further. Traditionally, these features have been modelled using hard constraints in the grammar. However, given the graded nature of grammaticality judgements in the case of animacy we argue a case for the use of a statistical model to rank competing preferences. Though subject-verb agreement is generally viewed to be syntactic in nature, a perusal of relevant examples discussed in the theoretical linguistics literature (Kathol, 1999; Pollard and Sag, 1994) points toward the heterogeneous nature of English agreement. Compared to writing grammar rules, our method is more robust and allows incorporating information from diverse sources in realization. We also show that the perceptron model can reduce balanced punctuation errors that would otherwise require a post-filter. The full model yields significant improvements in BLEU scores on Section 23 of the CCGbank and makes many fewer agreement errors."
W09-1508,Grammar Engineering for {CCG} using Ant and {XSLT},2009,5,1,3,1,16609,scott martin,"Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing ({SETQA}-{NLP} 2009)",0,"Corpus conversion and grammar extraction have traditionally been portrayed as tasks that are performed once and never again revisited (Burke et al., 2004). We report the successful implementation of an approach to these tasks that facilitates the improvement of grammar engineering as an evolving process. Taking the standard version of the CCGbank (Hockenmaier and Steedman, 2007) as input, our system then introduces greater depth of linguistic insight by augmenting it with attributes the original corpus lacks: Propbank roles and head lexicalization for case-marking prepositions (Boxwell and White, 2008), derivational re-structuring for punctuation analysis (White and Rajkumar, 2008), named entity annotation and lemmatization. Our implementation applies successive XSLT transforms controlled by Apache Ant (http://ant.apache.org/) to an XML translation of this corpus, finally producing an OpenCCG grammar (http://openccg.sourceforge.net/). This design is beneficial to grammar engineering both because of XSLT's unique suitability to performing arbitrary transformations of XML trees and the fine-grained control that Ant provides. The resulting system enables state-of-the-art BLEU scores for surface realization on section 23 of the CCGbank."
N09-2041,Exploiting Named Entity Classes in {CCG} Surface Realization,2009,9,10,2,1,2209,rajakrishnan rajkumar,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"This paper describes how named entity (NE) classes can be used to improve broad coverage surface realization with the OpenCCG realizer. Our experiments indicate that collapsing certain multi-word NEs and interpolating a language model where NEs are replaced by their class labels yields the largest quality increase, with 4-grams adding a small additional boost. Substantial further benefit is obtained by including class information in the hyper-tagging (supertagging for realization) component of the system, yielding a state-of-the-art BLEU score of 0.8173 on Section 23 of the CCGbank. A targeted manual evaluation confirms that the BLEU score increase corresponds to a significant rise in fluency."
D09-1043,Perceptron Reranking for {CCG} Realization,2009,38,51,1,1,1437,michael white,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper shows that discriminative reranking with an averaged perceptron model yields substantial improvements in realization quality with CCG. The paper confirms the utility of including language model log probabilities as features in the model, which prior work on discriminative training with log linear models for HPSG realization had called into question. The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features. The full model yields a state-of-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpus-engineered grammar."
W08-1703,A More Precise Analysis of Punctuation for Broad-Coverage Surface Realization with {CCG},2008,15,13,1,1,1437,michael white,Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks,0,"This paper describes a more precise analysis of punctuation for a bi-directional, broad coverage English grammar extracted from the CCGbank (Hockenmaier and Steedman, 2007). We discuss various approaches which have been proposed in the literature to constrain overgeneration with punctuation, and illustrate how aspects of Briscoe's (1994) influential approach, which relies on syntactic features to constrain the appearance of balanced and unbalanced commas and dashes to appropriate sentential contexts, is unattractive for CCG. As an interim solution to constrain overgeneration, we propose a rule-based filter which bars illicit sequences of punctuation and cases of improperly unbalanced apposition. Using the OpenCCG toolkit, we demonstrate that our punctuation-augmented grammar yields substantial increases in surface realization coverage and quality, helping to achieve state-of-the-art BLEU scores."
P08-1022,{H}ypertagging: Supertagging for Surface Realization with {CCG},2008,33,33,2,1,44158,dominic espinosa,Proceedings of ACL-08: HLT,1,"In lexicalized grammatical formalisms, it is possible to separate lexical category assignment from the combinatory processes that make use of such categories, such as parsing and realization. We adapt techniques from supertagging xe2x80x94 a relatively recent technique that performs complex lexical tagging before full parsing (Bangalore and Joshi, 1999; Clark, 2002) xe2x80x94 for chart realization in OpenCCG, an open-source NLP toolkit for CCG. We call this approach hypertagging, as it operates at a level xe2x80x9cabovexe2x80x9d the syntax, tagging semantic representations with syntactic lexical categories. Our results demonstrate that a hypertagger-informed chart realizer can achieve substantial improvements in realization speed (being approximately twice as fast) with superior realization quality."
boxwell-white-2008-projecting,Projecting {P}ropbank Roles onto the {CCG}bank,2008,9,24,2,0,44758,stephen boxwell,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,This paper describes a method of accurately projecting Propbank roles onto constituents in the CCGbank and automatically annotating verbal categories with the semantic roles of their arguments. This method will be used to improve the structure of the derivations in the CCGbank and to facilitate research on semantic role tagging and broad coverage generation with CCG.
W07-2305,Avoiding Repetition in Generated Text,2007,25,6,2,0.714286,40735,mary foster,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"We investigate two methods for enhancing variation in the output of a stochastic surface realiser: choosing from among the highest-scoring realisation candidates instead of taking the single highest-scoring result (e-best sampling), and penalising the words from earlier sentences in a discourse when generating later ones (anti-repetition scoring). In a human evaluation study, subjects were asked to compare texts generated with and without the variation enhancements. Strikingly, subjects judged the texts generated using these two methods to be better written and less repetitive than the texts generated with optimal n-gram scoring; at the same time, no significant difference in understandability was found between the two versions. In analysing the two methods, we show that the simpler e-best sampling method is considerably more prone to introducing dispreferred variants into the output, indicating that best results can be obtained using anti repetition scoring with strict or no e-best sampling."
2007.mtsummit-ucnlg.4,Towards broad coverage surface realization with {CCG},2007,-1,-1,1,1,1437,michael white,Proceedings of the Workshop on Using corpora for natural language generation,0,None
W06-1403,{CCG} Chart Realization from Disjunctive Inputs,2006,18,23,1,1,1437,michael white,Proceedings of the Fourth International Natural Language Generation Conference,0,"This paper presents a novel algorithm for efficiently generating paraphrases from disjunctive logical forms. The algorithm is couched in the framework of Combinatory Categorial Grammar (CCG) and has been implemented as an extension to the OpenCCG surface realizer. The algorithm makes use of packed representations similar to those initially proposed by Shemtov (1997), generalizing the approach in a more straightforward way than in the algorithm ultimately adopted therein."
P06-1140,Learning to Say It Well: Reranking Realizations by Predicted Synthesis Quality,2006,29,29,2,1,40965,crystal nakatsu,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice, thereby improving the naturalness of synthetic speech in a spoken language dialogue system. The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized. The ranker is trained on realizer and synthesizer features in supervised fashion, using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generator's capability. Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average, ameliorating the problem of highly variable synthesis quality typically encountered with today's unit selection synthesizers."
W05-1104,Designing an Extensible {API} for Integrating Language Modeling and Realization,2005,26,8,1,1,1437,michael white,Proceedings of Workshop on Software,0,"We present an extensible API for integrating language modeling and realization, describing its design and efficient implementation in the OpenCCG surface realizer. With OpenCCG, language models may be used to select realizations with preferred word orders, promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-first anytime search. The API enables a variety of n-gram models to be easily combined and used in conjunction with appropriate edge pruning strategies. The n-gram models may be of any order, operate in reverse (right-to-left), and selectively replace certain words with their semantic classes. Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form, pitch accent, stem, part of speech, supertag, and semantic class."
P05-3012,Multimodal Generation in the {COMIC} Dialogue System,2005,19,20,2,1,40735,mary foster,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"We describe how context-sensitive, user-tailored output is specified and produced in the COMIC multimodal dialogue system. At the conference, we will demonstrate the user-adapted features of the dialogue manager and text planner."
W04-0601,Techniques for Text Planning with {XSLT},2004,14,27,2,1,40735,mary foster,Proceeedings of the Workshop on {NLP} and {XML} ({NLPXML}-2004): {RDF}/{RDFS} and {OWL} in Language Technology,0,We describe an approach to text planning that uses the XSLT template-processing engine to create logical forms for an external surface realizer. Using a realizer that can process logical forms with embedded alternatives provides a substitute for backtracking in the text-planning process. This allows the text planner to combine the strengths of the AI-planning and template-based traditions in natural language generation.
W03-2316,Adapting Chart Realization to {CCG},2003,18,53,1,1,1437,michael white,Proceedings of the 9th {E}uropean Workshop on Natural Language Generation ({ENLG}-2003) at {EACL} 2003,0,"We describe a bottom-up chart realization algorithm adapted for use with Combinatory Categorial Grammar (CCG), and show how it can be used to efficiently realize a wide range of coordination phenomena, including argument cluster coordination and gapping. The algorithm has been implemented as an extension to the OpenNLP open source CCG parser. As an avenue for future exploration, we also suggest how the realizer could be used to simplify the treatment of aggregation in conjunction with higher level content planning components."
W02-1610,Learning Domain-Specific Transfer Rules: An Experiment with {K}orean to {E}nglish Translation,2002,20,9,2,1,51615,benoit lavoie,{COLING}-02: Machine Translation in Asia,0,"We describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results. The system learns lexico-structural transfer rules using syntactic pattern matching, statistical co-occurrence and error-driven filtering. In an experiment with domain-specific Korean to English translation, the approach yielded substantial improvements over three baseline systems."
W02-0402,Selecting sentences for multidocument summaries using randomized local search,2002,14,21,1,1,1437,michael white,Proceedings of the {ACL}-02 Workshop on Automatic Summarization,0,"We present and evaluate a randomized local search procedure for selecting sentences to include in a multidocument summary. The search favors the inclusion of adjacent sentences while penalizing the selection of repetitive material, in order to improve intelligibility without unduly affecting informativeness. Sentence similarity is determined using both surface-oriented measures and semantic groups obtained from merging the output templates of an information extraction subsystem. In a comparative evaluation against two DUC-like baselines and three simpler versions of our system, we found that our randomized local search method provided substantial improvements in both content and intelligibility, while the use of the IE groups also appeared to contribute a small further improvement in content."
W01-1403,Inducing Lexico-Structural Transfer Rules from Parsed Bi-texts,2001,12,14,2,1,51615,benoit lavoie,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"This paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching, statistical co-occurrence and error-driven filtering. We present initial evaluation results and discuss future directions."
H01-1054,Multidocument Summarization via Information Extraction,2001,12,90,1,1,1437,michael white,Proceedings of the First International Conference on Human Language Technology Research,0,"We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support user-directed multidocument summarization."
W00-0505,Towards Translingual Information Access using Portable Information Extraction,2000,10,1,1,1,1437,michael white,{ANLP}-{NAACL} 2000 Workshop: Embedded Machine Translation Systems,0,"We report on a small study undertaken to demonstrate the feasibility of combining portable information extraction with MT in order to support translingual information access. After describing the proposed system's usage scenario and system design, we describe our investigation of transferring information extraction techniques developed for English to Korean. We conclude with a brief discussion of related MT issues we plan to investigate in future work."
W98-1428,"{EXEMPLARS}: A Practical, Extensible Framework For Dynamic Text Generation",1998,10,52,1,1,1437,michael white,Natural Language Generation,0,"In this paper, we present EXEMPLARS, an object-oriented, rule-based framework designed to support practical, dynamic text generation, emphasizing its novel features compared to .existing hybrid systems that mix template-style and more sophisticated techniques. These features-.include an extensible classification-based text planning mechanism, a definition language that is a superset of the Java language, and advanced support for HTMIdSGML templates."
A97-1038,{C}ogent{H}elp: {NLG} meets {SE} in a tool for authoring dynamically generated on-line help,1997,18,5,1,1,1437,michael white,Fifth Conference on Applied Natural Language Processing,0,"CogentHelp is a prototype tool for authoring dynamically generated on-line help for applications with graphical user interfaces, embodying the evolution-friendly properties of tools in the literate programming tradition. In this paper, we describe Cogen Help, highlighting the usefulness of certain natural language generation techniques in supporting software-engineering goals for help authoring tools --- principally, quality and evolvability of help texts."
P93-1040,The Imperfective Paradox and Trajectory-of-Motion Events,1993,12,2,1,1,1437,michael white,31st Annual Meeting of the Association for Computational Linguistics,1,"In the first part of the paper, I present a new treatment of THE IMPERFECTIVE PARADOX (Dowty 1979) for the restricted case of trajectory-of-motion events. This treatment extends and refines those of Moens and Steedman (1988) and Jackendoff (1991). In the second part, I describe an implemented algorithm based on this treatment which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints and restrictive assumptions."
E93-1048,Delimitedness and Trajectory-of-Motion Events,1993,29,4,1,1,1437,michael white,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The first part of the paper develops a novel, sortally-based approach to the problem of aspectual composition. The account is argued to be superior on both empirical and computational grounds to previous semantic approaches relying on referential homogeneity tests. While the account is restricted to manner-of-motion verbs, it does cover their interaction with mass terms, amount phrases, locative PPs, and distance, frequency, and temporal modifiers. The second part of the paper describes an implemented system based on the theoretical treatment which determines whether a specified sequence of events is or is not possible under varying situationally supplied contraints, given certain restrictive and simplifying assumptions. Briefly, the system extracts a set of contraint equations from the derived logical forms and solves them according to a best-value metric. Three particular limitations of the system and possible ways of addressing them are discussed in the conclusion."
C92-4181,On the Interpretation of Natural Language Instructions,1992,9,14,2,0,1524,barbara eugenio,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, we discuss the approach we take to the interpretation of instructions. Instructions describe actions related to each other and to other goals the agent may have; our claim is that the agent must actively compute the actions that s/he has to perform, not simply extract their descriptions from the input.We will start by discussing some inferences that are necessary to understand instructions, and we will draw some conclusions about action representation formalisms and inference processes. We will discuss our approach, which includes an action representation formalism based on Conceptual Structures [Jac90], and the construction of the structure of the agent's intentions. We will conclude with an example that shows why such representations help us in analyzing instructions."
C92-1040,Conceptual Structures and {CCG}: Linking Theory and Incorporated Argument Adjuncts,1992,11,4,1,1,1437,michael white,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In Combinatory Categorial Grammar (CCG) [Ste90, Ste91], semantic function-argument structures are compositionally produced through the course of a derivation. These structures identify, inter alia, which entities play the same roles in different events for expressions involving a wide range of coordinate constructs. This sameness of role (i.e. thematic) information is not identified, however, across cases of verbal diathesis. To handle these cases as well, the present paper demonstrates how to adapt the solution developed in Conceptual Semantics [Jac90, Jac91] to fit the CCG paradigm.The essence of the approach is to redefine the Linking Theory component of Conceptual Semantics in terms of CCG categories, so that derivations yield conceptual structures representing the desired thematic information; in this way no changes are required on the CCG side. While this redefinition is largely straightforward, an interesting problem arises in the case of Conceptual Semantics' Incorporated Argument Adjuncts. In examining these, the paper shows that they cannot be treated as adjuncts in the CCG sense without introducing new machinery, nor without compromising the independence of the two theories. For this reason, the paper instead adopts the more traditional approach of treating them as oblique arguments."
