2020.lrec-1.678,J05-1004,0,0.280662,"erbs Figure 1: Distribution of verbs in the VQA dataset (Antol et al., 2015a), ’to be’ versus ’other verbs’. roles (Martin and Jurafsky, 2009), which may include roles like agent or patient as encoded in a resource such as VerbNet (Kipper et al., 2008). Semantic role labeling has been shown to improve performance in challenging tasks such as dialog systems, machine reading, translation and question answering (Strubell et al., 2018; Shen and Lapata, 2007). However, the difficulty of clearly defining such roles has given rise to other approaches, such as the abstract roles provided by PropBank (Palmer et al., 2005), or the specialized frame elements provided by FrameNet (Fillmore et al., 2003). In FrameNet, verb semantics is described by frames or situations. Frame elements are defined for each frame and correspond to major entities present in the evoked situation. For example, the frame Cooking creation has four core elements, namely Produced food, Ingredients, Heating Instrument, Container. In order to create a VQA dataset with verb semantic information, we took advantage of the imSitu dataset (Yatskar et al., 2016), developed for situation recognition. The im5524 fixing Agent Object Part Tool Place m"
2020.lrec-1.678,D07-1002,0,0.202441,"Missing"
2020.lrec-1.678,D18-1548,0,0.0397175,"Missing"
2020.lrec-1.74,W09-3929,0,0.0207864,"t to our work, (Manuvinakurike et al., 2018) evaluated various LSTM and CNN networks in an imageediting domain. They modeled spoken conversations incrementally (word-by-word), to efficiently process the multiple fine-grained utterances by the user. Inspired by hierarchical classification applied to questions (Li and Roth, 2002), we approach DA classification as two layers, in which the bottom layer is dependent on the classification outcome of the top layer. As far as we know, think aloud components of conversations have been occasionally studied, but have hardly been computationally modeled (Benotti, 2009); in some cases, they have been excluded from analysis, as being self-addressed speech that does not contribute to the conversation (Jovanovic et al., 2006). Data Augmentation. Available public datasets on dialogue are limited to a few domains, mostly chatbots or information search. Often, the only choice is to manually build a new corpus, whose size is limited by the time and effort necessary to collect and annotate the data. Data augmentation, which applies class-preserving transformations, has been effective to enlarge datasets. Paraphrasing has been popular for data augmentation for variou"
2020.lrec-1.74,P14-1133,0,0.017512,"that does not contribute to the conversation (Jovanovic et al., 2006). Data Augmentation. Available public datasets on dialogue are limited to a few domains, mostly chatbots or information search. Often, the only choice is to manually build a new corpus, whose size is limited by the time and effort necessary to collect and annotate the data. Data augmentation, which applies class-preserving transformations, has been effective to enlarge datasets. Paraphrasing has been popular for data augmentation for various NLP tasks. Representative examples include semantic parsing (Campagna et al., 2019; Berant and Liang, 2014; Jia and Liang, 2016), question answering (Fader et al., 2013; Dong et al., 2017), and semantic slot filling (Yoo et al., 2018; Hou et al., 2018), but not for DA classification as far as we know. Semantic Slot Filling. In spoken dialogue systems, semantic slot filling is tasked with identifying terms belonging to fixed slots and passing them as parameters to down-stream processing. It is common practice to treat semantic slot filling as a sequence labeling problem and apply a supervised learning method that trains on sequences (Mesnil et al., 2015; Hakkani-T¨ur et al., 2016; Wang et al., 2011"
2020.lrec-1.74,D17-1091,0,0.0269189,"Missing"
2020.lrec-1.74,P13-1158,0,0.0176714,"006). Data Augmentation. Available public datasets on dialogue are limited to a few domains, mostly chatbots or information search. Often, the only choice is to manually build a new corpus, whose size is limited by the time and effort necessary to collect and annotate the data. Data augmentation, which applies class-preserving transformations, has been effective to enlarge datasets. Paraphrasing has been popular for data augmentation for various NLP tasks. Representative examples include semantic parsing (Campagna et al., 2019; Berant and Liang, 2014; Jia and Liang, 2016), question answering (Fader et al., 2013; Dong et al., 2017), and semantic slot filling (Yoo et al., 2018; Hou et al., 2018), but not for DA classification as far as we know. Semantic Slot Filling. In spoken dialogue systems, semantic slot filling is tasked with identifying terms belonging to fixed slots and passing them as parameters to down-stream processing. It is common practice to treat semantic slot filling as a sequence labeling problem and apply a supervised learning method that trains on sequences (Mesnil et al., 2015; Hakkani-T¨ur et al., 2016; Wang et al., 2011; Vu, 2016). We require semantic slot filling as a sub-task of"
2020.lrec-1.74,C18-1105,0,0.0151972,"omains, mostly chatbots or information search. Often, the only choice is to manually build a new corpus, whose size is limited by the time and effort necessary to collect and annotate the data. Data augmentation, which applies class-preserving transformations, has been effective to enlarge datasets. Paraphrasing has been popular for data augmentation for various NLP tasks. Representative examples include semantic parsing (Campagna et al., 2019; Berant and Liang, 2014; Jia and Liang, 2016), question answering (Fader et al., 2013; Dong et al., 2017), and semantic slot filling (Yoo et al., 2018; Hou et al., 2018), but not for DA classification as far as we know. Semantic Slot Filling. In spoken dialogue systems, semantic slot filling is tasked with identifying terms belonging to fixed slots and passing them as parameters to down-stream processing. It is common practice to treat semantic slot filling as a sequence labeling problem and apply a supervised learning method that trains on sequences (Mesnil et al., 2015; Hakkani-T¨ur et al., 2016; Wang et al., 2011; Vu, 2016). We require semantic slot filling as a sub-task of our data augmentation pipeline. However, supporting supervised learning would requi"
2020.lrec-1.74,P16-1002,0,0.0204825,"e to the conversation (Jovanovic et al., 2006). Data Augmentation. Available public datasets on dialogue are limited to a few domains, mostly chatbots or information search. Often, the only choice is to manually build a new corpus, whose size is limited by the time and effort necessary to collect and annotate the data. Data augmentation, which applies class-preserving transformations, has been effective to enlarge datasets. Paraphrasing has been popular for data augmentation for various NLP tasks. Representative examples include semantic parsing (Campagna et al., 2019; Berant and Liang, 2014; Jia and Liang, 2016), question answering (Fader et al., 2013; Dong et al., 2017), and semantic slot filling (Yoo et al., 2018; Hou et al., 2018), but not for DA classification as far as we know. Semantic Slot Filling. In spoken dialogue systems, semantic slot filling is tasked with identifying terms belonging to fixed slots and passing them as parameters to down-stream processing. It is common practice to treat semantic slot filling as a sequence labeling problem and apply a supervised learning method that trains on sequences (Mesnil et al., 2015; Hakkani-T¨ur et al., 2016; Wang et al., 2011; Vu, 2016). We requir"
2020.lrec-1.74,E06-1022,0,0.158096,"Missing"
2020.lrec-1.74,W19-1601,0,0.0225233,"d to increase the KO vocabulary, for a total of 1,637 terms. transcribed data; we will come back to these points in the conclusions. 4.1. Data Augmentation Pipeline Our corpus, comprised of 3,179 utterances, is comparable in size to other multimodal dialogues collected in a situated setting (i.e., our subjects interacted physically with a large screen display during conversation). For example, ELDERLY-AT-HOME (Chen et al., 2015) comprises 4.8K utterances, capturing dialogue interactions relating to a helper assisting an elderly person in performing daily living activities. Another example is (Katsakioris et al., 2019), which comprises of approximately 2.9K utterances pertaining to collaborative planning dialogues for autonomous underwater vehicles. The small size of such kinds of corpora could limit the ability of machine learning models to be trained effectively. In contrast, many modern approaches to DA classification train on much larger datasets, and are either multimodal but not situated (e.g., the Augmented Multi-party Interaction (AMI) meeting corpus (Popescu-Belis and Estrella, 2007) contains transcription of over 171 meetings covering a duration of 100 total hours, but the subjects don’t interact"
2020.lrec-1.74,C16-1189,0,0.0438442,"Missing"
2020.lrec-1.74,D10-1084,0,0.226917,"CRF, it shows that for smaller datasets not only are more traditional models like CRFs still competitive, but perhaps more importantly, are much faster to train and less energy-hungry (Strubell et al., 2019). In the following, after discussing related work, we will present our corpus, our data augmentation pipeline, and experimental set-up. We will then discuss in detail the results of our experiments. 2. Related Work Dialogue Act Classification. Numerous methods have been studied for DA classification; here we focus specifically on structured prediction such as MaxEnt (Ang et al., ) and CRF (Kim et al., 2010). Recently, deep neural network models have achieved stateof-the-art results. Representative samples include (Khanpour et al., 2016; Kumar et al., 2018; Ahmadvand et al., 2019). Closest to our work, (Manuvinakurike et al., 2018) evaluated various LSTM and CNN networks in an imageediting domain. They modeled spoken conversations incrementally (word-by-word), to efficiently process the multiple fine-grained utterances by the user. Inspired by hierarchical classification applied to questions (Li and Roth, 2002), we approach DA classification as two layers, in which the bottom layer is dependent o"
2020.lrec-1.74,W16-3639,1,0.892696,"Missing"
2020.lrec-1.74,C02-1150,0,0.403898,"e we focus specifically on structured prediction such as MaxEnt (Ang et al., ) and CRF (Kim et al., 2010). Recently, deep neural network models have achieved stateof-the-art results. Representative samples include (Khanpour et al., 2016; Kumar et al., 2018; Ahmadvand et al., 2019). Closest to our work, (Manuvinakurike et al., 2018) evaluated various LSTM and CNN networks in an imageediting domain. They modeled spoken conversations incrementally (word-by-word), to efficiently process the multiple fine-grained utterances by the user. Inspired by hierarchical classification applied to questions (Li and Roth, 2002), we approach DA classification as two layers, in which the bottom layer is dependent on the classification outcome of the top layer. As far as we know, think aloud components of conversations have been occasionally studied, but have hardly been computationally modeled (Benotti, 2009); in some cases, they have been excluded from analysis, as being self-addressed speech that does not contribute to the conversation (Jovanovic et al., 2006). Data Augmentation. Available public datasets on dialogue are limited to a few domains, mostly chatbots or information search. Often, the only choice is to ma"
2020.lrec-1.74,W18-5033,0,0.021051,"ollowing, after discussing related work, we will present our corpus, our data augmentation pipeline, and experimental set-up. We will then discuss in detail the results of our experiments. 2. Related Work Dialogue Act Classification. Numerous methods have been studied for DA classification; here we focus specifically on structured prediction such as MaxEnt (Ang et al., ) and CRF (Kim et al., 2010). Recently, deep neural network models have achieved stateof-the-art results. Representative samples include (Khanpour et al., 2016; Kumar et al., 2018; Ahmadvand et al., 2019). Closest to our work, (Manuvinakurike et al., 2018) evaluated various LSTM and CNN networks in an imageediting domain. They modeled spoken conversations incrementally (word-by-word), to efficiently process the multiple fine-grained utterances by the user. Inspired by hierarchical classification applied to questions (Li and Roth, 2002), we approach DA classification as two layers, in which the bottom layer is dependent on the classification outcome of the top layer. As far as we know, think aloud components of conversations have been occasionally studied, but have hardly been computationally modeled (Benotti, 2009); in some cases, they have bee"
2020.lrec-1.74,D14-1162,0,0.0878102,"Missing"
2020.lrec-1.74,P07-2024,0,0.0188566,"gue interactions relating to a helper assisting an elderly person in performing daily living activities. Another example is (Katsakioris et al., 2019), which comprises of approximately 2.9K utterances pertaining to collaborative planning dialogues for autonomous underwater vehicles. The small size of such kinds of corpora could limit the ability of machine learning models to be trained effectively. In contrast, many modern approaches to DA classification train on much larger datasets, and are either multimodal but not situated (e.g., the Augmented Multi-party Interaction (AMI) meeting corpus (Popescu-Belis and Estrella, 2007) contains transcription of over 171 meetings covering a duration of 100 total hours, but the subjects don’t interact with / manipulate their physical environment) or are primarily unimodal (e.g., the Meeting Recorder Dialogue Act Corpus • Word-level suggestions. An utterance u is first tokenized on words using NLTK 6 . Subsequently, some of the tokens of u are merged to form longer tokens, through three kinds of matches, including hyphen matches (e.g., treat ”crime-type” as one term instead of two individual ones), regular expression matches on time-based terms (e.g. merge to make time interva"
2020.lrec-1.74,W04-2319,0,0.0599742,"T 57 ”If you want you can close these graphs as I won’t be needing it anymore” FACT-BASED 43 ”So, what kind of crime is what kin– what kind of crime is maximum?” PREFERENCEBASED 33 ”*Uh* okay I somehow feel the 06-2 and 07-2 they are they’re much clearer *uh* to visualize and understand rather than 06-1 and 07-1.” MODIFYVISUALIZATION 27 ”*Uh* do– can you show only the bar graph or do you have some any other way of visualizing the same?” APPEARANCE 7 ”*Um*, interesting, can I see labels on the data please?” HIGH-LEVELQUERY 2 ”So, according to you, which areas do I deploy the officers?” (MRDA) (Shriberg et al., 2004) and Switchboard (Jurafsky et al., 1997) contain 78k and 193k utterances respectively). We addressed data insufficiency in our domain by applying a data augmentation pipeline that uses paraphrasing. (a) Paraphrasing. The pipeline starts by generating 20 raw paraphrases using a domain independent, pretrained model (Wieting et al., 2017). This model uses machine translation to obtain paraphrases and then trains on them using an LSTM network to learn sentence embeddings. In a small number of cases, our pipeline removes paraphrases which contain different punctuation but share the same words. (b)"
2020.lrec-1.74,J00-3003,0,0.346998,"Missing"
2020.lrec-1.74,P19-1355,0,0.0230213,"aluation is concerned, we show that a layered approach to DA classification for our domain is effective. Of the sequential classifiers, across all settings, CRF outperformed all others including the LSTM networks, despite the LSTM models making modest improvements when supplied with domain-trained word em590 beddings. While this may not be surprising since a neural model has many more parameters to train than a CRF, it shows that for smaller datasets not only are more traditional models like CRFs still competitive, but perhaps more importantly, are much faster to train and less energy-hungry (Strubell et al., 2019). In the following, after discussing related work, we will present our corpus, our data augmentation pipeline, and experimental set-up. We will then discuss in detail the results of our experiments. 2. Related Work Dialogue Act Classification. Numerous methods have been studied for DA classification; here we focus specifically on structured prediction such as MaxEnt (Ang et al., ) and CRF (Kim et al., 2010). Recently, deep neural network models have achieved stateof-the-art results. Representative samples include (Khanpour et al., 2016; Kumar et al., 2018; Ahmadvand et al., 2019). Closest to o"
2020.lrec-1.74,W13-4017,0,0.145484,"urce consumption. Keywords: Dialogue, Corpus, Statistical and Machine Learning Methods 1. Introduction The role of context on interpretation has been recognized since the very beginning of NLP. This is even more important in dialogue processing, where determining the intent behind the interlocutor’s utterance is paramount to appropriately acting on that intent. The underlying intent is often captured via the notion of a speech act (Searle, 1975), commonly operationalized as a dialogue act (DA) in NLP. Most methods for DA classification, whether earlier sequential models (Stolcke et al., 2000; Tavafi et al., 2013) or more recent deep learning models (Kumar et al., 2018; Ahmadvand et al., 2019) operate on conversations consisting of adjacency pairs (Schegloff and Sacks, 1973), where the DA by speaker 1 strongly conditions the possible responses by speaker 2. However, such dependencies are not necessarily preserved in other kinds of conversation, for example asynchronous conversations such as email or online forums (Tavafi et al., 2013); or conversations where speakers solve a complex problem, and where multi-utterance turns may include, within the turn itself, a richer context for the intent that is com"
2020.lrec-1.74,D17-1026,0,0.0208447,"”*Uh* do– can you show only the bar graph or do you have some any other way of visualizing the same?” APPEARANCE 7 ”*Um*, interesting, can I see labels on the data please?” HIGH-LEVELQUERY 2 ”So, according to you, which areas do I deploy the officers?” (MRDA) (Shriberg et al., 2004) and Switchboard (Jurafsky et al., 1997) contain 78k and 193k utterances respectively). We addressed data insufficiency in our domain by applying a data augmentation pipeline that uses paraphrasing. (a) Paraphrasing. The pipeline starts by generating 20 raw paraphrases using a domain independent, pretrained model (Wieting et al., 2017). This model uses machine translation to obtain paraphrases and then trains on them using an LSTM network to learn sentence embeddings. In a small number of cases, our pipeline removes paraphrases which contain different punctuation but share the same words. (b) Semantic Slot Filling. In spoken dialogue systems, semantic slot filling is tasked with identifying terms belonging to fixed slots and passing them as parameters to down-stream processing. Consider the example ”So can we also get a breakdown of the type of crimes for 10 AM?” A better visualization can be realized for this user request"
2020.nlpmc-1.6,P90-1010,0,0.766152,"Missing"
2020.nuse-1.15,N15-1171,0,0.0480788,"Missing"
2020.nuse-1.15,D16-1148,0,0.196251,"Missing"
2020.nuse-1.15,D19-5905,0,0.0148438,"Each box represents the number of times a moral frame disagreement occurred at the sentence level. The figure shows that annotators often disagree on the most frequent frames, Harm, Subversion and Cheating. These results reflect the challenges in using nonexperts for moral frame annotation. A number of annotation studies have analyzed the reliability of non-expert annotations, and investigated whether corrections need to be applied to the annotation process and / or to the models derived from the nonexpert annotated datasets (Snow et al., 2008; Welinder and Perona, 2010; Patton et al., 2019; Lavee et al., 2019). However, many of these studies can actually compare the performance of non-expert and expert annotation, since datasets annotated by experts for the phenomenon of interest did exist; this was not the case for us. In fact, this initial effort of ours at annotation can be taken as an indication of how difficult annotating for moral frames is for non-experts; it remains to be seen how expert annotators would fare on this task. This is part of the future research we will undertake to understand whether this task can be crowdsourced successfully at scale or whether it requires expert annotators."
2020.nuse-1.15,L16-1591,0,0.178531,"Missing"
2020.nuse-1.15,D08-1027,0,0.073359,"t. Figure 2 shows the frame confusion matrix at the sentence level. Each box represents the number of times a moral frame disagreement occurred at the sentence level. The figure shows that annotators often disagree on the most frequent frames, Harm, Subversion and Cheating. These results reflect the challenges in using nonexperts for moral frame annotation. A number of annotation studies have analyzed the reliability of non-expert annotations, and investigated whether corrections need to be applied to the annotation process and / or to the models derived from the nonexpert annotated datasets (Snow et al., 2008; Welinder and Perona, 2010; Patton et al., 2019; Lavee et al., 2019). However, many of these studies can actually compare the performance of non-expert and expert annotation, since datasets annotated by experts for the phenomenon of interest did exist; this was not the case for us. In fact, this initial effort of ours at annotation can be taken as an indication of how difficult annotating for moral frames is for non-experts; it remains to be seen how expert annotators would fare on this task. This is part of the future research we will undertake to understand whether this task can be crowdsou"
2020.nuse-1.15,P15-1157,0,0.0688262,"Missing"
2020.nuse-1.15,P17-1092,0,0.0419505,"Missing"
2020.nuse-1.15,P17-1069,0,0.0405738,"Missing"
2020.nuse-1.15,W17-2913,0,0.0738095,"Missing"
2020.sigdial-1.30,W17-5526,0,0.0479654,"Missing"
2020.sigdial-1.30,D18-1547,0,0.0324937,"ty data collected via Fitbit cannot be shared, since consent did not include permission for such data; dataset 1 cannot be shared, because of lack of consent. tracking, dialogue act prediction, and response generation; labeled datasets for each of these tasks were provided (Williams et al., 2013). However, most of these datasets focused on traveling and restaurant booking domains (Henderson et al., 2014). Moreover, for data collection, predefined scenarios are given to the users and thus, the users’ responses are not as spontaneous as they would be in a real-life situation (Asri et al., 2017; Budzianowski et al., 2018). Unfortunately, there are no such publicly available datasets for dialogue systems in the health domain. review tasks, assess, counseling, assign task, preclosing, and closing. Conversely, our stages-phases schema looks at the fine-grained decomposition of review-tasks, counseling, and assign task, which Bickmore et al. (2011) did not do. As far as we know, no other work models HC dialogues collected in a SMART goal setting, focusing on slotvalues and higher-level conversation flow. 3 Based on the domain, practitioners modify the definition of SMART components to fit the task at hand. For phy"
2020.sigdial-1.30,D08-1100,0,0.0268695,"o this collaborative negotiation setting over multiple days in our corpus, goal information is spread throughout the dialogue. Motivated by these complexities, we decided to annotate our data for two types of information: (1) the SMART goal attributes in the dialogues to track patients’ goals, and (2) different stages and phases that model the conversation flow in HC dialogues. For our domain, SMART goal attributes are the slotvalues pertaining to a patient’s goal. Stages and phases are more abstract, but otherwise analogous to tasks and sub-tasks as defined in task-oriented dialogue systems (Chotimongkol and Rudnicky, 2008). We believe the SMART annotation schema that we designed can be applied to any task where SMART goal setting is being used and not just physical activity. Similarly, the stages-phases annotation schema can be used to model the flow of any collaborative decision making counseling dialogue. In this paper, we will discuss the two rounds of data collection process, the subsequent analysis of the dialogues, which includes developing schemas and annotating the data, and application of models trained on these annotations. Our contributions can be summarized as follows: • We describe the data collect"
2020.sigdial-1.30,L18-1631,0,0.166677,"viewing (MI) based counseling interviews from public sources such as YouTube and built models to predict the overall counseling quality using linguistic features. Before the YouTube data, the authors also worked on data collected in clinical settings, graduate student training and such, but didn’t release it due to privacy reasons (P´erez-Rosas et al., 2016). The authors used the well established Motivational Interviewing Treatment Integrity (MITI) coding system to annotate the data and score how well or poorly a clinician used MI (Moyers et al., 2016). The MITI coding system was also used by Guntakandla and Nielsen (2018) to annotate reflections in the health behavior change therapy conversations. Since MI based interventions focus on understanding patient’s attitudes towards the problem and persuading them to change, the MITI coding system supports assessing clinicians based on how well they bring forth patient’s experiences, cultivate change talk, provide education, persuade them through logical arguments, and such. However, specific goal setting is not the main focus of these interviews and is rarely discussed. SMART Goal Setting • Specific (S): Create a clear goal that is as specific as possible and focuse"
2020.sigdial-1.30,W14-4337,0,0.026391,"s such as the Dialogue State Tracking Challenge (DSTC) started in 2013 to provide a common testbed for different tasks related to domainspecific dialogue systems such as dialogue state 247 2 Unfortunately, the activity data collected via Fitbit cannot be shared, since consent did not include permission for such data; dataset 1 cannot be shared, because of lack of consent. tracking, dialogue act prediction, and response generation; labeled datasets for each of these tasks were provided (Williams et al., 2013). However, most of these datasets focused on traveling and restaurant booking domains (Henderson et al., 2014). Moreover, for data collection, predefined scenarios are given to the users and thus, the users’ responses are not as spontaneous as they would be in a real-life situation (Asri et al., 2017; Budzianowski et al., 2018). Unfortunately, there are no such publicly available datasets for dialogue systems in the health domain. review tasks, assess, counseling, assign task, preclosing, and closing. Conversely, our stages-phases schema looks at the fine-grained decomposition of review-tasks, counseling, and assign task, which Bickmore et al. (2011) did not do. As far as we know, no other work models"
2020.sigdial-1.30,W16-0305,0,0.135042,"Missing"
2020.sigdial-1.30,L18-1591,0,0.0626665,"Missing"
2020.sigdial-1.30,N18-1202,0,0.0296177,"Missing"
2020.sigdial-1.30,W13-4065,0,0.0183773,"range of actions that are found in human-human or humanmachine conversations in the given domain. Initiatives such as the Dialogue State Tracking Challenge (DSTC) started in 2013 to provide a common testbed for different tasks related to domainspecific dialogue systems such as dialogue state 247 2 Unfortunately, the activity data collected via Fitbit cannot be shared, since consent did not include permission for such data; dataset 1 cannot be shared, because of lack of consent. tracking, dialogue act prediction, and response generation; labeled datasets for each of these tasks were provided (Williams et al., 2013). However, most of these datasets focused on traveling and restaurant booking domains (Henderson et al., 2014). Moreover, for data collection, predefined scenarios are given to the users and thus, the users’ responses are not as spontaneous as they would be in a real-life situation (Asri et al., 2017; Budzianowski et al., 2018). Unfortunately, there are no such publicly available datasets for dialogue systems in the health domain. review tasks, assess, counseling, assign task, preclosing, and closing. Conversely, our stages-phases schema looks at the fine-grained decomposition of review-tasks,"
2021.sigdial-1.31,2020.lrec-1.74,1,0.842962,"Missing"
2021.sigdial-1.31,C18-1300,0,0.0561156,"-2, the international ISO standard for DA annotations (Bunt et al., 2010). It provides a domain- and task-independent DA 277 1 Many studies show Fitbit can help increase physical activity (Ringeval et al., 2020), but here we are interested in approaches with dialogue capabilities. schema with 56 DAs organized into nine dimensions. Paul et al. (2019) proposed a universal DA schema by aligning tags from different datasets such as the Dialogue State Tracking Challenge 2 (Henderson et al., 2014), Google Simulated Dialogue (Shah et al., 2018), and MultiWOZ 2.0 (Budzianowski et al., 2018) together. Mezza et al. (2018) reduced the ISO schema to 10 DAs and showed their applicability to datasets like Switchboard (Leech and Weisser, 2003), MapTask (Anderson et al., 1991), and VerbMobil (Alexandersson et al., 1998). On account of not reinventing the wheel, we used the ISO schema for our dialogues (Bunt et al., 2017a). Since many of the DAs didn’t apply to our dataset such as turn take/grab, stalling, and pausing, we reduced the schema to only 12 DAs, mostly following Mezza et al. (2018). Early work for DA modeling involved treating the task as a structured prediction or text classification problem. Stolcke et a"
2021.sigdial-1.31,P02-1040,0,0.109493,"ve dip in performance for the repetition attribute. Therefore, we adopt the goal extraction pipeline that uses both dialogue acts (BERT) and phases as shown in Figure 2. Given the small performance difference on time and location between phases and DAs, to process messages in real-time, we will use only the SMART+DA (BERT) model, as it only requires the dialogue history. Additionally, to generate messages in real-time, the current Goal-c could be used. E.g., if location is null in Goal-c, the coach can ask for location next. We previously showed in Gupta et al. (2020b) that metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are not appropriate for our extraction-based goal summaries as they are sensitive to exact word match (Reiter, 2018). That is, if a given word, say ‘two’, is classified as days number instead of distance, they will still output a high score as ‘two’ is in the reference summary. BLEU also favors shorter sentences, so missing attributes lead to a higher score. Human Evaluation Evaluating models with automatic metrics is important, but it is equally important to evaluate the usefulness and usability of these models with their users. We performed a pilot evaluation with the"
2021.sigdial-1.31,W16-0305,0,0.0692564,"Missing"
2021.sigdial-1.31,N18-1202,0,0.0244406,"st: 22/5 patients). We experimented with both sequential and nonsequential classifiers such as CRF, Structured Perceptron (SP) (Collins, 2002), Logistic Regression (LR) (Grimm and Yarnold, 1995), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Decision Trees (DT) (Quinlan, 1986). For features, we tried different combinations of - the current word, left and right context words, part-ofspeech (POS) tags, left and right context words’ POS tags, SpaCy named entity recognizer (NER), current word’s phase, and ELMo word embeddings Figure 3: Dialogue acts distribution (15 weeks of data) (Peters et al., 2018). The CRF, SP, and LR models performed the best without a significant difference between them using the current and context words, ELMo embeddings, and SpaCy NER. We decided to use the CRF model with an F1 macro score of 0.81. In our previous work (Gupta et al., 2020b), we used models with word2vec embeddings (Mikolov et al., 2013) but found ELMo embeddings to perform better. 4 The phase and SMART attribute models are described in Gupta et al. (2020b) and briefly summarized here and in the appendix. 280 4.2 Modeling Dialogue Acts For DA prediction, we annotated 15 weeks (377 messages, 655 utte"
2021.sigdial-1.31,J18-3002,0,0.0135433,"as shown in Figure 2. Given the small performance difference on time and location between phases and DAs, to process messages in real-time, we will use only the SMART+DA (BERT) model, as it only requires the dialogue history. Additionally, to generate messages in real-time, the current Goal-c could be used. E.g., if location is null in Goal-c, the coach can ask for location next. We previously showed in Gupta et al. (2020b) that metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are not appropriate for our extraction-based goal summaries as they are sensitive to exact word match (Reiter, 2018). That is, if a given word, say ‘two’, is classified as days number instead of distance, they will still output a high score as ‘two’ is in the reference summary. BLEU also favors shorter sentences, so missing attributes lead to a higher score. Human Evaluation Evaluating models with automatic metrics is important, but it is equally important to evaluate the usefulness and usability of these models with their users. We performed a pilot evaluation with the help of three health coaches to answer two main questions: (1) What is the health coaches’ understanding of a correct goal summary? and (2)"
2021.sigdial-1.31,N18-3006,1,0.809605,"in different domains. One such effort led to the formation of the ISO 24617-2, the international ISO standard for DA annotations (Bunt et al., 2010). It provides a domain- and task-independent DA 277 1 Many studies show Fitbit can help increase physical activity (Ringeval et al., 2020), but here we are interested in approaches with dialogue capabilities. schema with 56 DAs organized into nine dimensions. Paul et al. (2019) proposed a universal DA schema by aligning tags from different datasets such as the Dialogue State Tracking Challenge 2 (Henderson et al., 2014), Google Simulated Dialogue (Shah et al., 2018), and MultiWOZ 2.0 (Budzianowski et al., 2018) together. Mezza et al. (2018) reduced the ISO schema to 10 DAs and showed their applicability to datasets like Switchboard (Leech and Weisser, 2003), MapTask (Anderson et al., 1991), and VerbMobil (Alexandersson et al., 1998). On account of not reinventing the wheel, we used the ISO schema for our dialogues (Bunt et al., 2017a). Since many of the DAs didn’t apply to our dataset such as turn take/grab, stalling, and pausing, we reduced the schema to only 12 DAs, mostly following Mezza et al. (2018). Early work for DA modeling involved treating the"
2021.sigdial-1.31,2020.emnlp-main.425,0,0.0307855,"rs from applying state-of-the-art deep learning techniques and end-to-end approaches for building dialogue agents that require large datasets. Researchers like Althoff et al. (2016) and Zhang and Danescu-Niculescu-Mizil (2020) were able to access a large counseling conversations dataset from the Crisis Text Line (CTL), a free 24/7 crisis counseling platform for a mental health crisis, for computational analysis through a fellowship program with CTL. Online sources such as Reddit have also been used for analyzing empathy in conversations, but consist of question-answer pairs and not dialogues (Sharma et al., 2020). Lastly, Shen et al. (2020) used the MI dataset collected by P´erezRosas et al. (2016) to build a model that can generate sample responses of type reflection to assist counselors. As far as we know, no existing work has focused on building a dialogue agent involving coaching components such as negotiation and feedback for promoting PA using SMART goal setting. Dialogue act (DA) modeling. This task involves finding the intent behind the speaker’s utterance in a dialogue such as request, clarification, and acknowledgment. The DA tags may differ depending on the dialogue’s domain. E.g., negotiat"
2021.sigdial-1.31,2020.sigdial-1.2,0,0.0792036,"Missing"
2021.sigdial-1.31,J00-3003,0,0.433466,"Missing"
2021.sigdial-1.31,2020.emnlp-main.66,0,0.0815742,"ied to the task (Kumar et al., 2020; Anikina and Kruijff-Korbayova, 2019). Convolutional Neural Networks (CNN) were also used for intent classification of a query (Hashemi et al., 2016). However, queries can be treated as individual sentences without any context. Given context is important in a dialogue, we experiment with approaches that can take dialogue history into account such as Conditional Random Fields (CRF) (Lafferty et al., 2001) and recent transformer-based BERT (Bidirectional Encoder Representations from Transformers) models (Devlin et al., 2019). In particular, we use the work by Wu et al. (2020) and Cohan et al. (2019) as the guide for our BERT-based DA prediction models. 3 Datasets and Annotations No health coaching dialogue dataset is publicly available. Therefore, to understand the feasibility of using SMS for health coaching, the challenges with patient recruitment and retention, and conversation flow between the coaches and the patients, we collected two health coaching datasets (Dataset 1 and Dataset 2; Dataset 2 is available upon request2 , while Dataset 1 cannot be shared due to lack of subject consent). To collect Dataset 1, we hired one health coach who coached 28 patients,"
2021.sigdial-1.31,2020.acl-main.470,0,0.027333,"to reflect on their PA performance with a series of follow-up questions, however, no goal-setting is involved.1 Interactions in these dialogue agents are still mostly scripted. Dynamic interactions require large datasets that are unfortunately scarce in the health domain due to privacy reasons. Moreover, collecting and labeling data particularly in real scenarios is resource intensive. This limits the researchers from applying state-of-the-art deep learning techniques and end-to-end approaches for building dialogue agents that require large datasets. Researchers like Althoff et al. (2016) and Zhang and Danescu-Niculescu-Mizil (2020) were able to access a large counseling conversations dataset from the Crisis Text Line (CTL), a free 24/7 crisis counseling platform for a mental health crisis, for computational analysis through a fellowship program with CTL. Online sources such as Reddit have also been used for analyzing empathy in conversations, but consist of question-answer pairs and not dialogues (Sharma et al., 2020). Lastly, Shen et al. (2020) used the MI dataset collected by P´erezRosas et al. (2016) to build a model that can generate sample responses of type reflection to assist counselors. As far as we know, no exi"
C04-1202,W01-0100,0,0.0606338,"cs.uic.edu Abstract In this paper, we consider the automatic text summarization as a challenging task of machine learning. We proposed a novel summarization system architecture which employs Gene Expression Programming technique as its learning mechanism. The preliminary experimental results have shown that our prototype system outperforms the baseline systems. 1 Introduction Automatic text summarization has been studied for decades (Edmundson 1969) and is still a very active area (Salton et al. 1994; Kupiec et al. 1995; Brandow et al. 1995; Lin 1999; Aone et al. 1999; Sekine and Nobata 2001; Mani 2001; McKeown et al. 2001; Radev et al. 2003). Only a few have tried using machine learning to accomplish this difficult task (Lin 1999; Aone et al. 1999; Neto et al. 2002). Most research falls into combining statistical methods with linguistic analysis. We regard the summarization as a problem of empowering a machine to learn from human-summarized text documents. We employ an evolutionary algorithm, Gene Expression Programming (GEP) (Ferreira 2001), as the learning mechanism in our Adaptive Text Summarization (ATS) system to learn sentence ranking functions. Even though our system generates extra"
C04-1202,P03-1048,0,0.118679,"r, we consider the automatic text summarization as a challenging task of machine learning. We proposed a novel summarization system architecture which employs Gene Expression Programming technique as its learning mechanism. The preliminary experimental results have shown that our prototype system outperforms the baseline systems. 1 Introduction Automatic text summarization has been studied for decades (Edmundson 1969) and is still a very active area (Salton et al. 1994; Kupiec et al. 1995; Brandow et al. 1995; Lin 1999; Aone et al. 1999; Sekine and Nobata 2001; Mani 2001; McKeown et al. 2001; Radev et al. 2003). Only a few have tried using machine learning to accomplish this difficult task (Lin 1999; Aone et al. 1999; Neto et al. 2002). Most research falls into combining statistical methods with linguistic analysis. We regard the summarization as a problem of empowering a machine to learn from human-summarized text documents. We employ an evolutionary algorithm, Gene Expression Programming (GEP) (Ferreira 2001), as the learning mechanism in our Adaptive Text Summarization (ATS) system to learn sentence ranking functions. Even though our system generates extractive summaries, the sentence ranking fun"
C04-1202,C96-2166,0,0.224615,"ich is decided by the required number of words in a summary. Or it can be a specified percentage of the total number of sentences in the document. will be returned as the summary of that document and presented in their nature order. In the testing stage, a different document set is supplied to test the similarity between the machine summarized text and the human or other system summarized text. 3 System Architecture In addition to the traditional way of extracting the highest ranked sentences in a document to compose a summary as in (Edmundson 1969; Lin 1999; Kupiec et al. 1995; Brandow 1995; Zechner 1996), we embedded a machine learning mechanism in our system. The system architecture is shown in Figure 1 where the GEP module is highlighted. In the training stage, each of the training documents is passed to the GEP module after being preprocessed into a set of sentence feature vectors. The GEP runs m generations, and in each generation a population of p sentence scoring functions in the form of chromosomes in GEP is generated. Every candidate scoring function is then applied to sentence feature vectors from every training document and produces a score accordingly. Then all sentences in the sam"
C86-1081,P83-1007,0,0.0994932,"Missing"
C86-1081,P83-1009,0,0.0319962,"ce, even if not in a precise way. Another problem with determiners is their inherent a~bigulty. In this paper we propose a logical formaliem, which, among other things, is suitable fc~ representing determiners without forcing a particular interpretation when their meaning is still not clear. INTRODUCTION Ambiguity of determiners is one of the most striking phenomena of natural language; what is strange is the case with ~hich humans use them: it seems that the molteplicity of interpretations of a ncun phrase including a determiner is not explicitly perceived by human users of rmtural language [Hobbs 1983]. The approach we chose tries to model this behavier: each determiner has a charac~ teristic semantic interpretation, which is different from that of ot]qer determiners and which can be furtherly specified an the basis of the information contents gathered from the overall ean~ text and frfxn the remsining part of the sentence. If such an information contents is rot sufficient, then the meaning of the determiner remains ambiguous. What is of paramount i~rtance is that any determiner has a ""single"" meaning, that can be furtherly specified by the context. Of course, we need to express the se~snt"
C86-1081,P85-1008,0,0.0473516,"Missing"
C86-1081,P84-1114,1,0.886382,"Missing"
C86-1081,J82-1003,0,\N,Missing
C86-1081,P85-1023,1,\N,Missing
C90-2047,P87-1022,0,\N,Missing
C90-2068,H89-2010,1,0.866125,"Missing"
C90-2068,J88-2003,0,0.530404,"of the hill), it is not the intended termination of the action in the context of these instructions. Its intended termination is the point at which the action of &quot;taking the first right&quot; commences - that is, when the agent recognizes that s/he has reached the first right. In Section 3, we will provide many more examples of this feature of instructions. 2. I n s t r u c t i o n s m a y describe a range o f behavior appropriate u n d e r different circumstances. The agent is 2This is not the case in &quot;Simon Says&quot; type instructions, where each action description contains a n intrinsic culmination [6]. 2 Figure h Control Panel Animation o,dy meant to do that which s/he recognizes the situation as demanding during its performance. For example, the following are part of instructions for installing a diverter spout: face side; if you're using a power saw, saw from the back side. Otherwise you'll produce ragged edges on the face because a handsaw cuts down and a power saw cuts up. Diverter spout is provided with insert for 1/2&quot; pipe threads. If supply pipe is larger (3/4&quot;), unscrew insert and use spout without it. Such cases as these illustrate an indirect relation between instructions and beh"
C92-4181,P92-1016,1,0.682851,"Missing"
C92-4181,C90-2068,1,0.824336,"Missing"
C92-4181,C92-1040,1,0.866602,"Missing"
C96-1059,J89-4002,0,0.0177041,"work on preventative expressions, and in particular on negative imperatives, has been done. This lack of interest in the two coinmunities has been in some sense complementary. In semantics and pragmatics, negation has been extensively studied (cf. Itorn (1989)). hnperarives, on the other hand, have not (for a notahle exception, see Davies (1986)). In computational linguistics, on the other hand, positive imperatives have been extensively investigated, both from the point of view of interpretation (Vere and Bickmore, 1990; Alterman et al., 1991; Chapman, 1991; Di Eugenio, 1993) and generation (Mellish and Evans, 1989; McKeown et al., 1990; Paris et al., 1995; Vander Linden and Martin, 1995). Little work, however, has been (tirected at negative imt)eratives. (for exceptions see the work of Vere and Bickmore (1990) in interpretation and of Ansari (1995) in generation). 3 A Priori Hypotheses Di Eugenio (1993) lint forward the following hypothesis concerning the realization of preventative expressions. In this discussion, S refers to the instructor (speaker / writer) who is referred to with feminine pronouns, and H to the agent (hearer / reader), referred to with masculine t)ronouns: in the waste stock on the"
C96-1059,J96-2004,0,0.0166752,"Missing"
C96-1059,W96-0402,1,0.871118,"Missing"
C96-1059,J95-1002,1,\N,Missing
C96-1060,J94-2003,0,\N,Missing
C96-1060,C90-2047,1,\N,Missing
C96-1060,J95-2003,0,\N,Missing
C96-1060,P87-1022,0,\N,Missing
C96-1061,P92-1025,0,0.0612146,"Missing"
C96-1061,1995.tmi-1.13,1,0.858993,"Missing"
C96-1061,J95-3001,0,\N,Missing
C96-1061,P95-1016,0,\N,Missing
C96-1061,J81-4001,0,\N,Missing
C96-1061,P95-1005,1,\N,Missing
C96-1061,H93-1041,0,\N,Missing
C96-1061,1993.tmi-1.16,0,\N,Missing
C96-1061,J93-1002,0,\N,Missing
C98-1051,J96-2004,0,0.0906387,"Missing"
di-eugenio-2000-usage,P99-1032,0,\N,Missing
di-eugenio-2000-usage,J96-2004,0,\N,Missing
di-eugenio-2000-usage,P94-1001,0,\N,Missing
di-eugenio-etal-2002-binomial,W98-1428,0,\N,Missing
di-eugenio-etal-2002-binomial,P00-1020,0,\N,Missing
di-eugenio-etal-2002-binomial,P01-1057,0,\N,Missing
di-eugenio-etal-2002-binomial,P98-2199,0,\N,Missing
di-eugenio-etal-2002-binomial,C98-2194,0,\N,Missing
fossati-di-eugenio-2008-saw,J93-2004,0,\N,Missing
fossati-di-eugenio-2008-saw,A97-1025,0,\N,Missing
fossati-di-eugenio-2008-saw,N03-1033,0,\N,Missing
fossati-di-eugenio-2008-saw,J06-3001,0,\N,Missing
fossati-di-eugenio-2008-saw,P96-1010,0,\N,Missing
fossati-di-eugenio-2008-saw,J06-1003,0,\N,Missing
fossati-di-eugenio-2008-saw,P94-1013,0,\N,Missing
fossati-di-eugenio-2008-saw,W96-0108,0,\N,Missing
J00-2009,W90-0108,0,0.0388448,"nd speculates on a few directions for future research. At first I found Chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms. However, at a second, more-careful, reading, everything falls into place. The resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993). The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG, the Upper Model (Bateman 1990). Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system. Chapter 4 presents the domain model built by means of the description logic system LOOM (MacGregor and Burstein 1991). Stede is specifically interested in verbalizations of situations, to use his own neutral term. Thus, the domain model contains a representation for categories such as states, a"
J04-1005,J96-2004,0,0.0580055,"coders to make subtle distinctions among categories. The objectivity of these decisions can be assessed by evaluating the reliability of the tagging, namely, whether the coders reach a satisfying level of agreement when they perform the same coding task. Currently, the de facto standard for assessing intercoder agreement is the κ coefficient, which factors out expected agreement (Cohen 1960; Krippendorff 1980). κ had long been used in content analysis and medicine (e.g., in psychiatry to assess how well students’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981). Carletta (1996) deserves the credit for bringing κ to the attention of computational linguists. − P(E) κ is computed as P(A) 1 − P(E) , where P(A) is the observed agreement among the coders, and P(E) is the expected agreement, that is, P(E) represents the probability that the coders agree by chance. The values of κ are constrained to the interval [−1, 1]. A κ value of one means perfect agreement, a κ value of zero means that agreement is equal to chance, and a κ value of negative one means “perfect” disagreement. This squib addresses two issues that have been neglected in the computational linguistics litera"
J04-1005,J97-1002,0,0.0468359,"Missing"
J04-1005,di-eugenio-2000-usage,1,0.790767,"Missing"
J04-1005,P99-1032,0,0.0413839,"Missing"
J04-3003,P98-1013,0,0.0128841,"Missing"
J04-3003,P87-1022,0,0.941844,"Missing"
J04-3003,P98-2241,0,0.104716,"setting the theory’s parameters, a systematic comparison can be made only by computational means. A corpus-based evaluation has other advantages, as well, among which is that it is perhaps the best way to identify the aspects of the theory that need to be further specified, and the factors such as temporal coherence or stylistic variation that may interact with the preferences expressed by centering. (Also, knowing the extent to which real texts conform to centering preferences is an important goal in its own right.) In previous corpus-based studies of centering (Walker 1989; Passonneau 1993; Byron and Stent 1998; Di Eugenio 1998; Kameyama 1998; Strube and Hahn 1999; Tetreault 2001), only a few instantiations of centering were compared. The present study is more systematic in that it considers a greater number of parameters, as well as more parameter instantiations, including “crossing” instantiations in which the parameters are set according to proposals due to different researchers. Only re310 Poesio et al. Centering: A Parametric Theory liable annotation techniques were used; we produced an annotation manual that can be used to extend our analysis to other data, as well as a companion Web site (htt"
J04-3003,J96-2004,0,0.0771215,", so the hypotheses about coherence formulated in centering are likely to play an important part in the way these texts are constructed. 3.3 Annotation The previous corpus-based investigations of centering theory we are aware of (Walker 1989; Passonneau 1993, 1998; Byron and Stent 1998; Di Eugenio 1998; Hurewitz 1998; Kameyama 1998; Strube and Hahn 1999) were all carried out by a single annotator annotating her or his corpus according to her or his own subjective judgment. One of our goals was to use for this study only information that could be annotated reliably (Passonneau and Litman 1993; Carletta 1996), as we believe this will make our results easier to replicate. The price we paid to achieve replicability is that we couldn’t test all proposals about the computation of centering parameters proposed in the literature, especially those about segmentation and about ranking, as discussed below. The annotation followed a detailed manual, available on the companion Web site. Eight paid annotators were involved in the reliability studies and the annotation. In the following we briefly discuss the information that we were able to annotate, what we didn’t annotate, and the problems we encountered; f"
J04-3003,P97-1011,1,0.751061,"Missing"
J04-3003,P83-1007,0,0.589485,"Missing"
J04-3003,J95-2003,0,0.981377,"Missing"
J04-3003,J86-3001,0,0.509906,", U.K. E-mail: poesio@essex.ac.uk. † Department of Psychology, University of Durham, U.K. ‡ Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607-7053, USA. E-mail: bdieugen@cs.uic.edu § MITRE Corporation, 202 Burlington Road, Bedford, MA 01730-1428, USA. E-mail:hitze@mitre.org. Submission received: 16 April 2002; Revised submission received: 3 September 2003; Accepted for publication: 11 December 2003 c 2004 Association for Computational Linguistics  Computational Linguistics Volume 30, Number 3 of attention and coherence in discourse (Grosz 1977; Sidner 1979; Grosz and Sidner 1986) concerned with local coherence and salience, that is, coherence and salience within a discourse segment. A fundamental characteristic of centering is that it is better viewed as a linguistic theory than a computational one. By this we mean that its primary aim is to make cross-linguistically valid claims about which discourses are easier to process, abstracting away from specific algorithms for anaphora resolution or anaphora generation (although many such algorithms are based on the theory). The result is a very different theory from those one usually finds in computational linguistics. In c"
J04-3003,C00-1045,1,0.906405,"Missing"
J04-3003,P98-1090,1,0.899002,"Missing"
J04-3003,P86-1031,0,0.651811,"d to the RET transition, which is preferred to the Smooth Shift transition, which is preferred to the Rough Shift transition. This formulation of Rule 2 depends on a further distinction between two types of SHIFT: Smooth Shift (SSH), when CB(Un ) = CP(Un ), and Rough-Shift (RSH), when CB(Un ) = CP(Un ). Transitions can then be classified along two dimensions, as in the following table: CB(Un ) = CB(Un−1 ) or CB(Un−1 ) = NIL CB(Un ) = CB(Un−1 ) CB(Un ) = CP(Un ) Continue Smooth Shift CB(Un ) = CP(Un ) Retain Rough Shift Further refinements of these classification schemes have been proposed. Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. Bruno / He often taunted Tommy, the second sentence would be read more slowly when Bruno was used than when He was used. 315 Computational Linguistics Volume 30, Number 3 that establish a CB after an utterance without one, such as the first utterance of a segment. Walker, Iida, and Cote (1994) argued that these utterances should be classified as Center Continuations, the idea being that even the first utterance of a segment does have a CB, but this CB is initially un"
J04-3003,P93-1010,1,0.772367,"Missing"
J04-3003,J01-4007,0,0.352925,"ecific definition, even for English. Similarly undefined is the notion of “pronominalization” governed by Rule 1. But without further specification of these concepts it is impossible to evaluate the claims above, just as it is not possible to evaluate the predictions of, say, “government and binding theory” without providing an explicit definition of “command” or “argument”. As a result, a considerable amount of research has been concerned with establishing the best specification for what are, essentially, parameters of the theory. We briefly review some of these proposals in this section.7 6 Kibble (2001) proposed a version of Rule 2 that further develops the “decompositional” view of Rule 2 introduced by Brennan et al., while simultaneously incorporating Strube and Hahn’s intuition that “cheap” transitions should be preferred. Kibble formulates his version of Rule 2 as a series of preferences: for transitions that preserve the CB—that is, those such that CB(Un ) = CB(Un−1 ) (he calls these transitions cohesive), that identify CB(Un ) with CP(Un ), and/or that are cheap. Code to test an earlier version of Kibble’s form of Rule 2 (Kibble 2000) has been incorporated in the scripts discussed late"
J04-3003,J94-4002,0,0.243633,"rred over (sequences of) shifts. 2.3.1 Constraint 1, Topic Uniqueness, and Entity Coherence. If we view the CB as a formalization of the idea of “topic” (Vallduvi 1990; Gundel 1998; Hurewitz 1988; Miltsakaki 1999; Beaver 2004), Constraint 1 expresses, first and foremost, the original claim from Joshi and Kuhn (1979) and Joshi and Weinstein (1981) that discourses with exactly one (or no more than one) “topic” at each point are easier to process. This view contrasts both with Sidner’s (1979) hypothesis that utterances may have two “topics” and with theories such as Givon (1983), Alshawi (1987), Lappin and Leass (1994) and Arnold (1998), which view “topichood” as a matter of degree and therefore allow for an arbitrary number of topics. In the strong form just presented, Constraint 1 is also a claim about local coherence. It expresses a preference for discourses to be entity coherent: to continue talking about the same entities. Each utterance in a segment should realize at least one of the discourse entities realized in the previous utterance. A weaker form of Constraint 1 has also been suggested (e.g., Walker, Joshi, and Prince 1998a, footnote 2, page 3); the preference for a unique CB is preserved, but no"
J04-3003,J02-3003,0,0.172029,"in German. Conversely, before taking the evidence for a slight advantage of STRUBE-HAHN ranking over grammatical-function ranking as conclusive, one needs to supplement our studies with psychological experiments reconciling these results with numerous results indicating the important role played by grammatical function, especially subjecthood (among others, Hudson, Tanenhaus, and Dell [1986]; Gordon, Grosz, and Gillion [1993]; Brennan [1995]). Information structure has also been found not to be appropriate for languages including Greek, Hindi, and Turkish (Turan 1998; Prasad and Strube 2000; Miltsakaki 2002). Similar considerations apply to the definition of previous utterance, since we saw that a considerable amount of psychological evidence supports treating adjuncts as embedded, at least when the syntactically embedded clause is at the end of the sentence (Cooreman and Sanford 1996; Pearson, Stevenson, and Poesio 2000). In the case of the definition of utterance, our results indicate that identifying utterances with sentences, rather than finite clauses, leads to results much more consistent with the claimed preference for discourses to be entity-coherent. While this result is likely to be use"
J04-3003,J96-3006,0,0.0194465,"values of agreement. 325 Computational Linguistics Volume 30, Number 3 matched by any other CF in the same sentence. We tested only heuristic methods as well, using the layout structure of the texts as a rough indicator of discourse structure. In this article we discuss only the results with the heuristic proposed by Walker. In the extended technical report available on the Web site, we discuss the results with other segmentation heuristics, as well as further results with the tutorial dialogues subdomain of the GNOME corpus, independently annotated according to relational discourse analysis (Moser and Moore 1996), a technique inspired by Grosz and Sidner’s proposals, from which a Grosz and Sidner–like segmentation was extracted as proposed in Poesio and Di Eugenio (2001). 3.4 Automatic Computation of Centering Information The annotated corpus is used by Perl scripts that automatically compute the centering data structures (utterances, CFs, and CBs) according to the particular parameter instantiation chosen, find violations of Constraint 1, Rule 1, and Rule 2 (according to several versions of Rule 1 and Rule 2), and evaluate the claims using the statistical tests. The behavior of the scripts is control"
J04-3003,poesio-2000-annotating,1,0.700552,"dner (1986). According to Kameyama, only a few types of clauses, such as the complements of certain verbs, are embedded. For example, Kameyama proposes to break up (4) into utterances as follows, and to treat each of these utterances, including subordinate clauses such as (U2) or (U5), as an update: (4) (u1) Her entrance in Scene 2 Act 1 brought some disconcerting applause (u2) even before she had sung a note. (u3) Thereafter the audience waxed applause happy (u4) but discriminating operagoers reserved judgment (u5) as her singing showed signs of strain. Experiments by Pearson, Stevenson, and Poesio (2000) confirmed that CFs introduced in main clauses are significantly more likely to be subsequently mentioned than CFs introduced in complement clauses. However, a semicontrolled study by Suri and McCoy (1994) suggested that other types of clauses—specifically, adjunct clauses headed by after and before–are also “embedded,” not “permanent updates” as suggested by Kameyama; these results were subsequently confirmed by Cooreman and Sanford (1996). The status of other types of clauses is less clear. Kameyama (1998) also proposed a tentative analysis of relative clauses, according to which they are te"
J04-3003,W99-0309,0,0.104221,"Missing"
J04-3003,J98-2001,1,0.829069,"h that it can be used to identify the intentional structure of texts—which, according to Grosz and Sidner, determines their segmentation. As a result, only preliminary attempts at annotating texts according to Grosz and Sidner’s theory have been made. For this reason, most previous corpus-based studies of centering either ignored segmentation or used heuristics such as those proposed by Walker (1989): Consider every paragraph as a separate discourse segment, except when its first sentence contains a pronoun in subject position or a pronoun whose agreement features are not 18 In previous work (Poesio and Vieira 1998) we came to the conclusion that kappa, while appropriate when the number of categories is fixed and relatively small, is problematic for anaphoric reference, when neither condition applies, and may result in inflated values of agreement. 325 Computational Linguistics Volume 30, Number 3 matched by any other CF in the same sentence. We tested only heuristic methods as well, using the layout structure of the texts as a rough indicator of discourse structure. In this article we discuss only the results with the heuristic proposed by Walker. In the extended technical report available on the Web si"
J04-3003,W98-1427,0,0.0785317,"Missing"
J04-3003,P98-2204,0,0.0291145,"clauses headed by after and before–are also “embedded,” not “permanent updates” as suggested by Kameyama; these results were subsequently confirmed by Cooreman and Sanford (1996). The status of other types of clauses is less clear. Kameyama (1998) also proposed a tentative analysis of relative clauses, according to which they are temporarily treated as utterances and update the local focus but are then merged with the embedding clause; she didn’t, however provide empirical support for this hypothesis. Other types of subordinate clauses and parentheticals are not discussed in this literature. Strube (1998) and Miltsakaki (1999) question Kameyama’s identification of utterances with (tensed) clauses. Miltsakaki (1999) argues, on the basis of data from English and Greek, that the local focus is updated only after every sentence and that only the CFs in the main clause are considered when establishing the CB. 2.4.2 Realization. Grosz, Joshi, and Weinstein (1995) simply say that what it means for utterance U to realize center C depends on the particular semantic theory one adopts. They consider two ways in which a discourse entity may be “realized” in an utterance as required by Constraint 2. Direct"
J04-3003,J99-3001,0,0.0635668,"ntering argue that while these concepts play a central role in any theory of discourse coherence and salience, their precise characterization is best left for subsequent research, indeed, that some of these concepts (e.g., ranking) might be defined in a different way for each language (Walker, Iida, and Cote 1994). In other words, these notions should be viewed as parameters of centering. This feature of the theory has inspired a great deal of research attempting to specify centering’s parameters for different languages (Kameyama 1985; Walker, Iida, and Cote 1994; Di Eugenio 1998; Turan 1998; Strube and Hahn 1999). Competing versions of the central definitions and claims of the theory have also been proposed: For example, different definitions of backward-looking center (CB) can be found in Grosz, Joshi, and Weinstein (1983, 1995) and Gordon, Grosz, and Gillion (1993). As a result, a researcher wishing to test the predictions of centering, or to use it for practical applications, is confronted with a large number of possible instantiations of the theory. The main goal of the work reported in this article was to explore the space of parameter configurations, measuring the impact of different ways of set"
J04-3003,J94-2006,0,0.146961,"Missing"
J04-3003,J01-4003,0,0.385979,"computational means. A corpus-based evaluation has other advantages, as well, among which is that it is perhaps the best way to identify the aspects of the theory that need to be further specified, and the factors such as temporal coherence or stylistic variation that may interact with the preferences expressed by centering. (Also, knowing the extent to which real texts conform to centering preferences is an important goal in its own right.) In previous corpus-based studies of centering (Walker 1989; Passonneau 1993; Byron and Stent 1998; Di Eugenio 1998; Kameyama 1998; Strube and Hahn 1999; Tetreault 2001), only a few instantiations of centering were compared. The present study is more systematic in that it considers a greater number of parameters, as well as more parameter instantiations, including “crossing” instantiations in which the parameters are set according to proposals due to different researchers. Only re310 Poesio et al. Centering: A Parametric Theory liable annotation techniques were used; we produced an annotation manual that can be used to extend our analysis to other data, as well as a companion Web site (http://cswww.essex.ac.uk/staff/poesio/cbc/) to allow readers to try out in"
J04-3003,J00-4005,0,0.0611502,"Missing"
J04-3003,P89-1031,0,0.341262,"us number of possible ways of setting the theory’s parameters, a systematic comparison can be made only by computational means. A corpus-based evaluation has other advantages, as well, among which is that it is perhaps the best way to identify the aspects of the theory that need to be further specified, and the factors such as temporal coherence or stylistic variation that may interact with the preferences expressed by centering. (Also, knowing the extent to which real texts conform to centering preferences is an important goal in its own right.) In previous corpus-based studies of centering (Walker 1989; Passonneau 1993; Byron and Stent 1998; Di Eugenio 1998; Kameyama 1998; Strube and Hahn 1999; Tetreault 2001), only a few instantiations of centering were compared. The present study is more systematic in that it considers a greater number of parameters, as well as more parameter instantiations, including “crossing” instantiations in which the parameters are set according to proposals due to different researchers. Only re310 Poesio et al. Centering: A Parametric Theory liable annotation techniques were used; we produced an annotation manual that can be used to extend our analysis to other dat"
J04-3003,J94-2003,0,0.0518391,"Missing"
J04-3003,P00-1051,0,\N,Missing
J04-3003,P05-1018,0,\N,Missing
J04-3003,C98-1013,0,\N,Missing
J04-3003,C98-2236,0,\N,Missing
J04-3003,P97-1014,0,\N,Missing
J04-3003,J88-2006,0,\N,Missing
J04-3003,J92-4007,0,\N,Missing
J04-3003,W00-1411,0,\N,Missing
J04-3003,P93-1020,0,\N,Missing
J04-3003,J97-3006,0,\N,Missing
J04-3003,C98-1087,1,\N,Missing
J04-3003,N01-1002,1,\N,Missing
J04-3003,C98-2199,0,\N,Missing
J04-3003,W04-2327,1,\N,Missing
J04-3003,W03-2605,1,\N,Missing
J04-3003,C69-7001,0,\N,Missing
J04-3003,C69-6902,0,\N,Missing
J04-3003,W02-2111,0,\N,Missing
J04-3003,J96-2005,0,\N,Missing
J98-3001,P97-1011,1,0.853938,"Missing"
J98-3001,W96-0501,0,0.0270557,"and graphics and attempt to combine these in sensible ways. We predict that the World Wide Web will be a major factor in forcing some of the issues here: if systems are to automatically generate the text on Web pages (see, for example, Milosavljevic and Dale [1996]), then they also need to consider other elements of that container. Reusable resources. It may be an indication of a maturing of some subareas of NLG research that we are now in a position where there are reusable components for particular tasks. Specifically, three linguistic realization packages, FUF/SURGE (Elhadad 1993a, 1993b; Elhadad and Robin 1996), PENMAN/NIGEL (Penman group 1989), and its descendant KPML/NIGEL (Bateman 1997), are widely used in the field. For anything other than simple applications, it is now questionable whether it makes sense to build a linguistic realization component from scratch. We may expect other kinds of reusable components to be developed within the research community within the next 5-10 years; it is developments of this kind that signal significant progress, since being able to reuse the work of others obviously has the potential to increase research productivity. In related developments, there is a growin"
J98-3001,J97-1004,0,0.120327,"ial to increase research productivity. In related developments, there is a growing interest within the community in defining a reference architecture for NLG; if successful, this is likely to stimulate further research and development in NLG through the provision of a modular baseline for development, comparison, and evaluation. Evaluation. Although there have been attempts at the evaluation of NLG techniques and systems in the past, formal evaluation has only recently come to the fore. For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &quot;task efficacy&quot; measures (Young 1997). The major stumbling block for progress is determining what metrics and methods should be used: for example, how can the quality of an output text be measured? Because of the different nature of the task, it is unlikely that methods that have been used in NLU, such as the evaluation process adopted in the Message Understanding Conferences, can be carried over to"
J98-3001,W96-0417,1,0.887119,"Missing"
J98-3001,J97-1007,0,0.0299135,"ning a reference architecture for NLG; if successful, this is likely to stimulate further research and development in NLG through the provision of a modular baseline for development, comparison, and evaluation. Evaluation. Although there have been attempts at the evaluation of NLG techniques and systems in the past, formal evaluation has only recently come to the fore. For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &quot;task efficacy&quot; measures (Young 1997). The major stumbling block for progress is determining what metrics and methods should be used: for example, how can the quality of an output text be measured? Because of the different nature of the task, it is unlikely that methods that have been used in NLU, such as the evaluation process adopted in the Message Understanding Conferences, can be carried over to generation. Dale and Mellish (1998) suggest that the NLG community could make progress by devising specific evaluation"
N03-2032,P95-1016,0,0.0886122,"n DIAG, our own corpus of tutoring dialogues, and on the CallHome Spanish corpus. Our work has the theoretical goal of assessing whether LSA, an approach based only on raw text, can be improved by using additional features of the text. 1 Introduction Dialogue systems need to perform dialog act classification, in order to understand the role the user’s utterance plays in the dialog (e.g., a question for information or a request to perform an action), and to generate an appropriate next turn. In recent years, a variety of empirical techniques have been used to train the dialogue act classifier (Reithinger and Maier, 1995; Stolcke et al., 2000; Walker et al., 2001). In this paper, we propose Latent Semantic Analysis (LSA) as a method to train the dialogue act classifier. LSA can be thought as representing the meaning of a word as a kind of average of the meanings of all the passages in which it appears, and the meaning of a passage as a kind of average of the meaning of all the words it contains (Landauer et al., 1998). LSA learns from cooccurrence of words in collections of texts. It builds a semantic space where words and passages are represented as vectors. Their similarity is measured by the cosine of thei"
N03-2032,J00-3003,0,0.296711,"Missing"
N03-2032,P01-1066,0,0.015389,"on the CallHome Spanish corpus. Our work has the theoretical goal of assessing whether LSA, an approach based only on raw text, can be improved by using additional features of the text. 1 Introduction Dialogue systems need to perform dialog act classification, in order to understand the role the user’s utterance plays in the dialog (e.g., a question for information or a request to perform an action), and to generate an appropriate next turn. In recent years, a variety of empirical techniques have been used to train the dialogue act classifier (Reithinger and Maier, 1995; Stolcke et al., 2000; Walker et al., 2001). In this paper, we propose Latent Semantic Analysis (LSA) as a method to train the dialogue act classifier. LSA can be thought as representing the meaning of a word as a kind of average of the meanings of all the passages in which it appears, and the meaning of a passage as a kind of average of the meaning of all the words it contains (Landauer et al., 1998). LSA learns from cooccurrence of words in collections of texts. It builds a semantic space where words and passages are represented as vectors. Their similarity is measured by the cosine of their contained angle in the semantic space. LSA"
N03-2032,J97-1002,0,0.0973072,"Missing"
N03-2034,W00-2021,0,0.17529,"ructional text annotated with lexical semantics information. We have coupled the parser LCFLEX with a lexicon and ontology derived from two lexical resources, VerbNet for verbs and CoreLex for nouns. We discuss how we built our lexicon and ontology, and the parsing results we obtained. 1 Introduction This paper discusses the lexicon and ontology we built and coupled with the parser LCFLEX (Ros´e and Lavie, 2000), in order to automatically build a corpus of instructional text annotated with lexical semantics information. The lexicon and ontology are derived from two lexical resources: VerbNet (Kipper et al., 2000a) for verbs and CoreLex (Buitelaar, 1998) for nouns. We also report the excellent parsing results we obtained. Our ultimate goal is to develop a (semi)automatic method to derive domain knowledge from instructional text, in the form of linguistically motivated action schemes. To develop this acquisition engine, our approach calls for an instructional corpus where verbs are annotated with their semantic representation, and where relations such as precondition and effect between the actions denoted by those verbs are marked. Whereas the action relation annotation will be manual, the semantic ann"
N03-2034,J91-4003,0,0.164957,"evin’s work and accounts for 960 distinct verbs classified into 72 main classes. Moreover, given VerbNet strong syntactic components, it can be easily coupled with a parser and used to automatically generate a semantically annotated corpus. Of course, when building a representation for a sentence, we need semantics not only for verbs, but also for nouns. Whereas many NL applications use WordNet (Fellbaum, 1998), we were in need of a richer lexicon. We found CoreLex (Buitelaar, 1998) appropriate for our needs. CoreLex is based on a different theory than Levin’s (that of the generative lexicon (Pustejovsky, 1991)), but does provide a compatible decompositional meaning representation for nouns. The contribution of our work is to demonstrate that a meaning representation based on decompositional lexical semantics can be derived efficiently and effectively. We believe there is no other work that attaches a semantics of this type to a parser for a large coverage corpus. VerbNet has been coupled with the TAG formalism (Kipper et al., 2000b), but no parsing results are available. More( :morphology :syntax :semantics position (*or* ((cat n) (root position) (agr 3s) (semtag (*or* lap1 lap2))) ((cat vlex) (roo"
N09-1064,W05-0613,0,0.328616,"ns labeling their inner nodes is still a challenging and mostly unsolved problem in NLP. It is linguistically plausible that such structures are determined at least in part on the basis of the meaning of the related chunks of texts, and of the rhetorical intentions of their authors. However, such knowledge is extremely difficult to capture. Hence, previous work on discourse parsing (Wellner et. al., 2006; Sporleder and Lascarides, 2005; Marcu, 2000; Polanyi et. al., 2004; Soricut and Marcu, 2003; ∗ This work was done while the author was a student at the University of Illinois at Chicago. 566 Baldridge and Lascarides, 2005) has relied only on syntactic and lexical information, lexical chains and shallow semantics. We present an innovative discourse parser that uses compositional semantics (when available) and information on the structure of the segment being built itself. Our discourse parser, based on a modified shift-reduce algorithm, crucially uses a rhetorical relation classifier to determine the site of attachment of a new incoming chunk together with the appropriate relation label. Another novel aspect of our work is the usage of Inductive Logic Programming (ILP): ILP learns from first-order logic represen"
N09-1064,J95-4004,0,0.0376654,"Missing"
N09-1064,W04-2322,0,0.0179531,"Missing"
N09-1064,prasad-etal-2008-penn,0,0.0681927,"Missing"
N09-1064,N03-1030,0,0.769933,"e structure, as that shown in Figure 1. Discourse parsing, namely, deriving such tree structures and the rhetorical relations labeling their inner nodes is still a challenging and mostly unsolved problem in NLP. It is linguistically plausible that such structures are determined at least in part on the basis of the meaning of the related chunks of texts, and of the rhetorical intentions of their authors. However, such knowledge is extremely difficult to capture. Hence, previous work on discourse parsing (Wellner et. al., 2006; Sporleder and Lascarides, 2005; Marcu, 2000; Polanyi et. al., 2004; Soricut and Marcu, 2003; ∗ This work was done while the author was a student at the University of Illinois at Chicago. 566 Baldridge and Lascarides, 2005) has relied only on syntactic and lexical information, lexical chains and shallow semantics. We present an innovative discourse parser that uses compositional semantics (when available) and information on the structure of the segment being built itself. Our discourse parser, based on a modified shift-reduce algorithm, crucially uses a rhetorical relation classifier to determine the site of attachment of a new incoming chunk together with the appropriate relation la"
N09-1064,subba-etal-2006-building,1,0.807181,"Missing"
N09-1064,W06-1317,0,0.208422,"Missing"
N09-1064,J05-2005,0,0.108391,"results in FOL rules that are linguistically perspicuous. Our domain is that of instructional how-to-do manuals, and we describe our corpus in Section 2. In Section 3, we discuss the modified shift-reduce parser we developed. The bulk of the paper is devoted to the rhetorical relation classifier in Section 4. Experimental results of both the relation classifier and the discourse parser in its entirety are discussed in Section 5. Further details can be found in (Subba, 2008). 2 Discourse Annotated Instructional Corpus Existing corpora annotated with rhetorical relations (Carlson et. al., 2003; Wolf and Gibson, 2005; Prasad et. al., 2008) focus primarily on news articles. However, for us the development of the discourse parser is parasitic on our ultimate goal: developing resources and algorithms for language inHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 566–574, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics s1e1-s5e2 dddddZ Z Z Z Z Z Z Z Z Z ddddddd d d d d d d Z Z Z Z ddddd general:specific Z Z Z Z ddddddd Z ddddddd s1e1 s2e1-s5e2 ggggWWWWWWWWW ggggg WWWWW g g g g WWWWW preparation:act ggggg g WWWW g g g g s2e1"
N09-1064,C04-1020,0,\N,Missing
N09-1064,W01-1605,0,\N,Missing
N09-1064,J86-3001,0,\N,Missing
N10-2005,J98-3002,0,0.114275,"Missing"
N10-2005,W09-2109,1,0.270257,"Missing"
N10-2005,J96-2004,0,\N,Missing
N10-2005,P90-1010,0,\N,Missing
N12-1058,W11-2035,1,0.859456,"Missing"
N12-1058,W08-1301,0,0.0269645,"Missing"
N12-1058,N06-2010,0,0.104976,"er describes our ongoing work on resolving third person pronouns and deictic words in a multi-modal corpus. We show that about two thirds of these referring expressions have antecedents that are introduced by pointing gestures or by haptic-ostensive actions (actions that involve manipulating an object). After describing our annotation scheme, we discuss the co-reference models we learn from multi-modal features. The usage of hapticostensive actions in a co-reference model is a novel contribution of our work. 2 1 Introduction Co-reference resolution has received a lot of attention. However, as Eisenstein and Davis (2006) noted, most research on co-reference resolution has focused on written text. This task is much more difficult in dialogue, especially in multi-modal dialogue contexts. First, utterances are informal, ungrammatical and disfluent. Second, people spontaneously use gestures and other body language. As noticed by Kehler (2000), Goldin-Meadow (2003), and Chen et al. (2011), in a multi-modal corpus, the antecedents of referring expressions are often introduced via gestures. Whereas the role played by pointing gestures in referring has been studied, the same is not true for other types of gestures. I"
N12-1058,P03-1022,0,0.0972915,"Missing"
N18-4011,W16-6604,1,0.831854,"Missing"
N18-4011,W07-1007,0,0.0450518,"aries or corpora are difficult to understand (Ong et al., 2007; Kandula et al., 2010). These methods are unreliable because none of the currently available vocabularies are exhaustive. For providing explanations to terms that are identified as difficult, Horn et al. (2014) and Biran et al. (2011) use the replacement that was provided to terms from Wikipedia in the Simple Wikipedia parallel corpus. Elhadad (2006) supplements the selected terminologies with definitions obtained from Google “define”. In medical domain, some work has been done in obtaining pairs of medical terms and explanations: Elhadad and Sutaria (2007) prepare pairs of complex medical terms by using a parallel corpus of abstracts of clinical studies and corresponding news stories; Stilo et al. (2013) map medical jargon and everyday language by searching for their occurrence in Wikipedia and Google snippets. Our simplification metric is similar to that of (Shardlow, 2013) but we use five times as many features and a different approach for distinguishing between Simple and Complex terms. We provide definitions to terms similar to Ramesh et al. (2013), but we are not restricted to single word terms only. Unlike Klavans and Muresan (2000), we r"
N18-4011,P11-2087,0,0.0202482,"ent Research Workshop, pages 74–82 c New Orleans, Louisiana, June 2 - 4, 2018. 2017 Association for Computational Linguistics documentation contain different type of content (free text vs concepts). As concerns identifying terms that are Complex, some applications assume that all the terms that appear in specific vocabularies or corpora are difficult to understand (Ong et al., 2007; Kandula et al., 2010). These methods are unreliable because none of the currently available vocabularies are exhaustive. For providing explanations to terms that are identified as difficult, Horn et al. (2014) and Biran et al. (2011) use the replacement that was provided to terms from Wikipedia in the Simple Wikipedia parallel corpus. Elhadad (2006) supplements the selected terminologies with definitions obtained from Google “define”. In medical domain, some work has been done in obtaining pairs of medical terms and explanations: Elhadad and Sutaria (2007) prepare pairs of complex medical terms by using a parallel corpus of abstracts of clinical studies and corresponding news stories; Stilo et al. (2013) map medical jargon and everyday language by searching for their occurrence in Wikipedia and Google snippets. Our simpli"
N18-4011,W09-0613,0,0.0237993,"Missing"
N18-4011,P14-2075,0,0.0135032,"of NAACL-HLT 2018: Student Research Workshop, pages 74–82 c New Orleans, Louisiana, June 2 - 4, 2018. 2017 Association for Computational Linguistics documentation contain different type of content (free text vs concepts). As concerns identifying terms that are Complex, some applications assume that all the terms that appear in specific vocabularies or corpora are difficult to understand (Ong et al., 2007; Kandula et al., 2010). These methods are unreliable because none of the currently available vocabularies are exhaustive. For providing explanations to terms that are identified as difficult, Horn et al. (2014) and Biran et al. (2011) use the replacement that was provided to terms from Wikipedia in the Simple Wikipedia parallel corpus. Elhadad (2006) supplements the selected terminologies with definitions obtained from Google “define”. In medical domain, some work has been done in obtaining pairs of medical terms and explanations: Elhadad and Sutaria (2007) prepare pairs of complex medical terms by using a parallel corpus of abstracts of clinical studies and corresponding news stories; Stilo et al. (2013) map medical jargon and everyday language by searching for their occurrence in Wikipedia and Goo"
N18-4011,P13-3015,0,0.0214017,"to terms from Wikipedia in the Simple Wikipedia parallel corpus. Elhadad (2006) supplements the selected terminologies with definitions obtained from Google “define”. In medical domain, some work has been done in obtaining pairs of medical terms and explanations: Elhadad and Sutaria (2007) prepare pairs of complex medical terms by using a parallel corpus of abstracts of clinical studies and corresponding news stories; Stilo et al. (2013) map medical jargon and everyday language by searching for their occurrence in Wikipedia and Google snippets. Our simplification metric is similar to that of (Shardlow, 2013) but we use five times as many features and a different approach for distinguishing between Simple and Complex terms. We provide definitions to terms similar to Ramesh et al. (2013), but we are not restricted to single word terms only. Unlike Klavans and Muresan (2000), we refer to multiple knowledge sources for definitions of medical concepts. Attending: Dr. PHYSICIAN. Admission diagnosis: acute subcortical CVA. Secondary diagnosis: hypertension. Discharge diagnosis: Right sided weakness of unknown etiology. Consultations:. Physical Medicine Rehabilitation. Physical Therapy. Occupational Ther"
N18-4011,R13-1084,0,0.0409002,"Missing"
N18-4011,W11-2803,0,0.0318795,"sue) that guide our personalization process has not been explored before. There are several existing systems that produce personalized content in biomedical domain (Jimison et al., 1992; DiMarco et al., 1995) as well as in non-medical domains (Paris, 1988; Moraes et al., 2014). However, only a few of the existing biomedical systems generate personalized content for the patients (Buchanan et al., 1995; Williams et al., 2007). PERSIVAL system takes in a natural language query and provides customized summaries of medical literature for patients or doctors (Elhadad et al., 2005). BabyTalk system (Mahamood and Reiter, 2011) generates customized descriptions of patient status for people occupying different roles in Neonatal Intensive Care Unit. However, this system relies on handcrafted ontologies, which are very time intensive to create. Our approach to personalization uses several parameters that determine the content to be included in our summary, similarly to the PERSONAGE system (Mairesse and Walker, 2011), a parameteriz3 Dataset Our dataset consists of the doctor’s discharge note and shift-by-shift update of the nursing care plan for 60 patients. Discharge note is an unstructured plain text document that us"
N18-4011,W07-2328,0,0.0143908,"s for patients. Moreover, the combination of the four different factors (patient health literacy, motivation to self-care, strengths and concerns, and the patient’s familiarity with the health issue) that guide our personalization process has not been explored before. There are several existing systems that produce personalized content in biomedical domain (Jimison et al., 1992; DiMarco et al., 1995) as well as in non-medical domains (Paris, 1988; Moraes et al., 2014). However, only a few of the existing biomedical systems generate personalized content for the patients (Buchanan et al., 1995; Williams et al., 2007). PERSIVAL system takes in a natural language query and provides customized summaries of medical literature for patients or doctors (Elhadad et al., 2005). BabyTalk system (Mahamood and Reiter, 2011) generates customized descriptions of patient status for people occupying different roles in Neonatal Intensive Care Unit. However, this system relies on handcrafted ontologies, which are very time intensive to create. Our approach to personalization uses several parameters that determine the content to be included in our summary, similarly to the PERSONAGE system (Mairesse and Walker, 2011), a par"
N18-4011,J11-3002,0,0.0353548,"Missing"
N18-4011,W14-4409,0,0.0150208,"nt recommendations. To the best of our knowledge, there are no existing systems that generate comprehensible and personalized hospital-stay summaries for patients. Moreover, the combination of the four different factors (patient health literacy, motivation to self-care, strengths and concerns, and the patient’s familiarity with the health issue) that guide our personalization process has not been explored before. There are several existing systems that produce personalized content in biomedical domain (Jimison et al., 1992; DiMarco et al., 1995) as well as in non-medical domains (Paris, 1988; Moraes et al., 2014). However, only a few of the existing biomedical systems generate personalized content for the patients (Buchanan et al., 1995; Williams et al., 2007). PERSIVAL system takes in a natural language query and provides customized summaries of medical literature for patients or doctors (Elhadad et al., 2005). BabyTalk system (Mahamood and Reiter, 2011) generates customized descriptions of patient status for people occupying different roles in Neonatal Intensive Care Unit. However, this system relies on handcrafted ontologies, which are very time intensive to create. Our approach to personalization"
N18-4011,J88-3006,0,0.784093,"ates restaurant recommendations. To the best of our knowledge, there are no existing systems that generate comprehensible and personalized hospital-stay summaries for patients. Moreover, the combination of the four different factors (patient health literacy, motivation to self-care, strengths and concerns, and the patient’s familiarity with the health issue) that guide our personalization process has not been explored before. There are several existing systems that produce personalized content in biomedical domain (Jimison et al., 1992; DiMarco et al., 1995) as well as in non-medical domains (Paris, 1988; Moraes et al., 2014). However, only a few of the existing biomedical systems generate personalized content for the patients (Buchanan et al., 1995; Williams et al., 2007). PERSIVAL system takes in a natural language query and provides customized summaries of medical literature for patients or doctors (Elhadad et al., 2005). BabyTalk system (Mahamood and Reiter, 2011) generates customized descriptions of patient status for people occupying different roles in Neonatal Intensive Care Unit. However, this system relies on handcrafted ontologies, which are very time intensive to create. Our approa"
P04-1088,J97-1002,0,0.0358132,"κ coefficient (Krippendorff, 1980; Carletta, 1996) to assess the agreement between the classification made by FLSA and the classification from the corpora — see Table 8. A general rule of thumb on how to interpret the values of κ (Krippendorff, 1980) is to require a value of κ ≥ 0.8, with 0.67 < κ < 0.8 allowing tentative conclusions to be drawn. As a whole, Table 8 shows that FLSA achieves a satisfying level of agreement with human coders. To put Table 8 in perspective, note that expert human coders achieved κ = 0.83 on DA classification for MapTask, but also had available the speech source (Carletta et al., 1997). We also compared the confusion matrix from (Carletta et al., 1997) with the confusion matrix we obtained for our best result on MapTask (FLSA using Game + Speaker). For humans, the largest sources of confusion are between: check and queryyn; instruct and clarify; and acknowledge, reply-y and ready. Likewise, our FLSA method makes the most mistakes when distinguishing between instruct and clarify; and acknowledge, reply-y, and ready. Instead it performs better than humans on distinguishing check and query-yn. Thus, most of the sources of confusion for humans are the same as for FLSA. Future w"
P04-1088,J96-2004,0,0.0499821,"stem can reliably recognize the end of a game, the method just described needs to be used only for the first DA of each game. Then, the game label that gives the best result becomes the game label used for the next few DAs, until the end of the current game is detected. Another reason why we advocate FLSA over other approaches is that it appears to be close to human performance for DA classification, in the same way that LSA approximates well many aspects of human competence / performance (Landauer and Dumais, 1997). To support this claim, first, we used the κ coefficient (Krippendorff, 1980; Carletta, 1996) to assess the agreement between the classification made by FLSA and the classification from the corpora — see Table 8. A general rule of thumb on how to interpret the values of κ (Krippendorff, 1980) is to require a value of κ ≥ 0.8, with 0.67 < κ < 0.8 allowing tentative conclusions to be drawn. As a whole, Table 8 shows that FLSA achieves a satisfying level of agreement with human coders. To put Table 8 in perspective, note that expert human coders achieved κ = 0.83 on DA classification for MapTask, but also had available the speech source (Carletta et al., 1997). We also compared the confu"
P04-1088,W02-0205,1,0.881727,"Missing"
P04-1088,W03-0208,0,0.0129299,"Missing"
P04-1088,P98-2188,0,0.0391413,"Missing"
P04-1088,J00-3003,0,0.540194,"Missing"
P04-1088,C98-2183,0,\N,Missing
P05-1007,P00-1020,0,0.1007,"stem as well. We found that the generator which intuitively produces the best language does engender the most learning. Specifically, it appears that functional aggregation is responsible for the improvement. 1 Introduction The work we present in this paper addresses three issues: evaluation of Natural Language Generation (NLG) systems, the place of aggregation in NLG, and NL interfaces for Intelligent Tutoring Systems. NLG systems have been evaluated in various ways, such as via task efficacy measures, i.e., measuring how well the users of the system perform on the task at hand (Young, 1999; Carenini and Moore, 2000; Reiter et al., 2003). We also employed task efficacy, as we evaluated the learning that occurs in students interacting with an Intelligent Tutoring System (ITS) enhanced with NLG capabilities. We focused on sentence planning, and specifically, on aggregation. We developed two different feedback generation engines, that we systematically evaluated in a three way comparison that included the original system as well. Our work is novel for NLG evaluation in that we focus on one specific component of the NLG process, aggregation. Aggregation pertains to combining two or more of the messages to be"
P05-1007,C96-1043,0,0.0245562,"s guesswork at best. Our data collection attempts at providing a more solid empirical base for aggregation rules; we found that tutors exclude significant amounts of factual information, and use high degrees of aggregation based on functionality. As a consequence, while part of our rules implement standard types of aggregation, such as conjunction via shared participants, we also introduced functional aggregation (see conceptual aggregation (Reape and Mellish, 1998)). As regards evaluation, NLG systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures, i.e., measuring how well the users so, and the distribution of topics and of evaluations is too broad to be telling. 56 of the system perform on the task at hand (Young, 1999; Carenini and Moore, 2000; Reiter et al., 2003). The latter kind of studies generally contrast different interventions, i.e. a baseline that does not use NLG and one or more variations obtained by parameterizing the NLG system. However, the evaluation do"
P05-1007,W02-2116,1,0.637603,"Missing"
P05-1007,A97-1039,0,0.0287819,"which generate The combustion is abnormal and The water pump sound is OK in Figs. 2 and 3). 2.3 Feedback Generation in DIAG-NLP2 In DIAG-NLP1 the fact file provided by DIAG is directly processed by EXEMPLARS. In contrast, in DIAG-NLP2 a planning module manipulates the information before passing it to EXEMPLARS. This module decides which information to include according to the type of query the system is responding to, and produces one or more Sentence Structure objects. These are then passed to EXEMPLARS that transforms them into Deep Syntactic Structures. Then, a sentence realizer, RealPro (Lavoie and Rambow, 1997), transforms them into English sentences. Figs. 5 and 6 show the control flow in DIAGNLP2 for feedback generation for ConsultInd and ConsultRU. Step 3a in Fig. 5 chooses, among all the RUs that DIAG would talk about, only those that would definitely result in the observed symptom. Step 2 in the AGGREGATE procedure in Fig. 5 uses a simple heuristic to decide whether and how to use functional aggregation. For each RU, its possible aggregators and the number n of units it covers are listed in a table (e.g., electrical devices covers 4 RUs, ignitor, photoelectric cell, transformer and burner motor"
P05-1007,J97-1004,0,0.0807169,"at best. Our data collection attempts at providing a more solid empirical base for aggregation rules; we found that tutors exclude significant amounts of factual information, and use high degrees of aggregation based on functionality. As a consequence, while part of our rules implement standard types of aggregation, such as conjunction via shared participants, we also introduced functional aggregation (see conceptual aggregation (Reape and Mellish, 1998)). As regards evaluation, NLG systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures, i.e., measuring how well the users so, and the distribution of topics and of evaluations is too broad to be telling. 56 of the system perform on the task at hand (Young, 1999; Carenini and Moore, 2000; Reiter et al., 2003). The latter kind of studies generally contrast different interventions, i.e. a baseline that does not use NLG and one or more variations obtained by parameterizing the NLG system. However, the evaluation does not focus on a specifi"
P05-1007,C02-1015,0,0.354396,"mbining two or more of the messages to be communicated into one sentence (Reiter and Dale, 2000). Whereas it is considered an essential task of an NLG system, its specific contributions to the effectiveness of the text that is eventually produced have rarely been assessed (Harvey and Carberry, 1998). We found that syntactic aggregation does not improve learning, but that what we call functional aggregation does. Further, we ran a controlled data collection in order to provide a more solid empirical base for aggregation rules than what is normally found in the literature, e.g. (Dalianis, 1996; Shaw, 2002). As regards NL interfaces for ITSs, research on the next generation of ITSs (Evens et al., 1993; Litman et al., 2004; Graesser et al., 2005) explores NL as one of the keys to bridging the gap between current ITSs and human tutors. However, it is still not known whether the NL interaction between students and an ITS does in fact improve learning. We are among the first to show that this is the case. We will first discuss DIAG, the ITS shell we are using, and the two feedback generators that we developed, DIAG-NLP1 and DIAG-NLP2 . Since the latter is based on a corpus study, we will briefly des"
P05-1007,P98-1084,0,0.0615126,"pecifically, on aggregation. We developed two different feedback generation engines, that we systematically evaluated in a three way comparison that included the original system as well. Our work is novel for NLG evaluation in that we focus on one specific component of the NLG process, aggregation. Aggregation pertains to combining two or more of the messages to be communicated into one sentence (Reiter and Dale, 2000). Whereas it is considered an essential task of an NLG system, its specific contributions to the effectiveness of the text that is eventually produced have rarely been assessed (Harvey and Carberry, 1998). We found that syntactic aggregation does not improve learning, but that what we call functional aggregation does. Further, we ran a controlled data collection in order to provide a more solid empirical base for aggregation rules than what is normally found in the literature, e.g. (Dalianis, 1996; Shaw, 2002). As regards NL interfaces for ITSs, research on the next generation of ITSs (Evens et al., 1993; Litman et al., 2004; Graesser et al., 2005) explores NL as one of the keys to bridging the gap between current ITSs and human tutors. However, it is still not known whether the NL interaction"
P05-1007,W98-1428,0,0.016319,"h cases it is still DIAG that performs content determination, and provides to DIAG-NLP1 and DIAG-NLP2 a file in which the facts to be communicated are written – a fact is the basic unit of information that underlies each of the clauses in a reply by DIAG-orig. The only way we altered the interaction between student and system is the actual language that is presented in the output window. In DIAG-NLP1 we mostly explored using syntactic aggregation to improve the feedback, whereas DIAG-NLP2 is corpus-based and focuses on functional aggregation. In both DIAG-NLP1 and DIAGNLP2 , we use EXEMPLARS (White and Caldwell, 1998), an object-oriented, rule-based generator. The rules (called exemplars) are meant to capture an exemplary way of achieving a communicative goal in a given context. EXEMPLARS selects rules by traversing the exemplar specialization hierarchy and evaluating the applicability conditions associated with each exemplar. The visual combustion check is igniting which is abnormal (normal is combusting). Oil Nozzle always produces this abnormality when it fails. Oil Supply Valve always produces this abnormality when it fails. Oil pump always produces this abnormality when it fails. Oil Filter always pro"
P05-1007,W02-2114,0,0.0206686,"significant, χ2 = 9.49, p &lt; 0.1). DIAG-NLP1 and DIAG-NLP2 receive the same number of preferences; however, a more detailed analysis (Table 4) shows that subjects prefer DIAG-NLP1 for feedback to ConsultInd, but DIAG-NLP2 for feedback to ConsultRu (marginally significant, χ2 = 5.6, p &lt; 0.1). Finally, subjects find DIAG-NLP2 more natural, but DIAG-NLP1 more contentful (Table 5, χ2 = 10.66, p &lt; 0.025). 4 Discussion and conclusions Our work touches on three issues: aggregation, evaluation of NLG systems, and the role of NL interfaces for ITSs. In much work on aggregation (Huang and Fiedler, 1996; Horacek, 2002), aggregation rules and heuristics are shown to be plausible, but are not based on any hard evidence. Even where corpus work is used (Dalianis, 1996; Harvey and Carberry, 1998; Shaw, 2002), the results are not completely convincing because we do not know for certain the content to be communicated from which these texts supposedly have been aggregated. Therefore, positing empirically based rules is guesswork at best. Our data collection attempts at providing a more solid empirical base for aggregation rules; we found that tutors exclude significant amounts of factual information, and use high d"
P05-1007,J97-1007,0,0.0179168,"that tutors exclude significant amounts of factual information, and use high degrees of aggregation based on functionality. As a consequence, while part of our rules implement standard types of aggregation, such as conjunction via shared participants, we also introduced functional aggregation (see conceptual aggregation (Reape and Mellish, 1998)). As regards evaluation, NLG systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures, i.e., measuring how well the users so, and the distribution of topics and of evaluations is too broad to be telling. 56 of the system perform on the task at hand (Young, 1999; Carenini and Moore, 2000; Reiter et al., 2003). The latter kind of studies generally contrast different interventions, i.e. a baseline that does not use NLG and one or more variations obtained by parameterizing the NLG system. However, the evaluation does not focus on a specific component of the NLG process, as we did here for aggregation. Regarding the role of NL interfaces for ITSs,"
P05-1007,W96-0403,0,0.379365,"thin the water pump and safety cutoff valve, the water pump sound indicator is normal. The following indicators never display abnormally when this unit fails. Within the fire door sight hole, the visual combustion check indicator is igniting. The water pump is a poor suspect since the water pump sound is ok. You have seen that the combustion is abnormal. Check the units along the path of the oil and the electrical devices. Figure 3: Answers to ConsultRu by DIAG-orig, DIAG-NLP1 and DIAG-NLP2 2.1 DIAG-NLP1 : Syntactic aggregation DIAG-NLP1 1 (i) introduces syntactic aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Reape and Mellish, 1998; Shaw, 2002) and what we call structural aggregation, namely, grouping parts according to the structure of the system; (ii) generates some referring expressions; (iii) models a few rhetorical relations; and (iv) improves the format of the output. The middle parts of Figs. 2 and 3 show the revised output produced by DIAG-NLP1 . E.g., in Fig. 2 the RUs of interest are grouped by the system modules that contain them (Oil Burner and Furnace System), and by the likelihood that a certain RU causes the observed symptoms. In contrast to the original answer, the revised answer"
P05-1007,C98-1081,0,\N,Missing
P10-1140,J05-3002,0,0.0405735,"s a summary of those comments for each individual track recommended to the user. In Section 5, we report two types of evaluation: an intrinsic evaluation of the extraction components, and of the coverage of the summary; an extrinsic evaluation via a between-subject study. We found that users make quicker and more informed decisions when presented with the song review summaries as opposed to the full album review. 2 Related Work Over the last decade, summarization has become a hot topic for research. Quite a few systems were developed for different tasks, including multidocument summarization (Barzilay and McKeown, 2005; Soubbotin and Soubbotin, 2005; Nastase, 2008). 1376 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1376–1385, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics What’s not to get? Yes, Maxwell, and Octopus are a bit silly! ... ..... “Something” and “Here Comes The Sun” are two of George’s best songs ever (and “Something” may be the single greatest love song ever). “Oh Darling” is a bluesy masterpiece with Paul screaming..... ....... “Come Together” contains a great riff, but he ended up getting sued over the lyri"
P10-1140,esuli-sebastiani-2006-sentiwordnet,0,0.00746295,"the feature term guitar. In Step 3, sentence a is sentence fx -si such that its LCN with all other sentences (b and c) contains only the feature term; hence, sentence a is left on its own. Note that Steps 2 and 3 ensure that, among all the possible LNCs between pair of sentences, we only consider the ones containing the feature in question. As concerns polarity grouping, different reviews may express different opinions regarding a particular feature. To generate a coherent summary that mentions conflicting opinions, we need to subdivide f -sentences according to polarity. We use SentiWordNet (Esuli and Sebastiani, 2006), an extension of WordNet where each sense of a word is augmented with the probability of that sense being positive, negative or neutral. The overall sentence score is based on the scores of the adjectives contained in the sentence. Since there are a number of senses for each word, an adjective ai in a sentence is scored as the normalized weighted scores of each sense of the adjective. For each ai , we compute three scores, positive, as shown in Formula 1, negative and obFigure 7: Polarity Calculation jective, which are computed analogously: f req1 ∗ pos1 + ... + f reqn ∗ posn (f req1 + .... +"
P10-1140,P94-1002,0,0.166223,"separately. 4.2 Feature Extraction Once the song titles are identified in the album review, sentences with song titles are used as anchors to (1) identify segments of texts that talk about a specific song, and then (2) extract the feature(s) that the pertinent text segment discusses. The first step roughly corresponds to identifying the flow of topics in a review. The second step corresponds to identifying the properties of each song. Both steps would greatly benefit from reference resolution, but current algorithms still have a low accuracy. We devised an approach that combines text tiling (Hearst, 1994) and domain heuristics. The text tiling algorithm divides the text into coherent discourse units, to describe the sub-topic structure of the given text. We found the relatively coarse segments the text tiling algorithm provides sufficient to identify different topics. An album review is first divided into segments using the text tiling algorithm. Let [seg1 , seg2 , ..., segk ] be the segments obtained. The segments that contain potential features of a song are identified using the following heuristics: Step 1: Include segi if it contains a song title. 1379 These segments are more likely to con"
P10-1140,P06-1034,0,0.0288474,"tudied as a combination of machine learning and NLP techniques (Hu and Liu, 2004; Gamon et al., 2005). For example, (Hu and Liu, 2004) use associative mining techniques to identify features that frequently occur in reviews taken from www.epinions.com and www. amazon.com. Then, features are paired to the nearest words that express some opinion on that feature. Most work on product reviews focuses on identifying sentences and polarity of opinion terms, not on generating a coherent summary from the extracted features, which is the main goal of our research. Exceptions are (Carenini et al., 2006; Higashinaka et al., 2006), whose focus was on extracting domain specific ontologies in order to structure summarization of customer reviews. Summarizing reviews on objects different from products, such as restaurants (Nguyen et al., 2007), or movies (Zhuang et al., 2006), has also been tackled, although not as extensively. We are aware of only one piece of work that focuses on music reviews (Downie and Hu, 2006). This study is mainly concerned with identifying descriptive patterns in positive or negative reviews but not on summarizing the reviews. 2.1 Summarizing song reviews is different As mentioned earlier, using a"
P10-1140,W00-1427,0,0.0382897,"corresponding to Figure 1 become the components of the new f -sentence. Next, we need to adjust their number and forms. This is a natural language generation task, specifically, sentence realization. We use YAG (McRoy et al., 2003), a template based sentence realizer. clause is the main template used to generate a sentence. Slots in a template can in turn be templates. The grammatical relationships obtained from the Typed Dependency Parser such as subject and object identify the slots and the template the slots follows; the words in the relationship fill the slot. We use a morphological tool (Minnen et al., 2000) to obtain the base form from the original verb or noun, so that YAG can generate grammatical sentences. Figure 5 shows the regenerated review from Figure 1. YAG regenerates as many f -sentences from the original sentence, as many features were contained in it. By the end of this step, for each feature f of a certain song si , we have generated a set of f -sentences. This set also contains every original sentence that only covered the single feature f . 4.4 Grouping f -sentences are further grouped, by sub-feature and by polarity. As concerns sub-feature grouping, consider the following f -sen"
P10-1140,D08-1080,0,0.0191544,"ommended to the user. In Section 5, we report two types of evaluation: an intrinsic evaluation of the extraction components, and of the coverage of the summary; an extrinsic evaluation via a between-subject study. We found that users make quicker and more informed decisions when presented with the song review summaries as opposed to the full album review. 2 Related Work Over the last decade, summarization has become a hot topic for research. Quite a few systems were developed for different tasks, including multidocument summarization (Barzilay and McKeown, 2005; Soubbotin and Soubbotin, 2005; Nastase, 2008). 1376 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1376–1385, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics What’s not to get? Yes, Maxwell, and Octopus are a bit silly! ... ..... “Something” and “Here Comes The Sun” are two of George’s best songs ever (and “Something” may be the single greatest love song ever). “Oh Darling” is a bluesy masterpiece with Paul screaming..... ....... “Come Together” contains a great riff, but he ended up getting sued over the lyrics by Chuck Berry...... Figure 1: A sample revi"
P10-1140,E06-1039,0,\N,Missing
P13-1027,W11-2311,0,0.0883228,"ar less existing research and material to draw on than for, say, ASL (American Sign Language) or BSL (British Sign Language). Our work was undertaken within the purview of the ATLAS project (Bertoldi et al., 2010; Lombardo et al., 2010; Lombardo et al., 2011; Prinetto et al., 2011; Mazzei, 2012; Ahmad et al., 2012), which developed a full pipeline for translating Italian into LIS. ATLAS is part of a recent crop of projects devoted to developing automatic translation from language L spoken in geographic area G into the sign language spoken in G (Dreuw et al., 2010; L´opez-Lude˜na et al., 2011; Almohimeed et al., 2011; Lu and Huenerfauth, 2012). Input is taken in the form of written Italian text, parsed, and converted into a semantic representation of its contents; from this semantic representation, LIS output is produced, using a custom serialization format called AEWLIS (which we will describe later). This representation is then augmented with space positioning information, and fed into a final renderer component that performs the signs using a virtual actor. ATLAS focused on a limited domain for which a bilingual Italian/LIS corWe present a corpus analysis of how Italian connectives are translated into"
P13-1027,D07-1119,0,0.021661,"Missing"
P13-1027,P03-1011,0,0.0488822,"volamento sulla 19_f08_2011-06-08_10_04_10.xml Anche sulla Sardegna qualche annuvolamento pomeridiano, possibilità di qualche breve scroscio di pioggia, ma tendenza poi a schiarite pomeriggio Sardegna area nuvola pure acquazzone potere ma poi nuvola diminuire Figure 4: Example of integrated syntax trees. tion of signs corresponding to words. Therefore, it is not sufficient to consider the LIS sentence and the Italian sentence separately. Instead, their syntax trees must be reconstructed and aligned. Tree alignment in a variety of forms has been extensively used in machine translation systems (Gildea, 2003; Eisner, 2003; May and Knight, 2007). As far as we know, we are the first to attempt the usage of tree alignment to aid in the translation between a spoken and a sign language, partly because corpora that include synctactic trees for sign language sentences hardly exist. (L´opez-Lude˜na et al., 2011) does use alignment techniques for translation from Spanish to Spanish Sign Language (SSL), but it is limited to alignment between words or phrases in Spanish, and glosses or sequences of glosses in SSL. an existing parser, and retrieves / builds a parse tree for the LIS sentence. The two trees ar"
P13-1027,braffort-etal-2010-sign,0,0.0208773,"Di Eugenio Department of Computer Science University of Illinois at Chicago bdieugen@uic.edu Abstract transmission modality (gestures and expressions instead of sounds) means that existing writing systems are not easily adaptable to them. The resulting lack of a shared written form does nothing to improve the availability of sign language corpora; bilingual corpora, which are of particular importance to a translation system, are especially rare. In fact, various projects around the world are trying to ameliorate this sad state of affairs for specific Sign Languages (Lu and Huenerfauth, 2010; Braffort et al., 2010; Morrissey et al., 2010). In this paper, we describe the work we performed as concerns the translation of connectives from the Italian language into LIS, the Italian Sign Language (Lingua Italiana dei Segni). Because the communities of signers in Italy are relatively small and fragmented, and the language has a relatively short history, there is far less existing research and material to draw on than for, say, ASL (American Sign Language) or BSL (British Sign Language). Our work was undertaken within the purview of the ATLAS project (Bertoldi et al., 2010; Lombardo et al., 2010; Lombardo et a"
P13-1027,W11-2309,0,0.0608775,"Missing"
P13-1027,P07-1003,0,0.0193991,"rds belonging to different subtrees is linked by a path that goes through the connective in the original tree. Of these words, we select the ones that have aligned signs, and then we compute the path between each pair of signs aligned to words belonging to different subtrees. This gives us a set of paths to consider in the LIS syntax tree. For example, let us consider the connective “di” between “possibilit`a” and “scroscio” in Figure 4. Word Alignment. Having obtained syntax trees for the two sentences, we then needed to align them. For this purpose we used the Berkeley Word Aligner (BWA) 1 (Denero, 2007), a general tool for aligning sentences in bilingual corpora. BWA takes as input a series of matching sentences in two different languages, trains multiple unsupervised alignment models, and selects the optimal result using a testing set of manual alignments. The output is a list of aligned word indices for each input sentence pair. On our data set, BWA performance is as follows: Precision = 0.736364, Recall = 0.704348, AER = 0.280000. Integration. The result is an integrated syntax tree representation of the Italian and LIS versions of the sentence, with arcs bridging aligned word/sign pairs."
P13-1027,W10-1312,0,0.0151712,"no clugar2@uic.edu Barbara Di Eugenio Department of Computer Science University of Illinois at Chicago bdieugen@uic.edu Abstract transmission modality (gestures and expressions instead of sounds) means that existing writing systems are not easily adaptable to them. The resulting lack of a shared written form does nothing to improve the availability of sign language corpora; bilingual corpora, which are of particular importance to a translation system, are especially rare. In fact, various projects around the world are trying to ameliorate this sad state of affairs for specific Sign Languages (Lu and Huenerfauth, 2010; Braffort et al., 2010; Morrissey et al., 2010). In this paper, we describe the work we performed as concerns the translation of connectives from the Italian language into LIS, the Italian Sign Language (Lingua Italiana dei Segni). Because the communities of signers in Italy are relatively small and fragmented, and the language has a relatively short history, there is far less existing research and material to draw on than for, say, ASL (American Sign Language) or BSL (British Sign Language). Our work was undertaken within the purview of the ATLAS project (Bertoldi et al., 2010; Lombardo et a"
P13-1027,W12-2909,0,0.0148092,"and material to draw on than for, say, ASL (American Sign Language) or BSL (British Sign Language). Our work was undertaken within the purview of the ATLAS project (Bertoldi et al., 2010; Lombardo et al., 2010; Lombardo et al., 2011; Prinetto et al., 2011; Mazzei, 2012; Ahmad et al., 2012), which developed a full pipeline for translating Italian into LIS. ATLAS is part of a recent crop of projects devoted to developing automatic translation from language L spoken in geographic area G into the sign language spoken in G (Dreuw et al., 2010; L´opez-Lude˜na et al., 2011; Almohimeed et al., 2011; Lu and Huenerfauth, 2012). Input is taken in the form of written Italian text, parsed, and converted into a semantic representation of its contents; from this semantic representation, LIS output is produced, using a custom serialization format called AEWLIS (which we will describe later). This representation is then augmented with space positioning information, and fed into a final renderer component that performs the signs using a virtual actor. ATLAS focused on a limited domain for which a bilingual Italian/LIS corWe present a corpus analysis of how Italian connectives are translated into LIS, the Italian Sign Langu"
P13-1027,D07-1038,0,0.0181975,"06-08_10_04_10.xml Anche sulla Sardegna qualche annuvolamento pomeridiano, possibilità di qualche breve scroscio di pioggia, ma tendenza poi a schiarite pomeriggio Sardegna area nuvola pure acquazzone potere ma poi nuvola diminuire Figure 4: Example of integrated syntax trees. tion of signs corresponding to words. Therefore, it is not sufficient to consider the LIS sentence and the Italian sentence separately. Instead, their syntax trees must be reconstructed and aligned. Tree alignment in a variety of forms has been extensively used in machine translation systems (Gildea, 2003; Eisner, 2003; May and Knight, 2007). As far as we know, we are the first to attempt the usage of tree alignment to aid in the translation between a spoken and a sign language, partly because corpora that include synctactic trees for sign language sentences hardly exist. (L´opez-Lude˜na et al., 2011) does use alignment techniques for translation from Spanish to Spanish Sign Language (SSL), but it is limited to alignment between words or phrases in Spanish, and glosses or sequences of glosses in SSL. an existing parser, and retrieves / builds a parse tree for the LIS sentence. The two trees are then aligned by exploiting the word"
P13-1027,W12-1517,0,0.157523,"s paper, we describe the work we performed as concerns the translation of connectives from the Italian language into LIS, the Italian Sign Language (Lingua Italiana dei Segni). Because the communities of signers in Italy are relatively small and fragmented, and the language has a relatively short history, there is far less existing research and material to draw on than for, say, ASL (American Sign Language) or BSL (British Sign Language). Our work was undertaken within the purview of the ATLAS project (Bertoldi et al., 2010; Lombardo et al., 2010; Lombardo et al., 2011; Prinetto et al., 2011; Mazzei, 2012; Ahmad et al., 2012), which developed a full pipeline for translating Italian into LIS. ATLAS is part of a recent crop of projects devoted to developing automatic translation from language L spoken in geographic area G into the sign language spoken in G (Dreuw et al., 2010; L´opez-Lude˜na et al., 2011; Almohimeed et al., 2011; Lu and Huenerfauth, 2012). Input is taken in the form of written Italian text, parsed, and converted into a semantic representation of its contents; from this semantic representation, LIS output is produced, using a custom serialization format called AEWLIS (which we wi"
P13-1027,P03-2041,0,\N,Missing
P91-1044,J88-2003,0,0.0298441,"gories: 1. Terminological k n o w l e d g e about an actiontype: its participants and its relation to other action-types that it either specializes or abstracts - e.g. slice specializes cut, loosen a screw carefully specializes loosen a screw. 2. N o n - t e r m i n o l o g i c a l knowledge. First of all, knowledge about the effects expected to occur 1V~ta.t executable m e a n s is d e b a t a b l e : s e e for e x a m p l e [12], p. 63ff. when an action of a given type is performed. Because effects may occur during the performance of an action, the basic aspectua] profile of the action-type [11] should also be included. Clearly, this knowledge is not terminological; in E x . 3 Turn the screw counterclockwise don&apos;t loosen it completely. but the modifier not ... completely does not affect the fact that don&apos;t loosen it completely is a loosening action: only its default culmination condition is affected. Also, non-terminological knowledge must include information about relations between action-types: temporal, generation, enablement, and testing, where by testing I refer to the relation between two actions, one of which is a test on the outcome or execution of the other. The generation r"
P91-1044,C90-2068,1,0.750363,"placing the plank anywhere between the two ladders: this shows that in a) the agent must be inferring the proper position for the plank from the expressed why ""to create a simple scaffoldL My concern is with representations that allow specification of both bow&apos;s and why&apos;s, and with reasoning that allows inferences such as the above to be made. In the rest of the paper, I will argue that a hybrid representation formalism is best suited for the knowledge I need to represent. 2 A hybrid a c t i o n representation formalism As I have argued elsewhere based on analysis of naturally occurring data [14], [7], actions - action types, to be precise - must be part of the underlying ontology of the representation formalism; partial action descriptions must be taken as basic; not only must the usual participants in an action such as agent or patient be represented, but also means, manner, direction, extent etc. Given these basic assumptions, it seems that knowledge about actions falls into the following two categories: 1. Terminological k n o w l e d g e about an actiontype: its participants and its relation to other action-types that it either specializes or abstracts - e.g. slice specializes cu"
P92-1016,C92-4181,1,0.685373,"Missing"
P92-1016,C90-2068,1,0.843127,"Missing"
P92-1016,H89-1033,0,0.0903352,"Missing"
P92-1016,P91-1043,0,0.0586711,"Missing"
P95-1005,1993.iwpt-1.12,0,0.0588063,"Missing"
P95-1005,C88-2120,0,0.0306128,"w is the next week? (13) If all else fails there is always video conferencing. (14) S 1: Monday, Tuesday, and Wednesday I am out of town. (15) But Thursday and Friday are both good. (16) How about Thursday at twelve? (17) $2: Sounds good. (18) See you then. Figure 1: E x a m p l e o f D e l i b e r a t i n g O v e r A M e e t i n g T i m e purpose of making our argument easy to follow. Notice that in both of these examples, the speakers negotiate over multiple alternatives in parallel. We challenge an assumption underlying the best known theories of discourse structure (Grosz and Sidner 1986; Scha and Polanyi 1988; Polanyi 1988; Mann and Thompson 1986), namely that discourse has a recursive, tree-like structure. Webber (1991) points out that Attentional State i is modeled equivalently as a stack, as in Grosz and Sidner&apos;s approach, or by constraining the current discourse segment to attach on the rightmost frontier of the discourse structure, as in Polanyi and Scha&apos;s approach. This is because attaching a leaf node corresponds to pushing a new element on the stack; adjoining a node Di to a node Dj corresponds to popping all the stack elements through the one corresponding to Dj and pushing Di on the stac"
P95-1005,J86-3001,0,\N,Missing
P97-1011,P95-1018,1,0.923129,"ific rhetorical structures (RSsner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; M a n n and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, suggests factors such as structural features of the discourse (e.g.,levelof embedding and segment complexity), intentional and informational relations in that structure, ordering of relata, and syntactic form of discourse constituents. Moser and Moore (1995; 1997) coded a corpus of naturally occurring tutorial explanations for the range of features identified in prior work. Because they were also interested in the contrast between occurrence and non-occurrence of cues, they exhaustively coded for all of the factors thought to contribute to cue usage in all of the text. From their study, Moscr and Moore identifiedseveral interesting correlations between particular features and specific aspects of cue usage, and were able to test specific hypotheses from the hterature that were based on constructed examples. In this paper, we focus on cue occurren"
P97-1011,W96-0402,1,0.874243,"Missing"
P97-1011,P84-1055,0,0.151533,"r cue occurrence and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 Introduction Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Millis, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not"
P97-1011,C90-3018,0,0.42976,", but that their indiscriminate use may have detrimental effects on recall (Millis, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where the cue should be placed in the text, and (3) selection: what lexical item(s) should be used. Prior work in text generation has focused on cue selection (McKeown and Elhadad, 1991; Elhadad and McKeown, 1990), or on the relation between *Learning Research & Development Center tComputer Science Department, and Learning Research ~z Development Center tlntelllgentSystems Program P a o l u c c i ""+ cue occurrence and placement and specific rhetorical structures (RSsner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; M a n n and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, suggests"
P97-1011,J86-3001,0,0.889413,"ought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 Introduction Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Millis, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where the cue should be placed in the text, and (3) se"
P97-1011,W96-0208,0,0.0177431,"93, Ch. 4) for details. Thus, below we will report the average estimated error rate on the test set, as computed by 10-fold cross-validation experiments. The algorithm We chose the C4.5 learning algorithm (Quinlan, 1993) because it is well suited to a domain such as ours with discrete valued attributes. Moreover, C4.5 produces decision trees and rule sets, both often used in text generation to implement mappings from function features to forms? Finally, C4.5 is both readily available, and is a benchmark learning algorithm that has been extensively used in NLP applications, e.g. (Litman, 1996; Mooney, 1996; Vander Linden and Di Eugenio, 1996). As our dataset is small, the results we report are based on cross-validation, which (Weiss and Kulikowski, 1091) recommends as the best method to evaluate decision trees on datasets whose cardinality is in the hundreds. Data for learning should be divided into training and test sets; however, for small datasets this has the disadvantage that a sizable portion of the data is not available for learning. Crossvalidation obviates this problem by running the algorithm N times (N=10 is a typical value): in each run, (N~l)th of the data, randomly chosen, is used"
P97-1011,W94-0302,1,0.875054,"Missing"
P97-1011,J95-1002,0,\N,Missing
P97-1011,P87-1014,0,\N,Missing
P98-1052,J96-2004,0,0.156522,"Missing"
subba-etal-2006-building,W04-3326,0,\N,Missing
subba-etal-2006-building,A00-2018,0,\N,Missing
subba-etal-2006-building,J93-2004,0,\N,Missing
subba-etal-2006-building,N03-2034,1,\N,Missing
subba-etal-2006-building,C94-1042,0,\N,Missing
subba-etal-2006-building,J02-3001,0,\N,Missing
subba-etal-2006-building,J05-1004,0,\N,Missing
subba-etal-2006-building,W06-2605,1,\N,Missing
tretti-di-eugenio-2010-analysis,P97-1035,0,\N,Missing
W02-0205,W02-2116,1,\N,Missing
W02-0205,W99-0307,0,\N,Missing
W02-0205,W99-0302,0,\N,Missing
W02-0205,A97-1051,0,\N,Missing
W02-2116,P00-1020,0,0.0773654,"hat are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical students, but it was never evaluated vs. a lesssophisticated version; the ANDES system which teaches physics, and its NL version ATLAS (VanLehn et al., 2000) have"
W02-2116,C96-1043,0,0.133904,"s (Reiter and Dale, 2000). We also introduced what we call functional aggregation (perhaps a type of conceptual aggregation (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches"
W02-2116,di-eugenio-etal-2002-binomial,1,0.787419,"Missing"
W02-2116,P98-1084,0,0.332946,"troduced what we call functional aggregation (perhaps a type of conceptual aggregation (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical student"
W02-2116,W96-0403,0,0.871088,"he indicator named “Visual Combustion Check”. 2.1 The sentence planner We set out to rapidly improve DIAG’s feedback mechanism. Our main goals were to to assess whether simple NLG techniques would lead to measurable improvements in the system’s output, and to conduct a systematic evaluation that would focus on language only. Thus, we did not change the tutoring strategy, or alter the interaction between student and system in any way. Rather, we concentrated on improving each turn by avoiding excessive repetitions. We chose to achieve this by: introducing syntactic aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998; Reape and Mellish, 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2 shows the revised output produced by DIAG-NLP. The RUs under discussion are grouped by the system modules that contain them (Oil Burner and Furnace System), and by the likelihood that a certain RU causes the observed symptoms. In contrast to the original answer, the revised answer singles out the Ignitor Assembly, the only RU that cannot cause the symptom. As our sentence planner, we"
W02-2116,J97-1004,0,0.0866808,"d Dale, 2000). We also introduced what we call functional aggregation (perhaps a type of conceptual aggregation (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has be"
W02-2116,P01-1057,0,0.0815006,"w of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical students, but it was never evaluated vs. a lesssophisticated version; the ANDES system which teaches physics, and its NL version ATLAS (VanLehn et al., 2000) have been evaluated, but o"
W02-2116,P98-2199,0,0.137506,"l Combustion Check”. 2.1 The sentence planner We set out to rapidly improve DIAG’s feedback mechanism. Our main goals were to to assess whether simple NLG techniques would lead to measurable improvements in the system’s output, and to conduct a systematic evaluation that would focus on language only. Thus, we did not change the tutoring strategy, or alter the interaction between student and system in any way. Rather, we concentrated on improving each turn by avoiding excessive repetitions. We chose to achieve this by: introducing syntactic aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Shaw, 1998; Reape and Mellish, 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2 shows the revised output produced by DIAG-NLP. The RUs under discussion are grouped by the system modules that contain them (Oil Burner and Furnace System), and by the likelihood that a certain RU causes the observed symptoms. In contrast to the original answer, the revised answer singles out the Ignitor Assembly, the only RU that cannot cause the symptom. As our sentence planner, we use EXEMPLAR"
W02-2116,P97-1035,0,0.0445467,"an system A is to collect a number of measures, hoping that there will be at least one statistically significant measure in favor of system B and no significant measure in favor of system A. However, reality is often murkier than this ideal result. A typical result of an evaluation may be that out of twelve measures ten favor B and two favor A, but only two show statistical significance and those two point to opposite conclusions. The BCDF is an appropriate way of assessing whether B outperforms A on the whole. Using the BCDF addresses a different type of cumulative effect than e.g. PARADISE (Walker et al., 1997). This comprehensive framework for dialogue evaluation combines various measures to yield a cumulative score for each of the systems being evaluated. However, the cumulative scores are then arranged in pairs, and their difference tested for statistical significance. The BCDF is used not to obtain a single score, but to assess what the measures collectively say on the performance of the system. 4 Related work Our work touches on three issues: aggregation; evaluation of NLG systems; and work on evaluating NL interfaces for ITSs. Part of our rules implement standard types of aggregation such as s"
W02-2116,W98-1428,0,0.0755232,"eape and Mellish, 1998) and what we call functional aggregation, namely, grouping parts according to the structure of the system; and improving the format of the output. The bottom part of Figure 2 shows the revised output produced by DIAG-NLP. The RUs under discussion are grouped by the system modules that contain them (Oil Burner and Furnace System), and by the likelihood that a certain RU causes the observed symptoms. In contrast to the original answer, the revised answer singles out the Ignitor Assembly, the only RU that cannot cause the symptom. As our sentence planner, we use EXEMPLARS (White and Caldwell, 1998), an object-oriented, rule based generator. It mixes template-style and more sophisticated types of text planning. The rules (called exemplars) are meant to capture an exemplary way of achieving a communicative goal in a given communicative context. The text planner selects rules by traversing the exemplar specialization hierarchy, and evaluating the applicability conditions associated with each exemplar. In DIAG-NLP, exemplars are of two main types, description and aggregation / layout. The four description exemplars are used when the full description of a part is required, such as whether th"
W02-2116,J97-1007,0,0.11636,"n (Reape and Mellish, 1998)). Although it introduces semantic elements that are outside the purview of syntactic aggregation, it appears to be preferred by humans over syntactic aggregation (see Section 5). Evaluation is of great interest for the language generation community (Dale and Mellish, 1998), and much progress has been made in the last few years. Language generation systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the system’s performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures (Young, 1997; Carenini and Moore, 2000; Reiter et al., 2001). We have shown how different measures can be combined to assess what they collectively say on the performance of a system. Regarding evaluation of NL interfaces for ITSs, no experiment like ours has been published that compares two versions of the same system, one of which uses a NL interface.5 For example, the CIRCSIM-Tutor system (Evens et al., 1993) which teaches medical physiology has been used with medical students, but it was never evaluated vs. a lesssophisticated version; the ANDES system w"
W02-2116,C98-2194,0,\N,Missing
W02-2116,C98-1081,0,\N,Missing
W06-1708,P04-1036,0,0.0506961,"Missing"
W06-1708,W03-1015,0,0.027027,"Missing"
W06-1708,W02-0908,0,0.0292798,"does. How much of the improved effectiveness of Algorithm 3 is due to this fact? To answer this question, Algorithm 2 could be enhanced to include a morphological processor. • The effectiveness of Algorithms 3 and 4 may be hindered by the fact that many words are not yet included in the WordNet database (see Figure 6). Falling back on to Algorithm 2 proved not to be a solution. The impact of the incompleteness of the lexical resource should be investigated and assessed more precisely. Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). • The performance of Algorithm 4 might be improved by using more sophisticated word sense disambiguation methods. It would also be interesting to explore the application of the unsupervised method described in (McCarthy et al., 2004). 57 a bootstrapped model (Riloff and Jones, 1999; Ng and Cardie, 2003)—in ML, bootstrapping requires to seed the classifier with a small number of well chosen target examples. We could develop a web spider, based on the work described on this paper, to automatically retrieve larger amounts of training and test data, that in turn could be processed with more soph"
W06-2605,J03-4003,0,0.0211176,"Missing"
W06-2605,C94-1042,0,0.0430237,"components allow it to be easily coupled with a parser in order to automatically generate a semantically annotated corpus. To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon(Pustejovsky, 1991). CoreLex defines basic types such as art (artifact) or com (communication). Nouns that share the same bundle of basic types are grouped in the same Systematic Polysemous Class (SPC). The resulting 126 SPCs cover about 40,000 nouns. We modified and augmented LCFLEX’s existing lexicon to incorporate VerbNet and CoreLex. The lexicon is based on COMLEX (Grishman et al., 1994). Verb and noun entries in the lexicon contain a link to a semantic type defined in the ontology. VerbNet classes (including subclasses and frames) and CoreLex SPCs are realized as types in the ontology. The deep syntactic roles are mapped to the thematic roles, which are defined as variables in the ontology types. For more details on the parser see (Terenzi and Di Eugenio, 2003). Each of the 10,084 EDUs was parsed using the parser. The parser generates both a syntactic tree and the associated semantic representation – for the purpose of this paper, we only focus on the latter. Figure 2 shows"
W06-2605,J05-1004,0,0.0601932,"Missing"
W06-2605,J91-4003,0,0.0101383,"Lavie, 2000), a robust left-corner parser, with VerbNet (Kipper et al., 2000) and CoreLex (Buitelaar, 1998). Our interest in decompositional theories of lexical semantics led us to base our semantic representation on VerbNet. VerbNet operationalizes Levin’s work and accounts for 4962 distinct verbs classified into 237 main classes. Moreover, VerbNet’s strong syntactic components allow it to be easily coupled with a parser in order to automatically generate a semantically annotated corpus. To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon(Pustejovsky, 1991). CoreLex defines basic types such as art (artifact) or com (communication). Nouns that share the same bundle of basic types are grouped in the same Systematic Polysemous Class (SPC). The resulting 126 SPCs cover about 40,000 nouns. We modified and augmented LCFLEX’s existing lexicon to incorporate VerbNet and CoreLex. The lexicon is based on COMLEX (Grishman et al., 1994). Verb and noun entries in the lexicon contain a link to a semantic type defined in the ontology. VerbNet classes (including subclasses and frames) and CoreLex SPCs are realized as types in the ontology. The deep syntactic ro"
W06-2605,N03-1030,0,0.268361,"ns. Also of relevance to the topic of this workshop, is that discourse structure is inherently highly structured, since discourse structure is generally described in hierarchical terms: basic units of analysis, generally clauses, are related by discourse relations, resulting in more complex units, which in turn can be related via discourse relations. At the moment, we do not yet address the problem of parsing at higher levels of discourse. We intend to build on the work we present in this paper to achieve that goal. The task of discourse parsing can be divided into two disjoint sub-problems ((Soricut and Marcu, 2003) and (Polanyi et al., 2004)). The two sub-problems are automatic identification of segment boundaries and the labeling of rhetorical relations. Though we consider the problem of automatic segmentation to be an important part in discourse parsing, we have focused entirely on the latter problem of automatically labeling rhetorical 33 Figure 1: SemDP System Architecture (Discourse Parser) relations only. Our approach uses rich verb semantics1 of elementary discourse units (EDUs)2 based on VerbNet(Kipper et al., 2000) as background knowledge and manually annotated rhetorical relations as training"
W06-2605,N03-2034,1,0.707027,"Missing"
W06-2605,P02-1047,0,0.13861,"Missing"
W06-2605,kingsbury-palmer-2002-treebank,0,\N,Missing
W06-2605,W01-1605,0,\N,Missing
W06-2605,W04-0211,0,\N,Missing
W08-1114,A00-1008,0,0.0296121,"ur study starts exploring the role that positive feedback plays in tutoring and in ITSs. While it has long been observed that most tutors tend to avoid direct negative feedback, e.g. (Fox, 1993; Moore et al., 2004), ITSs mostly provide negative feedback, as they react to student errors. In this paper, we will first briefly describe our tutorial dialog collection. We will then present the planning architecture that underlies our feedback generator. Even if our ITS does not currently allow for student input, our generation architecture is inspired by state-of-the art tutorial dialog management (Freedman, 2000; Jordan et al., 2001; Zinn et al., 2002). One limitation of these approaches is that plan operators are difficult to maintain and extend, partly because they are manually defined and tuned. Crucially, our plan operators are automatically derived via the association rules mined from our corpus. Finally, we will devote a substantial amount of space to evaluation. Our work is among the first to show not only that a more sophisticated language interface results in more learning, but that it favorably compares with human tutors. Full details on our work can be found in (Lu, 2007). 2 Task and curri"
W09-2109,J96-2004,0,0.137936,"Missing"
W09-2109,J98-3002,0,0.632918,"pjordan+@pitt.edu ckerse2@uic.edu katz+@pitt.edu bdieugen@cs.uic.edu Abstract We present an innovative application of discourse processing concepts to educational technology. In our corpus analysis of peer learning dialogues, we found that initiative and initiative shifts are indicative of learning, and of learning-conducive episodes. We are incorporating this finding in KSC-PaL, the peer learning agent we have been developing. KSC-PaL will promote learning by encouraging shifts in task initiative. 1 Introduction Collaboration in dialogue has long been researched in computational linguistics (Chu-Carroll and Carberry, 1998; Constantino-Gonz´alez and Suthers, 2000; Jordan and Di Eugenio, 1997; Lochbaum and Sidner, 1990; Soller, 2004; Vizca´ıno, 2005), however, the study of peer learning from a computational perspective is still in the early stages. This is an important area of study because peer learning has been shown to be an effective mode of learning, potentially for all of the participants (Cohen et al., 1982; Brown and Palincsar, 1989; Birtz et al., 1989; Rekrut, 1992). Additionally, while there has been a focus on using natural language for intelligent tutoring systems (Evens et al., 1997; Graesser et al."
W09-2109,A97-2007,0,0.056784,"ics (Chu-Carroll and Carberry, 1998; Constantino-Gonz´alez and Suthers, 2000; Jordan and Di Eugenio, 1997; Lochbaum and Sidner, 1990; Soller, 2004; Vizca´ıno, 2005), however, the study of peer learning from a computational perspective is still in the early stages. This is an important area of study because peer learning has been shown to be an effective mode of learning, potentially for all of the participants (Cohen et al., 1982; Brown and Palincsar, 1989; Birtz et al., 1989; Rekrut, 1992). Additionally, while there has been a focus on using natural language for intelligent tutoring systems (Evens et al., 1997; Graesser et al., 2004; VanLehn et al., 2002), peer to peer interactions are notably different from those of expertnovice pairings, especially with respect to the richness of the problem-solving deliberations and negotiations. Using natural language in collaborative ∗ This work is funded by NSF grants 0536968 and 0536959. 55 learning could have a profound impact on the way in which educational applications engage students in learning. Previous research has suggested several mechanisms that explain why peer learning is effective for all participants. Among them are: self-directed explaining(Ch"
W09-2109,P90-1010,0,0.578064,"Carletta, 1996), is considered excellent (κ = 0.80). Our annotation of initiative was two fold. Since there is disagreement in the computational linguistics community as to the precise definition of initiative(Chu-Carroll and Carberry, 1998; Jordan and Di Eugenio, 1997), we annotated the dialogues for both dialogue initiative, which tracks who is leading the conversation and determining the current conversational focus, and task initiative, which tracks the lead in problem solving. For dialogue initiative annotation, we used the well-known utterance-based rules for allocation of control from (Walker and Whittaker, 1990). In this scheme, each utterance is tagged with one of four dialogue acts (assertion, command, question or prompt) and control is then allocated based on a set of rules. The dialogue act annotation was done automatically, by marking turns that end in a question mark as questions, those that start with a verb as commands, prompts from a list of commonly used prompts (e.g. ok, yeah) and the remaining turns as assertions. To verify that the automatic annotation was good, we manually annotated a sizable portion of the dialogues with those four dialogue acts. We then compared the automatic annotati"
W10-3016,W04-3103,0,0.0733903,"Missing"
W10-3016,P07-1125,0,\N,Missing
W10-3016,P09-2044,0,\N,Missing
W11-1408,J08-4004,0,0.0318418,"ly or after being prompted by the tutor. The tutor reacts to the mistake and possibly provides some form of explanation. After developing a first version of the coding manual, we refined it iteratively. During each iteration, two human annotators independently annotated several dialogues for one DA at a time, compared outcomes, discussed disagreements, and fine-tuned the scheme accordingly. This process was repeated until a sufficiently high inter-coder agreement was reached. The Kappa values we obtained in the final iteration of this process are listed in Table 2 (Di Eugenio and Glass, 2004; Artstein and Poesio, 2008). In Table 2, the “Double Coded*” column refers to the sessions that we double coded to calculate the inter-coder agreement. This number does not include the sessions which were double coded when coders were developing the coding manual. The numbers of double-coded sessions differ by DA since it depends on the frequency on the particular DA (recall that we coded for one DA at a time). For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI’s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs"
W11-1408,W10-3016,1,0.873484,"Missing"
W11-2035,J96-2004,0,0.45658,"Missing"
W11-2035,N06-2010,0,0.844569,"ion of markable type, and multiple statistical models. Our results show that knowing the type of the markable, and the presence of simultaneous pointing gestures improve co-reference resolution for personal and deictic pronouns. 1 Introduction pronouns, both personal (I, you, it, they), and deictic (this, that, these, those, here, there). Hence, this paper presents our first steps toward a full co-reference resolution module, and ultimately, the multi-modal interface. Co-reference resolution is likely the discourse and dialogue processing task that has received the most attention. However, as Eisenstein and Davis (2006) notes, research on co-reference resolution has mostly been applied to written text; this task is more difficult in dialogue. First, utterances may be informal, ungrammatical or disfluent; second, people spontaneously use hand gestures, body gestures and gaze. Pointing gestures are the easiest gestures to identify, and vision researchers in our project are working on recognizing pointing and other hand gestures (Di Eugenio et al., 2010). In this paper, we replicate the results from (Eisenstein and Davis, 2006), that pointing gestures help improve co-reference, in a very different domain. Other"
W11-2035,H05-1004,0,0.0106351,"This said, our model with gestures outperforms Strube and M¨uller (2003), who did not use gesture information to resolve pronouns in spoken dialogue. Strube and M¨uller (2003) used the 20 Switchboard dialogues as their experiment dataset, and used the MUC metrics. Our results are similar to Eisenstein and Davis (2006), but there are two main differences. First, the corpus they used is smaller than what we used in this paper. Their corpus was collected by themselves and consisted of 16 videos, each video was 2-3 minutes in length. Second, they used a difference measurement metrics called CEAF (Luo, 2005). 5 Conclusions In this paper, we presented the new ELDERLY-ATHOME multi-modal corpus we collected. A coreference resolution system for personal and deictic pronouns has been developed on the basis of the annotated corpus. Our results confirm that gestures improve co-reference resolution; a simple notion of type also helps. The Markable and Co-reference modules we presented are a first start in developing a full multi-modal co-reference resolution module. Apart from completing the annotation of our corpus, we will develop an annotation scheme for haptics, and investigate how haptics informatio"
W11-2035,rodriguez-etal-2010-anaphoric,0,0.0258043,"Missing"
W11-2035,P03-1022,0,0.385235,"Missing"
W12-1640,N12-1058,1,0.877963,"Missing"
W12-1640,W11-2035,1,0.862844,"Missing"
W12-1640,P03-1054,0,0.00315214,"those pairs for training, since we would run the risk of overfitting, and not being able to infer appropriate completions for other sentences. To generate additional <Interrupted sentences, candidate structure> pairs, we need to match an interrupted sentence IntS with its potential completions – basically, to check whether IntS can match the prefix of other sentences in the corpus. We do so by comparing the POS sequence and parse tree of IntS with the POS sequence and parse tree of the prefix of another sentence. Both IntS and other sentences in the corpus are parsed via the Stanford Parser (Klein and Manning, 2003). Before discussing the details though, we need to deal with one potential problem: the POS sequence for the incomplete portion of IntS may not be correctly assigned. For example, when the sentence ’The/DT, top/JJ, cabinet/NN.’ is interrupted as ’The/DT, top/NN’, the POS tag of NN is assigned to ’top’; this is incorrect, and engenders noise for finding correct completions. We first pre-process a dialogue by splitting turns into sentences, tokenizing sentences into tokens, and POS tagging tokens. Although for the interrupted sentences, we could obtain a correct POS tag sequence by parsing the i"
W12-1640,J11-1004,0,0.0317935,"pletion system, trained on the set of interrupted but eventually completed sentences from our corpus. In this paper, we will present the component of the system that predicts reasonable completion structures for an incomplete sentence. Sentence completion has been addressed within information retrieval, to satisfy user’s information needs (Grabski and Scheffer, 2004). Completing sentences in human-human dialogue is more difficult than in written text. First, utterances may be informal, ungrammatical or dis-fluent; second, people interrupt each other during conversations (DeVault et al., 2010; Yang et al., 2011). Additionally, the interaction is complex, as people spontaneously use hand gestures, body language and gaze besides spoken language. As noticed by (Bolden, 2003), during face-to-face interaction, the completion problem is not only an exclusively verbal phenomenon but ”an action embedded within a complex web of different meaning-making fields”. Accordingly, among our features, we will include pointing gestures, and haptic-ostensive (H-O) actions, e.g., referring to an object by manipulating it in the real world (Landragin et al., 2002; Foster et al., 2008). The paper is organized as follows."
W13-4031,P11-1119,0,0.120785,"tained by eliminating irrelevant content such as explanations of the tasks and interruptions by the person who accompanied the elderly subject (who is not playing the part of the helper). This 301 minutes contain 4782 spoken turns. The corpus includes video and audio data in .avi and .wav format, haptics data collected via instrumented gloves in .csv format, and the transcribed utterances in xml format. Related Work Due to its importance in dialogue research, DA classification has been the focus of a large body of research (Stolcke et al., 2000; Sridhar et al., 2009; Di Eugenio et al., 2010a; Boyer et al., 2011). Some of this work has been made possible by several available corpora tagged with DAs, including HCRC Map Task (Anderson et al., 1991), CallHome (Levin et al., 1998), Switchboard (Graff et al., 1998), ICSI Meeting Recorder (MRDA) (Shriberg et al., 2004), and the AMI multimodal corpus (Carletta, 2007). Researchers have applied various approaches to this task. Initially only simple textual features were used, e.g. n-grams were used to model the constraints for DA sequences in an HMM model (Stolcke et al., 2000). Zimmermann et al. (2006) investigated the joint segmentation and classification of"
W13-4031,W12-1634,0,0.367936,"Missing"
W13-4031,N12-1058,1,0.889891,"Missing"
W13-4031,N12-4000,0,0.212013,"Di Eugenio, 2012). In this paper, we focus on the Dialogue Act classification component. We will also touch on Dialogue Game inference. Our collaborators are developing the speech processing, vision and haptic recognition components (Franzini and Ben-Arie, 2012; Ma and Ben-Arie, ˇ 2012; Javaid and Zefran, 2012), that, when integrated with the dialogue manager we are building, 183 Proceedings of the SIGDIAL 2013 Conference, pages 183–192, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics to significantly improve the recognition of several DAs, whereas Ha et al. (2012) shows that automatically recognized postural features may help to disambiguate DAs. It should be pointed out that most of this work focuses on offline DA classification – namely, DA classification is performed on the corpus using the gold-standard classification for the previous DA(s). Since some sort of history of previous DAs is used by all systems, using online classification for the previous DAs will unavoidably impact performance (Sridhar et al., 2009; Kim et al., 2012). Additionally, for models such as HMMs and CRF that approach the problem as sequence labeling, online processing means"
W13-4031,W11-2035,1,0.877995,"Missing"
W13-4031,D10-1084,0,0.67237,"2012; Kim et al., 2012). We used Mallet (McCallum, 2002) to build CRF models. MaxEnt models were built using the MaxEnt 3 package from the Apache OpenNLP package. Naive Bayes and Decision Tree models were built with the Weka (Hall et al., 2009) package (for decision trees, we used the J48 implementation). All the results we will show below were obtained using 10 fold cross validation. Utterance features (UT) are extracted from the current utterance’s meta information. Previous research showed that utterance meta information such as the utterance speaker can help classify DAs (Ivanovic, 2008; Kim et al., 2010). • The actor of the utterance • The time length of the utterance • The distance of the current utterance from the beginning of the dialogue The pointing gesture feature (PT) indicates whether the actor of the current utterance ui is making a pointing gesture G, i.e., whether G is associated with ui , and hence, part of move mi . Haptic-Ostensive features (H-O) indicate whether the actor of the current utterance ui is performing any H-O action G i.e., whether G is associated with ui , and hence, part of move mi ; and the type of that action, if yes. Location features (LO) include the locations"
W13-4031,W08-1301,0,0.0553763,"erance features, and automatically inferred dialogue game features. Textual features (TX) are the most widely used features for DA classification (Stolcke et al., 2000; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010a; Kim et al., 2010; Boyer et al., 2011; Ha et al., 2012; Kim et al., 2012). The textual features we use include lexical, syntactic, and heuristic features. • Lexical features: Unigrams of the words and part-of-speech tags in the current utterance. The words used in the features are processed using the morphology tool from the Stanford parser (De Marneffe and Manning, 2008). • Syntactic features: The top node and its first two child nodes from the sentence parse tree. If an utterance contains multiple sentences, we use the last sentence. Sentences are parsed using the Stanford parser. • Number of sentences and number of words in the utterance. We use Apache OpenNLP library 4 to detect sentences and tokenize them. 3 4 http://maxent.sourceforge.net http://opennlp.apache.org/ 188 classifiers may be even more negatively affected in online mode with respect to their offline performance, as compared to other classifiers. We will see that indeed this will happen for CR"
W13-4031,Y12-1050,0,0.267905,"st 2013. 2013 Association for Computational Linguistics to significantly improve the recognition of several DAs, whereas Ha et al. (2012) shows that automatically recognized postural features may help to disambiguate DAs. It should be pointed out that most of this work focuses on offline DA classification – namely, DA classification is performed on the corpus using the gold-standard classification for the previous DA(s). Since some sort of history of previous DAs is used by all systems, using online classification for the previous DAs will unavoidably impact performance (Sridhar et al., 2009; Kim et al., 2012). Additionally, for models such as HMMs and CRF that approach the problem as sequence labeling, online processing means that only a partial sequence is available. Figure 1: System Architecture will make the interface situated in and able to deal with a real environment. After discussing related work in Section 2, we present our multimodal corpus and the multidimensional annotation scheme we devised in Section 3. In Section 4 we discuss all the features we used to build machine learning models to classify DAs. Sections 5 is devoted to our experiments and the results we obtained. We conclude and"
W13-4031,W04-2319,0,0.0490874,"video and audio data in .avi and .wav format, haptics data collected via instrumented gloves in .csv format, and the transcribed utterances in xml format. Related Work Due to its importance in dialogue research, DA classification has been the focus of a large body of research (Stolcke et al., 2000; Sridhar et al., 2009; Di Eugenio et al., 2010a; Boyer et al., 2011). Some of this work has been made possible by several available corpora tagged with DAs, including HCRC Map Task (Anderson et al., 1991), CallHome (Levin et al., 1998), Switchboard (Graff et al., 1998), ICSI Meeting Recorder (MRDA) (Shriberg et al., 2004), and the AMI multimodal corpus (Carletta, 2007). Researchers have applied various approaches to this task. Initially only simple textual features were used, e.g. n-grams were used to model the constraints for DA sequences in an HMM model (Stolcke et al., 2000). Zimmermann et al. (2006) investigated the joint segmentation and classification of DAs using prosodic features. Sridhar et al. (2009) showed that prosodic cues can improve DA classification for a Maximum Entropy based model. Di Eugenio et al. (2010a) extended Latent Semantic Analysis with linguistic features, including dialogue game in"
W13-4031,J00-3003,0,0.887221,"it can be a gesture (pointing or haptics) only, or a multimodal utterance. Third, when people use gestures and actions together with utterances, the utterances become shorter, hence the textual context that has been used to advantage in many previous models is impoverished. Our contributions concern: exploring the dialogue functions of what we call Haptic-Ostensive (H-O) actions (Foster et al., 2008), namely haptics actions that often perform a referential function; experimenting with both offline and online DA classification, whereas most previous work only focuses on offline classification (Stolcke et al., 2000; Hastie et al., 2002; Di Eugenio et al., 2010a); highlighting the role played by multimodal features and dialogue structure (in the form of dialogue games) as concerns DA classification. We describe the annotation of a multimodal corpus that includes pointing gestures and haptic actions (force exchanges). Haptic actions are rarely analyzed as fullfledged components of dialogue, but our data shows haptic actions are used to advance the state of the interaction. We report our experiments on recognizing Dialogue Acts in both offline and online modes. Our results show that multimodal features and"
W13-4031,P06-1026,0,\N,Missing
W14-4402,W13-2128,0,0.0310068,"y New York, NY, USA Jianrong Li, Yves A. Lussier University of Arizona Tucson, AZ, USA Abstract be communicated, and renders it in English via SimpleNLG (Gatt and Reiter, 2009). Related work. NLG and Summarization in the biomedical domain have been pursued for a few years (Di Eugenio and Green, 2010), but most work addresses health care personnel: to navigate cancer patients’ medical histories (Hallett, 2008; Scott et al., 2013); to generate textual summaries describing a hospitalized infant for nurses (Portet et al., 2009); to generates reports of care for hand-off between emergency workers (Schneider et al., 2013). Most applications of NLG that target patients focus on behavioral changes (Reiter et al., 2003), or patient counseling (Green et al., 2011). Only few NLG systems attempt at generating personalized medical information from medical records or data (Williams et al., 2007; Mahamood and Reiter, 2011). PatientNarr summarizes information taken from textual discharge notes written by physicians, and structured nursing documentation. It builds a graph that highlights the relationships between the two types of documentation; and extracts information from the graph for content planning. SimpleNLG is us"
W14-4402,W09-0613,0,0.252166,"Missing"
W14-4402,W07-2328,0,0.480554,"Missing"
W14-4402,W11-2803,0,0.288152,"Missing"
W14-4402,W07-2313,0,0.0176741,"mia management is remedy for tachycardia and atrial fibrillation. As a result, cardiac pump effectiveness, cardiopulmonary status and cardiac tissue perfusion status have improved slightly. Actual Negative Breathing Pattern related to respiration disorders was treated with respiration monitoring. Respiratory Status has improved significantly. You have an appointment at Union Medical Center on DATE at TIME. The list of medication is attached to this discharge. Figure 4: PatientNarr generated summary (Patient 9) traction and rendering of rhetorical relationships among events and their outcomes (Mancini et al., 2007). Last but not least, we will perform user studies, both controlled evaluation of our summaries while still at the development stage, and eventually longer-term assessments of whether our summaries engender better adherence to medications and better keeping of follow-up appointments, and ultimately, better health. tal signs [...]. On the other hand, [...]). For the moment, we do not mention outcomes for which no improvement, or a setback, has been recorded. The summary also includes: mentions of education that has been imparted; and reminders of future appointments and of medicines to be taken"
W16-0406,strapparava-valitutti-2004-wordnet,0,0.150257,"a single’s selling performance, radio airplay audience impression, and online streaming activity (Trust, Gary, 2013). We explored if correlation would reflect in the Thomson Reuters/University of Michigan Consumer Confidence Index (ICC) and the Dow Jones Industrial Average (DJIA), a major U.S. stock market index. For our work, we gathered the entire set of weekly Hot 100 songs between 2008 and 2013. We used OpinionFinder to analyze the positive and negative polarity of lyrics (Wilson et al., 2005). We then used a second tool, WordNet Affect, to perform sentiment analysis along ninedimensions (Strapparava and Valitutti, 2004). We assessed the strength of sentiment correlation to the DJIA and ICC. We then explored poignant Granger-causal relations and created a predictive model for both societal indicators. In this work, we discover, there are indeed correlating and causal relationships between the song sentiments and these societal indicators. Proceedings of NAACL-HLT 2016, pages 17–25, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Related Work Bollen et al. (2011) explored the notion that public mood can be correlated to and even predictive of economic indicators. The"
W16-0406,H05-1044,0,0.0534657,"lter out the noise of ephemeral, popular happenings which pervade Twitter. The Hot 100 listing is calculated based on a single’s selling performance, radio airplay audience impression, and online streaming activity (Trust, Gary, 2013). We explored if correlation would reflect in the Thomson Reuters/University of Michigan Consumer Confidence Index (ICC) and the Dow Jones Industrial Average (DJIA), a major U.S. stock market index. For our work, we gathered the entire set of weekly Hot 100 songs between 2008 and 2013. We used OpinionFinder to analyze the positive and negative polarity of lyrics (Wilson et al., 2005). We then used a second tool, WordNet Affect, to perform sentiment analysis along ninedimensions (Strapparava and Valitutti, 2004). We assessed the strength of sentiment correlation to the DJIA and ICC. We then explored poignant Granger-causal relations and created a predictive model for both societal indicators. In this work, we discover, there are indeed correlating and causal relationships between the song sentiments and these societal indicators. Proceedings of NAACL-HLT 2016, pages 17–25, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Related W"
W16-0406,P08-2034,0,0.0305441,"Missing"
W16-3639,J03-4003,0,0.0160058,"ral attributes, and hence the system produces a bar chart also added to the display. Finally, for the final request d), the system closes the most recently generated visualization, i.e. the bar chart (this is not shown in Figure 3). 4.1 Table 3: Feature Types ticlass classifier. We applied popular question classification features from (Loni et al., 2011) due to the general question-based construct of the requests. Apache OpenNLP (Apache Software Foundation, 2011) was used to generate unigrams, bigrams, trigrams, chunking, and tagged unigrams, while Stanford Parser’s implemented Collins rules (Collins, 2003) were used to obtain the headword. The feature vector is comprised of 7,244 total features, see Table 3. We used Weka (Hall et al., 2009) to experiment with several classifiers. We will discuss their performance in Sec. 5; currently, we use the SVM model, which performs the best. 4.3 Window Management Requests If the classifier assigns to an utterance the window management type, a logical form along the lines described above will be generated, but no SQL query will be produced. At the moment, keyword extraction is used to determine whether the window management instruction relates to closing,"
W16-3639,J05-1004,0,0.0307215,"performs the best. 4.3 Window Management Requests If the classifier assigns to an utterance the window management type, a logical form along the lines described above will be generated, but no SQL query will be produced. At the moment, keyword extraction is used to determine whether the window management instruction relates to closing, opening, or repositioning; the system only supports closing the most recently created new visualization. Parsing We begin by parsing the utterance we obtain from the Google Speech API into three NLP structures. ClearNLP (Choi, 2014) is used to obtain PropBank (Palmer et al., 2005) semantic role labels (SRLs), which are then mapped to Verbnet (Kipper et al., 2008) and Wordnet using SemLink (Palmer, 2009). The Stanford Parser is used to obtain the remaining two structures, i.e. the syntactic parse tree and dependency tree. The final formulation is the conjunction Cpredicate ∩ Cagent ∩ Cpatient ∩ Cdet ∩ Cmod ∩ Caction . The first three clauses are extracted from the SRL. The NPs from the syntactic parse tree contain the determiners for Cdet , adjectives for Cmod , and nouns as arguments for Caction . 4.2 Total Terms 3,203 2,311 784 584 314 33 15 4.4 Create/Modify Visualiz"
W16-6604,W14-4402,1,0.788869,"Missing"
W16-6604,W09-0613,0,0.219291,"Missing"
W16-6604,W14-1202,0,0.0425362,"Missing"
W16-6604,W11-2803,0,0.28101,"a patient. After summarizing our baseline work previously reported in (Di Eugenio et al., 2014), we focus on medical term complexity. The novelty of our work consists in the multiprong approach underlying our complexity metric, that includes linear regression and clustering; and in applying the metric not just to the term in question, but to its many available definitions, so as to choose the simplest one to refer the patient to. 2 Related Work Only few NLG systems generate personalized information from medical data for the patient, as opposed to health care personnel (Williams et al., 2007; Mahamood and Reiter, 2011). As concerns identifying 26 difficult terms, some applications search for them in vocabularies or in specific corpora (Ong et al., 2007; Kandula et al., 2010). The drawback of these approaches is that they make an underlying assumption that all the terms that appear in such resources are complex and need to be explained further. Moreover, since none of the currently available vocabularies/corpora are exhaustive enough, this method is not reliable. Our approach for identifying complex terms is closer to (Shardlow, 2013), but we are interested in medical terms and use five times as many feature"
W16-6604,P13-3015,0,0.0542413,"nt, as opposed to health care personnel (Williams et al., 2007; Mahamood and Reiter, 2011). As concerns identifying 26 difficult terms, some applications search for them in vocabularies or in specific corpora (Ong et al., 2007; Kandula et al., 2010). The drawback of these approaches is that they make an underlying assumption that all the terms that appear in such resources are complex and need to be explained further. Moreover, since none of the currently available vocabularies/corpora are exhaustive enough, this method is not reliable. Our approach for identifying complex terms is closer to (Shardlow, 2013), but we are interested in medical terms and use five times as many features, and a two-step approach, not their single SVM model. Similar to (Ramesh et al., 2013), we provide definitions for terms; but Ramesh et al. consider every term whose semantic type falls within a set of 16 types derived from the Unified Medical Language System1 (UMLS) as complex, while we don’t make such assumptions. 3 System Workflow In our previous work (Di Eugenio et al., 2014), we set up the core of the NLG pipeline represented by Component 1 in Figure 1. We also computationally demonstrated that doctor and nurses"
W16-6604,W07-2328,0,0.563225,"of the care provided to a patient. After summarizing our baseline work previously reported in (Di Eugenio et al., 2014), we focus on medical term complexity. The novelty of our work consists in the multiprong approach underlying our complexity metric, that includes linear regression and clustering; and in applying the metric not just to the term in question, but to its many available definitions, so as to choose the simplest one to refer the patient to. 2 Related Work Only few NLG systems generate personalized information from medical data for the patient, as opposed to health care personnel (Williams et al., 2007; Mahamood and Reiter, 2011). As concerns identifying 26 difficult terms, some applications search for them in vocabularies or in specific corpora (Ong et al., 2007; Kandula et al., 2010). The drawback of these approaches is that they make an underlying assumption that all the terms that appear in such resources are complex and need to be explained further. Moreover, since none of the currently available vocabularies/corpora are exhaustive enough, this method is not reliable. Our approach for identifying complex terms is closer to (Shardlow, 2013), but we are interested in medical terms and us"
W19-5928,W11-2803,0,0.0269693,"nt hospitalization and their experiences; 4) Patients answer the PAM questions; 5) If interested, patients talk about their interests or have a general conversation with the interviewer. For our analyses, we group the patients with PAM level 1 or 2 and refer to them as low PAM patients, while we refer to the group of patients with PAM levels 3 or 4 as high PAM patients. The general statistics on the interviews is shown in Table 1. Related Work While several systems exist that summarize medical content (Scott et al., 2013; Pauws et al., 2019), only a few of them produce personalized summaries (Mahamood and Reiter, 2011). Unlike these systems that focus on data-to-text summarization, our personalized summary generation system combines the information from physician and nursing documents and provides hospitalization information to patients in a form that they can understand. Even though a lot of studies have focused on verifying the reliability of the PAM metric (Fowles et al., 2009), no work uses it to produce personalized content for patients. Most of the existing qualitative studies on the narratives of heart failure patients (Jeon et al., 2010; Seah et al., 2016) focus on identifying the factors that impac"
W19-5928,W16-6604,1,0.906879,"Missing"
W19-5928,Q16-1033,0,0.0237907,"czynski and Greenberg, 1987). Finally, we discuss themes that emerge from those conversations, with the goal of highlighting aspects that hold significance in the patients’ lives: for example, patients with high PAM focus more on activities they are interested in, patients with low PAM on their own feelings (confirming the finding about pronouns just discussed). 2 tients and their motivation to participate in selfcare. Our quantitative analyses are inspired by Pennebaker (2003) and are similar to the studies that predict the empathy of the counselor based on the words used during the session (Althoff et al., 2016; P´erez-Rosas et al., 2017; Xiao et al., 2014). 3 Interview Collection Category Avg. number of words in an interview(P) Avg. number of words in an interview (I) Avg. number of words/utterance (P) Avg. number of words/utterance (I) Number of low PAM patients Number of high PAM patients Values 1655 1104 8 6 14 12 Table 1: Distributional analysis of the interviews (P: Patient, I: Interviewer) Since there are no existing publicly available data sets that provide information on the experiences of heart-failure patients, we proceeded to collect one.2 We interviewed 26 patients (age range 20-70 year"
W19-5928,P17-1131,0,0.0662729,"Missing"
W93-0204,P92-1016,1,0.88477,"Missing"
W93-0204,W93-0203,0,0.0609909,"Missing"
W93-0204,C92-4181,1,0.873357,"Missing"
W93-0204,P91-1007,0,0.0723577,"Missing"
W96-0402,J96-2004,0,0.0136803,"0.76 0.71 According to this table, therefore, the AWARENESS and SAFETY features show ""substantial"" agreement and the INTENTIONALITYfeature shows ""moderate"" agreement. We have coded other functional features as well, but they have either not proven as reliable as these, or are not as useful in text planning. In addition, Siegel and Castellan (1988) point out that it is possible to check the significance of K when the number of objects is large; this involves computing the distribution of K itself. Under this approach, the three values above are significant at the .000005 level. As advocated by Carletta (1996), we have used the Kappa coefficient (Siegel and Castellan, 1988) as a measure of coder agreement. For nominal data, this statistic not only measures agreement, but also factors out chance agreement. If P(A) is the proportion of times the coders agree, and P(E) is the proportion of times that coders are expected to agree by chance, K is computed as follows: K= K INTENTIONALITY 0.46 slight fair moderate substantial almost perfect form DONT Neg-TC NEVER frequency 100 57 22 The learning algorithm used these examples to derive a decision tree which we then integrated into an existing micro-planner"
W96-0402,J93-4004,0,0.0372802,"ave been implemented, but while the architectures themselves may be reused in a new domain, the library of plans typically cannot. One way to address this problem is to use machine learning techniques to automate the derivation of planning resources for new domains. In this paper, we apply this technique to build microplanning rules for preventative expressions in instructional text. 1 Introduction Building text planning resources by hand is timeconsuming and difficult. Certainly, much work has been done in this regard; there are a number of freely available text planning architectures (e.g., Moore and Paris, 1993). It is frequently the case, however, that while the architecture itself can be reused in a new domain, the library of text plans developed for it cannot. In particular, micro-planning rules, those rules that specify the low-level grammatical details of expression, are highly sensitive to variations between sublanguages, and are therefore difficult to reuse. When faced with a new domain in which to generate text, the typical scenario is to perform a *This work is partially supported by the Engineering and Physical Sciences Research Council (EPSRC) Grant J19221, by BC/DAA9ARC Project 293, and b"
W96-0402,W94-0308,1,\N,Missing
W96-0509,W90-0111,0,0.0301958,"Missing"
W96-0509,W94-0302,1,0.889601,"Missing"
W96-0509,J86-3001,0,\N,Missing
xie-etal-2008-extracting,W02-2103,0,\N,Missing
xie-etal-2008-extracting,J05-3002,0,\N,Missing
xie-etal-2008-extracting,P00-1041,0,\N,Missing
xie-etal-2008-extracting,P05-1009,0,\N,Missing
xie-etal-2008-extracting,J02-4006,0,\N,Missing
xie-etal-2008-extracting,P05-2018,1,\N,Missing
xie-etal-2008-extracting,P06-1139,0,\N,Missing
xie-etal-2008-extracting,W04-1013,0,\N,Missing
