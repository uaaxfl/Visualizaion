2007.mtsummit-papers.66,2006.tc-1.9,0,0.350399,"Missing"
2007.mtsummit-papers.66,2005.mtsummit-papers.11,0,0.0132269,"ranslation quality threshold (for their TM lookup) and “Number of candidate translations retrieved”. But disappointingly they did not train their own MT system but rather worked only with free-access web-based MT systems. They showed that a combination of Translation Memory with such free-access web-based MT systems works better than the web-based MT systems alone. For English to Spanish this resulted in an improvement of around 7 points BLEU scores (but hardly any improvement at all for English to Czech). It was also difficult to find other projects on Swedish to Danish Machine Translation. (Koehn, 2005) has trained his system on a parallel corpus of more than 20 million words from the European parliament. In fact he trained on all combinations of the 11 languages of the Europarl corpus. This corpus contains 27.1 million Danish tokens, but only 23.5 million Swedish tokens. The difference is due to the fact that some chapters are not translated into Swedish (there are 4,120 Danish chapters, but only 3,627 Swedish chapters). (Koehn, 2005) reports a BLEU score of 30.3 for Swedish to Danish translation which ranks somewhere in the middle when compared to other language pairs from the Europarl cor"
2007.mtsummit-papers.66,2006.tc-1.10,0,0.142751,"shSwedish subtitles that we had downloaded from http://www.opensubtitles.org/). The BLEU scores are on the order of 11 to 13 for German to English (and worse for the opposite direction). Thus they are very low. They also did user evaluations with 4-point scales for intelligibility and accuracy. They asked 5 people per language pair to rate a random set of 200 sentences of system output. The judges rated English to German translations higher than the opposite direction (which contradicts the BLEU scores). But due to the small scale of the evaluation it seems premature to draw any conclusions. (Melero et al., 2006) combined Translation Memory technology with Machine Translation, which looks interesting at first sight. But then it turns out that their Translation Memories for the language pairs Catalan-Spanish and SpanishEnglish were not filled with subtitles but rather with newspaper and UN texts. They don’t give any motivation for this. The paper contains a short section on “compression” which should probably be called “(named) entity classification”. The most interesting aspect in this part is that they use a parameter for translation quality threshold (for their TM lookup) and “Number of candidate tr"
2007.mtsummit-papers.66,J04-4002,0,0.0511169,"Missing"
2007.mtsummit-papers.66,2005.mtsummit-papers.19,0,0.0623724,"Missing"
2007.mtsummit-papers.66,2001.mtsummit-papers.68,0,0.0472296,"Missing"
2007.mtsummit-papers.66,P02-1040,0,\N,Missing
2010.amta-papers.14,W06-2810,0,0.0170237,"Missing"
2010.amta-papers.14,P91-1022,0,0.439489,"Missing"
2010.amta-papers.14,E06-1032,0,0.0217106,"Similarity Score BLEU has been developed as an automatic means to measure the translation quality of MT systems by comparing the system translation with one or more reference translations (Papineni et al., 2002). This is done by measuring the token n-gram precision of the system translation (hypothesis) for all n-gram levels up to 4, and combining the n-gram precisions using the geometric mean. The score of hypotheses shorter than the reference is reduced by a brevity penalty. BLEU has been criticised as a measure of translation quality, and it is not considered reliable on a sentence level (Callison-Burch and Osborne, 2006). On the other hand, judging the quality of a translation is a much harder task than deciding whether two sentences are possible translations of each other. We found that BLEU is very sensitive to misalignments, often yielding a score of 0 if two unrelated sentences are compared, which means that it is capable of discriminating between aligned and unaligned sentence pairs. Still, we made a number of modifications to the scoring implementation to fit it to our needs. Usually, BLEU is measured on up to 4-grams, motivated by the fact that n-gram scores of higher order are a good measure of a tran"
2010.amta-papers.14,P93-1002,0,0.212456,"approaches. 2 Related Work Overviews of sentence alignment algorithms are provided in (Manning and Sch¨utze, 1999; Singh and Husain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (2002) and Varga et al. (2005) describe a two-pass algorithm, using a length-based approach for a first alignment. The first alignment subsequently serves as training data for a translation model, which is then used in a complex similarity score. The two approaches differ in that Varga et al. (2005) use a dictionary-based translation model, with a dictionary that can be manually expanded, while Moore (2002) works with a IBM-1 translation model, following Chen (1993). Diffe"
2010.amta-papers.14,W04-3208,0,0.0114444,"two approaches differ in that Varga et al. (2005) use a dictionary-based translation model, with a dictionary that can be manually expanded, while Moore (2002) works with a IBM-1 translation model, following Chen (1993). Different degrees of textual and metatextual structure open different possibilities for sentence alignment. Tiedemann (2007) shows that movie subtitles are a highly attractive text type for sentence alignment because the texts can be aligned on the basis of time stamps. Sentence alignment is also performed on comparable corpora for which no parallel structure can be assumed (Fung and Cheung, 2004; Adafre and de Rijke, 2006; Yasuda and Sumita, 2008). Adafre and de Rijke (2006) describe an MTbased approach to find corresponding sentences in Wikipedia based on sentence similarity. Our approach is based on the same basic idea of first automatically translating one of the to-be-aligned language portions, and then measuring the similarity between this translation and the other language portion. 3 The Parallel Corpus Our sentence aligner has been developed to align the parallel part of the Text+Berg corpus, a corpus consisting of the yearbooks of the Swiss Alpine Club from 1864–19821 (Volk e"
2010.amta-papers.14,J93-1004,0,0.910396,"core. The approach requires an MT system with a reasonable performance for the language pair, but no other language-specific resources. It is thus fairly language-independent, and as we will show, more robust to textual noise than other approaches. 2 Related Work Overviews of sentence alignment algorithms are provided in (Manning and Sch¨utze, 1999; Singh and Husain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (2002) and Varga et al. (2005) describe a two-pass algorithm, using a length-based approach for a first alignment. The first alignment subsequently serves as training data for a translation model, which is then used in a complex similarity score. The"
2010.amta-papers.14,J93-1006,0,0.337859,"Missing"
2010.amta-papers.14,P07-2045,0,0.0147103,"the quality of the sentence alignment to correlate with the quality of the MT systems used, this should also give an indication as to how good translations need to be for this approach to work. A second question is how SMT performance scales with sentence alignment performance, and whether high-precision or high-recall alignment is preferable. 6.1 Method Except for the Google translations, which were obtained online through Google Translate, all systems were trained using the instructions for building a baseline system by the 2010 ACL Workshop on SMT.11 The SMT systems are built using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). The systems we trained are different from the baseline system described above in that we used various training sets, and did not train a recaser, using a BLEU implementation that ignores case differences. Table 5 describes the training data used for the different MT systems. Translation model (TM) training data is a subset of language model (LM) training data: only sentences with a valid alignment and fewer than 40 words in length are used to train the translation model. Europarl-K uses a sample of 1000 (1K) units of training data; all o"
2010.amta-papers.14,2005.mtsummit-papers.11,0,0.111691,"es. It is thus fairly language-independent, and as we will show, more robust to textual noise than other approaches. 2 Related Work Overviews of sentence alignment algorithms are provided in (Manning and Sch¨utze, 1999; Singh and Husain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (2002) and Varga et al. (2005) describe a two-pass algorithm, using a length-based approach for a first alignment. The first alignment subsequently serves as training data for a translation model, which is then used in a complex similarity score. The two approaches differ in that Varga et al. (2005) use a dictionary-based translation model, with a dictionary that can be"
2010.amta-papers.14,moore-2002-fast,0,0.758294,"ain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (2002) and Varga et al. (2005) describe a two-pass algorithm, using a length-based approach for a first alignment. The first alignment subsequently serves as training data for a translation model, which is then used in a complex similarity score. The two approaches differ in that Varga et al. (2005) use a dictionary-based translation model, with a dictionary that can be manually expanded, while Moore (2002) works with a IBM-1 translation model, following Chen (1993). Different degrees of textual and metatextual structure open different possibilities for sentence alignment. Tiedemann (2007) shows tha"
2010.amta-papers.14,J03-1002,0,0.00754503,"the quality of the MT systems used, this should also give an indication as to how good translations need to be for this approach to work. A second question is how SMT performance scales with sentence alignment performance, and whether high-precision or high-recall alignment is preferable. 6.1 Method Except for the Google translations, which were obtained online through Google Translate, all systems were trained using the instructions for building a baseline system by the 2010 ACL Workshop on SMT.11 The SMT systems are built using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). The systems we trained are different from the baseline system described above in that we used various training sets, and did not train a recaser, using a BLEU implementation that ignores case differences. Table 5 describes the training data used for the different MT systems. Translation model (TM) training data is a subset of language model (LM) training data: only sentences with a valid alignment and fewer than 40 words in length are used to train the translation model. Europarl-K uses a sample of 1000 (1K) units of training data; all other systems use all the data available for TM training"
2010.amta-papers.14,P02-1040,0,0.0994516,"t between two hard delimiters (including the beginning and end of file), and is composed of two steps. First, a set of anchor points is identified using BLEU as a similarity score between the translated source text and the target text. In a second step, the sentences between these anchor points are either aligned using BLEU-based heuristics or the length-based algorithm by Gale and Church. 4.1 Using BLEU as Similarity Score BLEU has been developed as an automatic means to measure the translation quality of MT systems by comparing the system translation with one or more reference translations (Papineni et al., 2002). This is done by measuring the token n-gram precision of the system translation (hypothesis) for all n-gram levels up to 4, and combining the n-gram precisions using the geometric mean. The score of hypotheses shorter than the reference is reduced by a brevity penalty. BLEU has been criticised as a measure of translation quality, and it is not considered reliable on a sentence level (Callison-Burch and Osborne, 2006). On the other hand, judging the quality of a translation is a much harder task than deciding whether two sentences are possible translations of each other. We found that BLEU is"
2010.amta-papers.14,W05-0816,0,0.105957,"agraph boundaries and 1to-many alignments. Since we found the performance of existing sentence alignment tools unsatisfactory, we developed an alignment algorithm based on automatic translations of the to-be-aligned texts and BLEU as a similarity score. The approach requires an MT system with a reasonable performance for the language pair, but no other language-specific resources. It is thus fairly language-independent, and as we will show, more robust to textual noise than other approaches. 2 Related Work Overviews of sentence alignment algorithms are provided in (Manning and Sch¨utze, 1999; Singh and Husain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (200"
2010.amta-papers.14,volk-etal-2010-challenges,1,0.536873,", 2004; Adafre and de Rijke, 2006; Yasuda and Sumita, 2008). Adafre and de Rijke (2006) describe an MTbased approach to find corresponding sentences in Wikipedia based on sentence similarity. Our approach is based on the same basic idea of first automatically translating one of the to-be-aligned language portions, and then measuring the similarity between this translation and the other language portion. 3 The Parallel Corpus Our sentence aligner has been developed to align the parallel part of the Text+Berg corpus, a corpus consisting of the yearbooks of the Swiss Alpine Club from 1864–19821 (Volk et al., 2010). Since 1957, the yearbooks are published in both French and German, with most articles being translated between the two languages. Currently, the parallel part of the corpus spans 138 000 sentences with 2.3/2.6 million tokens (German and French, respectively). Some examples from the 1911 yearbook illustrate the diversity. There are the typical reports on mountain expeditions: “Klettereien in der Gruppe der Engelh¨orner” (English: Climbing in the Engelh¨orner group) or “Aus den Hochregionen des Kaukasus” (English: From the high regions of the Caucasus). But the 1911 book also contains scientif"
2010.jec-1.7,2006.tc-1.9,0,0.0884004,"Missing"
2010.jec-1.7,W09-4610,1,0.847215,"prior translations (cf. the average of 57.3 in table 1) is probably due to the fact that subtitles are shorter and grammatically simpler than Europarl and Acquis sentences. 4.4 Linguistic Information in SMT for Subtitles The results reported in tables 1 and 2 are based on a purely statistical MT system. No linguistic knowledge was included. We wondered whether linguistic features such as Part-of-Speech tags or number information (singular vs. plural) could improve our system. We therefore ran a series of experiments to check this hypothesis using factored SMT for Swedish - Danish translation. Hardmeier and Volk (2009) describe these experiments in detail. Here we summarize the main findings. When we used a large training corpus of around 900,000 subtitles or 10 million tokens per language, the gains from adding linguistic information were generally small. Minor improvements were observed when using additional language models operating on part-of-speech tags and tags from morphological analysis. A technique called analytical translation, which enables the SMT system to back off to separate translation of lemmas and morphological tags (provided by Eckhard Bick’s tools) when the main phrase table does not pro"
2010.jec-1.7,2005.mtsummit-papers.11,0,0.0414572,"Missing"
2010.jec-1.7,2006.tc-1.10,0,0.787235,"Missing"
2010.jec-1.7,J04-4002,0,0.115149,"Missing"
2010.jec-1.7,2005.mtsummit-papers.19,0,0.123956,"Missing"
2010.jec-1.7,2001.mtsummit-papers.68,0,0.0877422,"Missing"
2010.jec-1.7,prokopidis-etal-2008-condensing,0,0.090432,"and (D´ıaz-Cintas and Remael, 2007). Gottlieb (2001) and Pedersen (2007) describe the peculiarities of subtitling in Scandinavia, Nagel et al. (2009) in other European countries. 3 Approaches to the Automatic Translation of Film Subtitles In this section we describe other projects on the automatic translation of subtitles.2 We assume subtitles in one language as input and aim at producing an automatic translation of these subtitles into another language. In this paper we do not deal with the conversion of the film transcript into subtitles which requires shortening the original dialogue (cf. (Prokopidis et al., 2008)). We distinguish between rulebased, example-based, and statistical approaches. 3.1 Rule-based MT of Film Subtitles Popowich et al. (2000) provide a detailed account of a MT system tailored towards the translation of English subtitles into Spanish. Their approach is based on a MT paradigm which relies heavily on lexical resources but is otherwise similar to the transfer-based approach. A unification-based parser analyzes the 2 Throughout this paper we focus on TV subtitles, but in this section we deliberately use the term “film subtitles” in a general sense covering both TV and movie subtitles"
2010.jec-1.7,2009.mtsummit-papers.16,0,0.0275594,"lation suggestions. This takes too much time. Suppressing Bad Translations An issue that has followed us throughout the project is the suppression of (presumably) bad translations. While good machine translations considerably increase the productivity of the post-editors, editing bad translations is tedious and frequently slower than translating from scratch. To take away some of this burden from the post-editors, we experimented with a Machine Learning component to predict confidence scores for the individual subtitles output by our Machine Translation systems. Closely following the work by (Specia et al., 2009), we prepared a data set of 4,000 machine-translated subtitles, manually annotated for translation quality on a 1-4 scale by the post-editors. We extracted around 70 features based on the MT input and output, their similarity and the similarity between the input and the MT training data. Then we trained a Partial Least Squares regressor to predict quality scores for unseen subtitles. Like (Specia et al., 2009), we used Inductive Confidence Machines to calibrate the acceptance threshold of our translation quality filter. We found that a confidence filter with the features proposed by Specia et"
2010.jec-1.7,2007.mtsummit-papers.66,1,0.472329,"rpus they have used for evaluating the MT system). This summary indicates that work on the automatic translation of film subtitles with Statistical MT is limited because of the lack of freely available high-quality training data. Our own efforts are based on large proprietary subtitle data and have resulted in mature MT systems. We will report on them in the following section. 4 Our MT Systems for TV Subtitles We have built Machine Translation systems for translating film subtitles from Swedish to Danish and to Norwegian in a commercial setting. Some of this work has been described earlier by Volk and Harder (2007) and Volk (2008). Most films are originally in English and receive Swedish subtitles based on the English video and audio (sometimes accompanied by an English transcript). The creation of the Swedish subtitle is a manual process done by specially trained subtitlers following company-specific guidelines. In particular, the subtitlers set the time codes (beginning and end time) for each subtitle. They use an in-house tool which allows them to link the subtitle to specific frames in the video. The Danish translator subsequently has access to the original English video and audio but also to the Sw"
2010.jec-1.7,P02-1040,0,\N,Missing
2011.eamt-1.29,1999.tmi-1.3,0,0.0226373,"segment receives a score. The best alignment is the one with the lowest score. The alignment score is the weighted sum of the values of eight features, which include: the number of SL words with no correspondences in the TL segment, the number of TL words with no correspondences in the SL fragment, the number of SL words with a correspondence in the TL sentence but not in the relevant TL segment, and the difference in length between the SL and the TL segment. Each translation is passed on to the recombination step as long as its score does not exceed five times the length of the SL fragment. Brown (1999) proposed an extension to CMUEBMT that makes use of semantic and syntactic generalized templates. He referred to the template categories as equivalence classes. Examples of semantic and syntactic equivalence classes are given in Table 2. The table shows that class members can in turn contain classes. This is evident from the last line (shown in bold). The system generalizes both the training and the test set: it recursively replaces words and phrases that are part of an equivalence class with the corresponding class tag. Syntactic classes are applied before semantic classes, and disambiguation"
2011.eamt-1.29,1996.amta-1.35,0,0.195739,"hunk with the name of its category, e. g., of a marathon → <PREP> a marathon. The generalized template extension is not part of the current Marclator system. We reimplemented it for our experiments. 3.2 CMU-EBMT The second EBMT system which we used for our experiments is CMU-EBMT.7 The system forms 4 http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC96L14 5 Note that both word and chunk alignment involve statistical knowledge. 6 This is a common procedure for recombinators that do not incorporate a language model. 7 http://sourceforge.net/projects/ cmu-ebmt/ 211 part of PanLite (Frederking and Brown, 1996), an MT architecture developed at Carnegie-Mellon University (CMU). It can also be invoked on its own. The system requires a parallel corpus and a bilingual dictionary. Brown (1996) used entries from a commercial bilingual dictionary for his experiments in translation from Spanish to English. Unlike Marclator, CMU-EBMT does not require subsentential units to be compiled before the actual translation step. The matching step resembles closely that of a traditional EBMT system: CMU-EBMT extracts every substring of the input sentence with a minimum length of two tokens that appears in the SL half"
2011.eamt-1.29,2003.mtsummit-papers.18,1,0.761469,". Each chunk that is not found in the example base is then split into single words. If several TL correspondences for an SL chunk or word are found in the example base, the one with the highest probability is chosen.6 Thus, for each input sentence, the recombinator outputs a single hypothesis. A problem inherent in the approach described above is that the chunks of an input sentence often cannot be found in the example base. Since translating a chunk as a whole is likely to yield a better translation than translating it word by word, it is desirable to increase the chunk coverage of a system. Gough and Way (2003) extended the precursor to Marclator by including an additional layer of abstraction: they produced generalized chunks from word form chunks by replacing the Marker word at the beginning of a word form chunk with the name of its category, e. g., of a marathon → <PREP> a marathon. The generalized template extension is not part of the current Marclator system. We reimplemented it for our experiments. 3.2 CMU-EBMT The second EBMT system which we used for our experiments is CMU-EBMT.7 The system forms 4 http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC96L14 5 Note that both word and"
2011.eamt-1.29,2004.tmi-1.11,1,0.827846,"tor recombination module: in its original form, the recombination module checks for the presence of matching sentences and word form chunks8 before reverting to word-byword translation. We added an additional matching step to follow the chunk matching: in this step, the system replaces the Marker word at the beginning of a chunk by its corresponding Marker tag and searches for the resulting generalized chunk in the example base. Where this attempt fails, the system reverts to word-by-word translation. The only difference remaining to the approach described in Section 3.1 is that the system of Gough and Way (2004) outputs all possible hypotheses for an input sentence, while the Marclator recombinator only outputs the one-best hypothesis. This means that once our system has 8 We subsequently refer to word form chunks (as opposed to generalized chunks) simply as chunks. established a generalized chunk match with the SL side of the example base and has extracted the corresponding TL generalized chunk, it has to make a decision as to which Marker word to insert for the Marker tag. For this, it identifies the SL Marker word underlying the SL generalized chunk that was matched. It gathers the word alignment"
2011.eamt-1.29,W05-0833,1,0.783819,"Translation (CBMT) paradigm. Hence, both SMT and EBMT rely on a sententially aligned bilingual corpus. EBMT systems make use of the parallel corpus by consulting the training set (their example base) directly at runtime. In contrast, SMT systems consult the probabilities of sourcelanguage–target-language (SL–TL) word or phrase pairs which they have learned from the training data offline. Hence, the main feature that distinguishes the two paradigms is the type of knowledge used during the translation step. EBMT systems have often performed worse than SMT systems in the past (cf., for example, Groves and Way (2005)). The biggest shortcoming of EBMT is that it does not combine translations of phrases well. This problem is known as boundary friction (Way, 2001, p. 2). It is particularly frequent when translating into a morphologically rich language. As an example for translating from English into German, assume that the sentence pairs listed in Example 1 are contained in the example base (Way, 2001). (1) A big dog eats a lot of meat. – Ein großer Hund frisst viel Fleisch. I have two ears. – Ich habe zwei Ohren. An EBMT system might make use of the phrases shown in bold to translate a sentence like I have"
2011.eamt-1.29,C92-2101,0,0.269809,"discussion thereof. In Section 5, we give an overview of the issues which we tackled and offer an outlook on future research questions. 2 Related Work When compiling generalized templates, there is a risk of replacing too many parts of an SL–TL pair with variables. To avoid this risk of overgeneralization, generalized templates are usually restricted to certain categories of words. Common candidates for generalization are content words, as replacing them with other content words does not affect the grammar of the sentence. Semantic generalization was explored by Kitamura and Matsumoto (1995). Kaji et al. (1992) applied semantic constraints to their approach to syntactic generalization. Pure syntactic generalization was performed by G¨uvenir and Tunc (1996). Cicekli and G¨uvenir (2001) generalized over sequences of words. The underlying assumption is that given two SL–TL sentence pairs, if the two SL sentences have certain word form sequences in common, the corresponding TL sentences are expected to exhibit the same similarities among each other. The similar parts of the SL sentences are then assumed to be translations of the similar parts of the TL sentences, and the same applies for the differing p"
2011.eamt-1.29,N03-1017,0,0.0135127,"rsity, Dublin 9, Ireland {away,snaskar}@computing.dcu.ie Abstract In this paper, we report our experiments in combining two EBMT systems that rely on generalized templates, Marclator and CMU-EBMT, on an English–German translation task. Our goal was to see whether a statistically significant improvement could be achieved over the individual performances of these two systems. We observed that this was not the case. However, our system consistently outperformed a lexical EBMT baseline system. 1 Introduction The state-of-the-art approach in MT is phrasebased Statistical Machine Translation (SMT) (Koehn et al., 2003). Together with ExampleBased Machine Translation (EBMT) (Nagao, 1984), SMT belongs to the Corpus-Based Machine Translation (CBMT) paradigm. Hence, both SMT and EBMT rely on a sententially aligned bilingual corpus. EBMT systems make use of the parallel corpus by consulting the training set (their example base) directly at runtime. In contrast, SMT systems consult the probabilities of sourcelanguage–target-language (SL–TL) word or phrase pairs which they have learned from the training data offline. Hence, the main feature that distinguishes the two paradigms is the type of knowledge used during"
2011.eamt-1.29,P07-2045,0,0.00906307,"lator OpenMaTrEx Moses 0.1274 0.1269 0.1277 0.0995 0.2763 0.2709 4.3948 4.3815 4.3937 4.2411 5.7880 5.7472 0.4052 0.4047 0.4051 0.3990 0.4914 0.4854 Table 3: Evaluation scores clusive. There are a number of overlaps, i. e., the CMU classes contain 50 words that are also Marker words for English (e. g., after, and, before), and 19 for German (e. g., aber, allen, er). We prompted the system to generalize over the Marker words first, thereby giving preference to the DCU scheme in case of overlaps. Baselines: We established three baselines: Marclator, OpenMaTrEx (Dandapat et al., 2010) and Moses (Koehn et al., 2007). The Marclator baseline was the purely lexical system described in Section 3.1. For the Moses baseline, we used the default system included in OpenMaTrEx. The system uses a 5-gram language model and modified Kneser-Ney smoothing. Training is performed according to the default options and thus includes tuning via MERT (Och, 2003). In addition, a lexicalized reordering model is learnt. The OpenMaTrEx baseline system makes use of EBMT chunk pairs from Marclator and SMT phrase pairs from Moses. We used the default configuration, which includes a 5-gram language model with modified Kneser-Ney smoo"
2011.eamt-1.29,W04-3250,0,0.0609552,"U, while System 1 performed best according to NIST and METEOR. The three systems outperformed the lexical baseline system Marclator according to all three training data English Marker file German Marker file chunking module chunking module English Marker chunks German Marker chunks word alignment module chunk alignment module aligned sentences aligned chunks generalization module aligned generalized chunks IV. III. I. input aligned words II. output recombination module Figure 1: System 1: training and translation process metrics.10 We measured statistical significance by bootstrap resampling (Koehn, 2004) on BLEU.11 The improvement of System 3 over System 2 is statistically significant, while the improvement of System 3 over System 1 is not. The improvements of Systems 1, 2 and 3 over the baseline Marclator system are all significant, as are the improvements of the baseline OpenMaTrEx and Moses system over Systems 1 to 3. 4.3 Chunk Coverage and Chunk-Internal Boundary Friction The evaluation results in Table 3 show that our generalized EBMT systems achieved higher scores than the lexical EBMT system Marclator. This observation supports earlier findings according to which EBMT systems benefit f"
2011.eamt-1.29,J03-1002,0,0.00330683,"ds were ex2 http://www.openmatrex.org/marclator/ marclator.html 3 http://www.apertium.org/ 210 tracted from the Celex database.4 The lists contain a total of 450 Marker words for English and 550 for German. Table 1 lists a sample Marker word for each category. The examples show that entries are included in their inflected forms. Stroppa and Way (2006) found that treating the punctuation marks ! ? , . : ; as additional Marker elements improved performance in their experiments. Following the chunking of the training data, Marclator performs word and chunk alignment. The system relies on Giza++ (Och and Ney, 2003) for word alignment. The chunk alignment algorithm is an edit-distance style algorithm in which the distances are replaced by opposite-log conditional probabilities (Tinsley et al., 2008).5 The recombinator of Marclator is a left-to-right monotone recombinator. When translating an input sentence, it first looks for a matching sentence in the example base. If none is found, the sentence is chunked. Each chunk that is not found in the example base is then split into single words. If several TL correspondences for an SL chunk or word are found in the example base, the one with the highest probabi"
2011.eamt-1.29,P03-1021,0,0.00534394,"an (e. g., aber, allen, er). We prompted the system to generalize over the Marker words first, thereby giving preference to the DCU scheme in case of overlaps. Baselines: We established three baselines: Marclator, OpenMaTrEx (Dandapat et al., 2010) and Moses (Koehn et al., 2007). The Marclator baseline was the purely lexical system described in Section 3.1. For the Moses baseline, we used the default system included in OpenMaTrEx. The system uses a 5-gram language model and modified Kneser-Ney smoothing. Training is performed according to the default options and thus includes tuning via MERT (Och, 2003). In addition, a lexicalized reordering model is learnt. The OpenMaTrEx baseline system makes use of EBMT chunk pairs from Marclator and SMT phrase pairs from Moses. We used the default configuration, which includes a 5-gram language model with modified Kneser-Ney smoothing and tuning via MERT. We included the optional binary feature that records whether a phrase pair is an EBMT chunk pair or not. To train the language models for Moses and OpenMaTrEx, we used the TL side of the training data. 4.2 Results of the MT Systems Table 3 shows the results of our experiments. The best of our systems (S"
2011.eamt-1.29,2006.iwslt-evaluation.4,1,0.926847,"ctic generalization. Category Example determiner personal pronoun demonstrative pronoun possessive pronoun interrogative pronoun indefinite pronoun relative pronoun preposition coordinative conjunction subordinative conjunction cardinal numeral numeric expression auxiliary/modal verb punctuation den euch jenem seine welch andere denen abseits aber falls eins neunundneunzig darf ! Table 1: German Marker categories and examples 3 3.1 Syntactic and Semantic Generalized Templates EBMT at DCU: Marclator Marclator was developed at Dublin City University (DCU) and is part of the MaTrEx architecture (Stroppa and Way, 2006).2 The system does not apply the greedy matching strategy typical of many EBMT systems. Instead, it segments both the training and the test data into chunks. Chunking is based on the Marker Hypothesis (Green, 1979). This is a psycholinguistic hypothesis stating that every language has a closed set of elements that are used to mark certain syntactic constructions. The set of elements includes function words and bound morphemes, such as -ing as an indicator of English progressive-tense verbs and -ly as an indicator of English adverbs. The Marclator chunking module solely considers function words"
2011.eamt-1.29,2001.mtsummit-ebmt.8,1,0.748398,"ting the training set (their example base) directly at runtime. In contrast, SMT systems consult the probabilities of sourcelanguage–target-language (SL–TL) word or phrase pairs which they have learned from the training data offline. Hence, the main feature that distinguishes the two paradigms is the type of knowledge used during the translation step. EBMT systems have often performed worse than SMT systems in the past (cf., for example, Groves and Way (2005)). The biggest shortcoming of EBMT is that it does not combine translations of phrases well. This problem is known as boundary friction (Way, 2001, p. 2). It is particularly frequent when translating into a morphologically rich language. As an example for translating from English into German, assume that the sentence pairs listed in Example 1 are contained in the example base (Way, 2001). (1) A big dog eats a lot of meat. – Ein großer Hund frisst viel Fleisch. I have two ears. – Ich habe zwei Ohren. An EBMT system might make use of the phrases shown in bold to translate a sentence like I have a big dog. into Ich habe ein großer Hund. In doing so, it would neglect the fact that German uses different inflectional forms to mark grammatical"
2011.eamt-1.29,C96-1030,0,\N,Missing
2011.eamt-1.29,W08-0326,1,\N,Missing
2011.eamt-1.29,W10-1720,1,\N,Missing
2011.jeptalnrecital-court.17,2010.amta-papers.14,1,0.871482,"Missing"
2011.jeptalnrecital-court.17,volk-etal-2010-challenges,1,0.87881,"Missing"
2011.jeptalnrecital-court.17,W10-1830,1,0.891027,"Missing"
2012.eamt-1.2,2010.jec-1.7,1,0.894254,"Missing"
2013.mtsummit-posters.9,W12-3102,0,0.256094,"ity. (b) size 8.5 J x 19 at front, size 11 J x 19 at rear, with 235/35 R 19 tires at front and 295/30 R 19 tires at rear Tailoring SMT systems to producing good pretranslations for our entire domain is thus challenging. Before describing our corresponding experiments in Section 4, we outline the technical setup and report on the performance of simple baseline systems in our use case. 3 3.1 Baseline Translation Systems Technical Setup The technical setup of all experiments closely resembles the setup of the baseline systems in the shared task of the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The only difference is that we used IRSTLM for language modeling (Federico et al., 2008) because of licensing issues. We used a test set of 500 in-domain segments for automatic evaluation; these were randomly drawn from contracts that were processed after compiling the training and development sets (see section 3.2). Using a moderate test set size enabled detailed manual inspection and categorization of the machine translations, e.g., identifying the number of compounds in out-of-vocabulary (OOV) types. All automatic metric scores were calculated using multeval (Clark et al., 2011). Unless o"
2013.mtsummit-posters.9,P11-2031,0,0.0301857,"(Callison-Burch et al., 2012). The only difference is that we used IRSTLM for language modeling (Federico et al., 2008) because of licensing issues. We used a test set of 500 in-domain segments for automatic evaluation; these were randomly drawn from contracts that were processed after compiling the training and development sets (see section 3.2). Using a moderate test set size enabled detailed manual inspection and categorization of the machine translations, e.g., identifying the number of compounds in out-of-vocabulary (OOV) types. All automatic metric scores were calculated using multeval (Clark et al., 2011). Unless otherwise stated, the scores are averages over five MERT runs (Och, 2003; Bertoldi et al., 2009). METEOR scores are only given for DE–FR systems since Italian is not fully supported (Denkowski and Lavie, 2011). As SemioticTransfer’s translation workflow is based on the Across workbench1 , we implemented an RPC layer that allows for integrating Moses server instances into Across. In this way, we were able to apply various pre- and post-processing methods to translations while ensuring seamless integration of the Moses systems as a pre-translation service into SemioticTransfer’s existin"
2013.mtsummit-posters.9,W11-2107,0,0.0304458,"luation; these were randomly drawn from contracts that were processed after compiling the training and development sets (see section 3.2). Using a moderate test set size enabled detailed manual inspection and categorization of the machine translations, e.g., identifying the number of compounds in out-of-vocabulary (OOV) types. All automatic metric scores were calculated using multeval (Clark et al., 2011). Unless otherwise stated, the scores are averages over five MERT runs (Och, 2003; Bertoldi et al., 2009). METEOR scores are only given for DE–FR systems since Italian is not fully supported (Denkowski and Lavie, 2011). As SemioticTransfer’s translation workflow is based on the Across workbench1 , we implemented an RPC layer that allows for integrating Moses server instances into Across. In this way, we were able to apply various pre- and post-processing methods to translations while ensuring seamless integration of the Moses systems as a pre-translation service into SemioticTransfer’s existing infrastructure. 3.2 Baseline Systems SemioticTransfer has been using Across for several years, through which they have accumulated a lot of quality-checked translations in their translation memories. We extracted all"
2013.mtsummit-posters.9,N09-1046,0,0.146612,"ng” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively high OOV rates suggest adding bigger general-domain"
2013.mtsummit-posters.9,P08-1115,0,0.0346988,"n method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively high OOV rates suggest adding bigger general-domain corpora to the training material. The specific nature of the indomain data makes it necessary to use domain adaptation techniques, to avoid having the bigger general-domain data override the original Metric Mode Avg ssel BLEU ↑ In-Domain Only Domai"
2013.mtsummit-posters.9,2011.mtsummit-papers.25,0,0.0163812,"sed on the translation system. The focus of the paper is the suitability of such texts for SMT; we present experiments in domain adaptation and decompounding that improve the baseline translation systems, the results of which are evaluated using automatic metrics as well as manual evaluation. 1 Introduction As machine translation and post-editing are gaining popularity on an industrial level, global companies turn to using their accumulated translations for setting up in-house SMT services. Substantial cost and time savings have been reported in various sectors, such as software localization (Flournoy, 2011; Zhechev, 2012) or film and television subtitling (Volk et al., 2010). In a joint project between the University of Zurich and SemioticTransfer AG, we target a new domain: automobile marketing texts. Martin Volk1 SemioticTransfer AG Bruggerstrasse 37 CH-5400 Baden manuela.weibel @semiotictransfer.ch We show that even limited amounts of indomain translation material allow for building domain-specific SMT systems (see Section 3), and that translation quality can be significantly improved by using out-of-domain material and language-specific preprocessing (see Section 4). Our aim is to give an e"
2013.mtsummit-posters.9,W07-0717,0,0.0332822,"s. OOV rates and automatic evaluation metrics refer to a test set of 500 in-domain segments (see Section 3.1). quality by reducing the rate of OOV input types. This was done by adding generaldomain corpora and combining them with our in-domain data via domain adaptation, as well as by decompounding methods on both the indomain data and the mixed-domain set. 4.1 Related Work Domain adaptation has been applied to most components of statistical machine translation: language models (Clarkson and Robinson, 1997; Koehn and Schroeder, 2007), word alignment (Hua et al., 2005), and translation models (Foster and Kuhn, 2007; Sennrich, 2012). Combinations of these methods often show that there is an overlap in the translation problems that the methods fix, which leads to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on t"
2013.mtsummit-posters.9,W10-1710,0,0.0618482,"problems that the methods fix, which leads to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively"
2013.mtsummit-posters.9,P05-1058,0,0.0753085,"Missing"
2013.mtsummit-posters.9,2005.mtsummit-papers.11,0,0.0358338,"ssel 0.1 0.2 0.2 0.3 0.00 + 0.00 + 0.00 – 32.3 33.0 33.2 33.2 1.4 1.5 1.5 1.5 0.3 0.1 0.1 0.2 0.00 + 0.40 0.74 1.2 1.2 1.2 1.2 0.3 0.6 0.4 0.5 0.00 + 0.00 + 0.00 – 55.9 53.3 53.1 53.7 1.4 1.3 1.3 1.4 0.6 0.5 0.3 0.4 0.00 + 0.53 0.00 – 1.1 1.1 1.1 1.1 0.1 0.2 0.2 0.3 0.00 + 0.00 + 0.80 p-value Table 2: Automatic evaluation of the baseline and combined systems. p-values are relative to the preceding system and indicate whether a score improves (+) or decreases (–) significantly. domain-specific translations. We used two freely available out-of-domain corpora for these experiments: Europarl v7 (Koehn, 2005) and OpenSubtitles (Tiedemann, 2009). Stand-alone systems trained on these corpora result in unusable translations with very low scores (see Table 1). For domain adaptation, we used multidomain mixture-modeling (Foster and Kuhn, 2007) for language and translation models. The main distinctive feature of that approach is that instead of a binary in-domain/out-ofdomain treatment of the data sets, each separate domain is assigned a weight, reflecting its similarity to in-domain (parallel) texts. Mixture-modeling and optimization of these weights is implemented in IRSTLM for language models (Federi"
2013.mtsummit-posters.9,E03-1076,0,0.307702,"ds to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively high OOV rates suggest adding bigger g"
2013.mtsummit-posters.9,W07-0733,0,0.024757,".4 73.0 Table 1: Training data for in- and out-of-domain language and translation models. OOV rates and automatic evaluation metrics refer to a test set of 500 in-domain segments (see Section 3.1). quality by reducing the rate of OOV input types. This was done by adding generaldomain corpora and combining them with our in-domain data via domain adaptation, as well as by decompounding methods on both the indomain data and the mixed-domain set. 4.1 Related Work Domain adaptation has been applied to most components of statistical machine translation: language models (Clarkson and Robinson, 1997; Koehn and Schroeder, 2007), word alignment (Hua et al., 2005), and translation models (Foster and Kuhn, 2007; Sennrich, 2012). Combinations of these methods often show that there is an overlap in the translation problems that the methods fix, which leads to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their"
2013.mtsummit-posters.9,P03-1021,0,0.0241122,"ling (Federico et al., 2008) because of licensing issues. We used a test set of 500 in-domain segments for automatic evaluation; these were randomly drawn from contracts that were processed after compiling the training and development sets (see section 3.2). Using a moderate test set size enabled detailed manual inspection and categorization of the machine translations, e.g., identifying the number of compounds in out-of-vocabulary (OOV) types. All automatic metric scores were calculated using multeval (Clark et al., 2011). Unless otherwise stated, the scores are averages over five MERT runs (Och, 2003; Bertoldi et al., 2009). METEOR scores are only given for DE–FR systems since Italian is not fully supported (Denkowski and Lavie, 2011). As SemioticTransfer’s translation workflow is based on the Across workbench1 , we implemented an RPC layer that allows for integrating Moses server instances into Across. In this way, we were able to apply various pre- and post-processing methods to translations while ensuring seamless integration of the Moses systems as a pre-translation service into SemioticTransfer’s existing infrastructure. 3.2 Baseline Systems SemioticTransfer has been using Across for"
2013.mtsummit-posters.9,E12-1055,0,0.0755085,"tic evaluation metrics refer to a test set of 500 in-domain segments (see Section 3.1). quality by reducing the rate of OOV input types. This was done by adding generaldomain corpora and combining them with our in-domain data via domain adaptation, as well as by decompounding methods on both the indomain data and the mixed-domain set. 4.1 Related Work Domain adaptation has been applied to most components of statistical machine translation: language models (Clarkson and Robinson, 1997; Koehn and Schroeder, 2007), word alignment (Hua et al., 2005), and translation models (Foster and Kuhn, 2007; Sennrich, 2012). Combinations of these methods often show that there is an overlap in the translation problems that the methods fix, which leads to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of"
2013.mtsummit-posters.9,2010.jec-1.7,1,0.909451,"Missing"
2013.mtsummit-posters.9,W12-3157,0,0.0127644,"ounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively high OOV rates suggest adding bigger general-domain corpora to the training material. The specific nature of the indomain data makes it necessary to use domain adaptation techniques, to avoid having the bigger general-domain data override the original Metric Mode Avg ssel BLEU ↑ In-Domain Only Domain Adaptation (DA) DA + Decompounding"
2013.mtsummit-posters.9,2012.amta-wptp.10,0,0.040833,"lation system. The focus of the paper is the suitability of such texts for SMT; we present experiments in domain adaptation and decompounding that improve the baseline translation systems, the results of which are evaluated using automatic metrics as well as manual evaluation. 1 Introduction As machine translation and post-editing are gaining popularity on an industrial level, global companies turn to using their accumulated translations for setting up in-house SMT services. Substantial cost and time savings have been reported in various sectors, such as software localization (Flournoy, 2011; Zhechev, 2012) or film and television subtitling (Volk et al., 2010). In a joint project between the University of Zurich and SemioticTransfer AG, we target a new domain: automobile marketing texts. Martin Volk1 SemioticTransfer AG Bruggerstrasse 37 CH-5400 Baden manuela.weibel @semiotictransfer.ch We show that even limited amounts of indomain translation material allow for building domain-specific SMT systems (see Section 3), and that translation quality can be significantly improved by using out-of-domain material and language-specific preprocessing (see Section 4). Our aim is to give an example of how ev"
2013.mtsummit-wptp.10,aziz-etal-2012-pet,0,0.0868482,"nto French: A reference translation produced by a specialized translator prior to our experiment and translations by participants in the TM-Only and Post-Edit conditions as well as the English gloss. project is to build a domain-specific machine translation system for the LSP to use in a post-editing scenario. There have been a number of studies that assess the efficiency of post-editing (e.g., Guerberof, 2009; Sousa et al., 2011; Green et al., 2013). Most of these set up controlled environments for their experiments and develop specially tailored user interfaces for post-editing tasks (e.g., Aziz et al., 2012). The main reason for this is the priority placed on precise measurements of translation time, pause durations and input device activities. Some evaluations of post-editing in an industrial context have also been reported. For example, Plitt and Masselot (2010) replace manual translation with post-editing for software localization. Other industry-oriented studies (Volk et al., 2010; Flournoy, 2011) focus more on the challenges of deploying machine translation in the respective sector or company. Our work strives to combine the key elements of these two approaches: We ensure precise time and ac"
2013.mtsummit-wptp.10,W12-3102,0,0.0235574,"condition. 5.2 Pairwise Ranking In addition, we tested whether the participants (P1–P6) prefer professional translations produced by the LSP staff over those produced in the study. We applied a pairwise ranking procedure4 in which evaluators compare two translations ht1 , t2 i of a given segment in the source language and choose the better fit, with ties allowed. We used six random German segments from each text (A–D) and had all participants compare the corresponding professional translation to those produced by all 4 A similar procedure was used at the 2012 Workshop for Machine Translation (Callison-Burch et al., 2012). In contrast to other human evaluation metrics such as fluency and adequacy judgments on ordinal scales, pairwise rankings are usually more comprehensible and better reproducible for non-expert evaluators. Condition TM-Only Post-Edit Wins Ties Losses p-value 112 128 94 96 154 136 0.012 0.667 Table 5: Pairwise ranking of translations produced in the study against professional reference translations. p-values indicate genuine differences between the number of wins and losses (Sign Test). other participants, such that each participant evaluated 120 htprof essional , tparticipanti i tuples in tot"
2013.mtsummit-wptp.10,W12-3123,0,0.119063,"the catalogue [to be translated] I would probably prefer the mode with pre-translations. P3 indicated that the machine translations were helpful for translating difficult texts in terms of vocabulary, [...] but I think [for translating] the two easiest texts [...], the pre-translations would have only confused me. This stands in sharp contrast to the fact that postediting resulted in significantly faster and even slightly better translations in our study. However, the discrepancy between translators’ perceptions and post-editing performance is a well-known phenomenon in the field (see, e.g., Koponen, 2012). On the other hand, our participants were by no means technology-averse in general: All of them used various computer-aided translation tools in the tasks and deemed both the domain-specific translation memory and the bilingual terminology database as either “sometimes useful” (3/3), “useful” (1/2), or “very useful” (2/1). 7 Conclusion We have proposed a design for translation efficiency experiments that compares post-editing to computer-aided translation using a fully-featured translation workbench. In contrast to the simplified user interfaces deployed in other studies (e.g., Sousa et al.,"
2013.mtsummit-wptp.10,2013.mtsummit-posters.9,1,0.825903,"ranslators. The participants were asked to translate the German source text (TM-Only) or revise the French MT output (Post-Edit) as needed to produce high-quality French target texts. They were encouraged to work however they wanted and had access to the fuzzy TM matches, the terminology database, and online resources. Pre-edit translation drafts were produced by a domain-specific statistical machine translation system. It was built using the same translation memory data that was used for the present study, as well as out-of-domain parallel corpora; a more detailed description can be found in Läubli et al. (2013a,b). We implemented a simple RPC-based software link to enable seamless integration of the translation system into the translation workbench. The German source texts for the translation tasks were provided by the LSP. We selected four typical texts (A–D) that cover specific aspects of our domain in scope (see Table 2). Since all of the texts had been translated by professional translators at the LSP in their normal workflow, we also had access to the corresponding reference translations into French. This allowed us to compare the output of translators experienced in the automobile industry te"
2013.mtsummit-wptp.10,R11-1014,0,0.0967918,"Missing"
2013.mtsummit-wptp.10,2010.jec-1.7,1,0.893471,"Missing"
2013.mtsummit-wptp.10,2012.eamt-1.31,0,\N,Missing
2013.mtsummit-wptp.10,2012.tc-1.5,0,\N,Missing
2015.eamt-1.27,abeille-barrier-2004-enriching,0,0.101771,"Missing"
2015.eamt-1.27,E14-1061,0,0.0232107,"challenge of this task lies in the desired translation direction, namely from French into German. As the target language is morphologically richer than the source language, we exc 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Martin Volk1 SWISS TXT Schweizerische Teletext AG Alexander-Sch¨oni-Strasse 40 CH-2501 Biel gion.linder@swisstxt.ch pect difficulties in generating grammatically correct output. This drawback can be overcome by means of hierarchical models (Huck et al., 2013), improved morphological processing (Cap et al., 2014) or models enriched with part-of-speech (POS) information (St¨uker et al., 2011). Another known issue with translations into German is the word order (e.g. the long-range disposal of separable prefix verbs or composed tenses), which can result in missing verbs or verb particles in the translated output. A general solution when translating between languages with different word order is to reorder the source texts according to the word order in the target language, as suggested by Niehues and Kolls (2009). In this paper we investigate how well these techniques can be applied for subtitles and we"
2015.eamt-1.27,2013.mtsummit-posters.9,1,0.899286,"Missing"
2015.eamt-1.27,W09-0435,0,0.0843548,"Missing"
2015.eamt-1.27,P03-1021,0,0.00631527,"0 FR Words 20,853,000 8,760,000 2,881,000 144,000 14,800 3,200 the main clause (due to length restrictions), we had to join the subtitles in order for the reordering to be effective. 3.3 Results Table 1: The size of the German-French data sets website, with the difference that we lowercase the data instead of truecasing it7 . The model combinations (phrase table combination, language model interpolation) are generated with the tools available in the Moses distribution. The parameters of the global models are optimized through Minimum Error Rate Training (MERT) on an in-domain development set (Och, 2003). The translation performance is measured in terms of several evaluation metrics on a single reference translation using multeval8. Since the collected data sets are very heterogeneous, training a system on concatenated data did not make any sense because we would risk that bigger corpora overpower the small in-domain one. To avoid this, we make use of a common domain adaptation technique, namely mixturemodeling (Sennrich, 2012), and we apply it to both the translation and the language models. The components of the combined translation models have been trained independently on the correspondin"
2015.eamt-1.27,E12-1055,0,0.0334346,"the tools available in the Moses distribution. The parameters of the global models are optimized through Minimum Error Rate Training (MERT) on an in-domain development set (Och, 2003). The translation performance is measured in terms of several evaluation metrics on a single reference translation using multeval8. Since the collected data sets are very heterogeneous, training a system on concatenated data did not make any sense because we would risk that bigger corpora overpower the small in-domain one. To avoid this, we make use of a common domain adaptation technique, namely mixturemodeling (Sennrich, 2012), and we apply it to both the translation and the language models. The components of the combined translation models have been trained independently on the corresponding parallel corpora (OPUS, TED etc.), whereas the language models are trained on the target side of these corpora. The Hierarchical system is trained by the same principles, but uses hierarchical models instead of plain phrase-based models. Such models learn translation rules from parallel data by means of probabilistic synchronous context-free grammars and are able to handle languages with different word order. The Improved syst"
2015.eamt-1.27,W13-2258,0,\N,Missing
2020.readi-1.7,2020.lrec-1.404,1,0.734187,"Missing"
2020.readi-1.7,Q17-1010,0,0.0352145,"Missing"
2020.readi-1.7,W14-1206,0,0.263777,"kar et al., 1996) and since then has been approached by means of rule-based and statistical methods. As part of a rule-based approach, the operations carried out typically include replacing complex lexical and syntactic units by simpler ones. A statistical approach generally conceptualizes the simplification task as one of converting a standardlanguage into a simplified-language text using machine translation techniques. Research on automatic text simplification has been documented for English (Zhu et al., 2010), Spanish (Saggion et al., 2015), Portuguese (Aluisio and Gasperin, 2010), French (Brouwers et al., 2014), and Italian (Barlacchi and Tonelli, 2013). To the authors’ knowledge, the work of Suter (2015) and Suter et al. (2016), who presented a prototype of a rulebased text simplification system, is the only proposal for German. The paper at hand presents the first experiments in datadriven simplification for German, relying on neural machine translation. The data consists of news items manually simplified according to a well-known set of guidelines. Hence, the contribution of the paper is twofold: 2 2.1 Parallel Corpora and Alignment Tools for Automatic Text Simplification Parallel Corpora Automat"
2020.readi-1.7,C96-2183,0,0.510244,"Simplified language is a variety of standard language characterized by reduced lexical and syntactic complexity, the addition of explanations for difficult concepts, and clearly structured layout.1 Among the target groups of simplified language are persons with cognitive impairment and learning disabilities, prelingually deaf persons, functionally illiterate persons, and foreign language learners (Bredel and Maaß, 2016). Automatic text simplification, the process of automatically producing a simplified version of a standard-language text, was initiated in the late 1990s (Carroll et al., 1998; Chandrasekar et al., 1996) and since then has been approached by means of rule-based and statistical methods. As part of a rule-based approach, the operations carried out typically include replacing complex lexical and syntactic units by simpler ones. A statistical approach generally conceptualizes the simplification task as one of converting a standardlanguage into a simplified-language text using machine translation techniques. Research on automatic text simplification has been documented for English (Zhu et al., 2010), Spanish (Saggion et al., 2015), Portuguese (Aluisio and Gasperin, 2010), French (Brouwers et al.,"
2020.readi-1.7,W11-1601,0,0.0338048,"a paragraph or sentence to obtain the final vector for the respective text unit. CWASA is based on the alignment of continuous words using directed edges. • Alignment strategy: CATS allows for adhering to a monotonicity restriction, i.e., requiring the order of information to be identical on the standard-language and simplified-language side, or abandoning it. 3 Data-Driven Automatic Text Simplification 4 Specia (2010) introduced statistical machine translation to the automatic text simplification task, using data from a small parallel corpus (roughly 4,500 parallel sentences) for Portuguese. Coster and Kauchak (2011) used the original PWKP Corpus (cf. Section 2.1) to train a machine translation system. Xu et al. (2016) performed syntax-based statistical machine translation on the English/simplified English part of the Newsela Corpus. 4.1 Automatic Text Simplification for German Training Data All data used in our experiments was taken from the Austria Press Agency (Austria Presse Agentur, APA) corpus built by our group. At this press agency, four to six news items covering the topics of politics, economy, culture, and sports are manually simplified into two language levels, B1 and A2, each day following th"
2020.readi-1.7,L18-1550,0,0.0295661,"Missing"
2020.readi-1.7,E17-3017,0,0.0180071,"trainings, it became clear that all of these models overfit quickly. Validation perplexity regularly reached its minimum before sentences of any kind of fluency were produced, and BLEU scores only started to increase after this point. Therefore, we decided to optimize for the BLEU score instead, i.e., stop training when BLEU scores on the validation set reached the maximum. We will discuss more specific implications of this decision in Section 4.4. All models in our experiments are based on the Transformer encoder-decoder architecture (Vaswani et al., 2017). We used Sockeye version 1.18.106 (Hieber et al., 2017) for training and translation into simplified German. Unless otherwise stated, the hyperparameters are defaults defined by Sockeye. The following is an overview of the models: BASE baseline model; embedding size of 256 BPE 5 K same as BASE but with less BPE merge operations (10,000 → 5,000) (Sennrich and Zhang, 2019) BATCH 1 K same as BASE but with a smaller token-based batch size (4096 → 1024) (Sennrich and Zhang, 2019) LINGFEAT same as BASE but extending embedding vectors with additional linguistic features (lemmas, partof-speech tags, morphological attributes, dependency tags, and BIEO tags"
2020.readi-1.7,W13-2902,1,0.79629,"al matches between the two Wikipedia versions. Another frequently used data collection, available for English and Spanish, is the Newsela Corpus (Xu et al., 2015) consisting of 1,130 news articles, each simplified into four school grade levels by professional editors. 1. Introducing a parallel corpus as data for automatic text simplification for German 2. Establishing a benchmark for automatic text simplification for German 1 The term plain language is avoided, as it refers to a specific level of simplification. Simplified language subsumes all efforts of reducing the complexity of a text. 41 Klaper et al. (2013) created the first parallel corpus for German/simplified German, consisting of 256 texts each (approximately 70,000 tokens) downloaded from the Web. More recently, Battisti et al. (2020) extended the corpus to 6,200 documents (nearly 211,000 sentences). The above-mentioned PorSimples and Newsela corpora present standard-language texts simplified into multiple levels, thus accounting for a recent consensus in the area of simplified-language research, according to which a single level of simplified language is not sufficient; instead, multiple levels are required to account for the heterogeneous"
2020.readi-1.7,P17-2014,0,0.0181227,"ed language is not sufficient; instead, multiple levels are required to account for the heterogeneous target usership. For simplified German, capito,2 the largest provider of simplification services (translations and translators’ training) in Austria, Germany, and Switzerland, distinguishes between three levels along the Common European Framework of Reference for Languages (CEFR) (Council of Europe, 2009): A1, A2, and B1.3 Each level is linguistically operationalized, i.e., specified with respect to linguistic constructions permitted or not permitted at the respective level. 2.2 Nisioi et al. (2017) introduced neural sequence-tosequence models to automatic text simplification, performing experiments on both the Wikipedia dataset of (Hwang et al., 2015) and the Newsela Corpus for English, with automatic alignments derived from CATS (cf. Section 2.2). The authors used a Long Short-term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) as instance of Recurrent Neural Networks (RNNs). Surya et al. (2019) proposed an unsupervised or partially supervised approach to text simplification. Their model is based on a neural encoder-decoder but differs from previous approaches by adding"
2020.readi-1.7,W19-2305,0,0.0352838,"Missing"
2020.readi-1.7,P02-1040,0,0.108258,"parallel data as well. However, the authors’ results prove that some parallel data is still essential. Finally, Palmero Aprosio et al. (2019) experimented with data augmentation methods for low-resource text simplification for Italian. Their unaugmented dataset is larger than the one presented in this paper but includes more lowquality simplifications due to automatic extraction of simplified sentences from the Web. Our work differs in that we benchmark and compare a wider variety of low-resource methods. The most commonly applied automatic evaluation metrics for text simplification are BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). BLEU, the de-facto standard metric for machine translation, computes token n-gram overlap between a hypothesis and one or multiple references. A shortcoming of BLEU with respect to automatic text simplification is that it rewards hypotheses that do not differ from the input. By contrast, SARI was designed to punish such output. It does so by explicitly considering the input and rewarding tokens in the hypothesis that do not occur in the input but in one of the references (addition) and tokens in the input that are retained (copying) or removed (deletion) in both th"
2020.readi-1.7,W18-6319,0,0.0115812,"ize of the training set (Sennrich et al., 2016a) TRG 2 TRG same as BASE but with additional target-totarget sentence pairs (same simplified sentence in source as in target), doubling the size of the training set (Palmero Aprosio et al., 2019) (cf. Section 3) BT 2 TRG same as BASE but with additional backtranslatedto-target sentence pairs (source sentence is machinetranslated from target sentence), doubling the size of the training set (Sennrich et al., 2016a) 4.3 Results of Our Simplication Experiments We report case-insensitive BLEU and SARI on the validation set, calculated using SacreBLEU (Post, 2018). Since we optimized the models for the BLEU score, these values may be taken as a kind of “upper bound” rather than true indicators of their performance. Figure 1 shows results for the models listed in Section 4.2. TRG 2 TRG is the only model whose improvements compared to the baseline reached high statistical significance (p = 0.00014 for BLEU, p = 0.00050 for SARI), although improvements by LINGFEAT look promising (p = 0.10 for BLEU, p = 0.020 for SARI). The low performance of BT 2 TRG is surprising, considering the significant BLEU score improvements we observed in a previous experiment wi"
2020.readi-1.7,W16-2209,0,0.0400894,"Missing"
2020.readi-1.7,sennrich-kunz-2014-zmorge,0,0.0112505,"), although improvements by LINGFEAT look promising (p = 0.10 for BLEU, p = 0.020 for SARI). The low performance of BT 2 TRG is surprising, considering the significant BLEU score improvements we observed in a previous experiment with a different German dataset (Battisti et al., 2020). BPE 5 K and BATCH 1 K , both proposed as low-resource optimizations in machine translation, do not have much of an effect in this context, either. For LINGFEAT, all linguistic features were obtained with ParZu (Sennrich et al., 2013), using clevertagger (Sennrich et al., 2013) for part-of-speech tags and Zmorge (Sennrich and Kunz, 2014) for morphological analysis. The embedding sizes for these features are: 221 for lemmas, 10 each for part-of-speech, morphology, and dependency tags, and 5 for subword BIEO tags, thus extending the total embedding size to 512. 36 8 35 6 SARI 4 33 32 2 BT 2 TRG TRG 2 TRG NULL 2 TRG LINGFEAT BATCH 1 K 30 BT 2 TRG TRG 2 TRG NULL 2 TRG LINGFEAT BATCH 1 K BPE 5 K 0 BPE 5 K 31 BASE BLEU 34 BASE 4.2 Figure 1: BLEU and SARI scores on the validation set (means and standard errors from three runs) 44 BASE BASE + LINGFEAT BLEU SARI 2.23 ± 0.55 2.94 ± 0.60 31.87 ± 0.46 33.13 ± 0.56 + TRG 2 TRG BLEU SARI 7"
2020.readi-1.7,P19-1021,0,0.036794,"Missing"
2020.readi-1.7,R13-1079,1,0.71431,"pared to the baseline reached high statistical significance (p = 0.00014 for BLEU, p = 0.00050 for SARI), although improvements by LINGFEAT look promising (p = 0.10 for BLEU, p = 0.020 for SARI). The low performance of BT 2 TRG is surprising, considering the significant BLEU score improvements we observed in a previous experiment with a different German dataset (Battisti et al., 2020). BPE 5 K and BATCH 1 K , both proposed as low-resource optimizations in machine translation, do not have much of an effect in this context, either. For LINGFEAT, all linguistic features were obtained with ParZu (Sennrich et al., 2013), using clevertagger (Sennrich et al., 2013) for part-of-speech tags and Zmorge (Sennrich and Kunz, 2014) for morphological analysis. The embedding sizes for these features are: 221 for lemmas, 10 each for part-of-speech, morphology, and dependency tags, and 5 for subword BIEO tags, thus extending the total embedding size to 512. 36 8 35 6 SARI 4 33 32 2 BT 2 TRG TRG 2 TRG NULL 2 TRG LINGFEAT BATCH 1 K 30 BT 2 TRG TRG 2 TRG NULL 2 TRG LINGFEAT BATCH 1 K BPE 5 K 0 BPE 5 K 31 BASE BLEU 34 BASE 4.2 Figure 1: BLEU and SARI scores on the validation set (means and standard errors from three runs) 44"
2020.readi-1.7,P16-1009,0,0.256083,"et is already very small German Simplified German 3316 300 50 Alignment Usage 3316 300 1:1, 1:n, n:1 1:1 training validation 3316 – data augmentation – evaluation Table 2: Number of sentences from the Austria Press Agency (APA) corpus in our experiments and the automatic alignments are not perfect, we decided not to use a parallel test set but to select models based on their best performance on the validation set and evaluate manually without a target reference. We chose the number of sentences for data augmentation to match the number of parallel sentences during training, in accordance with Sennrich et al. (2016a). We applied the following preprocessing steps: • In the simplified German text, we replaced all hyphenated compounds (e.g., Premier-Ministerin ‘female prime minister’) with their unhyphenated equivalents (Premierministerin), but only if they never occur in hyphenated form in the original German corpus. • We converted all tokens to lowercase. This reduces the subword vocabulary and ideally makes morpheme/subword correspondences more explicit across different parts of speech, since nouns are generally capitalized in German orthography. 4 https://fasttext.cc/docs/en/ crawl-vectors.html (last a"
2020.readi-1.7,P16-1162,0,0.244437,"et is already very small German Simplified German 3316 300 50 Alignment Usage 3316 300 1:1, 1:n, n:1 1:1 training validation 3316 – data augmentation – evaluation Table 2: Number of sentences from the Austria Press Agency (APA) corpus in our experiments and the automatic alignments are not perfect, we decided not to use a parallel test set but to select models based on their best performance on the validation set and evaluate manually without a target reference. We chose the number of sentences for data augmentation to match the number of parallel sentences during training, in accordance with Sennrich et al. (2016a). We applied the following preprocessing steps: • In the simplified German text, we replaced all hyphenated compounds (e.g., Premier-Ministerin ‘female prime minister’) with their unhyphenated equivalents (Premierministerin), but only if they never occur in hyphenated form in the original German corpus. • We converted all tokens to lowercase. This reduces the subword vocabulary and ideally makes morpheme/subword correspondences more explicit across different parts of speech, since nouns are generally capitalized in German orthography. 4 https://fasttext.cc/docs/en/ crawl-vectors.html (last a"
2020.readi-1.7,L18-1479,0,0.0135013,"nd apparently, the model partly succeeded in combining knowledge from this trivial copying job with knowledge about sentence shortening and lexical simplification, as demonstrated by Examples 1–3. In higher-resource scenarios, a frequent problem is that neural machine translation systems used for text simplification tasks are “over-conservative” (Sulem et al., 2018; Wubben et al., 2012), i.e., they tend to copy the input without simplifying anything. One possible solution to this is to enforce a less probable output during decoding, which is ˇ more likely to contain some changes to the input (Stajner and Nisioi, 2018). However, in the present setting, it is 45 BASE + LINGFEAT TRG 2 TRG TRG 2 TRG + LINGFEAT BT 2 TRG 50 40 30 20 10 BT 2 TRG + LINGFEAT 0 Content preservation 0 1 2 3 BASE Sentences 0 1 2 3 BASE Fluency BASE + LINGFEAT TRG 2 TRG TRG 2 TRG + LINGFEAT BT 2 TRG 50 40 30 20 10 0 BT 2 TRG + LINGFEAT Sentences 0 1 2 3 Simplicity BASE BASE + LINGFEAT TRG 2 TRG TRG 2 TRG + LINGFEAT BT 2 TRG Sentences Figure 2: Human evaluation results Criterion Values content preservation 0 1 2 3 no content preserved general topic preserved, but wrong in specifics main statement recognizable, but wrong in details all r"
2020.readi-1.7,L18-1615,0,0.0609693,"Missing"
2020.readi-1.7,P18-1016,0,0.105128,"ting the data with simple-to-simple pairs was relatively successful shows that the main difficulty for the other models was finding relevant correspondences between source and target. In the augmented data, these correspondences are trivial to find, and apparently, the model partly succeeded in combining knowledge from this trivial copying job with knowledge about sentence shortening and lexical simplification, as demonstrated by Examples 1–3. In higher-resource scenarios, a frequent problem is that neural machine translation systems used for text simplification tasks are “over-conservative” (Sulem et al., 2018; Wubben et al., 2012), i.e., they tend to copy the input without simplifying anything. One possible solution to this is to enforce a less probable output during decoding, which is ˇ more likely to contain some changes to the input (Stajner and Nisioi, 2018). However, in the present setting, it is 45 BASE + LINGFEAT TRG 2 TRG TRG 2 TRG + LINGFEAT BT 2 TRG 50 40 30 20 10 BT 2 TRG + LINGFEAT 0 Content preservation 0 1 2 3 BASE Sentences 0 1 2 3 BASE Fluency BASE + LINGFEAT TRG 2 TRG TRG 2 TRG + LINGFEAT BT 2 TRG 50 40 30 20 10 0 BT 2 TRG + LINGFEAT Sentences 0 1 2 3 Simplicity BASE BASE + LINGFE"
2020.readi-1.7,P19-1198,0,0.114102,": A1, A2, and B1.3 Each level is linguistically operationalized, i.e., specified with respect to linguistic constructions permitted or not permitted at the respective level. 2.2 Nisioi et al. (2017) introduced neural sequence-tosequence models to automatic text simplification, performing experiments on both the Wikipedia dataset of (Hwang et al., 2015) and the Newsela Corpus for English, with automatic alignments derived from CATS (cf. Section 2.2). The authors used a Long Short-term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) as instance of Recurrent Neural Networks (RNNs). Surya et al. (2019) proposed an unsupervised or partially supervised approach to text simplification. Their model is based on a neural encoder-decoder but differs from previous approaches by adding reconstruction, adversarial, and diversification loss, which allows for exploiting nonparallel data as well. However, the authors’ results prove that some parallel data is still essential. Finally, Palmero Aprosio et al. (2019) experimented with data augmentation methods for low-resource text simplification for Italian. Their unaugmented dataset is larger than the one presented in this paper but includes more lowquali"
2020.readi-1.7,P12-1107,0,0.36089,"Missing"
2020.readi-1.7,Q16-1029,0,0.36295,"he authors’ results prove that some parallel data is still essential. Finally, Palmero Aprosio et al. (2019) experimented with data augmentation methods for low-resource text simplification for Italian. Their unaugmented dataset is larger than the one presented in this paper but includes more lowquality simplifications due to automatic extraction of simplified sentences from the Web. Our work differs in that we benchmark and compare a wider variety of low-resource methods. The most commonly applied automatic evaluation metrics for text simplification are BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). BLEU, the de-facto standard metric for machine translation, computes token n-gram overlap between a hypothesis and one or multiple references. A shortcoming of BLEU with respect to automatic text simplification is that it rewards hypotheses that do not differ from the input. By contrast, SARI was designed to punish such output. It does so by explicitly considering the input and rewarding tokens in the hypothesis that do not occur in the input but in one of the references (addition) and tokens in the input that are retained (copying) or removed (deletion) in both the hypothesis and one of the"
2020.readi-1.7,C10-1152,0,0.252052,"ersion of a standard-language text, was initiated in the late 1990s (Carroll et al., 1998; Chandrasekar et al., 1996) and since then has been approached by means of rule-based and statistical methods. As part of a rule-based approach, the operations carried out typically include replacing complex lexical and syntactic units by simpler ones. A statistical approach generally conceptualizes the simplification task as one of converting a standardlanguage into a simplified-language text using machine translation techniques. Research on automatic text simplification has been documented for English (Zhu et al., 2010), Spanish (Saggion et al., 2015), Portuguese (Aluisio and Gasperin, 2010), French (Brouwers et al., 2014), and Italian (Barlacchi and Tonelli, 2013). To the authors’ knowledge, the work of Suter (2015) and Suter et al. (2016), who presented a prototype of a rulebased text simplification system, is the only proposal for German. The paper at hand presents the first experiments in datadriven simplification for German, relying on neural machine translation. The data consists of news items manually simplified according to a well-known set of guidelines. Hence, the contribution of the paper is twofo"
buitelaar-etal-2004-evaluation,W03-1302,1,\N,Missing
buitelaar-etal-2004-evaluation,E03-2012,1,\N,Missing
buitelaar-etal-2004-evaluation,A00-1031,0,\N,Missing
buitelaar-etal-2004-evaluation,vintar-etal-2002-efficient,1,\N,Missing
C02-1004,C94-2195,0,0.155953,"Missing"
C02-1004,W95-0103,0,0.610614,"Missing"
C02-1004,J93-1005,0,0.887929,"Missing"
C02-1004,P00-1014,0,0.451067,"Missing"
C02-1004,P98-2177,0,0.616062,"Missing"
C02-1004,W97-0109,0,0.383405,"Missing"
C02-1004,C98-2172,0,\N,Missing
D18-1512,1999.mtsummit-1.31,0,0.490937,"Missing"
D18-1512,D09-1030,0,0.050412,"wn the source and a candidate text, and asked: How accurately does the above candidate text convey the semantics of the source text? In doing so, they have translations produced by humans and machines rated independently, and parity is assumed if the mean score of the former does not significantly differ from the mean score of the latter. Raters To optimise cost, machine translation quality is typically assessed by means of crowdsourcing. Combined ratings of bilingual crowd workers have been shown to be more reliable than automatic metrics and “very similar” to ratings produced by “experts”2 (Callison-Burch, 2009). Graham et al. (2017) compare crowdsourced to “expert” ratings on machine translations from WMT 2012, concluding that, with proper quality control, “machine translation systems can indeed be evaluated by the crowd alone.” However, it is unclear whether this finding carries over to translations produced by NMT systems where, due to increased fluency, errors are more difficult to identify (Castilho et al., 2017a), and concurrent work by Toral et al. (2018) highlights the importance of expert translators for MT evaluation. Experimental Unit Machine translation evaluation is predominantly perform"
D18-1512,W07-0718,0,0.226812,"Missing"
D18-1512,2004.iwslt-evaluation.1,0,0.110253,"Missing"
D18-1512,W16-4616,0,0.0181594,"indings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs. 1 Martin Volk1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches in many data settings and shared translation tasks (Luong and Manning, 2015; Sennrich et al., 2016; Cromieres et al., 2016). Some recent results suggest that neural machine translation “approaches the accuracy achieved by average bilingual human translators [on some test sets]” (Wu et al., 2016), or even that its “translation quality is at human parity when compared to professional human translators” (Hassan et al., 2018). Claims of human parity in machine translation are certainly extraordinary, and require extraordinary evidence.1 Laudably, Hassan et al. (2018) have released their data publicly to allow external validation of their claims. Their claims are further strengthened by the fact that they follow best p"
D18-1512,W13-2305,0,0.0482831,"79), and we identify three aspects with particular relevance to assessing human parity: granularity of measurement (ordinal vs. interval scales), raters (experts vs. crowd workers), and experimental unit (sentence vs. document). 2.1 Related Work Granularity of Measurement Callison-Burch et al. (2007) show that ranking (Which of these translations is better?) leads to better inter-rater agreement than absolute judgement on 5-point Likert scales (How good is this translation?) but gives no insight about how much a candidate translation differs from a (presumably perfect) reference. To this end, Graham et al. (2013) suggest the use of continuous scales for direct assessment of translation quality. Implemented as a slider between 0 (Not at all) and 100 (Perfectly), their method yields scores on a 100-point interval scale in practice (Bojar et al., 2016b, 2017), with each raters’ rating being standardised to increase homogeneity. Hassan et al. (2018) use source-based direct assessment to avoid bias towards reference translations. In the shared task evaluation by Cettolo et al. (2017), raters are shown the source and a candidate text, and asked: How accurately does the above candidate text convey the semant"
D18-1512,D13-1176,0,0.0344536,"cols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs. 1 Martin Volk1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches in many data settings and shared translation tasks (Luong and Manning, 2015; Sennrich et al., 2016; Cromieres et al., 2016). Some recent results suggest that neural machine translation “approaches the accuracy achieved by average bilingual human translators [on some test sets]” (Wu et al., 2016), or even that its “translation quality is at human parity when compared to professional human translators” (Hassan et al., 2018). Claims of human parity"
D18-1512,W18-6312,0,0.0736235,"f bilingual crowd workers have been shown to be more reliable than automatic metrics and “very similar” to ratings produced by “experts”2 (Callison-Burch, 2009). Graham et al. (2017) compare crowdsourced to “expert” ratings on machine translations from WMT 2012, concluding that, with proper quality control, “machine translation systems can indeed be evaluated by the crowd alone.” However, it is unclear whether this finding carries over to translations produced by NMT systems where, due to increased fluency, errors are more difficult to identify (Castilho et al., 2017a), and concurrent work by Toral et al. (2018) highlights the importance of expert translators for MT evaluation. Experimental Unit Machine translation evaluation is predominantly performed on single sentences, presented to raters in random order (e. g., Bojar et al., 2017; Cettolo et al., 2017). There are two main reasons for this. The first is cost: if raters assess entire documents, obtaining the same number of data points in an evaluation campaign multiplies the cost by the average number of sentences per document. The second is experimental validity. When comparing systems that produce sentences without considering documentlevel cont"
D18-1512,2015.iwslt-evaluation.11,0,0.0466284,"cuments as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs. 1 Martin Volk1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches in many data settings and shared translation tasks (Luong and Manning, 2015; Sennrich et al., 2016; Cromieres et al., 2016). Some recent results suggest that neural machine translation “approaches the accuracy achieved by average bilingual human translators [on some test sets]” (Wu et al., 2016), or even that its “translation quality is at human parity when compared to professional human translators” (Hassan et al., 2018). Claims of human parity in machine translation are certainly extraordinary, and require extraordinary evidence.1 Laudably, Hassan et al. (2018) have released their data publicly to allow external validation of their claims. Their claims are further"
D18-1512,W16-2323,1,\N,Missing
D18-1512,W17-4739,1,\N,Missing
E03-2012,A00-1031,0,0.0199281,"ation 2. Semantic Relation Annotation Results Displaying Search I Engine  Figure 1. System Architecture 3 MeSH: Medical Subject Headings (http://www.nlm.nih.gov/mesh/meshhome.html) 231 ates the intermediary data representation, and a back-end tier consisting of a search engine system to provide the retrieval technology (see Figure 1.). 2.1 Query and Document Annotation The middle tier annotation module consists of more subtiers representing an advanced annotation system that automatically identifies a number of relevant linguistic and semantic features. Components for part-of-speech tagging (Brants, 2000), morphological analysis (Petitpierre and Russell., 1995), phrase tagging (chunking) (Skut and Brants., 1998), concept and semantic relations annotation are being loosely integrated, through input-output markup interfaces, and generate an intermediary XML representation (Vintar et al., 2001) of the input data (see Figure 2.). Semantic annotation represents the primary information that the retrieval system is using. Crossing the language barrier from a query in one language to the document collection in another language is done via concept codes as an interlingua representation. The multilingua"
E03-2012,W98-1117,0,0.0275225,"Missing"
E03-2012,vintar-etal-2002-efficient,1,0.7706,"Missing"
etchegoyhen-etal-2014-machine,D11-1033,0,\N,Missing
etchegoyhen-etal-2014-machine,P07-2045,0,\N,Missing
etchegoyhen-etal-2014-machine,E12-1055,0,\N,Missing
etchegoyhen-etal-2014-machine,N03-1017,0,\N,Missing
etchegoyhen-etal-2014-machine,P13-4014,0,\N,Missing
etchegoyhen-etal-2014-machine,petukhova-etal-2012-sumat,1,\N,Missing
etchegoyhen-etal-2014-machine,tiedemann-2012-parallel,0,\N,Missing
J91-3010,J90-3004,0,0.0179511,"lated phenomena. The present work shows what it is like to integrate these fragments into a single unified treatment. The book presents a set of syntactic-semantic rules which build up a description of the logical content of English sentences on the basis of the logical contents of their components. These rules, which are the heart of the book, give an account of a substantial fragment of English.&quot; Allan Ramsay is a professor of Artificial Intelligence at University College Dublin and in the past has done work in parsing (Ramsay 1985), formal methods in AI (Ramsay 1988), and in feature logic (Ramsay 1990). From the table of contents we learn that the book is organized into six chapters and comes with an additional preface, an epilogue, a bibliography, an appendix listing all the &quot;Rules and Frames&quot; introduced in the book, and an index. The bibliography consists of 63 very diverse entries, including the landmarks in the respective fields: from Koskenniemi on morphology to Searle on speech acts. It also contains a good number of entries on different aspects of logic, which gives you a warning that this book focuses on the formal aspects of language. The layout of the book is pleasant. The example"
P15-3002,N09-1014,0,0.0144851,"the model propagates the error to the following sentences. Gong et al. (2011) later extended Tiedemann’s proposal by initializing the cache with phrase pairs from similar documents at the beginning of the translation and by also applying a topic cache, which was introduced to deal with the error propagation issue. Xiao et al. (2011) defined a three step procedure that enforces the consistent translation of ambiguous words, achieving improvements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we compared caching and post-editing as ways of achieving this goal, but a document-level decoder such as Docent (Hardmeier et al., 2012) could be used as well. In other studies, factored translation models (Koehn and Hoang, 2007) have been used with the same purpose, by incorporating contextual informa"
P15-3002,W09-2404,0,0.492585,"ask. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al., 1992) and its application to machine translation is based on the premise that consistency in discourse (Carpuat, 2009) is desirable. The initial compound idea was first published by Mascarell et al. (2014), in which the coreference of compound noun phrases in German (e.g. Nordwand/Wand) was studied and used to improve DE/FR translation by assuming that the last constituent of the compound Y should share the same translation as that of Y in XY . Several other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrases across the document. However, caching is sensitive to error propagation, that is, when a phrase is incorrectly"
P15-3002,W10-1737,0,0.0683154,"Missing"
P15-3002,2012.eamt-1.60,0,0.0984312,"ignment. We verify if the translations of Y in both noun phrases are identical or different. Both elements comprising the compound structure XY /Y are identified, for the standard cases, with only one possible XY referring to one Y . The translation of both words are provided by the baseline SMT system, and our system subsequently verifies if the translations of Y in both noun phrases are identical or different. We keep them intact in the first case, while in the second 3 Experimental Settings The experiments are carried out on two different parallel corpora: the WIT3 Chinese-English dataset (Cettolo et al., 2012) with transcripts of TED lectures and their translations, and the Text+Berg German-French corpus (Bubenhofer et al., 2013), a collection of articles from the year4 Upon manual examination, we found that using the most recent XY was not a reliable candidate for the antecedent. 5 In fact, we can use the translation of Y as a translation candidate for XY . Our observations show that this helps to improve BLEU scores, but does not affect the specific scoring of Y in Section 4. 1 Using the Stanford Word Segmenter available from http://nlp.stanford.edu/software/segmenter.shtml. 2 Using the Stanford"
P15-3002,loaiciga-etal-2014-english,1,0.84855,"idates for Y in the phrase table (Mascarell et al., 2014). 4.2 Manual Evaluation of Undecided Cases When both the baseline and one of our systems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this sec"
P15-3002,H92-1045,0,0.725085,"derico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al., 1992) and its application to machine translation is based on the premise that consistency in discourse (Carpuat, 2009) is desirable. The initial compound idea was first published by Mascarell et al. (2014), in which the coreference of compound noun phrases in German (e.g. Nordwand/Wand) was studied and used to improve DE/FR translation by assuming that the last constituent of the compound Y should share the same translation as that of Y in XY . Several other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrase"
P15-3002,D11-1084,0,0.315107,"2014), in which the coreference of compound noun phrases in German (e.g. Nordwand/Wand) was studied and used to improve DE/FR translation by assuming that the last constituent of the compound Y should share the same translation as that of Y in XY . Several other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrases across the document. However, caching is sensitive to error propagation, that is, when a phrase is incorrectly translated and cached, the model propagates the error to the following sentences. Gong et al. (2011) later extended Tiedemann’s proposal by initializing the cache with phrase pairs from similar documents at the beginning of the translation and by also applying a topic cache, which was introduced to deal with the error propagation issue. Xiao et al. (2011) defined a three step procedure that enforces the consistent translation of ambiguous words, achieving improvements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences h"
P15-3002,E12-3001,0,0.0176957,"Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al., 1992) and its a"
P15-3002,W12-0117,1,0.841939,"since it only enforces a translation when it appears as one of the translation candidates for Y in the phrase table (Mascarell et al., 2014). 4.2 Manual Evaluation of Undecided Cases When both the baseline and one of our systems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system tran"
P15-3002,2010.iwslt-papers.10,0,0.0165377,"tems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al.,"
P15-3002,P03-1021,0,0.0144953,"imize the likelihood of coreference. In German, less restrictive rules selected 7,365 XY /Y pairs (a rate of one every 40 sentences). Still, in what follows, we randomly selected 261 XY /Y pairs for the DE/FR test data, to match their number in the ZH/EN test data. Our baseline SMT system is the Moses phrasebased decoder (Koehn et al., 2007), trained over tokenized and true-cased data. The language models were built using SRILM (Stolcke et al., 2011) at order 3 (i.e. up to trigrams) using the default smoothing method (i.e. Good-Turing). Optimization was done using Minimum Error Rate Training (Och, 2003) as provided with Moses. The effectiveness of proposed systems is measured in two ways. First, we use BLEU (Papineni et al., 2002) for overall evaluation, to verify whether our systems provide better translation for entire texts. Then, we focus on the XY /Y pairs and count the number of cases in which the translations of Y match the reference or not, which can be computed automatically using the alignments. However, the automatic comparison of a system’s translation with the reference is not entirely informative, because even if the two differ, the system’s translation can still be acceptable."
P15-3002,D12-1108,0,0.0604372,"vements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we compared caching and post-editing as ways of achieving this goal, but a document-level decoder such as Docent (Hardmeier et al., 2012) could be used as well. In other studies, factored translation models (Koehn and Hoang, 2007) have been used with the same purpose, by incorporating contextual information into labels used to indicate the meaning of ambiguous 6 Conclusion and Perspectives We presented a method to enforce the consistent translation of coreferences to a compound, when the coreference matches the head noun of the compound. Experimental results showed that baseline SMT systems often translate coreferences to compounds consistently for DE/FR, but much less so for ZH/EN. For a significant number of cases in which th"
P15-3002,P02-1040,0,0.0921765,"40 sentences). Still, in what follows, we randomly selected 261 XY /Y pairs for the DE/FR test data, to match their number in the ZH/EN test data. Our baseline SMT system is the Moses phrasebased decoder (Koehn et al., 2007), trained over tokenized and true-cased data. The language models were built using SRILM (Stolcke et al., 2011) at order 3 (i.e. up to trigrams) using the default smoothing method (i.e. Good-Turing). Optimization was done using Minimum Error Rate Training (Och, 2003) as provided with Moses. The effectiveness of proposed systems is measured in two ways. First, we use BLEU (Papineni et al., 2002) for overall evaluation, to verify whether our systems provide better translation for entire texts. Then, we focus on the XY /Y pairs and count the number of cases in which the translations of Y match the reference or not, which can be computed automatically using the alignments. However, the automatic comparison of a system’s translation with the reference is not entirely informative, because even if the two differ, the system’s translation can still be acceptable. Therefore, we analyzed these “undecided” situations BASELINE C ACHING P OST- EDITING O RACLE ZH/EN 11.18 11.23 11.27 11.30 DE/FR"
P15-3002,D07-1091,0,0.018175,"-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we compared caching and post-editing as ways of achieving this goal, but a document-level decoder such as Docent (Hardmeier et al., 2012) could be used as well. In other studies, factored translation models (Koehn and Hoang, 2007) have been used with the same purpose, by incorporating contextual information into labels used to indicate the meaning of ambiguous 6 Conclusion and Perspectives We presented a method to enforce the consistent translation of coreferences to a compound, when the coreference matches the head noun of the compound. Experimental results showed that baseline SMT systems often translate coreferences to compounds consistently for DE/FR, but much less so for ZH/EN. For a significant number of cases in which the noun phrase Y had multiple meanings, our system reduced the frequency of mistranslations in"
P15-3002,W10-2602,0,0.770009,"anslated word), in which case we apply post-editing as above on the word preceding Y , if it is aligned. In the caching method (Mascarell et al., 2014), once an XY compound is identified, we obtain the translation of the Y part of the compound through the word alignment given by the SMT decoder. Next, we check that this translation appears as a translation of Y in the phrase table, and if so, we cache both Y and the obtained translation. We then enforce the cached translation every time a coreference Y to XY is identified. Note that this is different from the probabilistic caching proposed by Tiedemann (2010), because in our case the cached translation is deterministically enforced as the translation of Y . Enforcing the Translation of Y Two language-independent methods have been designed to ensure that the translations of XY and Y are a consistent: post-editing and caching. The second one builds upon an earlier proposal tested only on DE/FR with subjective evaluations (Mascarell et al., 2014). In the post-editing method, for each XY /Y pair, the translations of XY and Y by a baseline SMT system (see Section 3) are first identified through word alignment. We verify if the translations of Y in both"
P15-3002,2011.mtsummit-papers.13,0,0.364573,"other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrases across the document. However, caching is sensitive to error propagation, that is, when a phrase is incorrectly translated and cached, the model propagates the error to the following sentences. Gong et al. (2011) later extended Tiedemann’s proposal by initializing the cache with phrase pairs from similar documents at the beginning of the translation and by also applying a topic cache, which was introduced to deal with the error propagation issue. Xiao et al. (2011) defined a three step procedure that enforces the consistent translation of ambiguous words, achieving improvements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we com"
P15-3002,N12-1046,0,\N,Missing
P15-3002,P07-2045,0,\N,Missing
petukhova-etal-2012-sumat,W09-4610,1,\N,Missing
petukhova-etal-2012-sumat,P07-2045,0,\N,Missing
petukhova-etal-2012-sumat,2005.eamt-1.29,0,\N,Missing
petukhova-etal-2012-sumat,R11-1014,0,\N,Missing
petukhova-etal-2012-sumat,2010.eamt-1.23,1,\N,Missing
R13-1079,schmid-etal-2004-smor,0,0.0167188,"er in the functional disambiguation of noun phrases, i.e. relations such as subject, object, and genitive modifier, while MaltParser finds more coordinations, albeit with lower precision. Some selected f1 val3 Morphology For parsing, morphology tools provide two useful types of information. Lemma information allows for less sparse representation of statistical data, and inflectional analyses can be used to enforce agreement constraints, and for the functional disambiguation of German noun phrases. As alternatives to GERTWOL, we investigate two morphology tools, both based on the SMOR grammar (Schmid et al., 2004), which is open source and licensed under GPL v2. The first is the SMOR grammar with the lexicon of the University of Stuttgart (consequently referred to as SMOR). The lexicon is closed-source, and can be licensed for research purposes. Secondly, we investigate Morphisto (Zielinski and Simon, 2009), which combines the SMOR grammar with an open-source lexicon, provided under the Creative Commons 3.0 BY-SA Non-Commercial license. One problem with the SMOR grammar is that the morphology does not produce conventional lemmas, but derivational analyses as shown in figure 2. Specifically, the word fo"
R13-1079,P05-1022,0,0.0313835,"Missing"
R13-1079,C10-2129,0,0.10406,"Missing"
R13-1079,2005.mtsummit-papers.11,0,0.0249766,"Missing"
R13-1079,telljohann-etal-2004-tuba,0,0.0198654,"sitional preferences, and is trained on the TüBa-D/Z, a hand-annotated treebank of Introduction German NLP tools such as part-of-speech taggers, morphology tools, and syntactic parsers often require licensing and suffer from usage restrictions, which makes the deployment of an NLP pipeline that combines several components cumbersome at best, impossible at worst (if no license can be obtained). Some restrictions are rooted in the copyright and/or licenses of the annotated corpora on which statistical taggers or parsers can be trained for German, such as TIGER (Brants et al., 2002) or Tüba-D/Z (Telljohann et al., 2004). There have been attempts to bypass these restrictions through corpus masking (Rehm et al., 2007), but for statistical models that require lexical information, this is not an option. We discuss ParZu, a German dependency parser that relies on external tools for POS tagging and morphological analysis, and combines a handwritten grammar and a statistical disambiguation module that is trained on a treebank. We describe attempts to move towards components with freer licensing. We also discuss techniques to improve 1 Among the lexicalized rules is a closed list of nouns which can head noun phrases"
R13-1079,P10-1052,0,0.0305138,"Missing"
R13-1079,P09-1040,0,0.0124778,"ensing. We also discuss techniques to improve 1 Among the lexicalized rules is a closed list of nouns which can head noun phrases with temporal function, such as Er schläft jeden Tag (English: ‘he sleeps every day’). 601 Proceedings of Recent Advances in Natural Language Processing, pages 601–609, Hissar, Bulgaria, 7-13 September 2013. adv subj objc obja adv subj pred det Jetzt frage ADV VVFIN ich PPER mich , wofür PRF $, PWAV das ART Ganze gut war . NN ADJD VAFIN $. Figure 1: TüBa-D/Z parse tree in dependency format. (English: ‘Now I ask myself what good it did.’) parsing, we use MaltParser (Nivre, 2009), with settings optimized with MaltOptimizer (Ballesteros and Nivre, 2012). MaltParser is a tool for data-driven dependency parsing which implements various algorithms. For TüBa-D/Z, MaltOptimizer selects the stack projective algorithm (Nivre, 2009) with pseudo-projective pre- and postprocessing. The algorithm generates a parse tree through a sequence of transitions from an initial configuration (a NULL word on the stack, all words of the sentence in the buffer, and an empty set of labelled dependency arcs) to a terminal configuration (a NULL word on the stack, an empty buffer, and a set of la"
R13-1079,ballesteros-nivre-2012-maltoptimizer-system,0,\N,Missing
volk-etal-2010-challenges,W03-0108,0,\N,Missing
volk-etal-2010-challenges,W03-0105,0,\N,Missing
volk-etal-2010-challenges,W03-0110,0,\N,Missing
volk-etal-2014-innovations,J03-1002,0,\N,Missing
volk-etal-2014-innovations,W07-1514,1,\N,Missing
volk-etal-2014-innovations,tiedemann-2012-parallel,0,\N,Missing
W04-1910,ahrenberg-etal-2002-system,0,0.262504,"=""3"" word=""die"" pos=""ART"" /> <t id=""4"" word=""am"" pos=""APPRART""/> <t id=""5"" word=""Freitag"" pos=""NN"" /> [...] </terminals> <nonterminals> <nt id=""500"" cat=""NP""> <edge label=""HD"" idref=""1"" /> </nt> [...] <nt id=""522"" cat=""S""> <edge label=""HD"" idref=""2"" /> <edge label=""SB"" idref=""500"" /> <edge label=""MO"" idref=""511"" /> <edge label=""OA"" idref=""521"" /> </nt> </nonterminals> </graph> </s> Since all tokens and all nodes are uniquely numbered, these numbers can be used for the phrase alignment. For the representation of the alignment we adapted a DTD that was developed for the Link¨oping Word Aligner (Ahrenberg et al., 2002). The XML-file with the alignment information then looks like this. The sentLink-tags each contain one sentence pair, while each phraseLink represents one aligned node pair. <!DOCTYPE DeSv SYSTEM ""align.dtd""> <DeSv fromDoc=""De.xml"" toDoc=""Sv.xml""> <linkList> <sentLink xtargets=""1 ; 1""> <phraseLink xtargets=""500; 500""/> <phraseLink xtargets=""501; 503""/> [...] </sentLink> </linkList> </DeSv> This fragment first specifies the two involved XML files for German (De.xml) and Swedish (Sv.xml). It then states the phrase pairs for the sentence pair 1 - 1 from these files. For example, phrase number 501"
W04-1910,P02-1050,0,0.0713569,"Missing"
W04-1910,H01-1035,0,0.0663937,"Missing"
W06-2112,W04-1910,1,0.842928,"Missing"
W06-2112,W95-0103,0,0.148731,"Missing"
W06-2112,C02-1004,1,0.896476,"Missing"
W06-2112,J93-1005,0,0.8734,"Missing"
W06-2112,J93-2004,0,0.0296901,"Missing"
W06-2112,W97-0317,0,0.300941,"Missing"
W06-2112,H94-1048,0,0.644669,"Missing"
W06-2112,P98-2177,0,0.395734,"Missing"
W06-2112,W97-0109,0,0.429126,"Missing"
W06-2112,C98-2172,0,\N,Missing
W06-2717,ahrenberg-etal-2002-system,0,0.0255053,"nts to be specified between an arbitrary number of nodes, for example nodes from three languages. And it includes an attribute type which we currently use to distinguish between exact and approximate alignments. 5 Figure 1: Tree pair German-Swedish in the TreeAligner. Our Tree Alignment Tool After finishing the monolingual trees we want to align them on the phrase level. For this purpose we have developed a “TreeAligner”. This program is a graphical user interface to insert (or correct) alignments between pairs of syntax trees.4 The TreeAligner can be seen in the line of tools such as I*Link (Ahrenberg et al., 2002) or Cairo (Smith and Jahr, 2000) but it is especially tailored to visualize and align full syntax trees. The TreeAligner requires three input files. One TIGER-XML file with the trees from language one, another TIGER-XML file with the trees from language two, plus the alignment file as described above. The alignment file might initially be empty when we want to start manual alignment from scratch, or it might contain automatically computed alignments for correction. The TreeAligner displays tree pairs with the trees in mirror orientation (one top-up and one top-down). See figure 1 for an exampl"
W06-2717,brants-hansen-2002-developments,0,0.0181797,"pkov´a, Joakim Lundborg, Torsten Marek, Yvonne Samuelsson, Frida Tidstr¨om Stockholm University Department of Linguistics 106 91 Stockholm, Sweden volk@ling.su.se with about 500 sentences.1 In addition it contains 500 sentences from economy texts (a quarterly report by a multinational company as well as part of a bank’s annual report). In creating the parallel treebank, we have first annotated the monolingual treebanks with the A NNOTATE treebank editor.2 It includes Thorsten Brants’ statistical Part-of-Speech Tagger and Chunker. The chunker follows the TIGER annotation guidelines for German (Brants and Hansen, 2002), which gives a flat phrase structure tree. This means, for instance, no unary nodes, no “unnecessary” NPs (noun phrases) within PPs (prepositional phrases) and no finite VPs (verb phrases). Using a flat tree structure for manual treebank annotation has two advantages for the human annotator: fewer annotation decisions, and a better overview of the trees. This comes at the prize of the trees not being complete from a linguistic point of view. Moreover, flat syntax trees are also problematic for node alignment in a parallel treebank. We prefer to have “deep trees” to be able to draw the alignme"
W06-2717,erk-pado-2004-powerful,0,0.0665061,"Missing"
W06-2717,smith-jahr-2000-cairo,0,0.211636,"itrary number of nodes, for example nodes from three languages. And it includes an attribute type which we currently use to distinguish between exact and approximate alignments. 5 Figure 1: Tree pair German-Swedish in the TreeAligner. Our Tree Alignment Tool After finishing the monolingual trees we want to align them on the phrase level. For this purpose we have developed a “TreeAligner”. This program is a graphical user interface to insert (or correct) alignments between pairs of syntax trees.4 The TreeAligner can be seen in the line of tools such as I*Link (Ahrenberg et al., 2002) or Cairo (Smith and Jahr, 2000) but it is especially tailored to visualize and align full syntax trees. The TreeAligner requires three input files. One TIGER-XML file with the trees from language one, another TIGER-XML file with the trees from language two, plus the alignment file as described above. The alignment file might initially be empty when we want to start manual alignment from scratch, or it might contain automatically computed alignments for correction. The TreeAligner displays tree pairs with the trees in mirror orientation (one top-up and one top-down). See figure 1 for an example. This has the advantage that t"
W06-2717,W04-1910,1,0.884782,"s novel “Sofie’s World” 93 We annotated the Swedish sentences by first tagging them with a Part-of-Speech tagger trained on SUC (the Stockholm-Ume˚a Corpus). Since we did not have a Swedish treebank to train a Swedish chunker, we used a trick to apply the German chunker for Swedish sentences. We mapped the Swedish Part-of-Speech tags in the Swedish sentences to the corresponding German tags. Since the German chunker works on these tags, it then suggested constituents for the Swedish sentences, assuming they were German sentences. These experiments and the resulting time gain were reported in (Volk and Samuelsson, 2004). Upon completion of the Swedish treebank with flat syntax trees, we applied the same deepening method as for German, and we then converted the Part-ofSpeech labels back to the Swedish labels. Finally, we annotated the English sentences according to the Penn Treebank guidelines. We trained the PoS tagger and the chunker on the Penn Treebank and integrated them into A NNOTATE. The English guidelines lead to complete trees so that the deepening step is not needed. 3 &lt;graph root=""s12_501""> &lt;terminals> &lt;t id=""s12_1"" word=""Jetzt"" pos=""ADV"" /> &lt;t id=""s12_2"" word=""bog"" pos=""VVFIN"" /> &lt;t id=""s12_3"" wo"
W07-1514,ahrenberg-etal-2002-system,0,0.0148115,"R-XML. TIGER-XML is a linebased (i.e. not nested and thus database-friendly) representation for graph structures which supports crossing edges and secondary edges.2 TIGER-XML has been defined as input format for TIGER-Search, a query tool for monolingual treebanks (see section 3). We use this format also as input format for our alignment tool, the Stockholm TreeAligner (Volk et al., 2006). The TreeAligner program is a graphical user interface to specify (or correct) word and phrase alignments between pairs of syntax trees.3 The TreeAligner is roughly similar to alignment tools such as I*Link (Ahrenberg et al., 2002) or Cairo (Smith and Jahr, 2000) but it is especially tailored to visualize and align full syntax trees (including trees with crossing edges). 2 For information about TIGER-XML see www.ims.unistuttgart.de/projekte/TIGER 3 The TreeAligner is freely available at www.ling.su.se/ DaLi/downloads/treealigner/index.htm 86 Figure 1: Tree pair German-English in the TreeAligner. The TreeAligner operates on an alignment file in an XML format developed by us. This file describes the alignments between two TIGER-XML treebanks (specified in the alignment file) holding the trees from language one and languag"
W07-1514,W07-2441,0,0.0595726,"word level or phrase level (some phrases can be as large as clauses). Parallel treebanks can be used as training or evaluation corpora for word and phrase alignment, as input 1 We gratefully acknowledge financial support for the S MULTRON project by Granholms stiftelse and Rausings stiftelse. for example-based machine translation (EBMT), as training corpora for transfer rules, or for translation studies. Similar projects include Croco (Hansen-Schirra et al., 2006) which is aimed at building a GermanEnglish parallel treebank for translation studies, LinES an English-Swedish parallel treebank (Ahrenberg, 2007), and the Czech-English parallel dependency treebank built in Prague (Cmejrek et al., 2005). S MULTRON is an English-German-Swedish parallel treebank (Samuelsson and Volk, 2006; Samuelsson and Volk, 2007). It contains the first two chapters of Jostein Gaarder’s novel “Sofie’s World” with about 500 sentences. In addition it contains 500 sentences from economy texts (a quarterly report by a multinational company as well as part of a bank’s annual report). We have (semiautomatically) annotated the German sentences with Part-of-Speech tags and phrase structure trees (incl. edges labeled with funct"
W07-1514,2005.eamt-1.11,0,0.0534218,"Missing"
W07-1514,W06-2705,0,0.0436896,"(i.e. parallel) documents. In addition, the syntax trees of corresponding sentence pairs are aligned on a sub-sentential level. This means they are aligned on word level or phrase level (some phrases can be as large as clauses). Parallel treebanks can be used as training or evaluation corpora for word and phrase alignment, as input 1 We gratefully acknowledge financial support for the S MULTRON project by Granholms stiftelse and Rausings stiftelse. for example-based machine translation (EBMT), as training corpora for transfer rules, or for translation studies. Similar projects include Croco (Hansen-Schirra et al., 2006) which is aimed at building a GermanEnglish parallel treebank for translation studies, LinES an English-Swedish parallel treebank (Ahrenberg, 2007), and the Czech-English parallel dependency treebank built in Prague (Cmejrek et al., 2005). S MULTRON is an English-German-Swedish parallel treebank (Samuelsson and Volk, 2006; Samuelsson and Volk, 2007). It contains the first two chapters of Jostein Gaarder’s novel “Sofie’s World” with about 500 sentences. In addition it contains 500 sentences from economy texts (a quarterly report by a multinational company as well as part of a bank’s annual repo"
W07-1514,petersen-2006-querying,0,0.0286674,"e annotated separately with the help of the treebank editor A NNOTATE. After finishing the monolingual treebanks, the trees were exported from the accompanying SQL database and converted into an XML format as input to our alignment tool, the Stockholm TreeAligner. In this paper we will first describe this alignment tool and then focus on its new search facility. To our knowledge this is the first dedicated tool that combines visualization, alignment and searching of parallel treebanks (although there are others who have experimented with parallel corpus searches (Nygaard and Johannesen, 2004; Petersen, 2006)). 2 The Stockholm TreeAligner When our monolingual treebanks were finished, the trees were exported from the editor system and converted into TIGER-XML. TIGER-XML is a linebased (i.e. not nested and thus database-friendly) representation for graph structures which supports crossing edges and secondary edges.2 TIGER-XML has been defined as input format for TIGER-Search, a query tool for monolingual treebanks (see section 3). We use this format also as input format for our alignment tool, the Stockholm TreeAligner (Volk et al., 2006). The TreeAligner program is a graphical user interface to spe"
W07-1514,W06-2717,1,0.851926,"as input 1 We gratefully acknowledge financial support for the S MULTRON project by Granholms stiftelse and Rausings stiftelse. for example-based machine translation (EBMT), as training corpora for transfer rules, or for translation studies. Similar projects include Croco (Hansen-Schirra et al., 2006) which is aimed at building a GermanEnglish parallel treebank for translation studies, LinES an English-Swedish parallel treebank (Ahrenberg, 2007), and the Czech-English parallel dependency treebank built in Prague (Cmejrek et al., 2005). S MULTRON is an English-German-Swedish parallel treebank (Samuelsson and Volk, 2006; Samuelsson and Volk, 2007). It contains the first two chapters of Jostein Gaarder’s novel “Sofie’s World” with about 500 sentences. In addition it contains 500 sentences from economy texts (a quarterly report by a multinational company as well as part of a bank’s annual report). We have (semiautomatically) annotated the German sentences with Part-of-Speech tags and phrase structure trees (incl. edges labeled with functional information) following the NEGRA/TIGER guidelines for German treebanking. For English we have used the Penn Treebank guidelines which are similar in that they also prescr"
W07-1514,smith-jahr-2000-cairo,0,0.0316076,".e. not nested and thus database-friendly) representation for graph structures which supports crossing edges and secondary edges.2 TIGER-XML has been defined as input format for TIGER-Search, a query tool for monolingual treebanks (see section 3). We use this format also as input format for our alignment tool, the Stockholm TreeAligner (Volk et al., 2006). The TreeAligner program is a graphical user interface to specify (or correct) word and phrase alignments between pairs of syntax trees.3 The TreeAligner is roughly similar to alignment tools such as I*Link (Ahrenberg et al., 2002) or Cairo (Smith and Jahr, 2000) but it is especially tailored to visualize and align full syntax trees (including trees with crossing edges). 2 For information about TIGER-XML see www.ims.unistuttgart.de/projekte/TIGER 3 The TreeAligner is freely available at www.ling.su.se/ DaLi/downloads/treealigner/index.htm 86 Figure 1: Tree pair German-English in the TreeAligner. The TreeAligner operates on an alignment file in an XML format developed by us. This file describes the alignments between two TIGER-XML treebanks (specified in the alignment file) holding the trees from language one and language two respectively. For example"
W07-2427,J93-2004,0,0.0303405,"Missing"
W07-2427,W01-0707,0,0.0137715,"ent ambiguities arises from the attachment of prepositional phrases (PPs). Simply stated, a PP that follows a noun (in French as in English, German or Swedish) can be attached to the preceding noun or to the verb of the same clause. In the last decade various methods for the resolution of PP attachment ambiguities have been proposed. The seminal paper by Hindle and Rooth (1993) started a sequence of studies for English. Volk (2001; 2002) has investigated similar methods for German. Recently other languages such as Dutch (Vandeghinste, 2002), Swedish (Kokkinakis, 2000; Aasa, 2004), and French (Gaussier and Cancedda, 2001; Gala and Lafourcade, 2005) have followed. Volk (2006) investigated the attachment tendencies of prepositions in English, German and Swedish. He found that English had the highest overall noun attachment rate followed by Swedish and German. He also showed that the high rate in English was highly influenced by the preposition of. From this study he derived a list of criteria for profiling data sets for PP attachment experiments. In the current paper we have applied this list of criteria to French. We have obtained a French treebank and converted it into TIGER-XML so that we can use the same ap"
W07-2427,J93-1005,0,0.477755,"ank. 1 Introduction Any computer system for natural language processing has to struggle with the problem of ambiguities. If the system is meant to extract precise information from a text, the ambiguities must be resolved. One of the most frequent ambiguities arises from the attachment of prepositional phrases (PPs). Simply stated, a PP that follows a noun (in French as in English, German or Swedish) can be attached to the preceding noun or to the verb of the same clause. In the last decade various methods for the resolution of PP attachment ambiguities have been proposed. The seminal paper by Hindle and Rooth (1993) started a sequence of studies for English. Volk (2001; 2002) has investigated similar methods for German. Recently other languages such as Dutch (Vandeghinste, 2002), Swedish (Kokkinakis, 2000; Aasa, 2004), and French (Gaussier and Cancedda, 2001; Gala and Lafourcade, 2005) have followed. Volk (2006) investigated the attachment tendencies of prepositions in English, German and Swedish. He found that English had the highest overall noun attachment rate followed by Swedish and German. He also showed that the high rate in English was highly influenced by the preposition of. From this study he de"
W07-2427,H94-1048,0,0.0717332,"investigated earlier. In the PP attachment research for other languages there is often a comparison of the disambiguation accuracy with the results for English. But are the results really comparable across languages? Volk (2006) showed that disambiguation efforts start from very different baselines in English, German and Swedish. In this paper we investigate how French fits into this picture. 2 Background In their pioneering work Hindle and Rooth (1993) did not have access to a large treebank. Therefore they proposed an unsupervised method for resolving PP attachment ambiguities. A year later Ratnaparkhi et al. (1994) published a supervised approach to the PP attachment problem. They had extracted quadruples V-N-P-N1 (plus the accompanying attachment decision) from both an IBM computer manuals treebank (about 9000 tuples) 1 The V-N-P-N quadruples also contain the head noun of the NP within the PP. Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit (Eds.) NODALIDA 2007 Conference Proceedings, pp. 191–198 Martin Volk and Frida Tidström and from the Wall Street Journal (WSJ) section of the Penn treebank (about 24,000 tuples). The latter tuple set has been reused by subsequent research, so let us f"
W07-2427,saint-dizier-2006-prepnet,0,0.0313366,"nstance, German which usually counts around 100 atomic preposition types (Volk, 2001). But a comparison with other French preposition lists confirms this number. For 6 Note that the query [cat=""PP""] > [pos=""P""] leads to 64 different preposition types. But manual inspection shows that 18 of them are spelling errors (e.g. ee instead of en), mathematical symbols (+-/) or compound prepositions. Comparing French PP-attachment to English, German and Swedish example the French PrepLex Database7 , which contains the merged information from a number of sources (including the syntactic part of PrepNet (Saint-Dizier, 2006)), lists 49 “simple” prepositions (in contrast to multi-word prepositions). 40 of them also occur as prepositions in the Le Monde treebank. The nine remaining ones either do not occur at all in the Le Monde treebank (circa, confer, versus) or they occur only with other PoS labels (dixit, pass´e, sitˆot, touchant, vu, e` s). On the other hand, there are six simple prepositions in the Le Monde treebank which are not listed in PrepLex (autour, courant, environ, plein, plus, pr`es). Two of them occur only once as preposition but many times with other PoS labels (autour is adverb in 79 cases, and p"
W07-2427,C02-1004,1,0.901363,"Missing"
W07-2427,W06-2112,1,0.828187,"s). Simply stated, a PP that follows a noun (in French as in English, German or Swedish) can be attached to the preceding noun or to the verb of the same clause. In the last decade various methods for the resolution of PP attachment ambiguities have been proposed. The seminal paper by Hindle and Rooth (1993) started a sequence of studies for English. Volk (2001; 2002) has investigated similar methods for German. Recently other languages such as Dutch (Vandeghinste, 2002), Swedish (Kokkinakis, 2000; Aasa, 2004), and French (Gaussier and Cancedda, 2001; Gala and Lafourcade, 2005) have followed. Volk (2006) investigated the attachment tendencies of prepositions in English, German and Swedish. He found that English had the highest overall noun attachment rate followed by Swedish and German. He also showed that the high rate in English was highly influenced by the preposition of. From this study he derived a list of criteria for profiling data sets for PP attachment experiments. In the current paper we have applied this list of criteria to French. We have obtained a French treebank and converted it into TIGER-XML so that we can use the same approach as for the other treebanks investigated earlier."
W08-1208,W07-2441,0,0.0119132,"Samuelsson and Volk, 2007)). After having solved the technical issues, the challenge was to compile precise and comprehensive guidelines to ensure smooth and consistent alignment decisions. In (Samuelsson and Volk, 3 Related Research Our research on word and phrase alignment is related to previous work on word alignment as e.g. in the Blinker project (Melamed, 1998) or in the UPLUG project (Ahrenberg et al., 2003). Alignment work on parallel treebanks is rare. Most notably there is the Prague Czech-English treebank (Kruijff-Korbayov´a et al., 2006) and the Link¨oping Swedish-English treebank (Ahrenberg, 2007). There has not been much work on the alignment of linguistically motivated phrases. Tinsley et al. (2007) and Groves et al. (2004) report on semi-automatic phrase alignment as part of their research on example-based machine translation. Considering the fact that the alignment task is essentially a semantic annotation task, we may also compare our results to other tasks in semantic corpus annotation. For example, we may consider the methods for resolving annotation conflicts and the figures for inter-annotator agreement in frame-semantic annotation as found in the German SALSA project (cf. (Bu"
W08-1208,burchardt-etal-2006-salsa,0,0.054238,"Missing"
W08-1208,C04-1154,0,0.0298781,"nsure smooth and consistent alignment decisions. In (Samuelsson and Volk, 3 Related Research Our research on word and phrase alignment is related to previous work on word alignment as e.g. in the Blinker project (Melamed, 1998) or in the UPLUG project (Ahrenberg et al., 2003). Alignment work on parallel treebanks is rare. Most notably there is the Prague Czech-English treebank (Kruijff-Korbayov´a et al., 2006) and the Link¨oping Swedish-English treebank (Ahrenberg, 2007). There has not been much work on the alignment of linguistically motivated phrases. Tinsley et al. (2007) and Groves et al. (2004) report on semi-automatic phrase alignment as part of their research on example-based machine translation. Considering the fact that the alignment task is essentially a semantic annotation task, we may also compare our results to other tasks in semantic corpus annotation. For example, we may consider the methods for resolving annotation conflicts and the figures for inter-annotator agreement in frame-semantic annotation as found in the German SALSA project (cf. (Burchardt et al., 2006)). 4 Our Alignment Guidelines We have compiled alignment guidelines for word and phrase alignment between anno"
W08-1208,W06-2705,0,0.0225049,"sed two colors to visualize a student’s alignment overlap with the Gold Standard in one color and his own alignments (that are not in the Gold Standard) in another color. In order to visualize the agreements of the whole group it would be desirable to have the option to increase the alignment line width in proportion to the number of annotators that have chosen a particular alignment link. This would give an intuitive impression of strong alignment links and weak alignment links. Another option for future extension of this work is an even more elaborate classification of the alignment links. (Hansen-Schirra et al., 2006) have demonstrated how a fine-grained distinction between different alignment types could look like. Annotating such a corpus will be labor-intensive but provide for a wealth of cross-language observations. nesses of the guidelines. For example, one alignment in the Gold Standard that was missed by all students concerns the alignment of a German pronoun (wenn sie die Hand ausstreckte) to an empty token in English (herself shaking hands). Our guidelines recommend to align such cases as fuzzy alignments, but of course it is difficult to determine that the empty token really corresponds to the Ge"
W08-1208,kruijff-korbayova-etal-2006-annotation,0,0.0673921,"Missing"
W08-1208,W06-2717,1,0.83881,"Missing"
W08-1208,smith-jahr-2000-cairo,0,0.0216047,"k guidelines for Swedish. Both German trees and Swedish trees are annotated with flat structures but subsequently automatically deepened to result in richer and linguistically more plausible tree structures. When the monolingual treebanks were finished, we started with the word and phrase alignment. For this purpose we have developed a special tool called the Stockholm TreeAligner (Lundborg et al., 2007) which displays two trees and allows the user to draw alignment lines by clicking on nodes and words. This tool is similar to word alignment tools like ILink (Ahrenberg et al., 2003) or Cairo (Smith and Jahr, 2000). As far as we know our tool is unique in that it allows the alignments of linguistically motivated phrases via node alignments in parallel constituent structure trees (cf. (Samuelsson and Volk, 2007)). After having solved the technical issues, the challenge was to compile precise and comprehensive guidelines to ensure smooth and consistent alignment decisions. In (Samuelsson and Volk, 3 Related Research Our research on word and phrase alignment is related to previous work on word alignment as e.g. in the Blinker project (Melamed, 1998) or in the UPLUG project (Ahrenberg et al., 2003). Alignme"
W08-1208,2007.mtsummit-papers.62,0,0.0231286,"recise and comprehensive guidelines to ensure smooth and consistent alignment decisions. In (Samuelsson and Volk, 3 Related Research Our research on word and phrase alignment is related to previous work on word alignment as e.g. in the Blinker project (Melamed, 1998) or in the UPLUG project (Ahrenberg et al., 2003). Alignment work on parallel treebanks is rare. Most notably there is the Prague Czech-English treebank (Kruijff-Korbayov´a et al., 2006) and the Link¨oping Swedish-English treebank (Ahrenberg, 2007). There has not been much work on the alignment of linguistically motivated phrases. Tinsley et al. (2007) and Groves et al. (2004) report on semi-automatic phrase alignment as part of their research on example-based machine translation. Considering the fact that the alignment task is essentially a semantic annotation task, we may also compare our results to other tasks in semantic corpus annotation. For example, we may consider the methods for resolving annotation conflicts and the figures for inter-annotator agreement in frame-semantic annotation as found in the German SALSA project (cf. (Burchardt et al., 2006)). 4 Our Alignment Guidelines We have compiled alignment guidelines for word and phra"
W08-1208,E03-1086,0,\N,Missing
W09-4610,W07-0735,0,0.0414301,"Missing"
W09-4610,D07-1091,0,0.416643,"Statistical Machine Translation (SMT) system of film subtitles can be improved by using linguistic annotations. To this end, a subset of Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 57–64 Martin Volk Universit¨at Z¨urich Inst. f¨ur Computerlinguistik Binzm¨uhlestrasse 14 CH-8050 Z¨urich volk@cl.uzh.ch 1 million subtitles of the training corpus used by Volk and Harder was morphologically annotated with the DanGram parser (Bick, 2001). We integrated the annotations into the translation process using the methods of factored Statistical Machine Translation (Koehn and Hoang, 2007) implemented in the widely used Moses software. After describing the corpus data and giving a short overview over the methods used, we present a number of experiments comparing different factored SMT setups. The experiments are then replicated with reduced training corpora which contain only part of the available training data. These series of experiments provide insights about the impact of corpus size on the effectivity of using linguistic abstractions for SMT. 2 Machine translation of subtitles As a text genre, subtitles play a curious role in a complex environment of different media and mo"
W09-4610,N03-1017,0,0.0046775,"obability is decomposed into a loglinear combination of a number of feature functions hi (S, T ), which map a pair of a source and a target language element to a score based on different submodels such as translation models or language models. Each feature function is associated with a weight λi that specifies its contribution to the overall score: Tˆ = arg max log p(T |S) T = arg max T ∑ λi hi (S, T ) i The translation models employed in factored SMT are phrase-based. The phrases included in a translation model are extracted from a wordaligned parallel corpus with the techniques described by Koehn et al. (2003). The associated probabilities are estimated by the relative frequencies of the extracted phrase pairs in the same corpus. For language modelling, we used the SRILM toolkit (Stolcke, 2002); unless otherwise specified, 6-gram language models with modified KneserNey smoothing were used. The SMT decoder tries to translate the words and phrases of the source language sentence in the order in which they occur in the input. If the target language requires a different word order, reordering is possible at the cost of a score penalty. The translation model has no notion of sequence, so it cannot contr"
W09-4610,2005.iwslt-1.8,0,0.0444571,"Missing"
W09-4610,P07-2045,0,0.126766,"Statistical Machine Translation (SMT) system of film subtitles can be improved by using linguistic annotations. To this end, a subset of Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 57–64 Martin Volk Universit¨at Z¨urich Inst. f¨ur Computerlinguistik Binzm¨uhlestrasse 14 CH-8050 Z¨urich volk@cl.uzh.ch 1 million subtitles of the training corpus used by Volk and Harder was morphologically annotated with the DanGram parser (Bick, 2001). We integrated the annotations into the translation process using the methods of factored Statistical Machine Translation (Koehn and Hoang, 2007) implemented in the widely used Moses software. After describing the corpus data and giving a short overview over the methods used, we present a number of experiments comparing different factored SMT setups. The experiments are then replicated with reduced training corpora which contain only part of the available training data. These series of experiments provide insights about the impact of corpus size on the effectivity of using linguistic abstractions for SMT. 2 Machine translation of subtitles As a text genre, subtitles play a curious role in a complex environment of different media and mo"
W09-4610,P03-1021,0,0.00961043,"Missing"
W09-4610,P02-1040,0,0.0753254,"Missing"
W09-4610,2007.mtsummit-papers.66,1,0.703767,"yntactic simplicity, and the immense text volumes processed daily by specialised subtitling companies make it possible to produce raw translations of film subtitles with statistical methods quite effectively. If these raw translations are subsequently post-edited by skilled staff, production quality translations can be obtained with considerably less effort than if the subtitles were translated by human translators with no computer assistance. A successful subtitle Machine Translation system for the language pair Swedish–Danish, which has now entered into productive use, has been presented by Volk and Harder (2007). The goal of the present study is to explore whether and how the quality of a Statistical Machine Translation (SMT) system of film subtitles can be improved by using linguistic annotations. To this end, a subset of Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 57–64 Martin Volk Universit¨at Z¨urich Inst. f¨ur Computerlinguistik Binzm¨uhlestrasse 14 CH-8050 Z¨urich volk@cl.uzh.ch 1 million subtitles of the training corpus used by Volk and Harder was morphologically annotated with the DanGram parser (Bick, 2001). We integrated the annotations into the trans"
W09-4610,C90-3030,0,0.0224009,"title text genre in this paper. D´ıazCintas and Remael (2007) provide a detailed introduction, including the linguistics of subtitling and translation issues, and Pedersen (2007) discusses the peculiarities of subtitling in Scandinavia. 3 Constraint Grammar annotations To explore the potential of linguistically annotated data, our complete subtitle corpus, both in Danish and in Swedish, was linguistically analysed with the DanGram Constraint Grammar (CG) parser (Bick, 2001), a system originally developed for the analysis of Danish for which there is also a Swedish grammar. Constraint Grammar (Karlsson, 1990) is a formalism for natural language parsing. Conceptually, a CG parser first produces possible analyses for each word by considering its morphological features and then applies constraining rules to filter out analyses that do not fit into the context. Thus, the word forms are gradually disambiguated, until only one analysis remains; multiple analyses may be retained if the sentence is ambiguous. The annotations produced by the DanGram parser were output as tags attached to individual words as in the following example: $Vad vet du om det $? [vad] &lt;interr&gt; INDP NEU S NOM @ACC&gt; [veta] &lt;mv&gt; V PR"
W10-1830,W07-2441,0,0.0154543,"2003) and from L’Arboratoire (Bick, 2010). The Le Monde treebank is a constituent structure treebank partially annotated with functional labels. L’Arboratoire is based on constraint grammar analysis but can also output constituent trees. 3.1 (2003) have built a parallel treebank for the specific purpose of machine translation, the CzechEnglish Penn Treebank with tecto-grammatical dependency trees. Other parallel treebank projects include Croco (Hansen-Schirra et al., 2006) which is aimed at building a English-German treebank for translation studies, LinES an EnglishSwedish parallel treebank (Ahrenberg, 2007), and the English-French HomeCentre treebank (Hearne and Way, 2006), a hand-crafted parallel treebank consisting of 810 sentence pairs from a Xerox printer manual. Some researchers have tried to exploit parallel treebanks for example-based or statistical machine translation (Tinsley et al., 2009). Since manually created treebanks are too small for this purpose, various researchers have worked on automatically parsing and aligning parallel treebanks. Zhechev (2009) and Tiedemann and Kotz´e (2009) have presented methods for automatic cross-language phrase alignment. There have been various attem"
W10-1830,bick-2010-frag,0,0.0145407,"rg.ch. 192 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 192–196, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics usage instructions for a DVD player (G¨ohring, 2009). We have annotated the English sentences according to the well-established Penn Treebank guidelines. For German we followed the TIGER annotation guidelines, and we adapted these guidelines also for Swedish (see (Volk and Samuelsson, 2004)). For French treebanking we are looking for inspiration from the Le Monde treebank (Abeill´e et al., 2003) and from L’Arboratoire (Bick, 2010). The Le Monde treebank is a constituent structure treebank partially annotated with functional labels. L’Arboratoire is based on constraint grammar analysis but can also output constituent trees. 3.1 (2003) have built a parallel treebank for the specific purpose of machine translation, the CzechEnglish Penn Treebank with tecto-grammatical dependency trees. Other parallel treebank projects include Croco (Hansen-Schirra et al., 2006) which is aimed at building a English-German treebank for translation studies, LinES an EnglishSwedish parallel treebank (Ahrenberg, 2007), and the English-French H"
W10-1830,burchardt-etal-2006-salsa,0,0.0745992,"Missing"
W10-1830,W04-1910,1,0.740864,"”, from economy texts (e.g. business reports from mechanical engineering company ABB and from the bank SEB), and from a technical manual with 1 See www.textberg.ch. 192 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 192–196, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics usage instructions for a DVD player (G¨ohring, 2009). We have annotated the English sentences according to the well-established Penn Treebank guidelines. For German we followed the TIGER annotation guidelines, and we adapted these guidelines also for Swedish (see (Volk and Samuelsson, 2004)). For French treebanking we are looking for inspiration from the Le Monde treebank (Abeill´e et al., 2003) and from L’Arboratoire (Bick, 2010). The Le Monde treebank is a constituent structure treebank partially annotated with functional labels. L’Arboratoire is based on constraint grammar analysis but can also output constituent trees. 3.1 (2003) have built a parallel treebank for the specific purpose of machine translation, the CzechEnglish Penn Treebank with tecto-grammatical dependency trees. Other parallel treebank projects include Croco (Hansen-Schirra et al., 2006) which is aimed at bu"
W10-1830,W06-2705,0,0.0250606,"also for Swedish (see (Volk and Samuelsson, 2004)). For French treebanking we are looking for inspiration from the Le Monde treebank (Abeill´e et al., 2003) and from L’Arboratoire (Bick, 2010). The Le Monde treebank is a constituent structure treebank partially annotated with functional labels. L’Arboratoire is based on constraint grammar analysis but can also output constituent trees. 3.1 (2003) have built a parallel treebank for the specific purpose of machine translation, the CzechEnglish Penn Treebank with tecto-grammatical dependency trees. Other parallel treebank projects include Croco (Hansen-Schirra et al., 2006) which is aimed at building a English-German treebank for translation studies, LinES an EnglishSwedish parallel treebank (Ahrenberg, 2007), and the English-French HomeCentre treebank (Hearne and Way, 2006), a hand-crafted parallel treebank consisting of 810 sentence pairs from a Xerox printer manual. Some researchers have tried to exploit parallel treebanks for example-based or statistical machine translation (Tinsley et al., 2009). Since manually created treebanks are too small for this purpose, various researchers have worked on automatically parsing and aligning parallel treebanks. Zhechev"
W10-1830,W07-1514,1,0.901602,"Missing"
W10-1830,2006.eamt-1.8,0,0.014218,"ank is a constituent structure treebank partially annotated with functional labels. L’Arboratoire is based on constraint grammar analysis but can also output constituent trees. 3.1 (2003) have built a parallel treebank for the specific purpose of machine translation, the CzechEnglish Penn Treebank with tecto-grammatical dependency trees. Other parallel treebank projects include Croco (Hansen-Schirra et al., 2006) which is aimed at building a English-German treebank for translation studies, LinES an EnglishSwedish parallel treebank (Ahrenberg, 2007), and the English-French HomeCentre treebank (Hearne and Way, 2006), a hand-crafted parallel treebank consisting of 810 sentence pairs from a Xerox printer manual. Some researchers have tried to exploit parallel treebanks for example-based or statistical machine translation (Tinsley et al., 2009). Since manually created treebanks are too small for this purpose, various researchers have worked on automatically parsing and aligning parallel treebanks. Zhechev (2009) and Tiedemann and Kotz´e (2009) have presented methods for automatic cross-language phrase alignment. There have been various attempts to enrich treebanks with semantic information. For example, the"
W10-1830,volk-etal-2010-challenges,1,0.700717,"ding information in our treebank. 1 2 Our Text+Berg Corpus In our project Text+Berg1 we digitize alpine heritage literature from various European countries. Currently our group digitizes all yearbooks of the Swiss Alpine Club (SAC) from 1864 until today. Each yearbook consists of 300 to 600 pages and contains reports on mountain expeditions, culture of mountain peoples, as well as the flora, fauna and geology of the mountains. The corpus preparation presented interesting challenges in automatic OCR correction, language identification, and text structure recognition which we have described in (Volk et al., 2010). As of March 2010 we have scanned and OCRconverted 142 books from 1864 to 1982, corresponding to nearly 70,000 pages. This resulted in a multilingual corpus of 6101 articles in German, 2659 in French, 155 in Italian, 13 in Romansch, and 3 in Swiss-German. The parallel part of our corpus currently contains 701 translated articles amounting to 2.6 million tokens in French and 2.3 million tokens in German. Introduction Treebanks have become valuable resources in natural language processing as training corpora for natural language parsers, as repositories for linguistic research, or as evaluation"
W10-1830,C08-1139,0,\N,Missing
W11-4115,J04-4003,0,0.0737341,"Missing"
W11-4115,C04-1120,0,\N,Missing
W11-4624,P91-1022,0,0.667755,"proach can supersede the dependence of the algorithm on existing MT systems. (Sennrich and Volk, 2010) have demonstrated that the sentence alignment quality of their MTbased algorithm depends on the quality of the MT system. If we can produce a superior MT system using Bleualign, it is worthwhile to test if the resulting MT system can in turn be used for an even Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 175–182 Rico Sennrich and Martin Volk better sentence alignment. 2 Related Work The first sentence alignment algorithms by (Brown et al., 1991) and (Gale and Church, 1993) are based on a length-comparison between source and target text and work without language-specific information.1 A second strand of sentence alignment algorithms work with lexical correspondences. This is either done on the basis of correspondence rules (Simard et al., 1993), with external dictionaries (Varga et al., 2005), or using a translation model trained on the parallel text itself (Moore, 2002; Varga et al., 2005). The latter requires a preliminary sentence alignment of the parallel text, usually performed with a lengthbased algorithm. After this first pass,"
W11-4624,P07-2045,0,0.0218326,"lowing steps: 1. Sentence-align the parallel training corpus. • In the first iteration, use an implementation of the Gale & Church algorithm (or any other sentence alignment tool that does not require additional resources). • In all subsequent iterations: – Automatically translate the corpus using the SMT system trained in the last iteration. – Align the texts using Bleualign and this translation. 2. Train an SMT system on the sentence-aligned corpus. The language model needs only be trained once; we use SRILM (Stolcke, 2002). The SMT system is built with GIZA++ (Och and Ney, 2003) and Moses (Koehn et al., 2007). The most timeconsuming part of each iteration is typically the automatic translation of the training set. In the remainder of this section, we will discuss how the alignment algorithm works, and what potential problems the iterative approach brings. 4.1 The Sentence Alignment Algorithm The sentence alignment algorithm, first described in (Sennrich and Volk, 2010), is a two-pass approach. In the first pass, dynamic programming is used to find a set of 1-to-1 beads that maximizes BLEU score in the document without violating the monotonic order of sentence pairs. In the second pass, unaligned s"
W11-4624,W04-3250,0,0.0459109,"es (Koehn et al., 2007), will be evaluated on a test set of 1000 sentences, held-out from training. The training set consists of 3 300 000 German and 3 740 000 French tokens, measured before sentence alignment.7 We test translation performance in the direction DE–FR, and use a language model trained on 9 511 000 tokens of in-domain text. We did not perform Minimum Error Rate Training, which is typically the most time-intensive step of training an SMT system, in order to limit the computational cost of the iterative approach. Statistical significance is tested with paired bootstrap resampling (Koehn, 2004). 5.1 Results We first establish baseline scores achieved by either using the Gale & Church algorithm or Bleualign with an out-of-domain MT system, shown in table 2. For a wider comparison of different sentence alignment algorithms, see (Lambert et al., 2010). On the alignment test set, Gale & Church’s algorithm fails almost entirely; only 1 out of 468 alignment hypotheses is correct. The reason for the bad performance of the Gale & Church algorithm in this evaluation is that errors tend to propagate, since misaligned sentences may cause neighbouring sentences to be misaligned as well. This is"
W11-4624,J03-1002,0,0.0238675,"iteration consists of the following steps: 1. Sentence-align the parallel training corpus. • In the first iteration, use an implementation of the Gale & Church algorithm (or any other sentence alignment tool that does not require additional resources). • In all subsequent iterations: – Automatically translate the corpus using the SMT system trained in the last iteration. – Align the texts using Bleualign and this translation. 2. Train an SMT system on the sentence-aligned corpus. The language model needs only be trained once; we use SRILM (Stolcke, 2002). The SMT system is built with GIZA++ (Och and Ney, 2003) and Moses (Koehn et al., 2007). The most timeconsuming part of each iteration is typically the automatic translation of the training set. In the remainder of this section, we will discuss how the alignment algorithm works, and what potential problems the iterative approach brings. 4.1 The Sentence Alignment Algorithm The sentence alignment algorithm, first described in (Sennrich and Volk, 2010), is a two-pass approach. In the first pass, dynamic programming is used to find a set of 1-to-1 beads that maximizes BLEU score in the document without violating the monotonic order of sentence pairs."
W11-4624,P02-1040,0,0.086343,"Missing"
W11-4624,2010.amta-papers.14,1,0.846756,"r performance, it is not sufficient to only use easily accessible and alignable texts as training material for SMT systems; ideally, SMT systems should be trained on texts that are similar to those one wishes to translate. This warrants continued research on more robust sentence alignment algorithms. Bleualign is a sentence alignment algorithm that, instead of computing an alignment between the source and target text directly, bases its alignment search on an MT translation of the source text. It has been shown that Bleualign can robustly align texts for which other algorithms perform poorly (Sennrich and Volk, 2010). The quality of sentence alignment has an effect on the performance of SMT systems (Lambert et al., 2010), but high quality is also desirable for other purposes, e.g. when building a translation memory from a text corpus. The main disadvantage of an MT-based algorithm is that it requires an existing MT system. For resource-poor language pairs, this requirement makes the algorithm unattractive. We have investigated the bootstrapping of MTbased sentence alignment with an MT system trained on the to-be-aligned corpus. For this first MT system, a length-based sentence alignment algorithm is used"
W11-4624,J93-1004,0,0.946152,"dependence of the algorithm on existing MT systems. (Sennrich and Volk, 2010) have demonstrated that the sentence alignment quality of their MTbased algorithm depends on the quality of the MT system. If we can produce a superior MT system using Bleualign, it is worthwhile to test if the resulting MT system can in turn be used for an even Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 175–182 Rico Sennrich and Martin Volk better sentence alignment. 2 Related Work The first sentence alignment algorithms by (Brown et al., 1991) and (Gale and Church, 1993) are based on a length-comparison between source and target text and work without language-specific information.1 A second strand of sentence alignment algorithms work with lexical correspondences. This is either done on the basis of correspondence rules (Simard et al., 1993), with external dictionaries (Varga et al., 2005), or using a translation model trained on the parallel text itself (Moore, 2002; Varga et al., 2005). The latter requires a preliminary sentence alignment of the parallel text, usually performed with a lengthbased algorithm. After this first pass, a translation model can be"
W11-4624,D07-1103,0,0.401493,"timation that the phrase - AlbertEggler : is translated to 1954 , Helmut Heuberger en g´eographie ; with a probability of 1.5 Hence, the sentence is mistranslated during the next iteration. The problem is that such mistranslations may cause the same alignment errors to be made in subsequent iterations. In order to prevent random misalignments to 5 The term phrase is used to denote arbitrary word sequences in SMT, without syntactic implications. In this case, the whole sentence is treated as a single phrase by the SMT system. be fossilized, we prune the translation model using the approach by (Johnson et al., 2007). The pruning is based on computing whether the cooccurrence frequency of phrase pairs in the translation model is statistically significant, or to be expected by chance. All phrase pairs whose significance value fall below a predefined threshold are discarded. We chose the significance threshold α + , which among others discards all phrase pairs that co-occur only once.6 5 Evaluation For the evaluation of alignment quality, we manually aligned an article consisting of 468 and 554 sentences (German and French, respectively). This manual alignment serves as a gold standard to which the automat"
W11-4624,volk-etal-2010-challenges,1,0.855556,"Missing"
W11-4624,moore-2002-fast,0,\N,Missing
W11-4624,J93-2003,0,\N,Missing
W11-4633,E06-1032,0,0.0633045,"Missing"
W11-4633,P07-1005,0,0.0257915,"researchers have worked on this topic before with varying success. For example, (Carpuat and Wu, 2005) reported that they could not find “significant better translations” when using Chinese WSD in Chinese-English MT. But two years later the same authors (Carpuat and Wu, 2007) come to the conclusion that the incorporation of WSD within a typical SMT system “consistently improves translation quality” for Chinese-English. They claim that a disambiguation of phrasal units rather than words leads to these improvements. They report on gains of up to 0.5 BLEU points. These findings are in line with (Chan et al., 2007) who have also shown WSD to be beneficial for Chinese to English translation. Basic research on WSD for MT is presented in various papers. For example (Specia et al., 2005) work with automatically derived rules for WSD of seven highly ambiguous verbs in EnglishPortuguese MT. (Apidianaki, 2009) questions the sense inventory which is frequently used in WSD and argues for a semantic analysis based on parallel corpora Greek-English in order to better tailor the sense inventory to MT. (Vickrey et al., 2005) investigate WSD for word translation FrenchEnglish. Our work is also similar to other prepro"
W11-4633,2010.iwslt-papers.6,0,0.0220094,"e beneficial for Chinese to English translation. Basic research on WSD for MT is presented in various papers. For example (Specia et al., 2005) work with automatically derived rules for WSD of seven highly ambiguous verbs in EnglishPortuguese MT. (Apidianaki, 2009) questions the sense inventory which is frequently used in WSD and argues for a semantic analysis based on parallel corpora Greek-English in order to better tailor the sense inventory to MT. (Vickrey et al., 2005) investigate WSD for word translation FrenchEnglish. Our work is also similar to other preprocessing suggestions such as (El-Kahlout and Yvon, 2010). They work on the opposite translation direction and prepare the German input text before training and translation to English. When testing various normalization steps, they obtained the biggest improvements on compound splitting. 3 Our MT Systems for TV Subtitles MT systems for subtitles date back to the work by Popowich et al. (2000) on English to Spanish translation. We have built Machine Translation systems for translating film and TV subtitles from English to Swedish and from Swedish to Danish and Norwegian in a commercial setting. Some of this work has been described earlier by Volk and"
W11-4633,W00-0506,0,0.0467053,"a semantic analysis based on parallel corpora Greek-English in order to better tailor the sense inventory to MT. (Vickrey et al., 2005) investigate WSD for word translation FrenchEnglish. Our work is also similar to other preprocessing suggestions such as (El-Kahlout and Yvon, 2010). They work on the opposite translation direction and prepare the German input text before training and translation to English. When testing various normalization steps, they obtained the biggest improvements on compound splitting. 3 Our MT Systems for TV Subtitles MT systems for subtitles date back to the work by Popowich et al. (2000) on English to Spanish translation. We have built Machine Translation systems for translating film and TV subtitles from English to Swedish and from Swedish to Danish and Norwegian in a commercial setting. Some of this work has been described earlier by Volk and Harder (2007) and Volk (2008). Most films are originally in English and receive English or Swedish subtitles in a first manual step. The subtitler uses the English video and audio (sometimes accompanied by an English transcript). The target language translator subsequently has access to the original English video and audio but also to"
W11-4633,H05-1097,0,0.0211466,"when using Chinese WSD in Chinese-English MT. But two years later the same authors (Carpuat and Wu, 2007) come to the conclusion that the incorporation of WSD within a typical SMT system “consistently improves translation quality” for Chinese-English. They claim that a disambiguation of phrasal units rather than words leads to these improvements. They report on gains of up to 0.5 BLEU points. These findings are in line with (Chan et al., 2007) who have also shown WSD to be beneficial for Chinese to English translation. Basic research on WSD for MT is presented in various papers. For example (Specia et al., 2005) work with automatically derived rules for WSD of seven highly ambiguous verbs in EnglishPortuguese MT. (Apidianaki, 2009) questions the sense inventory which is frequently used in WSD and argues for a semantic analysis based on parallel corpora Greek-English in order to better tailor the sense inventory to MT. (Vickrey et al., 2005) investigate WSD for word translation FrenchEnglish. Our work is also similar to other preprocessing suggestions such as (El-Kahlout and Yvon, 2010). They work on the opposite translation direction and prepare the German input text before training and translation t"
W11-4633,2007.mtsummit-papers.66,1,0.816798,"n, 2010). They work on the opposite translation direction and prepare the German input text before training and translation to English. When testing various normalization steps, they obtained the biggest improvements on compound splitting. 3 Our MT Systems for TV Subtitles MT systems for subtitles date back to the work by Popowich et al. (2000) on English to Spanish translation. We have built Machine Translation systems for translating film and TV subtitles from English to Swedish and from Swedish to Danish and Norwegian in a commercial setting. Some of this work has been described earlier by Volk and Harder (2007) and Volk (2008). Most films are originally in English and receive English or Swedish subtitles in a first manual step. The subtitler uses the English video and audio (sometimes accompanied by an English transcript). The target language translator subsequently has access to the original English video and audio but also to the source language subtitles and the time codes. In most cases the translator will reuse the time codes and insert the target language subtitle. She can, on occasion, change the time codes if she deems them inappropriate for the target language text. We have built SMT system"
W11-4633,2010.jec-1.7,1,0.900795,"Missing"
W11-4633,P05-1048,0,0.0351715,"ins also capitalized subtitles like CLEOPATRA’S BEAUTY SALON which we have not counted here. Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 238–245 Disambiguation of English Contractions for Machine Translation of TV Subtitles by describing some related disambiguation work and then our MT systems for subtitles. 2 Related Work on Word Sense Disambiguation for MT Our approach can be seen as a special type of word sense disambiguation (WSD) for MT. Many researchers have worked on this topic before with varying success. For example, (Carpuat and Wu, 2005) reported that they could not find “significant better translations” when using Chinese WSD in Chinese-English MT. But two years later the same authors (Carpuat and Wu, 2007) come to the conclusion that the incorporation of WSD within a typical SMT system “consistently improves translation quality” for Chinese-English. They claim that a disambiguation of phrasal units rather than words leads to these improvements. They report on gains of up to 0.5 BLEU points. These findings are in line with (Chan et al., 2007) who have also shown WSD to be beneficial for Chinese to English translation. Basic"
W11-4633,D07-1007,0,0.0319294,"1 Conference Proceedings, pp. 238–245 Disambiguation of English Contractions for Machine Translation of TV Subtitles by describing some related disambiguation work and then our MT systems for subtitles. 2 Related Work on Word Sense Disambiguation for MT Our approach can be seen as a special type of word sense disambiguation (WSD) for MT. Many researchers have worked on this topic before with varying success. For example, (Carpuat and Wu, 2005) reported that they could not find “significant better translations” when using Chinese WSD in Chinese-English MT. But two years later the same authors (Carpuat and Wu, 2007) come to the conclusion that the incorporation of WSD within a typical SMT system “consistently improves translation quality” for Chinese-English. They claim that a disambiguation of phrasal units rather than words leads to these improvements. They report on gains of up to 0.5 BLEU points. These findings are in line with (Chan et al., 2007) who have also shown WSD to be beneficial for Chinese to English translation. Basic research on WSD for MT is presented in various papers. For example (Specia et al., 2005) work with automatically derived rules for WSD of seven highly ambiguous verbs in Engl"
W11-4633,E09-1010,0,\N,Missing
W13-2514,W06-2810,0,0.480262,"Missing"
W13-2514,W11-2107,0,0.0134056,"he extraction pipeline. Although they both contain information about the valleys connected by the Turini pass, the German sentence contains a fragment about its position, which has not been translated into French. DE: Der Pass liegt in der a¨ usseren, besiedelten Zone des Nationalpark Mercantour und stellt den ¨ Ubergang zwischen dem Tal der B´ev´era und dem Tal der V´esubie dar. • METEOR score We used the METEOR similarity metric because, unlike other string-based metrics (e.g. BLEU (Papineni et al., 2002)), it considers not only exact matches, but also word stems, synonyms, and paraphrases (Denkowski and Lavie, 2011). Suppose that we compute the similarity between the following sentences in French: j’ aimerais bien vous voir and je voudrais vous voir (both meaning I would like to see you). BLEU, which is a stringbased metric, would assign a similarity score of 52.5. This value could hardly be considered reliable, given that the sentence ta voiture vous voir (paired with the first sentence) would get the same BLEU score, although the latter sentence (EN: your car see you) is obviously nonsense. On the other hand, METEOR would return a score of 90.3 for the original sentence pair, since it can appreciate th"
W13-2514,W04-3208,0,0.0758699,"Missing"
W13-2514,2005.mtsummit-papers.11,0,0.0611791,"conducted experiments have focused only on the extraction of parallel clauses and their use in a 6 117 http://www.statmt.org/wmt11/baseline.html Translation model Europarl TM Europarl TM EP+Wiki TM Europarl TM EP+Wiki TM EP+WMix TM SAC TM SAC+Wiki TM SAC+WMix TM lation models, language models or both). The first baseline system is an in-domain one, trained on the Text+Berg corpus and is the same used for the automatic translations required in the extraction step (see section 3). The second system is purely outof-domain and it is trained on Europarl, a collection of parliamentary proceedings (Koehn, 2005). The development set and the test set contain indomain data, held out from the Text+Berg corpus. Table 3 lists the sizes of the data sets used for the SMT experiments. Data set SAC Europarl Wikipedia Dev set Test set Sentences 220 000 1 680 000 120 000 1424 991 DE Words 4 200 000 37 000 000 1 000 000 30 000 19 000 Language model EP LM EP+Wiki LM EP+Wiki LM EP+Wiki+SAC LM EP+Wiki+SAC LM EP+Wiki+SAC LM SAC LM SAC+Wiki LM SAC+Wiki LM BLEU score 9.45 10.39 10.37 11.22 11.74 10.40 16.71 16.51 16.37 Table 4: SMT results for German-French FR Words 4 700 000 43 000 000 1 000 000 33 000 21 000 transla"
W13-2514,J05-4003,0,0.108747,"Missing"
W13-2514,P02-1040,0,0.0883309,"t span over the whole sentence. As an example, consider the following sentences which have been retrieved by the extraction pipeline. Although they both contain information about the valleys connected by the Turini pass, the German sentence contains a fragment about its position, which has not been translated into French. DE: Der Pass liegt in der a¨ usseren, besiedelten Zone des Nationalpark Mercantour und stellt den ¨ Ubergang zwischen dem Tal der B´ev´era und dem Tal der V´esubie dar. • METEOR score We used the METEOR similarity metric because, unlike other string-based metrics (e.g. BLEU (Papineni et al., 2002)), it considers not only exact matches, but also word stems, synonyms, and paraphrases (Denkowski and Lavie, 2011). Suppose that we compute the similarity between the following sentences in French: j’ aimerais bien vous voir and je voudrais vous voir (both meaning I would like to see you). BLEU, which is a stringbased metric, would assign a similarity score of 52.5. This value could hardly be considered reliable, given that the sentence ta voiture vous voir (paired with the first sentence) would get the same BLEU score, although the latter sentence (EN: your car see you) is obviously nonsense."
W13-2514,W05-0908,0,0.0175546,"investigations with regard to the usefulness of the extracted corpus for SMT. In this evaluation scenario, we use only pairs with a similarity score above 0.35. The results discussed in this section refer only to the translation direction GermanFrench. The SMT systems are trained with the Moses toolkit (Koehn et al., 2007), according to the WMT 2011 guidelines6 . The translation performance was measured using the BLEU evaluation metric on a single reference translation. We also report statistical significance scores, in order to indicate the validity of the comparisons between the MT systems (Riezler and Maxwell, 2005). We consider the BLEU score difference significant if the computed p-value is below 0.05. We compare two baseline MT systems and several systems with different model mixtures (transExperiments and Results The conducted experiments have focused only on the extraction of parallel clauses and their use in a 6 117 http://www.statmt.org/wmt11/baseline.html Translation model Europarl TM Europarl TM EP+Wiki TM Europarl TM EP+Wiki TM EP+WMix TM SAC TM SAC+Wiki TM SAC+WMix TM lation models, language models or both). The first baseline system is an in-domain one, trained on the Text+Berg corpus and is"
W13-2514,E12-1055,0,0.0139843,"o, we added the extracted data to an SMT system for which no in-domain parallel data was available. For this purpose, we experimented with different combinations of the models involved in the translation process, namely the German-French translation model (responsible for the translation variants) and the French language model (ensures the fluency of the output). Besides of the models trained on the parallel data available in each of the data sets, we also built combined models with optimized weights for each of the involved data sets. The optimization was performed with the tools provided by Sennrich (2012) as part of the Moses toolkit. We also want to compare several language models, some trained on the individual data sets, others obtained by linearly interpolating different data sets, all optimized for minimal perplexity on the in-domain development set. The results are summarized in table 4. A first remark is that an out-of-domain language model (LM) adapted with in-domain data (extracted from Wikipedia and/or SAC data) significantly improves on top of a baseline system trained with out-of-domain texts (Europarl, EP) with up to 1.7 BLEU points. And this improvement can be achieved with only"
W13-2514,N10-1063,0,0.0603508,"ods for identifying parallel sentences across it based on monolingual sentence similarity (MT and respectively, lexicon based). Fung et al. (2010) approach the problem by combining recalland precision-oriented methods for sentence alignment, such as the DK-vec algorithm or algorithms based on cosine similarities. Both approaches have achieved good results in terms of precision and recall. However, we are interested in real application scenarios, such as SMT systems. The following approaches report significant performance improvements when using the extracted data as training material for SMT: Smith et al. (2010) use a maximum entropy-based classifier with various feature functions (e.g. alignment coverage, word fertility, translation probability, distortion). S¸tef˘anescu et al. (2012) propose an algorithm based on cross-lingual information retrieval, which also considers similarity features equivalent to the ones mentioned in the previous paper. The presented approaches extract general purpose sentences, but we are interested in a specific topical domain. We have previously tackled the problem (Plamada and Volk, 2012) and encountered two major bottlenecks: the alignment algorithm for matching possib"
W13-2514,2012.eamt-1.37,0,0.0360675,"Missing"
W13-2514,P07-2045,0,\N,Missing
W13-2902,P05-1033,0,0.156843,"Missing"
W13-2902,W11-1601,0,0.0670495,"ith respect to the language and domain of our data and suggest ways of circumventing them. 1 Introduction Simple language (or, “plain language”, “easy-toread language”) is language with low lexical and syntactic complexity. It provides access to information to people with cognitive disabilities (e.g., aphasia, dyslexia), foreign language learners, Deaf people,1 and children. Text in simple language is obtained through simplification. Simplification is a text-to-text generation task involving multiple operations, such as deletion, rephrasing, reordering, sentence splitting, and even insertion (Coster and Kauchak, 2011a). By contrast, paraphrasing and compression, two other text-to-text generation tasks, involve merely rephrasing and reordering (paraphrasing) and deletion (compression). Text simplification also shares common ground with grammar and style checking as well as with controlled natural language generation. Text simplification approaches exist for various languages, including English, French, Spanish, and Swedish. As Matausch and Nietzio (2012) write, “plain language is still underrepresented in 1 It is an often neglected fact that Deaf people tend to exhibit low literacy skills (Gutjahr, 2006)."
W13-2902,P11-2117,0,0.171494,"ith respect to the language and domain of our data and suggest ways of circumventing them. 1 Introduction Simple language (or, “plain language”, “easy-toread language”) is language with low lexical and syntactic complexity. It provides access to information to people with cognitive disabilities (e.g., aphasia, dyslexia), foreign language learners, Deaf people,1 and children. Text in simple language is obtained through simplification. Simplification is a text-to-text generation task involving multiple operations, such as deletion, rephrasing, reordering, sentence splitting, and even insertion (Coster and Kauchak, 2011a). By contrast, paraphrasing and compression, two other text-to-text generation tasks, involve merely rephrasing and reordering (paraphrasing) and deletion (compression). Text simplification also shares common ground with grammar and style checking as well as with controlled natural language generation. Text simplification approaches exist for various languages, including English, French, Spanish, and Swedish. As Matausch and Nietzio (2012) write, “plain language is still underrepresented in 1 It is an often neglected fact that Deaf people tend to exhibit low literacy skills (Gutjahr, 2006)."
W13-2902,W03-1004,0,0.294555,"have been created by drawing on external information in addition. 3.2 Sentence Alignment Algorithm Sentence alignment algorithms differ according to whether they have been developed for bilingual or monolingual corpora. For bilingual parallel corpora many—typically length-based—algorithms exist. However, our data was monolingual. While the length of a regular/simple language sentence pair might be different, an overlap in vocabulary can be expected. Hence, monolingual sentence alignment algorithms typically exploit lexical similarity. We applied the monolingual sentence alignment algorithm of Barzilay and Elhadad (2003). The algorithm has two main features: Firstly, it uses a hierarchical approach by assigning paragraphs to clusters and learning mapping rules. Secondly, it aligns sentences despite low lexical similarity if the context suggests an alignment. This is achieved through local sequence alignment, a dynamic programming algorithm. The overall algorithm has two phases, a training and a testing phase. The training phase in turn consists of two steps: Firstly, all paragraphs of the texts of one side of the parallel corpus (henceforth referred to as “AS texts”) are clustered independently of all paragra"
W13-2902,W11-1603,0,0.179308,"erpart. 5 Hence, the corpus as a whole is a monolingual parallel corpus. 6 The underlying assumption here was that not every sentence needs simplification. 7 Note that this runs contrary to the assumption Coster and Kauchak (2011a; 2011b) made. 12 input sentences were given preference over those that were identical. Difference was calculated on the basis of the Levenshtein score (edit distance). Wubben et al. (2012) found their system to work better than that of Zhu et al. (2010) when evaluated with BLEU, but not when evaluated with the Flesch-Kincaid grade level, a common readability metric. Bott and Saggion (2011) presented a monolingual sentence alignment algorithm, which uses a Hidden Markov Model for alignment. In contrast to other monolingual alignment algorithms, Bott and Saggion (2011) introduced a monotonicity restriction, i.e., they assumed the order of sentences to be the same for the original and simplified texts. Apart from purely rule-based and purely corpus-based approaches to text simplification, hybrid approaches exist. For example, Bott et al. (2012) in their Simplext project for Spanish8 let a statistical classifier decide for each sentence of a text whether it should be simplified (co"
W13-2902,W12-2910,0,0.149483,"Zhu et al. (2010) when evaluated with BLEU, but not when evaluated with the Flesch-Kincaid grade level, a common readability metric. Bott and Saggion (2011) presented a monolingual sentence alignment algorithm, which uses a Hidden Markov Model for alignment. In contrast to other monolingual alignment algorithms, Bott and Saggion (2011) introduced a monotonicity restriction, i.e., they assumed the order of sentences to be the same for the original and simplified texts. Apart from purely rule-based and purely corpus-based approaches to text simplification, hybrid approaches exist. For example, Bott et al. (2012) in their Simplext project for Spanish8 let a statistical classifier decide for each sentence of a text whether it should be simplified (corpus-based approach). The actual simplification was then performed by means of a rule-based approach. As has been shown, many MT approaches to text simplification have used English/Simple English Wikipedia as their data. The only exception we know of is Specia (2010), who together with her colleagues in the PorSimples project built her own parallel corpus. This is presumably because there exists no Simple Brazilian Portuguese Wikipedia. The same is true for"
W13-2902,P12-1107,0,0.11613,"Missing"
W13-2902,D07-1091,0,0.027352,"Missing"
W13-2902,N10-1056,0,0.0841246,"Missing"
W13-2902,P07-2045,0,0.0107951,"Missing"
W13-2902,C10-1152,0,0.311099,"onstructions, etc. Moreover, definitions of difficult terms or concepts are often added, e.g., the term web crawler is defined as “a computer program that searches the Web automatically”. Gasperin et al. (2010) pursued a rule-based approach to text simplification for Brazilian Portuguese within the PorSimples project,2 as did Brouwers et al. (2012) for French. As part of the corpus-based approach, machine translation (MT) has been employed. Yatskar et al. (2010) pointed out that simplification is “a form of MT in which the two ‘languages’ in question are highly related”. As far as we can see, Zhu et al. (2010) were the first to use English/Simple English Wikipedia data for automatic simplification via machine translation.3 They assembled a monolingual comparable corpus4 of 108,016 sentence pairs based on the interlanguage links in Wikipedia and the sentence alignment algorithm of Nelken and Shieber (2006) (cf. Section 3.2). Their system applies a “tree-based simplification model” including machine translation techniques. The system learns probabilities for simplification operations (substitution, reordering, splitting, deletion) offline from 2 http://www2.nilc.icmc.usp.br/wiki/ index.php/English 3"
W13-2902,E06-1021,0,0.25631,"in the PorSimples project,2 as did Brouwers et al. (2012) for French. As part of the corpus-based approach, machine translation (MT) has been employed. Yatskar et al. (2010) pointed out that simplification is “a form of MT in which the two ‘languages’ in question are highly related”. As far as we can see, Zhu et al. (2010) were the first to use English/Simple English Wikipedia data for automatic simplification via machine translation.3 They assembled a monolingual comparable corpus4 of 108,016 sentence pairs based on the interlanguage links in Wikipedia and the sentence alignment algorithm of Nelken and Shieber (2006) (cf. Section 3.2). Their system applies a “tree-based simplification model” including machine translation techniques. The system learns probabilities for simplification operations (substitution, reordering, splitting, deletion) offline from 2 http://www2.nilc.icmc.usp.br/wiki/ index.php/English 3 English Wikipedia: http://en.wikipedia. org/; Simple English Wikipedia: http://simple. wikipedia.org/. 4 We consider this corpus to be comparable rather than parallel because not every Simple English Wikipedia article is necessarily a translation of an English Wikipedia article. Rather, Simple Englis"
W13-2902,P02-1040,0,0.0964171,"Missing"
W13-2902,W04-3219,0,0.120782,"Missing"
W13-2902,P99-1068,0,0.226897,"Missing"
W13-5630,W12-3102,0,0.0245979,"onal effort. We provided our subject with a source segment (DE) alongside a reference translation (FR/IT) and two translation hypotheses for 150 randomly selected segments. One hypothesis was produced by the baseline, the other by the weighted TM system. The subject’s task was to rate which hypothesis was better, (a) in general and (b) with regard to domain technology, with ties allowed. This evaluation setup, known as pairwise system comparison, has lately been favored by the MT community for it is simpler and better reproducible than, e.g., fluency/adequacy judgements on a five-point scale (Callison-Burch et al., 2012). Furthermore, genuine differences between two systems can easily be quantified using the Sign Test for paired observations. The results, shown in Table 3, reveal the evaluator’s clear preference towards the weighted TM system’s translations. This holds especially for DE–IT. Statistically, the weighted TM systems outperform the baseline in all regards at p ≤ 0.001, except for the adequacy of domain terminology in the DE–FR systems (p ≤ 0.01). 4 Discusson Our evaluation (see Section 3.3) shows that adding large amounts of out-of-domain data without adequate weights is no suitable means of impro"
W13-5630,P11-2031,0,0.0816429,"Missing"
W13-5630,W11-2107,0,0.029414,"Missing"
W13-5630,N09-1046,0,0.0474811,"Missing"
W13-5630,W07-0712,0,0.0321312,"ing Data Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 334 of 474] 3.2 Domain-Specific SMT Systems We have trained an in-domain-only baseline and three systems that combine in-domain and out-of-domain data (see Section 3.1) in different weighting modes for each language pair. 3.2.1 Baseline Systems Using only the in-domain data, we trained a standard phrase-based statistical machine translation system for DE–FR and DE–IT. 5-gram language models were trained using the IRST Language Modeling Toolkit (Federico and Cettolo, 2007). Otherwise, we relied on the Moses decoder (Koehn et al., 2007) and its dedicated scripts for training, tokenization, and truecasing. 3.2.2 Combined Systems Additionally, we trained separate translation and language models using our out-of-domain corpora. Apart from the training data, the setup was the same as for our baseline systems. This left us with three phrase tables and language models per language pair: in-domain, EP7, and OS11. All combined systems presented in this section comprise all of these models, but they are differently weighted. The weighting modes are defined as follows. Un"
W13-5630,W07-0717,0,0.532367,"Senellart (2010) have proposed to retrieve a fuzzy match in the TM for each source segment to be translated, identify the mismatched parts, and replace these parts by an SMT translation. Their approach relies on automatic word alignment to find the target words that are affected by the mismatch. In contrast to Koehn and Senellart (2010) we approach combining parallel texts from domainspecific translation memories and general-domain corpora as a domain adaptation problem. We use the approach of mixture-modeling, commonly used for language model adaptation and extended to translation models by Foster and Kuhn (2007). The main distinctive feature of this approach for both language and translation models is that instead of separating the data and models into in-domain/out-of-domain in a binary setting, different domains are assigned real-valued weights, reflecting their similarity to in-domain text material. Using these weights the single domain models p = {pi }i=1...N are combined into a single adapted model: p(x) = wp(x) = N X w i pi (x). i=1 The weights w are selected to optimize the performance of the adapted model on an in-domain development set. Language model performance is estimated with its entrop"
W13-5630,W10-1710,0,0.0258641,"Missing"
W13-5630,2010.jec-1.3,0,0.0226418,"ublished by the European Union (Pym et al., 2012), there is evidence that the per-word rate for professional translations has decreased significantly in some western European countries. On the other hand, computer-assisted translation tools such as translation memory (TM) systems—allowing translators to store their translations in a personal or corporate database and reuse them on future occasions—have become an integral part of state-of-the-art translation workflows, even for freelancers. More recently, there have been efforts to combine machine translation systems with translation memories (Kanavos and Kartsaklis, 2010; Koehn and Senellart, 2010). As a result, major commercial systems such as SDL TradosStudio1 , Across LanguageServer2 , and even open source alternatives such as OmegaT3 now offer machine translation interfaces, allowing translators to have segments automatically (pre-)translated in case there is no corresponding translation available in their translation memory. In a joint project between the UZH Institute of Computational Linguistics and SemioticTransfer AG, we currently explore the potential of using domain-specific statistical machine translation (SMT) systems in human-based translation w"
W13-5630,2005.mtsummit-papers.11,0,0.0237065,"t may refer to a whole sentence, but also to smaller units such as short phrases or even single words of table entries. After cleaning the memories, we were able to extract 166’957 segments for DE–FR and 112’166 segments for DE–IT, which corresponds to ∼2.0 and ∼1.5 million tokens, respectively. Please note that all segments are unique. Unlike text from a corpus of running words, each segment from a translation memory occurs only once. The exact numbers are shown in Table 1. 3.1.2 Out-of-Domain As for out-of-domain data, we have chosen two freely available parallel corpora: Europarl v7 (EP7) (Koehn, 2005) and OpenSubtitles 2011 (OS11) (Tiedemann, 2009). We extracted the DE–FR and DE–IT translations from each of them, resulting in ∼48.5 (EP7) and∼16.0 (OS11) million tokens per language pair. See Table 1. We point out that these parallel corpora are not thematically related to our in-domain data. Rather than that, they are much more extensive and thus cover a broader vocabulary, which is missing in our in-domain data due to its limited size. In-Domain Segments Tokens F Tokens E Tokens/Segment F Tokens/Segment E DE–FR DE–IT 166’957 Europarl OpenSubtitles DE–FR DE–IT DE–FR DE–IT 112’166 1’903’628"
W13-5630,P07-2045,0,0.00689617,"uistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 334 of 474] 3.2 Domain-Specific SMT Systems We have trained an in-domain-only baseline and three systems that combine in-domain and out-of-domain data (see Section 3.1) in different weighting modes for each language pair. 3.2.1 Baseline Systems Using only the in-domain data, we trained a standard phrase-based statistical machine translation system for DE–FR and DE–IT. 5-gram language models were trained using the IRST Language Modeling Toolkit (Federico and Cettolo, 2007). Otherwise, we relied on the Moses decoder (Koehn et al., 2007) and its dedicated scripts for training, tokenization, and truecasing. 3.2.2 Combined Systems Additionally, we trained separate translation and language models using our out-of-domain corpora. Apart from the training data, the setup was the same as for our baseline systems. This left us with three phrase tables and language models per language pair: in-domain, EP7, and OS11. All combined systems presented in this section comprise all of these models, but they are differently weighted. The weighting modes are defined as follows. Unweighted Combination (unweighted) In the unweighted mode, we sim"
W13-5630,E03-1076,0,0.0214312,"Missing"
W13-5630,2010.jec-1.4,0,0.14978,"(Pym et al., 2012), there is evidence that the per-word rate for professional translations has decreased significantly in some western European countries. On the other hand, computer-assisted translation tools such as translation memory (TM) systems—allowing translators to store their translations in a personal or corporate database and reuse them on future occasions—have become an integral part of state-of-the-art translation workflows, even for freelancers. More recently, there have been efforts to combine machine translation systems with translation memories (Kanavos and Kartsaklis, 2010; Koehn and Senellart, 2010). As a result, major commercial systems such as SDL TradosStudio1 , Across LanguageServer2 , and even open source alternatives such as OmegaT3 now offer machine translation interfaces, allowing translators to have segments automatically (pre-)translated in case there is no corresponding translation available in their translation memory. In a joint project between the UZH Institute of Computational Linguistics and SemioticTransfer AG, we currently explore the potential of using domain-specific statistical machine translation (SMT) systems in human-based translation workflows. We hypothesize tha"
W13-5630,P03-1021,0,0.0330494,"Missing"
W13-5630,E12-1055,0,0.0154273,"ain text material. Using these weights the single domain models p = {pi }i=1...N are combined into a single adapted model: p(x) = wp(x) = N X w i pi (x). i=1 The weights w are selected to optimize the performance of the adapted model on an in-domain development set. Language model performance is estimated with its entropy on the said set. Unlike Foster and Kuhn (2007) who use a monolingual performance measure for translation Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 333 of 474] models we follow Sennrich (2012) and use the cross-entropy of the adapted translation model on the development set, a bilingual performance measure: X ˆ (x) = arg min H(p), where H(p) = − ˜p(x) log2 p(x) p w x∈X(d) Here H(p) is the cross-entropy of the adapted language or translation model p and X(d) is the development set. The empirical probability distribution ˜p is based on the development set. The nature of x ∈ X(d) depends on which models are handled: in case of language models, it represents a single sentence; in case of translation models it stands for a 〈sour ce, t r ansl at ion〉 tuple. For more details see (Sennrich"
W14-3903,P13-2037,0,0.0891857,"robability plus the lemma as provided by both the Spanish and the English TreeTagger as well as the position of the word in the Beginning-Inside-Outside scheme as features for making the decision. In order to keep the number of experiments manageable they restricted their history to one or two preceding words. As an interesting experiment they generated code-switching sentences Spanish-English based on their different predictors and asked human judges to rate the naturalness of the resulting sentences. This helped them to identify the most useful code-switching predictor. Vu et al. (2013) and Adel et al. (2013) consider English-Mandarin code-switching in speech recognition. They investigate recurrent neural network language models and factored language models to the task in an attempt to integrate syntactic features. For the experiments they use SEAME, 2. http ://search.cpan.org/dist/Lingua-Ident/ 3. We are aware that the Text+Berg corpus contains also occasional sentences (or sentence fragments) in other German dialects (e.g. Austrian German, Bavarian German) and in old German spellings. Since these varieties are rare in the corpus, we do not deal with them explicitly. 27 The word sequence had to b"
W14-3903,E14-1001,0,0.0212551,"Missing"
W14-3903,hughes-etal-2006-reconsidering,0,0.0256577,"Missing"
W14-3903,I11-1062,0,0.0381089,"Missing"
W14-3903,W11-4624,1,0.786192,"ctured and annotated by automatically marking article boundaries, by tokenization, language identification, Part-of-Speech tagging and lemmatization. Our processing pipeline also includes toponym recognition and geo-coding of mountains, glaciers, cabins, valleys, lakes and towns. Furthermore we recognize and co-reference person names (Ebling et al., 2011), and we annotate temporal expressions (date, time, duration and set) with a variant of HeidelTime (Rettich, 2013). Finally we analyze the parallel parts of our corpus and provide sentence alignment information that is computed via BLEUalign (Sennrich and Volk, 2011). In order to process our texts with languagespecific tools (e.g. PoS tagging and person name recognition) we employed automatic language identification on the sentence level. We used Lingua-Ident 2 (developed by Michael Piotrowski) to determine for each sentence in our corpus whether it is in English, French, German, Italian or Romansh. Lingua-Ident is a statistical language identifier based on letter n-gram frequencies. For long sentences it reliably distinguishes between the languages. Unfortunately it often misclassifies short sentences. Therefore we decided to use it only for sentences wi"
W14-3903,D08-1102,0,0.189783,"rkt dar¨uber : “On peut remarquer a` cette occasion qu’il est rare que par un effort de l’esprit on puisse mettre du brouillard en bouteille, et . . . ” Die etwas a¨ ltere Sektion Diablerets, deren Steuer Herr August Bernus mit kundiger Hand . . . Most code-switching occurs with direct speech, quotes and book titles. The communicative goal is obviously to make the text more authentic. 4 Related Work on Detection of Code-Switching Most previous work on automatically detecting code-switching focused on the switches between two known languages (whereas we have to deal with a mix of 6 languages). Solorio and Liu (2008) worked on real-time prediction of code-switching points in SpanishEnglish conversations. This means that the judgement whether the current word is in a different language than the language of the matrix clause can only be based on the previous words. They use the PoS tag and its probability plus the lemma as provided by both the Spanish and the English TreeTagger as well as the position of the word in the Beginning-Inside-Outside scheme as features for making the decision. In order to keep the number of experiments manageable they restricted their history to one or two preceding words. As an"
W14-3903,vatanen-etal-2010-language,0,0.117189,"Missing"
W14-3903,volk-etal-2010-challenges,1,0.834219,"Missing"
W15-2506,W09-2404,0,0.072161,"Missing"
W15-2506,P14-1137,0,0.184341,"Missing"
W15-2506,2012.eamt-1.60,0,0.0177002,"ans, for example, that Experiments In this preliminary evaluation of our method we focus on the specific case of co-references to compounds, where the co-reference is an ambiguous word with several translations. The co-reference is disambiguated using a trigger word from the preceding context (i.e. the compound that the word co-refers to). The idea is that knowing which these compounds are we assess whether our method is able to detect them as relevant triggers. p(tgt = “pilote”|src = “driver”, trig = “road”) should be low, while The data comes from the German-English part of the WIT3 corpus (Cettolo et al., 2012), which is a collection of TED talks in multiple languages. The corpus consists of 194’533 sentences and 3.6 million tokens split into 1’596 talks (i.e. documents). The test set is also a collection of TED talks, consisting of 6’047 sentences and about 100’000 tokens. The talks differ greatly in terms of the covered topics, and therefore, have a high potential for ambiguous translations between them. This topic variety is so high that it is not feasible to tune SMT systems separately to each topic. However, it makes the corpus a feasible target for dynamic adaptation like our method. p(tgt = “"
W15-2506,D11-1084,0,0.0208297,"lated words, instead of only filtering out infrequent words. The reason is that trigger words that only appear in the context of an ambiguous term would be detected as infrequent, and therefore, incorrectly discarded. We are then planning to combine the distribution difference (measured with the KL divergence or other metrics) with a measure of similarity between the trigger candidate and the ambiguous word. Their simi49 post-editing method, they show improvement of translation correctness of co-referring terms in German-French and Chinese-English. Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-base"
W15-2506,W14-3358,0,0.0170521,"Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-based translation model that integrates word senses using maximum entropy classifiers. Meng et al. (2014) propose three term translation models to disambiguate, enforce consistency and guarantee integrity. Finally, Xiong et al. (2013) introduce a method that translates the coherence chain of the source, and uses it to produce a coherent translation. This topic modeling line of research can be combined with our own by including preceding sentences or their parts into the topic model training process. Figure 1: Comparison of the KL divergence rankings consi"
W15-2506,D14-1060,0,0.0177559,"n of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-based translation model that integrates word senses using maximum entropy classifiers. Meng et al. (2014) propose three term translation models to disambiguate, enforce consistency and guarantee integrity. Finally, Xiong et al. (2013) introduce a method that translates the coherence chain of the source, and uses it to produce a coherent translation. This topic modeling line of research can be combined with our own by including preceding sentences or their parts into the topic model training process. Figure 1: Comparison of the KL divergence rankings considering up to 4 previous sentences. The position of the compounds listed in table 3 are pointed out among all trigger candidates. larity can be m"
W15-2506,P15-3002,1,0.870699,"Missing"
W15-2506,H94-1013,0,0.0160155,"to combine the distribution difference (measured with the KL divergence or other metrics) with a measure of similarity between the trigger candidate and the ambiguous word. Their simi49 post-editing method, they show improvement of translation correctness of co-referring terms in German-French and Chinese-English. Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-based translation model that integrates word senses using maximum entropy classifiers. Meng et al. (2014) propose three term translation models to disambiguate, enforce consistency and guarantee integrity. Finally, Xiong et al. (2013) intro"
W15-2506,W10-2602,0,0.0276668,"rch to semantically related words, instead of only filtering out infrequent words. The reason is that trigger words that only appear in the context of an ambiguous term would be detected as infrequent, and therefore, incorrectly discarded. We are then planning to combine the distribution difference (measured with the KL divergence or other metrics) with a measure of similarity between the trigger candidate and the ambiguous word. Their simi49 post-editing method, they show improvement of translation correctness of co-referring terms in German-French and Chinese-English. Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (20"
W15-2506,2011.mtsummit-papers.13,0,0.0431267,"The reason is that trigger words that only appear in the context of an ambiguous term would be detected as infrequent, and therefore, incorrectly discarded. We are then planning to combine the distribution difference (measured with the KL divergence or other metrics) with a measure of similarity between the trigger candidate and the ambiguous word. Their simi49 post-editing method, they show improvement of translation correctness of co-referring terms in German-French and Chinese-English. Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-based translation model that integrates word senses using maximu"
W15-4926,abeille-barrier-2004-enriching,0,0.104851,"Missing"
W15-4926,E14-1061,0,0.0207915,"hallenge of this task lies in the desired translation direction, namely from French into German. As the target language is morphologically richer than the source language, we exc 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Martin Volk1 SWISS TXT Schweizerische Teletext AG Alexander-Sch¨oni-Strasse 40 CH-2501 Biel gion.linder@swisstxt.ch pect difﬁculties in generating grammatically correct output. This drawback can be overcome by means of hierarchical models (Huck et al., 2013), improved morphological processing (Cap et al., 2014) or models enriched with part-of-speech (POS) information (St¨uker et al., 2011). Another known issue with translations into German is the word order (e.g. the long-range disposal of separable preﬁx verbs or composed tenses), which can result in missing verbs or verb particles in the translated output. A general solution when translating between languages with different word order is to reorder the source texts according to the word order in the target language, as suggested by Niehues and Kolls (2009). In this paper we investigate how well these techniques can be applied for subtitles and we"
W15-4926,W13-2258,0,0.0263004,"ent (e.g. documentaries, informative broadcasts). The challenge of this task lies in the desired translation direction, namely from French into German. As the target language is morphologically richer than the source language, we exc 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Martin Volk1 SWISS TXT Schweizerische Teletext AG Alexander-Sch¨oni-Strasse 40 CH-2501 Biel gion.linder@swisstxt.ch pect difﬁculties in generating grammatically correct output. This drawback can be overcome by means of hierarchical models (Huck et al., 2013), improved morphological processing (Cap et al., 2014) or models enriched with part-of-speech (POS) information (St¨uker et al., 2011). Another known issue with translations into German is the word order (e.g. the long-range disposal of separable preﬁx verbs or composed tenses), which can result in missing verbs or verb particles in the translated output. A general solution when translating between languages with different word order is to reorder the source texts according to the word order in the target language, as suggested by Niehues and Kolls (2009). In this paper we investigate how well"
W15-4926,2013.mtsummit-posters.9,1,0.9038,"Missing"
W15-4926,W09-0435,0,0.0794562,"Missing"
W15-4926,P03-1021,0,0.00494672,"0 FR Words 20,853,000 8,760,000 2,881,000 144,000 14,800 3,200 the main clause (due to length restrictions), we had to join the subtitles in order for the reordering to be effective. 3.3 Results Table 1: The size of the German-French data sets website, with the difference that we lowercase the data instead of truecasing it7 . The model combinations (phrase table combination, language model interpolation) are generated with the tools available in the Moses distribution. The parameters of the global models are optimized through Minimum Error Rate Training (MERT) on an in-domain development set (Och, 2003). The translation performance is measured in terms of several evaluation metrics on a single reference translation using multeval8. Since the collected data sets are very heterogeneous, training a system on concatenated data did not make any sense because we would risk that bigger corpora overpower the small in-domain one. To avoid this, we make use of a common domain adaptation technique, namely mixturemodeling (Sennrich, 2012), and we apply it to both the translation and the language models. The components of the combined translation models have been trained independently on the correspondin"
W15-4926,E12-1055,0,0.0338139,"the tools available in the Moses distribution. The parameters of the global models are optimized through Minimum Error Rate Training (MERT) on an in-domain development set (Och, 2003). The translation performance is measured in terms of several evaluation metrics on a single reference translation using multeval8. Since the collected data sets are very heterogeneous, training a system on concatenated data did not make any sense because we would risk that bigger corpora overpower the small in-domain one. To avoid this, we make use of a common domain adaptation technique, namely mixturemodeling (Sennrich, 2012), and we apply it to both the translation and the language models. The components of the combined translation models have been trained independently on the corresponding parallel corpora (OPUS, TED etc.), whereas the language models are trained on the target side of these corpora. The Hierarchical system is trained by the same principles, but uses hierarchical models instead of plain phrase-based models. Such models learn translation rules from parallel data by means of probabilistic synchronous context-free grammars and are able to handle languages with different word order. The Improved syst"
W17-0231,J03-1002,0,0.0292816,"onal Linguistics, pages 247–250, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press the list of tokens aligned with a particular search hit. Word alignment is usually preceded by sentence alignment as word alignment tools are typically not capable of aligning whole documents.1 For our corpus data, we used hunalign (Varga et al. 2005) for sentence alignment, which can be provided with a dictionary for a particular language combination, or learn the dictionary from the parallel documents using a two-pass bootstrapping approach. Word alignment tools such as Giza++ (Och and Ney 2003) or fast_align (Dyer et al. 2013) produce unidirectional alignments which need to be symmetrized to obtain symmetric alignments. This requirement does not apply to the Berkeley Aligner (Liang et al. 2006) whose models are trained to produce symmetric alignments in the first place. Multilingwis expects word alignments to be symmetric. Independent of whether they are symmetric or not, union symmetrization is performed during corpus initialization, which has no effect on already symmetric alignments. Additional attributes used by Multilingwis for visualization purposes are: white spaces that have"
W17-0231,petrov-etal-2012-universal,0,0.0462291,"et al. (2016) and is designed to be easily employable on any parallel corpus comprising universal part-ofspeech tags, lemmas and word alignments. In addition to corpus exploration, it has proven useful for the assessment of word alignment quality. Loading the results of different alignment methods on the same corpus as different corpora into Multilingwis2 alleviates their comparison. 1 2 Corpus Preparation We discriminate between content and function words and define content words to be either adjectives, adverbs, nouns or verbs, which we tell apart by means of universal part-of-speech tags (Petrov et al. 2012). Any corpus to be used with Multilingwis2 thus requires these tags. They can be obtained directly using a tagger that produces universal tags or indirectly by mapping the language-specific tagsets to the universal one. In addition to tagging, lemmatization is required by Multilingwis to provide a lemma-based search. The new version of our search engine is also capable to perform searches on word forms, but the resulting translation variants are always conflated to lemma sequences. For our own corpus, we use the TreeTagger (Schmid 1994) for both, tagging and lemmatization and apply a subsequen"
W17-0231,N13-1073,0,0.0585137,"c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press the list of tokens aligned with a particular search hit. Word alignment is usually preceded by sentence alignment as word alignment tools are typically not capable of aligning whole documents.1 For our corpus data, we used hunalign (Varga et al. 2005) for sentence alignment, which can be provided with a dictionary for a particular language combination, or learn the dictionary from the parallel documents using a two-pass bootstrapping approach. Word alignment tools such as Giza++ (Och and Ney 2003) or fast_align (Dyer et al. 2013) produce unidirectional alignments which need to be symmetrized to obtain symmetric alignments. This requirement does not apply to the Berkeley Aligner (Liang et al. 2006) whose models are trained to produce symmetric alignments in the first place. Multilingwis expects word alignments to be symmetric. Independent of whether they are symmetric or not, union symmetrization is performed during corpus initialization, which has no effect on already symmetric alignments. Additional attributes used by Multilingwis for visualization purposes are: white spaces that have been deleted during tokenization"
W17-0231,2005.mtsummit-papers.11,0,0.0250073,"for Multilingwis. Any translation variant is derived from Introduction In (ibid.), we introduced Multilingwis (Multilingual Word Information System), our approach for exploring translation variants of multi-word units in multiparallel corpora. It relies on a part-ofspeech tagged and word-aligned parallel corpus as source material, a PostgreSQL database for efficient retrieval (see Graën, Clematide, et al. 2016) and a standard web server equipped with PHP for the user interface. Our corpus data comes from CoStEP (Graën, Batinic, et al. 2014), which is a cleaner version of the Europarl corpus (Koehn 2005), and comprises 240 million tokens in English, German, French, Italian, and Spanish. We, subsequently, received several requests regarding the portability of our retrieval engine and search interface to other corpora. Our decision to decouple Multilingwis from the particular data structure that our corpus had grown into and to release a version that can easily be adopted to other corpora coincided with the introduction of 2 This is the second version, not a footnote number. 247 Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 247–250, c Gothenburg, Sweden, 23-24 Ma"
W17-0231,N06-1014,0,0.0601756,"ded by sentence alignment as word alignment tools are typically not capable of aligning whole documents.1 For our corpus data, we used hunalign (Varga et al. 2005) for sentence alignment, which can be provided with a dictionary for a particular language combination, or learn the dictionary from the parallel documents using a two-pass bootstrapping approach. Word alignment tools such as Giza++ (Och and Ney 2003) or fast_align (Dyer et al. 2013) produce unidirectional alignments which need to be symmetrized to obtain symmetric alignments. This requirement does not apply to the Berkeley Aligner (Liang et al. 2006) whose models are trained to produce symmetric alignments in the first place. Multilingwis expects word alignments to be symmetric. Independent of whether they are symmetric or not, union symmetrization is performed during corpus initialization, which has no effect on already symmetric alignments. Additional attributes used by Multilingwis for visualization purposes are: white spaces that have been deleted during tokenization and any meta information related to a particular document in form of attribute value pairs. All this information is optional and will merely be visualized if available. 3"
W19-6626,D16-1025,0,0.02867,"Missing"
W19-6626,P17-2061,0,0.0204327,"on scores (Quality) as assigned by two expert raters per translation. mann, 2009), as well as 385’320 and 186’647 in-domain segments for FR and IT, respectively. We filtered both in- and out-of-domain segments through a set of mostly length-based heuristics (Zwahlen et al., 2016), and oversampled the former as a simple means of domain adaptation. While this has proven effective in other contexts (e. g., Sennrich et al., 2016a), we note that translation quality could likely be improved by means of more advanced techniques such as fine-tuning (Luong and Manning, 2015) or multi-domain modelling (Chu et al., 2017). dle of the experiment. A post-experimental survey concluded the experiment. We encountered no problems with data collection, with the exception of a temporary failure of IT-1’s screen in the last working block. The device went into standby mode, which was not reported immediately and resulted in a total interruption of 4 minutes, which we deducted from the respective session before calculating translation speed as shown in Table 2. Subjects A total of four professional translators took part in the productivity test, two each for the target languages FR (FR-1, FR-2) and IT (IT-1, IT-2). All w"
W19-6626,R11-1014,0,0.0772457,"Missing"
W19-6626,2012.amta-papers.22,0,0.139836,"Missing"
W19-6626,D13-1176,0,0.0528235,"n post-editing productivity is still pending thorough investigation. We empirically test how the inclusion of NMT, in addition to domain-specific translation memories and termbases, impacts speed and quality in professional translation of financial texts. We find that even with language pairs that have received little attention in research settings and small amounts of in-domain data for system adaptation, NMT post-editing allows for substantial time savings and leads to equal or slightly better quality. 1 2 Patrick Düggelin 1 Introduction The use of neural networks for sequence transduction (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has led to astounding progress in the field of machine translation (MT), establishing a new level of quality in applications such as automatic news translation (Sennrich et al., 2016b; Hassan et al., 2018). Nevertheless, the creation of publication-grade translations still requires human involvement (Läubli et al., 2018; Toral et al., 2018a), and previous work has explored human–machine collaboration in the form of post-editing, where human experts revise machine translated text where necessary. Empirical investigations of post-editing productiv"
W19-6626,W14-0307,0,0.553329,"Missing"
W19-6626,2015.iwslt-evaluation.11,0,0.0472471,"ced per hour (Words/h) and averaged overall impression scores (Quality) as assigned by two expert raters per translation. mann, 2009), as well as 385’320 and 186’647 in-domain segments for FR and IT, respectively. We filtered both in- and out-of-domain segments through a set of mostly length-based heuristics (Zwahlen et al., 2016), and oversampled the former as a simple means of domain adaptation. While this has proven effective in other contexts (e. g., Sennrich et al., 2016a), we note that translation quality could likely be improved by means of more advanced techniques such as fine-tuning (Luong and Manning, 2015) or multi-domain modelling (Chu et al., 2017). dle of the experiment. A post-experimental survey concluded the experiment. We encountered no problems with data collection, with the exception of a temporary failure of IT-1’s screen in the last working block. The device went into standby mode, which was not reported immediately and resulted in a total interruption of 4 minutes, which we deducted from the respective session before calculating translation speed as shown in Table 2. Subjects A total of four professional translators took part in the productivity test, two each for the target languag"
W19-6626,2013.mtsummit-wptp.10,1,0.900436,"Missing"
W19-6626,D18-1512,1,0.82627,"data for system adaptation, NMT post-editing allows for substantial time savings and leads to equal or slightly better quality. 1 2 Patrick Düggelin 1 Introduction The use of neural networks for sequence transduction (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has led to astounding progress in the field of machine translation (MT), establishing a new level of quality in applications such as automatic news translation (Sennrich et al., 2016b; Hassan et al., 2018). Nevertheless, the creation of publication-grade translations still requires human involvement (Läubli et al., 2018; Toral et al., 2018a), and previous work has explored human–machine collaboration in the form of post-editing, where human experts revise machine translated text where necessary. Empirical investigations of post-editing productivity with NMT are still scarce, especially for language combinations that do not include English as c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 either the source or target language. In this paper, we describe and discuss the results of a productivit"
W19-6626,W16-2323,0,0.128976,"inancial texts. We find that even with language pairs that have received little attention in research settings and small amounts of in-domain data for system adaptation, NMT post-editing allows for substantial time savings and leads to equal or slightly better quality. 1 2 Patrick Düggelin 1 Introduction The use of neural networks for sequence transduction (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has led to astounding progress in the field of machine translation (MT), establishing a new level of quality in applications such as automatic news translation (Sennrich et al., 2016b; Hassan et al., 2018). Nevertheless, the creation of publication-grade translations still requires human involvement (Läubli et al., 2018; Toral et al., 2018a), and previous work has explored human–machine collaboration in the form of post-editing, where human experts revise machine translated text where necessary. Empirical investigations of post-editing productivity with NMT are still scarce, especially for language combinations that do not include English as c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Procee"
W19-6626,P16-1162,0,0.419483,"inancial texts. We find that even with language pairs that have received little attention in research settings and small amounts of in-domain data for system adaptation, NMT post-editing allows for substantial time savings and leads to equal or slightly better quality. 1 2 Patrick Düggelin 1 Introduction The use of neural networks for sequence transduction (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has led to astounding progress in the field of machine translation (MT), establishing a new level of quality in applications such as automatic news translation (Sennrich et al., 2016b; Hassan et al., 2018). Nevertheless, the creation of publication-grade translations still requires human involvement (Läubli et al., 2018; Toral et al., 2018a), and previous work has explored human–machine collaboration in the form of post-editing, where human experts revise machine translated text where necessary. Empirical investigations of post-editing productivity with NMT are still scarce, especially for language combinations that do not include English as c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Procee"
W19-6626,2006.amta-papers.25,0,0.0966814,"Missing"
W19-6626,W18-6312,0,0.0145967,"tation, NMT post-editing allows for substantial time savings and leads to equal or slightly better quality. 1 2 Patrick Düggelin 1 Introduction The use of neural networks for sequence transduction (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has led to astounding progress in the field of machine translation (MT), establishing a new level of quality in applications such as automatic news translation (Sennrich et al., 2016b; Hassan et al., 2018). Nevertheless, the creation of publication-grade translations still requires human involvement (Läubli et al., 2018; Toral et al., 2018a), and previous work has explored human–machine collaboration in the form of post-editing, where human experts revise machine translated text where necessary. Empirical investigations of post-editing productivity with NMT are still scarce, especially for language combinations that do not include English as c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 either the source or target language. In this paper, we describe and discuss the results of a productivity test of domain-ada"
W19-9003,C18-1139,0,0.0683782,"Adige) remain a major challenge in identifying all toponyms in this corpus. 4 A Neural Approach to Geotagging a Heritage Corpus In an attempt to improve upon the previous gazetteer-based approach, we adopt a neural approach to toponym recognition, inspired by current state-of-the-art techniques in NER. Similar to Huang et al. (2015); Ma and Hovy (2016), we apply a bidirectional recurrent neural network architecture (BiLSTMs) followed by a conditional random field (CRF) layer for the detection of toponyms in our corpus. This approach incorporates contextual string embeddings, as introduced by Akbik et al. (2018) in the Flair framework. Contextual string embeddings have been shown to perform well in downstream sequence-labelling tasks, such as NER. Due to the fact that they comprise character-level information, these embeddings are particularly well suited to modelling an open vocabulary. We use a stacked embedding architecture (Akbik et al., 2018) to concatenate Flair’s character-based forward embeddings trained on our corpus with general-purpose fastText word embeddings pre-trained on web data (Grave et al., 2018). The idea here is that the general-purpose embeddings provide sufficient global syntac"
W19-9003,P16-1101,0,0.0229765,"particularly among frequent categories (see Table 4). Manual inspection reveals that prevalent historical spelling variations (e.g. Fiesch and pre-1905 Viesch) and the extensive use of endonyms for places in and around Switzerland (e.g. German: Etsch and Italian: Adige) remain a major challenge in identifying all toponyms in this corpus. 4 A Neural Approach to Geotagging a Heritage Corpus In an attempt to improve upon the previous gazetteer-based approach, we adopt a neural approach to toponym recognition, inspired by current state-of-the-art techniques in NER. Similar to Huang et al. (2015); Ma and Hovy (2016), we apply a bidirectional recurrent neural network architecture (BiLSTMs) followed by a conditional random field (CRF) layer for the detection of toponyms in our corpus. This approach incorporates contextual string embeddings, as introduced by Akbik et al. (2018) in the Flair framework. Contextual string embeddings have been shown to perform well in downstream sequence-labelling tasks, such as NER. Due to the fact that they comprise character-level information, these embeddings are particularly well suited to modelling an open vocabulary. We use a stacked embedding architecture (Akbik et al.,"
W19-9003,L16-1155,1,0.903766,"tailing many aspects of life in the mountains. Topics covered range from mountaineering and hiking expeditions, flora and fauna, geography, and geological changes to social, cultural and linguistic diversity in alpine regions. Being a diarchronic heritage corpus, its development has posed a number of challenges regarding digitisation and linguistic annotation. Numerous experiments have been undertaken to semantically enrich this corpus as both a historic and a linguistic resource. These include, but are not limited to, a novel approach to correcting optical character recognition (OCR) errors (Clematide et al., 2016); gazetteer and rule-based NER for the annotation of personal names, toponyms, organisations and time expressions (Ebling et al., 2011); improved lemmatisation for German separable prefix verbs and elliptical compound nouns (Volk et al., 2016); innovative techniques for sentence alignment in parallel texts (Sennrich and Volk, 2010); and the creation of a manually annotated parallel treebank with more than 1000 sentences in French and German for the purpose of assisting statistical machine translation (G¨ohring and Volk, 2011). 3 Figure 1: A page from the 1906 SAC yearbook with the article “On"
W19-9003,2010.amta-papers.14,1,0.64553,"ion and linguistic annotation. Numerous experiments have been undertaken to semantically enrich this corpus as both a historic and a linguistic resource. These include, but are not limited to, a novel approach to correcting optical character recognition (OCR) errors (Clematide et al., 2016); gazetteer and rule-based NER for the annotation of personal names, toponyms, organisations and time expressions (Ebling et al., 2011); improved lemmatisation for German separable prefix verbs and elliptical compound nouns (Volk et al., 2016); innovative techniques for sentence alignment in parallel texts (Sennrich and Volk, 2010); and the creation of a manually annotated parallel treebank with more than 1000 sentences in French and German for the purpose of assisting statistical machine translation (G¨ohring and Volk, 2011). 3 Figure 1: A page from the 1906 SAC yearbook with the article “On the research of mountain names”. as towns, mountains, lakes, glaciers, valleys and mountain cabins in order to enable location-based searching. Gazetteer-based approaches make use of curated lists of known geographic locations along with their relevant metadata (e.g. longitude and latitude, population, elevation, etc.). Relying on"
W19-9003,L18-1550,0,0.015401,"our corpus. This approach incorporates contextual string embeddings, as introduced by Akbik et al. (2018) in the Flair framework. Contextual string embeddings have been shown to perform well in downstream sequence-labelling tasks, such as NER. Due to the fact that they comprise character-level information, these embeddings are particularly well suited to modelling an open vocabulary. We use a stacked embedding architecture (Akbik et al., 2018) to concatenate Flair’s character-based forward embeddings trained on our corpus with general-purpose fastText word embeddings pre-trained on web data (Grave et al., 2018). The idea here is that the general-purpose embeddings provide sufficient global syntactic knowledge, while the in-domain Flair embeddings, which are trained on our own corpus, capture high-level contextual and orthographic idiosyncrasies typical for historical mountaineering reports. distinct types 0.3k 2k 0.4k 0.3k 8k 0.6k 4.1 Table 2: Corpus-wide toponym recognition and resolution ID counts for the gazetteer-based approach. Exploiting Gazetteer-based Labels as Silver Standard Training Data Typically, the major challenge in machine learning and neural approaches to toponym recognition is acq"
