2020.coling-main.450,P17-1080,0,0.072133,"t al. (2019a). They probe on all layers and discover that, on eight tasks including part-of-speech tagging, parsing, named entity recognition, semantic role labeling and coreference, the importance of layers on the performance corresponds to a classical NLP pipeline, with the lowest-level tasks having a better performance on the lowest layers. Peters et al. (2018), who probe their ELMo model for part-of-speech tagging, also suggest that consistencies with previous pipelined multi-task learning approaches support this finding: They find that part-of-speech tags are best recovered from layer 1. Belinkov et al. (2017a) find that lower layers in neural machine translation (NMT) systems better capture word structure, and suggest that higher layers have more information about word meaning. Shi et al. (2016) suggest that representations from lower layers in NMT are better at “learning” local syntax, while those from higher layers are better at more global syntax tasks. Belinkov et al. (2017b) find that representations from lower layers in neural machine translation systems get better results at part-of-speech tagging, while those from higher layers lead to a better performance on word-level semantic tagging,"
2020.coling-main.450,I17-1001,0,0.0440883,"t al. (2019a). They probe on all layers and discover that, on eight tasks including part-of-speech tagging, parsing, named entity recognition, semantic role labeling and coreference, the importance of layers on the performance corresponds to a classical NLP pipeline, with the lowest-level tasks having a better performance on the lowest layers. Peters et al. (2018), who probe their ELMo model for part-of-speech tagging, also suggest that consistencies with previous pipelined multi-task learning approaches support this finding: They find that part-of-speech tags are best recovered from layer 1. Belinkov et al. (2017a) find that lower layers in neural machine translation (NMT) systems better capture word structure, and suggest that higher layers have more information about word meaning. Shi et al. (2016) suggest that representations from lower layers in NMT are better at “learning” local syntax, while those from higher layers are better at more global syntax tasks. Belinkov et al. (2017b) find that representations from lower layers in neural machine translation systems get better results at part-of-speech tagging, while those from higher layers lead to a better performance on word-level semantic tagging,"
2020.coling-main.450,W19-4828,0,0.0297876,"Missing"
2020.coling-main.450,N19-1423,0,0.220696,"o a framework in which analysis efforts can be scrutinized and argue that, with current models and baselines, conclusions that representations contain linguistic structure are not well-founded. Current probing methodology, such as restricting the classifier’s expressiveness or using strong baselines, can help to better estimate the complexity of learning, but not build a foundation for speculations about the nature of the linguistic structure encoded in the learned representations. 1 Introduction The impressive performance of neural language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has led many authors to believe that these models must have learned linguistic structure, and a whole new research branch concerned with the analysis and interpretation of neural models has emerged, with dedicated areas at many conferences and the BlackboxNLP workshop being established as a large and well-known venue in a short time. An appealing and widespread analysis technique, perhaps due to its simplicity and generalizability, is to use a model’s word representations as input to a simple classifier, and train this classifier on an auxiliary task that can be expected to benefit from lingu"
2020.coling-main.450,D19-1275,0,0.314281,"alized embeddings where the contextualization process is not learned. Random Word Embeddings In this baseline, each word type is assigned a random embedding. This baseline is used, e.g., by Zhang and Bowman (2018). As random embeddings contain no structural information at all, this baseline is obviously weak with regard to the context-only hypothesis. It only tests the model’s capacities to memorize random inputs, but discards even similarities of words that have been shown useful already in earlier uncontextualized word embeddings. Random Targets (Control Tasks) This baseline was proposed by Hewitt and Liang (2019). Each word type is assigned an output randomly using a deterministic function. As this baseline cannot meaningfully use any pre-existing information, it is obviously weak with regard to the hypothesis—it only tests the model’s capacities to memorize. As the targets are randomly assigned, no information from the embeddings can be useful, which makes this baseline comparable to the random word embeddings baseline with respect to the context-only hypothesis. Uncontextualized Embeddings This baseline is often the uncontextualized part of the model, e.g. the embedding layer in ELMo (word embedding"
2020.coling-main.450,N19-1419,0,0.436555,"ord representations as input to a simple classifier, and train this classifier on an auxiliary task that can be expected to benefit from linguistic knowledge. The argument behind this technique is that, if a simple classifier is enough to learn the task based on word-level representations alone, then it is probable that these representations indeed encode the hypothesized linguistic structure (Hupkes et al., 2018). Many of the papers applying classifier probes suggest that models such as BERT and ELMo encode structure that is similar to well-known syntactic annotations, e.g. dependency trees (Hewitt and Manning, 2019), or even to a traditional NLP pipeline (Tenney et al., 2019a). A standard assumption in probing is that, if we feed only the representation of one word at a time to the classifier, then no complex inference based on other words in the sentence can happen, and the probe will not be able to learn the classification task well—unless the necessary linguistic information already is encoded in the representation (Lin et al., 2019). Therefore, the argument goes, high performance on a probing task can be taken as evidence that the word-level representations encode linguistic structure. In Section 3,"
2020.coling-main.450,W19-4825,0,0.161022,"applying classifier probes suggest that models such as BERT and ELMo encode structure that is similar to well-known syntactic annotations, e.g. dependency trees (Hewitt and Manning, 2019), or even to a traditional NLP pipeline (Tenney et al., 2019a). A standard assumption in probing is that, if we feed only the representation of one word at a time to the classifier, then no complex inference based on other words in the sentence can happen, and the probe will not be able to learn the classification task well—unless the necessary linguistic information already is encoded in the representation (Lin et al., 2019). Therefore, the argument goes, high performance on a probing task can be taken as evidence that the word-level representations encode linguistic structure. In Section 3, we challenge this view by showing experimentally that the word-level representations learned by ELMo and BERT encode the closer neighborhood of the word in a surprisingly exact way. To explore the consequences of this observation, we formulate the context-only hypothesis, which offers an alternative explanation of how a probe could reach high accuracy on the auxiliary task: it could simply make use of a This work is licensed"
2020.coling-main.450,N18-1202,0,0.763402,"We develop this hypothesis into a framework in which analysis efforts can be scrutinized and argue that, with current models and baselines, conclusions that representations contain linguistic structure are not well-founded. Current probing methodology, such as restricting the classifier’s expressiveness or using strong baselines, can help to better estimate the complexity of learning, but not build a foundation for speculations about the nature of the linguistic structure encoded in the learned representations. 1 Introduction The impressive performance of neural language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has led many authors to believe that these models must have learned linguistic structure, and a whole new research branch concerned with the analysis and interpretation of neural models has emerged, with dedicated areas at many conferences and the BlackboxNLP workshop being established as a large and well-known venue in a short time. An appealing and widespread analysis technique, perhaps due to its simplicity and generalizability, is to use a model’s word representations as input to a simple classifier, and train this classifier on an auxiliary task that can be"
2020.coling-main.450,2020.acl-main.420,0,0.0756429,"Missing"
2020.coling-main.450,D16-1159,0,0.0292413,"ce of layers on the performance corresponds to a classical NLP pipeline, with the lowest-level tasks having a better performance on the lowest layers. Peters et al. (2018), who probe their ELMo model for part-of-speech tagging, also suggest that consistencies with previous pipelined multi-task learning approaches support this finding: They find that part-of-speech tags are best recovered from layer 1. Belinkov et al. (2017a) find that lower layers in neural machine translation (NMT) systems better capture word structure, and suggest that higher layers have more information about word meaning. Shi et al. (2016) suggest that representations from lower layers in NMT are better at “learning” local syntax, while those from higher layers are better at more global syntax tasks. Belinkov et al. (2017b) find that representations from lower layers in neural machine translation systems get better results at part-of-speech tagging, while those from higher layers lead to a better performance on word-level semantic tagging, especially for the tags that are most semantic, like discourse functions and noun concepts. Looking at our results in Figure 1 and Table 1, it is tempting to offer an alternative interpretati"
2020.coling-main.450,P19-1452,0,0.240076,"s classifier on an auxiliary task that can be expected to benefit from linguistic knowledge. The argument behind this technique is that, if a simple classifier is enough to learn the task based on word-level representations alone, then it is probable that these representations indeed encode the hypothesized linguistic structure (Hupkes et al., 2018). Many of the papers applying classifier probes suggest that models such as BERT and ELMo encode structure that is similar to well-known syntactic annotations, e.g. dependency trees (Hewitt and Manning, 2019), or even to a traditional NLP pipeline (Tenney et al., 2019a). A standard assumption in probing is that, if we feed only the representation of one word at a time to the classifier, then no complex inference based on other words in the sentence can happen, and the probe will not be able to learn the classification task well—unless the necessary linguistic information already is encoded in the representation (Lin et al., 2019). Therefore, the argument goes, high performance on a probing task can be taken as evidence that the word-level representations encode linguistic structure. In Section 3, we challenge this view by showing experimentally that the wo"
2020.coling-main.450,2020.emnlp-main.14,0,0.0371137,"erence between learning a task and identifying linguistic structure is non-existent. They argue against any restrictions on the expressiveness of the probe, stating that there is no information gain induced by them. We argue, in line with them, that valid conclusions using restricted probing classifiers can to this point only relate to the ease of learning with the features present in the representations. We propose that, as probing classifiers’ simplicity is still appealing, future work should aim at providing 5144 theoretically well-motivated setups for this idea, as for example explored by Voita and Titov (2020) and Yogatama et al. (2019). We believe that approximating the ease of learning the task with the respective embeddings of the baseline and the representations in question e.g. with plots can be insightful to get an impression of the learning process, but interpretations remain difficult without an extensive error analysis, and comparisons of embeddings or data sets are especially hard due to differences in dimensionalities, numbers and distributions of labels. Acknowledgements We thank our group at Linköping University for helpful discussions of this work, and the three anonymous reviewers fo"
2020.coling-main.450,W18-5448,0,0.433731,"iple, be able to learn any task which can be learned by end-to-end-systems that include the encoders as architectural components. We hypothesize that BERT’s masked language model objective in particular may make the learned representations memorize words in the immediate neighborhood, as the states associated with these words are what is used for predictions during pre-training. 3.2 Neighboring Word Identity Probes To assess the extent to which the sentential context of a token is encoded in pre-trained representations, we use a neighboring word identity prediction task, as first suggested by Zhang and Bowman (2018). In this task, given a single token representation ?(? ? ) as input, the probe predicts the identity of the word type of the token ? ?+? , for some fixed offset ? ∈ Z. We interpret the performance on this task as a conservative estimate of the utility of the information encoded in ?(? ? ) about the exact linear neighborhood of ? ? . It is a conservative estimate because, even if a probe is unable to predict the exact neighbor of ? ? , it may still be able to recover a distributionally similar word, say, a different inflectional form, which for many tasks would be almost as useful as an exact"
2020.iwpt-1.3,K18-2002,1,0.872508,"Missing"
2020.iwpt-1.3,N19-1423,0,0.204273,"sults from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) successfully improved previous results by a considerable margin. The 2018 follow-up to the EPE shared task (Fares et al., 2018) again used S HER LOCK to evaluate parsing performance, this time restricting itself to participating systems in the colocated 2018 CoNLL Shared Task on Universal Dependency Parsing (Zeman et al., 2018). 3 Data The"
2020.iwpt-1.3,P18-2077,0,0.450072,"g its scope, is relevant for a large number of applications in natural language processing, and has been the subject of several contrastive research efforts (Morante and Blanco, 2012; Oepen et al., 2017; Fares et al., 2018). In this paper we cast NR as a graph parsing problem. More specifically, we represent negation cues and corresponding scopes as a bi-lexical graph and learn to predict this graph from the tokens. Under this representation, we may apply any dependency graph parser to the task of negation resolution. The specific parsing architecture that we use in this paper extends that of Dozat and Manning (2018). Contributions This work (a) rationally reconstructs the previous state of the art in negation resolution; (b) develops a novel approach to the problem based on general graph parsing techniques; (c) proposes and evaluates different ways of integrating ‘external’ grammatical information; (d) gauges the utility of morpho-syntactic preprocessing at different levels of accuracy; (e) shifts experimental focus (back) to a complete, end-toend perspective on the task; and (f) reflects on un14 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages"
2020.iwpt-1.3,P19-1267,0,0.0119054,"r good performance on the final evaluation set. This notion is further reinforced by the lack of significant difference in performance of our best systems, compared to S HERLOCK. For the S TANFORD -PARIS version of the data, even nearly three points of FN F1 (64.27 vs. 61.42) do not constitue a significant difference. The difficulty to confidently analyse the results is also illustrated by the somewhat erratic performance differences across different settings and runs. The NLP community has recently realized the importance of proper testing in favour of simple comparisons of benchmark scores (Gorman and Bedrick, 2019). This becomes even more pronounced when working with deep learning architectures, where model selection is more complicated (Moss et al., 2019) due to sensitivity to different random seeds. When working with smaller datasets such as ConanDoyle-neg, it is particularly important to thoroughly analyse the results before claiming that a new system improves the state of the art (Dror et al., 2017). Predicted Cues When we task our system to also predict cues, as in *SEM 2012, our best system outperforms Read et al. (2012) and Packard et al. (2014) on most measures. Our neural graph parsing approch"
2020.iwpt-1.3,K17-3002,0,0.0421719,"ping Interpolation constant L2 regularization 200 50 0 0.95 1 · 10−3 5 0.025 3 · 10−9 tion task of the 2017 edition of EPE, S TANFORD PARIS-06 (Schuster et al., 2017), uses enhanced Universal Dependencies (v1) and data from the Penn Treebank (Marcus et al., 1993), the Brown Corpus (Francis and Kuˇcera, 1985) and the GENIA treebank (Tateisi et al., 2005). In contrast to this, the best performing system for the 2018 edition, T URKU NLP (Kanerva et al., 2018), only uses the English training data provided by the co-located UD parsing shared task. Both systems use the parser and hyperparameters of Dozat et al. (2017), the winning submission of the CoNLL 2017 Shared Task on parsing Universal Dependencies. In the overview paper for the 2018 EPE shared task (Fares et al., 2018), the organizers report that the version of the S HERLOCK negation system that was used for EPE 2017 had a deficiency that could leak gold-standard scope and event annotations into system predictions, leading to potentially inflated scores.2 The EPE 2018 version of S HERLOCK corrected this problem and added automated hyperparameter tuning, which Fares et al. (2018) suggest largely offset the negative effect on overall scores from the b"
2020.iwpt-1.3,Q17-1033,0,0.012549,"at erratic performance differences across different settings and runs. The NLP community has recently realized the importance of proper testing in favour of simple comparisons of benchmark scores (Gorman and Bedrick, 2019). This becomes even more pronounced when working with deep learning architectures, where model selection is more complicated (Moss et al., 2019) due to sensitivity to different random seeds. When working with smaller datasets such as ConanDoyle-neg, it is particularly important to thoroughly analyse the results before claiming that a new system improves the state of the art (Dror et al., 2017). Predicted Cues When we task our system to also predict cues, as in *SEM 2012, our best system outperforms Read et al. (2012) and Packard et al. (2014) on most measures. Our neural graph parsing approch is clearly better at identifying the relevant scope tokens (ST), due to its pairwise classification approach, respectively gaining 5.32 and 2.29 points in FN F1 . This generally also results in better performance for matching complete scopes (SM). The system does however struggle with telling events and regular scopes apart, and is clearly outperformed by Read et al. (2012) on that measure (6."
2020.iwpt-1.3,2020.cl-1.5,0,0.167676,"Missing"
2020.iwpt-1.3,K18-2013,0,0.0849187,"Missing"
2020.iwpt-1.3,N13-1070,0,0.145481,"{ S O CUE N } E N N N N SO Figure 1: An example of how overlapping ConanDoyle-neg annotations are converted to flat sequences of labels in S HERLOCK. In this example, an in-scope token is labelled with N, a cue with CUE, a negated event with E, a negation stop with S, and an out-of-scope token with O. Illustration taken from Lapponi et al. (2017). 3.1 in-scope or out-of-scope using a conditional random field (CRF). Another CRF further classifies scope tokens as events, and a heuristic is applied to distribute scope tokens to their respective cues. The S HERLOCK system was subsequently used by Elming et al. (2013) to evaluate various dependency conversions, and similarly served as one of three reference ‘downstream’ applications in the 2017 Extrinsic Parser Evaluation initiative (EPE; Oepen et al., 2017). The best results from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used"
2020.iwpt-1.3,2020.lrec-1.704,0,0.139041,"gation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) successfully improved previous results by a considerable margin. The 2018 follow-up to the EPE shared task (Fares et al., 2018) again used S HER LOCK to evaluate parsing performance, this time restricting itself to participating systems in the colocated 2018 CoNLL Shared Task on Universal Dependency Parsing (Zeman et al., 2018). 3 Data The negation data of *SEM 2012 consists of selected Sherlock Holmes stories from the works of Arthur Conan Doyle, and contains 3,644 sentences in the training set, 787 sentences in the development set, and 1,089 sentences in the evaluation set. The corpus annot"
2020.iwpt-1.3,P16-1047,0,0.0549824,"is applied to distribute scope tokens to their respective cues. The S HERLOCK system was subsequently used by Elming et al. (2013) to evaluate various dependency conversions, and similarly served as one of three reference ‘downstream’ applications in the 2017 Extrinsic Parser Evaluation initiative (EPE; Oepen et al., 2017). The best results from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) succe"
2020.iwpt-1.3,S12-1035,0,0.813225,"cribes the specific NR task that we address in this paper. In Section 4 we present our new encoding of negations and our parsing model, followed by the description of our experiments and results in Section 5. We discuss these results in Section 6 and summarize our findings in Section 7. 2 Related Work While there exist a variety of datasets that annotate negation (Jim´enez-Zafra et al., 2020), the BioScope (Szarvas et al., 2008) and Conan Doyle datasets (ConanDoyle-neg; Morante and Daelemans, 2012) are most commonly used for evaluation. The latter was created for the shared task at *SEM 2012 (Morante and Blanco, 2012), where competing systems needed to predict both negation cues (linguistic expressions of negation) and their corresponding scopes, i.e. the part of the utterance being negated. Cues can be simple negation markers (such as not or without), but may also consist of multiple words (i.e. neither . . . nor), or be mere affixes (i.e. infrequent or clueless). In contrast to other datasets, ConanDoyle-neg also annotates negated events that are part of the scopes. The analysis of negation is divided into two related sub-tasks, cue detection and scope resolution. While cue detection is mostly dependent"
2020.iwpt-1.3,morante-daelemans-2012-conandoyle,0,0.620195,"Missing"
2020.iwpt-1.3,W19-6202,1,0.752559,"dependents (D), and the nodes themselves (S), weighted by ~ l: layer-specific weight matrices W   l−1  ~ l = ReLU A ~W ~ l +A ~ >W ~ l +W ~l X ~ X H D S ~~ score(~hi , d~j ) = ~h> i U dj ~ corresponds to The inner dimension of the tensor U the number of negation graph labels plus a special N ONE label indicating the absence of an arc, and thus predicts arcs and labels jointly. 4.3 Adding External Graph Features Similarly to S HERLOCK, our neural model is able to process external morpho-syntactic or surfacesemantic analyses of the input sentence in the form of dependency graphs. Inspired by Kurtz et al. (2019), we extend the contextualized embeddings that are computed by our parser by information derived from the external graph. For this we use three approaches: (i) attaching the sum of heads; (ii) scaled attention on the heads; and (iii) Graph Convolutional Networks (Kipf and Welling, 2017). In the following, we view the external graph in ~ and the terms of its n × n adjacency matrix A ~ contextualized embeddings as an n × d matrix C. When applying the next layer l, each node is up~ l−1 from dated with respect to its representation X the previous layer, thus indirectly taking into account grandpar"
2020.iwpt-1.3,P19-1281,0,0.0123128,"ems, compared to S HERLOCK. For the S TANFORD -PARIS version of the data, even nearly three points of FN F1 (64.27 vs. 61.42) do not constitue a significant difference. The difficulty to confidently analyse the results is also illustrated by the somewhat erratic performance differences across different settings and runs. The NLP community has recently realized the importance of proper testing in favour of simple comparisons of benchmark scores (Gorman and Bedrick, 2019). This becomes even more pronounced when working with deep learning architectures, where model selection is more complicated (Moss et al., 2019) due to sensitivity to different random seeds. When working with smaller datasets such as ConanDoyle-neg, it is particularly important to thoroughly analyse the results before claiming that a new system improves the state of the art (Dror et al., 2017). Predicted Cues When we task our system to also predict cues, as in *SEM 2012, our best system outperforms Read et al. (2012) and Packard et al. (2014) on most measures. Our neural graph parsing approch is clearly better at identifying the relevant scope tokens (ST), due to its pairwise classification approach, respectively gaining 5.32 and 2.29"
2020.iwpt-1.3,S12-1042,0,0.564707,"ch as not or without), but may also consist of multiple words (i.e. neither . . . nor), or be mere affixes (i.e. infrequent or clueless). In contrast to other datasets, ConanDoyle-neg also annotates negated events that are part of the scopes. The analysis of negation is divided into two related sub-tasks, cue detection and scope resolution. While cue detection is mostly dependent on lexical or morphological features, relating cues to scopes is a structured prediction problem and will likely benefit from an analysis of morpho-syntactic or surface-semantic properties. The UiO2 system S HERLOCK (Lapponi et al., 2012), the winner of the open track of the *SEM 2012 shared task, uses morpho-syntactic parts of speech and syntactic dependencies to classify tokens as either Introduction Negation resolution (NR), the task of detecting negation and determining its scope, is relevant for a large number of applications in natural language processing, and has been the subject of several contrastive research efforts (Morante and Blanco, 2012; Oepen et al., 2017; Fares et al., 2018). In this paper we cast NR as a graph parsing problem. More specifically, we represent negation cues and corresponding scopes as a bi-lexi"
2020.iwpt-1.3,P14-1007,1,0.907349,"et al. (2017). 3.1 in-scope or out-of-scope using a conditional random field (CRF). Another CRF further classifies scope tokens as events, and a heuristic is applied to distribute scope tokens to their respective cues. The S HERLOCK system was subsequently used by Elming et al. (2013) to evaluate various dependency conversions, and similarly served as one of three reference ‘downstream’ applications in the 2017 Extrinsic Parser Evaluation initiative (EPE; Oepen et al., 2017). The best results from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using B"
2020.iwpt-1.3,P10-1052,0,0.04826,"Missing"
2020.iwpt-1.3,D14-1162,0,0.0830174,"Missing"
2020.iwpt-1.3,L18-1547,0,0.124555,"tup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) successfully improved previous results by a considerable margin. The 2018 follow-up to the EPE shared task (Fares et al., 2018) again used S HER LOCK to evaluate parsing performance, this time restricting itself to participating systems in the colocated 2018 CoNLL Shared Task on Universal Dependency Parsing (Zeman et al., 2018). 3 Data The negation data of *SEM 2012 consists of selected Sherlock Holmes stories from the works of Arthur C"
2020.iwpt-1.3,N18-1202,0,0.0368131,"*SEM 2012 shared task on negation resolution, clearly outperforming each previously best system, albeit none of our results is statistically significant. We believe that our approach can be used to restructure other tasks as dependency-style graphs in similar fashion, and thus reuse existing systems as general purpose tools. Recasting the negation resolution task as a graph-parsing problem allows us to straightforwardly use a variety of such tools. With most of these now using neural networks, we can extend them to employ massive pre-trained models such as BERT (Devlin et al., 2019) or ELMo (Peters et al., 2018). This would allow us to leverage their general power into more specific tasks that have only limited data available. Significant Learning While the boxplots in Figures 3 and 4 show the same general trends as our particular systems in Tables 2 and 3, they also illustrate the considerable variance of performance between runs. Choosing the final system with regards to performance on the development sets may lead to state-of-the-art performance on the evaluation sets—this is the case for our best performing system using gold cues and additional syntax processed by a GCN, which performs more than"
2020.iwpt-1.3,D17-1159,0,0.0304333,"and dense. LSTM, and 100-dimensional GloVe (Pennington et al., 2014) embeddings. Based on the contextdependent embeddings, two feedforward neural networks (FNN) create specialized representations of each word as a potential head and dependent: ~hi = FNNh (~ci ) d~i = FNNd (~ci ) These new representations are then scored via a ~: bilinear model with weight tensor U Graph Convolutional Networks Graph Convolutional Networks (GCNs; Kipf and Welling, 2017) generalize convolutional networks to graph-structured data. While they were developed with graphs much larger than our negation graphs in mind, Marcheggiani and Titov (2017) showed their use~0 = C ~ fulness for semantic role labelling. With X at the first level, we compute, for each level l > 0, a combined representation of heads (H), dependents (D), and the nodes themselves (S), weighted by ~ l: layer-specific weight matrices W   l−1  ~ l = ReLU A ~W ~ l +A ~ >W ~ l +W ~l X ~ X H D S ~~ score(~hi , d~j ) = ~h> i U dj ~ corresponds to The inner dimension of the tensor U the number of negation graph labels plus a special N ONE label indicating the absence of an arc, and thus predicts arcs and labels jointly. 4.3 Adding External Graph Features Similarly to S HER"
2020.iwpt-1.3,S12-1041,1,0.867849,"9 73.10 82.37 85.40 67.02 – 57.63 – S TANFORD -PARIS no syntax sumoh scatt gcn 91.76 91.62 91.51 93.26 73.76 74.10 76.16 73.11 86.57 85.63 84.72 84.22 71.96 69.43 70.00 72.40 65.69 63.39 66.42 65.19 90.98◦ 91.05 90.98 92.68∗ 75.81 68.64 72.25 73.83 87.69 86.66 86.09 86.89 60.66 59.74 61.21 63.69 59.40 52.58 57.64 58.07 T URKU NLP no syntax sumoh scatt gcn 92.98 92.49 92.54 91.02 76.22∗ 73.45 76.60 72.46◦ 85.61 85.03 85.34 84.90 71.11 75.35 71.03 73.49 66.42 62.31 64.15 63.15 90.71 90.66 90.13 90.98 72.09 71.73 71.85 71.43 86.92 87.91 87.06 86.57 59.88 60.06 57.23 63.40 55.18 53.19 52.08 54.54 Read et al. (2012) Packard et al. (2014) Table 3: Results of our NR parser on the S TANFORD -PARIS and T URKU NLP versions of the ConanDoyle-neg development and evaluation sets when cues are predicted. The numerically best results are shown in bold. We test for significant differences between our gcn with syntax models for S TANFORD -PARIS and T URKU NLP and respective models using no additional inputs. Only the ∗-marked measures are significantly different from their ◦-marked counterparts. 19 with the winning system of the *SEM 2012 shared task by Read et al. (2012), and also with the MRS Crawler of Packard et"
2020.iwpt-1.3,J93-2004,0,0.0727284,"Missing"
2020.iwpt-1.3,D19-6221,0,0.0275757,"in the 2017 Extrinsic Parser Evaluation initiative (EPE; Oepen et al., 2017). The best results from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) successfully improved previous results by a considerable margin. The 2018 follow-up to the EPE shared task (Fares et al., 2018) again used S HER LOCK to evaluate parsing performance, this time restricting itself to participating systems in the colocated"
2020.iwpt-1.3,W08-0606,0,0.216358,"n Doyle benchmark dataset, including a new top result for our best model. 1 Paper Structure In the following Section 2, we review selected related work on negation resolution. Section 3 describes the specific NR task that we address in this paper. In Section 4 we present our new encoding of negations and our parsing model, followed by the description of our experiments and results in Section 5. We discuss these results in Section 6 and summarize our findings in Section 7. 2 Related Work While there exist a variety of datasets that annotate negation (Jim´enez-Zafra et al., 2020), the BioScope (Szarvas et al., 2008) and Conan Doyle datasets (ConanDoyle-neg; Morante and Daelemans, 2012) are most commonly used for evaluation. The latter was created for the shared task at *SEM 2012 (Morante and Blanco, 2012), where competing systems needed to predict both negation cues (linguistic expressions of negation) and their corresponding scopes, i.e. the part of the utterance being negated. Cues can be simple negation markers (such as not or without), but may also consist of multiple words (i.e. neither . . . nor), or be mere affixes (i.e. infrequent or clueless). In contrast to other datasets, ConanDoyle-neg also a"
2020.iwpt-1.3,I05-2038,0,0.0495751,"Missing"
2020.iwpt-1.3,J12-2005,1,0.69606,"relationship between tokens and their cue(s), while being able to easily differentiate between regular scopes and events. An example for a negation graph is shown in Figure 2. We adopt a convention from dependency parsing and visualize negation graphs with their nodes laid out as the words of the respective sentence, and their arcs drawn above the nodes. When ~c1 , . . . , ~cn = BiLSTM(w ~ 1, . . . , w ~ n) 1 One main focus of the EPE task was the downstream evaluation of different syntactic representations; but the subtask of cue detection is relatively insensitive to grammatical structure (Velldal et al., 2012). We augment the input word embeddings w ~ i with additional part-of-speech tag and lemma embeddings, embeddings created by a character-based 16 Here, d is the size of the contextualized embeddings. In our case, where we merely want to ex~ is tract features from a given graph, the matrix A known and sparse; but the same scaled attention model could also be used in a multi-task setup to jointly learn to parse syntactico-semantic graphs ~ would be learned and negations, in which case A and dense. LSTM, and 100-dimensional GloVe (Pennington et al., 2014) embeddings. Based on the contextdependent"
2020.iwpt-1.3,K18-2001,0,0.0551834,"Missing"
2021.blackboxnlp-1.2,P17-1080,0,0.164196,"ime being transparent, intuitive, and easy to control. 1 Introduction The use of contextualised language models such as ELMo and BERT has brought about remarkable performance gains on a wide range of downstream tasks (Peters et al., 2018a; Devlin et al., 2019); but the question to what extent these models have acquired linguistic knowledge remains open. One way to investigate this question is through the use of probing classifiers trained to solve diagnostic prediction tasks that are considered to require linguistic information, such as parts-of-speech, syntactic structure, or semantic roles (Belinkov et al., 2017a; Conneau et al., 2018; Tenney et al., 2019). However, what conclusions can be drawn from probing experiments is disputed. In particular, a central point of debate is how to know whether probes ‘decode linguistic knowledge’ or simply ‘learn to 15 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 15–25 Online, November 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work In the domain of natural language understanding, authors have shown that Transformers lack the capability to extrapolate to longer sequences (Dubois"
2021.blackboxnlp-1.2,D19-1275,0,0.219669,", 2020). Evidently, test data outside the training distribution is a great challenge, and contextualised language models are easily broken on such data. The method that we propose in this paper synthesises several strands of related work: 2.1 Probing (and its Limitations) Probing aims at detecting linguistic knowledge in word representations. While this can be done in a zero-shot setting (Goldberg, 2019; Talmor et al., 2020) or as a structural probe (Hewitt and Manning, 2019), a dominant approach is to train and evaluate simple classifiers on relevant diagnostic tasks (Belinkov et al., 2017b; Hewitt and Liang, 2019), where the classifier receives one word representations at a time as its input. This is based on the idea that the accuracy of the trained probe can indicate to what extent the representations encode linguistic knowledge that is useful for the diagnostic task. Recent work has questioned the validity of this methodology, suggesting that analysis should shift focus to measuring ‘amount of effort’ rather than task-based accuracy (Pimentel et al., 2020; Voita and Titov, 2020). Moreover, many probing tasks are relatively easy to learn with local context and strong independence assumptions. It thus"
2021.blackboxnlp-1.2,I17-1001,0,0.161569,"ime being transparent, intuitive, and easy to control. 1 Introduction The use of contextualised language models such as ELMo and BERT has brought about remarkable performance gains on a wide range of downstream tasks (Peters et al., 2018a; Devlin et al., 2019); but the question to what extent these models have acquired linguistic knowledge remains open. One way to investigate this question is through the use of probing classifiers trained to solve diagnostic prediction tasks that are considered to require linguistic information, such as parts-of-speech, syntactic structure, or semantic roles (Belinkov et al., 2017a; Conneau et al., 2018; Tenney et al., 2019). However, what conclusions can be drawn from probing experiments is disputed. In particular, a central point of debate is how to know whether probes ‘decode linguistic knowledge’ or simply ‘learn to 15 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 15–25 Online, November 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work In the domain of natural language understanding, authors have shown that Transformers lack the capability to extrapolate to longer sequences (Dubois"
2021.blackboxnlp-1.2,N19-1419,0,0.0303818,"her values (Weiss et al., 2018); and that even large neural models such as RoBERTa can compare ages only within a restricted range (Talmor et al., 2020). Evidently, test data outside the training distribution is a great challenge, and contextualised language models are easily broken on such data. The method that we propose in this paper synthesises several strands of related work: 2.1 Probing (and its Limitations) Probing aims at detecting linguistic knowledge in word representations. While this can be done in a zero-shot setting (Goldberg, 2019; Talmor et al., 2020) or as a structural probe (Hewitt and Manning, 2019), a dominant approach is to train and evaluate simple classifiers on relevant diagnostic tasks (Belinkov et al., 2017b; Hewitt and Liang, 2019), where the classifier receives one word representations at a time as its input. This is based on the idea that the accuracy of the trained probe can indicate to what extent the representations encode linguistic knowledge that is useful for the diagnostic task. Recent work has questioned the validity of this methodology, suggesting that analysis should shift focus to measuring ‘amount of effort’ rather than task-based accuracy (Pimentel et al., 2020; Vo"
2021.blackboxnlp-1.2,D17-1215,0,0.0231212,"en fail to extrapolate, i.e. to generalise to inputs outside the range of the training data. For example, Barrett et al. (2018) show that in visual reasoning, popular models such as ResNets perform at levels barely above a random choice baseline in extrapolation settings. As the ability to extrapolate is generally considered a hallmark of intelligence, such findings raise the question whether deep models are capable of human-like reasoning. Similar concerns come from observations that performance can suffer greatly when models are confronted with adversarial examples (Goodfellow et al., 2015; Jia and Liang, 2017) or challenge sets (Zellers et al., 2018, 2019). Zellers et al. (2019) suggest that deep models may ‘pick up on dataset-specific distributional biases’ instead of learning the actual task. 2.3.2 Learning-Based Criteria Instead of using inherent properties, another way to quantify the hardness of training examples is to look at the effort that a model has to put into learning them. Here we take inspiration from developments in curriculum learning, which moved from heuristic metrics on artificial datasets (Bengio et al., 2009) to learning-specific metrics. In particular, self-paced learning empl"
2021.blackboxnlp-1.2,P18-1198,0,0.0234454,"ntuitive, and easy to control. 1 Introduction The use of contextualised language models such as ELMo and BERT has brought about remarkable performance gains on a wide range of downstream tasks (Peters et al., 2018a; Devlin et al., 2019); but the question to what extent these models have acquired linguistic knowledge remains open. One way to investigate this question is through the use of probing classifiers trained to solve diagnostic prediction tasks that are considered to require linguistic information, such as parts-of-speech, syntactic structure, or semantic roles (Belinkov et al., 2017a; Conneau et al., 2018; Tenney et al., 2019). However, what conclusions can be drawn from probing experiments is disputed. In particular, a central point of debate is how to know whether probes ‘decode linguistic knowledge’ or simply ‘learn to 15 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 15–25 Online, November 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work In the domain of natural language understanding, authors have shown that Transformers lack the capability to extrapolate to longer sequences (Dubois et al., 2020) and numb"
2021.blackboxnlp-1.2,kocmi-bojar-2017-curriculum,0,0.0262041,"alisation for the probing classifier (Prasanna et al., 2020). 2.2 2.3 What are Hard Examples? Most of the aforementioned works on extrapolation and abstraction employ synthetic datasets or adversarial attacks to challenge a model. Here we propose a method based on the stratification of existing probing datasets according to a measure of expected difficulty or ‘hardness’. 2.3.1 Readability Criteria One way to quantify the difficulty of training examples is to use readability criteria, which are typically motivated on linguistic grounds or with reference to studies on human language processing (Kocmi and Bojar, 2017; Platanios et al., 2019). A widely used and widely applicable metric is sentence length, which is intuitive and straightforward to measure (Sherman, 1893), but only weakly correlated with processing complexity (Bailin and Grafstein, 2001). There are also many more specific measures, such as the respective averages of parse tree height, length of arcs in syntactic dependency trees, number of noun phrases and number of verb phrases, or word frequency. These measures often inform systems that help authors improve writing quality, and automatically transform texts to make them more understandable"
2021.blackboxnlp-1.2,N19-1423,0,0.010981,"iscuss the relative merits of these criteria in the context of two syntactic probing tasks, part-of-speech tagging and syntactic dependency labelling. From our theoretical and experimental analysis, we conclude that distance-based and hard statistical criteria show the clearest differences between interpolation and extrapolation settings, while at the same time being transparent, intuitive, and easy to control. 1 Introduction The use of contextualised language models such as ELMo and BERT has brought about remarkable performance gains on a wide range of downstream tasks (Peters et al., 2018a; Devlin et al., 2019); but the question to what extent these models have acquired linguistic knowledge remains open. One way to investigate this question is through the use of probing classifiers trained to solve diagnostic prediction tasks that are considered to require linguistic information, such as parts-of-speech, syntactic structure, or semantic roles (Belinkov et al., 2017a; Conneau et al., 2018; Tenney et al., 2019). However, what conclusions can be drawn from probing experiments is disputed. In particular, a central point of debate is how to know whether probes ‘decode linguistic knowledge’ or simply ‘lea"
2021.blackboxnlp-1.2,D19-1277,0,0.0215461,"Missing"
2021.blackboxnlp-1.2,2020.acl-main.39,0,0.159917,"nto the capabilities and limitations of probes and probing methodology. In this paper, we propose to analyse probes in the context of an extrapolation setting, where the inputs at test time are deliberately chosen to be ‘harder’ than the training examples. While machine learning models and neural networks in particular have proved to be very effective learners in interpolation scenarios, where the examples at training time and those at test time are drawn from the same (idealised) underlying distribution, the ability of these models to extrapolate from the training data appears to be limited (Dubois et al., 2020). At the same time, extrapolation has been proposed as a litmus test for abstract reasoning in neural networks (Barrett et al., 2018). In the context of probing, we posit that the better the extrapolation capability of a probe, i.e. the higher its performance even in situations where the training and the test examples are substantially different, the more evidence we have for claiming that the probe actually uses abstract linguistic knowledge encoded in the input word representations. To construct extrapolation challenges, we propose a conceptually simple approach where we start from standard"
2021.blackboxnlp-1.2,2020.coling-main.450,1,0.75116,"resentations encode linguistic knowledge that is useful for the diagnostic task. Recent work has questioned the validity of this methodology, suggesting that analysis should shift focus to measuring ‘amount of effort’ rather than task-based accuracy (Pimentel et al., 2020; Voita and Titov, 2020). Moreover, many probing tasks are relatively easy to learn with local context and strong independence assumptions. It thus remains unclear whether the probed word representations actually encode linguistic knowledge, contain predictive but superficial features extracted from the words’ linear context (Kunz and Kuhlmann, 2020), or rather provide an effective initialisation for the probing classifier (Prasanna et al., 2020). 2.2 2.3 What are Hard Examples? Most of the aforementioned works on extrapolation and abstraction employ synthetic datasets or adversarial attacks to challenge a model. Here we propose a method based on the stratification of existing probing datasets according to a measure of expected difficulty or ‘hardness’. 2.3.1 Readability Criteria One way to quantify the difficulty of training examples is to use readability criteria, which are typically motivated on linguistic grounds or with reference to"
2021.blackboxnlp-1.2,2020.acl-main.420,0,0.0118572,"Hewitt and Manning, 2019), a dominant approach is to train and evaluate simple classifiers on relevant diagnostic tasks (Belinkov et al., 2017b; Hewitt and Liang, 2019), where the classifier receives one word representations at a time as its input. This is based on the idea that the accuracy of the trained probe can indicate to what extent the representations encode linguistic knowledge that is useful for the diagnostic task. Recent work has questioned the validity of this methodology, suggesting that analysis should shift focus to measuring ‘amount of effort’ rather than task-based accuracy (Pimentel et al., 2020; Voita and Titov, 2020). Moreover, many probing tasks are relatively easy to learn with local context and strong independence assumptions. It thus remains unclear whether the probed word representations actually encode linguistic knowledge, contain predictive but superficial features extracted from the words’ linear context (Kunz and Kuhlmann, 2020), or rather provide an effective initialisation for the probing classifier (Prasanna et al., 2020). 2.2 2.3 What are Hard Examples? Most of the aforementioned works on extrapolation and abstraction employ synthetic datasets or adversarial attacks t"
2021.blackboxnlp-1.2,2020.iwpt-1.3,1,0.681102,"n analysis tool. Another benefit of arc length and the most frequent tag criterion is that they are applicable to a wide range of tasks. The most frequent tag criterion can be applied to any word labelling task that has a limited number of labels. Examples for further tasks where it can be applied include named entity recognition and word sense disambiguation. Arc length can be applied to all tasks that can be formulated as operating on pairs of words. Besides other parsing tasks such as semantic dependency parsing, this is the case for e.g. coreference resolution or negation scope detection (Kurtz et al., 2020). 5.2 6 Conclusion We identified and suggested several ways to define the difficulty of training and validation examples based on linguistic, statistical, and learning-based criteria, to create extrapolation splits for natural language datasets. We demonstrated the usefulness of these measures for the analysis of two linguistic tasks, and proposed an evaluation protocol with baselines and metrics. Our experimental results suggest that a probe trained on BERT hidden representations is capable of applying patterns learned from easier examples to harder examples to some extent; but in well-motiva"
2021.blackboxnlp-1.2,N19-1119,0,0.057605,"Missing"
2021.blackboxnlp-1.2,2020.findings-emnlp.48,0,0.0321257,"ing them. Here we take inspiration from developments in curriculum learning, which moved from heuristic metrics on artificial datasets (Bengio et al., 2009) to learning-specific metrics. In particular, self-paced learning employs the loss of a model to rate and rank the difficulty of examples in a dataset (Kumar et al., 2010; Hacohen and Wein16 xtij to denote the BERT representation of wij . We omit the superscript when the base set (training or development) is understood or irrelevant. shall, 2019). This approach is widely used, but has also been criticised as being inherently modelspecific (Lalor and Yu, 2020). Other approaches that have been successfully employed in curriculum learning are rankings based on the norms of word embeddings (Liu et al., 2020) and on model uncertainty (Zhou et al., 2020). 3 T1: Part-of-speech tagging This is our prototypical single-word labelling task. Examples take the form e = (xij , yij ), where xij is the representation of a single word wij , and yij is the corresponding gold-standard tag. The POS class of a word captures some of its most basic syntactic properties, and can be predicted with local or even without context information at a high accuracy. For our data,"
2021.blackboxnlp-1.2,2020.acl-main.41,0,0.0806846,"Missing"
2021.blackboxnlp-1.2,2020.emnlp-main.259,0,0.0113426,"stioned the validity of this methodology, suggesting that analysis should shift focus to measuring ‘amount of effort’ rather than task-based accuracy (Pimentel et al., 2020; Voita and Titov, 2020). Moreover, many probing tasks are relatively easy to learn with local context and strong independence assumptions. It thus remains unclear whether the probed word representations actually encode linguistic knowledge, contain predictive but superficial features extracted from the words’ linear context (Kunz and Kuhlmann, 2020), or rather provide an effective initialisation for the probing classifier (Prasanna et al., 2020). 2.2 2.3 What are Hard Examples? Most of the aforementioned works on extrapolation and abstraction employ synthetic datasets or adversarial attacks to challenge a model. Here we propose a method based on the stratification of existing probing datasets according to a measure of expected difficulty or ‘hardness’. 2.3.1 Readability Criteria One way to quantify the difficulty of training examples is to use readability criteria, which are typically motivated on linguistic grounds or with reference to studies on human language processing (Kocmi and Bojar, 2017; Platanios et al., 2019). A widely use"
2021.blackboxnlp-1.2,D07-1013,0,0.0789922,"straightforward to measure (Sherman, 1893), but only weakly correlated with processing complexity (Bailin and Grafstein, 2001). There are also many more specific measures, such as the respective averages of parse tree height, length of arcs in syntactic dependency trees, number of noun phrases and number of verb phrases, or word frequency. These measures often inform systems that help authors improve writing quality, and automatically transform texts to make them more understandable or accessibile (Zamanian and Heydari, 2012), but are also used to evaluate systems such as dependency parsers (McDonald and Nivre, 2007; Kulmizev et al., 2019). Interpolation and Extrapolation A growing body of research suggests that, while deep neural models can reach remarkable performance in interpolation settings, they often fail to extrapolate, i.e. to generalise to inputs outside the range of the training data. For example, Barrett et al. (2018) show that in visual reasoning, popular models such as ResNets perform at levels barely above a random choice baseline in extrapolation settings. As the ability to extrapolate is generally considered a hallmark of intelligence, such findings raise the question whether deep models"
2021.blackboxnlp-1.2,2020.tacl-1.48,0,0.0312701,"whether probes ‘decode linguistic knowledge’ or simply ‘learn to 15 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 15–25 Online, November 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work In the domain of natural language understanding, authors have shown that Transformers lack the capability to extrapolate to longer sequences (Dubois et al., 2020) and number representations of higher values (Weiss et al., 2018); and that even large neural models such as RoBERTa can compare ages only within a restricted range (Talmor et al., 2020). Evidently, test data outside the training distribution is a great challenge, and contextualised language models are easily broken on such data. The method that we propose in this paper synthesises several strands of related work: 2.1 Probing (and its Limitations) Probing aims at detecting linguistic knowledge in word representations. While this can be done in a zero-shot setting (Goldberg, 2019; Talmor et al., 2020) or as a structural probe (Hewitt and Manning, 2019), a dominant approach is to train and evaluate simple classifiers on relevant diagnostic tasks (Belinkov et al., 2017b; Hewitt"
2021.blackboxnlp-1.2,2020.lrec-1.497,0,0.0454441,"Missing"
2021.blackboxnlp-1.2,P19-1452,0,0.0469707,"control. 1 Introduction The use of contextualised language models such as ELMo and BERT has brought about remarkable performance gains on a wide range of downstream tasks (Peters et al., 2018a; Devlin et al., 2019); but the question to what extent these models have acquired linguistic knowledge remains open. One way to investigate this question is through the use of probing classifiers trained to solve diagnostic prediction tasks that are considered to require linguistic information, such as parts-of-speech, syntactic structure, or semantic roles (Belinkov et al., 2017a; Conneau et al., 2018; Tenney et al., 2019). However, what conclusions can be drawn from probing experiments is disputed. In particular, a central point of debate is how to know whether probes ‘decode linguistic knowledge’ or simply ‘learn to 15 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 15–25 Online, November 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work In the domain of natural language understanding, authors have shown that Transformers lack the capability to extrapolate to longer sequences (Dubois et al., 2020) and number representations of"
2021.blackboxnlp-1.2,2020.emnlp-demos.15,0,0.0330298,"the comparatively low accuracy in the control setup shows that longer arcs are a challenge in themselves, restricting the training set to short arcs limits the accuracy of the probe even further. Extrapolation capability is limited even in the softened setup where we decrease the distance between training and test set (Figure 2, right). Thus, under this scoring function, we find no evidence that the probe extracts useful linguistic knowledge from the word representations – a conclusion that establishes a difference between our extrapolation setup and results for interpolation-based learning (Tenney et al., 2020; Hewitt and Liang, 2019). Sample-specific loss The most opaque of all scoring functions is arguably the loss-based criterion. It is even less transparent than the learningbased criterion, where we can possibly identify the learned (and missed) patterns in an error analysis. With the loss-based criterion, we will be unlikely to identify commonalities between examples that share the same ranking with respect to the scoring function. While the loss-based criterion strongly discriminates between the standard setup and the extrapolation setup, this is largely an effect of the construction of the t"
2021.blackboxnlp-1.2,N18-1202,0,0.0167457,"nge of NLP tasks. We discuss the relative merits of these criteria in the context of two syntactic probing tasks, part-of-speech tagging and syntactic dependency labelling. From our theoretical and experimental analysis, we conclude that distance-based and hard statistical criteria show the clearest differences between interpolation and extrapolation settings, while at the same time being transparent, intuitive, and easy to control. 1 Introduction The use of contextualised language models such as ELMo and BERT has brought about remarkable performance gains on a wide range of downstream tasks (Peters et al., 2018a; Devlin et al., 2019); but the question to what extent these models have acquired linguistic knowledge remains open. One way to investigate this question is through the use of probing classifiers trained to solve diagnostic prediction tasks that are considered to require linguistic information, such as parts-of-speech, syntactic structure, or semantic roles (Belinkov et al., 2017a; Conneau et al., 2018; Tenney et al., 2019). However, what conclusions can be drawn from probing experiments is disputed. In particular, a central point of debate is how to know whether probes ‘decode linguistic kn"
2021.blackboxnlp-1.2,2020.emnlp-main.14,0,0.0177195,"9), a dominant approach is to train and evaluate simple classifiers on relevant diagnostic tasks (Belinkov et al., 2017b; Hewitt and Liang, 2019), where the classifier receives one word representations at a time as its input. This is based on the idea that the accuracy of the trained probe can indicate to what extent the representations encode linguistic knowledge that is useful for the diagnostic task. Recent work has questioned the validity of this methodology, suggesting that analysis should shift focus to measuring ‘amount of effort’ rather than task-based accuracy (Pimentel et al., 2020; Voita and Titov, 2020). Moreover, many probing tasks are relatively easy to learn with local context and strong independence assumptions. It thus remains unclear whether the probed word representations actually encode linguistic knowledge, contain predictive but superficial features extracted from the words’ linear context (Kunz and Kuhlmann, 2020), or rather provide an effective initialisation for the probing classifier (Prasanna et al., 2020). 2.2 2.3 What are Hard Examples? Most of the aforementioned works on extrapolation and abstraction employ synthetic datasets or adversarial attacks to challenge a model. Her"
2021.blackboxnlp-1.2,D18-1179,0,0.013443,"nge of NLP tasks. We discuss the relative merits of these criteria in the context of two syntactic probing tasks, part-of-speech tagging and syntactic dependency labelling. From our theoretical and experimental analysis, we conclude that distance-based and hard statistical criteria show the clearest differences between interpolation and extrapolation settings, while at the same time being transparent, intuitive, and easy to control. 1 Introduction The use of contextualised language models such as ELMo and BERT has brought about remarkable performance gains on a wide range of downstream tasks (Peters et al., 2018a; Devlin et al., 2019); but the question to what extent these models have acquired linguistic knowledge remains open. One way to investigate this question is through the use of probing classifiers trained to solve diagnostic prediction tasks that are considered to require linguistic information, such as parts-of-speech, syntactic structure, or semantic roles (Belinkov et al., 2017a; Conneau et al., 2018; Tenney et al., 2019). However, what conclusions can be drawn from probing experiments is disputed. In particular, a central point of debate is how to know whether probes ‘decode linguistic kn"
2021.blackboxnlp-1.2,P18-2117,0,0.0121211,"conclusions can be drawn from probing experiments is disputed. In particular, a central point of debate is how to know whether probes ‘decode linguistic knowledge’ or simply ‘learn to 15 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 15–25 Online, November 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work In the domain of natural language understanding, authors have shown that Transformers lack the capability to extrapolate to longer sequences (Dubois et al., 2020) and number representations of higher values (Weiss et al., 2018); and that even large neural models such as RoBERTa can compare ages only within a restricted range (Talmor et al., 2020). Evidently, test data outside the training distribution is a great challenge, and contextualised language models are easily broken on such data. The method that we propose in this paper synthesises several strands of related work: 2.1 Probing (and its Limitations) Probing aims at detecting linguistic knowledge in word representations. While this can be done in a zero-shot setting (Goldberg, 2019; Talmor et al., 2020) or as a structural probe (Hewitt and Manning, 2019), a do"
2021.blackboxnlp-1.2,D18-1009,0,0.0135856,"se to inputs outside the range of the training data. For example, Barrett et al. (2018) show that in visual reasoning, popular models such as ResNets perform at levels barely above a random choice baseline in extrapolation settings. As the ability to extrapolate is generally considered a hallmark of intelligence, such findings raise the question whether deep models are capable of human-like reasoning. Similar concerns come from observations that performance can suffer greatly when models are confronted with adversarial examples (Goodfellow et al., 2015; Jia and Liang, 2017) or challenge sets (Zellers et al., 2018, 2019). Zellers et al. (2019) suggest that deep models may ‘pick up on dataset-specific distributional biases’ instead of learning the actual task. 2.3.2 Learning-Based Criteria Instead of using inherent properties, another way to quantify the hardness of training examples is to look at the effort that a model has to put into learning them. Here we take inspiration from developments in curriculum learning, which moved from heuristic metrics on artificial datasets (Bengio et al., 2009) to learning-specific metrics. In particular, self-paced learning employs the loss of a model to rate and rank"
2021.blackboxnlp-1.2,P19-1472,0,0.015523,"e of the training data. For example, Barrett et al. (2018) show that in visual reasoning, popular models such as ResNets perform at levels barely above a random choice baseline in extrapolation settings. As the ability to extrapolate is generally considered a hallmark of intelligence, such findings raise the question whether deep models are capable of human-like reasoning. Similar concerns come from observations that performance can suffer greatly when models are confronted with adversarial examples (Goodfellow et al., 2015; Jia and Liang, 2017) or challenge sets (Zellers et al., 2018, 2019). Zellers et al. (2019) suggest that deep models may ‘pick up on dataset-specific distributional biases’ instead of learning the actual task. 2.3.2 Learning-Based Criteria Instead of using inherent properties, another way to quantify the hardness of training examples is to look at the effort that a model has to put into learning them. Here we take inspiration from developments in curriculum learning, which moved from heuristic metrics on artificial datasets (Bengio et al., 2009) to learning-specific metrics. In particular, self-paced learning employs the loss of a model to rate and rank the difficulty of examples in"
2021.blackboxnlp-1.2,W18-5448,0,0.0225173,"ll examples that are classified incorrectly. For tasks where the performance of a standard probe is already low, the test set will solely We start by arguing for the merits of the different scoring functions in the context of probing. 21 consist of misclassified examples. Applying the loss of fully trained probes on the test set can therefore be seen as circular. is infinite. However, classical interpolation-based setups using probing classifiers tend to overestimate the information present in the representations, as classifiers can learn a task even from randomly initialised word embeddings (Zhang and Bowman, 2018; Hewitt and Liang, 2019). Therefore we argue that, at this time, we need to be more aware of false positives than of false negatives in probing. Extrapolation probes have the potential to reduce the false positive rate while providing new insights into the generalisability of the features they use. Summary To summarise, from a perspective of transparency, controllability, and demonstrable success in separating the data into easier and harder examples, we argue that the most interesting metrics for the identification of extrapolation challenges are arc length and the most frequent tag criterio"
2021.blackboxnlp-1.2,2020.acl-main.620,0,0.0381736,"Missing"
C04-1026,P01-1024,1,0.9126,"he underspecification approach. We assume a set of linguistic dimensions, such as (syntactic) immediate dominance and predicate-argument structure; a grammatical analysis is a tuple with one component for each dimension, and a grammar describes a set of such tuples. While we make no a priori functionality assumptions about the relation of the linguistic dimensions, functional mappings can be obtained as a special case. We formalise our syntax-semantics interface using Extensible Dependency Grammar (XDG), a new grammar formalism which generalises earlier work on Topological Dependency Grammar (Duchier and Debusmann, 2001). The relational syntax-semantics interface is supported by a parser for XDG based on constraint programming. The crucial feature of this parser is that it supports the concurrent flow of possibly partial information between any two dimensions: once additional information becomes available on one dimension, it can be propagated to any other dimension. Grammaticality conditions and preferences (e. g. selectional restrictions) can be specified on their natural level of representation, and inferences on each dimension can help reduce ambiguity on the others. This generalises the idea of underspec"
C04-1026,E03-1054,1,0.834095,"ces on SC . This means that semantic information can be used to disambiguate syntactic ambiguities, and semantic information such as selectional preferences can be stated on their natural level of representation, rather than be forced into the ID dimension directly. Similarly, the introduction of new edges on SC could trigger a similar reasoning process which would infer new PA-edges, and thus indirectly also new ID-edges. Such new edges on SC could come from inferences with world or discourse knowledge (Koller and Niehren, 2000), scope preferences, or interactions with information structure (Duchier and Kruijff, 2003). 4 Traditional Semantics Our syntax-semantics interface represents semantic information as graphs on the PA and SC dimensions. While this looks like a radical departure from traditional semantic formalisms, we consider these graphs simply an alternative way of presenting more traditional representations. We devote the rest of the paper to demonstrating that a pair of a PA and a SC structure can be interpreted as a Montague-style formula, and that a partial analysis on these two dimensions can be seen as an underspecified semantic description. 4.1 Montague-style Interpretation In order to extr"
C04-1026,P98-1077,0,0.0225832,"ctic ambiguity by compiling semantic distinctions into the syntax (Montague, 1974; Steedman, 1999; Moortgat, 2002). This restores functionality, but comes at the price of an artificial blowup of syntactic ambiguity. A second approach is to assume a non-deterministic mapping from syntax to semantics as in generative grammar (Chomsky, 1965), but it is not always obvious how to reverse the relation, e. g. for generation. For LFG, the operation of functional uncertaintainty allows for a restricted form of relationality (Kaplan and Maxwell III, 1988). Finally, underspecification (Egg et al., 2001; Gupta and Lamping, 1998; Copestake et al., 2004) introduces a new level of representation, which can be computed functionally from a syntactic analysis and encapsulates semantic ambiguity in a way that supports the enumeration of all semantic readings by need. In this paper, we introduce a completely relational syntax-semantics interface, building upon the underspecification approach. We assume a set of linguistic dimensions, such as (syntactic) immediate dominance and predicate-argument structure; a grammatical analysis is a tuple with one component for each dimension, and a grammar describes a set of such tuples."
C04-1026,C88-1060,0,0.253565,"Missing"
C04-1026,C00-1067,1,0.832765,"a prep-child of student on ID. In the other direction, the solver will infer more dominances on SC . This means that semantic information can be used to disambiguate syntactic ambiguities, and semantic information such as selectional preferences can be stated on their natural level of representation, rather than be forced into the ID dimension directly. Similarly, the introduction of new edges on SC could trigger a similar reasoning process which would infer new PA-edges, and thus indirectly also new ID-edges. Such new edges on SC could come from inferences with world or discourse knowledge (Koller and Niehren, 2000), scope preferences, or interactions with information structure (Duchier and Kruijff, 2003). 4 Traditional Semantics Our syntax-semantics interface represents semantic information as graphs on the PA and SC dimensions. While this looks like a radical departure from traditional semantic formalisms, we consider these graphs simply an alternative way of presenting more traditional representations. We devote the rest of the paper to demonstrating that a pair of a PA and a SC structure can be interpreted as a Montague-style formula, and that a partial analysis on these two dimensions can be seen as"
C04-1026,P02-1003,1,0.835191,"e present, and that others are excluded. Partial analyses will play an important role in Section 3.3. Because propagation operates on all dimensions concurrently, the constraint solver can frequently infer information about one dimension from information on another, if there is a multi-dimensional principle linking the two dimensions. These inferences take place while the constraint problem is being solved, and they can often be drawn before the solver commits to any single solution. Because XDG allows us to write grammars with completely free word order, XDG solving is an NPcomplete problem (Koller and Striegnitz, 2002). This means that the worst-case complexity of the solver is exponential, but the average-case complexity for the hand-crafted grammars we experimented with is often better than this result suggests. We hope there are useful fragments of XDG that would guarantee polynomial worst-case complexity. 3 A Relational Syntax-Semantics Interface Now that we have the formal and processing frameworks in place, we can define a relational syntaxsemantics interface for XDG. We will first show how we encode semantics within the XDG framework. Then we will present an example grammar (including some principle"
C04-1026,J93-4001,0,0.0305064,"get bidirectional grammars for free. While the solver is reasonably efficient for many (hand-crafted) grammars, it is an important goal for the future to ensure that it can handle largescale grammars imported from e.g. XTAG (XTAG Research Group, 2001) or induced from treebanks. One way in which we hope to achieve this is to identify fragments of XDG with provably polynomial parsing algorithms, and which contain most useful grammars. Such grammars would probably have to specify word orders that are not completely free, and we would have to control the combinatorics of the different dimensions (Maxwell and Kaplan, 1993). One interesting question is also whether different dimensions can be compiled into a single dimension, which might improve efficiency in some cases, and also sidestep the monostratal vs. multistratal distinction. The crucial ingredient of XDG that make relational syntax-semantics processing possible are the declaratively specified principles. So far, we have only given some examples for principle specifications; while they could all be written as Horn clauses, we have not committed to any particular representation formalism. The development of such a representation formalism will of course b"
C04-1026,P99-1039,0,0.0162758,"nambiguous sentence can have multiple semantic readings. Conversely, a common situation in natural language generation is that one semantic representation can be verbalised in multiple ways. This means that the relation between syntax and semantics is not functional at all, but rather a true m-to-n relation. There is a variety of approaches in the literature on syntax-semantics interfaces for coping with this situation, but none of them is completely satisfactory. One way is to recast semantic ambiguity as syntactic ambiguity by compiling semantic distinctions into the syntax (Montague, 1974; Steedman, 1999; Moortgat, 2002). This restores functionality, but comes at the price of an artificial blowup of syntactic ambiguity. A second approach is to assume a non-deterministic mapping from syntax to semantics as in generative grammar (Chomsky, 1965), but it is not always obvious how to reverse the relation, e. g. for generation. For LFG, the operation of functional uncertaintainty allows for a restricted form of relationality (Kaplan and Maxwell III, 1988). Finally, underspecification (Egg et al., 2001; Gupta and Lamping, 1998; Copestake et al., 2004) introduces a new level of representation, which"
C04-1026,C98-1074,0,\N,Missing
E09-1053,E03-1036,0,0.170267,"anguage is an bn cn dn . This language is not itself context-free, and therefore any PF-CCG grammar whose language contains it also contains permutations in which the order of the symbols is mixed up. The culprit for this among the restrictions that distinguish PF-CCG from full CCG seems to be that PF-CCG grammars must allow all instances of the application rules. This would mean that the ability of CCG to generate noncontext-free languages (also linguistically relevant ones) hinges crucially on its ability to restrict the allowable instances of rule schemata, for instance, using slash types (Baldridge and Kruijff, 2003). 5 Conclusion In this paper, we have shown how to read derivations of PF-CCG as dependency trees. Unlike previous proposals, our view on CCG dependencies is in line with the mainstream dependency parsing literature, which assumes tree-shaped dependency structures; while our dependency trees are less informative than the CCG derivations themselves, they contain sufficient information to reconstruct the semantic representation. We used our new dependency view to compare the strong generative capacity of PF-CCG with other mildly contextsensitive grammar formalisms. It turns out that the valency"
E09-1053,C04-1180,0,0.0401481,"sms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the"
E09-1053,J07-4004,0,0.0442998,"s tree-shaped. We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG"
E09-1053,P02-1042,0,0.690196,"exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (McDonald et al., 2005; Nivre et al., 2007) and TAG derivation trees. The price we pay for restricting ourselves to trees is that we derive fewer dependencies than the more powerful approach by Clark et al. (2002). Indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled. However, we show that our dependency trees are still informative enough to reconstruct the semantic representations. The paper is structured as follows. In Section 2, we introduce CCG and the fragment PF-CCG that we consider in this paper, and compare our contribution to earlier research. In Section 3, we then show how to read off a dependency tree from a CCG derivation. Finally, we explore the strong generative capacity of CCG in Section 4 and conclud"
E09-1053,J07-3004,0,0.0454784,"trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic p"
E09-1053,W08-2306,0,0.243319,"rage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another. In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG. We then explore the strong generative capacity of PF-CCG in terms of dependency trees. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)). We show that if we only look at valencies and ignore wo"
E09-1053,P06-1064,0,0.0170593,"rative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scr"
E09-1053,P07-1043,1,0.833484,"e technically restricted to the fragment of PF-CCG, and one focus of future work will be to extend them to as large a fragment of CCG as possible. In particular, we plan to extend the lambda notation used in Figure 3 to cover typeraising and higher-order categories. We would then be set to compare the behavior of wide-coverage statistical parsers for CCG with statistical dependency parsers. We anticipate that our results about the strong generative capacity of PF-CCG will be useful to transfer algorithms and linguistic insights between formalisms. For instance, the CRISP generation algorithm (Koller and Stone, 2007), while specified for TAG, could be generalized to arbitrary grammar formalisms that use regular tree languages— given our results, to CCG in particular. On the other hand, we find it striking that CCG and TAG generate the same string languages from the same tree languages by incomparable mechanisms for ordering the words in the tree. Indeed, the exact characterization of the class of CCG-inducable dependency languages is an open issue. This also has consequences for parsing complexity: We can understand why TAG and LCFRS can be parsed in polynomial time from the bounded block-degree of their"
E09-1053,P07-1021,1,0.947562,"pendency tree, it is usually convenient to specify its tree structure and the linear order of its nodes separately. The tree structure encodes the valency structure of the sentence (immediate dominance), whereas the linear precedence of the words is captured by the linear order. For the purposes of this paper, we represent a dependency tree as a pair d = (t, s), where t is a ground term over some suitable alphabet, and s is a linearization of the nodes (term addresses) of t, where by a linearization of a set S we mean a list of elements of S in which each element occurs exactly once (see also Kuhlmann and Möhl (2007)). As examples, consider (f (a, b), [1, ε, 2]) and (f (g(a)), [1 · 1, ε, 1]) . These expressions represent the dependency trees d1 = and a f b d2 = . a f g Notice that it is because of the separate specification of the tree and the order that dependency trees can become non-projective; d2 is an example. A partial dependency tree is a pair (t, s) where t is a term that may contain variables, and s is a linearization of those nodes of t that are not labelled with variables. We restrict ourselves to terms in which each variable appears exactly once, and will also prefix partial dependency trees w"
E09-1053,H05-1066,0,0.0611727,"trees induced by a PFCCG grammar form a regular tree language, just as for TAG and LCFRS. To our knowledge, this is the first time that the regularity of CCG’s derivational structures has been exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (McDonald et al., 2005; Nivre et al., 2007) and TAG derivation trees. The price we pay for restricting ourselves to trees is that we derive fewer dependencies than the more powerful approach by Clark et al. (2002). Indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled. However, we show that our dependency trees are still informative enough to reconstruct the semantic representations. The paper is structured as follows. In Section 2, we introduce CCG and the fragment PF-CCG that we consider in this paper, and compare our contribut"
E09-1053,1985.tmi-1.17,0,0.0434485,"Missing"
E09-1053,P90-1001,0,0.101252,"Missing"
E09-1053,P87-1015,0,0.896728,"istic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another. In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG. We then explore the strong generative capacity of PF-CCG in terms of dependency trees. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)). We show that if we only look at valencies and ignore word order, then the dependency trees induced by a PFCCG grammar form a regular tree language, just as for TAG and LCFRS. To our knowledge, this is the first time that the regularity of CCG’s derivational structures has been exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures ar"
E09-1055,W06-2922,0,0.0574397,"University Uppsala, Sweden marco.kuhlmann@lingfil.uu.se Giorgio Satta University of Padua Padova, Italy satta@dei.unipd.it Abstract The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. We propose to attack this problem using"
E09-1055,W06-2920,0,0.150008,"Missing"
E09-1055,C96-1058,0,0.0654075,"Missing"
E09-1055,E09-1034,0,0.81497,"Missing"
E09-1055,P07-1077,0,0.230527,"j  jwjf .rC1/ /, where P is the set of productions of the input grammar G, w is the input string, r is the maximal rank, and f is the maximal fan-out of a production in G (Seki et al., 1991). For a grammar G extracted by our technique, the number f equals the maximal block-degree per node. Hence, without any further modification, we obtain a parsing algorithm that is polynomial in the length of the sentence, but exponential in both the block-degree and the rank. This is clearly unacceptable in practical systems. The relative frequency of analyses with a block-degree  2 is almost negligible (Havelka, 2007); the bigger obstacle in applying the treebank grammar is the rank of the resulting LCFRS. Therefore, in the remainder of the paper, we present an algorithm that can transform the productions of the input grammar G into an equivalent set of productions with rank at most 2, while preserving the fan-out. This transformation, if it succeeds, yields a parsing algorithm that runs in time O.jP j  r  jwj3f /. 481 nmod root node tmp pp np sbj vc nmod 1 A 2 hearing 3 is 4 scheduled 5 on nmod ! g1 sbj ! g2 .nmod; pp/ root ! g3 .sbj; vc/ vc ! g4 .tmp/ pp ! g5 .np/ nmod ! g6 np ! g7 .nmod/ tmp ! g8 6 th"
E09-1055,P07-1021,1,0.853683,"pose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms. This proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Kuhlmann and Nivre, 2006), and by the close link between such ‘mildly non-projective’ dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative capacity on the other (Kuhlmann and Möhl, 2007). Furthermore, as pointed out by McDonald and Satta (2007), chart-parsing algorithms are amenable to augmentation by non-local information such as arity constraints and Markovization, and therefore should allow for more predictive statistical models than those used by current systems for non-projective dependency parsing. Hence, mildly non-projective dependency parsing promises to be both efficient and accurate. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 478–486, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 478 Cont"
E09-1055,P06-2066,1,0.945858,"e Prague Dependency Treebank of Czech (Hajiˇc et al., 2001) are nonprojective, and for German and Dutch treebanks, the proportion of non-projective structures is even higher (Havelka, 2007). In this paper, we propose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms. This proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Kuhlmann and Nivre, 2006), and by the close link between such ‘mildly non-projective’ dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative capacity on the other (Kuhlmann and Möhl, 2007). Furthermore, as pointed out by McDonald and Satta (2007), chart-parsing algorithms are amenable to augmentation by non-local information such as arity constraints and Markovization, and therefore should allow for more predictive statistical models than those used by current systems for non-projective dependency parsing. Hence, mildly non-projective dependency parsing promises to be bo"
E09-1055,E06-1011,0,0.0581456,"l and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms. In this paper, we provide two key tools for this approach. First, we show how to reduce nonprojective dependency parsing to parsing with Linear Context-Free Rewriting Systems (LCFRS), by presenting a technique for extracting LCFRS from dependency treebanks. For efficient parsing, the extracted grammars need to be transformed in order to minimize the number of nonte"
E09-1055,W07-2216,1,0.114973,"ectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms. In this paper, we provide two key tools for this approach. First, we show how to reduce nonprojective dependency parsing to parsing with Linear Context-Free Rewriting Systems (LCFRS), by presenting a technique for extracting LCFRS from dependency treebanks. For efficient parsing, the ex"
E09-1055,H05-1066,0,0.272775,"Missing"
E09-1055,P05-1013,0,0.133784,"Missing"
E09-1055,W03-3017,0,0.0960098,"Missing"
E09-1055,N07-1050,0,0.0628004,"sala, Sweden marco.kuhlmann@lingfil.uu.se Giorgio Satta University of Padua Padova, Italy satta@dei.unipd.it Abstract The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. We propose to attack this problem using chart-parsing"
E09-1055,P87-1015,0,0.576452,"u. The number of blocks of u is called the block-degree of u. The blockdegree of a dependency tree is the maximum among the block-degrees of its nodes. A dependency tree is projective, if its block-degree is 1. Example 2 The tree shown in Figure 2 is not projective: both node 2 (hearing) and node 4 (scheduled) have block-degree 2. Their blocks are f 2 g; f 5; 6; 7 g and f 4 g; f 8 g, respectively. 2.2 LCFRS Linear Context-Free Rewriting Systems (LCFRS) have been introduced as a generalization of several mildly context-sensitive grammar formalisms. Here we use the standard definition of LCFRS (Vijay-Shanker et al., 1987) and only fix our notation; for a more thorough discussion of this formalism, we refer to the literature. Let G be an LCFRS. Recall that each nonterminal symbol A of G comes with a positive integer called the fan-out of A, and that a production p of G has the form A ! g.A1 ; : : : ; Ar / I g.xE1 ; : : : ; xEr / D ˛E ; where A; A1 ; : : : ; Ar are nonterminals with fan-out f; f1 ; : : : ; fr , respectively, g is a function symbol, and the equation to the right of the semicolon specifies the semantics of g. For each i 2 Œr, xEi is an fi -tuple of variables, and ˛E D h˛1 ; : : : ; ˛f i is a tupl"
E09-1055,W05-1505,0,\N,Missing
J12-3006,C92-2066,0,0.47281,"sk whether lexicalized TAGs can provide a strong lexicalization of CFGs. Schabes’ second result is that this is indeed the case. This means that, given a CFG G, one can always construct a lexicalized TAG generating the same set of parse trees as G, and consequently the same string language. Following from this result, there arose the possibility of establishing a third result, stating that TAGs are closed under strong lexicalization. Schabes (1990) states that this is the case, and provides an informal argument to justify the claim. The same claim still appears in two subsequent publications (Joshi and Schabes 1992, 1997), but no precise proof of it has appeared until now. We speculate that the claim could be due to the fact that adjunction is more powerful than substitution with respect to weak generative capacity. It turns out, however, that when it comes to strong generative capacity, adjunction also shares some of the restrictions of substitution. This observation leads to the main result of this article: TAGs are not closed under strong lexicalization. In other words, there are TAGs that lack a strongly equivalent lexicalized version. In the same line of investigation, Schabes and Waters (1995) int"
J12-3006,J95-4002,0,0.934162,"tions (Joshi and Schabes 1992, 1997), but no precise proof of it has appeared until now. We speculate that the claim could be due to the fact that adjunction is more powerful than substitution with respect to weak generative capacity. It turns out, however, that when it comes to strong generative capacity, adjunction also shares some of the restrictions of substitution. This observation leads to the main result of this article: TAGs are not closed under strong lexicalization. In other words, there are TAGs that lack a strongly equivalent lexicalized version. In the same line of investigation, Schabes and Waters (1995) introduce a restricted variant of TAG called tree insertion grammars (TIGs). This formalism severely restricts the adjunction operation originally deﬁned for TAGs, in such a way that the class of generated string languages, as well as the class of generated parse trees, are the same as those of CFGs. Schabes and Waters then conjecture that TIGs are closed under strong lexicalization. In this article we also disprove their conjecture. 2. Preliminaries We assume familiarity with the TAG formalism; for a survey, we refer the reader to Joshi and Schabes (1997). We brieﬂy introduce here the basic"
J13-2004,W01-1807,0,0.105805,"Missing"
J13-2004,W06-2920,0,0.311412,"y have become the basis for several linguistic theories, such as Functional Generative Description (Sgall, Hajiˇcov´a, and Panevov´a 1986), Meaning–Text Theory (Mel’ˇcuk 1988), and Word Grammar (Hudson 2007). In recent years they have also been used for a wide range of practical applications, such as information extraction, machine translation, and question answering. We ascribe the widespread interest in dependency structures to their intuitive appeal, their conceptual simplicity, and in particular to the availability of accurate and efﬁcient dependency parsers for a wide range of languages (Buchholz and Marsi 2006; Nivre et al. 2007). Although there exist both a considerable practical interest and an extensive linguistic literature, dependency syntax has remained something of an island from a formal point of view. In particular, there are relatively few results that bridge between dependency syntax and other traditions, such as phrase structure or categorial syntax. ∗ Department of Linguistics and Philology, Box 635, 751 26 Uppsala, Sweden. E-mail: marco.kuhlmann@lingfil.uu.se. Submission received: 17 December 2009; revised submission received: 3 April 2012; accepted for publication: 24 May 2012. doi:1"
J13-2004,W98-0106,0,0.199508,"tion, see Koller and Kuhlmann (2009). We have deﬁned the dependency trees that an LCFRS induces by means of a compositional mapping on the derivations. While we would claim that compositionality is a generally desirable property, the particular notion of induction is up for discussion. In particular, our interpretation of derivations may not always be in line with how the grammar producing these derivations is actually used. One formalism for which such a mismatch between derivation trees and dependency trees has been pointed out is treeadjoining grammar (Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998). Resolving this mismatch provides an interesting line of future work. One aspect that we have not discussed here is the linguistic adequacy of blockdegree and well-nestedness. Each of our dependency grammars is restricted to a ﬁnite block-degree. As a consequence of this restriction, our dependency grammars are not expressive enough to capture linguistic phenomena that require unlimited degrees of non-projectivity, such as the “scrambling” in German subordinate clauses (Becker, Rambow, and Niv 1992). The question whether it is reasonable to assume a bound on the block-degree of dependency tre"
J13-2004,W10-4407,0,0.0408776,"Missing"
J13-2004,P11-1046,0,0.0124524,"e degree of the polynomial depends on two grammar-speciﬁc measures called fan-out and rank. We show that even in the restricted case of canonical grammars, parsing is an NPhard problem. It is important therefore to keep the fan-out and the rank of a grammar as low as possible, and much of the recent work on LCFRSs has been devoted to the development of techniques that optimize parsing complexity in various scenarios ´ ´ Gomez-Rodr´ ıguez and Satta 2009; Gomez-Rodr´ ıguez et al. 2009; Kuhlmann and Satta ´ 2009; Gildea 2010; Gomez-Rodr´ ıguez, Kuhlmann, and Satta 2010; Sagot and Satta 2010; and Crescenzi et al. 2011). In this article we explore the impact of non-projectivity on parsing complexity. In Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS, a measure called block-degree (or gap-degree) (Holan et al. 1998). Although there is no theoretical upper bound on the block-degree of the dependency trees needed for linguistic analysis, we provide evidence from several dependency treebanks showing that, from a practical point of view, this upper bound can be put at a value of as low as 2. In Section 8 we study a second constraint on non-projectivity called well-nestedne"
J13-2004,dzeroski-etal-2006-towards,0,0.0146856,"Missing"
J13-2004,N10-1118,0,0.0786396,"though the runtime of this algorithm is polynomial in the length of the sentence, the degree of the polynomial depends on two grammar-speciﬁc measures called fan-out and rank. We show that even in the restricted case of canonical grammars, parsing is an NPhard problem. It is important therefore to keep the fan-out and the rank of a grammar as low as possible, and much of the recent work on LCFRSs has been devoted to the development of techniques that optimize parsing complexity in various scenarios ´ ´ Gomez-Rodr´ ıguez and Satta 2009; Gomez-Rodr´ ıguez et al. 2009; Kuhlmann and Satta ´ 2009; Gildea 2010; Gomez-Rodr´ ıguez, Kuhlmann, and Satta 2010; Sagot and Satta 2010; and Crescenzi et al. 2011). In this article we explore the impact of non-projectivity on parsing complexity. In Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS, a measure called block-degree (or gap-degree) (Holan et al. 1998). Although there is no theoretical upper bound on the block-degree of the dependency trees needed for linguistic analysis, we provide evidence from several dependency treebanks showing that, from a practical point of view, this upper bound can be put at a value of"
J13-2004,J11-3004,0,0.0658998,"Missing"
J13-2004,N09-1061,1,0.936127,"Missing"
J13-2004,P09-1111,0,0.268389,"sent a simple parsing algorithm for LCFRSs. Although the runtime of this algorithm is polynomial in the length of the sentence, the degree of the polynomial depends on two grammar-speciﬁc measures called fan-out and rank. We show that even in the restricted case of canonical grammars, parsing is an NPhard problem. It is important therefore to keep the fan-out and the rank of a grammar as low as possible, and much of the recent work on LCFRSs has been devoted to the development of techniques that optimize parsing complexity in various scenarios ´ ´ Gomez-Rodr´ ıguez and Satta 2009; Gomez-Rodr´ ıguez et al. 2009; Kuhlmann and Satta ´ 2009; Gildea 2010; Gomez-Rodr´ ıguez, Kuhlmann, and Satta 2010; Sagot and Satta 2010; and Crescenzi et al. 2011). In this article we explore the impact of non-projectivity on parsing complexity. In Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS, a measure called block-degree (or gap-degree) (Holan et al. 1998). Although there is no theoretical upper bound on the block-degree of the dependency trees needed for linguistic analysis, we provide evidence from several dependency treebanks showing that, from a practical point of view, th"
J13-2004,J99-4004,0,0.0164174,"g to the premises of the inference rule can be combined into the substrings corresponding to the conclusion by means of the yield function f . Based on the deduction system, a tabular parser for LCFRSs can be implemented using standard dynamic programming techniques. This parser will compute a packed representation of the set of all derivation trees that the grammar G assigns to the string w. Such a packed representation is often called a shared forest (Lang 1994). In combination with appropriate semirings, the shared forest is useful for many tasks in syntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009). 6.2 Parsing Complexity We are interested in an upper bound on the runtime of the tabular parser that we have just presented. We can see that the parser runs in time O(|G||w|c ), where |G |denotes the size of some suitable representation of the grammar G, and c denotes the maximal number of instantiations of an inference rule (cf. McAllester 2002). Let us write c( f ) for the specialization of c to inference rules for productions with yield function f . We refer to this value as the parsing complexity of f (cf. Gildea 2010). Then to show an upper bound on c it sufﬁces to"
J13-2004,P07-1077,0,0.0449045,"Missing"
J13-2004,W98-0503,0,0.350076,"Missing"
J13-2004,P92-1012,0,0.799733,"d by the complexity of constructing the shared forest. Conversely, parsing cannot be faster than universal recognition. In the next three lemmas we prove that the universal recognition problem for canonical lexicalized LCFRSs is NP-complete unless we restrict ourselves to a class of grammars where both the fan-out and the rank of the yield functions are bounded by constants. Lemma 6, which shows that the universal recognition problem of lexicalized LCFRSs is in NP, distinguishes lexicalized LCFRSs from general LCFRSs, for which the universal recognition problem is known to be PSPACE-complete (Kaji et al. 1992). The crucial difference between general and lexicalized LCFRSs is the fact that in the latter, the size of the generated terms is bounded by the length of the input string. Lemma 7 and Lemma 8, which establish two NP-hardness results for lexicalized LCFRSs, are stronger versions of the corresponding results for general LCFRSs presented by Satta (1992), and are proved using similar reductions. They show that the hardness results hold under signiﬁcant restrictions of the formalism: to lexicalized form and to canonical yield functions. Note that, whereas in Section 5.2 we have shown that every l"
J13-2004,E09-1053,1,0.839279,"ntify a class of dependency grammars that can be parsed in polynomial time, it also provides us with a new perspective on the question about the descriptive adequacy of a grammar formalism. This question has traditionally been discussed on the basis of strong and weak generative capacity (Bresnan et al. 1982; Huybregts 1984; Shieber 1985). A notion of generative capacity based on dependency trees makes a useful addition to this discussion, in particular when comparing formalisms for which no common concept of strong generative capacity exists. As an example for a result in this direction, see Koller and Kuhlmann (2009). We have deﬁned the dependency trees that an LCFRS induces by means of a compositional mapping on the derivations. While we would claim that compositionality is a generally desirable property, the particular notion of induction is up for discussion. In particular, our interpretation of derivations may not always be in line with how the grammar producing these derivations is actually used. One formalism for which such a mismatch between derivation trees and dependency trees has been pointed out is treeadjoining grammar (Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998). Resolving"
J13-2004,P06-2066,1,0.644042,"Missing"
J13-2004,E09-1055,1,0.736591,"can be used as dependency grammars. In Section 4 we show how to acquire lexicalized LCFRSs from dependency treebanks. This works in much the same way as the extraction of context-free grammars from phrase structure treebanks (cf. Charniak 1996), except that the derivation trees of dependency trees are not immediately accessible in the treebank. We therefore present an efﬁcient algorithm for computing a canonical derivation tree for an input dependency tree; from this derivation tree, the rules of the grammar can be extracted in a straightforward way. The algorithm was originally published by Kuhlmann and Satta (2009). It produces a restricted type of lexicalized LCFRS that we call “canonical.” In Section 5 we provide a declarative characterization of this class of grammars, and show that every lexicalized LCFRS is (strongly) equivalent to a canonical one, in the sense that it induces the same set of dependency trees. In Section 6 we present a simple parsing algorithm for LCFRSs. Although the runtime of this algorithm is polynomial in the length of the sentence, the degree of the polynomial depends on two grammar-speciﬁc measures called fan-out and rank. We show that even in the restricted case of canonica"
J13-2004,D09-1005,0,0.0167244,"Missing"
J13-2004,W10-4415,0,0.231725,"Missing"
J13-2004,P95-1021,0,0.49982,"Missing"
J13-2004,P10-1054,0,0.0145386,"length of the sentence, the degree of the polynomial depends on two grammar-speciﬁc measures called fan-out and rank. We show that even in the restricted case of canonical grammars, parsing is an NPhard problem. It is important therefore to keep the fan-out and the rank of a grammar as low as possible, and much of the recent work on LCFRSs has been devoted to the development of techniques that optimize parsing complexity in various scenarios ´ ´ Gomez-Rodr´ ıguez and Satta 2009; Gomez-Rodr´ ıguez et al. 2009; Kuhlmann and Satta ´ 2009; Gildea 2010; Gomez-Rodr´ ıguez, Kuhlmann, and Satta 2010; Sagot and Satta 2010; and Crescenzi et al. 2011). In this article we explore the impact of non-projectivity on parsing complexity. In Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS, a measure called block-degree (or gap-degree) (Holan et al. 1998). Although there is no theoretical upper bound on the block-degree of the dependency trees needed for linguistic analysis, we provide evidence from several dependency treebanks showing that, from a practical point of view, this upper bound can be put at a value of as low as 2. In Section 8 we study a second constraint on non-proje"
J13-2004,C88-2121,0,0.534506,"Missing"
J13-2004,P87-1015,0,0.84802,"Missing"
J13-2004,C02-1028,0,0.246529,"Missing"
J13-2004,E06-1010,0,\N,Missing
J13-2004,W06-1517,1,\N,Missing
J13-2004,N10-1035,1,\N,Missing
J13-2004,C96-1058,0,\N,Missing
J13-2004,P96-1025,0,\N,Missing
J13-2004,H05-1049,0,\N,Missing
J13-2004,P04-1054,0,\N,Missing
J13-2004,H05-1066,0,\N,Missing
J13-2004,P05-1034,0,\N,Missing
J13-2004,W06-2922,0,\N,Missing
J13-2004,P05-1013,0,\N,Missing
J13-2004,P07-1021,1,\N,Missing
J13-2004,D07-1096,0,\N,Missing
J13-2004,E09-1034,0,\N,Missing
J15-2002,E03-1036,0,0.74025,"1994), one may restrict backward-crossed composition to instances where X and Y are both verbal categories—that is, functions into the category S of sentences (cf. Steedman 2000, Section 4.2.2). With this restriction the unwanted derivation in Figure 1 can be blocked, and a powerful shot by Rivaldo is still accepted as grammatical. Other syntactic phenomena require other grammar-specific restrictions, including the complete ban of certain combinatory rules (cf. Steedman 2000, Section 4.2.1). Over the past 20 years, CCG has evolved to put more emphasis on supporting fully lexicalized grammars (Baldridge and Kruijff 2003; Steedman and Baldridge 2011), in which as much grammatical information as possible is pushed into the lexicon. This follows the tradition of other frameworks such as Lexicalized Tree-Adjoining Grammar (LTAG) and Head-Driven Phrase Structure Grammar (HPSG). Grammar-specific rule restrictions are not connected to individual lexicon entries, and are therefore avoided. Instead, recent versions of CCG have introduced a new, lexicalized control mechanism in the form of modalities or slash types. The basic idea here is that combinatory rules only apply if the slashes in their input categories have"
J15-2002,T75-2001,0,0.4864,"Missing"
J15-2002,P96-1011,0,0.320562,"Missing"
J15-2002,P10-1055,1,0.827264,"Missing"
J15-2002,J93-4002,0,0.599437,"Missing"
J15-2002,C86-1048,0,0.777914,"Missing"
J15-2002,P88-1034,0,0.797741,"Missing"
J16-4009,W13-0101,0,0.0858078,"d arguably increasing) levels of abstraction over the surface signal and its syntactic structure, viz. (a) Combinatory Categorial Grammar word–word dependencies (CCD); (b) Semantic Dependency Parsing targets from SemEval 2014 and 2015 (SDP); (c) the Elementary Dependency Structures (EDS) of Oepen and Lønning (2006); and (d) Abstract Meaning Representation (AMR; Banarescu et al. 2013). Additional candidate graph banks for inclusion in a community-maintained on-line catalogue are, for example, the Groningen Meaning Bank (GMB; Basile et al. 2012), Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport 2013), as well as combinations of layers of annotations from the Penn Treebank (PTB; Marcus, Santorini, and Marcinkiewicz 1993) and OntoNotes (Hovy et al. 2006) ecosystems. Also, recent work on “deeper” syntax (Ballesteros et al. 2015) and the Universal Dependencies initiative (de Marneffe et al. 2014) push towards increasing use of non-tree structures. There are multiple linguistic and formal differences between these resources. Most importantly, CCD and SDP represent bilexical dependencies, where graph nodes correspond to surface lexical units (words or tokens). In contrast, EDS and AMR take the"
J16-4009,W13-2322,0,0.562349,"er graph banks that are generally available (through the Linguistic Data Consortium) and have already been applied in training and evaluation of data-driven parsers. To capture relevant variation, this selection represents different (and arguably increasing) levels of abstraction over the surface signal and its syntactic structure, viz. (a) Combinatory Categorial Grammar word–word dependencies (CCD); (b) Semantic Dependency Parsing targets from SemEval 2014 and 2015 (SDP); (c) the Elementary Dependency Structures (EDS) of Oepen and Lønning (2006); and (d) Abstract Meaning Representation (AMR; Banarescu et al. 2013). Additional candidate graph banks for inclusion in a community-maintained on-line catalogue are, for example, the Groningen Meaning Bank (GMB; Basile et al. 2012), Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport 2013), as well as combinations of layers of annotations from the Penn Treebank (PTB; Marcus, Santorini, and Marcinkiewicz 1993) and OntoNotes (Hovy et al. 2006) ecosystems. Also, recent work on “deeper” syntax (Ballesteros et al. 2015) and the Universal Dependencies initiative (de Marneffe et al. 2014) push towards increasing use of non-tree structures. There are"
J16-4009,N10-1138,0,0.0187963,"al language parsing traditionally have been trees, in the formal sense that every node is reachable from a distinguished root node by exactly one directed path. With a gradual shift of emphasis from more surfaceoriented, morpho-syntactic target representations in parsing towards “deeper,” more semantic analyses, there is increasing interest in processing structures where characteristic properties of trees like the unique root, connectedness, or lack of reentrancies can be relaxed. Some recent parsing work targets graph-structured representations more general than trees (Sagae and Tsujii 2008, Das et al. 2010, Jones, Goldwater, and Johnson 2013, Flanigan et al. 2014, Martins and Almeida 2014; among others). This development is made possible by ongoing efforts to annotate deeper syntacticosemantic analyses at scale, and typically such annotations either directly take the form of directed graph structures, or can be interpreted as such under moderate transformations. In computational linguistics and in particular in natural language parsing, however, there is less of an established tradition of using general graphs than in, say, theoretical computer science (although the central role of feature stru"
J16-4009,de-marneffe-etal-2014-universal,0,0.056497,"Missing"
J16-4009,P15-1149,0,0.0311655,"Missing"
J16-4009,P14-1134,0,0.441658,"the formal sense that every node is reachable from a distinguished root node by exactly one directed path. With a gradual shift of emphasis from more surfaceoriented, morpho-syntactic target representations in parsing towards “deeper,” more semantic analyses, there is increasing interest in processing structures where characteristic properties of trees like the unique root, connectedness, or lack of reentrancies can be relaxed. Some recent parsing work targets graph-structured representations more general than trees (Sagae and Tsujii 2008, Das et al. 2010, Jones, Goldwater, and Johnson 2013, Flanigan et al. 2014, Martins and Almeida 2014; among others). This development is made possible by ongoing efforts to annotate deeper syntacticosemantic analyses at scale, and typically such annotations either directly take the form of directed graph structures, or can be interpreted as such under moderate transformations. In computational linguistics and in particular in natural language parsing, however, there is less of an established tradition of using general graphs than in, say, theoretical computer science (although the central role of feature structures in unification-based grammar formalisms arguably ma"
J16-4009,P10-1035,0,0.0194341,"actic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank (LDC2005 T13), the strings of the venerable PTB Wall Street Journal (WSJ) corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bilexical dependency triples, which we term CCD. The latter “include most semantically relevant non-anaphoric local and long-range dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. Although CCD has mainly been used for contrastive parser evaluation (Clark and Curran 2007, Fowler and Penn 2010; among others), there is current work that views each set of triples 820 Kuhlmann and Oepen Towards a Catalogue of Linguistic Graph Banks as a directed graph and parses directly into these target representations (Du, Sun, and Wan 2015). SDP 2014 and 2015: DM and PSD. For the SDP tasks at SemEval, Oepen et al. (2014, 2015) prepared aligned sets of semantic dependency graphs over the same WSJ text by reduction (i.e., lossy conversion) of independently developed syntactico-semantic treebanks into bilexical semantic dependencies. SDP (LDC2016 T10) comprises multiple linguistic frameworks, but for"
J16-4009,hajic-etal-2012-announcing,0,0.23683,"Missing"
J16-4009,N06-2015,0,0.0149941,"CD); (b) Semantic Dependency Parsing targets from SemEval 2014 and 2015 (SDP); (c) the Elementary Dependency Structures (EDS) of Oepen and Lønning (2006); and (d) Abstract Meaning Representation (AMR; Banarescu et al. 2013). Additional candidate graph banks for inclusion in a community-maintained on-line catalogue are, for example, the Groningen Meaning Bank (GMB; Basile et al. 2012), Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport 2013), as well as combinations of layers of annotations from the Penn Treebank (PTB; Marcus, Santorini, and Marcinkiewicz 1993) and OntoNotes (Hovy et al. 2006) ecosystems. Also, recent work on “deeper” syntax (Ballesteros et al. 2015) and the Universal Dependencies initiative (de Marneffe et al. 2014) push towards increasing use of non-tree structures. There are multiple linguistic and formal differences between these resources. Most importantly, CCD and SDP represent bilexical dependencies, where graph nodes correspond to surface lexical units (words or tokens). In contrast, EDS and AMR take the form of semantic networks (or conceptual graphs), where nodes represent concepts and there need not be an explicit mapping to surface linguistic forms. In"
J16-4009,W12-3602,1,0.575347,"y into these target representations (Du, Sun, and Wan 2015). SDP 2014 and 2015: DM and PSD. For the SDP tasks at SemEval, Oepen et al. (2014, 2015) prepared aligned sets of semantic dependency graphs over the same WSJ text by reduction (i.e., lossy conversion) of independently developed syntactico-semantic treebanks into bilexical semantic dependencies. SDP (LDC2016 T10) comprises multiple linguistic frameworks, but for our pilot comparison we focus on two sets of target representations that are not derivative of the PTB, viz. (a) DELPH-IN MRS-Derived Dependencies (DM; Oepen and Lønning 2006, Ivanova et al. 2012) and (b) Prague Semantic Dependencies (PSD; Hajiˇc et al. 2012, Miyao, Oepen, and Zeman 2014). Both are rooted in general theories of grammar—Head-Driven Phrase Structure Grammar (Pollard and Sag 1994) and Prague Functional Generative Description (FGD; Sgall, Hajiˇcová, and Panevová 1986), respectively—and there are numerous current reports on parsing into these target representations. Elementary Dependency Structures (EDS). The DM bilexical dependencies originally derive from the underspecified logical forms of Copestake et al. (2005), which Oepen and Lønning (2006), by elimination of scope c"
J16-4009,W13-1810,0,0.245433,"Missing"
J16-4009,Q15-1040,1,0.810581,"Missing"
J16-4009,P06-2066,1,0.360948,"Missing"
J16-4009,P98-1116,0,0.0297687,"Representation (AMR). Unlike the bilexical dependency graphs of CCD, DM, and PSD, AMR eschews explicit syntactic derivations and consideration of the syntax–semantics interface; it rather seeks to directly annotate “whole-sentence logical meanings” (Banarescu et al. 2013). Node labels in AMR name abstract concepts, which in large part draw on the ontology of OntoNotes predicate senses and corresponding semantic roles. Nodes are not overtly related to surface lexical units, and thus are unordered. Although AMR has its roots in semantic networks and earlier knowledge representation approaches (Langkilde and Knight 1998), largerscale manual AMR annotation is a recent development only. We sample two variants of AMR, viz. (a) the graphs as annotated in AMRBank 1.0 (LDC2014 T12), and (b) a normalized version that we call AMR−1 , where so-called “inverse roles” (like ARG0-of) are reversed. Such inverted edges are frequently used in AMR in order to render the graph as a single rooted structure, where the root is interpreted as the top-level focus.1 In Section 3, we map this interpretation to our concept of top nodes for both AMR and AMR−1 . Flanigan et al. (2014) published the first parser targeting AMR, and the s"
J16-4009,J93-2004,0,0.0578282,"Missing"
J16-4009,S14-2082,0,0.0382965,"is reachable from a distinguished root node by exactly one directed path. With a gradual shift of emphasis from more surfaceoriented, morpho-syntactic target representations in parsing towards “deeper,” more semantic analyses, there is increasing interest in processing structures where characteristic properties of trees like the unique root, connectedness, or lack of reentrancies can be relaxed. Some recent parsing work targets graph-structured representations more general than trees (Sagae and Tsujii 2008, Das et al. 2010, Jones, Goldwater, and Johnson 2013, Flanigan et al. 2014, Martins and Almeida 2014; among others). This development is made possible by ongoing efforts to annotate deeper syntacticosemantic analyses at scale, and typically such annotations either directly take the form of directed graph structures, or can be interpreted as such under moderate transformations. In computational linguistics and in particular in natural language parsing, however, there is less of an established tradition of using general graphs than in, say, theoretical computer science (although the central role of feature structures in unification-based grammar formalisms arguably marks an exception to this c"
J16-4009,S14-2056,1,0.928126,"Missing"
J16-4009,S14-2008,1,0.7848,"Missing"
J16-4009,C08-1095,0,0.0552776,"epresentations in natural language parsing traditionally have been trees, in the formal sense that every node is reachable from a distinguished root node by exactly one directed path. With a gradual shift of emphasis from more surfaceoriented, morpho-syntactic target representations in parsing towards “deeper,” more semantic analyses, there is increasing interest in processing structures where characteristic properties of trees like the unique root, connectedness, or lack of reentrancies can be relaxed. Some recent parsing work targets graph-structured representations more general than trees (Sagae and Tsujii 2008, Das et al. 2010, Jones, Goldwater, and Johnson 2013, Flanigan et al. 2014, Martins and Almeida 2014; among others). This development is made possible by ongoing efforts to annotate deeper syntacticosemantic analyses at scale, and typically such annotations either directly take the form of directed graph structures, or can be interpreted as such under moderate transformations. In computational linguistics and in particular in natural language parsing, however, there is less of an established tradition of using general graphs than in, say, theoretical computer science (although the central rol"
J16-4009,S15-1031,0,0.269151,"Missing"
J16-4009,J07-4004,0,\N,Missing
J16-4009,C98-1112,0,\N,Missing
J16-4009,P13-1091,0,\N,Missing
J16-4009,J07-3004,0,\N,Missing
J16-4009,P13-2131,0,\N,Missing
J16-4009,basile-etal-2012-developing,0,\N,Missing
J16-4009,D15-1136,0,\N,Missing
J16-4009,oepen-lonning-2006-discriminant,1,\N,Missing
J18-3004,E03-1036,0,0.0328142,"his article) how specific features of the grammar contribute to the complexity of the parsing task. More precisely, when investigating the universal recognition problem one expresses the computational complexity of parsing in terms of several parameters (other than the input string length), as for instance the number of nonterminals, maximum size of rules, or maximum length of unary derivations. This provides a much more finegrained picture than the one that we obtain when analyzing the membership problem, and discloses the effects that each individual feature of the grammar has on parsing. 1 Baldridge and Kruijff (2003) show that the weak generative power of their formalism for multi-modal CCG is at most as strong as that of TAG, but they do not show that it is at least as strong. 449 Computational Linguistics Volume 44, Number 3 Structure of the Article. The remainder of this article is structured as follows. After presenting the VW-CCG formalism in Section 2, we first study in Section 3 the universal recognition problem for a restricted class of VW-CCG, where each category is “lexicalized” in the sense of the Principle of Adjacency. We show that for this subclass, universal recognition is NP-complete. Unde"
J18-3004,J07-4004,0,0.064094,"kenmaier and Steedman 2007), there has been a surge of interest in CCG within statistical and, more recently, neural natural Submission received: 21 February 2017; revised version received: 4 May 2018; accepted for publication: 18 May 2018. doi:10.1162/COLI_a_00324 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhang and Clark 2011), natural language generation (White, Clark, and Moore 2010; Zhang and Clark 2015), machine translation (Lewis and Steedman 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing a"
J18-3004,N10-1035,1,0.876518,"Missing"
J18-3004,J07-3004,0,0.059077,"and Baldridge 2011) is a wellestablished grammatical framework that has supported a large amount of work both in linguistic analysis and natural language processing. From the perspective of linguistics, the two most prominent features of CCG are its tight coupling of syntactic and semantic information, and its capability to compactly encode this information entirely within the lexicon. Despite the strong lexicalization that characterizes CCG, it is able to handle non-local dependencies in a simple and effective way (Rimell, Clark, and Steedman 2009). After the release of annotated data sets (Hockenmaier and Steedman 2007), there has been a surge of interest in CCG within statistical and, more recently, neural natural Submission received: 21 February 2017; revised version received: 4 May 2018; accepted for publication: 18 May 2018. doi:10.1162/COLI_a_00324 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhan"
J18-3004,P92-1012,1,0.392571,"Missing"
J18-3004,J15-2002,1,0.890049,"Missing"
J18-3004,Q14-1032,1,0.843929,"orithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result that CCG and TAG are weakly equivalent (Weir and Joshi 1988; Vijay-Shanker and Weir 1994). However, although the runtime of Vijay-Shanker and Weir’s algorithm is polynomial in the length of the input sentence, it is exponential in the size of the grammar. This is in contrast with the situation for TAG, where the runtime is (roughly) quadratic with respect to grammar size (Schabes 1990). The only other polynomial-time parsing algorithms for CCG that we are aware of (Vijay-Shanker and Weir 1993; Kuhlmann and Satta 2014) exhibit the same behavior. Kuhlmann and Satta (2014) ask whether parsing may be inherently more complex for CCG than for TAG when grammar size is taken into account. Our main technical result in this article is that the answer to this question is positive: We show that any parsing algorithm for CCG in the formalism considered by Vijay-Shanker and Weir will necessarily take in the worst case exponential time when the size of the grammar is included in the analysis. Formally, we prove that the universal recognition problem for this formalism is EXPTIME-complete. The following paragraphs provide"
J18-3004,D16-1262,0,0.0351114,"Missing"
J18-3004,D13-1064,0,0.0198982,"Missing"
J18-3004,P87-1016,0,0.777533,"Missing"
J18-3004,D09-1085,0,0.052595,"Missing"
J18-3004,P86-1006,0,0.376979,"mmar assigns to a generated sentence, not in the membership of the sentence per se. Therefore, the universal recognition problem is a more accurate model of parsing than the membership problem, as the latter also admits decision procedures where the grammar is replaced with some other mechanism that may produce no or completely different descriptions than the ones we are interested in. The universal recognition problem is also favored when the ambition is to characterize parsing time in terms of all relevant inputs—both the length of the input string and the size and structure of the grammar (Ristad 1986). Such an analysis often reveals (and does so even in this article) how specific features of the grammar contribute to the complexity of the parsing task. More precisely, when investigating the universal recognition problem one expresses the computational complexity of parsing in terms of several parameters (other than the input string length), as for instance the number of nonterminals, maximum size of rules, or maximum length of unary derivations. This provides a much more finegrained picture than the one that we obtain when analyzing the membership problem, and discloses the effects that ea"
J18-3004,P85-1011,0,0.783263,"Missing"
J18-3004,P90-1001,0,0.728604,"ommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhang and Clark 2011), natural language generation (White, Clark, and Moore 2010; Zhang and Clark 2015), machine translation (Lewis and Steedman 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing algorithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result that CCG and TAG are weakly equivalent (Weir and Joshi 1988; Vijay-Shanker and Weir 1994). However, although the runtime of Vijay-Shanker and Weir’s algorithm is polynomial in the length of the input sentence, it is exponential in the size of the grammar. This is in cont"
J18-3004,J93-4002,0,0.5991,"exity of standard parsing algorithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result that CCG and TAG are weakly equivalent (Weir and Joshi 1988; Vijay-Shanker and Weir 1994). However, although the runtime of Vijay-Shanker and Weir’s algorithm is polynomial in the length of the input sentence, it is exponential in the size of the grammar. This is in contrast with the situation for TAG, where the runtime is (roughly) quadratic with respect to grammar size (Schabes 1990). The only other polynomial-time parsing algorithms for CCG that we are aware of (Vijay-Shanker and Weir 1993; Kuhlmann and Satta 2014) exhibit the same behavior. Kuhlmann and Satta (2014) ask whether parsing may be inherently more complex for CCG than for TAG when grammar size is taken into account. Our main technical result in this article is that the answer to this question is positive: We show that any parsing algorithm for CCG in the formalism considered by Vijay-Shanker and Weir will necessarily take in the worst case exponential time when the size of the grammar is included in the analysis. Formally, we prove that the universal recognition problem for this formalism is EXPTIME-complete. The fo"
J18-3004,P87-1015,0,0.822758,"Missing"
J18-3004,P88-1034,0,0.625578,"man 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing algorithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result that CCG and TAG are weakly equivalent (Weir and Joshi 1988; Vijay-Shanker and Weir 1994). However, although the runtime of Vijay-Shanker and Weir’s algorithm is polynomial in the length of the input sentence, it is exponential in the size of the grammar. This is in contrast with the situation for TAG, where the runtime is (roughly) quadratic with respect to grammar size (Schabes 1990). The only other polynomial-time parsing algorithms for CCG that we are aware of (Vijay-Shanker and Weir 1993; Kuhlmann and Satta 2014) exhibit the same behavior. Kuhlmann and Satta (2014) ask whether parsing may be inherently more complex for CCG than for TAG when gramm"
J18-3004,J10-2001,0,0.0286866,"Missing"
J18-3004,P11-1069,0,0.0220947,"007), there has been a surge of interest in CCG within statistical and, more recently, neural natural Submission received: 21 February 2017; revised version received: 4 May 2018; accepted for publication: 18 May 2018. doi:10.1162/COLI_a_00324 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhang and Clark 2011), natural language generation (White, Clark, and Moore 2010; Zhang and Clark 2015), machine translation (Lewis and Steedman 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing algorithms for Tree Adjo"
J18-3004,J15-3005,0,0.0229674,"tly, neural natural Submission received: 21 February 2017; revised version received: 4 May 2018; accepted for publication: 18 May 2018. doi:10.1162/COLI_a_00324 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhang and Clark 2011), natural language generation (White, Clark, and Moore 2010; Zhang and Clark 2015), machine translation (Lewis and Steedman 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing algorithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result th"
J18-3004,D14-1107,0,\N,Missing
K19-2001,W13-0101,1,0.863351,"or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior (or ‘phrasal’) graph nodes are formally unanchored. The UCCA graph for the running example (see the bottom of Figure 2) includes a single scene, whose main relation is the Process (P) evoked by apply. It also contains a secondary relation labeled Adverbial (D), almost impossible, which is broken Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used 5 ing is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such t"
K19-2001,D15-1198,0,0.0255627,"9 parsers also diverge in terms of their assumptions regarding the syntax–semantics interface, some parsing raw text directly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating"
K19-2001,K19-2008,0,0.403223,"niversity of Oslo, Department of Informatics The Hebrew University of Jerusalem, School of Computer Science and Engineering Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics ♦ University of Copenhagen, Department of Computer Science ◦ Link¨oping University, Department of Computer and Information Science ? University of Colorado at Boulder, Department of Linguistics • Brandeis University, Department of Computer Science ♠ ♥ mrp-organizers@nlpl.eu , jchun@brandeis.edu , {straka |uresova}@ufal.mff.cuni.cz Abstract Representation Parsing (MRP 2019). The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in"
K19-2001,D17-1130,0,0.100948,"Missing"
K19-2001,W13-2322,0,0.179348,"aphs need not be rooted trees: Argument sharing across units will give rise to reentrant nodes much like in the other frameworks. For example, technique in Figure 2 is both a Participant in the scene evoked by similar and a Center in the parent unit. UCCA in principle also supports implicit (unexpressed) units which do not correspond to any tokens, but these are currently excluded from parsing evaluation and, thus, suppressed in the UCCA graphs distributed in the context of the shared task. Abstract Meaning Representation Finally, the shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although most AMR parsing research presupposes a pre-processing step that ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchor6 DM PSD EDS UCCA AMR Flavor 0 0 1 1 2 TRAIN Text Type Sentences Tokens newspaper 35,656"
K19-2001,S16-1176,0,0.0127804,"ition-based, as well as sequence-to-sequence systems. Existing 19 parsers also diverge in terms of their assumptions regarding the syntax–semantics interface, some parsing raw text directly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target"
K19-2001,basile-etal-2012-developing,0,0.0638169,"Missing"
K19-2001,W15-0128,1,0.854139,"ing example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: DELPH-IN MRS Bi-Lexical Dependencies (DM; top) and Prague Semantic Dependencies (PSD; bottom). DELPH-IN MRS Bi-Lexical Dependencies The DM bi-lexical dependencies (Ivanova et al., 2012) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015). The underlying grammar is rooted in the general linguistic theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Ivanova et al. (2012) propose a two-stage conversion from ERS into bi-lexical semantic dependency graphs, where ERS logical forms are first recast as Elementary Dependency Structures (EDS; Oepen and Lønning, 2006; see below) and then further simplified into pure bi-lexical semantic dependencies, dubbed DELPH-IN MRS Bi-Lexical Dependencies (or DM). As a Flavor (0) framework, graph nodes in DM are restricted to surface tokens. But DM graphs are neither lexica"
K19-2001,D16-1134,1,0.82449,"y Dependency Structures (EDS; top) and Universal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior"
K19-2001,P17-1112,0,0.291422,"vich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014), who performed alignment a"
K19-2001,P13-2131,0,0.502571,"range of the corresponding sub-strings (rather than by token indices, which would not be robust to tokenization mis-matches). In the Flavor (1) graphs (EDS and UCCA), multiple distinct nodes can have overlapping or even identical anchors; in EDS, for example, the semantics of an adverb like today is decomposed into four nodes, all anchored to the same substring: Evaluation For each of the individual frameworks, there are established ways of evaluating the quality of parser outputs in terms of graph similarity to goldstandard target representations called EDM (Dridan and Oepen, 2011), SMATCH (Cai and Knight, 2013), SDP (Oepen et al., 2014), and UCCA (Hershcovich et al., 2019). There is broad similarity between the framework-specific evaluation metrics used to date, but also some subtle differences. Meaning representation parsing is commonly evaluated in terms of a graph similarity F1 score at implicit q x : time n(x) ∧ today a 1(x) ∧ temp loc(e, x) . The standard EDS and UCCA evaluation metrics determine node identities through anchors (and 6 In principle, one could further view unlabeled edges and their labels as two distinct pieces of information, but the task design shies away from such formal purit"
K19-2001,K19-2006,0,0.10424,"Missing"
K19-2001,K19-2013,0,0.128351,"Missing"
K19-2001,W11-2927,1,0.84568,"quely determined as the character range of the corresponding sub-strings (rather than by token indices, which would not be robust to tokenization mis-matches). In the Flavor (1) graphs (EDS and UCCA), multiple distinct nodes can have overlapping or even identical anchors; in EDS, for example, the semantics of an adverb like today is decomposed into four nodes, all anchored to the same substring: Evaluation For each of the individual frameworks, there are established ways of evaluating the quality of parser outputs in terms of graph similarity to goldstandard target representations called EDM (Dridan and Oepen, 2011), SMATCH (Cai and Knight, 2013), SDP (Oepen et al., 2014), and UCCA (Hershcovich et al., 2019). There is broad similarity between the framework-specific evaluation metrics used to date, but also some subtle differences. Meaning representation parsing is commonly evaluated in terms of a graph similarity F1 score at implicit q x : time n(x) ∧ today a 1(x) ∧ temp loc(e, x) . The standard EDS and UCCA evaluation metrics determine node identities through anchors (and 6 In principle, one could further view unlabeled edges and their labels as two distinct pieces of information, but the task design sh"
K19-2001,P12-2074,1,0.629774,"and UD labeled dependency trees. We then trained the currently best-performing UDPipe architecture (Straka, 2018; Straka et al., 2019), which implements a joint part-of-speech tagger, lemmatizer, and dependency parser employing contextualized BERT embeddings. To avoid overlap of morpho-syntactic training data with the texts underlying the semantic graphs of the shared task, we performed five-fold jack-knifing on the WSJ and EWT corpora. For compatibility with the majority of the training data, the ‘raw’ input strings for the MRP semantic graphs were tokenized using the PTB-style REPP rules of Dridan and Oepen (2012) and input to UDPipe in pre-tokenized form. Whether as merely a source of state-of-the-art PTBstyle tokenization, or as a vantage point for approaches to meaning representation parsing that start from explicit syntactic structure, the optional morpho-syntactic companion data offers community value in its own right. respectively; see Table 2. The shared task has, for the first time, repackaged the five graph banks into a uniform and normalized abstract representation with a common serialization format. The common interchange format for semantic graphs implements the abstract model of Kuhlmann a"
K19-2001,K19-2007,0,0.24925,"niversity of Oslo, Department of Informatics The Hebrew University of Jerusalem, School of Computer Science and Engineering Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics ♦ University of Copenhagen, Department of Computer Science ◦ Link¨oping University, Department of Computer and Information Science ? University of Colorado at Boulder, Department of Linguistics • Brandeis University, Department of Computer Science ♠ ♥ mrp-organizers@nlpl.eu , jchun@brandeis.edu , {straka |uresova}@ufal.mff.cuni.cz Abstract Representation Parsing (MRP 2019). The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in"
K19-2001,K19-2015,0,0.0373872,"Missing"
K19-2001,P18-1038,0,0.0751759,"recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014), who performed alignment as a preprocessing step during training. They developed their own rule-based alignment method, complemented by Pourdamghani et al. (2014),"
K19-2001,K19-2016,0,0.133819,"Missing"
K19-2001,S14-2080,0,0.0289716,"lti-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text and meaning representations, outperforming the previous best results"
K19-2001,S15-2154,0,0.216771,"Missing"
K19-2001,N18-2020,1,0.780781,"iversal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior (or ‘phrasal’) graph nodes are formally unanch"
K19-2001,S16-1186,0,0.0968157,"Missing"
K19-2001,N18-1104,0,0.0451061,"iding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating alignment as a latent variable with a copy mechanism. Their parser additionally supports UCCA and SDP, and establishes the stateof-the-art in AMR parsing, though without using multi-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by usi"
K19-2001,P14-1134,0,0.12208,"of the scores of its subgraphs. Factorization-Based Architectures These parsing models for meaning representation also have their roots in syntactic dependency parsing (where they are often called graph-based; McDonald and Pereira, 2006). Given a set of nodes, the basic idea of the factorization-based approach is to find the graph that has the highest score among all possible graphs. In the case of dependency parsing, the goal is to find the Maximum Spanning Tree, and this has been extended to meaning representation parsing, where the goal is to find the Maximum Spanning Connected Subgraphs (Flanigan et al., 2014). To make the computation of the score of a graph practical, the typical strategy is to factorize the score of a graph into the sum of the scores of its subgraphs, and in the case of first-order factorization, into the sum of the scores of its nodes and edges. A popular choice for predicting the edge is to feed the output of an LSTM encoder to a biaffine classifier to predict if an edge exists between a pair of nodes as well as the label of the edge (SJTU– NICT, SUDA–Alibaba, Hitachi, and JBNU), with slight variations as to the input to the LSTM encoder. Due to the difference in anchoring betw"
K19-2001,S15-2161,0,0.0179913,"and establishes the stateof-the-art in AMR parsing, though without using multi-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text"
K19-2001,kingsbury-palmer-2002-treebank,0,0.377337,"iyadarshi, 2017). The AMR example graph in Figure 3 has a topology broadly comparable to EDS, with some notable differences. Similar to the UCCA example graph (and unlike EDS), the AMR representation of the coordinate structure is flat. Although most lemmas are linked to derivationally related forms in the sense lexicon, this is not universal, as seen by the nodes corresponding to similar and such as, which are labeled as resemble-01 and exemplify-01, respectively. These sense distinctions (primarily for verbal predicates) are grounded in the inventory of predicates from the PropBank lexicon (Kingsbury and Palmer, 2002; Hovy et al., 2006). Role labels in AMR encode semantic argument positions, with the particular roles defined according to each PropBank sense, though the counting in AMR is zero-based such that the ARG1 and ARG2 roles in Figure 3 often correspond to ARG2 and ARG3, respectively, in the EDS of Figure 2. PropBank distinguishes such numbered arguments from non-core roles labeled from a general semantic inventory, such as frequency, duration, or domain. Figure 3 also shows the use of inverted edges in AMR, for example ARG1-of and mod. These serve to allow annotators (and in principle also parsing"
K19-2001,hajic-etal-2012-announcing,0,0.575983,"Missing"
K19-2001,P19-4002,1,0.832307,"listed. 12 tial submissions declined the invitation to submit a system description for publication in the shared task proceedings (and one team asked to remain anonymous), such that only limited information is available about these parsers, and they will not be considered in further detail in §7. Finally, based on input by task participants, Table 4 also provides an indication of which submissions employed multi-task learning (MTL) and a high-level characterization of the overall parsing approach. The distinction between transition-, factorization-, and composition-based architectures follows Koller et al. (2019) and is discussed in more detail in §7 below. In some submissions there can of course be elements of more than one of these high-level architecture types. Also, not all of the teams who indicate the use of multi-task learning actually apply it across different semantic graph frameworks, but in some cases rather to multiple sub-tasks within the parsing architecture for a single framework.11 The main task results are summarized in Table 6, showing average MRP scores across frameworks, broken down by the different component pieces (see §5 above). These cross-framework averages can only be meaning"
K19-2001,P17-1104,1,0.901739,"e, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the nodes or edges, such as L A BEL in the version of TUPA used in the shared task. CUHK developed a transition-based parser with a general transition system suited for all five frameworks, by including a variable-arity R ESOLVE action. F"
K19-2001,P17-1014,0,0.0341005,"ly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating alignment as a latent variable with a copy mechanism. Their parser additionally supports UCCA and SDP, and establishes the s"
K19-2001,P18-1035,1,0.82103,"rticipants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; or Stanovsky and Dagan, 2018). The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks. Five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the training and evaluation data for the task, packaged in a uniform graph abstraction and serialization. The task received submissions from eighteen teams, of which five do not participate in the official ranking because they arrived after the closing deadline, made use of extra training data,"
K19-2001,K19-2011,0,0.145598,"3 3 3 3 7 3 3 3 3 3 3 3 7 7 3 3 7 7 3 3 3 3 3 3 7 7 3 3 7 3 7 ´ UFAL MRPipe§ ∦ Peking § ´ UFAL–Oslo CUHK§ Anonymous§ Peking∦§ 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 7 3 ∦§† Approach Reference 7 7 3 Composition Transition Transition Oepen and Flickinger (2019) Hershcovich and Arviv (2019) Hershcovich and Arviv (2019) 3 3 3 3 3 3 3 3 7 3 7 7 3 7 7 (3) 7 (3) 7 7 7 7 3 7 ? ? Transition Factorization Factorization Composition Factorization Transition Factorization Factorization Factorization Transition Transition Che et al. (2019) Li et al. (2019) Zhang et al. (2019c) Donatelli et al. (2019) Koreeda et al. (2019) Straka and Strakov´a (2019) Wang et al. (2019) Cao et al. (2019) Na et al. (2019) Bai and Zhao (2019) Droganova et al. (2019) 3 7 3 3 7 7 7 7 7 3 ? 7 Transition Factorization Transition Transition Straka and Strakov´a (2019) Chen et al. (2019) Droganova et al. (2019) Lai et al. (2019) Composition Chen et al. (2019) Table 4: Overview of participating teams. The top and bottom blocks represent ‘unofficial’ submissions, which are not considered for the primary ranking because they used training data beyond the white-listed resources (indicated by the symbol “∦”), arrived after the closing deadli"
K19-2001,S19-2001,1,0.68669,"ng representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-level analysis beyond surfac"
K19-2001,J16-4009,1,0.942174,"ng (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning 1 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 1–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2001 tations across frameworks, the MRP 2019 shared task is regrettably limited to parsing English for the time being. 2 A natural generalization of the noncrossing property, where one is allowed to also use the halfplane below the sentence for drawing edges is a property called pagenumber two. Kuhlmann and Oepen (2016) provide additional definitions and a quantitative summary of various formal graph properties across frameworks. Definitions: Graphs and Flavors Reflecting different traditions and communities, there is wide variation in how individual meaning representation frameworks think (and talk) about semantic graphs, down to the level of visual conventions used in rendering graph structures. The following paragraphs provide semi-formal definitions of core graph-theoretic concepts that can be meaningfully applied across the range of frameworks represented in the shared task. Hierarchy of Formal Flavors"
K19-2001,K19-2002,1,0.70568,"ing parser outputs for all target frameworks. Albeit not the ultimate goal of the cross-framework shared task design, such partiality was explicitly allowed to lower the technical barrier to entry and make it possible to include framework-specific parsers in the comparison. Seven (of thirteen) of the official submissions, as well as the two TUPA baselines, provide semantic graphs for all five frameworks. Three highly par. ? ! : ;,“&quot;”‘&apos;’()[]{} 6 Submissions and Results The task received submissions from sixteen teams, plus another two ‘reference’ submissions prepared by the task co-organizers (Hershcovich and Arviv, 2019; Oepen and Flickinger, 2019). These reference points are not considered in the overall ranking. Non-reference submissions are further subdivided into ‘official’ and ‘unofficial’ ones, where the latter are characterized by either arriving after the closing deadline of the evaluation period or using training data beyond the official resources provided (and white-listed) for the task; see §4 above. Table 4 provides an inventory of participating teams, where the top block corresponds to reference submissions from the co-organizers, and 10 In the case of the factorization-based Peking submission,"
K19-2001,K19-2010,0,0.116608,"Missing"
K19-2001,K19-2004,0,0.0682072,"Missing"
K19-2001,W12-3602,1,0.783003,"NNS n rice NN n ADDR.m ACT ADDR.m PAT RSTR technique NN as IN p conj ADDR.m PAT similar JJ ARG2 ADDR.m EXT be almost VBZ RB ev-w218f2 impossible JJ RSTR apply other VB JJ ev-w119f2 CONJ.m APPS.m crop NNS as IN APPS.m cotton NN CONJ.m soybean NNS CONJ.m and CC rice NN Figure 1: Bi-lexical semantic dependencies for the running example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: DELPH-IN MRS Bi-Lexical Dependencies (DM; top) and Prague Semantic Dependencies (PSD; bottom). DELPH-IN MRS Bi-Lexical Dependencies The DM bi-lexical dependencies (Ivanova et al., 2012) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015). The underlying grammar is rooted in the general linguistic theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Ivanova et al. (2012) propose a two-stage conversion from ERS into bi-lexical semantic dependency graphs, where ERS logical forms are first"
K19-2001,W19-3304,1,0.80829,"dicated in each cell: ShanghaiTech, for example, ranks much higher in the framework-specific SDP metric than in the official MRP ranks. These divergences likely reflect the more limited scope of the SDP approach to scoring, which essentially only considers labeled edges (and top nodes, as a pseudo-edge) but ignores node labels, properties, and anchors (which all used to be provided as part of the parser inputs in the original SDP parsing tasks; see §3 above). trastive, phenomena-oriented studies would likely be called for, as for example the comparison of parsing accuracies for EDS vs. AMR by Lin and Xue (2019). 7 Overview of Approaches The participating systems in the shared task have approached this multi-meaning representation task in a variety of ways, which we characterize into three broad families of approaches: transition-, factorization-, or composition-based architectures. Transition-Based Architectures In these parsing system, the meaning representation graph is generated via a series of actions, in a process that is very similar to dependency tree parsing, with the difference being that the actions for graph parsing need to allow reentrancies, as well as (possibly) non-token nodes, labels"
K19-2001,S19-2002,0,0.0756992,"Missing"
K19-2001,P19-1450,0,0.171664,"used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text and meaning representations, outperforming the previous best results on PSD. Lindemann et al. (2019) trained a composition-based parser on DM, PAS, PSD, AMR and EDS, using the Apply–Modify algebra, on which the Saarland submission to the shared task is based. They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. UCCA parsing was first tackled by Hershcovich et al. (2017), who used a neural transition-based parser. Hershcovich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared t"
K19-2001,S17-2156,0,0.033905,"Missing"
K19-2001,P18-1037,0,0.260682,"le anchoring in the parsing system has a significant impact on parser performance. Some of the participating systems follow early approaches in AMR parsing and use a separate ‘alignment’ model to provide hard anchorings and then proceed with the rest of the parsing process (e.g. the HIT-SCIR system) assuming the alignments are already in place. Other submissions use a soft alignment component that is trained jointly with other components of their systems. For example, the Amazon and the SUDA– Alibaba parsers jointly model anchoring, node detection, and edge detection, adopting the approach of Lyu and Titov (2018), while the SJTU–NICT system uses a sequence-to-sequence model with a pointer-generator network to predict the concepts in AMR, following Zhang et al. (2019a). That sequence-to-sequence model is trained jointly with other components of their system. Benefits of Multi-Task Learning Another research question the shared task seeks to advance is whether and how multi-task learning (MTL) helps with multi-framework meaning representation parsing. The term, in fact, seems to be applied somewhat variably in the system descriptions. In one sense, it is equated with traditional joint learning, where dif"
K19-2001,W03-3017,0,0.384782,"t and a buffer for yet-to-be processed elements, needs to be maintained. Which action to take next is predicted by a classifier using a representation of the parser state as input. When this parsing procedure is complete, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the n"
K19-2001,J93-2004,0,0.0674913,"DM are restricted to surface tokens. But DM graphs are neither lexically fully covering nor rooted trees, i.e. some tokens do not contribute to the graph, and for some nodes there are multiple incoming edges. In the example DM graph in Figure 1, technique semantically depends on the determiner (the quantificational locus), the modifier similar, and the predicate apply. Conversely, the predicative copula, infinitival to, and the vacusection reviews the frameworks and presents example graphs for sentence #20209013 from the venerable Wall Street Journal (WSJ) Corpus from the Penn Treebank (PTB; Marcus et al., 1993): (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. The example exhibits some interesting linguistic complexity, including what is called a tough adjective (impossible), a scopal adverb (almost), a tripartite coordinate structure, and apposition. The example graphs in Figures 1 through 3 are presented in order of (arguably) increasing ‘abstraction’ from the surface string, i.e. ranging from ordered Flavor (0) to unanchored Flavor (2). Two of the frameworks in the shared task present simplifications into bi-lexical semantic dependencies (i."
K19-2001,S16-1166,0,0.537725,"‘balkanization’ in the field of meaning representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate targe"
K19-2001,W17-7306,0,0.0913721,"Missing"
K19-2001,S17-2090,0,0.515681,"tion’ in the field of meaning representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-l"
K19-2001,K19-2003,1,0.784391,"rget frameworks. Albeit not the ultimate goal of the cross-framework shared task design, such partiality was explicitly allowed to lower the technical barrier to entry and make it possible to include framework-specific parsers in the comparison. Seven (of thirteen) of the official submissions, as well as the two TUPA baselines, provide semantic graphs for all five frameworks. Three highly par. ? ! : ;,“&quot;”‘&apos;’()[]{} 6 Submissions and Results The task received submissions from sixteen teams, plus another two ‘reference’ submissions prepared by the task co-organizers (Hershcovich and Arviv, 2019; Oepen and Flickinger, 2019). These reference points are not considered in the overall ranking. Non-reference submissions are further subdivided into ‘official’ and ‘unofficial’ ones, where the latter are characterized by either arriving after the closing deadline of the evaluation period or using training data beyond the official resources provided (and white-listed) for the task; see §4 above. Table 4 provides an inventory of participating teams, where the top block corresponds to reference submissions from the co-organizers, and 10 In the case of the factorization-based Peking submission, the extra training data is li"
K19-2001,P13-2017,0,0.074515,"Missing"
K19-2001,S15-2153,1,0.900924,"Missing"
K19-2001,E06-1011,0,0.0181086,"of the Saarland parser, the lexical items are produced by a BiLSTM-based supertagger, and the best derivation is selected in a tree dependency parsing process where the edge between a head and its argument or modifier is labeled with the derivation operation. In the case of the Peking system, the SHRG rules are extracted with a context-free parser, and the derivation is scored by a sum of the scores of its subgraphs. Factorization-Based Architectures These parsing models for meaning representation also have their roots in syntactic dependency parsing (where they are often called graph-based; McDonald and Pereira, 2006). Given a set of nodes, the basic idea of the factorization-based approach is to find the graph that has the highest score among all possible graphs. In the case of dependency parsing, the goal is to find the Maximum Spanning Tree, and this has been extended to meaning representation parsing, where the goal is to find the Maximum Spanning Connected Subgraphs (Flanigan et al., 2014). To make the computation of the score of a graph practical, the typical strategy is to factorize the score of a graph into the sum of the scores of its subgraphs, and in the case of first-order factorization, into t"
K19-2001,S14-2008,1,0.915565,"Missing"
K19-2001,S14-2056,1,0.708772,"g covert quantifiers (e.g. on bare nominals, labeled udef q3 ), the two-place such+as p relation, as well as the implicit conj(unction) relation (which reflects recursive decomposition of the coordinate structure Prague Semantic Dependencies Another instance of simplification from richer syntacticosemantic representations into Flavor (0) bi-lexical semantic dependencies is the reduction of tectogrammatical trees (or t-trees) from the linguistic school of Functional Generative Description (FGD; Sgall et al., 1986; Hajiˇc et al., 2012) into what are called Prague Semantic Dependencies (or PSD). Miyao et al. (2014) sketch the nature of this conversion, which essentially collapses empty (or generated, in FGD terminology) t-tree nodes with corresponding surface nodes and forward-projects incoming dependencies onto all members of paratactic constructions, e.g. the appositive and coordinate structures in the bottom of Figure 1. The PSD graph for our running example has many of the same dependency edges as the DM one (albeit using a different labeling scheme and inverse directionality in a few cases), but it analyzes the predicative copula as semantically contentful and does not treat almost as ‘scoping’ ove"
K19-2001,K19-2009,0,0.0353841,"Missing"
K19-2001,N12-2006,0,0.0317062,"parser. Hershcovich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014),"
K19-2001,P18-1173,0,0.0339442,"Missing"
K19-2001,N18-1135,0,0.164463,"Missing"
K19-2001,K19-2012,1,0.876098,"Missing"
K19-2001,W15-3502,1,0.870363,"y apply. It also contains a secondary relation labeled Adverbial (D), almost impossible, which is broken Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used 5 ing is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 3 has a topology broa"
K19-2001,D14-1048,0,0.0260749,"quences of ‘raw’ sentence strings and (b) in pretokenized, part-of-speech–tagged, lemmatized, and syntactically parsed form. For the latter, premiumquality English morpho-syntactic analyses were provided to participants, described in more detail below. These parser outputs are referred to as the MRP 2019 morpho-syntactic companion trees. Additional companion data available to participants includes automatically generated reference anchorings (commonly called ‘alignments’ in AMR parsing) for the AMR graphs in the training data, obtained from the JAMR and ISI tools of Flanigan et al. (2016) and Pourdamghani et al. (2014), respectively. Because some of the semantic graph banks involved in the shared task had originally been released by the Linguistic Data Consortium (LDC), the training data was made available to task participants by the LDC under no-cost evaluation licenses. Upon completion of the competition, all task data (including system submissions and evaluation results) are being prepared for general release through the LDC, while those subsets that are copyright-free will also become available for direct, open-source download. Additional Resources For reasons of comparability and fairness, the shared t"
K19-2001,P18-1016,1,0.787871,"hnique is almost impossible to apply to other crops, such as cotton, soybeans and rice: Elementary Dependency Structures (EDS; top) and Universal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of"
K19-2001,L16-1376,0,0.145661,"Missing"
K19-2001,I05-2038,0,0.0146077,"he full novel have long served as a common reference point for AMR, and gold-standard DM and EDS graphs could be converted from the ERS inter-annotator agreement study by Bender et al. (2015). For PSD and UCCA, the 100-sentence subset used for MRP evaluation has been annotated specifically for the shared task. 5 See http://svn.nlpl.eu/mrp/2019/public/ resources.txt for the full list of seventeen generally available third-party resources, including a broad range of large English corpora and distributed word representations. 8 Corpus, as well as to the PTB-style annotations of the GENIA Corpus (Tateisi et al., 2005). This conversion targets Universal Dependencies (UD; McDonald et al., 2013; Nivre, 2015) version 2.x, so that the resulting gold-standard annotations could be concatenated with the UD English Web Treebank (Silveira et al., 2014), for a total of 2.2 million tokens annotated with lemmas, Universal and PTBstyle parts of speech, and UD labeled dependency trees. We then trained the currently best-performing UDPipe architecture (Straka, 2018; Straka et al., 2019), which implements a joint part-of-speech tagger, lemmatizer, and dependency parser employing contextualized BERT embeddings. To avoid ove"
K19-2001,silveira-etal-2014-gold,0,0.0451563,"Missing"
K19-2001,N18-2040,1,0.872943,"Missing"
K19-2001,P19-1446,0,0.0232282,"Missing"
K19-2001,D17-1129,1,0.913091,"Missing"
K19-2001,N15-1040,1,0.925482,"Missing"
K19-2001,K19-2005,0,0.0368897,"Missing"
K19-2001,W03-3023,0,0.0220925,"r for yet-to-be processed elements, needs to be maintained. Which action to take next is predicted by a classifier using a representation of the parser state as input. When this parsing procedure is complete, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the nodes or edges, such as L A BE"
K19-2001,P19-1009,0,0.3054,"Missing"
K19-2001,D19-1392,0,0.220016,"Missing"
K19-2001,K19-2014,0,0.380663,"rov 3 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 3 3 7 7 3 3 7 7 3 3 3 3 3 3 7 7 3 3 7 3 7 ´ UFAL MRPipe§ ∦ Peking § ´ UFAL–Oslo CUHK§ Anonymous§ Peking∦§ 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 7 3 ∦§† Approach Reference 7 7 3 Composition Transition Transition Oepen and Flickinger (2019) Hershcovich and Arviv (2019) Hershcovich and Arviv (2019) 3 3 3 3 3 3 3 3 7 3 7 7 3 7 7 (3) 7 (3) 7 7 7 7 3 7 ? ? Transition Factorization Factorization Composition Factorization Transition Factorization Factorization Factorization Transition Transition Che et al. (2019) Li et al. (2019) Zhang et al. (2019c) Donatelli et al. (2019) Koreeda et al. (2019) Straka and Strakov´a (2019) Wang et al. (2019) Cao et al. (2019) Na et al. (2019) Bai and Zhao (2019) Droganova et al. (2019) 3 7 3 3 7 7 7 7 7 3 ? 7 Transition Factorization Transition Transition Straka and Strakov´a (2019) Chen et al. (2019) Droganova et al. (2019) Lai et al. (2019) Composition Chen et al. (2019) Table 4: Overview of participating teams. The top and bottom blocks represent ‘unofficial’ submissions, which are not considered for the primary ranking because they used training data beyond the white-listed resources (indicated by t"
L16-1630,D11-1031,0,0.0192561,"linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where subst"
L16-1630,W13-2322,0,0.201241,"ns as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer t"
L16-1630,W08-2222,0,0.0201912,"and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collec"
L16-1630,Q15-1040,1,0.86691,"banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was perf"
L16-1630,J93-2004,0,0.0550061,"ies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “includ"
L16-1630,S14-2082,0,0.0694129,"information. We envision that general availability of a standardized and comprehensive set of semantic dependency graphs and associated tools will stimulate more research in this sub-area of semantic parsing. To date, reported ‘parsing success’ 7 To seek to relate these different approaches to the encoding of lexical valency, one can multiply out the DM frame identifiers with verb lemmata, which yields a count of some 4,600 distinct combinations, i.e. slightly less than the set of observed sense distinctions in PSD. measures in terms of dependency F1 range between the high seventies for PSD (Martins & Almeida, 2014) and high eighties to low nineties for CCD, DM, and PAS (Du et al., 2015; Miyao et al., 2014). Such variation may in principle be owed to differences in the number and complexity of linguistic distinctions made, to homogeneity and consistency of training and test data, and of course to the cumulative effort that has gone into pushing the state of the art on individual target representations. A deeper understanding of these parameters, as well as of contentful vs. superficial linguistic differences across frameworks, will be a prerequisite to judging the relative suitability of different resour"
L16-1630,J07-4004,0,0.0382917,"bank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannot"
L16-1630,copestake-flickinger-2000-open,1,0.723309,"al trees), and with corresponding ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic propert"
L16-1630,S14-2056,1,0.955729,"s). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Jo"
L16-1630,P15-1149,0,0.063476,"s of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional"
L16-1630,flickinger-etal-2014-towards,1,0.859802,"ing ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks a"
L16-1630,P10-1035,0,0.0195635,"n of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBan"
L16-1630,hajic-etal-2012-announcing,1,0.917423,"Missing"
L16-1630,S15-2153,1,0.925074,"Missing"
L16-1630,S14-2008,1,0.928327,"Missing"
L16-1630,oepen-lonning-2006-discriminant,1,0.921801,"rivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). To"
L16-1630,J07-3004,0,0.0404021,"hon tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Tree"
L16-1630,W12-3602,1,0.888177,"of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). Top nodes in this representat"
N09-1061,N07-1019,0,0.20138,"Missing"
N09-1061,E09-1034,1,0.84924,"Missing"
N09-1061,C92-2066,0,0.278857,"valent to LCFRS that has been introduced for syntax-based machine translation. However, the grammar produced by our algorithm has optimal (minimal) fan-out. This is an important improvement over the result in (Melamed et al., 2004), as this quantity enters into the parsing complexity of both multitext grammars and LCFRS as an exponential factor, and therefore must be kept as low as possible to ensure practically viable parsing. Rank reduction is also investigated in Nesson et al. (2008) for synchronous tree-adjoining grammars, a synchronous rewriting formalism based on tree-adjoining grammars Joshi and Schabes (1992). In this case the search space of possible reductions is strongly restricted by the tree structures specified by the formalism, resulting in simplified computation for the reduction algorithms. This feature is not present in the case of LCFRS. There is a close parallel between the technique used in the M INIMAL -B INARIZATION algorithm and deductive parsing techniques as proposed by Shieber et al. (1995), that are usually implemented by means of tabular methods. The idea of exploiting tabular parsing in production factorization was first expressed in Zhang et al. (2006). In fact, the 546 part"
N09-1061,P06-2066,1,0.79325,"th a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS was originally introduced as a generalization of several so-called mildly context-sensitive grammar formalisms. In the context of machine translation, LCFRS is an interesting generalization of SCFG because it does not restrict the fan-out to 2, allowing productions with arbitrary fan-out (and arbitrary rank). Given an LCFRS, our algorithm computes a strongly equivalent grammar with rank 2 and minHuman Langua"
N09-1061,E09-1055,1,0.585799,"re particularly relevant to this paper. 5.1 The tradeoff between rank and fan-out The algorithm introduced in this paper can be used to transform an LCFRS into an equivalent form with rank 2. This will result into a more efficiently parsable LCFRS, since rank exponentially affects parsing complexity. However, we must take into account that parsing complexity is also influenced by fan-out. Our algorithm guarantees a minimal increase in fan-out. In practical cases it seems such an increase is quite small. For example, in the context of dependency parsing, both G´omezRodr´ıguez et al. (2009) and Kuhlmann and Satta (2009) show that all the structures in several wellknown non-projective dependency treebanks are binarizable without any increase in their fan-out. More in general, it has been shown by Seki et al. (1991) that parsing of LCFRS can be carried out in time O(n|pM |), where n is the length of the input string and pM is the production in the grammar with largest size.3 Thus, there may be cases in which one has to find an optimal tradeoff between rank and fanout, in order to minimize the size of pM . This requires some kind of Viterbi search over the space of all possible binarizations, constructed as des"
N09-1061,P04-1084,1,0.120203,"ions of the original grammar can be reconstructed using some simple homomorphism (c.f. Nijholt, 1980). Our contribution is significant because the existing algorithms for decomposing SCFG, based on Uno and Yagiura (2000), cannot be applied to LCFRS, as they rely on the crucial property that components of biphrases are strictly separated in the generated string: Given a pair of synchronized nonterminal symbols, the material derived from the source nonterminal must precede the material derived from the target nonterminal, or vice versa. The problem that we solve has been previously addressed by Melamed et al. (2004), but in contrast to our result, their algorithm does not guarantee an optimal (minimal) increase in the fanout of the resulting grammar. However, this is essential for the practical applicability of the transformed grammar, as the parsing complexity of LCFRS is exponential in both the rank and the fan-out. Structure of the paper The remainder of the paper is structured as follows. Section 2 introduces the terminology and notation that we use for LCFRS. In Section 3, we present the technical background of our algorithm; the algorithm itself is discussed in Section 4. Section 5 concludes the pa"
N09-1061,P08-1069,1,0.851562,"Missing"
N09-1061,P87-1015,1,0.604286,"Missing"
N09-1061,P06-1123,0,0.0348496,"racted grammar are transformed so as to minimise this quantity. Not only is this beneficial in 539 Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar (SCFG) (Uno and Yagiura, 2000; Zhang et al., 2008). However, the SCFG formalism is limited to modelling word-to-word alignments in which a single continuous phrase in the source language is aligned with a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS was originally"
N09-1061,C08-1136,0,0.0189435,"ences therein. One practical problem with this approach, apart from the sheer number of the rules that result from the extraction procedure, is that the parsing complexity of all synchronous formalisms that we are aware of is exponential in the rank of a rule, defined as the number of nonterminals on the righthand side. Therefore, it is important that the rules of the extracted grammar are transformed so as to minimise this quantity. Not only is this beneficial in 539 Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar (SCFG) (Uno and Yagiura, 2000; Zhang et al., 2008). However, the SCFG formalism is limited to modelling word-to-word alignments in which a single continuous phrase in the source language is aligned with a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studi"
N09-1061,N06-1033,0,0.095259,"ear Context-Free Rewriting Systems Carlos G´omez-Rodr´ıguez1 , Marco Kuhlmann2 , Giorgio Satta3 and David Weir4 1 Departamento de Computaci´on, Universidade da Coru˜na, Spain (cgomezr@udc.es) Department of Linguistics and Philology, Uppsala University, Sweden (marco.kuhlmann@lingfil.uu.se) 2 3 Department of Information Engineering, University of Padua, Italy (satta@dei.unipd.it) 4 Department of Informatics, University of Sussex, United Kingdom (davidw@sussex.ac.uk) Abstract terms of parsing complexity, but smaller rules can also improve a translation model’s ability to generalize to new data (Zhang et al., 2006). Linear Context-free Rewriting Systems (LCFRS) is an expressive grammar formalism with applications in syntax-based machine translation. The parsing complexity of an LCFRS is exponential in both the rank of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, called fan-out. In this paper, we present an algorithm that transforms an LCFRS into a strongly equivalent form in which all productions have rank at most 2, and has minimal fan-out. Our results generalize previous work on Synchronous Context-Free Grammar, and are pa"
N09-1061,J07-2003,0,\N,Missing
N10-1035,C02-1028,0,0.0403826,"e form x1,1 · · · xm,1 ; and (ii) for each i ∈ [m], the sequence obtained from f by reading variables of the form xi,j from left to right has the form xi,1 · · · xi,ki . An LCFRS is called canonical, if each of its composition operations is canonical. We omit the proof that every LCFRS can be transformed into a weakly equivalent canonical LCFRS. However, we point out that both the normal form and the parsing algorithm that we present in this paper can be applied also to general LCFRS. This is in contrast to some left-to-right parsers in the literature on LCFRS and equivalent formalisms (de la Clergerie, 2002; Kallmeyer and Maier, 2009), which actually depend on productions in canonical form. 2.5 Well-nested LCFRS We now characterize the class of well-nested LCFRS that are the focus of this paper. Well-nestedness was first studied in the context of dependency grammars (Kuhlmann and Möhl, 2007). Kanazawa (2009) defines well-nested multiple context-free grammars, which are weakly equivalent to well-nested LCFRS. A composition operation is called well-nested, if it does not contain a substring of the form xi,i1 · · · xj,j1 · · · xi,i2 · · · xj,j2 , where i 6= j . For example, the operation x1,1 x2,1"
N10-1035,N10-1118,0,0.32636,". . . , (lm,ϕ(Am ) , rm,ϕ(Am ) )] [A0 , (l0,1 , r0,1 ), . . . , (l0,ϕ(A0 ) , r0,ϕ(A0 ) )] (a) The general rule for a parsing schema for LCFRS [B, (l1 , r1 ), . . . , (lm , rm )] [C, (l10 , r10 ), . . . (ln0 , rn0 )] [A, (l1 , r1 ), . . . , (lm , r10 ), . . . (ln0 , rn0 )] rm = l10 (b) Deduction step for concatenation [B, (l1 , r1 ), . . . , (lm , rm )] [C, (l10 , r10 ), . . . (ln0 , rn0 )] [A, (l1 , r1 ), . . . , (li , r10 ), . . . (ln0 , ri+1 ), . . . , (lm , rm )] ri = l10 , rn0 = li+1 (c) Deduction step for wrapping Figure 2: Deduction steps for parsing LCFRS. Thus, the parsing complexity (Gildea, 2010) of a production p = A0 → fP (A1 , . . . , Am ) is determined by ϕ(A0 ) l-indexes and i∈[m] ϕ(Ai ) r-indexes, for a total complexity of P O(|w|ϕ(A0 )+ i∈[m] ϕ(Ai ) ) where |w |is the length of the input string. The parsing complexity of an LCFRS will correspond to the maximum parsing complexity among its productions. Note that this general complexity matches the result given by Seki et al. (1991). In an LCFRS of rank ρ and fan-out ϕ, the maximum possible parsing complexity is O(|w|ϕ(ρ+1) ), obtained by applying the above expression to a production of rank ρ and where each nonterminal has fanou"
N10-1035,N09-1061,1,0.733957,"le parsing complexity is O(|w|ϕ(ρ+1) ), obtained by applying the above expression to a production of rank ρ and where each nonterminal has fanout ϕ. The asymptotic time complexity of LCFRS parsing is therefore exponential both in its rank and its fan-out. This means that it is interesting to transform LCFRS into equivalent forms that reduce their rank while preserving the fan-out. For sets of LCFRS that can be transformed into a binary form (i.e., such that all its rules have rank at most 2), the ρ factor in the complexity is reduced to a constant, and complexity is improved to O(|w|3ϕ ) (see Gómez-Rodríguez et al. (2009) for further discussion). Unfortunately, it is known by previous results (Rambow and Satta, 1999) that it is not always possible to convert an LCFRS into such a binary form without increasing the fan-out. However, we will show that it is always possible to build such a binarization for well-nested LCFRS. Combining this result with the inference rule and complexity analysis given above, we would obtain a parser for well-nested LCFRS running in 280 O(|w|3ϕ ) time. But the construction of our binary normal form additionally restricts binary composition operations in the binarized LCFRS to be of t"
N10-1035,P07-1077,0,0.032643,"arsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal forms having binary derivation trees only. It has recently been argued that LCFRS might be too powerful for modeling languages with discontinuous constituents or with relatively free word order, and that additional restrictions on the rearrangement of components might be needed. More specifically, analyses of both dependency and constituency treebanks (Kuhlmann and Nivre, 2006; Havelka, 2007; Maier and Lichte, 2009) have shown that rearrangements of argument tuples almost always satisfy the so-called well-nestedness condition, a generalization 276 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276–284, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of the standard condition on balanced brackets. This condition states that any two components x1 , x2 of some tuple will never be composed with any two components y1 , y2 of some other tuple in such a way that a ‘crossing’ configuration is re"
N10-1035,W09-3808,0,0.0143045,"xm,1 ; and (ii) for each i ∈ [m], the sequence obtained from f by reading variables of the form xi,j from left to right has the form xi,1 · · · xi,ki . An LCFRS is called canonical, if each of its composition operations is canonical. We omit the proof that every LCFRS can be transformed into a weakly equivalent canonical LCFRS. However, we point out that both the normal form and the parsing algorithm that we present in this paper can be applied also to general LCFRS. This is in contrast to some left-to-right parsers in the literature on LCFRS and equivalent formalisms (de la Clergerie, 2002; Kallmeyer and Maier, 2009), which actually depend on productions in canonical form. 2.5 Well-nested LCFRS We now characterize the class of well-nested LCFRS that are the focus of this paper. Well-nestedness was first studied in the context of dependency grammars (Kuhlmann and Möhl, 2007). Kanazawa (2009) defines well-nested multiple context-free grammars, which are weakly equivalent to well-nested LCFRS. A composition operation is called well-nested, if it does not contain a substring of the form xi,i1 · · · xj,j1 · · · xi,i2 · · · xj,j2 , where i 6= j . For example, the operation x1,1 x2,1 $x2,2 x1,2 is wellnested, wh"
N10-1035,P07-1021,1,0.883517,"LCFRS can be transformed into a weakly equivalent canonical LCFRS. However, we point out that both the normal form and the parsing algorithm that we present in this paper can be applied also to general LCFRS. This is in contrast to some left-to-right parsers in the literature on LCFRS and equivalent formalisms (de la Clergerie, 2002; Kallmeyer and Maier, 2009), which actually depend on productions in canonical form. 2.5 Well-nested LCFRS We now characterize the class of well-nested LCFRS that are the focus of this paper. Well-nestedness was first studied in the context of dependency grammars (Kuhlmann and Möhl, 2007). Kanazawa (2009) defines well-nested multiple context-free grammars, which are weakly equivalent to well-nested LCFRS. A composition operation is called well-nested, if it does not contain a substring of the form xi,i1 · · · xj,j1 · · · xi,i2 · · · xj,j2 , where i 6= j . For example, the operation x1,1 x2,1 $x2,2 x1,2 is wellnested, while x1,1 x2,1 $ x1,2 x2,2 is not. An LCFRS is called well-nested, if it contains only well-nested composition operations. The class of languages generated by well-nested LCFRS is properly included in the class of languages generated by general LCFRS; see Kanazaw"
N10-1035,P06-2066,1,0.896161,"ly convenient in tabular parsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal forms having binary derivation trees only. It has recently been argued that LCFRS might be too powerful for modeling languages with discontinuous constituents or with relatively free word order, and that additional restrictions on the rearrangement of components might be needed. More specifically, analyses of both dependency and constituency treebanks (Kuhlmann and Nivre, 2006; Havelka, 2007; Maier and Lichte, 2009) have shown that rearrangements of argument tuples almost always satisfy the so-called well-nestedness condition, a generalization 276 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276–284, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of the standard condition on balanced brackets. This condition states that any two components x1 , x2 of some tuple will never be composed with any two components y1 , y2 of some other tuple in such a way that a ‘crossing’ conf"
N10-1035,E09-1055,1,0.547229,"stigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al. (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. This feature has been used to model phrase structure treebanks with discontinuous constituents (Maier and Søgaard, 2008), as well as to map non-projective dependency trees into discontinuous phrase structures (Kuhlmann and Satta, 2009). The freedom in the rearrangement of components has specific consequences in terms of the computational and descriptional complexity of LCFRS. Even for grammars with bounded fan-out, the universal recognition problem is NP-hard (Satta, 1992), and these systems lack Chomsky-like normal forms for fixed fan-out (Rambow and Satta, 1999) that are especially convenient in tabular parsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal fo"
N10-1035,P92-1012,1,0.837614,"r et al. (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. This feature has been used to model phrase structure treebanks with discontinuous constituents (Maier and Søgaard, 2008), as well as to map non-projective dependency trees into discontinuous phrase structures (Kuhlmann and Satta, 2009). The freedom in the rearrangement of components has specific consequences in terms of the computational and descriptional complexity of LCFRS. Even for grammars with bounded fan-out, the universal recognition problem is NP-hard (Satta, 1992), and these systems lack Chomsky-like normal forms for fixed fan-out (Rambow and Satta, 1999) that are especially convenient in tabular parsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal forms having binary derivation trees only. It has recently been argued that LCFRS might be too powerful for modeling languages with discontinuous constituents or with relatively free word order, and that additional restrictions on the rearrange"
N10-1035,P87-1015,0,0.833476,"l linguistics has been the modeling of natural language syntax by means of formal grammars. Following results by Huybregts (1984) and Shieber (1985), special attention has been given to formalisms that enlarge the generative power of context-free grammars, but still remain below the full generative power of context-sensitive grammars. On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al. (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. This feature has been used to model phrase structure treebanks with discontinuous constituents (Maier and Søgaard, 2008), as well as to map non-projective dependency trees into discontinuous phrase structures (Kuhlmann and Satta, 2009). The freedom in the rearrangement of components has specific consequences in terms of the computational and descriptional complexity of LCFRS. Even for grammars with bounded fan-out, the universal recognition problem is NP-hard (Satta, 1992), a"
P06-2066,P99-1065,0,0.137744,"Missing"
P06-2066,P01-1024,0,0.0276704,"e rooted at j . These subtrees interleave, as T1 contains the nodes 2 and 4, and T2 contains the nodes 3 and 5. 1 2i 3 4 5 j (a) gd D 0, ed D 0, wnC 6 1 2i 3 4 j 5 6 (b) gd D 1, ed D 1, wnC 1 2i 3 j 4 5 6 (c) gd D 2, ed D 1, wn Figure 3: Gap degree, edge degree, and well-nestedness 3.3 or more edges .i; k/ in T by edges .j ; k/, where j ! i . The exact conditions under which a certain lifting may take place are specified in the rules of the grammar. A dependency tree is acceptable, if it can be lifted to form a projective graph.3 A similar design is pursued in Topological Dependency Grammar (Duchier and Debusmann, 2001), where a dependency analysis consists of two, mutually constraining graphs: the ID graph represents information about immediate dominance, the LP graph models the topological structure of a sentence. As a principle of the grammar, the LP graph is required to be a lift of the ID graph; this lifting can be constrained in the lexicon. Edge degree The notion of edge degree was introduced by Nivre (2006) in order to allow mildly non-projective structures while maintaining good parsing efficiency in data-driven dependency parsing.2 Define the span of an edge .i; j / as the interval S..i; j // WD Œm"
P06-2066,C96-1058,0,0.0661308,"ing of the sentence, it corresponds to a ban on discontinuous constituents in phrase structure representations. Projectivity is an interesting constraint on dependency structures both from a theoretical and a practical perspective. Dependency grammars that only allow projective structures are closely related to context-free grammars (Gaifman, 1965; Obre¸bski and Grali´nski, 2004); among other things, they have the same (weak) expressivity. The projectivity constraint also leads to favourable parsing complexities: chart-based parsing of projective dependency grammars can be done in cubic time (Eisner, 1996); hard-wiring projectivity into a deterministic dependency parser leads to linear-time parsing in the worst case (Nivre, 2003). 508 3 Relaxations of projectivity While the restriction to projective analyses has a number of advantages, there is clear evidence that it cannot be maintained for real-world data (Zeman, 2004; Nivre, 2006). For example, the graph in Figure 1 is non-projective: the yield of the node 1 (marked by the dashed rectangles) does not form an interval—the node 2 is ‘missing’. In this section, we present several proposals for structural constraints that relax projectivity, and"
P06-2066,E06-1012,0,0.0178577,"Missing"
P06-2066,W05-1505,0,0.0357835,"antees good parsing complexity, it is well-known that certain syntactic constructions can only be adequately represented by non-projective dependency structures, where the projection of a head can be discontinuous. This is especially relevant for languages with free or flexible word order. However, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Nivre and Nilsson, 2005; Hall and Novák, 2005; McDonald and Pereira, 2006). This raises the question of whether it is possible to characterize a class of mildly non-projective dependency structures that is rich enough to account for naturally occurring syntactic constructions, yet restricted enough to enable efficient parsing. In this paper, we review a number of proposals for classes of dependency structures that lie between strictly projective and completely unrestricted non-projective structures. These classes have in common that they can be characterized in terms of properties of the dependency structures themselves, rather than in t"
P06-2066,E06-1011,0,0.0143615,"mplexity, it is well-known that certain syntactic constructions can only be adequately represented by non-projective dependency structures, where the projection of a head can be discontinuous. This is especially relevant for languages with free or flexible word order. However, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Nivre and Nilsson, 2005; Hall and Novák, 2005; McDonald and Pereira, 2006). This raises the question of whether it is possible to characterize a class of mildly non-projective dependency structures that is rich enough to account for naturally occurring syntactic constructions, yet restricted enough to enable efficient parsing. In this paper, we review a number of proposals for classes of dependency structures that lie between strictly projective and completely unrestricted non-projective structures. These classes have in common that they can be characterized in terms of properties of the dependency structures themselves, rather than in terms of grammar formalisms th"
P06-2066,H05-1066,0,0.266303,"Missing"
P06-2066,P05-1013,1,0.388197,"hile this constraint guarantees good parsing complexity, it is well-known that certain syntactic constructions can only be adequately represented by non-projective dependency structures, where the projection of a head can be discontinuous. This is especially relevant for languages with free or flexible word order. However, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Nivre and Nilsson, 2005; Hall and Novák, 2005; McDonald and Pereira, 2006). This raises the question of whether it is possible to characterize a class of mildly non-projective dependency structures that is rich enough to account for naturally occurring syntactic constructions, yet restricted enough to enable efficient parsing. In this paper, we review a number of proposals for classes of dependency structures that lie between strictly projective and completely unrestricted non-projective structures. These classes have in common that they can be characterized in terms of properties of the dependency structures themse"
P06-2066,W03-3017,1,0.874638,"an interesting constraint on dependency structures both from a theoretical and a practical perspective. Dependency grammars that only allow projective structures are closely related to context-free grammars (Gaifman, 1965; Obre¸bski and Grali´nski, 2004); among other things, they have the same (weak) expressivity. The projectivity constraint also leads to favourable parsing complexities: chart-based parsing of projective dependency grammars can be done in cubic time (Eisner, 1996); hard-wiring projectivity into a deterministic dependency parser leads to linear-time parsing in the worst case (Nivre, 2003). 508 3 Relaxations of projectivity While the restriction to projective analyses has a number of advantages, there is clear evidence that it cannot be maintained for real-world data (Zeman, 2004; Nivre, 2006). For example, the graph in Figure 1 is non-projective: the yield of the node 1 (marked by the dashed rectangles) does not form an interval—the node 2 is ‘missing’. In this section, we present several proposals for structural constraints that relax projectivity, and relate them to each other. 3.1 1 Yli-Jyrä (2003) proposes multiplanarity as a generalization of planarity suitable for modell"
P06-2066,E06-1010,1,0.876226,"(Gaifman, 1965; Obre¸bski and Grali´nski, 2004); among other things, they have the same (weak) expressivity. The projectivity constraint also leads to favourable parsing complexities: chart-based parsing of projective dependency grammars can be done in cubic time (Eisner, 1996); hard-wiring projectivity into a deterministic dependency parser leads to linear-time parsing in the worst case (Nivre, 2003). 508 3 Relaxations of projectivity While the restriction to projective analyses has a number of advantages, there is clear evidence that it cannot be maintained for real-world data (Zeman, 2004; Nivre, 2006). For example, the graph in Figure 1 is non-projective: the yield of the node 1 (marked by the dashed rectangles) does not form an interval—the node 2 is ‘missing’. In this section, we present several proposals for structural constraints that relax projectivity, and relate them to each other. 3.1 1 Yli-Jyrä (2003) proposes multiplanarity as a generalization of planarity suitable for modelling dependency analyses, and evaluates it experimentally using data from DDT. Definition 5 A dependency graph G D .V I E/ is m-planar, if it can be split into m planar graphs 1 2 3 4 5 (b) 2-planar Figure 2:"
P06-2066,W04-1508,0,0.0346234,"Missing"
P06-2066,P92-1012,0,0.861113,"Missing"
P06-2066,1993.iwpt-1.22,0,0.0968269,"an interval (.2; 3; 4/), so i has gap degree 0. In Graph 3b, i D .2; 3; 6/ contains a single gap (.3; 6/), so the gap degree of i is 1. In the rightmost graph, the gap degree of i is 2, since i D .2; 4; 6/ contains two gaps (.2; 4/ and .4; 6/). Definition 7 The gap degree of a dependency graph G , gd.G/, is the maximum among the gap degrees of its nodes. Definition 4 A dependency graph is planar, if it does not contain nodes a; b; c; d such that linked.a; c/ ^ linked.b; d/ ^ a &lt; b &lt; c &lt; d : 3 (a) 1-planar Planarity and multiplanarity The notion of planarity appears in work on Link Grammar (Sleator and Temperley, 1993), where it is traced back to Mel’ˇcuk (1988). Informally, a dependency graph is planar, if its edges can be drawn above the sentence without crossing. We emphasize the word above, because planarity as it is understood here does not coincide with the standard graph-theoretic concept of the same name, where one would be allowed to also use the area below the sentence to disentangle the edges. Figure 2a shows a dependency graph that is planar but not projective: while there are no crossing edges, the yield of the node 1 (the set f1; 3g) does not form an interval. Using the notation linked.i; j /"
P06-2066,P98-1106,0,0.104755,"ependent is .1; 5/, but the root of the connected component f2; 3; 4g is dominated by 1. Both Graph 3b and 3c have edge degree 1: the edge .3; 6/ in Graph 3b and the edges .2; 4/, .3; 5/ and .4; 6/ in Graph 3c each span a single connected component that is not dominated by the respective head. 3.4 Related work Apart from proposals for structural constraints relaxing projectivity, there are dependency frameworks that in principle allow unrestricted graphs, but provide mechanisms to control the actually permitted forms of non-projectivity in the grammar. The non-projective dependency grammar of Kahane et al. (1998) is based on an operation on dependency trees called lifting: a ‘lift’ of a tree T is the new tree that is obtained when one replaces one 3.5 Discussion The structural conditions we have presented here naturally fall into two groups: multiplanarity, gap degree and edge degree are parametric constraints with an infinite scale of possible values; planarity and well-nestedness come as binary constraints. We discuss these two groups in turn. Parametric constraints With respect to the graded constraints, we find that multiplanarity is different from both gap degree and edge degree in that it involv"
P06-2066,C98-1102,0,\N,Missing
P07-1021,P04-1054,0,0.0625191,"fied structures, which makes them hard to integrate into models where structures are composed from lexical information. In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages. 1 Introduction Syntactic representations based on word-to-word dependencies have a long tradition in descriptive linguistics. Lately, they have also been used in many computational tasks, such as relation extraction (Culotta and Sorensen, 2004), parsing (McDonald et al., 2005), and machine translation (Quirk et al., 2005). Especially in recent work on parsing, there is a particular interest in non-projective dependency structures, in which a word and its dependents may be spread out over a discontinuous region of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such 160 Mathias Möhl Programming Systems Lab Saarland University Saarbrücken, Germany mmohl@ps.uni-sb.de as Czech (Veselá et al., 2004). Unfortunately, most formal results on non-projectivity are discouraging: Wh"
P07-1021,P99-1059,0,0.0167078,"non-projective dependency structures, in which a word and its dependents may be spread out over a discontinuous region of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such 160 Mathias Möhl Programming Systems Lab Saarland University Saarbrücken, Germany mmohl@ps.uni-sb.de as Czech (Veselá et al., 2004). Unfortunately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999), parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally wel"
P07-1021,P06-2066,1,0.848215,"exibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. Both constraints have been shown to be in very good fit with data from dependency treebanks (Kuhlmann and Nivre, 2006). However, like all other such proposals, they are formulated on fully specified structures, which makes it hard to integrate them into a generative model, where dependency structures are composed from elementary units of lexicalized information. Consequently, little is known about the generative capacity and computational complexity of languages over restricted non-projective dependency structures. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 160–167, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Contents o"
P07-1021,E06-1011,0,0.0198248,"nately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999), parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. Both constraints have been shown to be in very"
P07-1021,H05-1066,0,0.242909,"Missing"
P07-1021,P97-1043,0,0.0991052,"of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such 160 Mathias Möhl Programming Systems Lab Saarland University Saarbrücken, Germany mmohl@ps.uni-sb.de as Czech (Veselá et al., 2004). Unfortunately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999), parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restric"
P07-1021,E06-1010,0,0.343131,"ve when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997). Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005), but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006). In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006). In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. Both constraints have been shown to be in very good fit with data from dependency treebanks (Kuhlmann and Nivre, 2006). However, like all other such proposals, they are formulated on fully specified structures, which makes it hard to integrate them into a generative model, where dependency structures are com"
P07-1021,P05-1034,0,0.0549893,"mposed from lexical information. In this paper, we show how two empirically relevant relaxations of projectivity can be lexicalized, and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages. 1 Introduction Syntactic representations based on word-to-word dependencies have a long tradition in descriptive linguistics. Lately, they have also been used in many computational tasks, such as relation extraction (Culotta and Sorensen, 2004), parsing (McDonald et al., 2005), and machine translation (Quirk et al., 2005). Especially in recent work on parsing, there is a particular interest in non-projective dependency structures, in which a word and its dependents may be spread out over a discontinuous region of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such 160 Mathias Möhl Programming Systems Lab Saarland University Saarbrücken, Germany mmohl@ps.uni-sb.de as Czech (Veselá et al., 2004). Unfortunately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structu"
P07-1021,P92-1012,0,0.104644,"ties of these entries. Sets of gap-restricted dependency structures can be described using regular tree grammars. This gives rise to a notion of regular dependency languages, and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms (Joshi, 1985): We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al., 1987), and that the gap-degree measure is the structural correspondent of the concept of ‘fan-out’ in this formalism (Satta, 1992). We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996), and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag). This result generalizes previous work on the relation between ltag and dependency representations (Rambow and Joshi, 1997; Bodirsky et al., 2005). Structure of the paper The remainder of this paper is structured as follows. Section 2 contains some basic notions related to tr"
P07-1021,C04-1042,0,0.0519543,"Missing"
P07-1021,P87-1015,0,0.897899,"ees can be read as lexical entries, and both the gap-degree restriction and the well-nestedness condition can be couched as syntactic properties of these entries. Sets of gap-restricted dependency structures can be described using regular tree grammars. This gives rise to a notion of regular dependency languages, and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms (Joshi, 1985): We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al., 1987), and that the gap-degree measure is the structural correspondent of the concept of ‘fan-out’ in this formalism (Satta, 1992). We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996), and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag). This result generalizes previous work on the relation between ltag and dependency representations (Rambow and Joshi, 1997; Bodirsky et al., 2005). Str"
P10-1055,T75-2001,0,0.544577,"ages. Two things are worth noting. First, our result shows that the ability of CCG to generate non-context-free languages does not hinge on the availability of substitution and type-raising rules: The derivations of G1 only use generalized compositions. Neither does it require the use of functional argument categories: The grammar G1 is first-order in the sense of Koller and Kuhlmann (2009). Proof. To see the inclusion, it suffices to note that pure CCG when restricted to application rules is the same as AB-grammar, the classical categorial formalism investigated by Ajdukiewicz and BarHillel (Bar-Hillel et al., 1964). This formalism is weakly equivalent to context-free grammar. Second, it is important to note that if the composition degree n is restricted to 0 or 1, pure CCG actually collapses to context-free expressive power. This is clear for n D 0 because of the equivalence to AB grammar. For n D 1, observe that the arity of the result of a composition is at most as high as 3.1 CFG ¨ PCCG 537 that of each premise. This means that the arity of any derived category is bounded by the maximal arity of lexical categories in the grammar, which together with Lemma 1 implies that there is only a finite set of"
P10-1055,C04-1180,0,0.0341993,"t-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, w"
P10-1055,J07-4004,0,0.0370889,"of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is"
P10-1055,P96-1011,0,0.391099,"ct the application of individual rules. This means that these formalisms cannot be fully lexicalized, in the sense that certain languages can only be described by selecting language-specific rules. Our result generalizes Koller and Kuhlmann’s (2009) result for pure first-order CCG. Our proof is not as different as it looks at first glance, as their construction of mapping a CCG derivation to a valency tree and back to a derivation provides a different transformation on derivation trees. Our transformation is also technically related to the normal form construction for CCG parsing presented by Eisner (1996). Of course, at the end of the day, the issue that is more relevant to computational linguistics than a formalism’s ability to generate artificial languages such as L3 is how useful it is for modeling natural languages. CCG, and multi-modal CCG in particular, has a very good track record for this. In this sense, our formal result can also be understood as a contribution to a discussion about the expressive power that is needed to model natural languages. Acknowledgments We have profited enormously from discussions with Jason Baldridge and Mark Steedman, and would also like to thank the anonymo"
P10-1055,P02-1043,0,0.0156084,"G is strictly smaller than that of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The gener"
P10-1055,W08-2306,0,0.193753,"nguages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, which allow it to model more complex word orders that context-free grammar can. The discussion of the (weak and strong) generative capacity of CCG and TAG has recently been revived (Hockenmaier and Young, 2008; Koller and Kuhlmann, 2009). In particular, Koller and Kuhlmann (2009) have shown that CCGs that are pure (i.e., they can only use generalized composition rules, and there is no way to restrict the instances of these rules that may be used) and first-order (i.e., all argument categories are atomic) can not generate an b n c n . This shows that the generative capacity of at least first-order CCG crucially relies on its ability to restrict rule instantiations, and is at odds with the general conception of CCG as a fully lexicalized formalism, in which all grammars use one and the same set of un"
P10-1055,E09-1053,1,0.930194,"rsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, which allow it to model more complex word orders that context-free grammar can. The discussion of the (weak and strong) generative capacity of CCG and TAG has recently been revived (Hockenmaier and Young, 2008; Koller and Kuhlmann, 2009). In particular, Koller and Kuhlmann (2009) have shown that CCGs that are pure (i.e., they can only use generalized composition rules, and there is no way to restrict the instances of these rules that may be used) and first-order (i.e., all argument categories are atomic) can not generate an b n c n . This shows that the generative capacity of at least first-order CCG crucially relies on its ability to restrict rule instantiations, and is at odds with the general conception of CCG as a fully lexicalized formalism, in which all grammars use one and the same set of universal rules. A question th"
P10-1055,W06-1637,0,0.0149884,"ammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, which allow it to model more complex word orders that context-fr"
P10-1055,P88-1034,0,0.702937,"string w is called the yield of the resulting derivation tree. A derivation tree is complete, if the last category is the final category of G. The language generated by G, denoted by L.G/, is formed by the yields of all complete derivation trees. 2.4 Degree Restrictions Work on CCG generally assumes an upper bound on the degree of composition rules that can be used in derivations. We also employ this restriction, and only consider grammars with compositions of some bounded (but arbitrary) degree n  0.1 CCG with unbounded-degree compositions is more expressive than bounded-degree CCG or TAG (Weir and Joshi, 1988). Bounded-degree grammars have a number of useful properties, one of which we mention here. The following lemma rephrases Lemma 3.1 in Vijay-Shanker and Weir (1994). Lemma 2 For every grammar G, there is a finite number of categories that can occur as secondary premises in derivations of G. Proof. The arity of a secondary premise c can be written as m C n, where m is the arity of the first argument of the corresponding primary premise, and n is the degree of the rule applied. Since each argument is an argument of some lexical category of G (Lemma 1), and since n is assumed to be bounded, both"
P10-1055,E03-1036,0,\N,Missing
P11-1068,W06-2922,0,0.039634,"e transition. It is called complete whenever c0 D I.w/, and cm 2 C t . We note that a computation can be uniquely specified by its initial configuration c0 and the sequence of its transitions, understood as a string over T . Complete computations, where c0 is fixed, can be specified by their transition sequences alone. 3 Arc-Standard Model To introduce the core concepts of the paper, we first look at a particularly simple model for transitionbased dependency parsing, known as the arc-standard model. This model has been used, in slightly different variants, by a number of parsers (Nivre, 2004; Attardi, 2006; Huang and Sagae, 2010). 3.1 Transition System The arc-standard model uses three types of transitions: Shift (sh) removes the first node in the buffer and pushes it to the stack. Left-Arc (la) creates a new arc with the topmost node on the stack as the head and the second-topmost node as the dependent, and removes the second-topmost node from the stack. Right-Arc (ra) is symmetric to Left-Arc in that it creates an arc with the second-topmost node as the head and the topmost node as the dependent, and removes the topmost node. The three transitions can be formally specified as in Figure 1. The"
P11-1068,P89-1018,0,0.0650338,"es where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eage"
P11-1068,P96-1025,0,0.0250951,"ntermediate configuration, and thereby violates property P1. 3.5 Discussion Let us briefly take stock of what we have achieved so far. We have provided a deduction system capable of tabulating the set of all computations of an arcstandard parser on a given input string, and proved the correctness of this system relative to an interpretation based on push computations. Inspecting the system, we can see that its generic implementation takes space in O.jwj3 / and time in O.jwj5 /. Our deduction system is essentially the same as the one for the CKY algorithm for bilexicalized contextfree grammar (Collins, 1996; Gómez-Rodríguez et al., 2008). This equivalence reveals a deep correspondence between the arc-standard model and bilexicalized context-free grammar, and, via results by Eisner and Satta (1999), to head automata. In particular, Eisner’s and Satta’s “hook trick” can be applied to our tabulation to reduce its runtime to O.jwj4 /. 4 Adding Features 1 are both push computations with strictly fewer transitions than . Suppose that the last transition in is ra. In this case, ˇ.c/ D ˇk for some i < k < j , .c/ D .c0 /jh with h < k, ˇ.cm 1 / D ˇj , and .cm 1 / D  .c0 /jhjh0 for some k  h0 < j . B"
P11-1068,P99-1059,1,0.293239,"bulating the set of all computations of an arcstandard parser on a given input string, and proved the correctness of this system relative to an interpretation based on push computations. Inspecting the system, we can see that its generic implementation takes space in O.jwj3 / and time in O.jwj5 /. Our deduction system is essentially the same as the one for the CKY algorithm for bilexicalized contextfree grammar (Collins, 1996; Gómez-Rodríguez et al., 2008). This equivalence reveals a deep correspondence between the arc-standard model and bilexicalized context-free grammar, and, via results by Eisner and Satta (1999), to head automata. In particular, Eisner’s and Satta’s “hook trick” can be applied to our tabulation to reduce its runtime to O.jwj4 /. 4 Adding Features 1 are both push computations with strictly fewer transitions than . Suppose that the last transition in is ra. In this case, ˇ.c/ D ˇk for some i < k < j , .c/ D .c0 /jh with h < k, ˇ.cm 1 / D ˇj , and .cm 1 / D  .c0 /jhjh0 for some k  h0 < j . By induction, we may assume that we have generated items Œi; h; k and Œk; h0 ; j . Applying the inference 676 The main goal with the tabulation of transition-based dependency parsers is to obta"
P11-1068,P08-1110,1,0.950177,"s: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our algorithms, and in particular the conditions under which the feature models traditionally used in transition-based dependency parsing can be integrated into our framework. While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). We instead simulate computations as in Lang (1974), which results in simpler algorithm specifications, and also reveals deep similarities bet"
P11-1068,J99-4004,0,0.0177162,"ming algorithms, also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic progra"
P11-1068,P10-1110,0,0.0834865,"ons in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper"
P11-1068,D09-1127,0,0.0208373,"Missing"
P11-1068,D09-1005,0,0.0163563,", also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a"
P11-1068,W03-3017,0,0.0554594,"s that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of featur"
P11-1068,W04-0308,0,0.783918,"Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008"
P11-1068,J08-4003,0,0.774527,"polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time"
P11-1068,W03-3023,0,0.0736894,"stem takes space O.jwj2 / and time O.jwj3 /. In the original interpretation of the deduction system, an item Œi; j  asserts the existence of a pair of (projective) dependency trees: the first tree rooted at token wi , having all nodes in the substring wi    wk 1 as descendants, where i < k  j ; and the second tree rooted at token wj , having all nodes in the substring wk    wj as descendants. (Note that we use fencepost indexes, while Gómez-Rodríguez et al. (2008) indexes positions.) Deduction System Gómez-Rodríguez et al. (2008) present a deductive version of the dependency parser of Yamada and Matsumoto (2003); their deduction system is given in Fig680 .ji jj; ˇ; A/ ` .ji; ˇ; A [ fi ! j g/ .ra/ We call this transition system the hybrid model, as sh and ra are just like in arc-standard, while lah is like the Left-Arc transition in the arc-eager model (lae ), except that it does not have the precondition. Like the arc-standard but unlike the arc-eager model, the hybrid model builds dependencies bottom-up. 7 Conclusion In this paper, we have provided a general technique for the tabulation of transition-based dependency parsers, and applied it to obtain dynamic programming algorithms for two widely-u"
P11-1068,D08-1059,0,0.415993,"the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our alg"
Q13-1022,W06-2920,0,0.0790276,"nherit property, this even increases to two orders of magnitude. However, as already stated in 2, this improvement is paid for by a loss in coverage; for instance, trees of the form shown in Figure 3 cannot be parsed any longer. 7.1 Quantitative Evaluation In order to assess the empirical loss in coverage that the restriction to head-split trees incurs, we evaluated the coverage of several classes of dependency trees on standard data sets. Following Pitler et al. (2012), we report in Table 1 figures for the training sets of six languages used in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006). As we can see, the O.n6 / class of head-split trees has only slightly lower coverage on this data than the baseline class of well-nested dependency trees with block-degree at most 2. The losses are up to 0.2 percentage points on five of the six languages, and 0.9 points on the Dutch data. Our even more restricted O.n5 / class of 1-inherit head-split trees has the same coverage as our O.n6 / class, which is expected given the results of Pitler et al. (2012): Their O.n6 / class of 1-inherit trees has exactly the same coverage as the baseline (and thereby more coverage than our O.n6 / class). I"
Q13-1022,D07-1101,0,0.0725403,"l property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O.n2 / (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and comple"
Q13-1022,W10-4407,0,0.121032,"‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2. This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restriction to this class (Maier and Lichte, 2011; Chen-Main and Joshi, 2010), Kuhlmann and Nivre (2006) found that it has excellent coverage on standard data sets. Assuming an arc-factored model, well-nested dependency trees with block-degree  2 can be parsed in time O.n7 / using the algorithm of G´omez-Rodr´ıguez et al. (2011). Recently, Pitler et al. (2012) have shown that if an additional restriction called 1-inherit is imposed, parsing can be done in time O.n6 /, without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O.n5 / time complexity. The O.n3 / time resu"
Q13-1022,P99-1059,1,0.817837,"g can be done in time O.n6 /, without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O.n5 / time complexity. The O.n3 / time result reported by Eisner (1997) has been obtained by exploiting more sophisticated dynamic programming techniques that ‘split’ dependency trees at the position of their heads, in order to save bookkeeping. Splitting techniques have also been exploited to speed up parsing time for other lexicalized formalisms, such as bilexical context-free grammars and head automata (Eisner and Satta, 1999). However, to our knowledge no attempt has been made in the literature to extend these techniques to non-projective dependency parsing. In this article we leverage the central idea from Eisner’s algorithm and extend it to the class of wellnested dependency trees with block-degree at most 2. 267 Transactions of the Association for Computational Linguistics, 1 (2013) 267–278. Action Editor: Brian Roark. c Submitted 3/2013; Published 7/2013. 2013 Association for Computational Linguistics. We introduce a structural property, called head-split, that allows us to split these trees at the positions o"
Q13-1022,W00-2011,1,0.902321,"Missing"
Q13-1022,1997.iwpt-1.10,0,0.756975,"under the arc-factored model. In this article we extend these techniques to a class of non-projective dependency trees, called well-nested dependency trees with block-degree at most 2, which has been previously investigated in the literature. We define a structural property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time"
Q13-1022,J11-3004,0,0.273733,"Missing"
Q13-1022,P10-1001,0,0.0284387,"allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O.n2 / (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre"
Q13-1022,P06-2066,1,0.833046,"e. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O.n2 / (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2."
Q13-1022,W07-2216,1,0.907291,"odel can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O.n2 / (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2. This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restr"
Q13-1022,H05-1066,0,0.501381,"Missing"
Q13-1022,D12-1044,0,0.651878,"This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restriction to this class (Maier and Lichte, 2011; Chen-Main and Joshi, 2010), Kuhlmann and Nivre (2006) found that it has excellent coverage on standard data sets. Assuming an arc-factored model, well-nested dependency trees with block-degree  2 can be parsed in time O.n7 / using the algorithm of G´omez-Rodr´ıguez et al. (2011). Recently, Pitler et al. (2012) have shown that if an additional restriction called 1-inherit is imposed, parsing can be done in time O.n6 /, without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O.n5 / time complexity. The O.n3 / time result reported by Eisner (1997) has been obtained by exploiting more sophisticated dynamic programming techniques that ‘split’ dependency trees at the position of their heads, in order to save bookkeeping. Splitting techniques have also been exploited to speed up parsing time for other le"
Q13-1022,P98-2192,1,0.858164,"Missing"
Q13-1022,C98-2187,1,\N,Missing
Q13-1022,Q13-1002,0,\N,Missing
Q14-1032,E03-1036,0,0.113637,"r context. With this information, rule (7) can perform a check against the restrictions specified for the composition rule, and rules (3) and (6) merely need to test whether the target categories of their two antecedents match, and propagate the common target category to the conclusion. This is essentially the same solution as the one adopted in the V&W algorithm. 6.2 Support for Multi-Modal CCG The modern version of CCG has abandoned rule restrictions in favor of a new, lexicalized control mechanism in the form of modalities or slash types (Steedman and Baldridge, 2011). However, as shown by Baldridge and Kruijff (2003), every multi-modal CCG can be translated into an equivalent CCG with rule restrictions. The basic idea is to specialize the target of each category and argument for a slash type, and to reformulate the multi-modal rules as rules with restrictions that reference this information. With this simulation, our parsing algorithm can also be used as a parsing algorithm for multi-modal CCG. 3 Such restrictions can be used, for example, to impose the linguistically relevant distinction between harmonic and crossed forms of composition. 6.3 Comparison with the V&W Algorithm As already mentioned in Secti"
Q14-1032,C04-1180,0,0.0321294,"ivations into small, shareable parts. Our algorithm has the same asymptotic complexity, O.n6 /, as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algor"
Q14-1032,J07-4004,0,0.434328,"parsing algorithm for CCG, based on a new decomposition of derivations into small, shareable parts. Our algorithm has the same asymptotic complexity, O.n6 /, as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, r"
Q14-1032,P10-1035,0,0.223189,"tion is a binary rule of the (forward) form X =Y jZ Y jZ ) X=Z. This rule is easy to implement if both =Y and jZ are stored in the same item. Otherwise, we need to pass jZ to any item storing the =Y . This can be done by changing the second antecedent of rule (6) to allow a single argument jZ instead of the empty excess &quot;. The price of this change is spurious ambiguity in the derivations of the grammatical deduction system. 7 Conclusion Recently, there has been a surge of interest in the mathematical properties of CCG; see for instance Hockenmaier and Young (2008), Koller and Kuhlmann (2009), Fowler and Penn (2010) and Kuhlmann et al. (2010). Following this line, this article has revisited the parsing problem for CCG. Our work, like the polynomial-time parsing algorithm previously discovered by Vijay-Shanker and Weir (1993), is based on the idea of decomposing large CCG derivations into smaller, shareable pieces. Here we have proposed a derivation decomposition different from the one adopted by Vijay-Shanker and Weir (1993). This results in an algorithm which, in our own opinion, is simpler and easier to understand. Although we have specified only a recognition version of the algorithm, standard techniq"
Q14-1032,W08-2306,0,0.0215106,"large enough to accomodate all instances of T =X. Substitution is a binary rule of the (forward) form X =Y jZ Y jZ ) X=Z. This rule is easy to implement if both =Y and jZ are stored in the same item. Otherwise, we need to pass jZ to any item storing the =Y . This can be done by changing the second antecedent of rule (6) to allow a single argument jZ instead of the empty excess &quot;. The price of this change is spurious ambiguity in the derivations of the grammatical deduction system. 7 Conclusion Recently, there has been a surge of interest in the mathematical properties of CCG; see for instance Hockenmaier and Young (2008), Koller and Kuhlmann (2009), Fowler and Penn (2010) and Kuhlmann et al. (2010). Following this line, this article has revisited the parsing problem for CCG. Our work, like the polynomial-time parsing algorithm previously discovered by Vijay-Shanker and Weir (1993), is based on the idea of decomposing large CCG derivations into smaller, shareable pieces. Here we have proposed a derivation decomposition different from the one adopted by Vijay-Shanker and Weir (1993). This results in an algorithm which, in our own opinion, is simpler and easier to understand. Although we have specified only a re"
Q14-1032,E09-1053,1,0.810304,"instances of T =X. Substitution is a binary rule of the (forward) form X =Y jZ Y jZ ) X=Z. This rule is easy to implement if both =Y and jZ are stored in the same item. Otherwise, we need to pass jZ to any item storing the =Y . This can be done by changing the second antecedent of rule (6) to allow a single argument jZ instead of the empty excess &quot;. The price of this change is spurious ambiguity in the derivations of the grammatical deduction system. 7 Conclusion Recently, there has been a surge of interest in the mathematical properties of CCG; see for instance Hockenmaier and Young (2008), Koller and Kuhlmann (2009), Fowler and Penn (2010) and Kuhlmann et al. (2010). Following this line, this article has revisited the parsing problem for CCG. Our work, like the polynomial-time parsing algorithm previously discovered by Vijay-Shanker and Weir (1993), is based on the idea of decomposing large CCG derivations into smaller, shareable pieces. Here we have proposed a derivation decomposition different from the one adopted by Vijay-Shanker and Weir (1993). This results in an algorithm which, in our own opinion, is simpler and easier to understand. Although we have specified only a recognition version of the alg"
Q14-1032,P10-1055,1,0.787431,"e (forward) form X =Y jZ Y jZ ) X=Z. This rule is easy to implement if both =Y and jZ are stored in the same item. Otherwise, we need to pass jZ to any item storing the =Y . This can be done by changing the second antecedent of rule (6) to allow a single argument jZ instead of the empty excess &quot;. The price of this change is spurious ambiguity in the derivations of the grammatical deduction system. 7 Conclusion Recently, there has been a surge of interest in the mathematical properties of CCG; see for instance Hockenmaier and Young (2008), Koller and Kuhlmann (2009), Fowler and Penn (2010) and Kuhlmann et al. (2010). Following this line, this article has revisited the parsing problem for CCG. Our work, like the polynomial-time parsing algorithm previously discovered by Vijay-Shanker and Weir (1993), is based on the idea of decomposing large CCG derivations into smaller, shareable pieces. Here we have proposed a derivation decomposition different from the one adopted by Vijay-Shanker and Weir (1993). This results in an algorithm which, in our own opinion, is simpler and easier to understand. Although we have specified only a recognition version of the algorithm, standard techniques can be applied to obtai"
Q14-1032,D10-1119,0,0.0197398,"l, shareable parts. Our algorithm has the same asymptotic complexity, O.n6 /, as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we"
Q14-1032,Q13-1015,0,0.0178317,"gorithm has the same asymptotic complexity, O.n6 /, as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practi"
Q14-1032,P87-1012,0,0.712675,"and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We speculate that this has two main reasons: First, some authors The V&W algorithm is based on a special decomposition of CCG derivations into smaller parts that can then be shared among di"
Q14-1032,P88-1031,0,0.649303,"xicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We speculate that this has two main reasons: First, some authors The V&W algorithm is based on a special decomposition of CCG derivations into smaller parts that can then be shared among different derivat"
Q14-1032,P90-1001,0,0.719207,"s of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We speculate that this has two main reasons: First, some authors The V&W algorithm is based on a special decomposition of CCG derivations into smaller parts that can then be shared among different derivations. This sharing is the key to the polynomial runtime. In this art"
Q14-1032,J93-4002,0,0.636143,"itive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We speculate that this has two main reasons: First, some authors The V&W algorithm is based on a special decomposition of CCG derivations into smaller parts that can then be shared among different derivations. This sharing is the key to the polynomial runtime. In this article we build on the same idea, bu"
Q14-1032,W12-3127,0,0.0143754,"as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We"
Q14-1032,P88-1034,0,0.465222,"onential-time parser for CCG, from which we derive our polynomial-time parser in Section 4. Section 5 further simplifies the algorithm and proves its correctness. We then provide a discussion of our algorithm and possible extensions in Section 6. Section 7 concludes the article. 405 Transactions of the Association for Computational Linguistics, 2 (2014) 405–418. Action Editor: Mark Steedman. c Submitted 4/2014; Revised 8/2014; Published 10/2014. 2014 Association for Computational Linguistics. 2 Combinatory Categorial Grammar We assume basic familiarity with CCG in general and the formalism of Weir and Joshi (1988) in particular. In this section we set up our terminology and notation. A CCG has two main parts: a lexicon that associates words with categories, and rules that specify how categories can be combined into other categories. Together, these components give rise to derivations such as the one shown in Figure 1. 2.1 Lexicon The CCG lexicon is a finite set of word–category pairs w WD X.1 Categories are built from a finite set of atomic categories and two binary operators: forward slash (=) and backward slash (=). Atomic categories represent the syntactic types of complete constituents; they includ"
Q15-1040,D11-1031,0,0.267787,"85.33 90.93 87.32 89.09 92.90 89.67 91.26 78.60 72.93 75.66 This work 79.63 86.43 79.17 82.64 88.68 83.94 86.24 74.58 65.96 70.01 UP UR UF Auli and Lopez 93.98 This work 93.06 86.65 89.74 Table 2: Parsing results on the SDP data (Oepen, 2014) and on CCG dependencies (Hockenmaier and Steedman, 2005). The metrics reported are precision, recall and F-score on labeled (LP, LR, LF) and unlabeled (UP, UR, UF) dependencies. LF is the averaged LF score that was used to rank systems in the SemEval-2015 Task on Broad-Coverage Semantic Dependency Parsing. References are to Du et al. (2015b) (Peking) and Auli and Lopez (2011). 564 4.4 Uniqueness of Derivations Besides being sound and complete, the deduction system also has the property that it assigns a unique derivation to every noncrossing dependency graph. The proof, again, is by induction on the size. To illustrate the argument, suppose that we want to construct a min–max-connected graph H on some interval Œi; k. The only rules that can be used to derive the corresponding item (of type ! ) are R01 and R05. In both rules, the graph corresponding to the second antecedent must be a min–max-covered graph ( ) on some interval Œj; k. This means that the vertex j i"
Q15-1040,W06-2925,0,0.020197,"297 in the OEIS. These graphs are the target representations of Link Grammar (Sleator and Temperley, 1993). 5 Practical Parsing While the main focus of this paper is theoretical, in this section we extend our parsing algorithm into a practical parser. In the context of our general model (Section 2), this requires two additional components: a feature representation and a training algorithm. 5.1 Features We use the arc-based features of TurboParser (Martins et al., 2009), which descend from several other feature models from the literature on syntactic dependency parsing (McDonald et al., 2005a; Carreras et al., 2006; Koo and Collins, 2010). In these models, the feature vector for an arc i ! j represents information about various combinations of the exact forms, lemmas and part-of-speech tags of the words at positions i and j ; the tags of the immediately surrounding words and the words between i and j ; as well as the length of the arc and its direction. To support parsing to labeled dependency graphs, we additionally conjoin some of these features with the arc label. For details we refer to the source code. 5.2 Training To learn the feature weights in the weight vector from data we use online passive–ag"
Q15-1040,D07-1101,0,0.0693644,"rrent loss and being close to the old weight vector. We use Hamming loss, defined as the number of (labeled) arcs that are excluO Following custom practice, we sive to either G or G. apply weight vector averaging (Freund and Schapire, 1999; Collins, 2002). 5.3 Parsing Experiments We report parsing experiments on the four data sets described in Section 3.3.1 and discuss their results. We use gold-standard lemmas and part-of-speech tags, train each parser for 10 epochs, and report results for the final model on the test data. We use the splits recommended for the respective data sets. Following Carreras (2007), prior to training we transform each dependency graph in the training data to a closest noncrossing dependency graph.6 In a prestudy using the DM development data we found the best value for the tradeoff parameter C to be 0:01. 5.3.1 Results and Discussion The experimental results are shown in Table 2. For the SDP data, we report standard metrics used in the SemEval task (Oepen et al., 2015): precision, recall, and F1 on labeled dependencies. For the CCG data, we report the same metrics for unlabeled dependencies; these take into account only the two dependent words but not the lexical catego"
Q15-1040,W02-1001,0,0.255179,"he sum vectors of where ˚.x; G/ and ˚.x; G/ the arc-specific feature vectors for the dependency O and the scalar  is computed as graphs G and G, 0 1 q O O B w  .˚.x; G/ ˚.x; G// C `.G; G/ C min @C; A: 2 O ˚.x; G/ ˚.x; G/ O is a user-defined loss funcIn this formula, `.G; G/ tion and C > 0 is a parameter that controls the tradeoff between optimizing the current loss and being close to the old weight vector. We use Hamming loss, defined as the number of (labeled) arcs that are excluO Following custom practice, we sive to either G or G. apply weight vector averaging (Freund and Schapire, 1999; Collins, 2002). 5.3 Parsing Experiments We report parsing experiments on the four data sets described in Section 3.3.1 and discuss their results. We use gold-standard lemmas and part-of-speech tags, train each parser for 10 epochs, and report results for the final model on the test data. We use the splits recommended for the respective data sets. Following Carreras (2007), prior to training we transform each dependency graph in the training data to a closest noncrossing dependency graph.6 In a prestudy using the DM development data we found the best value for the tradeoff parameter C to be 0:01. 5.3.1 Resul"
Q15-1040,P15-1149,0,0.515179,"tion of our parsing framework to dependency graphs with pagenumber at most k and show that the resulting optimization problem is NP-hard for k  2. 1 Introduction Dependency parsers provide lightweight representations for the syntactic and the semantic structure of natural language. Syntactic dependency parsing (Kübler et al., 2009) has been an extremely active research area for the last decade or so, resulting in accurate and efficient parsers for a wide range of languages. Semantic dependency parsing has only recently been addressed in the literature (Oepen et al., 2014; Oepen et al., 2015; Du et al., 2015a). Syntactic dependency parsing has been formalized as the search for maximum spanning trees in weighted digraphs (McDonald et al., 2005b). For semantic dependency parsing, where target representations are not necessarily tree-shaped, it is natural to generalize this view to maximum acyclic subgraphs, with or without the additional requirement of weak connectivity (Schluter, 2014). While a maximum spanning tree of a weighted digraph can be found in polynomial time (Tarjan, 1977), computing a maximum acyclic subgraph is intractable, and even good approximate solutions are hard to find (Guruswa"
Q15-1040,S15-2154,0,0.262958,"tion of our parsing framework to dependency graphs with pagenumber at most k and show that the resulting optimization problem is NP-hard for k  2. 1 Introduction Dependency parsers provide lightweight representations for the syntactic and the semantic structure of natural language. Syntactic dependency parsing (Kübler et al., 2009) has been an extremely active research area for the last decade or so, resulting in accurate and efficient parsers for a wide range of languages. Semantic dependency parsing has only recently been addressed in the literature (Oepen et al., 2014; Oepen et al., 2015; Du et al., 2015a). Syntactic dependency parsing has been formalized as the search for maximum spanning trees in weighted digraphs (McDonald et al., 2005b). For semantic dependency parsing, where target representations are not necessarily tree-shaped, it is natural to generalize this view to maximum acyclic subgraphs, with or without the additional requirement of weak connectivity (Schluter, 2014). While a maximum spanning tree of a weighted digraph can be found in polynomial time (Tarjan, 1977), computing a maximum acyclic subgraph is intractable, and even good approximate solutions are hard to find (Guruswa"
Q15-1040,P99-1059,0,0.452263,"Missing"
Q15-1040,J13-4002,0,0.0732407,"Missing"
Q15-1040,J99-4004,0,0.150637,"tem can be modified to parse into (and count) various other classes of noncrossing graphs. For example, we can adapt our system to find a maximum noncrossing acyclic subgraph under the additional restriction that this subgraph should be weakly connected (Schluter, 2015). To do so we distinguish between two types of bland subgraphs, (a) weakly connected graphs and (b) graphs with exactly two weakly connected components, and adapt the inference rules and goal items. The change can be implemented without affecting the asymptotic runtime of the algorithm. 5 Using the parlance of semiring parsing (Goodman, 1999), we switch from the max–plus semiring to the counting semiring. 565 Note that when we take the modified deduction system and consider undirected edges instead of directed arcs, we obtain an algorithm for finding maximum connected noncrossing graphs, which are counted by sequence A007297 in the OEIS. These graphs are the target representations of Link Grammar (Sleator and Temperley, 1993). 5 Practical Parsing While the main focus of this paper is theoretical, in this section we extend our parsing algorithm into a practical parser. In the context of our general model (Section 2), this requires"
Q15-1040,J07-3004,0,0.597079,"troduce the noncrossing condition, compare it to other structural constraints from the literature, and study its empirical coverage (Section 3). We then present our parsing algorithm (Section 4). To turn this algorithm into a practical parser, we combine it with an off-the-shelf feature model and online training (Section 5); the source code of our system is released with this paper.1 We evaluate the performance of our parser on four linguistic data sets: those used in the recent SemEval task on semantic dependency parsing (Oepen et al., 2015), and the dependency graphs extracted from CCGbank (Hockenmaier and Steedman, 2007). Finally, we explore the limits of our approach by showing that finding the maximum acyclic subgraph under a natural generalization of the noncrossing condition, pagenumber at most k, is NPhard for k  2 (Section 6). We conclude the paper by discussing related and future work (Section 7). 1 https://github.com/liu-nlp/gamma 559 Transactions of the Association for Computational Linguistics, vol. 3, pp. 559–570, 2015. Action Editor: Joakim Nivre. Submission batch: 9/2015;Published 11/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. arg3 arg1 bv The a"
Q15-1040,P10-1001,0,0.0416323,"graphs are the target representations of Link Grammar (Sleator and Temperley, 1993). 5 Practical Parsing While the main focus of this paper is theoretical, in this section we extend our parsing algorithm into a practical parser. In the context of our general model (Section 2), this requires two additional components: a feature representation and a training algorithm. 5.1 Features We use the arc-based features of TurboParser (Martins et al., 2009), which descend from several other feature models from the literature on syntactic dependency parsing (McDonald et al., 2005a; Carreras et al., 2006; Koo and Collins, 2010). In these models, the feature vector for an arc i ! j represents information about various combinations of the exact forms, lemmas and part-of-speech tags of the words at positions i and j ; the tags of the immediately surrounding words and the words between i and j ; as well as the length of the arc and its direction. To support parsing to labeled dependency graphs, we additionally conjoin some of these features with the arc label. For details we refer to the source code. 5.2 Training To learn the feature weights in the weight vector from data we use online passive–aggressive training as des"
Q15-1040,P09-1039,0,0.0358775,"directed edges instead of directed arcs, we obtain an algorithm for finding maximum connected noncrossing graphs, which are counted by sequence A007297 in the OEIS. These graphs are the target representations of Link Grammar (Sleator and Temperley, 1993). 5 Practical Parsing While the main focus of this paper is theoretical, in this section we extend our parsing algorithm into a practical parser. In the context of our general model (Section 2), this requires two additional components: a feature representation and a training algorithm. 5.1 Features We use the arc-based features of TurboParser (Martins et al., 2009), which descend from several other feature models from the literature on syntactic dependency parsing (McDonald et al., 2005a; Carreras et al., 2006; Koo and Collins, 2010). In these models, the feature vector for an arc i ! j represents information about various combinations of the exact forms, lemmas and part-of-speech tags of the words at positions i and j ; the tags of the immediately surrounding words and the words between i and j ; as well as the length of the arc and its direction. To support parsing to labeled dependency graphs, we additionally conjoin some of these features with the a"
Q15-1040,P05-1012,0,0.779648,"-hard for k  2. 1 Introduction Dependency parsers provide lightweight representations for the syntactic and the semantic structure of natural language. Syntactic dependency parsing (Kübler et al., 2009) has been an extremely active research area for the last decade or so, resulting in accurate and efficient parsers for a wide range of languages. Semantic dependency parsing has only recently been addressed in the literature (Oepen et al., 2014; Oepen et al., 2015; Du et al., 2015a). Syntactic dependency parsing has been formalized as the search for maximum spanning trees in weighted digraphs (McDonald et al., 2005b). For semantic dependency parsing, where target representations are not necessarily tree-shaped, it is natural to generalize this view to maximum acyclic subgraphs, with or without the additional requirement of weak connectivity (Schluter, 2014). While a maximum spanning tree of a weighted digraph can be found in polynomial time (Tarjan, 1977), computing a maximum acyclic subgraph is intractable, and even good approximate solutions are hard to find (Guruswami et al., 2011). In this paper we therefore address maximum acyclic subgraph parsing under the restriction that the subgraph should be n"
Q15-1040,H05-1066,0,0.512223,"Missing"
Q15-1040,J03-1006,0,0.103023,"Missing"
Q15-1040,S14-2008,1,0.859273,"Missing"
Q15-1040,S15-2153,1,0.851081,"Missing"
Q15-1040,Q13-1002,0,0.159965,"Missing"
Q15-1040,Q14-1004,0,0.070595,"Missing"
Q15-1040,C08-1095,0,0.802283,"most k, is NPhard for k  2 (Section 6). We conclude the paper by discussing related and future work (Section 7). 1 https://github.com/liu-nlp/gamma 559 Transactions of the Association for Computational Linguistics, vol. 3, pp. 559–570, 2015. Action Editor: Joakim Nivre. Submission batch: 9/2015;Published 11/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. arg3 arg1 bv The arg1 gene thus can arg1 arg2 bv arg1 prevent a plant from arg2 fertilizing itself Figure 1: A noncrossing dependency graph (Oepen, 2014, DM #20209003). Using the terminology of Sagae and Tsujii (2008), the vertices the, thus, a and from are roots; of these, thus, a and from are covered by arcs. 2 Background Dependency parsing is the task of mapping a natural language sentence into a formal representation of its syntactic or semantic structure in the form of a dependency graph. arc-weighted digraph. They start from the complete graph on n vertices where each arc i ! j carries a real-valued weight wij , defined as a dot product wij D w  ˚.x; i ! j / A directed graph or digraph is a pair G D .V; A/ where V is a set of vertices and A  V  V is a set of arcs. We consider an arc .u; v/ to be d"
Q15-1040,W14-2412,0,0.115792,"e last decade or so, resulting in accurate and efficient parsers for a wide range of languages. Semantic dependency parsing has only recently been addressed in the literature (Oepen et al., 2014; Oepen et al., 2015; Du et al., 2015a). Syntactic dependency parsing has been formalized as the search for maximum spanning trees in weighted digraphs (McDonald et al., 2005b). For semantic dependency parsing, where target representations are not necessarily tree-shaped, it is natural to generalize this view to maximum acyclic subgraphs, with or without the additional requirement of weak connectivity (Schluter, 2014). While a maximum spanning tree of a weighted digraph can be found in polynomial time (Tarjan, 1977), computing a maximum acyclic subgraph is intractable, and even good approximate solutions are hard to find (Guruswami et al., 2011). In this paper we therefore address maximum acyclic subgraph parsing under the restriction that the subgraph should be noncrossing, which informally means that its arcs can be drawn on the half-plane above the sentence in such a way that no two arcs cross (and without changing the order of the words). The main contribution of this paper is an algorithm that finds a"
Q15-1040,S15-1031,0,0.475506,"it can be drawn on the plane (not half-plane!) in such a way that no arcs cross each other. 561 More specifically, a dependency tree is projective if and only if it is noncrossing and its root is not “covered”, meaning that there is no arc i ! j such that the root lies properly between i and j . Sagae and Tsujii (2008) propose a generalization of this two-part characterization of projectivity to graphs. We find that the noncrossing condition alone is more practical. For example, the dependency graph in Figure 1 is not projective in the sense of Sagae and Tsujii (2008); but it is noncrossing. Schluter (2015) uses the term “projective” with the same meaning as our term “noncrossing.” Pagenumber A natural generalization of the noncrossing condition is to relax property 2 of the arc diagram characterization and allow arcs to be drawn also in the lower half-plane bounded by the vertex line, or in any of some fixed number k of half-planes. These half-planes may be thought of as the pages of a book, with the vertex line corresponding to the book’s spine, and the embedding of a graph into such a structure is known as a book embedding (Bernhart and Kainen, 1979). A graph that permits a crossing-free book"
Q15-1040,1993.iwpt-1.22,0,0.157218,"with exactly two weakly connected components, and adapt the inference rules and goal items. The change can be implemented without affecting the asymptotic runtime of the algorithm. 5 Using the parlance of semiring parsing (Goodman, 1999), we switch from the max–plus semiring to the counting semiring. 565 Note that when we take the modified deduction system and consider undirected edges instead of directed arcs, we obtain an algorithm for finding maximum connected noncrossing graphs, which are counted by sequence A007297 in the OEIS. These graphs are the target representations of Link Grammar (Sleator and Temperley, 1993). 5 Practical Parsing While the main focus of this paper is theoretical, in this section we extend our parsing algorithm into a practical parser. In the context of our general model (Section 2), this requires two additional components: a feature representation and a training algorithm. 5.1 Features We use the arc-based features of TurboParser (Martins et al., 2009), which descend from several other feature models from the literature on syntactic dependency parsing (McDonald et al., 2005a; Carreras et al., 2006; Koo and Collins, 2010). In these models, the feature vector for an arc i ! j repres"
S14-2008,D12-1133,0,0.0149346,"ained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given in the LF column. In cases where a team submitted two runs to a track, only the highestranked score is included in the t"
S14-2008,W06-2920,0,0.101128,"em in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz & Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier simil"
S14-2008,de-marneffe-etal-2006-generating,0,0.0702807,"Missing"
S14-2008,oepen-lonning-2006-discriminant,1,0.758648,"antic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czec"
S14-2008,W09-1201,1,0.855031,"Missing"
S14-2008,J05-1004,0,0.129381,"ependency graphs for Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in o"
S14-2008,P06-1055,0,0.0124328,"e of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given i"
S14-2008,hajic-etal-2012-announcing,1,0.772691,"Missing"
S14-2008,W12-3602,1,0.885947,"yses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical ("
S14-2008,C08-1095,0,0.478753,".27 to 75.89 and the corresponding scores across systems are 88.64 for PAS, 84.95 for DM, and 67.52 for PCEDT. While these scores are consistently higher than in the closed track, the differences are small. In fact, for each of the three teams that submitted to both tracks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not exceed two points LF. 7 dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae & Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency par"
S14-2008,P10-5006,0,0.0693315,"ple inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task definition does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words. Finally, a third closely related area of much current interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to most work in this tradition, our SDP target representations aim to be task- and domainindependent, though at least part of this generality comes at the expense of ‘completeness’ in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure"
S14-2008,J93-2004,0,0.0590496,"t of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP 2014),1 seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom? For English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; Marcus et al., 1993). These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure 1 below for the WSJ sentence: Task 8 at SemEval 2014 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure"
S14-2008,P07-1031,0,0.0115637,"ersely, in PCEDT the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.7 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase–internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PCEDT, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PCEDT annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely ass"
S14-2008,meyers-etal-2004-annotating,0,0.0465249,"Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another"
S14-2008,S14-2056,1,0.893424,"pBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees). PCEDT bi-lexical dependencies in this task have been extracted from the t-trees. The specifics of the PCE"
S14-2008,C10-1011,0,\N,Missing
S14-2008,S14-2080,0,\N,Missing
S14-2008,S14-2082,0,\N,Missing
S14-2008,J02-3001,0,\N,Missing
S14-2008,W15-0128,1,\N,Missing
S14-2008,D07-1096,0,\N,Missing
S14-2008,cinkova-2006-propbank,0,\N,Missing
S14-2068,S14-2080,0,0.116235,"Missing"
S14-2068,P99-1059,0,0.0728974,"algorithm. The remaining components of our system will be described in Section 3. 395 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 395–399, Dublin, Ireland, August 23-24, 2014. Items: i j i j Rules: i j i i j 1 j Axioms: 1i j n j k i j j attach-r i i i i i Goal: 1 n k complete-r k i k Figure 1: The Eisner algorithm for building the packed forest of all projective dependency trees with n nodes. Only the rightward versions of attach and complete are shown here. 2.1 The Eisner Algorithm We recall the algorithm for projective dependency parsing by Eisner and Satta (1999). The declarative specification of this algorithm in terms of a deduction system (Shieber et al., 1995) is given in Figure 1. The algorithm uses four types of items, , , , and , and two types of inference rules called attach and complete. These rules can be interpreted as operations on graphs: An attach rule concatenates two graphs and adds one of two possible edges—from the left endpoint of the first graph to the right endpoint of the second graph, or vice versa. Similarly, a complete rule fuses two graphs by unifying the right endpoint of the first with the left endpoint of the second. The a"
S14-2068,P05-1012,0,0.358473,"Missing"
S14-2068,S14-2008,1,0.804837,"Missing"
S14-2068,W02-1001,0,\N,Missing
S15-2153,D12-1133,0,0.036307,"semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example—provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of the SDP 2015 test data.11 To simplify participation in the open track, the organizers prepared ready-touse ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). Finally, to more directly gauge the the contributions of syntactic structure on the semantic dependency parsing problem, an idealized gold track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtaine"
S15-2153,cinkova-2006-propbank,1,0.772792,".6993 .5743 .6719 − .5630 .5675 .5490 − Table 2: Pairwise F1 similarities, including punctuation (upper right diagonals) or not (lower left). Frame or sense distinctions are a new property in SDP 2015 and currently are only available for the English DM and PSD data. Table 1 reveals a stark difference in granularity: DM limits itself to argument structure distinctions that are grammaticized, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames; PSD, on the other hand, draws on the much richer sense inventory of the EngValLex database (Cinková, 2006). Accordingly, the two target representations represent quite different challenges for the predicate disambiguation sub-task of SDP 2015. Finally, in Table 2 we seek to quantify pairwise structural similarity between the three representations in terms of unlabeled dependency F1 (dubbed UF in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discardin"
S15-2153,de-marneffe-etal-2006-generating,0,0.0580061,"Missing"
S15-2153,S14-2080,0,0.42538,"ably because the additional dependency parser they used was trained on data from the target domain. 7 Overview of Approaches Table 5 shows a summary of the tracks in which each submitted system participated, and Table 6 shows an overview of approaches and additionally used resources. All the teams except In-House submitted results for cross-lingual data (Czech and Chinese). Teams except Lisbon also tackled with predicate disambiguation. Only Turku participated in the Gold track. The submitted teams explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) tra"
S15-2153,hajic-etal-2012-announcing,1,0.91158,"Missing"
S15-2153,W12-3602,1,0.93906,"ctionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PSD. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PSD on the other hand. Linguistic Comparison Among other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 1 above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks;9 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PSD, they function as predicates in PAS, though do not always serve as ‘loca"
S15-2153,P10-5006,0,0.0554855,"R-arg ACT-arg PAT-arg RSTR EXT RSTR CONJ.m APPS.m ADDR-arg APPS.m CONJ.m A similar technique is almost impossible to apply to other crops , such as cotton , soybeans _ _ _ ev-w218f2 _ _ _ ev-w119f2 _ _ _ _ _ _ _ _ _ CONJ.m and _ rice . _ _ (c) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PSD). Figure 1: Sample semantic dependency graphs for Example (1). sentence’ semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Finally, a third related area of much interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to much work in this tradition, our SDP target representations aim to be task- and domain-independent. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure 1), showing what are called the DM, PAS, and PSD semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (o"
S15-2153,J93-2004,0,0.0679778,"stitute of Informatics, Tokyo Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics • Stanford University, Center for the Study of Language and Information ♠ ◦ sdp-organizers@emmtee.net Abstract more general graph processing, to thus enable a more direct analysis of Who did What to Whom? Extending the very similar predecessor task SDP 2014 (Oepen et al., 2014), we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al., 1993) for English, as well as comparable resources for Chinese and Czech. Figure 1 below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence: Task 18 at SemEval 2015 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic de"
S15-2153,S14-2082,0,0.0634842,"ormation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) transition-based dependency graph parsing in the spirit of Titov et al. (2009) (Du et al., 2014) extended with weighted tree approximation, parser ensemble (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble sequence labeling for argument detection for each predicate, SVM classifiers for top node recognition and sense prediction ERG & Enju companion — — — companion Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structure"
S15-2153,meyers-etal-2004-annotating,0,0.0555434,"the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:"
S15-2153,S14-2056,1,0.858363,"ndencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. Advancing in-depth comparison of representations and underlying design decisions, in fact, is among the moand other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. 916 tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived f"
S15-2153,S14-2008,1,0.363094,"Missing"
S15-2153,J05-1004,0,0.359612,"s preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-"
S15-2153,P07-1031,0,0.0187644,"ersely, in PSD the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.10 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase– internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PSD, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PSD annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely assigns"
W04-3320,W00-2027,0,0.0286812,"in the constraint framework. We have started work into this direction by implementing a prototypical constraint parser for LTAG, and investigating its properties. The implementation can be done in a straightforward way by transforming the axiomatization of well-ordered derivation trees that was given in Section 3 into a constraint satisfaction problem along the lines of Duchier (2003). The resulting parser is available as a module for the XDG system (Debusmann, 2003). Preliminary evaluation of the parser using the XTAG grammar shows that it is not competitive with state-ofthe-art TAG parsers (Sarkar, 2000) in terms of run-time; however, this measure is not the most significant one for an evaluation of the constraint-based approach anyway. More importantly, a closer look on the search spaces ex6 The parsing problem of LTAG can be decided in time O(n6 ). plored by the parser indicates that the inferences drawn from the axiomatic principles are not strong enough to rule out branches of the search that lead to only inconsistent assignments of the problem variables. Future work needs to closely investigate this issue; ideally, we would arrive at an implementation that enumerates all well-ordered der"
W04-3320,P01-1024,1,0.830681,"0/ (c2 , ε) 0/ (d1 , ε) 0/ (d2 , ε) 0/ (b1 , ε) 0/ (b2 , ε) 0/ (b2 , 2) 0/ inserted pasted 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ {b1 , c2 } {a1 } 0/ {c2 } 0/ {d2 } 0/ {a2 } 0/ {c1 } 0/ {d1 } 0/ {a2 , b2 , c1 , c2 , d1 } 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ below 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ 0/ {a1 } {a2 } {c1 } {c2 } {d1 } {d2 } {a1 , a2 , b1 , b2 , c1 , c2 , d1 , d2 } {a2 , b2 , c1 , c2 , d1 } {b1 , b2 , c1 , c2 } Figure 2: Yields in the analysis of string aabbccdd employ an existing constraint-based parser for Topological Dependency Grammar (Duchier and Debusmann, 2001). In light of the fact that surface realization is an NP-complete problem, the efficiency of this parser is quite remarkable. One of the major questions for a description-based approach to LTAG parsing is, whether the benign computational properties of existing, derivationbased parsers for LTAG6 can be exploited even in the constraint framework. We have started work into this direction by implementing a prototypical constraint parser for LTAG, and investigating its properties. The implementation can be done in a straightforward way by transforming the axiomatization of well-ordered derivation"
W06-1517,C88-1001,0,0.195879,"that has the well-nestedness constraint hardwired is preferable over one that has not. The results of this paper can be summarized as follows: Derivations in lexicalized multi-component TAGs (Weir, 1988; Kallmeyer, 2005), in which a single adjunction adds a set of elementary trees, either induce exactly the same dependency structures as TAG, or induce all structures of bounded gap degree, even non-well-nested ones. This depends on the decision whether one takes ‘lexicalized’ to mean ‘one lexical anchor per tree’, or ‘one lexical anchor per tree set’. In contrast, multi-foot extensions of TAG (Abe, 1988; Hotz and Pitsch, 1996), where a single elementary tree may have more than one foot node, only induce well-nested dependency structures of bounded gap degree. Thus, from the dependency point of view, they constitute the structurally more conservative extension of TAG. 121 Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 121–126, c Sydney, July 2006. 2006 Association for Computational Linguistics 2 We start with a presentation of the dependency view on TAG that constitutes the basis for our work, and introduce the relevant terminology. The m"
W06-1517,P01-1018,0,0.019681,"derivable. In the former case, MCTAG has the same structural limits as standard TAG; in the latter case, even non-well-nested dependency structures are induced.  The multi-foot extension CCFG (and its equivalent RNRG) is restricted to well-nested dependency structures, but in contrast to TAG, it can induce structures with any bounded gap degree. The rank of a grammar is an upper bound on the gap degree of the dependency structures it induces. Since the extensions inherent to MCTAG and CCFG are orthogonal, it is possible to combine them: Multi-Component Multi-Foot TAG (MMTAG) as described by Chiang (2001) allows to simultaneously adjoin sets of trees, where each tree may have multiple foot nodes. The structural limitations of the dependency structures inducible by MCTAG and CCFG generalize to MMTAG as one would expect. As in the case of MCTAG, there are two different understandings of how a dependency structure is induced by an MMTAG. Under the ‘one anchor per component’ perspective, MM TAG, just like CCFG , derives well-nested structures of bounded gap-degree. Under the ‘one anchor per tree set’ perspective, just like MCTAG, it also derives non-well-nested gap-bounded structures. Acknowledgem"
W06-1517,2005.jeptalnrecital-court.13,0,0.0373707,") to MCTAGs. 122 8 ; ˆ ˆ < B1 A;  a B1 C1 B2 C2 ˆ ˆ : b 9 &gt; B2; &gt; = D &gt; &gt; ; 9 &gt; C2; &gt; = 8 ; ˆ ˆ < C1 ˆ ˆ : c E &gt; &gt; ; D; E; d e a b c d e Figure 2: An MCTAG and a not well-nested dependency structure derived by it. 3.1 One anchor per component If we commit to the view that each component of a tree set introduces a separate lexical anchor and its syntactic dependencies, the dependency structures induced by MCTAG are exactly the structures induced by TAG. In particular, each node in the derivation tree, and therefore each token in the dependency tree, corresponds to a single elementary tree. As Kallmeyer (2005) puts it, one can then consider an MCTAG as a TAG G ‘where certain derivation trees in G are disallowed since they do not satisfy certain constraints.’ The ability of MCTAG to perform multiple adjunctions simultaneously allows one to induce more complex sets of dependency structures—each individual structure is limited as in the case of standard TAG. 3.2 One anchor per tree set If, on the other hand, we take a complete tree set as the level on which syntactic dependencies are specified, MCTAGs can induce a larger class of dependency structures. Under this perspective, tokens in the dependency"
W06-1517,P06-2066,1,0.849766,"Missing"
W06-1517,P92-1012,0,0.43705,"Missing"
W09-3811,P06-2066,1,0.802427,"Missing"
W09-3811,W07-2216,0,0.0168965,"Missing"
W09-3811,W06-2932,0,0.0595059,"Missing"
W09-3811,P08-1108,1,0.817536,"Missing"
W09-3811,W06-2933,1,0.712712,"Missing"
W09-3811,J08-4003,1,0.159262,"Missing"
W09-3811,P09-1040,1,0.77487,"makes the treatment of non-projectivity central for accurate dependency parsing. Unfortunately, parsing with unrestricted non-projective structures is a hard problem, for which exact inference is not possible in polynomial time except under drastic independence assumptions (McDonald and Satta, 2007), and most data-driven parsers therefore use approximate methods (Nivre et al., 2006; McDonald et al., 2006). One recently explored approach is to perform online reordering by swapping adjacent words of the input sentence while building the dependency structure. Using this technique, the system of Nivre (2009) processes unrestricted non-projective structures with state-ofthe-art accuracy in observed linear time. The normal procedure for training a transitionbased parser is to use an oracle that predicts an Background The fundamental reason why sentences with nonprojective dependency trees are hard to parse is that they contain dependencies between non-adjacent substructures. The basic idea in online reordering is to allow the parser to swap input words so that all dependency arcs can be constructed between adjacent subtrees. This idea is implemented in the transition system proposed by Nivre (2009)"
W11-2902,J09-4009,0,0.0211078,"and T2∗ are interpretations of the same derivation tree t ∈ L(G). As above, we can parse synchronously by parsing separately for the two interpretations and intersecting the results. This yields a parsing complexity for SCFG parsing of O(nm+1 · nm+1 ), where n1 and n2 are the lengths 1 2 of the input strings and m is the rank of the RTG G. Unlike in the monolingual case, this is now consistent with the result that the membership problem of SCFGs is NP-complete (Satta and Peserico, 2005). The reason for the intractability of SCFG parsing is that SCFGs, in general, cannot be binarized. However, Huang et al. (2009) define the class of binarizable SCFGs, which can be brought into a weakly equivalent normal form in which all production rules are binary and the membership problem can be solved in time O(n31 · n32 ). The key property of binarizable SCFGs, in our terms, is that if r is any production rule pair of the SCFG, h1 (r) and h2 (r) can be chosen in such a way that they can be transformed into each other by locally swapping the subterms of a node. For instance, an SCFG rule pair hA → A1 A2 A3 A4 , B → B3 B4 B2 B1 i can be represented by h1 (r) = (x1 • x2 ) • (x3 • x4 ) and h2 (r) = (x4 • x3 ) • (x1 •"
W11-2902,P89-1018,0,0.0582684,"her formalisms, such as STSG, the nonterminals of Ga generally record non7 terminals of G and positions in the input objects, as encoded in the nonterminals of D(a1 ), . . . , D(an ); the spans [i, k] occurring in CKY parse items simply happen to be the nonterminals of the D(a) for the string algebra. In fact, we maintain that the fundamental purpose of a chart is to act as a device for generating the set of derivation trees for an input. This treegenerating nature of parse charts is made explicit by modeling them directly as RTGs; the well-known view of parse charts as context-free grammars (Billot and Lang, 1989) captures the same intuition, but abuses context-free grammars (which are primarily string-generating devices) as tree description formalisms. One difference between the two views is that regular tree languages are closed under intersection, which means that parse charts that are modeled as RTGs can be easily restricted by external constraints (see Koller and Thater (2010) for a related approach), whereas this is hard in the context-free view. 4.4 S0,7 VP1,7 VP1,7 NP2,7 N3,7 VP1,4 NP2,4 PP4,7 → → → → → → → → r1 (NP0,1 , VP1,7 ) r3 (V1,2 , NP2,7 ) r5 (VP1,4 , PP4,7 ) r2 (Det2,3 , N3,7 ) r4 (N3,"
W11-2902,J07-2003,0,0.0716943,"which are interpreted in arbitrary algebras. We obtain generic parsing algorithms by exploiting closure properties of regular tree languages. 1 Introduction Over the past years, grammar formalisms that relate pairs of grammatical structures have received much attention. These formalisms include synchronous grammars (Lewis and Stearns, 1968; Shieber and Schabes, 1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of formalisms are available, it is useful to take a step back and look for a generalized model that explains the precise formal relationship between them. There is a long tradition of such research on monolingual"
W11-2902,E09-1053,1,0.843824,"(LCFRS, VijayShanker et al. (1987)). LCFRSs are essentially GCFGs with a “yield” homomorphism that maps objects of A to strings or tuples of strings. Therefore every grammar formalism that can be seen as an LCFRS, including certain dependency grammar formalisms (Kuhlmann, 2010), can be phrased as string-generating IRTGs. One particular advantage that our framework has over LCFRS is that we do not need to impose a bound on the length of the string tuples. This makes it possible to model formalisms such as combinatory categorial grammar (Steedman, 2001), which may be arbitrarily discontinuous (Koller and Kuhlmann, 2009). 7 Conclusion In this paper, we have defined interpreted RTGs, a grammar formalism that generalizes over a wide range of existing formalisms, including various synchronous grammars, tree transducers, and LCFRS. We presented a generic parser for IRTGs; to apply it to a new type of IRTG, we merely need to define how to compute decomposition grammars D(a) for input objects a. This makes it easy to define synchronous grammars that are heterogeneous both in the grammar formalism and in the objects that each of its dimensions describes. The purpose of our paper was to pull together a variety of exi"
W11-2902,P10-1004,1,0.838507,"as a device for generating the set of derivation trees for an input. This treegenerating nature of parse charts is made explicit by modeling them directly as RTGs; the well-known view of parse charts as context-free grammars (Billot and Lang, 1989) captures the same intuition, but abuses context-free grammars (which are primarily string-generating devices) as tree description formalisms. One difference between the two views is that regular tree languages are closed under intersection, which means that parse charts that are modeled as RTGs can be easily restricted by external constraints (see Koller and Thater (2010) for a related approach), whereas this is hard in the context-free view. 4.4 S0,7 VP1,7 VP1,7 NP2,7 N3,7 VP1,4 NP2,4 PP4,7 → → → → → → → → r1 (NP0,1 , VP1,7 ) r3 (V1,2 , NP2,7 ) r5 (VP1,4 , PP4,7 ) r2 (Det2,3 , N3,7 ) r4 (N3,4 , PP4,7 ) r3 (V1,2 , NP2,4 ) r2 (Det2,3 , N3,4 ) r6 (P4,5 , NP5,7 ) NP5,7 NP0,1 V1,2 Det2,3 N3,4 P4,5 Det5,6 N6,7 → → → → → → → → r2 (Det5,6 , N6,7 ) r7 r11 r8 r9 r12 r8 r10 Figure 5: A “parse chart” RTG for the sentence “Sue watches the man with the telescope”. 5 Membership and Binarization A binarization transforms an m-ary grammar into an equivalent binary one. Binari"
W11-2902,P03-2041,0,0.0299975,"work that generalizes a variety of monolingual and synchronous grammar formalisms for parsing and translation. Our framework is based on regular tree grammars that describe derivation trees, which are interpreted in arbitrary algebras. We obtain generic parsing algorithms by exploiting closure properties of regular tree languages. 1 Introduction Over the past years, grammar formalisms that relate pairs of grammatical structures have received much attention. These formalisms include synchronous grammars (Lewis and Stearns, 1968; Shieber and Schabes, 1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of formalisms are avai"
W11-2902,P10-1109,0,0.0401155,"(a1 , . . . , an ) = bin−1 (L(H)); this is where the exponential blowup can happen. The constructions in Sections 5.1 and 5.2 are both special cases of this generalized approach, which however also maintains a clear connection to the strong generative capacity. It is not obvious to us that it is necessary that the homomorphisms h2i must be delabelings for the membership algorithm to be optimal. Exploring this landscape, which ties in with the very active current research on binarization, is an interesting direction for future research. 6 into a more powerful algebra. This approach is taken by Maletti (2010), who uses an ordinary tree homomorphism to map a derivation tree t into a tree t0 of ‘building instructions’ for a derived tree, and then applies a function ·E to execute these building instructions and build the TAG derived tree. Maletti’s approach fits nicely into our framework if we assume an algebra in which the building instruction symbols are interpreted according to ·E . Synchronous tree-adjoining grammars (Shieber and Schabes, 1990) can be modeled simply as an RTG with two separate TAG interpretations. We can separately choose to interpret each side as trees or strings, as described i"
W11-2902,N10-1035,1,0.339654,"Missing"
W11-2902,J08-3004,0,0.0901766,"egular tree grammars – which generalizes both synchronous grammars, tree transducers, and LCFRSstyle monolingual grammars. A grammar of this formalism consists of a regular tree grammar (RTG, Comon et al. (2007)) defining a language of derivation trees and an arbitrary number of interpretations which map these trees into objects of arbitrary algebras. This allows us to capture a wide variety of (synchronous and monolingual) grammar formalisms. We can also model heterogeneous synchronous languages, which relate e.g. trees with strings; this is necessary for applications in machine translation (Graehl et al., 2008) and in parsing strings with synchronous tree grammars. Second, we also provide parsing and decoding algorithms for our framework. The key concept that we introduce is that of a regularly decomposable algebra, where the set of all terms that evaluate to a given object form a regular tree language. Once an algorithm that computes a compact representation of this language is known, parsing algorithms follow from a generic construction. All important algebras in natural language processing that we are aware of – in particular the standard algebras of strings and trees – are regularly decomposable"
W11-2902,P96-1016,0,0.0867666,"e present a formal framework that generalizes a variety of monolingual and synchronous grammar formalisms for parsing and translation. Our framework is based on regular tree grammars that describe derivation trees, which are interpreted in arbitrary algebras. We obtain generic parsing algorithms by exploiting closure properties of regular tree languages. 1 Introduction Over the past years, grammar formalisms that relate pairs of grammatical structures have received much attention. These formalisms include synchronous grammars (Lewis and Stearns, 1968; Shieber and Schabes, 1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of form"
W11-2902,C92-2065,0,0.263568,"apping derivation trees into derived trees. This cannot easily be done by a homomorphic interpretation in an ordinary tree algebra. One way to deal with this, which is taken by Shieber (2006), is to replace homomorphisms by a more complex class of tree translation functions called embedded pushdown tree transducers. A second approach is to interpret homomorphically 10 these assume that all RTG rule applications are statistically independent. That is, the canonical probabilistic version of context-free grammars is PCFG, and the canonical probabilistic version of tree-adjoining grammar is PTAG (Resnik, 1992). A final point is that Graehl et al. invest considerable effort into defining different versions of their transducer training algorithms for the tree-to-tree and tree-to-string translation cases. The core of their paper, in our terms, is to define synchronous parsing algorithms to compute an RTG of derivation trees for (tree, tree) and (tree, string) input pairs. In their setup, these two cases are formally completely different objects, and they define two separate algorithms for these problems. Our approach is more modular: The training and parsing algorithms can be fully generic, and all th"
W11-2902,H05-1101,0,0.0108654,"ons of the RTG G into string algebras T1∗ and T2∗ ; as above, the synchronization is ensured by requiring that related strings in T1∗ and T2∗ are interpretations of the same derivation tree t ∈ L(G). As above, we can parse synchronously by parsing separately for the two interpretations and intersecting the results. This yields a parsing complexity for SCFG parsing of O(nm+1 · nm+1 ), where n1 and n2 are the lengths 1 2 of the input strings and m is the rank of the RTG G. Unlike in the monolingual case, this is now consistent with the result that the membership problem of SCFGs is NP-complete (Satta and Peserico, 2005). The reason for the intractability of SCFG parsing is that SCFGs, in general, cannot be binarized. However, Huang et al. (2009) define the class of binarizable SCFGs, which can be brought into a weakly equivalent normal form in which all production rules are binary and the membership problem can be solved in time O(n31 · n32 ). The key property of binarizable SCFGs, in our terms, is that if r is any production rule pair of the SCFG, h1 (r) and h2 (r) can be chosen in such a way that they can be transformed into each other by locally swapping the subterms of a node. For instance, an SCFG rule"
W11-2902,W08-2319,0,0.190571,"Missing"
W11-2902,P01-1061,0,0.0303276,"eous both in the grammar formalism and in the objects that each of its dimensions describes. The purpose of our paper was to pull together a variety of existing research and explain it in a new, unified light: We have not shown how to do something that was not possible before, only how to do it in a uniform way. Nonetheless, we expect that future work will benefit from the clarified formal setup we have proposed here. In particular, we believe that the view of parse charts as RTGs may lead to future algorithms which exploit their closure under intersection, e.g. to reduce syntactic ambiguity (Schuler, 2001). Generalized Context-Free Grammars Finally, the view we advocate here embraces a tradition of grammar formalisms going back to generalized context-free grammar (GCFG, Pollard (1984)), which follows itself research in theoretical computer science (Mezei and Wright, 1967; Goguen et al., 1977). A GCFG grammar can be seen as an RTG over a signature Σ whose trees are evaluated as terms of some Σ-algebra A. This is a special case of an IRTG, in which the homomorphism is simply the identical function on TΣ , and the algebra is A. In fact, we could have equivalently defined an IRTG as an RTG whose tr"
W11-2902,C90-3045,0,0.551406,"applied to existing grammar formalisms. We present a formal framework that generalizes a variety of monolingual and synchronous grammar formalisms for parsing and translation. Our framework is based on regular tree grammars that describe derivation trees, which are interpreted in arbitrary algebras. We obtain generic parsing algorithms by exploiting closure properties of regular tree languages. 1 Introduction Over the past years, grammar formalisms that relate pairs of grammatical structures have received much attention. These formalisms include synchronous grammars (Lewis and Stearns, 1968; Shieber and Schabes, 1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shie"
W11-2902,W04-3312,0,0.0677419,"ysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of formalisms are available, it is useful to take a step back and look for a generalized model that explains the precise formal relationship between them. There is a long tradition of such research on monolingual grammar formalisms, where e.g. linear context-free rewriting systems (LCFRS, Vijay-Shanker et al. (1987)) generalize various mildly context-sensitive formalisms. However, few such results exist for synchronous formalisms. A notable exception is the work by Shieber (2004), who unified synchronous treeadjoining grammars with tree transducers. 2 Proceedings of the 12th International Conference on Parsing Technologies, pages 2–13, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. 2 Formal Foundations where { ti /xi |i ∈ [n] } represents the substitution that replaces all occurrences of xi with the respective ti . A homomorphism is called linear if every term h(f ) contains each variable at most once; and a delabeling if every term h(f ) is of the form g(xπ(1) , . . . , xπ(n) ) where n is the rank of f and π a permutation"
W11-2902,E06-1048,0,0.529935,"1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of formalisms are available, it is useful to take a step back and look for a generalized model that explains the precise formal relationship between them. There is a long tradition of such research on monolingual grammar formalisms, where e.g. linear context-free rewriting systems (LCFRS, Vijay-Shanker et al. (1987)) generalize various mildly context-sensitive formalisms. However, few such results exist for synchronous formalisms. A notable exception is the work by Shieber (2004), who unified synchronous treeadjoining grammars with tree transducers. 2 Proceedings of the 1"
W11-2902,P87-1015,0,0.816122,"Missing"
W11-2902,W90-0102,0,\N,Missing
W12-4613,W98-0106,0,0.208008,"TAG), the operations of substitution and adjunction establish an asymmetric relation between two lexical items: The elementary tree for one item is substituted or adjoined into the elementary tree for another one. In many cases, this relation can be interpreted as a relation of syntactic dependency (complementation or adjunction), and it is natural then to try to interpret LTAG derivations as dependency trees. However, as several authors have pointed out, the correspondence between derivation trees and dependency structures is not as direct as it may seem at first glance (Rambow et al., 1995; Candito and Kahane, 1998; Frank and van Genabith, 2001). Examples of mismatches are in particular those where predicates adjoin into their arguments, an analysis that is chosen whenever an argument allows for long extractions. In these cases, the edge The paper is structured as follows. We start by reviewing the divergence between derivation trees and dependency trees in Section 2. In Section 3 we present the basic ideas behind our transformation. A formalization of this transformation is given in Section 4. In Section 5 we examine the structural properties of the dependency trees that can be induced using our transf"
W12-4613,W08-2303,0,0.0287142,"whenever different parts of an elementary tree can be split. Arguments are added by substitution but an ‘extracted’ part of an argument, linked to the lower part by a dominance link, can be separated from this lower part and end up much higher. This gives more flexibility concerning the modeling of discontinuities and non-projective dependencies. Another LTAG variant that has been discussed a lot recently in the context of the ‘missing link problem’ is tree-local MCTAG with flexible composition (Joshi et al., 2007; Chen-Main and Joshi, 2012), formalized by the notion of delayed treelocality (Chiang and Scheffler, 2008). The idea is roughly to perform the reversal of complementtaking adjunctions not on the derivation tree, but already during derivation. More precisely, instead of considering such an operation as a standard adjunction, it is considered as a wrapping operation directed from the adjunction site to the auxiliary tree. Consider for instance the derivation in Fig. 3. If we take the adjunction of the think tree into the prefer tree to be a wrapping of prefer, split at its internal S node, around the think tree. If this is reflected in the derivation tree by an edge from prefer to think, then one ob"
W12-4613,E03-1030,1,0.90113,"s is reversed. We show that, under this transformation, LTAG is able to induce both ill-nested dependency trees and dependency trees with gap-degree greater than 1, which is not possible under the direct reading of derivation trees as dependency trees. Different strategies have been adopted to obtain linguistically plausible dependency structures using LTAG. Some proposals adopt variants of the formalism with different derivation operations (Rambow et al., 2001; Chen-Main and Joshi, 2012); others retrieve the missing dependencies from the derivation tree and the derived tree (Kallmeyer, 2002; Gardent and Kallmeyer, 2003). In this paper we follow the second line of work in that we take a two-step approach: To get a dependency tree, we first construct a derivation tree, and then obtain the dependencies in a postprocessing step. However, in contrast to previous work we retain the property that dependencies should form a tree structure: We do not regard the missing dependencies as additions to the derivation tree, but view the correspondence between derivation trees and dependency structures as a tree-to-tree mapping. The crucial feature of this mapping is that it can reverse some of the edges in the derivation t"
W12-4613,W02-2218,1,0.885941,"the original edges is reversed. We show that, under this transformation, LTAG is able to induce both ill-nested dependency trees and dependency trees with gap-degree greater than 1, which is not possible under the direct reading of derivation trees as dependency trees. Different strategies have been adopted to obtain linguistically plausible dependency structures using LTAG. Some proposals adopt variants of the formalism with different derivation operations (Rambow et al., 2001; Chen-Main and Joshi, 2012); others retrieve the missing dependencies from the derivation tree and the derived tree (Kallmeyer, 2002; Gardent and Kallmeyer, 2003). In this paper we follow the second line of work in that we take a two-step approach: To get a dependency tree, we first construct a derivation tree, and then obtain the dependencies in a postprocessing step. However, in contrast to previous work we retain the property that dependencies should form a tree structure: We do not regard the missing dependencies as additions to the derivation tree, but view the correspondence between derivation trees and dependency structures as a tree-to-tree mapping. The crucial feature of this mapping is that it can reverse some of"
W12-4613,J13-2004,1,0.816552,"side. We call this tuple the template of f , and use it as a unique name for f . This means that we can talk about e.g. the standard binary concatenation function as ‘the yield function ""x11 x21 #’. The yield function associated with an operation symbol in a derivation tree can be extracted in a systematic way; see Boullier (1999) for a procedure that performs this extraction in the formalism of range concatenation grammars. 4.3 Direct Interpretation The direct interpretation of a derivation tree as a dependency tree can be formalized by interpreting symbols as operations on dependency trees (Kuhlmann, 2013). Continuing our running example, suppose that for each subtree of t0 we are not only given a string or a pair of strings as before, but also a corresponding dependency tree. Then the symbol to_love has a straightforward reading as constructing a new dependency tree for the full sentence (1): Preserve all the old dependencies, 111 and add new edges from to_love to the roots of the dependency trees associated with the subterms. 4.4 Transformation v1 v4 w1 w2 We illustrate the formal transformation by means of our running example. In order to convey the intuitive idea, we first present the trans"
W12-4613,P95-1021,0,0.686923,"adjoining grammar (LTAG), the operations of substitution and adjunction establish an asymmetric relation between two lexical items: The elementary tree for one item is substituted or adjoined into the elementary tree for another one. In many cases, this relation can be interpreted as a relation of syntactic dependency (complementation or adjunction), and it is natural then to try to interpret LTAG derivations as dependency trees. However, as several authors have pointed out, the correspondence between derivation trees and dependency structures is not as direct as it may seem at first glance (Rambow et al., 1995; Candito and Kahane, 1998; Frank and van Genabith, 2001). Examples of mismatches are in particular those where predicates adjoin into their arguments, an analysis that is chosen whenever an argument allows for long extractions. In these cases, the edge The paper is structured as follows. We start by reviewing the divergence between derivation trees and dependency trees in Section 2. In Section 3 we present the basic ideas behind our transformation. A formalization of this transformation is given in Section 4. In Section 5 we examine the structural properties of the dependency trees that can b"
W12-4613,J01-1004,0,0.195329,"iew the correspondence between derivation trees and dependency structures as a tree transformation during which the direction of some of the original edges is reversed. We show that, under this transformation, LTAG is able to induce both ill-nested dependency trees and dependency trees with gap-degree greater than 1, which is not possible under the direct reading of derivation trees as dependency trees. Different strategies have been adopted to obtain linguistically plausible dependency structures using LTAG. Some proposals adopt variants of the formalism with different derivation operations (Rambow et al., 2001; Chen-Main and Joshi, 2012); others retrieve the missing dependencies from the derivation tree and the derived tree (Kallmeyer, 2002; Gardent and Kallmeyer, 2003). In this paper we follow the second line of work in that we take a two-step approach: To get a dependency tree, we first construct a derivation tree, and then obtain the dependencies in a postprocessing step. However, in contrast to previous work we retain the property that dependencies should form a tree structure: We do not regard the missing dependencies as additions to the derivation tree, but view the correspondence between der"
W12-4613,P87-1015,0,0.21392,"John claim Mary seems 1 Bill claim claim 1 Bill ! to_love 1 21 22 John Mary seems Bill ! seems to_love John Mary Figure 4: Transformation of the derivation tree for (1) 2 Kallmeyer and Romero (2008) use specific features on the head projection line of verbs (the verbal spine in their terminology) in order to retrieve the missing dependency links. 110 4 Formal Version of the Transformation In this section we give a formal account of the transformation sketched above. 4.1 Derivation Trees as Terms We represent derivation trees as terms, formal expressions over a signature of operation symbols (Vijay-Shanker et al., 1987). For example, we represent the derivation in Fig. 2 by the term t0 : respectively. The yield of the derived tree corresponding to a derivation tree t can be obtained by associating with each operation symbol in t a yield function (Weir, 1988). To illustrate the idea, suppose that the yields of the four subterms of t0 are given as ""John#, ""Bill claims, ε#, ""Mary#, and ""seems, ε#, respectively. Then the full yield (1) is obtained by assigning the following yield function to ‘to_love’: to_love(""x11 #, ""x21 , x22 #, ""x31 #, ""x41 , x42 #) = t0 = to_love(John, claims(Bill), Mary, seems) Here ‘to_lo"
W12-4616,W05-1502,0,0.0213285,"entative, strikes a balance between the other two approaches. This research has done much to clarify the formal connections between TAG and other formalisms in terms of generative capacity. It has not been particularly productive with respect to finding new algorithms for parsing, training, and (in the synchronous case) decoding. This is regrettable because standard parsing algorithms for TAG (Vijay-Shanker and Joshi, 1985; Shieber et al., 1995) are complicated, require relatively involved correctness proofs, and are hard to teach. A similar criticism applies to parsing algorithms for LCFRSs (Burden and Ljunglöf, 2005). So far, no new parsing algorithms have arisen from Shieber’s work on bimorphisms, or from the CFTL-based view. Indeed, Maletti (2010) leaves the development of such algorithms as an open problem. This paper makes two contributions. First, we show how a number of the formal perspectives on TAG mentioned above can be recast in a uniform way as interpreted regular tree grammars (IRTGs, Koller and Kuhlmann (2011)). IRTGs capture the fundamental idea of generating strings, derived trees, or other objects from a regular tree language, and allow us to make the intuitive differences in how different"
W12-4616,N10-1035,1,0.845858,"ction 4. Unfortunately, this has the effect that the algebra contains operations of rank greater than two, which increases the parsing complexity. 5.1 concA .w1 ; w2 / D .w1 w21 ; w22 / T concA .w1 ; w2 / D .w11 ; w12 w2 / Finally, there is a binary partial wrapping operation wrap, which is defined if its first argument is a string pair. This operation wraps its first argument around the second, as follows: T wrapA .w1 ; w2 / D w11 w2 w12 T wrapA .w1 ; w2 / D .w11 w21 ; w22 w12 / Notice that these operations closely mirror the operations for well-nested LCFRSs with fan-out 2 that were used in Gómez-Rodríguez et al. (2010). 5.2 An IRTG for TAG String Languages We can use AT to construct, for any given TAG grammar G over some alphabet A of terminal symbols, an IRTG G D .G ; .hs ; AT // such that L.G/ consists of exactly the strings that are generated by G. We again describe the derivation trees using a ˙-RTG G . Say that AT is a -algebra. It remains to construct a homomorphism hs from T˙ to T . This is most easily done by defining a second homomorphism hst that maps from TD to AT . hst effectively reads off the yield of a tree or context, and is defined by mapping each operation symbol of TD to a term over A"
W12-4616,J09-4009,0,0.111425,"Missing"
W12-4616,W11-2902,1,0.922779,"Joshi, 1985; Shieber et al., 1995) are complicated, require relatively involved correctness proofs, and are hard to teach. A similar criticism applies to parsing algorithms for LCFRSs (Burden and Ljunglöf, 2005). So far, no new parsing algorithms have arisen from Shieber’s work on bimorphisms, or from the CFTL-based view. Indeed, Maletti (2010) leaves the development of such algorithms as an open problem. This paper makes two contributions. First, we show how a number of the formal perspectives on TAG mentioned above can be recast in a uniform way as interpreted regular tree grammars (IRTGs, Koller and Kuhlmann (2011)). IRTGs capture the fundamental idea of generating strings, derived trees, or other objects from a regular tree language, and allow us to make the intuitive differences in how different perspectives divide the labour over the various modules formally precise. Second, we introduce two new algebras. One captures TAG string languages; the other describes TAG derived tree languages. We show that both of these algebras are regularly decomposable, which means that the very modular algorithms that are available for parsing, training, and decoding of IRTGs can be applied to TAG. As an immediate conse"
W12-4616,P10-1109,0,0.214187,"tree is generated, then this derivation tree is mapped into a term over some algebra and evaluated there. Under this view, one can take different perspectives on how the labour of generating a string or derived tree should be divided between the mapping process and the algebra. In a way that we will make precise, linear context-free rewriting systems (LCFRSs, Weir (1988)) push much of the work into the algebra; Shieber (2006)’s analysis of synchronous TAG as bimorphisms puts the burden mostly on the mapping procedure; and a line of research using context-free tree languages (CFTLs), of which Maletti (2010) is a recent representative, strikes a balance between the other two approaches. This research has done much to clarify the formal connections between TAG and other formalisms in terms of generative capacity. It has not been particularly productive with respect to finding new algorithms for parsing, training, and (in the synchronous case) decoding. This is regrettable because standard parsing algorithms for TAG (Vijay-Shanker and Joshi, 1985; Shieber et al., 1995) are complicated, require relatively involved correctness proofs, and are hard to teach. A similar criticism applies to parsing algo"
W12-4616,C92-2065,0,0.410573,"Missing"
W12-4616,W08-2319,0,0.187265,"Missing"
W12-4616,W04-3312,0,0.0338373,"m substituting and adjoining t1 , t2 , and t3 into ˛1 at appropriate places. Using such functions, one can directly interpret the tree ˛1 .˛2 .nop/; nop; ˇ1 .nop// as the derived tree in Fig. 2c. Therefore, for the IRTG GL D .G0 ; .id; AL //, where id is the identity homomorphism on T˙0 , L.GL / is exactly the set of derived trees of G. One could instead obtain an IRTG for describing the string language of G by using an interpretation into a ˙0 -algebra of strings and string tuples in which the elementary trees evaluate to appropriate generalized concatenation operations. STAG as Bimorphisms. Shieber (2004) proposes a different perspective on the generative process of synchronous tree substitution grammar (STSG). He builds upon earlier work on bimorphisms and represents an STSG as an IRTG .G0 ; .h1 ; T1 /; .h2 ; T2 //, where T1 and T2 are appropriate term algebras. In this construction, the homomorphisms must carry some of the load: In an STSG whose left-hand side contains the trees ˛1 and ˛2 from Fig. 2a, we would have h1 .˛1 / D S.x1 ; VP .sleeps//. Shieber (2006) later extended this approach to STAG by replacing the tree homomorphisms with embedded push-down transducers, a more powerful t"
W12-4616,E06-1048,0,0.143524,"isms. A common approach in this line of research has been to conceive the way in which TAG generates a string or a derived tree from a grammar as a two-step process: first a derivation tree is generated, then this derivation tree is mapped into a term over some algebra and evaluated there. Under this view, one can take different perspectives on how the labour of generating a string or derived tree should be divided between the mapping process and the algebra. In a way that we will make precise, linear context-free rewriting systems (LCFRSs, Weir (1988)) push much of the work into the algebra; Shieber (2006)’s analysis of synchronous TAG as bimorphisms puts the burden mostly on the mapping procedure; and a line of research using context-free tree languages (CFTLs), of which Maletti (2010) is a recent representative, strikes a balance between the other two approaches. This research has done much to clarify the formal connections between TAG and other formalisms in terms of generative capacity. It has not been particularly productive with respect to finding new algorithms for parsing, training, and (in the synchronous case) decoding. This is regrettable because standard parsing algorithms for TAG ("
W12-4616,P85-1011,0,0.695953,"’s analysis of synchronous TAG as bimorphisms puts the burden mostly on the mapping procedure; and a line of research using context-free tree languages (CFTLs), of which Maletti (2010) is a recent representative, strikes a balance between the other two approaches. This research has done much to clarify the formal connections between TAG and other formalisms in terms of generative capacity. It has not been particularly productive with respect to finding new algorithms for parsing, training, and (in the synchronous case) decoding. This is regrettable because standard parsing algorithms for TAG (Vijay-Shanker and Joshi, 1985; Shieber et al., 1995) are complicated, require relatively involved correctness proofs, and are hard to teach. A similar criticism applies to parsing algorithms for LCFRSs (Burden and Ljunglöf, 2005). So far, no new parsing algorithms have arisen from Shieber’s work on bimorphisms, or from the CFTL-based view. Indeed, Maletti (2010) leaves the development of such algorithms as an open problem. This paper makes two contributions. First, we show how a number of the formal perspectives on TAG mentioned above can be recast in a uniform way as interpreted regular tree grammars (IRTGs, Koller and K"
W12-4616,P87-1015,0,0.573716,"Missing"
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W17-6312,S14-2082,0,0.0675852,"Missing"
W17-6312,J03-1006,0,0.0570589,"he graph in this way makes it impossible to retain arcs that cover k – and inside the interval (1, 5) every vertex is covered by some arc. This property prevents the decomposition of not only the graph in Figure 5, but more generally every cog belt. Relaxed Deduction System for 1ec Trees To obtain a parsing algorithm for 1ec graphs, an obvious idea is to take the corresponding algorithm for trees (Pitler et al., 2013) and ‘relax’ it by deleting all book-keeping that is used to enforce the tree constraint. We present the resulting algorithm as a weighted deduction system (Shieber et al., 1995; Nederhof, 2003). Such a system uses inference rules to derive information about sets of graphs; this information is represented by weighted formulas called items. Parsing amounts to finding the derivation of a goal item with maximum weight, starting from a set of initial items. We assume that we are given an arc-weighted digraph G = (V, A) with vertices V = {1, . . . , n}. Items represent subgraphs of G corresponding to either isolated intervals [i, j], where there are no arcs between vertices in the open interval (i, j) and vertices outside of [i, j], or isolated crossing regions [i, j] ∪ {x}, where there a"
W17-6312,L16-1630,1,0.902154,"Missing"
W17-6312,P17-1193,0,0.0701283,"lly restricted parsing algorithm such as the one described in this paper, even if its coverage is high, has the drawback that it is harder to combine with an expressive learning approach such as the recurrent neural networks used by Kiperwasser and Goldberg (2016). Parsing Algorithm A second contribution of this paper is the extension of the parsing algorithm for 1ec trees (Pitler et al., 2013) to a quintic-time algorithm for the full class of 1ec graphs, and a quartic-time algorithm for the restricted class of 1ec graphs without cog belts. Closely related algorithms were recently proposed by Cao et al. (2017) and Kummerfeld and Klein (2017). The former use an approach similar to ours in Section 4.4 to parse what they call ‘coupled staggered patterns’ (our cog belts), albeit restricted to pagenumber 2; they report state-ofthe-art results on the SemEval data. Kummerfeld and Klein (2017) apply 1ec graphs in the context of parsing to phrase structure representations with traces; their algorithm cannot parse what they call ‘locked chains’ (our cog belts), but has the benefit of enforcing acyclicity and uniqueness. The proposed quintic-time algorithm may not be the most attractive one for practical pars"
W17-6312,P17-1186,0,0.0497947,"Missing"
W17-6312,Q13-1002,0,0.108142,"int related to projectivity as known from syntactic parsing. When the search space is restricted to the class of noncrossing graphs, maximum subgraph parsing is possible in time O(n3 ), where n is the length of the input sentence. Unfortunately, the restriction to noncrossing graphs excludes a large proportion of the linguistic data. It seems clear that deep dependency parsing, much more than syntactic parsing, needs algorithms that can handle graphs with crossing arcs. An interesting weaker restriction than the noncrossing condition is the restriction to graphs which are 1-endpoint-crossing (Pitler et al., 2013), a constraint originally formulated for tree-shaped graphs. The maximum 1-endpoint-crossing subtree of a weighted digraph can be found in time O(n4 ). In this paper we show how to generalize this result to non-trees. This is not straightforward, as the obvious modification of existing algorithm for trees turns out to be incomplete for general graphs. The key to a complete algorithm, and our main technical contribution, is a characterization of 1-endpointcrossing graphs as a certain subset of the class of graphs with pagenumber at most 3 (Section 3). The exact characterization refers to the re"
W17-6312,P10-1151,0,0.067372,"Missing"
W17-6312,W14-2412,0,0.0306862,"by applications in natural language understanding, recent work in dependency parsing has targeted ‘deep’ graphs, a term used to refer to representations that are not necessarily tree-shaped. Such graphs support intuitive analyses of argument sharing in control constructions, quantification, and semantic modification, among others. Data sets of deep dependency graphs are often derived from the derivations of expressive grammar formalisms; for an overview, see Kuhlmann and Oepen (2016). Deep dependency parsing has been formalized as the search for maximum acyclic subgraphs in weighted digraphs (Schluter, 2014; Kuhlmann and Jonsson, 2015). Because this problem is known to be intractable in the general case (Guruswami et al., 2011), it is interesting to identify structural restrictions on the target graphs that can yield polynomial-time parsing algorithms without sacrificing too much of the empirical coverage. 78 Proceedings of the 15th International Conference on Parsing Technologies, pages 78–87, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics Chief executives and presidents had come and gone Figure 1: A sample dependency graph (Flickinger et al., 2016, CCD #20"
W17-6312,S15-1031,0,0.0133024,"ral times in a derivation. To avoid this, we would need to extend the items with information on whether an arc has already been set, very similar to the booleans used to control treeness in the original algorithm. For example, a modified version of rule (5) could look like this: 6 Conclusion We have shown how a new structural characterization of 1ec graphs in terms of their crossing graphs can be used to extend the parsing algorithm for 1ec trees to the full class of graphs. The class of 1ec graphs has a significantly higher coverage than the previously considered class of noncrossing graphs (Schluter, 2015; Kuhlmann and Jonsson, 2015) and may thus be a useful constraint on the search space for deep dependency parsing. However, to achieve state-of-the-art results the new parsing algorithm needs to be combined with a powerful machine learning component, a practical challenge that we leave to future work. Int[i, j; F ] ← s[i, k] + LR[i, k, l; F, bi,l , bk,l ] + Int[k, l; F ] + Int[l, j; bl,j ] The LR item has three booleans, corresponding to 86 Acknowledgements Marco Kuhlmann and Stephan Oepen. 2016. Towards a catalogue of linguistic graph banks. Computational Linguistics 42(4):819–827. We thank t"
W17-6312,Q16-1023,0,0.0481454,"ch would at least double the number of rules, and the increased complexity coming from labelled parsing. The most promising parsing results so far on the SDP data have been achieved with a simple, quadratic-time parsing algorithm with very few constraints on the search space but a strong learning component (Martins and Almeida, 2014; Peng et al., 2017). A structurally restricted parsing algorithm such as the one described in this paper, even if its coverage is high, has the drawback that it is harder to combine with an expressive learning approach such as the recurrent neural networks used by Kiperwasser and Goldberg (2016). Parsing Algorithm A second contribution of this paper is the extension of the parsing algorithm for 1ec trees (Pitler et al., 2013) to a quintic-time algorithm for the full class of 1ec graphs, and a quartic-time algorithm for the restricted class of 1ec graphs without cog belts. Closely related algorithms were recently proposed by Cao et al. (2017) and Kummerfeld and Klein (2017). The former use an approach similar to ours in Section 4.4 to parse what they call ‘coupled staggered patterns’ (our cog belts), albeit restricted to pagenumber 2; they report state-ofthe-art results on the SemEval"
W17-6312,Q15-1040,1,0.931928,"in natural language understanding, recent work in dependency parsing has targeted ‘deep’ graphs, a term used to refer to representations that are not necessarily tree-shaped. Such graphs support intuitive analyses of argument sharing in control constructions, quantification, and semantic modification, among others. Data sets of deep dependency graphs are often derived from the derivations of expressive grammar formalisms; for an overview, see Kuhlmann and Oepen (2016). Deep dependency parsing has been formalized as the search for maximum acyclic subgraphs in weighted digraphs (Schluter, 2014; Kuhlmann and Jonsson, 2015). Because this problem is known to be intractable in the general case (Guruswami et al., 2011), it is interesting to identify structural restrictions on the target graphs that can yield polynomial-time parsing algorithms without sacrificing too much of the empirical coverage. 78 Proceedings of the 15th International Conference on Parsing Technologies, pages 78–87, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics Chief executives and presidents had come and gone Figure 1: A sample dependency graph (Flickinger et al., 2016, CCD #20604004), drawn as an arc diag"
W17-6312,J16-4009,1,\N,Missing
W19-6202,S15-2162,0,0.0202764,"ent or non-improvement in parsing accuracy as an indicator, our study also contributes to a better understanding of the similarities and contentful differences between semantic and syntactic representations. Semantic dependency parsers are typically conceptualized as systems for structured prediction, combining a data-driven component that learns how to score dependency graphs with a decoder that retrieves one or several highest-scoring target graphs from the exponentially large search space of candidate graphs. Among decoding algorithms we find approaches based on integer linear programming (Almeida and Martins, 2015; Peng et al., 2017), dynamic programming algorithms that support exact decoding for restricted classes of graphs (Kuhlmann and Jonsson, 2015; Cao et al., 2017), and transition-based approaches introducing new shift–reduce-style automata (Zhang et al., 2016; Wang et al., 2018). Regarding the learning component, state-of-the-art parsing results have been achieved using neural architectures (Peng et al., 2017; Wang et al., 2018; Dozat and Manning, 2018). The system of Dozat and Manning (2018) even draws essentially all of its strength from its neural core, employing a trivial decoder. The parser"
W19-6202,P17-1193,0,0.0115485,"semantic and syntactic representations. Semantic dependency parsers are typically conceptualized as systems for structured prediction, combining a data-driven component that learns how to score dependency graphs with a decoder that retrieves one or several highest-scoring target graphs from the exponentially large search space of candidate graphs. Among decoding algorithms we find approaches based on integer linear programming (Almeida and Martins, 2015; Peng et al., 2017), dynamic programming algorithms that support exact decoding for restricted classes of graphs (Kuhlmann and Jonsson, 2015; Cao et al., 2017), and transition-based approaches introducing new shift–reduce-style automata (Zhang et al., 2016; Wang et al., 2018). Regarding the learning component, state-of-the-art parsing results have been achieved using neural architectures (Peng et al., 2017; Wang et al., 2018; Dozat and Manning, 2018). The system of Dozat and Manning (2018) even draws essentially all of its strength from its neural core, employing a trivial decoder. The parser used in this paper is a (slightly modified) re-implementation of that system developed by Roxbo (2019), which adds syntactic information via a simple head feat"
W19-6202,W08-1301,0,0.215365,"Missing"
W19-6202,P18-2077,0,0.166767,"rom the exponentially large search space of candidate graphs. Among decoding algorithms we find approaches based on integer linear programming (Almeida and Martins, 2015; Peng et al., 2017), dynamic programming algorithms that support exact decoding for restricted classes of graphs (Kuhlmann and Jonsson, 2015; Cao et al., 2017), and transition-based approaches introducing new shift–reduce-style automata (Zhang et al., 2016; Wang et al., 2018). Regarding the learning component, state-of-the-art parsing results have been achieved using neural architectures (Peng et al., 2017; Wang et al., 2018; Dozat and Manning, 2018). The system of Dozat and Manning (2018) even draws essentially all of its strength from its neural core, employing a trivial decoder. The parser used in this paper is a (slightly modified) re-implementation of that system developed by Roxbo (2019), which adds syntactic information via a simple head feature, along the lines of Peng et al. (2018). Paper Structure. After providing some background in Section 2, we describe the architecture of our parser in Section 3, and our data and experimental setup in Section 4. In Section 5 we present our empirical results and complement them with an error a"
W19-6202,hajic-etal-2012-announcing,0,0.184897,"Missing"
W19-6202,W12-3602,0,0.0233795,"ates. 4 Method In this section we describe our data and the setup of our experiments. 4.1 Data The main dataset for our experiments is the English part of the standard SDP distribution (Flickinger et al., 2016), which contains semantic dependency graphs for Sections 00–21 of the venerable Penn Treebank (Marcus et al., 1993) in a predefined train/test split, as well as graphs for out-of-domain test sentences from the Brown Corpus (Francis and Kuˇcera, 1985). The graphs come in four different representation types, of which we use three: graphs derived from DeepBank (DM, Oepen and Lønning, 2006; Ivanova et al., 2012); predicate– argument structures computed by the Enju parser (PAS, Miyao, 2006); and graphs derived from the tectogrammatical layer of the Prague Dependency Treebank (PSD, Hajic et al., 2012). Due to their structural differences, the three graph types are more or less difficult to parse into; PSD graphs, for example, feature a considerably larger label inventory than the other types. The graphs in the SDP dataset come with different types of gold-standard syntactic analyses, of which we use Stanford Basic Dependencies (SB, de Marneffe and Manning, 2008), derived from the Penn Treebank, and DEL"
W19-6202,Q15-1040,1,0.892352,"Missing"
W19-6202,J93-2004,0,0.0696986,"Missing"
W19-6202,P18-1173,0,0.0313907,"Missing"
W19-6202,D14-1162,0,0.0807151,"Missing"
W19-6202,K18-2016,0,0.0242167,"dicted trees during both training and testing. Dataset Our parser id SB DT EWT ood 93.2 89.9 94.0 90.3 85.9 StanfordNLP id ood 94.2 94.9 90.9 91.7 UDPipe 85.4 Table 3: Parsing accuracy for our syntactic models on the in-domain (id) and out-of-domain (ood) test sets for the SDP data, and the regular test set for the EWT data. Our three syntactic models (for SB, DT, and EWT) were trained using the same architecture and specifications as the semantic models, but use the factorized approach with CLE-decoding instead. Their accuracy is reported in Table 3, with additional results from StanfordNLP (Qi et al., 2018) and UDPipe (Straka and Strakov´a, 2017) for reference. We note that in contrast to those systems’ results, ours were achieved without gold POS tags. 5 Empirical Results The results for our semantic dependency parsing models for the three graph types in the SDP dataset are presented in Table 4. For comparison, we add results reported by Dozat and Manning (2018) and Peng et al. (2018), and emphasize for each test set the overall best-performing model. Baseline Our baseline using no syntactic features performs comparable to the systems of Dozat and Manning (2018) and Peng et al. (2018). We note"
W19-6202,K17-3009,0,0.0206007,"Missing"
W19-6202,J16-3001,0,0.0216523,"as systems for structured prediction, combining a data-driven component that learns how to score dependency graphs with a decoder that retrieves one or several highest-scoring target graphs from the exponentially large search space of candidate graphs. Among decoding algorithms we find approaches based on integer linear programming (Almeida and Martins, 2015; Peng et al., 2017), dynamic programming algorithms that support exact decoding for restricted classes of graphs (Kuhlmann and Jonsson, 2015; Cao et al., 2017), and transition-based approaches introducing new shift–reduce-style automata (Zhang et al., 2016; Wang et al., 2018). Regarding the learning component, state-of-the-art parsing results have been achieved using neural architectures (Peng et al., 2017; Wang et al., 2018; Dozat and Manning, 2018). The system of Dozat and Manning (2018) even draws essentially all of its strength from its neural core, employing a trivial decoder. The parser used in this paper is a (slightly modified) re-implementation of that system developed by Roxbo (2019), which adds syntactic information via a simple head feature, along the lines of Peng et al. (2018). Paper Structure. After providing some background in S"
