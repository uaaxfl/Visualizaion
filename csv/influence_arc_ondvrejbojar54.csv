2005.mtsummit-posters.9,2004.eamt-1.3,0,0.0455254,"e of texts was very different from the nature of texts from PCEDT and did not contain that many NE or idioms. As mentioned above, the PCEDT contains a translation of texts from the Wall Street Journal section of the PennTreebank, in which the NE, terminological units and idioms occur rather very often. Named entities are atomic units such as proper names, temporal expressions (e.g., dates) and quantities (e.g., monetary expressions). They occur quite often in various texts and carry important information. Hence, proper analysis of NE and their translation has an enormous impact on MT quality (Babych and Hartley, 2004). NE translation involves both semantic translation and phonetic transliteration. Each type of NE is handled in a different way. For instance, person names do not undergo semantic translation (only transliteration is required), while certain titles and part of names do (e.g., prvn´ı d´ ama Laura Bushov´ a → first lady Laura Bush). In case of organizations, application of regular transfer rules for NPs seems to be suffi´ cient (e.g., Ustav form´ aln´ı a aplikovan´e lingvistiky → Institute of formal and applied linguistics), although an idiomatic translation may be preferable sometimes. With res"
2005.mtsummit-posters.9,A00-2018,0,0.0349016,"ional morphological tags with a wildcard character “*” at places where the value is context-dependent (i.e. where no unary constraint holds). Binary constraints indicate which attributes have to be shared between some words of the expression. E.g., “cng:2=3” denotes that the case, number and gender have to match between the word 2 and word 3. Figure 2: Examples of automatically selected morphological constraints. Adding syntactic information 3 When using a treebank as the input corpus, dependency graphs are available, when using plain corpus, we first employ a Czech adaptation of a parser by (Charniak, 2000). 384 Syntactic information (dependency relations among words in the expression) is needed mainly during the analysis of input sentences, therefore we focussed on adding the information to the Czech part of entries first. For most of the entries, it was possible to add the dependency structure manually, based on the partof-speech pattern of the entry. For instance all the entries containing an adjective followed by a noun get the same structure: the noun governs the preceding adjective. For the remaining entries (with very varied POS patterns), we employ a corpus based search similar to the au"
2005.mtsummit-posters.9,E03-1004,0,0.0368094,"Missing"
2005.mtsummit-posters.9,P99-1065,0,0.0873315,"Missing"
2005.mtsummit-posters.9,2003.mtsummit-papers.21,1,0.870313,"Missing"
2005.mtsummit-posters.9,C90-3057,0,\N,Missing
2009.tc-1.3,koen-2004-pharaoh,0,0.0403126,"ssistance that an MT system can offer to human translators. Unlike TMs that suggest usually translations of whole sentences, caitra presents multiple translation options of short subsequences of words of the input sentence. The user (translator) is then allowed to construct the translation by both typing as well as by clicking on the selected translation options. We use a similar principle in our solution, see Section 4 below. 3 Phrase-Based Machine Translation Current state-of-the-art generic (i.e. more or less language independent) MT systems are statistical and phrase-based (PBMT, see e.g. [1, 3]). The “phrases” are simply any contiguous sequences of words with a known translation, as extracted automatically from a parallel corpus. Often, the phrases span across the boundaries of linguistically motivated constituents of the sentence, e.g. as segments or is a valid “phrase” but not a constituent in most grammars. 1 http://www.omegat.org/ http://translate.google.com/toolkit/ 3 Specifically, the terms of service say: By submitting your content through the Service, you grant Google the permission to use your content permanently to promote, improve or offer the Services. 4 http://www.caitr"
2009.tc-1.3,2009.mtsummit-papers.8,0,0.0585264,"ealed its CAT tool. Google Translator Toolkit (GTT2 ) is a tool based on Google Translate MT system. When starting the translation in GTT, the source text is translated by Google MT and the user’s task is to post-edit the MT output. Additionally, the user is allowed to import custom translation memories or glossaries and share them with other users. All text submitted to GTT is probably planned to extend the training data of Google MT.3 GTT is a nice example of an AJAX application, i.e. a lightweight web-based solution that seamlessly communicates with an underlying server. 2.3 Caitra Caitra4 [2] is an experimental translation tool developed by the Machine Translation Group at the University of Edinburgh. Caitra illustrates a fine-grained assistance that an MT system can offer to human translators. Unlike TMs that suggest usually translations of whole sentences, caitra presents multiple translation options of short subsequences of words of the input sentence. The user (translator) is then allowed to construct the translation by both typing as well as by clicking on the selected translation options. We use a similar principle in our solution, see Section 4 below. 3 Phrase-Based Machine"
2009.tc-1.3,P07-2045,1,0.0111884,"ssistance that an MT system can offer to human translators. Unlike TMs that suggest usually translations of whole sentences, caitra presents multiple translation options of short subsequences of words of the input sentence. The user (translator) is then allowed to construct the translation by both typing as well as by clicking on the selected translation options. We use a similar principle in our solution, see Section 4 below. 3 Phrase-Based Machine Translation Current state-of-the-art generic (i.e. more or less language independent) MT systems are statistical and phrase-based (PBMT, see e.g. [1, 3]). The “phrases” are simply any contiguous sequences of words with a known translation, as extracted automatically from a parallel corpus. Often, the phrases span across the boundaries of linguistically motivated constituents of the sentence, e.g. as segments or is a valid “phrase” but not a constituent in most grammars. 1 http://www.omegat.org/ http://translate.google.com/toolkit/ 3 Specifically, the terms of service say: By submitting your content through the Service, you grant Google the permission to use your content permanently to promote, improve or offer the Services. 4 http://www.caitr"
2014.tc-1.29,I13-1041,0,0.0147368,"to be somehow considered by the translators since the translation is going to be posted as a tweet again. We are planning to cast no technical limit on the translations but to carry out automatic abbreviations if necessary. 6. NLP Aspects of TCT The specifics of social media from the point of view of computational linguistics and natural language processing have been well studied in the past, see e.g. Hachey and Osborne, 2010. Most of this research so far has focused on input normalization and adaptation of NLP tools like taggers, parsers or named entity recognizers for this domain, see e.g. Baldwin et al. (2013) or Bontcheva et al. (2013) for a number of references. In contrast to this, there seems to be much less research on translation of social media. Microblogs can certainly serve as an interesting source of parallel or comparable corpora, see e.g. Ling et al. (2013), Xing et al. (2013), Rajjem et al. (2013) and Jehl et al. (2012). Gerlach et al. (2013) combine pre-editing and post-editing for user-generated content in a tech forum. Since the phrase-based approach to MT as implemented e.g. in the Moses toolkit (Koehn et al., 2007) has been shown to successfully circumvent the need for most of lin"
2014.tc-1.29,N10-1064,0,0.0121505,"al. (2012). Gerlach et al. (2013) combine pre-editing and post-editing for user-generated content in a tech forum. Since the phrase-based approach to MT as implemented e.g. in the Moses toolkit (Koehn et al., 2007) has been shown to successfully circumvent the need for most of linguistic processing, we would like to jump-start MT for tweets in a similar fashion. Nevertheless, we are well aware that most of the mentioned pre-processing tools could bring us an improvement and we plan to gradually add them to future versions of TCT. Some of such tools are already being developed in open source: Bertoldi et al. 2010 evaluate the utility of confusion networks (code available in Moses) for the recovery from spelling errors and there have been two related MT Marathon 2013 projects: MTSpell (spell checking for machine translation) and SMMTT (Social Media Machine Translation Toolkit, see http://ufal.mff.cuni.cz/mtm13/projects.html). We have already started adapting MTSpell for our needs and our languages of interest. 225 Translating and The Computer 36 7. Conclusion We presented TCT, an infrastructure for translation of tweets. As of now, the system is not much more than a playground for researchers in MT. Th"
2014.tc-1.29,W14-3302,1,0.780716,"slation through TCT website. As of writing this article, this functionality is still in an early alpha stage. 4.3. Translation Evaluation Since our approach relies on volunteer translators and MT engines, some quality checks need to be part of the processing loop. We follow the common practice and solicit multiple translations. The next step is to select the best translation. In future, we plan to use automatic quality estimation to either pre-select better translation or to actually perform the selection, see e.g. the system QuEst (Shah et al., 2013) or the shared task on quality estimation (Bojar et al., 2014). The current version of TCT is again limited to manual judging. Judging via e-mail seemed somewhat cumbersome and we hope to be able to automate this process fully anyway, so we implemented a simple web-based interface. To ease the work of judges we have first implemented relative pairwise comparison. Presented with the original tweet and a pair of translations, the judge was asked to select the better one or indicate that both are equally good or equally bad (inacceptable). After some preliminary operation and a long discussion, we moved from relative comparison to absolute judgements. The r"
2014.tc-1.29,R13-1011,0,0.0178673,"by the translators since the translation is going to be posted as a tweet again. We are planning to cast no technical limit on the translations but to carry out automatic abbreviations if necessary. 6. NLP Aspects of TCT The specifics of social media from the point of view of computational linguistics and natural language processing have been well studied in the past, see e.g. Hachey and Osborne, 2010. Most of this research so far has focused on input normalization and adaptation of NLP tools like taggers, parsers or named entity recognizers for this domain, see e.g. Baldwin et al. (2013) or Bontcheva et al. (2013) for a number of references. In contrast to this, there seems to be much less research on translation of social media. Microblogs can certainly serve as an interesting source of parallel or comparable corpora, see e.g. Ling et al. (2013), Xing et al. (2013), Rajjem et al. (2013) and Jehl et al. (2012). Gerlach et al. (2013) combine pre-editing and post-editing for user-generated content in a tech forum. Since the phrase-based approach to MT as implemented e.g. in the Moses toolkit (Koehn et al., 2007) has been shown to successfully circumvent the need for most of linguistic processing, we woul"
2014.tc-1.29,2013.mtsummit-wptp.6,0,0.0118827,"ocessing have been well studied in the past, see e.g. Hachey and Osborne, 2010. Most of this research so far has focused on input normalization and adaptation of NLP tools like taggers, parsers or named entity recognizers for this domain, see e.g. Baldwin et al. (2013) or Bontcheva et al. (2013) for a number of references. In contrast to this, there seems to be much less research on translation of social media. Microblogs can certainly serve as an interesting source of parallel or comparable corpora, see e.g. Ling et al. (2013), Xing et al. (2013), Rajjem et al. (2013) and Jehl et al. (2012). Gerlach et al. (2013) combine pre-editing and post-editing for user-generated content in a tech forum. Since the phrase-based approach to MT as implemented e.g. in the Moses toolkit (Koehn et al., 2007) has been shown to successfully circumvent the need for most of linguistic processing, we would like to jump-start MT for tweets in a similar fashion. Nevertheless, we are well aware that most of the mentioned pre-processing tools could bring us an improvement and we plan to gradually add them to future versions of TCT. Some of such tools are already being developed in open source: Bertoldi et al. 2010 evaluate the"
2014.tc-1.29,W10-0513,0,0.0552219,"Missing"
2016.eamt-2.1,W15-3009,1,0.880814,"Missing"
2016.eamt-2.1,W15-5711,1,0.827881,"Missing"
2016.eamt-2.1,W15-3006,1,0.877207,"Missing"
2016.eamt-2.1,P15-4020,0,0.0200302,"Missing"
2016.eamt-2.1,P13-4014,0,0.0192755,"Missing"
2016.eamt-2.1,W15-2505,0,0.0245204,"Missing"
2016.eamt-2.1,P15-3002,0,0.0279562,"Missing"
2020.acl-srw.34,P19-1309,1,0.667856,"ther language pairs. 1 Introduction Parallel corpora constitute an essential training data resource for machine translation as well as other cross-lingual NLP tasks. However, large parallel corpora are only available for a handful of language pairs while the rest relies on semi-supervised or unsupervised methods for training. Since monolingual data are generally more abundant, parallel sentence mining from non-parallel corpora provides another opportunity for low-resource language pairs. An effective approach to parallel data mining is based on multilingual sentence embeddings (Schwenk, 2018; Artetxe and Schwenk, 2019b). However, existing methods to generate crosslingual representations are either heavily supervised or only apply to static word embeddings. An alternative approach to unsupervised multilingual training is that of Devlin et al. (2018) or Lample and Conneau (2019), who train a masked language model (M-BERT, XLM) on a concatenation of monolingual corpora in different languages to learn a joint structure of these languages together. While several authors (Pires et al., 2019; Wu and Dredze, 2019; Karthikeyan et al., 2019; Libovick´y et al., 2019) bring evidence of cross-lingual transfer within th"
2020.acl-srw.34,Q19-1038,1,0.698776,"ther language pairs. 1 Introduction Parallel corpora constitute an essential training data resource for machine translation as well as other cross-lingual NLP tasks. However, large parallel corpora are only available for a handful of language pairs while the rest relies on semi-supervised or unsupervised methods for training. Since monolingual data are generally more abundant, parallel sentence mining from non-parallel corpora provides another opportunity for low-resource language pairs. An effective approach to parallel data mining is based on multilingual sentence embeddings (Schwenk, 2018; Artetxe and Schwenk, 2019b). However, existing methods to generate crosslingual representations are either heavily supervised or only apply to static word embeddings. An alternative approach to unsupervised multilingual training is that of Devlin et al. (2018) or Lample and Conneau (2019), who train a masked language model (M-BERT, XLM) on a concatenation of monolingual corpora in different languages to learn a joint structure of these languages together. While several authors (Pires et al., 2019; Wu and Dredze, 2019; Karthikeyan et al., 2019; Libovick´y et al., 2019) bring evidence of cross-lingual transfer within th"
2020.acl-srw.34,D18-2029,0,0.0564986,"Missing"
2020.acl-srw.34,W18-6317,0,0.025539,"rable Corpora 255 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 255–262 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al. (2017) derive sentence embeddings from internal representations of a neural machine translation system with a shared encoder. The universal sentence encoder (USE) (Cer et al., 2018; Yang et al., 2019) family covers sentence embedding models with a multi-task dual-encoder training framework including the tasks of question-answer prediction or natural language inference. Guo et al. (2018) directly optimize the cosine similarity between the source and target sentences using a bidirectional dual-encoder. These approaches rely on heavy supervision by parallel corpora which is not available for low-resource languages. Unsupervised multilingual word embeddings. Cross-lingual embeddings of words can be obtained by post-hoc alignment of monolingual word embeddings (Mikolov et al., 2013) and mean-pooled with IDF weights to represent sentences (Litschko et al., 2019). Unsupervised techniques to find a linear mapping between embedding spaces were proposed by Artetxe et al. (2018) and Co"
2020.acl-srw.34,P19-1493,0,0.117045,"Missing"
2020.acl-srw.34,P19-1492,1,0.775032,"se approaches rely on heavy supervision by parallel corpora which is not available for low-resource languages. Unsupervised multilingual word embeddings. Cross-lingual embeddings of words can be obtained by post-hoc alignment of monolingual word embeddings (Mikolov et al., 2013) and mean-pooled with IDF weights to represent sentences (Litschko et al., 2019). Unsupervised techniques to find a linear mapping between embedding spaces were proposed by Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependency parsing. Pires et al."
2020.acl-srw.34,P19-1018,0,0.0258298,"al dual-encoder. These approaches rely on heavy supervision by parallel corpora which is not available for low-resource languages. Unsupervised multilingual word embeddings. Cross-lingual embeddings of words can be obtained by post-hoc alignment of monolingual word embeddings (Mikolov et al., 2013) and mean-pooled with IDF weights to represent sentences (Litschko et al., 2019). Unsupervised techniques to find a linear mapping between embedding spaces were proposed by Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependen"
2020.acl-srw.34,D19-1410,0,0.128866,"DF weights to represent sentences (Litschko et al., 2019). Unsupervised techniques to find a linear mapping between embedding spaces were proposed by Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependency parsing. Pires et al. (2019) extract contextualized embeddings directly from unsupervised multilingual LMs and use them for parallel sentence retrieval. Other authors improve the alignment of representations in a multilingual LM using a parallel corpus as an anchor (Cao et al., 2020) or using iterative self-learning (Wang et a"
2020.acl-srw.34,P19-1178,0,0.0378085,"Missing"
2020.acl-srw.34,N19-1162,0,0.0168312,"spaces were proposed by Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependency parsing. Pires et al. (2019) extract contextualized embeddings directly from unsupervised multilingual LMs and use them for parallel sentence retrieval. Other authors improve the alignment of representations in a multilingual LM using a parallel corpus as an anchor (Cao et al., 2020) or using iterative self-learning (Wang et al., 2019a). None of these works apply multilingual embeddings to mine parallel sentences. Our work is the first in impro"
2020.acl-srw.34,P18-2037,0,0.405792,"e results for other language pairs. 1 Introduction Parallel corpora constitute an essential training data resource for machine translation as well as other cross-lingual NLP tasks. However, large parallel corpora are only available for a handful of language pairs while the rest relies on semi-supervised or unsupervised methods for training. Since monolingual data are generally more abundant, parallel sentence mining from non-parallel corpora provides another opportunity for low-resource language pairs. An effective approach to parallel data mining is based on multilingual sentence embeddings (Schwenk, 2018; Artetxe and Schwenk, 2019b). However, existing methods to generate crosslingual representations are either heavily supervised or only apply to static word embeddings. An alternative approach to unsupervised multilingual training is that of Devlin et al. (2018) or Lample and Conneau (2019), who train a masked language model (M-BERT, XLM) on a concatenation of monolingual corpora in different languages to learn a joint structure of these languages together. While several authors (Pires et al., 2019; Wu and Dredze, 2019; Karthikeyan et al., 2019; Libovick´y et al., 2019) bring evidence of cross"
2020.acl-srw.34,W17-2619,0,0.0242961,"supervised methods to model multilingual sentence embeddings and unsupervised methods to model multilingual word embeddings which can be aggregated into sentences. Furthermore, our approach is closely related to the recent research in cross-lingual language model (LM) pretraining. Supervised multilingual sentence embeddings. The state-of-the-art performance in parallel data mining is achieved by LASER (Artetxe and Schwenk, 2019b) – a multilingual BiLSTM model sharing a single encoder for 93 languages trained on parallel corpora to produce language agnostic sentence representations. Similarly, Schwenk and Douze (2017); Schwenk (2018); Espana-Bonet 1 11th Workshop on Building and Using Comparable Corpora 255 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 255–262 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al. (2017) derive sentence embeddings from internal representations of a neural machine translation system with a shared encoder. The universal sentence encoder (USE) (Cer et al., 2018; Yang et al., 2019) family covers sentence embedding models with a multi-task dual-encoder training framework includi"
2020.acl-srw.34,P16-1162,0,0.0146403,"o the method remains unsupervised. In this section, we describe the pretrained model (Section 3.1), the fine-tuning objective (Section 3.2) and the extraction of sentence embeddings (Section 3.3). We provide details on the unsupervised MT system in Section 3.4. 3.1 XLM Pretraining The starting point for our experiments is a crosslingual language model (XLM) (Lample and Conneau, 2019) of the BERT family pretrained on concatenated monolingual texts in 100 languages using the masked language model (MLM) training objective (Devlin et al., 2018). The model processes the input in BPE subword units (Sennrich et al., 2016) with a shared vocabulary for all languages. In this work, we use the publicly available pretrained model XLM-1002 (Lample and Conneau, 2019) with 16 transformer layers, 16 attention heads and a hidden unit size of 1280. The model was trained on monolingual corpora in 100 languages with the BPE vocabulary of 240k subwords. 3.2 XLM Fine-tuning with a Translation Objective When parallel data is available, it can be leveraged in training of the multilingual language model using a translation language model loss (TLM) (Lample and Conneau, 2019). Pairs of sentences are concatenated, random tokens a"
2020.acl-srw.34,D19-1449,0,0.0533004,"Missing"
2020.acl-srw.34,D19-1575,0,0.0182848,"Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependency parsing. Pires et al. (2019) extract contextualized embeddings directly from unsupervised multilingual LMs and use them for parallel sentence retrieval. Other authors improve the alignment of representations in a multilingual LM using a parallel corpus as an anchor (Cao et al., 2020) or using iterative self-learning (Wang et al., 2019a). None of these works apply multilingual embeddings to mine parallel sentences. Our work is the first in improving unsupervised c"
2020.acl-srw.34,D19-1077,0,0.0143831,"ve approach to parallel data mining is based on multilingual sentence embeddings (Schwenk, 2018; Artetxe and Schwenk, 2019b). However, existing methods to generate crosslingual representations are either heavily supervised or only apply to static word embeddings. An alternative approach to unsupervised multilingual training is that of Devlin et al. (2018) or Lample and Conneau (2019), who train a masked language model (M-BERT, XLM) on a concatenation of monolingual corpora in different languages to learn a joint structure of these languages together. While several authors (Pires et al., 2019; Wu and Dredze, 2019; Karthikeyan et al., 2019; Libovick´y et al., 2019) bring evidence of cross-lingual transfer within the model, its internal representations are not entirely language agnostic. We propose a method to further align representations from such models into the cross-lingual space and use them to derive sentence embeddings. Our approach is completely unsupervised and is applicable even for very distant language pairs. The proposed method outperforms previous unsupervised approaches on the BUCC 20181 shared task, and is even competitive with several supervised baselines. The paper is organized as fol"
2020.acl-srw.34,W17-2512,0,0.0208881,"sentence mining task (News test set). The supervised and unsupervised winners are highlighted in bold. Artetxe and Schwenk (2019b) values obtained using the public implementation of the LASER toolkit. 4.4 Evaluation I: Parallel Corpus Mining We measure the performance of our method on the BUCC shared task of parallel corpus mining where the system is expected to search two comparable non-aligned corpora and identify pairs of parallel sentences. We evaluate on two data sets – the original BUCC 2018 corpus created by inserting parallel sentences into monolingual texts extracted from Wikipedia (Zweigenbaum et al., 2017) and a new BUCC-like data set (News train and test) which we created by shuffling 10k parallel sentence from News Commentary into 400k monolingual sentences from News Crawl. The BUCC and News data sets are comparable in size and contain parallel sentences from the same source, but differ in overall domain. In order to score all candidate sentence pairs, we use the margin-based approach of Artetxe and Schwenk (2019a) which was proved to eliminate the hubness problem of embedding spaces and yield superior results (Artetxe and Schwenk, 2019b). The score relies on cosine similarity to measure the"
2020.eamt-1.3,D18-1549,0,0.0500172,"g language with the parent model (in our case Czech). All in all, these experiments illustrate the robustness of the Transformer model that it is able to train and reasonably well utilize pre-trained weights even if they are severely crippled. MT system. Firat et al. (2016) propose multi-way multi-lingual systems. Their goal is to reduce the total number of parameters needed to train multiple source and target models. In all cases, the methods are dependent on a special training schedule. The lack of parallel data in low-resource language pairs can also be tackled by unsupervised translation (Artetxe et al., 2018; Lample et al., 2018). The general idea is to train monolingual autoencoders for both source and target languages separately, followed by mapping both embeddings to the same space and training simultaneously two models, each translating in a different direction. In an iterative training, this pair of NMT systems is further refined, each system providing training data for the other one by back-translating monolingual data (Sennrich et al., 2016). For very closely related language pairs, transliteration can be used to generate training data from a high-resourced pair to support the low-resource"
2020.eamt-1.3,N16-1101,0,0.0243032,"oder , 3 4 (resp. decoder , 7 ) 8 being the only non-frozen component shows that model is not capable of providing all the needed capacity for the new language, unlike the self-attention where the loss is not that large. This behaviour correlates with our intuition that the model needs to update the most the component that handles the differing language with the parent model (in our case Czech). All in all, these experiments illustrate the robustness of the Transformer model that it is able to train and reasonably well utilize pre-trained weights even if they are severely crippled. MT system. Firat et al. (2016) propose multi-way multi-lingual systems. Their goal is to reduce the total number of parameters needed to train multiple source and target models. In all cases, the methods are dependent on a special training schedule. The lack of parallel data in low-resource language pairs can also be tackled by unsupervised translation (Artetxe et al., 2018; Lample et al., 2018). The general idea is to train monolingual autoencoders for both source and target languages separately, followed by mapping both embeddings to the same space and training simultaneously two models, each translating in a different d"
2020.eamt-1.3,N18-1032,0,0.0147122,"air has been approached from various angles. One option is to build multilingual models (Liu et al., 2020), ideally so that they are capable of zero-shot, i.e. translating in a translation direction that was never part of the training data. Johnson et al. (2017) and Lu et al. (2018) achieve this with a unique language tag that specifies the desired target language. The training data includes sentence pairs from multiple language pairs, and the model implicitly learns translation among many languages. In some cases, it achieves zero-shot and can translate between languages never seen together. Gu et al. (2018) tackled the problem by creating universal embedding space across multiple languages and training many-to-one Conclusion In this paper, we focus on a setting where existing models are re-used without any preparation for knowledge transfer of original model ahead of its training. This is a relevant and prevailing situation in academia due to computing restrictions, and industry, where updating existing models and scaling to more language pairs is essential. We evaluate and propose methods of re-using Transformer NMT models for any “child” language pair regardless of the original “parent” traini"
2020.eamt-1.3,P19-1120,0,0.121138,"ral possible mappings between the parent and child vocabulary. We tried to assign subwords based on frequency, by random assignment, or based on Levenshtein distance of parent and child subwords. However, all the approaches reached comparable performance; neither of them significantly outperformed the others. One exception is when assigning all subwords randomly, even those that are shared between parent and child. This method leads to worse performance, having several BLEU points lower than other approaches. Another approach would be to use pretrained subword embeddings similarly as proposed Kim et al. (2019). However, in this paper, we focus on showing, that transfer learning can be as simple as not using any modifications at all. 3 Experiments In this section, we first provide the details of the NMT model used in our experiments and the examined set of language pairs. We then discuss the convergence and a stopping criterion and finally present the results of our method for recycling the NMT model as well as improvements thanks to the vocabulary transformation. 3.1 Parent Model and its Training Data In order to document that our method functions in general and is not restricted to our laboratory"
2020.eamt-1.3,W18-6325,1,0.706253,"ropose a second method based on a vocabulary transformation technique that makes even larger improvements, especially for languages using an alphabet different from the re-used parent model. Our transfer learning approach leads to better performance as well as faster convergence speed of the “child” model compared to training the model from scratch. We document that our methods are 1 The paper reports numbers based on the U.S. energy mix. not restricted only to low-resource languages, but they can be used even for high-resource ones. Previous transfer learning techniques (Neubig and Hu, 2018; Kocmi and Bojar, 2018) rely on a shared vocabulary between the parent and child models. As a result, these techniques separately train parent model for each different child language pair. In contrast, our approach can re-use one parent model for multiple various language pairs, thus further lowering the total training time needed. In order to document that our approach is not restricted to parent models trained by us, we re-use parent model trained by different researchers: we use the winning model of WMT 2019 for CzechEnglish language pair (Popel et al., 2019). The paper is organized as follows: Section 2 describe"
2020.eamt-1.3,W04-3250,0,0.0329077,"Missing"
2020.eamt-1.3,J82-2005,0,0.581597,"Missing"
2020.eamt-1.3,2020.tacl-1.47,0,0.0187131,"ining time and translation quality without any need to modify the model or pre-trained weights. Lakew et al. (2018) presented two model modifications for multilingual MT and showed that transfer learning could be extended to transferring from the parent to the first child, followed by the second child and then the third one. They achieved improvements with dynamically updating embeddings for the vocabulary of a target language. The use of other language pairs for improving results for the target language pair has been approached from various angles. One option is to build multilingual models (Liu et al., 2020), ideally so that they are capable of zero-shot, i.e. translating in a translation direction that was never part of the training data. Johnson et al. (2017) and Lu et al. (2018) achieve this with a unique language tag that specifies the desired target language. The training data includes sentence pairs from multiple language pairs, and the model implicitly learns translation among many languages. In some cases, it achieves zero-shot and can translate between languages never seen together. Gu et al. (2018) tackled the problem by creating universal embedding space across multiple languages and t"
2020.eamt-1.3,W18-6309,0,0.0213148,"ed that transfer learning could be extended to transferring from the parent to the first child, followed by the second child and then the third one. They achieved improvements with dynamically updating embeddings for the vocabulary of a target language. The use of other language pairs for improving results for the target language pair has been approached from various angles. One option is to build multilingual models (Liu et al., 2020), ideally so that they are capable of zero-shot, i.e. translating in a translation direction that was never part of the training data. Johnson et al. (2017) and Lu et al. (2018) achieve this with a unique language tag that specifies the desired target language. The training data includes sentence pairs from multiple language pairs, and the model implicitly learns translation among many languages. In some cases, it achieves zero-shot and can translate between languages never seen together. Gu et al. (2018) tackled the problem by creating universal embedding space across multiple languages and training many-to-one Conclusion In this paper, we focus on a setting where existing models are re-used without any preparation for knowledge transfer of original model ahead of i"
2020.eamt-1.3,P16-1009,0,0.0638716,"are dependent on a special training schedule. The lack of parallel data in low-resource language pairs can also be tackled by unsupervised translation (Artetxe et al., 2018; Lample et al., 2018). The general idea is to train monolingual autoencoders for both source and target languages separately, followed by mapping both embeddings to the same space and training simultaneously two models, each translating in a different direction. In an iterative training, this pair of NMT systems is further refined, each system providing training data for the other one by back-translating monolingual data (Sennrich et al., 2016). For very closely related language pairs, transliteration can be used to generate training data from a high-resourced pair to support the low-resourced one as described in Karakanta et al. (2018). 7 6 Related Work This paper focuses on re-using an existing NMT model in order to improve the performance in terms of training time and translation quality without any need to modify the model or pre-trained weights. Lakew et al. (2018) presented two model modifications for multilingual MT and showed that transfer learning could be extended to transferring from the parent to the first child, followe"
2020.eamt-1.3,P19-1355,0,0.0614056,"Missing"
2020.eamt-1.3,P12-3005,0,0.0506021,"Missing"
2020.eamt-1.3,D17-1156,0,0.0176977,"t vocabulary. We suppose that the subword vocabulary can handle the child language pair, although it is not optimized for it. We take an already trained model and use it as initialization for a child model using a different language pair. We continue the training process without any change to the vocabulary or hyperparameters. This applies even to the training parameters, such as the learning rate or moments. This method of continued training on different data while preserving hyper-parameters is used under the name “continued training” or “fine-tuning” (Hinton and Salakhutdinov, 2006; Miceli Barone et al., 2017), but it is mostly used as a domain adaptation within a given language pair. Direct Transfer relies on the fact that the current NMT uses subword units instead of words. The subwords are designed to handle unseen words or even characters, breaking the input into shorter units, possibly down to individual bytes as implemented, for example, by Tensor2Tensor (Vaswani et al., 2018). Avg. # per: Odia Estonian Finnish German Russian French Child-specific Sent. Word 95.8 3.7 26.0 1.1 22.9 1.1 27.4 1.3 33.3 1.3 42.0 1.6 EN-CS vocab. Sent. Word 496.8 19.1 56.2 2.3 55.9 2.6 55.4 2.5 134.9 5.3 65.7 2.5 I"
2020.eamt-1.3,W18-6313,0,0.119852,". The most substantial improvements are roughly ten times faster for Estonian-to-English, and the smallest difference for English-to-Russian is two times faster. This is mostly because their method first needs to train the parent model that is specialized for the child, while our method can directly re-use any already trained model. Moreover, we can see that their method is even slower than the baseline model. 5 Analysis by Freezing Parameters To discover which transferred parameters are the most helpful for the child model and which need to be changed the most, we follow the analysis used by Thompson et al. (2018): When training the child, we freeze some of the parameters. Based on the internal layout of the Transformer model in Tensor2Tensor, we divide the model into four components. (i) Word embeddings (shared between encoder and decoder) map each subword unit to a dense vector representation. (ii) The encoder component includes all the six feed-forward layers converting the input sequence to the deeper representation. (iii) The decoder component consists again of six feed-forward layers preparing the choice of the next output subword unit. (iv) The multi-head attention is used throughout encoder and"
2020.eamt-1.3,D18-1103,0,0.0792565,"rs. Furthermore, we propose a second method based on a vocabulary transformation technique that makes even larger improvements, especially for languages using an alphabet different from the re-used parent model. Our transfer learning approach leads to better performance as well as faster convergence speed of the “child” model compared to training the model from scratch. We document that our methods are 1 The paper reports numbers based on the U.S. energy mix. not restricted only to low-resource languages, but they can be used even for high-resource ones. Previous transfer learning techniques (Neubig and Hu, 2018; Kocmi and Bojar, 2018) rely on a shared vocabulary between the parent and child models. As a result, these techniques separately train parent model for each different child language pair. In contrast, our approach can re-use one parent model for multiple various language pairs, thus further lowering the total training time needed. In order to document that our approach is not restricted to parent models trained by us, we re-use parent model trained by different researchers: we use the winning model of WMT 2019 for CzechEnglish language pair (Popel et al., 2019). The paper is organized as fol"
2020.eamt-1.3,W19-5337,1,0.883259,"Missing"
2020.eamt-1.3,W18-6424,0,0.0345832,"Missing"
2020.eamt-1.3,W18-6319,0,0.0779036,"Missing"
2020.eamt-1.3,W18-1819,0,0.133185,"as the learning rate or moments. This method of continued training on different data while preserving hyper-parameters is used under the name “continued training” or “fine-tuning” (Hinton and Salakhutdinov, 2006; Miceli Barone et al., 2017), but it is mostly used as a domain adaptation within a given language pair. Direct Transfer relies on the fact that the current NMT uses subword units instead of words. The subwords are designed to handle unseen words or even characters, breaking the input into shorter units, possibly down to individual bytes as implemented, for example, by Tensor2Tensor (Vaswani et al., 2018). Avg. # per: Odia Estonian Finnish German Russian French Child-specific Sent. Word 95.8 3.7 26.0 1.1 22.9 1.1 27.4 1.3 33.3 1.3 42.0 1.6 EN-CS vocab. Sent. Word 496.8 19.1 56.2 2.3 55.9 2.6 55.4 2.5 134.9 5.3 65.7 2.5 Input: Parent vocabulary (an ordered list of parent subwords) and the training corpus for the child language pair. Generate child-specific vocabulary with the maximum number of subwords equal to the parent vocabulary size; for subword S in parent vocabulary do if S in child vocabulary then continue; else Replace position of S in the parent vocabulary with the first unused child"
2020.eamt-1.3,D16-1163,0,0.0260274,"escribes the results of our methods followed by the analysis in Section 5. Related work is summarized in Section 6 and we conclude the discussion in Section 7. 2 Transfer Learning In this work, we present the use of transfer learning to reduce the training time and improve the performance in comparison to training from random initialization even for high-resource language pairs. Transfer learning is an approach of using training data from a related task to improve the accuracy of the main task in question (Tan et al., 2018). One of the first transfer learning techniques in NMT was proposed by Zoph et al. (2016). They used wordlevel NMT and froze several model parts, especially embeddings of words that are shared between parent and child model. We build upon the work of Kocmi and Bojar (2018), who simplified the transfer learning technique thanks to the use of subword units (Wu et al., 2016) in contrast to word-level NMT transfer learning (Zoph et al., 2016) and extended the applicability to unrelated languages. Their only requirement, and also the main disadvantage of the method, is that the vocabulary has to be shared and constructed for the given parent and child languages jointly, which makes the"
2020.eamt-1.53,D19-1081,1,0.842597,"erpreter would not be available for capacity reasons. The 43 languages are 24 EU official languages and 19 others, spoken between Morocco and Kazachstan. With our other user partner, alfatraining, we connect our system with an online meeting platform, alfaview®. 2.2 Other Research Topics The most visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascade"
2020.eamt-1.53,P19-1116,1,0.81532,"erpreter would not be available for capacity reasons. The 43 languages are 24 EU official languages and 19 others, spoken between Morocco and Kazachstan. With our other user partner, alfatraining, we connect our system with an online meeting platform, alfaview®. 2.2 Other Research Topics The most visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascade"
2020.eamt-1.53,W19-5355,1,0.883897,"Missing"
2020.eamt-1.53,D19-1083,1,0.761049,"st visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascaded and fully end-to-end neural spoken language translation (Pham et al., 2019; Nguyen et al., 2019; Nguyen et al., 2020) and co-organize shared tasks at WMT and IWSLT. 2.3 Automatic Minuting The last objective of our project is an automatic system for structured summaries of meetings. It is a ch"
2020.iwltp-1.7,2012.eamt-1.60,0,0.030274,"(Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of data. • In practice, having a small number of multilingual systems to cover all language pairs significantly reduces the development and deployment efforts compared with having one system for each pair. 5. While each of the components (ASR, punctuation, M"
2020.iwltp-1.7,2015.iwslt-papers.8,1,0.871161,"Missing"
2020.iwltp-1.7,Q17-1024,0,0.0913848,"Missing"
2020.iwltp-1.7,2005.mtsummit-papers.11,0,0.143289,"f that sentence is displayed and never changed again by the update mechanism, to under 5 seconds. Machine Translation System With the ultimate goal of featuring a translation system for all EUROSAI languages, we opt for the multilingual approach (Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of"
2020.iwltp-1.7,W19-5337,1,0.607584,"al conferences and remote conferencing) is described in the respective sections below. Our multilingual systems are based on the neural sequenceto-sequence with attention framework (Bahdanau et al., 2014) and shares the internal representation across languages (Pham et al., 2017). At present, we have one manyto-many Transformer model (Vaswani et al., 2017) providing translation between all pairings of 36 languages, along with several specialized models focused on subsets of languages, in particular the project’s primary languages of English, Czech, and German, see i.a. (Popel and Bojar, 2018; Popel et al., 2019). The resulting multilingual models after training can be used immediately in deployment or can go through a language adaptation step. This language adaptation is simply continuing training the multilingual model on the data of a specific language pair for a few epochs in order to improve the individual translation performance. While we need to do this language adaptation for every single language pair in our system, it is a trivial job since we could automate the process with the same settings and it takes only a little of time and computing resources to reach decent performances. 4.2. Practi"
2020.iwltp-1.7,steinberger-etal-2006-jrc,0,0.163597,"Missing"
2020.iwslt-1.1,P18-4020,0,0.0279555,"Missing"
2020.iwslt-1.1,2005.iwslt-1.19,0,0.174539,"Missing"
2020.iwslt-1.1,2020.iwslt-1.11,0,0.0306756,"Missing"
2020.iwslt-1.1,W18-6319,0,0.0215087,"Missing"
2020.iwslt-1.1,2013.iwslt-papers.14,0,0.069836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.9,0,0.053316,"Missing"
2020.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0549836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.22,0,0.0613503,"Missing"
2020.iwslt-1.24,W17-3203,0,0.023048,"log/ models/nvidia:multidataset_jasper10x5dr Text Encoding Considerations We use Byte Pair Encoding (BPE) (Sennrich et al., 2016) for text encoding in our experiments. We use the implementation in YouTokenToMe6 library. It is fast and offers BPE-dropout (Provilkov et al., 2019) regularization technique. First, we decided to use separate vocabularies for source and target sentences, because the source and target representations, IPA phonemes and English graphemes, have no substantial overlap. There has been a quite intensive discussion about vocabulary size in neural machine translation (NMT) (Denkowski and Neubig, 2017; Gupta et al., 2019; Ding et al., 2019). All works agree that for low-resource translation tasks, it is better to apply smaller vocabulary sizes. For a high-resource task, it is convenient to use larger vocabulary. Our task, translation of phonemes into graphemes in the same language, differs from the previous works. Hence, we decided to experiment with vocabulary sizes. We also want to know whether we should train the sub-word units for source on clean data (phonemes without errors), or we should introduce ASR-like errors to these data. We design the experiment as follows: we test character-"
2020.iwslt-1.24,W19-6620,0,0.0167302,"t Encoding Considerations We use Byte Pair Encoding (BPE) (Sennrich et al., 2016) for text encoding in our experiments. We use the implementation in YouTokenToMe6 library. It is fast and offers BPE-dropout (Provilkov et al., 2019) regularization technique. First, we decided to use separate vocabularies for source and target sentences, because the source and target representations, IPA phonemes and English graphemes, have no substantial overlap. There has been a quite intensive discussion about vocabulary size in neural machine translation (NMT) (Denkowski and Neubig, 2017; Gupta et al., 2019; Ding et al., 2019). All works agree that for low-resource translation tasks, it is better to apply smaller vocabulary sizes. For a high-resource task, it is convenient to use larger vocabulary. Our task, translation of phonemes into graphemes in the same language, differs from the previous works. Hence, we decided to experiment with vocabulary sizes. We also want to know whether we should train the sub-word units for source on clean data (phonemes without errors), or we should introduce ASR-like errors to these data. We design the experiment as follows: we test character-level encoding and BPE vocabulary sizes"
2020.iwslt-1.24,P03-1020,0,0.0584133,"ed systems outperform publicly available Google and Microsoft ASR on all files in the development set, see the last two rows of Table 3. 5 Punctuation, Truecasing and Segmentation Our ASR system produces lowercased, unpunctuated text, but the machine translation works on capitalized, punctuated text, segmented to individual sentences. We use the same biRNN punctuator, truecaser and segmenter as Mach´acˇ ek et al. (2020). The punctuator is a bidirectional recurrent neural network by Tilk and Alum¨ae (2016) trained on the English side of CzEng (Bojar et al., 2016). The truecaser uses tri-grams (Lita et al., 2003). We use a rule-based Moses Sentence Splitter (Koehn et al., 2007). More details are in Mach´acˇ ek et al. (2020), Section 4.2. 6 Machine Translation Our submission to the SLT track relies on the MT systems, which are used also by ELITR project and are described in their submission to this task (Mach´acˇ ek et al., 2020). We do not rely on their validation for this task. As our primary MT systems, we select “WMT18 T2T” for Czech and “de T2T” for German, because they were easily accessible 196 Name Initialization Encoder Decoder asr (primary) asr lm asr slt asr slt lm bert random random EN CS E"
2020.iwslt-1.24,2020.iwslt-1.25,1,0.823596,"Missing"
2020.iwslt-1.24,P07-2045,1,0.0153207,"Missing"
2020.iwslt-1.24,W18-6424,0,0.018557,"In this manner, we yield five different configurations for submission (see Table 6). The transcripts are then punctuated and truecased. Based on the punctuation, we further segment the transcripts. Our primary submission for the ASR track is the “asr” system. We do not have our own translation model. To participate in the translation track, we utilize the MT systems of the ELITR project, which are mostly Transformer neural models. We select as our primary submission the “asr” system. 8 service7 . through Lindat “WMT18 T2T” was originally trained for English-Czech WMT18 news translation task (Popel, 2018), and was also between the top systems in WMT19 (Popel et al., 2019). It is a singlesentence Transformer Big model in Tensor2Tensor framework (Vaswani et al., 2018). “de T2T” is a similar system, but trained on the data for EnglishGerman WMT news translation. Tables 4 and 5 present BLEU scores of our primary systems for Czech and German, respectively. Note that the files Teddy, Autocentrum and Audit are very short. We submit also all other machine translation systems for Czech and German by ELITR with our “asr” source for contrastive evaluation. See Mach´acˇ ek et al. (2020) for more details."
2020.iwslt-1.24,W19-5337,1,0.845068,"ubmission (see Table 6). The transcripts are then punctuated and truecased. Based on the punctuation, we further segment the transcripts. Our primary submission for the ASR track is the “asr” system. We do not have our own translation model. To participate in the translation track, we utilize the MT systems of the ELITR project, which are mostly Transformer neural models. We select as our primary submission the “asr” system. 8 service7 . through Lindat “WMT18 T2T” was originally trained for English-Czech WMT18 news translation task (Popel, 2018), and was also between the top systems in WMT19 (Popel et al., 2019). It is a singlesentence Transformer Big model in Tensor2Tensor framework (Vaswani et al., 2018). “de T2T” is a similar system, but trained on the data for EnglishGerman WMT news translation. Tables 4 and 5 present BLEU scores of our primary systems for Czech and German, respectively. Note that the files Teddy, Autocentrum and Audit are very short. We submit also all other machine translation systems for Czech and German by ELITR with our “asr” source for contrastive evaluation. See Mach´acˇ ek et al. (2020) for more details. 7 Submission Summary Conclusion We presented our submissions to the"
2020.iwslt-1.24,P19-1179,0,0.0206105,"jolje (1992). An important work popularizing neural networks in ASR to phonemes is Waibel et al. (1989). This work proposes using a time-delayed neural network (TDNN) to model acoustic-phonetic features and the temporal relationship between them. The authors demonstrate that the proposed TDNN can learn shift-invariant internal abstraction of speech and use it to make a robust final decision. 191 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 191–199 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Salesky et al. (2019) suggest using of phonemebased ASR in speech translation. Their end-to-end speech translation pipeline first obtains phoneme alignment using the deep neural network hidden Markov models (DNN-HMM) system and then averages feature vectors with the same phoneme for consecutive frames. Phonemes outputted by DNNHMM then serve as input features for speech translation. 2.2 Phoneme-to-Grapheme Models In most past studies that included a separate phoneme-to-grapheme (P2G) translation component into the ASR, the phoneme representation was used only for out-of-vocabulary (OOV) words, see, e.g. Decadt et"
2020.iwslt-1.24,P16-1162,0,0.0596235,"lity. Setups that use this component are further in this paper marked with “ lm”. Results of training after the Adaptation phase (the “Adaptation” column) and the Full training are in Table 1. Note that these scores are calculated on the reference transcript converted to phonemes using phonemizer. Token ambiguities thus change, and these scores are not comparable to standard grapheme WER. The training is executed on 10 NVIDIA GTX 1080 Ti GPUs with 11 GB VRAM. 5 https://ngc.nvidia.com/catalog/ models/nvidia:multidataset_jasper10x5dr Text Encoding Considerations We use Byte Pair Encoding (BPE) (Sennrich et al., 2016) for text encoding in our experiments. We use the implementation in YouTokenToMe6 library. It is fast and offers BPE-dropout (Provilkov et al., 2019) regularization technique. First, we decided to use separate vocabularies for source and target sentences, because the source and target representations, IPA phonemes and English graphemes, have no substantial overlap. There has been a quite intensive discussion about vocabulary size in neural machine translation (NMT) (Denkowski and Neubig, 2017; Gupta et al., 2019; Ding et al., 2019). All works agree that for low-resource translation tasks, it i"
2020.iwslt-1.24,W18-1819,0,0.0292616,"uation, we further segment the transcripts. Our primary submission for the ASR track is the “asr” system. We do not have our own translation model. To participate in the translation track, we utilize the MT systems of the ELITR project, which are mostly Transformer neural models. We select as our primary submission the “asr” system. 8 service7 . through Lindat “WMT18 T2T” was originally trained for English-Czech WMT18 news translation task (Popel, 2018), and was also between the top systems in WMT19 (Popel et al., 2019). It is a singlesentence Transformer Big model in Tensor2Tensor framework (Vaswani et al., 2018). “de T2T” is a similar system, but trained on the data for EnglishGerman WMT news translation. Tables 4 and 5 present BLEU scores of our primary systems for Czech and German, respectively. Note that the files Teddy, Autocentrum and Audit are very short. We submit also all other machine translation systems for Czech and German by ELITR with our “asr” source for contrastive evaluation. See Mach´acˇ ek et al. (2020) for more details. 7 Submission Summary Conclusion We presented our submissions to the Non-Native Speech Translation Task for IWSLT 2020. For the non-native speech recognition, we pro"
2020.iwslt-1.25,W19-5304,0,0.0104835,"4 have reference translations. The target languages are Czech and German. The task objectives are quality and simultaneity, unlike the previous tasks, which focused only on 1 http://elitr.eu the quality. Despite the complexity, the resulting systems can be potentially appreciated by many users attending an event in a language they do not speak or having difficulties understanding due to unfamiliar non-native accents or unusual vocabulary. We build on our experience from the past IWSLT and WMT tasks, see e.g. Pham et al. (2019); Nguyen et al. (2017); Pham et al. (2017); Wetesko et al. (2019); Bawden et al. (2019); Popel et al. (2019). Each of the participating institutions has offered independent ASR and MT systems trained for various purposes and previous shared tasks. We also create some new systems for this task and deployment for the purposes of the ELITR project. Our short-term motivation for this work is to connect the existing systems into a working cascade for SLT and evaluate it empirically, end-to-end. In the long-term, we want to advance state of the art in non-native speech translation. 2 Overview of Our Submissions This paper is a joint report for two primary submissions, for online and o"
2020.iwslt-1.25,2015.iwslt-papers.8,0,0.111954,"Missing"
2020.iwslt-1.25,2012.iwslt-papers.15,0,0.0217227,"ercased text. The MT systems are designed mostly for individual sentences with proper casing and punctuation. To overcome this, we first insert punctuation and casing to the ASR output. Then, we split it into individual sentences by the punctuation marks by a rule-based language-dependent Moses sentence splitter (Koehn et al., 2007). Depending on the ASR system, we use one of two possible punctuators. Both of them are usable in online mode. 4.1 KIT Punctuator The KIT ASR systems use an NMT-based model to insert punctuation and capitalization in an otherwise unsegmented lowercase input stream (Cho et al., 2012, 2015). The system is a monolingual translation system that translates from raw ASR output to well-formed text by converting words to upper case, inserting punctuation marks, and dropping words that belong to disfluency phenomena. It does not use the typical sequence-to-sequence approach of machine translation. However, it considers a sliding window of recent (uncased) words and classifying each one according to the punctuation that should be inserted and whether the word should be dropped for being a part of disfluency. This gives the system a constant input and output size, removing the nee"
2020.iwslt-1.25,2020.iwltp-1.7,1,0.836557,"Missing"
2020.iwslt-1.25,P18-4020,0,0.0221898,"Missing"
2020.iwslt-1.25,P07-2045,1,0.0196919,"Missing"
2020.iwslt-1.25,D18-2012,0,0.0177928,"e 0-100) for SLT into Czech from ASR sources. The column “gold” is translation from the gold transript. It shows the differences between MT systems, but was not used in validation. The size and the model type of WMT19 Marian and WMT18 T2T are the same (see Popel et al., 2019), but they differ in implementation. WMT19 Marian is slightly faster than IWSLT19 model because the latter is an ensemble of two models. OPUS-B is slower than OPUS-A because the former is bigger. Both are slower than WMT19 Marian due to multi-targeting and different preprocessing. WMT19 Marian uses embedded SentencePiece (Kudo and Richardson, 2018), while the multi-target models use an external Python process for BPE (Sennrich et al., 2016). The timing may be affected also by different hardware. At the validation time, T2T-multi and T2T-multibig used suboptimal setup. 6 able validation. Therefore, we submitted all our unselected candidates for contrastive evaluation on the test set. For the results, we refer to Ansari et al. (2020). Acknowledgments The research was partially supported by the grants 19-26934X (NEUREM3) of the Czech Science Foundation, H2020-ICT-2018-2-825460 (ELITR) of the EU, 398120 of the Grant Agency of Charles Univer"
2020.iwslt-1.25,P03-1020,0,0.534644,"network with an attention-based mechanism by Tilk and Alum¨ae (2016) to restore punctuation in the raw stream of ASR output. The model was trained on 4M English sentences from CzEng 1.6 (Bojar et al., 2016) data and a vocabulary of 100K most frequently occurring words. We use CzEng because it is a mixture of domains, both originally spoken, which is close to the target domain, and written, which has richer vocabulary, and both original English texts and translations, which we also expect in the target domain. The punctuated transcript is then capitalized using an English tri-gram truecaser by Lita et al. (2003). The truecaser was trained on 2M English sentences from CzEng. 5 Machine Translation This section describes the translation part of SLT. 5.1 MT Systems See Table 4 for the summary of the MT systems. All except de-LSTM are Transformer-based neural models using Marian (Junczys-Dowmunt et al., 2018) or Tensor2Tensor (Vaswani et al., 2018) back-end. All of them, except de-T2T, are unconstrained because they are trained not only on the data sets allowed in the task description, but all the used data are publicly available. 5.1.1 WMT Models WMT19 Marian and WMT18 T2T models are Marian and T2T singl"
2020.iwslt-1.25,2005.iwslt-1.19,0,0.0481099,"et al. (2019) and Arivazhagan et al. (2019), considering only the currently completed sentences, or only the “stable” sentences, which are beyond the ASR and punctuator processing window and never change. We do not tune these parameters in the validation. We do not mask any words or segments in our primary submission, but we submit multiple non-primary systems differing in these parameters. 5.4 Quality Validation For comparing the MT candidates for SLT, we processed the validation set by three online ASR systems, translated them by the candidates, aligned them with reference by mwerSegmenter (Matusov et al., 2005) and evaluated the BLEU score (Post, 2018; Papineni et al., 2002) of the individual documents. However, we were aware that the size of the validation set is extremely limited (see Table 2) and that the automatic metrics as the BLEU score estimate the human judgment of the MT quality reliably only if there is a sufficient number of sentences or references. It is not the case of this validation set. Therefore, we examined them by a simple comparison with source and reference. We realized that the high BLEU score in the Autocentrum document is induced by the fact that one of the translated senten"
2020.iwslt-1.25,N16-3017,0,0.324918,"Missing"
2020.iwslt-1.25,P02-1040,0,0.115494,"e currently completed sentences, or only the “stable” sentences, which are beyond the ASR and punctuator processing window and never change. We do not tune these parameters in the validation. We do not mask any words or segments in our primary submission, but we submit multiple non-primary systems differing in these parameters. 5.4 Quality Validation For comparing the MT candidates for SLT, we processed the validation set by three online ASR systems, translated them by the candidates, aligned them with reference by mwerSegmenter (Matusov et al., 2005) and evaluated the BLEU score (Post, 2018; Papineni et al., 2002) of the individual documents. However, we were aware that the size of the validation set is extremely limited (see Table 2) and that the automatic metrics as the BLEU score estimate the human judgment of the MT quality reliably only if there is a sufficient number of sentences or references. It is not the case of this validation set. Therefore, we examined them by a simple comparison with source and reference. We realized that the high BLEU score in the Autocentrum document is induced by the fact that one of the translated sentences matches exactly matches a reference because it is a single wo"
2020.iwslt-1.25,W18-6424,0,0.0712802,"nces from CzEng. 5 Machine Translation This section describes the translation part of SLT. 5.1 MT Systems See Table 4 for the summary of the MT systems. All except de-LSTM are Transformer-based neural models using Marian (Junczys-Dowmunt et al., 2018) or Tensor2Tensor (Vaswani et al., 2018) back-end. All of them, except de-T2T, are unconstrained because they are trained not only on the data sets allowed in the task description, but all the used data are publicly available. 5.1.1 WMT Models WMT19 Marian and WMT18 T2T models are Marian and T2T single-sentence models from Popel et al. (2019) and Popel (2018). WMT18 T2T was originally trained for the English-Czech WMT18 news translation task, and reused in WMT19. WMT19 Marian is its reimplementation in Marian for WMT19. The T2T model has a slightly higher quality on the news text domain than the Marian model. The Marian model translates faster, as we show in Section 5.5. 5.1.2 IWSLT19 Model The IWSLT19 system is an ensemble of two English-to-Czech Transformer Big models trained using the Marian toolkit. The models were originally trained on WMT19 data and then finetuned 203 system back-end source-target constrained reference WMT19 Marian WMT18 T2T"
2020.iwslt-1.25,W19-5337,1,0.67048,"slations. The target languages are Czech and German. The task objectives are quality and simultaneity, unlike the previous tasks, which focused only on 1 http://elitr.eu the quality. Despite the complexity, the resulting systems can be potentially appreciated by many users attending an event in a language they do not speak or having difficulties understanding due to unfamiliar non-native accents or unusual vocabulary. We build on our experience from the past IWSLT and WMT tasks, see e.g. Pham et al. (2019); Nguyen et al. (2017); Pham et al. (2017); Wetesko et al. (2019); Bawden et al. (2019); Popel et al. (2019). Each of the participating institutions has offered independent ASR and MT systems trained for various purposes and previous shared tasks. We also create some new systems for this task and deployment for the purposes of the ELITR project. Our short-term motivation for this work is to connect the existing systems into a working cascade for SLT and evaluate it empirically, end-to-end. In the long-term, we want to advance state of the art in non-native speech translation. 2 Overview of Our Submissions This paper is a joint report for two primary submissions, for online and offline sub-track of t"
2020.iwslt-1.25,W18-6319,0,0.0120837,"ring only the currently completed sentences, or only the “stable” sentences, which are beyond the ASR and punctuator processing window and never change. We do not tune these parameters in the validation. We do not mask any words or segments in our primary submission, but we submit multiple non-primary systems differing in these parameters. 5.4 Quality Validation For comparing the MT candidates for SLT, we processed the validation set by three online ASR systems, translated them by the candidates, aligned them with reference by mwerSegmenter (Matusov et al., 2005) and evaluated the BLEU score (Post, 2018; Papineni et al., 2002) of the individual documents. However, we were aware that the size of the validation set is extremely limited (see Table 2) and that the automatic metrics as the BLEU score estimate the human judgment of the MT quality reliably only if there is a sufficient number of sentences or references. It is not the case of this validation set. Therefore, we examined them by a simple comparison with source and reference. We realized that the high BLEU score in the Autocentrum document is induced by the fact that one of the translated sentences matches exactly matches a reference b"
2020.iwslt-1.25,rousseau-etal-2012-ted,0,0.0699257,"Missing"
2020.iwslt-1.25,P16-1162,0,0.0390235,"ript. It shows the differences between MT systems, but was not used in validation. The size and the model type of WMT19 Marian and WMT18 T2T are the same (see Popel et al., 2019), but they differ in implementation. WMT19 Marian is slightly faster than IWSLT19 model because the latter is an ensemble of two models. OPUS-B is slower than OPUS-A because the former is bigger. Both are slower than WMT19 Marian due to multi-targeting and different preprocessing. WMT19 Marian uses embedded SentencePiece (Kudo and Richardson, 2018), while the multi-target models use an external Python process for BPE (Sennrich et al., 2016). The timing may be affected also by different hardware. At the validation time, T2T-multi and T2T-multibig used suboptimal setup. 6 able validation. Therefore, we submitted all our unselected candidates for contrastive evaluation on the test set. For the results, we refer to Ansari et al. (2020). Acknowledgments The research was partially supported by the grants 19-26934X (NEUREM3) of the Czech Science Foundation, H2020-ICT-2018-2-825460 (ELITR) of the EU, 398120 of the Grant Agency of Charles University, and by SVV project number 260 575. Conclusion We presented ELITR submission for non-nati"
2020.iwslt-1.25,tiedemann-2012-parallel,0,0.0490265,"l. (2019), Section 5.1.1 Wetesko et al. (2019), Section 5.1.2 Section 5.1.3 Section 5.1.3 Section 5.1.4 Section 5.1.4 Dessloch et al. (2018), Section 5.1.6 Section 5.1.5 Table 4: The summary of our MT systems. on MuST-C TED data. The ensemble was a component of Edinburgh and Samsung’s submission to the IWSLT19 Text Translation task. See Section 4 of Wetesko et al. (2019) for further details of the system. 5.1.3 OPUS Multi-Lingual Models The OPUS multilingual systems are one-to-many systems developed within the ELITR project. Both were trained on data randomly sampled from the OPUS collection (Tiedemann, 2012), although they use distinct datasets. OPUS-A is a Transformer Base model trained on 1M sentence pairs each for 7 European target languages: Czech, Dutch, French, German, Hungarian, Polish, and Romanian. OPUSB is a Transformer Big model trained on a total of 231M sentence pairs covering 41 target languages that are of particular interest to the project5 After initial training, OPUS-B was finetuned on an augmented version of the dataset that includes partial sentence pairs, artificially generated by truncating the original sentence pairs (similar to Niehues et al., 2018). We produce up to 10 tr"
2020.iwslt-1.25,W18-1819,0,0.10722,"Missing"
2020.lrec-1.434,Q18-1034,0,0.0313696,"Missing"
2020.lrec-1.434,P17-1080,0,0.0116779,"verb tense by Conneau et al. (2018). Both transfer and probing tasks are integrated into SentEval (Conneau and Kiela, 2018) framework for sentence vector representations. Perone et al. (2018) applied SentEval to eleven different encoding methods revealing that there is no consistently well-performing method across all tasks. SentEval was further criticized for pitfalls such as comparing different embedding sizes or correlation between tasks (Eger et al., 2019; Wieting and Kiela, 2019). Shi et al. (2016) show that NMT encoder is able to capture syntactic information about the source sentence. Belinkov et al. (2017) examine the ability of NMT to learn morphology through POS and morphological tagging. Still, very little is known about the semantic properties of sentence embeddings. Interestingly, C´ıfka and Bojar (2018) observe that the better self-attention embeddings serve in NMT, the worse they perform in most of SentEval tasks. Zhu et al. (2018) generate automatically sentence variations such as: (1) Original sentence: A rooster pecked grain. (2) Synonym Substitution: A cock pecked grain. (3) Not-Negation: A rooster didn’t peck grain. (4) Quantifier-Negation: There was no rooster pecking grain. and co"
2020.lrec-1.434,D15-1075,0,0.0396832,"rain. (3) Not-Negation: A rooster didn’t peck grain. (4) Quantifier-Negation: There was no rooster pecking grain. and compare their triplets by examining distances between their embeddings, i.e. distance between (1) and (2) should be smaller than distances between (1) and (3), (2) and (3), similarly, (3) and (4) should be closer together than (1)–(3) or (1)–(4). In our previous study (Baranˇc´ıkov´a and Bojar, 2019), we examined the effect of small sentence alternations in sentence vector spaces. We used sentence pairs automatically extracted from datasets for natural language inference SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). We observed that the vector difference, familiar from word embeddings, serves reasonably well also in sentence embedding spaces. The examined relations were, however, very simple: a change of gender, number, the addition of an adjective, etc. The structure of the sentence and its wording remained almost identical. We would like to move to more interesting non-trivial sentence comparison, beyond those in Zhu et al. (2018) or Baranˇc´ıkov´a and Bojar (2019), such as change of style of a sentence, the introduction of a small modification that drastically cha"
2020.lrec-1.434,P18-1126,1,0.900089,"Missing"
2020.lrec-1.434,L18-1269,0,0.0191201,"t, sentence sentiment analysis, natural language inference and other assignments. However, even simple bag-of-words (BOW) approaches often achieve competitive results on such tasks (Wieting et al., 2015). Adi et al. (2016) introduce intrinsic evaluation by measuring the ability of models to encode basic linguistic properties of a sentence such as its length, word order, and word occurrences. These so-called ‘probing tasks’ are further extended by a depth of the syntactic tree, top constituent or verb tense by Conneau et al. (2018). Both transfer and probing tasks are integrated into SentEval (Conneau and Kiela, 2018) framework for sentence vector representations. Perone et al. (2018) applied SentEval to eleven different encoding methods revealing that there is no consistently well-performing method across all tasks. SentEval was further criticized for pitfalls such as comparing different embedding sizes or correlation between tasks (Eger et al., 2019; Wieting and Kiela, 2019). Shi et al. (2016) show that NMT encoder is able to capture syntactic information about the source sentence. Belinkov et al. (2017) examine the ability of NMT to learn morphology through POS and morphological tagging. Still, very lit"
2020.lrec-1.434,D17-1070,0,0.0251054,"examples were translated to English for presentation purposes only. The paper is structured as follows: Section 2. summarizes existing methods of sentence embeddings evaluation and related work. Section 3. describes our methodology for constructing our dataset. Section 4. details the obtained dataset and some first observations. We conclude and provide the link to the dataset in Section 5. 2. Background As hinted above, there are many methods of converting a sentence into a vector in a highly dimensional space. To name a few: BiLSTM with the max-pooling trained for natural language inference (Conneau et al., 2017), masked language modelling and next sentence prediction using bidirectional Transformer (Devlin et al., 2018), maxpooling last states of neural machine translation among many languages (Artetxe and Schwenk, 2018) or the encoder final state in attentionless neural machine translation (C´ıfka and Bojar, 2018). The most common way of evaluating methods of sentence embeddings is extrinsic, using so-called ‘transfer tasks’, i.e., comparing embeddings via the performance in downstream tasks such as paraphrasing, entailment, sentence sentiment analysis, natural language inference and other assignmen"
2020.lrec-1.434,P18-1198,0,0.0177882,"ng embeddings via the performance in downstream tasks such as paraphrasing, entailment, sentence sentiment analysis, natural language inference and other assignments. However, even simple bag-of-words (BOW) approaches often achieve competitive results on such tasks (Wieting et al., 2015). Adi et al. (2016) introduce intrinsic evaluation by measuring the ability of models to encode basic linguistic properties of a sentence such as its length, word order, and word occurrences. These so-called ‘probing tasks’ are further extended by a depth of the syntactic tree, top constituent or verb tense by Conneau et al. (2018). Both transfer and probing tasks are integrated into SentEval (Conneau and Kiela, 2018) framework for sentence vector representations. Perone et al. (2018) applied SentEval to eleven different encoding methods revealing that there is no consistently well-performing method across all tasks. SentEval was further criticized for pitfalls such as comparing different embedding sizes or correlation between tasks (Eger et al., 2019; Wieting and Kiela, 2019). Shi et al. (2016) show that NMT encoder is able to capture syntactic information about the source sentence. Belinkov et al. (2017) examine the a"
2020.lrec-1.434,N19-1423,0,0.0614945,"Missing"
2020.lrec-1.434,W19-4308,0,0.0351162,"Missing"
2020.lrec-1.434,P14-5004,0,0.0283648,", multi-purpose multi-lingual sentence embeddings suggests that the LASER space does not exhibit the desired properties. Keywords: sentence embeddings, sentence transformations, paraphrasing, semantic relations 1. Introduction Vector representations are essential in the majority of natural language processing tasks. The popularity of word embeddings started with the introduction of word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) and their properties have been analyzed at length from various aspects. Studies of word embeddings range from word similarity (Hill et al., 2014; Faruqui and Dyer, 2014), over the ability to capture derivational relations (Musil et al., 2019), linear superposition of multiple senses (Arora et al., 2016), the ability to predict semantic hierarchies (Fu et al., 2014) or POS tags (Musil, 2019) up to data efficiency (Jastrzkebski et al., 2017). Several studies (Mikolov et al., 2013c; Mikolov et al., 2013b; Levy and Goldberg, 2014; Vylomova et al., 2015) show that word vector representations are capable of capturing meaningful syntactic and semantic regularities. These include, for example, male/female relation demonstrated by the pairs “man:woman”, “king:queen” a"
2020.lrec-1.434,P14-1113,0,0.0304847,"tions 1. Introduction Vector representations are essential in the majority of natural language processing tasks. The popularity of word embeddings started with the introduction of word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) and their properties have been analyzed at length from various aspects. Studies of word embeddings range from word similarity (Hill et al., 2014; Faruqui and Dyer, 2014), over the ability to capture derivational relations (Musil et al., 2019), linear superposition of multiple senses (Arora et al., 2016), the ability to predict semantic hierarchies (Fu et al., 2014) or POS tags (Musil, 2019) up to data efficiency (Jastrzkebski et al., 2017). Several studies (Mikolov et al., 2013c; Mikolov et al., 2013b; Levy and Goldberg, 2014; Vylomova et al., 2015) show that word vector representations are capable of capturing meaningful syntactic and semantic regularities. These include, for example, male/female relation demonstrated by the pairs “man:woman”, “king:queen” and the country/capital relation (“Russia:Moscow”, “Japan:Tokyo”). These regularities correspond to simple arithmetic operations in the vector space. Sentence embeddings are becoming equally ubiquito"
2020.lrec-1.434,W14-1618,0,0.013492,"th the introduction of word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) and their properties have been analyzed at length from various aspects. Studies of word embeddings range from word similarity (Hill et al., 2014; Faruqui and Dyer, 2014), over the ability to capture derivational relations (Musil et al., 2019), linear superposition of multiple senses (Arora et al., 2016), the ability to predict semantic hierarchies (Fu et al., 2014) or POS tags (Musil, 2019) up to data efficiency (Jastrzkebski et al., 2017). Several studies (Mikolov et al., 2013c; Mikolov et al., 2013b; Levy and Goldberg, 2014; Vylomova et al., 2015) show that word vector representations are capable of capturing meaningful syntactic and semantic regularities. These include, for example, male/female relation demonstrated by the pairs “man:woman”, “king:queen” and the country/capital relation (“Russia:Moscow”, “Japan:Tokyo”). These regularities correspond to simple arithmetic operations in the vector space. Sentence embeddings are becoming equally ubiquitous in NLP, with novel representations appearing almost every other week. With an overwhelming number of methods to compute sentence vector representations, the stud"
2020.lrec-1.434,L16-1147,0,0.0152927,"h test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations that should be challenging for embeddings include minimal change, in which the sentence meaning should be significantly modified by only minimal alternation, or nonsense, in which words of the source sentence should be rearranged into a grammatically correct sentence without any sense. Seed Data The source sentences for annotations were selected from the Czech data of Global Voices (Tiedemann, 2012) and OpenSubtitles4 (Lison and Tiedemann, 2016). We used two sources in order to have different styles of seed sentences, both journalistic and common spoken language. We considered only sentences with more than 5 and less than 15 words and we manually selected 150 of them for further annotation. This step was necessary to remove sentences that are: • too unreal, out of this world, such as: Jedno fotonov´y torp´edo a je z tebe vesm´ırn´a topinka. “One photon torpedo and you’re a space toast.” 3 This requirement was not always respected. The annotators sometimes created very complex descriptions such as specification of information about th"
2020.lrec-1.434,W18-6108,0,0.0537072,"Missing"
2020.lrec-1.434,N13-1090,0,0.0363921,", we should be able to test semantic properties of sentence embeddings and perhaps even to find some topologically interesting “skeleton” in the sentence embedding space. A preliminary analysis using LASER, multi-purpose multi-lingual sentence embeddings suggests that the LASER space does not exhibit the desired properties. Keywords: sentence embeddings, sentence transformations, paraphrasing, semantic relations 1. Introduction Vector representations are essential in the majority of natural language processing tasks. The popularity of word embeddings started with the introduction of word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) and their properties have been analyzed at length from various aspects. Studies of word embeddings range from word similarity (Hill et al., 2014; Faruqui and Dyer, 2014), over the ability to capture derivational relations (Musil et al., 2019), linear superposition of multiple senses (Arora et al., 2016), the ability to predict semantic hierarchies (Fu et al., 2014) or POS tags (Musil, 2019) up to data efficiency (Jastrzkebski et al., 2017). Several studies (Mikolov et al., 2013c; Mikolov et al., 2013b; Levy and Goldberg, 2014; Vylomova et al., 2015) show"
2020.lrec-1.434,D14-1162,0,0.0813217,"Missing"
2020.lrec-1.434,C12-2099,0,0.0424688,"Missing"
2020.lrec-1.434,D16-1159,0,0.0163477,"s. These so-called ‘probing tasks’ are further extended by a depth of the syntactic tree, top constituent or verb tense by Conneau et al. (2018). Both transfer and probing tasks are integrated into SentEval (Conneau and Kiela, 2018) framework for sentence vector representations. Perone et al. (2018) applied SentEval to eleven different encoding methods revealing that there is no consistently well-performing method across all tasks. SentEval was further criticized for pitfalls such as comparing different embedding sizes or correlation between tasks (Eger et al., 2019; Wieting and Kiela, 2019). Shi et al. (2016) show that NMT encoder is able to capture syntactic information about the source sentence. Belinkov et al. (2017) examine the ability of NMT to learn morphology through POS and morphological tagging. Still, very little is known about the semantic properties of sentence embeddings. Interestingly, C´ıfka and Bojar (2018) observe that the better self-attention embeddings serve in NMT, the worse they perform in most of SentEval tasks. Zhu et al. (2018) generate automatically sentence variations such as: (1) Original sentence: A rooster pecked grain. (2) Synonym Substitution: A cock pecked grain. ("
2020.lrec-1.434,tiedemann-2012-parallel,0,0.0201556,"itly selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations that should be challenging for embeddings include minimal change, in which the sentence meaning should be significantly modified by only minimal alternation, or nonsense, in which words of the source sentence should be rearranged into a grammatically correct sentence without any sense. Seed Data The source sentences for annotations were selected from the Czech data of Global Voices (Tiedemann, 2012) and OpenSubtitles4 (Lison and Tiedemann, 2016). We used two sources in order to have different styles of seed sentences, both journalistic and common spoken language. We considered only sentences with more than 5 and less than 15 words and we manually selected 150 of them for further annotation. This step was necessary to remove sentences that are: • too unreal, out of this world, such as: Jedno fotonov´y torp´edo a je z tebe vesm´ırn´a topinka. “One photon torpedo and you’re a space toast.” 3 This requirement was not always respected. The annotators sometimes created very complex description"
2020.lrec-1.434,N18-1101,0,0.0289516,"idn’t peck grain. (4) Quantifier-Negation: There was no rooster pecking grain. and compare their triplets by examining distances between their embeddings, i.e. distance between (1) and (2) should be smaller than distances between (1) and (3), (2) and (3), similarly, (3) and (4) should be closer together than (1)–(3) or (1)–(4). In our previous study (Baranˇc´ıkov´a and Bojar, 2019), we examined the effect of small sentence alternations in sentence vector spaces. We used sentence pairs automatically extracted from datasets for natural language inference SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). We observed that the vector difference, familiar from word embeddings, serves reasonably well also in sentence embedding spaces. The examined relations were, however, very simple: a change of gender, number, the addition of an adjective, etc. The structure of the sentence and its wording remained almost identical. We would like to move to more interesting non-trivial sentence comparison, beyond those in Zhu et al. (2018) or Baranˇc´ıkov´a and Bojar (2019), such as change of style of a sentence, the introduction of a small modification that drastically changes the meaning of a sentence or res"
2020.lrec-1.434,P18-2100,0,0.0282311,"Missing"
2020.lrec-1.823,W97-0703,0,0.781906,"hem to produce a shorter summary which is concise, co6663 herent and readable. In this case, the target or output text contains verbatim copies of words or phrases taken from the source or input. The abstractive approach, on the other hand, learns to paraphrase the information required for the summary, instead of directly copying it from the source. This is somehow better, but the methodology is more complex and requires more resources. The TS research of the 90s and early 00s was mostly based on extractive methods. The respective techniques used unsupervised learning (Goldstein et al., 2000; Barzilay and Elhadad, 1997), supervised learning (Wong et al., 2008; Fukumoto, 2004) or graph methods (Erkan and Radev, 2004; Mani and Bloedorn, 1997) to select the most important lexical units from the source documents. The abstractive approach has become popular in recent years, following the progress in sequence-to-sequence learning with neural networks (the encoder-decoder framework). LSTM neural networks (Hochreiter and Schmidhuber, 1997) are combined and enhanced with advanced mechanisms like the attention of Bahdanau et al. (2015) for a more effective learning of the alignments between the text sequences. Attenti"
2020.lrec-1.823,I13-1062,0,0.0273861,"2 16.8 12.83 18.88 19.12 RL 18.44 28.14 24.03 30.21 30.78 Table 5: Results on OAGSX Table 6: KE scores on OAGKX Method TopicRank RAKE Maui CopyRNN CovRNN F1 @5 17.12 16.36 24.58 28.15 27.76 F1 @7 20.81 18.84 23.49 28.93 29.15 F1 @10 20.75 18.91 23.6 28.96 29.04 Table 7: KE scores on OAGKX 4.2. Keyphrase Generation We ran similar experiments on three evaluation sets derived from OAGKX: a training set of 631705 samples, a validation set of 10 thousand samples and a test set of 10 thousand samples. Once again, we tried and compared both extractive and abstractive KG methods. We used TopicRank of Bougouin et al. (2013) which is a popular graph-based extractive method that makes use of the PageRank algorithm (Brin and Page, 1998). It first uses clustering to group lexical units of the same topic. Then, it uses the graph-based ranking algorithm to score each topic cluster that is formed. At the end, one keyword is picked from each of the ranked clusters. RAKE proposed by Rose et al. (2010) is one of the fastest available methods for extractive KG. It first removes punctuation symbols together with the stop words of the specified language and then creates a graph of word cooccurrences. Candidate words or phras"
2020.lrec-1.823,W19-8630,1,0.776044,"Missing"
2020.lrec-1.823,P19-1208,0,0.0162546,"g (Bahdanau et al., 2015; Vaswani et al., 2017) motivated several researchers like Meng et al. (2017) or Zhang and Xiao (2018) to explore abstractive KG in the context of the encoder-decoder framework. The encoderdecoder network structures were initially utilized to perform MT and got quick adoption on similar tasks like TS and KG that are also based on the sequence-to-sequence transformation between source and target texts. Furthermore, same as in TS research, various reinforcement learning concepts like adaptive rewards that are being explored are raising the performance scores even higher (Chan et al., 2019). Abstractive KG is now a vibrant research direction with more than a dozen of publications only in the last three years. More comprehensive surveys of KG literature can found in other recent publications like (Papagiannopoulou and Tsoumakas, 2019) and (C¸ano and Bojar, 2019b). 2.3. Scientific Article Data Sources The current hype of deep neural networks has created strong incentives for producing data collections by crawling the web. The richest sets of language resources are used for machine translation (Resnik and Smith, 2003; Tiedemann, 2012; Mahata et al., 2016; Shi et al., 2005) and for"
2020.lrec-1.823,W00-0405,0,0.532457,"e effect. From academia, there are initiatives such as ArnetMiner (Tang et al., 2008) that try to integrate existing scientific data from various resources in common networks for easier retrieval and exploitation. Among the various types of texts published in the Web, the metadata of research articles (e.g., titles, abstracts, keywords, etc.) are probably the easiest to find in large quantities, since they are usually not restricted. In fact, small corpora of research articles were used since the 90s to explore extractive KG (Witten et al., 1999; Turney, 1999) and TS (Mani and Bloedorn, 1997; Goldstein et al., 2000) techniques. Research on these tasks has switched from the extractive paradigm to the recent abstractive one that is based on sequence-to-sequence learning with neural networks. The respective models are usually data-hungry, emphasizing the need for larger corpora in both TS and KG tasks. In this paper, we first review the popular existing datasets used for TS and KG research. We later describe the processing steps we followed, starting from the retrieval of ArnetMiner OAG (Open Academic Graph) data collection to the creation of two novel and huge corpora: OAGSX2 and OAGKX.3 The first one cont"
2020.lrec-1.823,N18-1065,0,0.0150056,"amework. The current TS and KG methods are also highly dependent on the big language corpora since they are mainly based on sequence-to-sequence learning with neural networks. Some of the most popular corpora in TS and KG literature are presented in Table 1. One of the first big datasets was the annotated English Gigaword (Napoles et al., 2012) used for abstractive TS by Rush et al. (2015). It contains about nine million news articles and headline summaries. Each headline was paired with the first sentence of the corresponding article to create the training base for the experiments. Newsroom (Grusky et al., 2018) is a very recent and heterogeneous bundle of about 1.3 million news articles. It contains writings published from 1998 to 2017 by 38 major newsrooms. Another recent dataset of news articles 6664 Reference Napoles et al. (2012) Grusky et al. (2018) Nallapati et al. (2016) Hyperlink Hulth (2003) Krapivin et al. (2010) Kim et al. (2010) Meng et al. (2017) Nikolov et al. (2018) Nikolov et al. (2018) Name Gigaword Newsroom CNN/DM DUC-2004 Inspec Krapivin SemEval KP20k tit-gen abs-gen Content News News News News Papers Papers Papers Papers Papers Papers Attribute Total Min / Max Mean (Std) Jindex O"
2020.lrec-1.823,P16-1154,0,0.162419,"owing the progress in sequence-to-sequence learning with neural networks (the encoder-decoder framework). LSTM neural networks (Hochreiter and Schmidhuber, 1997) are combined and enhanced with advanced mechanisms like the attention of Bahdanau et al. (2015) for a more effective learning of the alignments between the text sequences. Attention allows the model to focus on different segments of the input during generation and was successfully used by Rush et al. (2015) to summarize news articles. The problem of unknown words (not seen in source texts) was also mitigated by the copying technique (Gu et al., 2016; Gulcehre et al., 2016). Furthermore, the coverage (Tu et al., 2016a) and intra-attention (Paulus et al., 2017) mechanisms were proposed to alleviate word repetitions in the summaries, a notorious problem of the encoder-decoder models. Scoring results were pushed even further just recently by mixing reinforcement learning concepts such as policy gradient (Rennie et al., 2017) into the encoder-decoder architecture. It optimizes the learning objective (higher summarization score) and still keeps an appropriate quality of the produced summaries. A recent performance comparison of various abstrac"
2020.lrec-1.823,P16-1014,0,0.156726,"s in sequence-to-sequence learning with neural networks (the encoder-decoder framework). LSTM neural networks (Hochreiter and Schmidhuber, 1997) are combined and enhanced with advanced mechanisms like the attention of Bahdanau et al. (2015) for a more effective learning of the alignments between the text sequences. Attention allows the model to focus on different segments of the input during generation and was successfully used by Rush et al. (2015) to summarize news articles. The problem of unknown words (not seen in source texts) was also mitigated by the copying technique (Gu et al., 2016; Gulcehre et al., 2016). Furthermore, the coverage (Tu et al., 2016a) and intra-attention (Paulus et al., 2017) mechanisms were proposed to alleviate word repetitions in the summaries, a notorious problem of the encoder-decoder models. Scoring results were pushed even further just recently by mixing reinforcement learning concepts such as policy gradient (Rennie et al., 2017) into the encoder-decoder architecture. It optimizes the learning objective (higher summarization score) and still keeps an appropriate quality of the produced summaries. A recent performance comparison of various abstractive TS methods can be f"
2020.lrec-1.823,W03-1028,0,0.472172,"ted English Gigaword (Napoles et al., 2012) used for abstractive TS by Rush et al. (2015). It contains about nine million news articles and headline summaries. Each headline was paired with the first sentence of the corresponding article to create the training base for the experiments. Newsroom (Grusky et al., 2018) is a very recent and heterogeneous bundle of about 1.3 million news articles. It contains writings published from 1998 to 2017 by 38 major newsrooms. Another recent dataset of news articles 6664 Reference Napoles et al. (2012) Grusky et al. (2018) Nallapati et al. (2016) Hyperlink Hulth (2003) Krapivin et al. (2010) Kim et al. (2010) Meng et al. (2017) Nikolov et al. (2018) Nikolov et al. (2018) Name Gigaword Newsroom CNN/DM DUC-2004 Inspec Krapivin SemEval KP20k tit-gen abs-gen Content News News News News Papers Papers Papers Papers Papers Papers Attribute Total Min / Max Mean (Std) Jindex Overlap Total size # Docs 9M 1.3 M 287 K 500 2000 2304 244 567 K 900 K 5M Table 2: Token statistics of OAGSX Attribute Total Min / Max Mean (Std) Jindex Overlap Total size Table 1: Summary and keyword generation datasets is CNN/Dailymail of Nallapati et al. (2016). It has become the most popular"
2020.lrec-1.823,D15-1302,0,0.04396,"Missing"
2020.lrec-1.823,S10-1004,0,0.277089,", 2012) used for abstractive TS by Rush et al. (2015). It contains about nine million news articles and headline summaries. Each headline was paired with the first sentence of the corresponding article to create the training base for the experiments. Newsroom (Grusky et al., 2018) is a very recent and heterogeneous bundle of about 1.3 million news articles. It contains writings published from 1998 to 2017 by 38 major newsrooms. Another recent dataset of news articles 6664 Reference Napoles et al. (2012) Grusky et al. (2018) Nallapati et al. (2016) Hyperlink Hulth (2003) Krapivin et al. (2010) Kim et al. (2010) Meng et al. (2017) Nikolov et al. (2018) Nikolov et al. (2018) Name Gigaword Newsroom CNN/DM DUC-2004 Inspec Krapivin SemEval KP20k tit-gen abs-gen Content News News News News Papers Papers Papers Papers Papers Papers Attribute Total Min / Max Mean (Std) Jindex Overlap Total size # Docs 9M 1.3 M 287 K 500 2000 2304 244 567 K 900 K 5M Table 2: Token statistics of OAGSX Attribute Total Min / Max Mean (Std) Jindex Overlap Total size Table 1: Summary and keyword generation datasets is CNN/Dailymail of Nallapati et al. (2016). It has become the most popular corpus for text summarization experiment"
2020.lrec-1.823,W04-1013,0,0.0733436,"Missing"
2020.lrec-1.823,P11-1015,0,0.0737392,"han a dozen of publications only in the last three years. More comprehensive surveys of KG literature can found in other recent publications like (Papagiannopoulou and Tsoumakas, 2019) and (C¸ano and Bojar, 2019b). 2.3. Scientific Article Data Sources The current hype of deep neural networks has created strong incentives for producing data collections by crawling the web. The richest sets of language resources are used for machine translation (Resnik and Smith, 2003; Tiedemann, 2012; Mahata et al., 2016; Shi et al., 2005) and for sentiment analysis (Bosco et al., 2013; C¸ano and Bojar, 2019c; Maas et al., 2011; C¸ano and Morisio, 2019; Jim´enez Zafra et al., 2015). They are mostly driven by the information technology giants that continuously improve their language-related applications and marketing companies to understand customers’ perceptions about various online products. TS and KG research of the 90s and early 00s was mostly based on extractive methods that did not rely on big training corpora. Things gradually changed in the late 00s with the rising popularity of the encoder-decoder framework. The current TS and KG methods are also highly dependent on the big language corpora since they are ma"
2020.lrec-1.823,W16-2373,0,0.0218576,"formance scores even higher (Chan et al., 2019). Abstractive KG is now a vibrant research direction with more than a dozen of publications only in the last three years. More comprehensive surveys of KG literature can found in other recent publications like (Papagiannopoulou and Tsoumakas, 2019) and (C¸ano and Bojar, 2019b). 2.3. Scientific Article Data Sources The current hype of deep neural networks has created strong incentives for producing data collections by crawling the web. The richest sets of language resources are used for machine translation (Resnik and Smith, 2003; Tiedemann, 2012; Mahata et al., 2016; Shi et al., 2005) and for sentiment analysis (Bosco et al., 2013; C¸ano and Bojar, 2019c; Maas et al., 2011; C¸ano and Morisio, 2019; Jim´enez Zafra et al., 2015). They are mostly driven by the information technology giants that continuously improve their language-related applications and marketing companies to understand customers’ perceptions about various online products. TS and KG research of the 90s and early 00s was mostly based on extractive methods that did not rely on big training corpora. Things gradually changed in the late 00s with the rising popularity of the encoder-decoder fra"
2020.lrec-1.823,P14-5010,0,0.00254399,"e. From that same article set, we filtered the records containing at least the title and the abstract for OAGSX and those with the title, abstract, and keywords for OAGKX. We dropped the duplicate entries in each of our two collections. As a result, the samples inside each of the corpora are unique (there is still overlapping between OAGSX and OAGKX samples, since they were both derived from the OAG collection). An automatic language identifier6 was used to remove the records with abstracts not in English. We also cleared the messy symbols and lowercased everything. Finally, Stanford CoreNLP (Manning et al., 2014) was used to tokenize the title and abstract texts. After the preprocessing steps, we observed the size and token lengths of the records. Since there were many outliers (e.g., records with very long or very short abstracts), we removed all records with a title not in the range of 3-25 tokens and abstract not within 50-400 tokens. In the case of OAGKX, we also removed samples with keyword string not in the range of 2-60 tokens or 2-12 keywords. After this, OAGSX was reduced to a total of about 34.4 million records. OAGKX, on the other hand, shrank to about 22.6 million records. https://duc.nist"
2020.lrec-1.823,P17-1054,0,0.146371,"gorithms of that time were used by authors like Turney (2000) or Witten et al. (1999) in combination with lexical features to extract keywords from the documents. Furthermore, graph-based methods (Rose et al., 2010; Wan and Xiao, 2008) or other unsupervised KG methods (Campos et al., 2018; Nart and Tasso, 2014) were proposed later in the 00s. The above extractive KG solutions were very successful because of their simplicity and execution speed. However, extractive KG suffers from a serious inherent handicap: its inability to produce absent keywords (keywords not appearing in the source text). Meng et al. (2017) analyzed the author’s keywords in popular corpora. They observed that absent and present (keywords that also appear in the source text) keywords assigned by paper authors are almost equally frequent. It is thus a serious drawback to completely ignore the absent keywords. The recent advances in language representation (Mikolov et al., 2013; Pennington et al., 2014) and sequence-tosequence learning (Bahdanau et al., 2015; Vaswani et al., 2017) motivated several researchers like Meng et al. (2017) or Zhang and Xiao (2018) to explore abstractive KG in the context of the encoder-decoder framework."
2020.lrec-1.823,K16-1028,0,0.0235602,"first big datasets was the annotated English Gigaword (Napoles et al., 2012) used for abstractive TS by Rush et al. (2015). It contains about nine million news articles and headline summaries. Each headline was paired with the first sentence of the corresponding article to create the training base for the experiments. Newsroom (Grusky et al., 2018) is a very recent and heterogeneous bundle of about 1.3 million news articles. It contains writings published from 1998 to 2017 by 38 major newsrooms. Another recent dataset of news articles 6664 Reference Napoles et al. (2012) Grusky et al. (2018) Nallapati et al. (2016) Hyperlink Hulth (2003) Krapivin et al. (2010) Kim et al. (2010) Meng et al. (2017) Nikolov et al. (2018) Nikolov et al. (2018) Name Gigaword Newsroom CNN/DM DUC-2004 Inspec Krapivin SemEval KP20k tit-gen abs-gen Content News News News News Papers Papers Papers Papers Papers Papers Attribute Total Min / Max Mean (Std) Jindex Overlap Total size # Docs 9M 1.3 M 287 K 500 2000 2304 244 567 K 900 K 5M Table 2: Token statistics of OAGSX Attribute Total Min / Max Mean (Std) Jindex Overlap Total size Table 1: Summary and keyword generation datasets is CNN/Dailymail of Nallapati et al. (2016). It has"
2020.lrec-1.823,W12-3018,0,0.0278641,"Missing"
2020.lrec-1.823,D14-1162,0,0.085018,"Missing"
2020.lrec-1.823,J03-3002,0,0.248651,"at are being explored are raising the performance scores even higher (Chan et al., 2019). Abstractive KG is now a vibrant research direction with more than a dozen of publications only in the last three years. More comprehensive surveys of KG literature can found in other recent publications like (Papagiannopoulou and Tsoumakas, 2019) and (C¸ano and Bojar, 2019b). 2.3. Scientific Article Data Sources The current hype of deep neural networks has created strong incentives for producing data collections by crawling the web. The richest sets of language resources are used for machine translation (Resnik and Smith, 2003; Tiedemann, 2012; Mahata et al., 2016; Shi et al., 2005) and for sentiment analysis (Bosco et al., 2013; C¸ano and Bojar, 2019c; Maas et al., 2011; C¸ano and Morisio, 2019; Jim´enez Zafra et al., 2015). They are mostly driven by the information technology giants that continuously improve their language-related applications and marketing companies to understand customers’ perceptions about various online products. TS and KG research of the 90s and early 00s was mostly based on extractive methods that did not rely on big training corpora. Things gradually changed in the late 00s with the rising"
2020.lrec-1.823,D15-1044,0,0.314295,"edorn, 1997) to select the most important lexical units from the source documents. The abstractive approach has become popular in recent years, following the progress in sequence-to-sequence learning with neural networks (the encoder-decoder framework). LSTM neural networks (Hochreiter and Schmidhuber, 1997) are combined and enhanced with advanced mechanisms like the attention of Bahdanau et al. (2015) for a more effective learning of the alignments between the text sequences. Attention allows the model to focus on different segments of the input during generation and was successfully used by Rush et al. (2015) to summarize news articles. The problem of unknown words (not seen in source texts) was also mitigated by the copying technique (Gu et al., 2016; Gulcehre et al., 2016). Furthermore, the coverage (Tu et al., 2016a) and intra-attention (Paulus et al., 2017) mechanisms were proposed to alleviate word repetitions in the summaries, a notorious problem of the encoder-decoder models. Scoring results were pushed even further just recently by mixing reinforcement learning concepts such as policy gradient (Rennie et al., 2017) into the encoder-decoder architecture. It optimizes the learning objective"
2020.lrec-1.823,P17-1099,0,0.0367824,"which concisely explains the main idea of a text in its first sentence or first few sentences. Lead-1 picks the first sentence from the source text to generate its title. LexRank is a stochastic graph-based method for assessing the importance of textual units in a source text (Erkan and Radev, 2004). When used to perform extractive TS, it computes the importance of those units using the concept of eigenvector centrality in the graph. The top k units (the top sentence in this case) are returned as the best summary of the document. One of the abstractive text summarizers we used is PointCov of See et al. (2017) which is based on the encoderdecoder framework. In each decoding step, it implements the pointing/copying mechanism (Gu et al., 2016; Gulcehre et al., 2016) to compute a generation probability. The latter is used to decide whether the next word should be predicted or directly copied from the source sequence. Another feature is the implementation of the coverage mechanism (Tu et al., 2016a) which helps to avoid word repetitions in the target sequence. We trained PointCov with a hidden layer of 256 dimensions and word embeddings of 128 dimensions. 6666 Abstract: the central bank ’s lender of la"
2020.lrec-1.823,tiedemann-2012-parallel,0,0.0145219,"e raising the performance scores even higher (Chan et al., 2019). Abstractive KG is now a vibrant research direction with more than a dozen of publications only in the last three years. More comprehensive surveys of KG literature can found in other recent publications like (Papagiannopoulou and Tsoumakas, 2019) and (C¸ano and Bojar, 2019b). 2.3. Scientific Article Data Sources The current hype of deep neural networks has created strong incentives for producing data collections by crawling the web. The richest sets of language resources are used for machine translation (Resnik and Smith, 2003; Tiedemann, 2012; Mahata et al., 2016; Shi et al., 2005) and for sentiment analysis (Bosco et al., 2013; C¸ano and Bojar, 2019c; Maas et al., 2011; C¸ano and Morisio, 2019; Jim´enez Zafra et al., 2015). They are mostly driven by the information technology giants that continuously improve their language-related applications and marketing companies to understand customers’ perceptions about various online products. TS and KG research of the 90s and early 00s was mostly based on extractive methods that did not rely on big training corpora. Things gradually changed in the late 00s with the rising popularity of th"
2020.lrec-1.823,P16-1008,0,0.204896,"orks (the encoder-decoder framework). LSTM neural networks (Hochreiter and Schmidhuber, 1997) are combined and enhanced with advanced mechanisms like the attention of Bahdanau et al. (2015) for a more effective learning of the alignments between the text sequences. Attention allows the model to focus on different segments of the input during generation and was successfully used by Rush et al. (2015) to summarize news articles. The problem of unknown words (not seen in source texts) was also mitigated by the copying technique (Gu et al., 2016; Gulcehre et al., 2016). Furthermore, the coverage (Tu et al., 2016a) and intra-attention (Paulus et al., 2017) mechanisms were proposed to alleviate word repetitions in the summaries, a notorious problem of the encoder-decoder models. Scoring results were pushed even further just recently by mixing reinforcement learning concepts such as policy gradient (Rennie et al., 2017) into the encoder-decoder architecture. It optimizes the learning objective (higher summarization score) and still keeps an appropriate quality of the produced summaries. A recent performance comparison of various abstractive TS methods can be found in C ¸ ano and Bojar (2019a). 2.2. Keyp"
2020.lrec-1.823,C08-1124,0,0.0495059,"co6663 herent and readable. In this case, the target or output text contains verbatim copies of words or phrases taken from the source or input. The abstractive approach, on the other hand, learns to paraphrase the information required for the summary, instead of directly copying it from the source. This is somehow better, but the methodology is more complex and requires more resources. The TS research of the 90s and early 00s was mostly based on extractive methods. The respective techniques used unsupervised learning (Goldstein et al., 2000; Barzilay and Elhadad, 1997), supervised learning (Wong et al., 2008; Fukumoto, 2004) or graph methods (Erkan and Radev, 2004; Mani and Bloedorn, 1997) to select the most important lexical units from the source documents. The abstractive approach has become popular in recent years, following the progress in sequence-to-sequence learning with neural networks (the encoder-decoder framework). LSTM neural networks (Hochreiter and Schmidhuber, 1997) are combined and enhanced with advanced mechanisms like the attention of Bahdanau et al. (2015) for a more effective learning of the alignments between the text sequences. Attention allows the model to focus on differen"
2020.lrec-1.823,P06-1062,0,\N,Missing
2020.lrec-1.860,N13-1073,0,0.0240609,"-German. (TP = True positive, FP = False positive, TN = True negative, FN = False negative) of the following. Either the German sentence itself was representative enough for the annotator to produce classes with similar distributions, or that both the English and the Czech sentences provided the same level information. In both cases, the pairs (EN, DE) and (CS, DE) seem equally usable which means that we should be able to train similarly good quality estimation model based on the synthetic Czech source. 3.5. Alignment We use Hunalign (Varga et al., 2007) for sentence alignment and fast align (Dyer et al., 2013) for word alignment, both because of their ease of use and good performance. Both sentence and word alignment systems are unsupervised, operating only on the given input data. Because the real input received by Ptakopˇet is generally very short, we always mix it with a baseline parallel corpus. This increases the vocabulary coverage for word alignment and improves the stability of sentence alignment. The training data for quality estimation (Section 3.4.4.) already limited us to the IT domain. We thus choose a similar domain also for this additional corpus for alignment, the widely available U"
2020.lrec-1.860,C18-1266,0,0.0150161,"asso, using the LARS algorithm. Especially the feature extraction part is not optimized and it is quite slow. The original feature extractor system supports EnglishSpanish quality estimation. We experimented with feeding it English-Czech quality estimation data and expected that the ML part would disregard noisy or low information features caused by feeding the feature extractor unsupported language. We found that the performance regressed so considerably (even on the training data) that we did not experiment further with Czech-German quality estimation in QuEst++. 3.4.2. DeepQuest DeepQuest (Ive et al., 2018) takes a neural approach to quality estimation and is capable of performing well on any language pair. The toolkit offers two architectures: a reimplementation of Predictor-Estimator architecture (Kim et al., 2017) and a bidirectional recurrent neural network (bRNN) system. DeepQuest offers document-level, sentence-level, phrase-level and word-level quality estimation. We trained the bRNN model on WMT17 English-German data and synthesized WMT17 Czech-German data described below. This architecture does not require pretraining, is less complex and provides results close to PredictorEstimator (Iv"
2020.lrec-1.860,P19-3020,0,0.0158996,"ion and is capable of performing well on any language pair. The toolkit offers two architectures: a reimplementation of Predictor-Estimator architecture (Kim et al., 2017) and a bidirectional recurrent neural network (bRNN) system. DeepQuest offers document-level, sentence-level, phrase-level and word-level quality estimation. We trained the bRNN model on WMT17 English-German data and synthesized WMT17 Czech-German data described below. This architecture does not require pretraining, is less complex and provides results close to PredictorEstimator (Ive et al., 2018). 3.4.3. OpenKiwi OpenKiwi (Kepler et al., 2019) implements three quality estimation models: QUality Estimation from ScraTCH (Kreutzer et al., 2015), NuQE (Martins et al., 2016) used for WMT1911 baseline and Predictor-Estimator (Kim et al., 2017). Additionally, OpenKiwi implements stacked ensembling as proposed by Martins et al. (2017). We opted for the Predictor-Estimator architecture for our experiment, because even though it requires pretraining, it does not consume so many resources compared to the stacked ensemble. This architecture also provides the best results in comparison with other architectures without ensembling, as shown in (K"
2020.lrec-1.860,W15-3037,0,0.012886,"a reimplementation of Predictor-Estimator architecture (Kim et al., 2017) and a bidirectional recurrent neural network (bRNN) system. DeepQuest offers document-level, sentence-level, phrase-level and word-level quality estimation. We trained the bRNN model on WMT17 English-German data and synthesized WMT17 Czech-German data described below. This architecture does not require pretraining, is less complex and provides results close to PredictorEstimator (Ive et al., 2018). 3.4.3. OpenKiwi OpenKiwi (Kepler et al., 2019) implements three quality estimation models: QUality Estimation from ScraTCH (Kreutzer et al., 2015), NuQE (Martins et al., 2016) used for WMT1911 baseline and Predictor-Estimator (Kim et al., 2017). Additionally, OpenKiwi implements stacked ensembling as proposed by Martins et al. (2017). We opted for the Predictor-Estimator architecture for our experiment, because even though it requires pretraining, it does not consume so many resources compared to the stacked ensemble. This architecture also provides the best results in comparison with other architectures without ensembling, as shown in (Kepler et al., 2019). OpenKiwi, in general, proved to be faster, more robust and easier to use than D"
2020.lrec-1.860,W19-5323,1,0.870138,"Missing"
2020.lrec-1.860,W16-2387,0,0.0143573,"r-Estimator architecture (Kim et al., 2017) and a bidirectional recurrent neural network (bRNN) system. DeepQuest offers document-level, sentence-level, phrase-level and word-level quality estimation. We trained the bRNN model on WMT17 English-German data and synthesized WMT17 Czech-German data described below. This architecture does not require pretraining, is less complex and provides results close to PredictorEstimator (Ive et al., 2018). 3.4.3. OpenKiwi OpenKiwi (Kepler et al., 2019) implements three quality estimation models: QUality Estimation from ScraTCH (Kreutzer et al., 2015), NuQE (Martins et al., 2016) used for WMT1911 baseline and Predictor-Estimator (Kim et al., 2017). Additionally, OpenKiwi implements stacked ensembling as proposed by Martins et al. (2017). We opted for the Predictor-Estimator architecture for our experiment, because even though it requires pretraining, it does not consume so many resources compared to the stacked ensemble. This architecture also provides the best results in comparison with other architectures without ensembling, as shown in (Kepler et al., 2019). OpenKiwi, in general, proved to be faster, more robust and easier to use than DeepQuest. Because of this, th"
2020.lrec-1.860,Q17-1015,0,0.0158046,"level and word-level quality estimation. We trained the bRNN model on WMT17 English-German data and synthesized WMT17 Czech-German data described below. This architecture does not require pretraining, is less complex and provides results close to PredictorEstimator (Ive et al., 2018). 3.4.3. OpenKiwi OpenKiwi (Kepler et al., 2019) implements three quality estimation models: QUality Estimation from ScraTCH (Kreutzer et al., 2015), NuQE (Martins et al., 2016) used for WMT1911 baseline and Predictor-Estimator (Kim et al., 2017). Additionally, OpenKiwi implements stacked ensembling as proposed by Martins et al. (2017). We opted for the Predictor-Estimator architecture for our experiment, because even though it requires pretraining, it does not consume so many resources compared to the stacked ensemble. This architecture also provides the best results in comparison with other architectures without ensembling, as shown in (Kepler et al., 2019). OpenKiwi, in general, proved to be faster, more robust and easier to use than DeepQuest. Because of this, the experiment was conducted with OpenKiwi quality estimation backend. 3.4.4. Czech-German Quality Estimation dataset Since relevant Czech-German training data fo"
2020.lrec-1.860,W18-6424,0,0.0243868,"use of this, the experiment was conducted with OpenKiwi quality estimation backend. 3.4.4. Czech-German Quality Estimation dataset Since relevant Czech-German training data for QE were not available, we synthesized them from WMT 2017 EnglishGerman Word Level Quality Estimation dataset in the IT domain (Specia and Logacheva, 2017). Such data are composed of source language sentences (EN), target language sentences (DE) and OK/BAD tags for each word (QE). We processed the WMT17 English-German data to obtain Czech-German data by translating the source language sentences using LINDAT Translation (Popel, 2018) from English to Czech. Given triplets (EN, DE, QE), we thus create triplets of (CS, DE, QE). An example of this can be seen in Figure 3. To make sure the data did not lose quality, we performed the following experiment: We manually annotated 30 CzechGerman and 20 English-German sentences for word-level quality estimation, in the same format as the original English-German dataset, i.e. labelling German words with OK/BAD labels given the source sentence. The original English-German annotation served as the golden standard. Our annotation for English-German was created independently of it and it"
2020.lrec-1.860,P18-2124,0,0.0734544,"Missing"
2020.lrec-1.860,U05-1019,0,0.0830032,"with machine translation tools to produce the best result. Machine translation can prepare a first version of the text, or it can be used to verify the user’s own translation to some extent. Users translating into languages which they do not master enough to validate the translation need some additional system for verification and assurance that the machine translation output is valid. For this, Ptakopˇet offers word-level quality estimation (QE), simulated source complexity and backward translation. While round trip translation may be unreliable for fully automatic evaluation of MT quality (Somers, 2005), it is still a widespread strategy for users. The paper is structured as follows: We briefly introduce the components we rely on in Section 2. and describe Ptakopˇet in Section 3., including the underlying models. The experiment setup is presented in Section 4. and the results in Section 5. We conclude in Section 6. 2. 2.1. Machine translation quality estimation is used mostly in translation companies to minimize post-editing costs. Unfortunately, quality estimation cues are missing in most of the mainstream public translation services, such as Google Translate2 (provides alternatives to word"
2020.lrec-1.860,P15-4020,0,0.0300883,"ependent. number of subword units per translation computation. Most sentences fit into this limit, but longer sentences do not, which results in context loss. Generally, both MT models made mistakes occasionally, such as adding extra words or phrases. 3.4. Quality Estimation models Ptakopˇet uses quality estimation for highlighting poorly translated words. There were three available implementations, but none of them was suitable for online use out of the box. We also synthesized QE training data for our language pair of interest, see Section 3.4.4. 3.4.1. QuEst++ The main pipeline of QuEst++ (Specia et al., 2015) consists of feature extraction and machine learning prediction. It first extracts features WMT12-13-14-1710 from the input data, such as POS, indication of words’ presence in a dictionary and word length and then runs a standard ML algorithm e.g. Cross-validated Lasso, using the LARS algorithm. Especially the feature extraction part is not optimized and it is quite slow. The original feature extractor system supports EnglishSpanish quality estimation. We experimented with feeding it English-Czech quality estimation data and expected that the ML part would disregard noisy or low information fe"
2020.lrec-1.860,tiedemann-2012-parallel,0,0.101228,"use of their ease of use and good performance. Both sentence and word alignment systems are unsupervised, operating only on the given input data. Because the real input received by Ptakopˇet is generally very short, we always mix it with a baseline parallel corpus. This increases the vocabulary coverage for word alignment and improves the stability of sentence alignment. The training data for quality estimation (Section 3.4.4.) already limited us to the IT domain. We thus choose a similar domain also for this additional corpus for alignment, the widely available Ubuntu 14.10 parallel corpora (Tiedemann, 2012). Specifically, we use parallel corpora for the following language pairs: EN-CS (6492 sentence pairs), DE-CS (6604 sentence pairs), DE-EN (13245 sentence pairs), CS-FR (6603 sentence pairs), EN-FR (9375 sentence pairs). These corpora are used both for word and sentence alignment. 4. Experiment Setup The goal of our pilot experiment was to observe and describe strategies users take when tasked to do outbound translation and see if and how Ptakopˇet helps in the task. The experiment was carried out remotely, in two phases. In the first phase, annotators were presented with a sequence of web page"
2020.wat-1.1,L18-1548,1,0.762787,"emselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training, development and test data. Par ticipants of this taks must get a copy of BSD corpus by themselves. 2.12 IITB Hindi–English task In this task we use IIT Bombay EnglishHindi Cor pus (Kunchukuttan et al., 2018) which contains EnglishHindi parallel corpus as well as mono lingual Hindi corpus collected from a variety of sources and corpora (Bojar et al., 2014). This cor pus had been developed at the Center for Indian Language Technology, IIT Bombay over the years. The corpus is used for mixed domain tasks hi↔en. The statistics for the corpus are shown in Table 13. 3 4.1 Tokenization We used the following tools for tokenization. 4.1.1 For ASPEC, JPC, TDDC, JIJI, ALT, UCSY, ECCC, and IITB • Juman version 7.024 for Japanese segmenta tion. • Stanford Word Segmenter version 201401 0425 (Chinese Penn"
2020.wat-1.1,E06-1031,0,0.0814841,"ipants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcomes. The scores are thus comparable only wi"
2020.wat-1.1,W17-5701,1,0.728933,"on to documentlevel evaluation. 2.11 Documentlevel Translation Task In WAT2020, we set up 2 documentlevel transla tion tasks: ParaNatCom and BSD. 20 21 http://www2.nict.go.jp/astrec-att/member/ mutiyama/paranatcom/ https://github.com/nlabmpg/Flickr30kEntJP 7 Lang hien hi Train 1,609,682 – Dev 520 – Test 2,507 – Mono – 45,075,279 systems were published on the WAT web page.23 We also have SMT baseline systems for the tasks that started at WAT 2017 or before 2017. The base line systems are shown in Tables 16, 17, and 18. SMT baseline systems are described in the WAT 2017 overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online transla tion systems were operated by the organizers. We note that these RBMT companies and online trans lation companies did not submit themselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training"
2020.wat-1.1,Y18-3001,1,0.753826,"raining, develop ment, and test data of the KhmerEnglish transla tion tasks are listed in Table 6. 1 4 http://opus.nlpl.eu/ Lang.pair Ja↔Ru Ja↔En Ru↔En Table 8: task. Partition train development test train development test train development test #sent. 12,356 486 600 47,082 589 600 82,072 313 600 #tokens 341k / 229k 16k / 11k 22k / 15k 1.27M / 1.01M 21k / 16k 22k / 17k 1.61M / 1.83M 7.8k / 8.4k 15k / 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k 2.7 Indic Multilingual Task In 2018, we had organized an Indic languages task (Nakazawa et al., 2018) but due to lack of reli able evaluation corpora we discontinued it in WAT 2019. However, in 2020, high quality publicly available evaluation (and training) corpora became available which motivated us to relaunch the task. The Indic task involves mixed domain corpora for evaluation consisting of various articles composed by Indian Prime Minister. The languages involved are Hindi (Hi), Marathi (Mr), Tamil (Ta), Telugu (Te), Gujarati (Gu), Malayalam (Ml), Bengali (Bg) and English (En). English is either the source or the target language during evaluation leading to a total of 14 translation dir"
2020.wat-1.1,W12-5611,1,0.853557,"Missing"
2020.wat-1.1,P11-2093,0,0.0137734,"eq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 30 https://github.com/tensorflow/ tensor2tensor https://taku910.github.io/mecab/ 12 (separated by 1000 batches) and performed decod ing with a beam of size 4 and a length penalty of 0.6. 4.2.3 Before the calculation of the automatic evalua tion scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmenta tion, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model33 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.34 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 20140616 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.35 For Korean segmentation, we used mecabko.36 For Myanmar and Khmer segmen tations, we used myseg.py37 and kmseg.py38 . For English and Russian tokenizations, we used tokenizer.perl39 in the Moses toolkit. For Indonesian and Malay tokenizations, we used tokenizer.perl as same as the Engl"
2020.wat-1.1,2020.lrec-1.518,1,0.770784,") which are not publicly available at the time of WAT 2020. Note that phrasetoregion an notation is not included in the test data. There are two settings of submission: with and without resource constraints. In the constrained setting, external resources such as additional data and pretrained models (with external data) are not allowed to use, except for pretrained convo lutional neural networks (for visual analysis) and basic linguistic tools such as taggers, parsers, and morphological analyzers. As the baseline system to compute the Pairwise score, we implement the textonly model in (Nishihara et al., 2020) under the constrained setting. 2.11.1 Documentlevel Scientific Paper Translation Traditional ASPEC translation tasks are sentence level and the translation quality of them seem to be saturated. We think it’s high time to move on to documentlevel evaluation. For the first year, we use ParaNatCom 21 (Parallel EnglishJapanese ab stract corpus made from Nature Communications articles) for the development and test sets of the Documentlevel Scientific Paper Translation sub task. We cannot provide documentlevel training corpus, but you can use ASPEC and any other ex tra resources. 2.11.2 Do"
2020.wat-1.1,P02-1040,0,0.120093,"ase tophrase and phrasetoregion annotations avail able in the training dataset. 5 5.2 Automatic Evaluation System The automatic evaluation system receives transla tion results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each sub mission: Automatic Evaluation 5.1 Procedure for Calculating Automatic Evaluation Score • Human Evaluation: whether or not they sub mit the results for human evaluation; We evaluated translation results by three met rics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.31 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2020 web page.32 All scores for each task were calculated using the corresponding reference translations. 33 http://www.phontron.com/kytea/model.html http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 35 http://nlp.stanford.edu/software/segmenter"
2020.wat-1.1,2020.lrec-1.462,0,0.0440733,"Missing"
2020.wat-1.1,2006.amta-papers.25,0,0.152126,"conducted pairwise evaluation for participants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcome"
2020.wat-1.1,J82-2005,0,0.692397,"Missing"
2020.wat-1.1,L16-1249,1,0.823126,"pur pose of this task was to test the feasibility of multi domain multilingual solutions for extremely low resource language pairs and domains. Naturally the solutions could be onetomany, manytoone or manytomany NMT models. The domains in question are Wikinews and IT (specifically, Soft ware Documentation). The total number of evalu ation directions are 16 (8 for each domain). There is very little clean and publicly available data for these domains and language pairs and thus we en couraged participants to not only utilize the small Asian Language Treebank (ALT) parallel corpora (Thu et al., 2016) but also the parallel corpora from OPUS1 . The ALT dataset contains 18,088, 1,000 and 1,018 training, development and test ing sentences. As for corpora for the IT domain we only provided evaluation (dev and test sets) 2016), consisting of twenty thousand Khmer English parallel sentences from news articles. • The ECCC corpus consists of 100 thousand KhmerEnglish parallel sentences extracted from document pairs of KhmerEnglish bi lingual records in Extraordinary Chambers in the Court of Cambodia, collected by National Institute of Posts, Telecoms & ICT, Cambo dia. The ALT corpus has been"
2020.wat-1.10,P17-4012,0,0.0319783,"s task, the organizers provided filtered data of the PMIndia dataset (Haddow and Kirefu, 2020).8 We have not used any additional resources in this task. The statistics of the dataset are shown in Table 3. We focussed only on the text translation and image captioning task. In the English→Hindi Multimodal task, for the ‘Text-Only’ subtask, we used the Transformer model (Vaswani et al., 2018) which is popular for machine translation and other text-processing tasks, such as low resource text summarization (Parida and Motlicek, 2019). We have used the Transformer model as implemented in OpenNMTpy (Klein et al., 2017).9 We used InceptionResNetV210 for ‘Hindi-only’ image captioning subtask. In the Odia↔English translation task, we used the Transformer model as implemented in OpenNMT-tf.11 In the Indic Multilingual translation task, we used the Transformer (big) model with relative position representations (Shaw et al., 2018) as implemented in OpenNMT-tf (Klein et al., 2017). 3.1 Tokenization and Vocabulary Subword units were constructed using the word pieces algorithm (Johnson et al., 2017). Tokenization is handled automatically as part of the preprocessing pipeline of word pieces. In the English→Hindi Mult"
2020.wat-1.10,D19-5224,0,0.0407519,"Missing"
2020.wat-1.10,2020.wildre-1.3,1,0.838886,"r Mallick KIIT University, India {sdashfca,1939014}@kiit.ac.in Satya Prakash Biswal University Of Chicago, USA satyapb2002@gmail.com Priyanka Pattnaik COE AI LAB, India priyankapattanaik2013@gmail.com Biranchi Narayan Nayak Vettons, Malaysia biranchi125@gmail.com Ondřej Bojar Charles University, MFF, ÚFAL bojar@ufal.mff.cuni.cz Abstract nowadays spoken by 50 million speakers. It is heavily influenced by the Dravidian languages as well as Arabic, Persian, and English. Odia’s inflectional morphology is rich with a three-tier tense system. The prototypical word order is subjectobject-verb (SOV) (Parida et al., 2020a,b). In this system description paper, we explain our approach for the participated tasks. Section 2 describes the datasets used in our experiment. Section 3 presents the model and experimental setups used in our approach. Section 5 provides the official evaluation results of WAT20203 followed by the conclusion in Section 6. This paper describes the team (“ODIANLP”)’s submission to WAT 2020. We have participated in the English→Hindi Multimodal task and Indic task. We have used the state-ofthe-art Transformer model for the translation task and InceptionResNetV2 for the Hindi Image Captioning t"
2020.wat-1.10,D19-5223,1,0.844934,"Missing"
2020.wat-1.10,D19-1616,1,0.83672,"tistics of the datasets are shown in Table 2. Task 3: Indic Multilingual Translation For this task, the organizers provided filtered data of the PMIndia dataset (Haddow and Kirefu, 2020).8 We have not used any additional resources in this task. The statistics of the dataset are shown in Table 3. We focussed only on the text translation and image captioning task. In the English→Hindi Multimodal task, for the ‘Text-Only’ subtask, we used the Transformer model (Vaswani et al., 2018) which is popular for machine translation and other text-processing tasks, such as low resource text summarization (Parida and Motlicek, 2019). We have used the Transformer model as implemented in OpenNMTpy (Klein et al., 2017).9 We used InceptionResNetV210 for ‘Hindi-only’ image captioning subtask. In the Odia↔English translation task, we used the Transformer model as implemented in OpenNMT-tf.11 In the Indic Multilingual translation task, we used the Transformer (big) model with relative position representations (Shaw et al., 2018) as implemented in OpenNMT-tf (Klein et al., 2017). 3.1 Tokenization and Vocabulary Subword units were constructed using the word pieces algorithm (Johnson et al., 2017). Tokenization is handled automati"
2020.wat-1.10,N18-2074,0,0.0308868,"r the ‘Text-Only’ subtask, we used the Transformer model (Vaswani et al., 2018) which is popular for machine translation and other text-processing tasks, such as low resource text summarization (Parida and Motlicek, 2019). We have used the Transformer model as implemented in OpenNMTpy (Klein et al., 2017).9 We used InceptionResNetV210 for ‘Hindi-only’ image captioning subtask. In the Odia↔English translation task, we used the Transformer model as implemented in OpenNMT-tf.11 In the Indic Multilingual translation task, we used the Transformer (big) model with relative position representations (Shaw et al., 2018) as implemented in OpenNMT-tf (Klein et al., 2017). 3.1 Tokenization and Vocabulary Subword units were constructed using the word pieces algorithm (Johnson et al., 2017). Tokenization is handled automatically as part of the preprocessing pipeline of word pieces. In the English→Hindi Multimodal task, we generated the vocabulary of 32k sub-word types jointly for both the source and target languages. In Odia↔English task and Indic Multilingual task, we generated the vocabulary of 24k sub-word types jointly for both the source and target languages. The vocabulary is shared between the encoder and"
2020.wat-1.10,W18-1819,0,0.0319104,"2020b).6 To train the model, we used an additional dataset (OdiEnMonoCorp7 ) suggested by the organizers (Parida et al., 2020a). The statistics of the datasets are shown in Table 2. Task 3: Indic Multilingual Translation For this task, the organizers provided filtered data of the PMIndia dataset (Haddow and Kirefu, 2020).8 We have not used any additional resources in this task. The statistics of the dataset are shown in Table 3. We focussed only on the text translation and image captioning task. In the English→Hindi Multimodal task, for the ‘Text-Only’ subtask, we used the Transformer model (Vaswani et al., 2018) which is popular for machine translation and other text-processing tasks, such as low resource text summarization (Parida and Motlicek, 2019). We have used the Transformer model as implemented in OpenNMTpy (Klein et al., 2017).9 We used InceptionResNetV210 for ‘Hindi-only’ image captioning subtask. In the Odia↔English translation task, we used the Transformer model as implemented in OpenNMT-tf.11 In the Indic Multilingual translation task, we used the Transformer (big) model with relative position representations (Shaw et al., 2018) as implemented in OpenNMT-tf (Klein et al., 2017). 3.1 Token"
2020.wildre-1.3,Q17-1024,0,0.178213,"Missing"
2020.wildre-1.3,P17-4012,0,0.0409187,": Learning curve (EN→OD) 5.1. 4 Figure 6: Learning curve (OD→EN) 1 5. 3 7. Conclusion and Future Work We presented OdiEnCorp 2.0, an updated version of OdiaEnglish parallel corpus aimed for linguistic research and applications in natural language processing, primarily machine translation. The corpus will be used for low resource machine translation shared tasks. The first such task is WAT 202013 Indic shared task on Odia↔English machine translation. Our plans for future include: Neural Machine Translation Setup We used the Transformer model (Vaswani et al., 2018) as implemented in OpenNMT-py (Klein et al., 2017).8 Subword units were constructed using the word pieces algorithm (Johnson et al., 2017). Tokenization is handled automatically as part of the pre-processing pipeline of word pieces. We generated the vocabulary of 32k sub-word types jointly for both the source and target languages, sharing it between the encoder and decoder. To train the model, we used a single GPU and followed the standard “Noam” learning rate decay,9 see (Vaswani et al., 2017) or (Popel and Bojar, 2018) for more details. Our starting learning rate was 0.2 and we used 8000 warm-up steps. The learning curves are shown in Figur"
2020.wildre-1.3,2020.wildre-1.3,1,0.0617925,"tannica.com/topic/ Oriya-language word sense disambiguation, bilingual terminology extraction as well as induction of tools across languages. The Odia language is not available in many machine translation systems. Several researchers explored these goals, developing Odia resources and prototype machine translation systems but these are not available online and benefitting users (Das et al., 2018; Balabantaray and Sahoo, 2013; Rautaray et al., 2019). We have analysed the available English-Odia parallel corpora (OdiEnCorp 1.0, PMIndia) and their performance (BLEU score) for machine translation (Parida et al., 2020; Haddow and Kirefu, 2020). OdiEnCorp 1.0 contains OdiaEnglish parallel and monolingual data. The statistics of OdiEnCorp 1.0 are shown in Table 1. In OdiEnCorp 1.0, the parallel sentences are mostly derived from the English-Odia parallel Bible and the size of the parallel corpus (29K) is not sufficient for neural machine translation (NMT) as documented by the baseline results (Parida et al., 2020) as well as attempts at improving them using NMT techniques such as transfer learning (Kocmi and Bojar, 2019). The recently released PMIndia corpus (Haddow and Kirefu, 2020) contains 38K English-Odia"
2020.wildre-1.3,P18-2037,0,0.0567994,"7611 756967 648025 1692092 1476768 Book Name and Author (Parallel) A Tiger at Twilight by Manoj Dash Yajnaseni by Prativa Ray Wings of Fire by APJ Abdul Kalam with Arun Tiwari Word Book by Shibashis Kar and Shreenath Chaterjee Spoken English by Partha Sarathi Panda and Prakhita Padhi Sarala (Tribhasi) Bhasa Sikhana Petika - Table 2: OdiEnCorp 2.0 parallel corpus details. Training, dev and test sets together. 2.3. Additional Online Resources 3.2. Finding potential parallel texts in a collection of web documents is a challenging task, see e.g. (Antonova and Misyurev, 2011; K´udela et al., 2017; Schwenk, 2018; Artetxe and Schwenk, 2019). We have explored websites and prepared a list of such websites which are potential for us to collect Odia-English parallel data. The websites were then crawled with a simple Python script. We found Odisha’s government portals of each district (e.g. Nayagarh district5 ) of Odisha containing general information about the district in both English and Odia version. Analyzing extracted text, we found a few cases where English was repeated in both sides of the website. We have aligned the extracted text manually to obtain the parallel text. We also extracted parallel da"
2020.wildre-1.3,W18-1819,0,0.0544102,"n Dev and Test sets for OdiEnCorp 2.0. ·104 Figure 5: Learning curve (EN→OD) 5.1. 4 Figure 6: Learning curve (OD→EN) 1 5. 3 7. Conclusion and Future Work We presented OdiEnCorp 2.0, an updated version of OdiaEnglish parallel corpus aimed for linguistic research and applications in natural language processing, primarily machine translation. The corpus will be used for low resource machine translation shared tasks. The first such task is WAT 202013 Indic shared task on Odia↔English machine translation. Our plans for future include: Neural Machine Translation Setup We used the Transformer model (Vaswani et al., 2018) as implemented in OpenNMT-py (Klein et al., 2017).8 Subword units were constructed using the word pieces algorithm (Johnson et al., 2017). Tokenization is handled automatically as part of the pre-processing pipeline of word pieces. We generated the vocabulary of 32k sub-word types jointly for both the source and target languages, sharing it between the encoder and decoder. To train the model, we used a single GPU and followed the standard “Noam” learning rate decay,9 see (Vaswani et al., 2017) or (Popel and Bojar, 2018) for more details. Our starting learning rate was 0.2 and we used 8000 war"
2020.wildre-1.3,W11-1218,0,0.180285,"13936 12068 1688 1652 1492 1471 4297 3653 690634 607611 756967 648025 1692092 1476768 Book Name and Author (Parallel) A Tiger at Twilight by Manoj Dash Yajnaseni by Prativa Ray Wings of Fire by APJ Abdul Kalam with Arun Tiwari Word Book by Shibashis Kar and Shreenath Chaterjee Spoken English by Partha Sarathi Panda and Prakhita Padhi Sarala (Tribhasi) Bhasa Sikhana Petika - Table 2: OdiEnCorp 2.0 parallel corpus details. Training, dev and test sets together. 2.3. Additional Online Resources 3.2. Finding potential parallel texts in a collection of web documents is a challenging task, see e.g. (Antonova and Misyurev, 2011; K´udela et al., 2017; Schwenk, 2018; Artetxe and Schwenk, 2019). We have explored websites and prepared a list of such websites which are potential for us to collect Odia-English parallel data. The websites were then crawled with a simple Python script. We found Odisha’s government portals of each district (e.g. Nayagarh district5 ) of Odisha containing general information about the district in both English and Odia version. Analyzing extracted text, we found a few cases where English was repeated in both sides of the website. We have aligned the extracted text manually to obtain the paralle"
2020.wildre-1.3,Q19-1038,0,0.023522,"025 1692092 1476768 Book Name and Author (Parallel) A Tiger at Twilight by Manoj Dash Yajnaseni by Prativa Ray Wings of Fire by APJ Abdul Kalam with Arun Tiwari Word Book by Shibashis Kar and Shreenath Chaterjee Spoken English by Partha Sarathi Panda and Prakhita Padhi Sarala (Tribhasi) Bhasa Sikhana Petika - Table 2: OdiEnCorp 2.0 parallel corpus details. Training, dev and test sets together. 2.3. Additional Online Resources 3.2. Finding potential parallel texts in a collection of web documents is a challenging task, see e.g. (Antonova and Misyurev, 2011; K´udela et al., 2017; Schwenk, 2018; Artetxe and Schwenk, 2019). We have explored websites and prepared a list of such websites which are potential for us to collect Odia-English parallel data. The websites were then crawled with a simple Python script. We found Odisha’s government portals of each district (e.g. Nayagarh district5 ) of Odisha containing general information about the district in both English and Odia version. Analyzing extracted text, we found a few cases where English was repeated in both sides of the website. We have aligned the extracted text manually to obtain the parallel text. We also extracted parallel data from the Odia digital lib"
2020.wildre-1.3,W16-3719,0,0.149678,"ia-English parallel data. Although these methods need a considerable amount of manual processing, we opted for them, to achieve the largest possible data size. In sum, we used these sources: • Data extracted using OCR, • Data extracted from Odia Wikipedia, • Data extracted from other online resources, • Data reused from existing corpora. The overall process of the OdiEnCorp 2.0 is shown in Figure 1. 2.1. OCR-Based Text Extraction Many books are translated in more than one language and they could serve as a reliable source to obtain parallel sentences, but they are unfortunately not digitized (Bakliwal et al., 2016; Premjith et al., 2016). OCR technology has 15 (a) Sample scanned image of parallel (English-Odia) data. (b) Extracted parallel data. Figure 2: An illustration of the scanned image containing parallel English-Odia data and extracted data. Figure 3: Dilation Figure 4: Erosion by applying the luminosity method which also averages the values, but it takes a weighted average to account for human perception (Joshi, 2019): where R is the maximum standard deviation of all the windows and M is the gray level of the current image. Grayscale = 0.299R + 0.587G + 0.114B However, some black pixels vanish"
2020.wildre-1.3,I17-1101,0,0.265553,"g both automatic and manual processing to build the final Corpus OdiEnCorp 2.0. Source English-Odia Parallel Bible Odisha Government Portal Odisha Govt Home Department Portal Odia Digital Library (Odia Bibhaba) Odia Digital Library (Odia Virtual Academy) Total Sentences (Parallel) 29069 122 82 393 31 29697 Tokens English Odia 756861 640157 1044 930 367 327 7524 6233 453 378 766249 648025 Table 1: Statistics of OdiEnCorp 1.0. 2. Data Sources improved substantially, which has allowed for large-scale digitization of textual resources such as books, old newspapers, ancient hand-written documents (Dhondt et al., 2017). That said, it should be kept in mind that there are often mistakes in the scanned texts as OCR system occasionally misrecognizes letters or falsely identifies text regions, leading to misspellings and linguistics errors in the output text (Afli et al., 2016). Odia language has a rich literary heritage and many books are available in printed form. We have explored books having either English and Odia parallel text together or books having both versions (English and Odia). We have used the study, translation, grammar, literature, and motivational books for this purpose, obtaining the source im"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2020.wmt-1.41,P02-1040,0,0.137289,"al domains: News (6.19), Audit (2.34) and News-Lease (2.74). The Audit domain was generally the least successful for most of the submitted systems (see Table 3) and the Lease domain was more stable in terms of variance. The MT system BLEU variance over annotated lines hints that the better the system, the higher variance it has. This may be because most of the best MT systems are focused on News and fail on other domains, while the lower performant MT systems are low performant systematically across all domains. 3.2 3 3.1 Results Automatic Evaluation We measured the system quality using BLEU (Papineni et al., 2002) against a single reference. The results sorted by the score across all documents are shown in Table 2. BLEU scores across different test sets are, of course, not comparable directly. Only a very big difference, such as that of eTranslation Overall Manual Evaluation From the first phase (Section 2.1) we collected 13×328 = 4264 line annotations. From the second phase (Section 2.2) we collected 13×499 = 6487 markable annotations. The average duration for one annotation of one translated line in the first phase was 25s, while one annotation of one systemmarkable occurrence in the second phase too"
2020.wmt-1.41,W19-5355,1,0.888247,"Missing"
2020.wmt-1.77,2020.wmt-1.98,0,0.0309332,"Missing"
2020.wmt-1.77,W17-4755,1,0.838846,"s the average rating of the segments translated by the system. • Regular News Tasks Systems. These are all the other MT systems in the evaluation; differing in whether they are trained only on WMT provided data (“Constrained”, or “Unconstrained”) as in the previous years. With all language pairs, in addition to the submissions to the task, the test sets also include translations from freely available web services (online MT systems), which are deemed unconstrained. Overall, the results are based on 208 systems across 18 language pairs. 2.3 2.3.2 Segment-level Golden Truth: DA RR Starting from Bojar et al. (2017), when WMT fully switched to DA, we had to come up with a solid golden standard for segment-level judgements. Standard DA scores are reliable only when averaged over sufficient number of judgments.3 Fortunately, when we have at least two DA scores for translations of the same source input, it is possible to convert those DA scores into a relative ranking judgement, if the difference in DA scores allows conclusion that one translation is better than the other. In the following, we denote these re-interpreted DA judgements as “DA RR”, to distinguish it clearly from the relative ranking (“RR”) go"
2020.wmt-1.77,W16-2302,1,0.803613,"Missing"
2020.wmt-1.77,P17-1152,0,0.0122551,"re lexical, morphological and semantic equivalence. Semantic evaluation is achieved by using pretrained fasttext embeddings provided by Facebook to calculate the word similarity score between the candidate and the reference words. MEE computes evaluation score using three modules namely exact match, root match and synonym match. In each module, fmean-score is calculated using harmonic mean of precision and recall by assigning more weight to recall. The final translation score is obtained by taking average of fmean-scores from individual modules. 3.2.7 ESIM Enhanced Sequential Inference Model (Chen et al., 2017) is a neural model proposed for Natural Language Inference that has been adapted for MT evaluation by Mathur et al. (2019). It uses crosssentence attention and sentence matching heuristics to generate a representation of the translation and the reference, which is fed to a feedforward regressor. This year’s scores were submitted by Bawden et al. (2020) as part of the submission on PAR ESIM. 3.3 PAR BLEU, PARCHR F++, PAR ESIM PAR BLEU, PARCHR F++, and PAR ESIM (Bawden et al., 2020) are variants of their respective core metrics computed against the provided human reference and a set of automatic"
2020.wmt-1.77,N19-1423,0,0.0772793,"was not available for this evaluation. ric 6 with the default parameters --ncorder 6 to transform the MT output to the reference. TER+lang.LANGPAIR- --nwworder 2 --beta 2 +tok.tercom-nonorm-punct3.2 Submissions noasian-uncased+version.1.4.14 The rest of this section summarizes participating • CHR F (Popovi´c, 2015) uses character metrics. n-grams instead of word n-grams to compare the MT output with the reference 5 . 3.2.1 BERT- BASE -L2, BERT- LARGE -L2, M BERT-L2 Version string: chrF2+lang.LANGPAIR+numchars.6+space.falseThe three baselines were obtained by fine-tuning +version.1.4.14. BERT (Devlin et al., 2019) on the ratings of WMT Metrics years 2015 to 2018, using a regression 3.1.2 CHR F++ loss. What distinguishes the metrics is the initial BERT checkpoint: BERT- BASE -L2 uses a CHR F++ (Popovi´c, 2017) includes word unigrams and bigrams in addition to character ngrams. We 12-layer Transformer architecture pre-trained on ran the original Python implementation of the met- English data, M BERT-L2 is similar but trained 5 6 chrF++.py available at https://github.com/ m-popovic/chrF Note that the SacreBLEU scorer does not yet implement with multiple references CHR F 692 Baselines Reference-based metri"
2020.wmt-1.77,N12-1017,0,0.0256405,"ventional Kendall’s Tau coefficient. Since we do not have a total order ranking of all translations, it is not possible to apply conventional Kendall’s Tau given the current DA RR human evaluation setup (Graham et al., 2015). Our Kendall’s Tau-like formulation, τ , is as follows: τ= Influence of References Rewarding multiple alternative translations is the primary motivation behind multiple-reference based evaluation. It is generally assumed that using multiple reference translation for automatic evaluation is helpful as we cover a wider space of possible translations (Papineni et al., 2002b; Dreyer and Marcu, 2012; Bojar et al., 2013). Nevertheless, new studies (Freitag et al., 2020) showed that multi-reference evaluation does not improve the correlation for high quality output anymore. Since we have multiple references available for five language pairs, we can look at how much the choice of reference(s) influences correlation. Table 8 compares metric correlations on the primary reference set newstest2020, alternative reference newstestB2020, paraphrased reference newstestP2020 (only for English-German), or using all available references newstestM2020. We only report system-level correlations of metric"
2020.wmt-1.77,2020.emnlp-main.5,1,0.899126,"itions, the source, reference texts, and MT system outputs for the metric Multiple References This year, we have two independently generated references for English ↔ German, English ↔ Russian, and Chinese → English. This lets us investigate the influence of references and the utility of multiple references. We instructed participants to score MT systems against the references individually as well as with all available references. In addition, we also supplied a set of references for English to German, that were generated by asking linguists to paraphrase the WMT reference as much as possible (Freitag et al., 2020). These references are designed to minimise translationese in the reference which could lead to metrics to be biased against systems that generate more natural text. 1 http://www.statmt.org/wmt20/ metrics-task.html 2 Note that the metrics task inputs also included MT systems translating between German ↔ French in the News Translation Task, and English → Khmer and Pashto from the WMT parallel corpus filtering task. We are unable to evaluate metrics on these language pairs as human evaluation is not available 688 Proceedings of the 5th Conference on Machine Translation (WMT), pages 688–725 c Onl"
2020.wmt-1.77,D14-1020,0,0.0372044,"h human scores, whether on the system, document or segment level: higher scores have to indicate better translation quality. 4 4.1 Results System-Level Evaluation As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics. The Pearson correlation is as follows: Pn − H)(Mi − M ) qP n 2 2 i=1 (Hi − H) i=1 (Mi − M ) r = qP n i=1 (Hi (1) where Hi are human assessment scores of all systems in a given translation direction, Mi are the corresponding scores as predicted by a given metric. H and M are their means, respectively. As recommended by Graham and Baldwin (2014), we employ Williams significance test (Williams, 1959) to identify differences in correlation that are statistically significant. Williams test is a test of significance of a difference in dependent correlations and therefore suitable for evaluation of metrics. Correlations not significantly outperformed by any other metric for the given language pair are highlighted in bold in all the results tables that show Pearson correlation of metric and human scores. Pearson correlation is ideal for reporting whether metric scores have the same trend as human scores. In practice, we use metrics to make"
2020.wmt-1.77,N15-1124,1,0.838446,"andle human translations much better on average. Also, the Paraphrased references help the lexical metrics correctly identify the high quality of human translations. We present a deeper analysis of how metrics score human translations in Section 5.1.2. We base this discussion on scatterplots of human vs metric scores. We include scatterplots of selected metrics in Appendix B. 4.2 Segment- and Document-Level Evaluation Segment-level evaluation relies on the manual judgements collected in the News Translation Task evaluation. This year, again we were unable to follow the methodology outlined in Graham et al. (2015) for evaluating of segment-level metrics because the sampling of segments did not provide sufficient number of assessments of the same segment. We therefore convert pairs of DA scores for competing translations to DA RR better/worse preferences as described in Section 2.3.2. We further follow the same process to generate DA RR ground truth for documents, as we do not have enough annotations to obtain accurate human scores. We measure the quality of metrics’ scores against the DA RR golden truth using a Kendall’s Tau-like formulation, which is an adaptation of the conventional Kendall’s Tau coe"
2020.wmt-1.77,W13-2305,0,0.0954211,"d to train a neural machine translation system (fairseq). This was tested on a held-out subset of Wikipedia translations. 2.3.1 System-level Golden Truth: DA For the system-level evaluation, the collected continuous DA scores, standardized for each annotator, are averaged across all assessed segments for each MT system to produce a scalar rating for the system’s performance. The underlying set of assessed segments is different for each system. Thanks to the fact that the system-level DA score is an average over many judgments, mean scores are consistent and have been found to be reproducible (Graham et al., 2013). For more details see Findings 2020. The score of an MT system is calculated as the average rating of the segments translated by the system. • Regular News Tasks Systems. These are all the other MT systems in the evaluation; differing in whether they are trained only on WMT provided data (“Constrained”, or “Unconstrained”) as in the previous years. With all language pairs, in addition to the submissions to the task, the test sets also include translations from freely available web services (online MT systems), which are deemed unconstrained. Overall, the results are based on 208 systems acros"
2020.wmt-1.77,E17-2057,1,0.838885,"ed segments, but we have no evidence that they actually do so. Second, document-level phenomena are rather scarce and averaging segment-level scores is likely to average out these sparse observations even if they were marked at individual sentences. And lastly, in some situations, lack of cross-sentence coherence can be so critical that any strategy of composing sentencelevel scores is bound to downplay the severity of the error, see e.g. Vojtˇechov´a et al. (2019). At the current point, we have nothing better to start with but we believe that better techniques will be proposed in the future. Graham et al. (2017) recommend around averaging 100 annotations per document to obtain reliable document scores. Since the average number of assessments we have is much less than that, we compute the ground truth in the same way as the segment level evaluation. We first compute document scores as the average of all segment scores in the document, which we denote as DOC -DA. We then generate DOC - DA RR pairs of better and worse translations of the same source document when there is at least a 25 point 3 Metrics 3.1 Baselines We agree with the call to use SacreBLEU (Post, 2018) as the standard MT evaluation scorer"
2020.wmt-1.77,W14-3333,1,0.896921,"Missing"
2020.wmt-1.77,P19-3020,0,0.054661,"Missing"
2020.wmt-1.77,W04-3250,0,0.61903,"Missing"
2020.wmt-1.77,2020.wmt-1.78,0,0.297077,"Missing"
2020.wmt-1.77,W19-5358,0,0.0449728,"Missing"
2020.wmt-1.77,W14-3336,1,0.423503,"Missing"
2020.wmt-1.77,P19-1269,1,0.863654,"ngs provided by Facebook to calculate the word similarity score between the candidate and the reference words. MEE computes evaluation score using three modules namely exact match, root match and synonym match. In each module, fmean-score is calculated using harmonic mean of precision and recall by assigning more weight to recall. The final translation score is obtained by taking average of fmean-scores from individual modules. 3.2.7 ESIM Enhanced Sequential Inference Model (Chen et al., 2017) is a neural model proposed for Natural Language Inference that has been adapted for MT evaluation by Mathur et al. (2019). It uses crosssentence attention and sentence matching heuristics to generate a representation of the translation and the reference, which is fed to a feedforward regressor. This year’s scores were submitted by Bawden et al. (2020) as part of the submission on PAR ESIM. 3.3 PAR BLEU, PARCHR F++, PAR ESIM PAR BLEU, PARCHR F++, and PAR ESIM (Bawden et al., 2020) are variants of their respective core metrics computed against the provided human reference and a set of automatically generated paraphrases. PAR BLEU used five paraphrases, while the other two used only one. Both BLEU and CHR F++ have"
2020.wmt-1.77,2020.acl-main.448,1,0.653947,"he official results, but also report Kendall’s Tau correlation in the appendix. The calculation of Pearson correlation coefficient is dependent on the mean, which is very sensitive to outliers. So if we have systems whose scores are far away from the rest of the systems, the presence of these “outlier” systems can give a misleadingly high impression of the correlations, and potentially change ranking of metrics. To avoid this, we also report correlations over non-outlier systems only. To remove outliers, we are guided by the robust outlier detection method proposed for MT metric evaluation by Mathur et al. (2020). This method, recommended by the statistics literature (Iglewicz and Hoaglin, 1993; Rousseeuw and Hubert, 2011; Leys et al., 2013) depends on the median and the median absolute deviation (MAD) which is the median of the absolute difference between each point and the median. The method removes systems whose human scores are greater than 2.5 MAD away from the median. The cutoff of 2.5 is subjective, and Leys et al. (2013) suggest the guidelines of using 3 (very conservative), 2.5 (moderately conservative) or 2 (poorly conservative), and recommends 2.5. For some language pairs, we override the 2"
2020.wmt-1.77,P02-1040,0,0.133493,"uation. We first compute document scores as the average of all segment scores in the document, which we denote as DOC -DA. We then generate DOC - DA RR pairs of better and worse translations of the same source document when there is at least a 25 point 3 Metrics 3.1 Baselines We agree with the call to use SacreBLEU (Post, 2018) as the standard MT evaluation scorer. We no longer report scores of the metrics from the Moses scorer, which requires tokenized text. We use the following metrics from the SacreBLEU scorer as baselines, with the default parameters: 691 3.1.1 SacreBLEU baselines • BLEU (Papineni et al., 2002a) is the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations. BLEU+case.mixed+lang.LANGPAIR+numrefs.1+smooth.exp+tok.13a+version.1.4.14 We run SacreBLEU with the --sentence-score option to obtain sentence scores for SENT BLEU; this uses the same parameters as BLEU. Although not it’s intended use, we also compute system- and document-level scores for SENT BLEU as the mean segment score. • TER (Snover et al., 2006) measures the number of edits (insertions, deletions, shifts and substitutions) required DA>1 Ave DA pai"
2020.wmt-1.77,W15-3049,0,0.128613,"Missing"
2020.wmt-1.77,2020.wmt-1.99,0,0.0341923,"Missing"
2020.wmt-1.77,2020.wmt-1.100,0,0.012614,"d MT evaluation metric that measures the semantic similarity between a ma-chine translation and human references by aggregating the idf-weighted lexical semantic similarities based on the contextual embeddings extracted from pretrained language models (BERT, CamemBERT, RoBERTa, XLM, XLM-RoBERTa, etc.) and optionally incorporating shallow semantic structures (denoted as Y I S I -1 SRL; not participating this year). Y I S I -0 is the degenerate version of Y I S I -1 that is ready-to-deploy to any language. It uses longest common character substring to measure the lexical similarity. Y I S I -2 (Lo and Larkin, 2020) is the bilingual, reference-less version for MT quality estimation, which uses bilingual mappings of the contextual embeddings extracted from pretrained language models (XLM or XLM-RoBERTa) to evaluate the crosslingual lexical semantic similarity between the input and 695 MT output. Like Y I S I -1, Y I S I -2 can exploit shallow semantic structures as well (denoted as Y I S I 2 SRL; does not participate this year). 3.4 Pre-processing Since some metrics, such as BLEU, aim to achieve a strong positive correlation with human assessment, while error metrics, such as TER, aim for a strong negativ"
2020.wmt-1.77,W18-6450,1,0.939069,"l sample size. Furthermore, Williams test takes into account the correlation between each pair of metrics, in addition to the correlation between the metric scores themselves, and this latter correlation increases the likelihood of a significant difference being identified. In extreme cases, the test would have low power when comparing a metric that doesn’t correlate well with other metrics, resulting in this metric not being outperformed by other metrics despite having a much lower value of correlation. To strengthen the conclusions of our evaluation, in past years (Bojar et al., 2016, 2017; Ma et al., 2018), we included significance test results for large hybrid-super-samples of systems 10K hybrid systems were created per language pair, with corresponding DA human assessment scores by sampling pairs of systems from the News Translation Task, creating hybrid systems by randomly selecting each candidate translation from one of the two selected systems. However, as WMT human annotations are collected with document context in 2020, this style of hybridization is susceptible to breaking cross-segment references in MT outputs and it would be unreasonable to shuffle individual segments. The creation of"
2020.wmt-1.77,W19-5302,1,0.799443,"discarding outliers. In particular, CHR F and PAR ESIM both had a correlation of 0.95 when computed over all systems, but this drops to 0.69 and 0.83 respectively after removing outliers, revealing that PAR ESIM is more reliable with this language pair. An even larger drop is observed for CHR F and CHR F++ in English → Czech, from 0.8 to 0.3. We find this particularly surprising because CHR F has always performed well on this language pair, including in the evaluation on the gradually reducing set of top N systems, i.e. in harder and harder conditions, see SACRE BLEU- CHR F in Appendix A.4 of Ma et al. (2019). In some cases, metrics can be inaccurate when scoring outliers, resulting in an increased correlation when correlation is recomputed over nonoutlier systems. For example, with Chinese → English, the score of WMTB IOMED BASE LINE score is much lower than the next system. Most metrics correctly rank it last as well, but COMET-HTER, COMET-MQM, COMET-QE and O PEN K IWI -BERT give it a higher score than the next system(s). Note that the other metrics all have a correlation of above 0.9 even after removing the outlier. In other cases, removing outliers decreases the correlation of a metric and yet"
2020.wmt-1.77,W17-4770,0,0.0320406,"Missing"
2020.wmt-1.77,W18-6319,0,0.031609,"be proposed in the future. Graham et al. (2017) recommend around averaging 100 annotations per document to obtain reliable document scores. Since the average number of assessments we have is much less than that, we compute the ground truth in the same way as the segment level evaluation. We first compute document scores as the average of all segment scores in the document, which we denote as DOC -DA. We then generate DOC - DA RR pairs of better and worse translations of the same source document when there is at least a 25 point 3 Metrics 3.1 Baselines We agree with the call to use SacreBLEU (Post, 2018) as the standard MT evaluation scorer. We no longer report scores of the metrics from the Moses scorer, which requires tokenized text. We use the following metrics from the SacreBLEU scorer as baselines, with the default parameters: 691 3.1.1 SacreBLEU baselines • BLEU (Papineni et al., 2002a) is the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations. BLEU+case.mixed+lang.LANGPAIR+numrefs.1+smooth.exp+tok.13a+version.1.4.14 We run SacreBLEU with the --sentence-score option to obtain sentence scores for SENT BLEU; t"
2020.wmt-1.77,2020.emnlp-main.213,0,0.692783,"ngs predictor-estimator model predictor-estimator model predictor-estimator model predictor-estimator model predictor-estimator model ? predictor-estimator model predictor-estimator model predictor-estimator model contextual word embeddings C HARAC TER EED SWSS+METEOR MEE YISI BERT- BASE -L2 BERT- LARGE -L2, M BERT-L2 BLEURT BLEURT- EXTENDED Y ISI - COMBII BLEURT- COMBI COMET COMET-R ANK COMET-HTER COMET-2R COMET-MQM BAQ, EQ COMET-QE O PEN K IWI -B ERT O PEN K IWI -XLMR Y I S I -2 PAR BLEU PRISM PAR ESIM PARCHR F++ yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes ? yes • • Unbabel (Rei et al., 2020b) Unbabel Kepler et al. (2019) Unbabel Kepler et al. (2019) NRC Lo and Larkin (2020) Univ of Edinburgh, Univ of Tartu, JHU Bawden et al. (2020) Univ of Edinburgh, Univ of Tartu, JHU Bawden et al. (2020) Univ of Edinburgh, Univ of Tartu, JHU Bawden et al. (2020) Johns Hopkins University RWTH Aachen Wang et al. (2016) RWTH Aachen Stanchev et al. (2019) , Xu et al. (2020) IIIT - Hyderabad, Ananya Mukherjee and Sharma (2020) NRC Lo (2019, 2020) Google (Devlin et al., 2019) Google (Devlin et al., 2019) Google (Devlin et al., 2019) Google (Devlin et al., 2019) Google (Devlin et al., 2019) Google (D"
2020.wmt-1.77,2020.wmt-1.101,0,0.484192,"ngs predictor-estimator model predictor-estimator model predictor-estimator model predictor-estimator model predictor-estimator model ? predictor-estimator model predictor-estimator model predictor-estimator model contextual word embeddings C HARAC TER EED SWSS+METEOR MEE YISI BERT- BASE -L2 BERT- LARGE -L2, M BERT-L2 BLEURT BLEURT- EXTENDED Y ISI - COMBII BLEURT- COMBI COMET COMET-R ANK COMET-HTER COMET-2R COMET-MQM BAQ, EQ COMET-QE O PEN K IWI -B ERT O PEN K IWI -XLMR Y I S I -2 PAR BLEU PRISM PAR ESIM PARCHR F++ yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes ? yes • • Unbabel (Rei et al., 2020b) Unbabel Kepler et al. (2019) Unbabel Kepler et al. (2019) NRC Lo and Larkin (2020) Univ of Edinburgh, Univ of Tartu, JHU Bawden et al. (2020) Univ of Edinburgh, Univ of Tartu, JHU Bawden et al. (2020) Univ of Edinburgh, Univ of Tartu, JHU Bawden et al. (2020) Johns Hopkins University RWTH Aachen Wang et al. (2016) RWTH Aachen Stanchev et al. (2019) , Xu et al. (2020) IIIT - Hyderabad, Ananya Mukherjee and Sharma (2020) NRC Lo (2019, 2020) Google (Devlin et al., 2019) Google (Devlin et al., 2019) Google (Devlin et al., 2019) Google (Devlin et al., 2019) Google (Devlin et al., 2019) Google (D"
2020.wmt-1.77,2020.acl-main.704,0,0.0579417,"scores. “−” indicates that the metric didn’t participate the track (Seg/Doc/Sys-level). “?” indicates that we computed the metric’s document or system score for this track as the macro-average of segment scores, though the metric is not defined this way. A metric is learned if it is trained on a QE or metric evaluation dataset (i.e. pretraining or parsers don’t count, but training on WMT 2019 metrics task data does). src-based 693 on Wikipedia data in 102 languages, and BERTL ARGE -L2 is English-only with 24 layers. 3.2.2 BLEURT, BLEURT- EXTENDED , Y ISI - COMBI , BLEURT- YISI - COMBI BLEURT (Sellam et al., 2020a) is a BERT-based regression model trained twice: first on million synthetic pairs obtained by random perturbations, then on ratings from years 2015 to 2019 of the WMT Workshop. BLEURT- EXTENDED (Sellam et al., 2020b) is a BERT-based regression model trained on human ratings of years 2015 to 2019 of the WMT Workshop, combined with BERT-Chinese for to-Chinese sentence pairs. The main checkpoint is a 24-layer Transformer, trained on a mixture of Wikipedia articles and training data from WMT Newstest in 20 languages. Y ISI - COMBI: We are using Y I S I -1 on an mBERT model that is fine tuned on"
2020.wmt-1.77,2020.wmt-1.102,1,0.84064,"scores. “−” indicates that the metric didn’t participate the track (Seg/Doc/Sys-level). “?” indicates that we computed the metric’s document or system score for this track as the macro-average of segment scores, though the metric is not defined this way. A metric is learned if it is trained on a QE or metric evaluation dataset (i.e. pretraining or parsers don’t count, but training on WMT 2019 metrics task data does). src-based 693 on Wikipedia data in 102 languages, and BERTL ARGE -L2 is English-only with 24 layers. 3.2.2 BLEURT, BLEURT- EXTENDED , Y ISI - COMBI , BLEURT- YISI - COMBI BLEURT (Sellam et al., 2020a) is a BERT-based regression model trained twice: first on million synthetic pairs obtained by random perturbations, then on ratings from years 2015 to 2019 of the WMT Workshop. BLEURT- EXTENDED (Sellam et al., 2020b) is a BERT-based regression model trained on human ratings of years 2015 to 2019 of the WMT Workshop, combined with BERT-Chinese for to-Chinese sentence pairs. The main checkpoint is a 24-layer Transformer, trained on a mixture of Wikipedia articles and training data from WMT Newstest in 20 languages. Y ISI - COMBI: We are using Y I S I -1 on an mBERT model that is fine tuned on"
2020.wmt-1.77,2006.amta-papers.25,0,0.333217,"Missing"
2020.wmt-1.77,W19-5359,0,0.0296173,"ted Translation Edit Rate (HTER) and a proprietary metric compliant with the Multidimensional Quality Metrics framework (MQM), respectively. COMET-Rank uses the Translation Ranking architecture to directly optimize the distance between “better“ hypothesis and the respective source and reference, while pushing the “worse“ hypothesis away. This Translation Ranking model was directly optimised on DA relative-ranks from 2017 to 2019. Finally, COMET-QE removes the reference at input and proportionately reduces the dimensions of the estimator network to accommodate the reduced input. 3.2.5 EED EED (Stanchev et al., 2019) is a character-based metric, which builds upon CDER. It is defined as the minimum number of operations of an extension to the conventional edit distance containing a “jump” operation. The edit distance operations (insertions, deletions and substitutions) are performed at the character level and jumps are performed when a blank space is reached. Furthermore, the coverage of multiple characters in the hypothesis is penalised by the introduction of a coverage penalty. The sum of the length of the reference and the coverage penalty is used as the normalisation term. 694 3.2.6 MEE MEE (Ananya Mukh"
2020.wmt-1.77,2020.emnlp-main.8,0,0.122531,"ed with WMT Metrics data from 2017 to 2019. 3.3.1 casts machine translation evaluation as a zero-shot paraphrasing task, producing segment-level scores by force-decoding between a system output and a reference, in both directions, and averaging the model scores. System-level scores are produced by averaging segment-level ones. For evaluation in Inuktikut, Khmer, Pashto, and Tamil, we used a “Prism44” model that was retrained after adding WMT-provided data for these languages to its original training data set. All other languages were evaluated with the original “Prism39” model. P RISM P RISM (Thompson and Post, 2020) is a manymany multilingual neural machine translation system trained on data for 39 language pairs, with data derived largely from WMT and Wikimatrix. It 3.3.4 Y I S I -0, Y I S I -1, Y I S I -2 Y I S I (Lo, 2019, 2020) is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available re-sources. Y I S I -1 is a reference-based MT evaluation metric that measures the semantic similarity between a ma-chine translation and human references by aggregating the idf-weighted lexical semantic similarities based on the contextual embeddings extracted fr"
2020.wmt-1.77,W18-6312,0,0.183424,"including BLEU and chrF give really high scores to O NLINE -B, which results in low correlations. 6 Use Automatic Metrics to Detect Incorrect Human Preference Evaluation It has been argued that non-expert translators lack knowledge of translation and so might not notice subtle differences that make one translation better than another. Castilho et al. (2017) compared the evaluation of MT output of professional translators against crowd workers. Results showed that for all language pairs, the crowd workers tend to be more accepting of the MT output by giving higher fluency and adequacy scores. Toral et al. (2018) showed that the ratings acquired by professional translators show a wider gap between human and machine translations compared to judgments by non-experts. They recommend using professional linguists for MT evaluation going forward. L¨aubli et al. (2020) show that non-experts assess parity between human and machine translation where professional translators do not, indicating that the former neglect more subtle differences between different translation outputs. Given the previous work and the fact that the WMT human evaluation has been conducted with a mix of researchers and crowd workers, we"
2020.wmt-1.77,W19-5355,1,0.89245,"Missing"
2020.wmt-1.77,W16-2342,0,0.0706757,"Missing"
2021.acl-long.311,D17-1098,0,0.0295328,"ches post-process the output. Prior to subword handling (Sennrich et al., 2016; Kudo and Richardson, 2018), unknown words were corrected by replacing them with word translation pairs from a bilingual dictionary (Luong et al., 2015). Crego et al. (2016) use placeholders to translate numbers and named entities. Placeholders have also been found useful for translation of text with formal mark-up and its interaction with content (Hanneman and Dinu, 2020). 2.2 Constrained decoding An alternative way of adding constraints to the final translation is by manipulating the beam search decoding process. Anderson et al. (2017) use a finite state machine (FSM) that recognizes target sentence with constraint patterns. Each state of the FSM has its own beam and only hypotheses in beams that are in accepting states can be finished. Hasler et al. (2018) improve upon this work by utilizing encoder-decoder attention weights to guide the placement of a constraint. Chatterjee et al. (2017) also use attention weights and beam search look-ahead to choose constraint positions. Hokamp and Liu (2017) present Grid Beam Search, which extends the usual beam search (Och and Ney, 2004) with a mechanism to ensure the coverage of all c"
2021.acl-long.311,W19-5301,1,0.894694,"Missing"
2021.acl-long.311,2021.eacl-main.271,0,0.0413049,"stics for inflections in inflected languages. Dinu et al. (2019) use input factors to annotate source sentences with desired translations and train the model to copy these translations into the output sequence. Chen et al. (2020) append constraints to the end of the source sentence. Their goal is to train the model to place constraints in the output translation without the need of a bilingual dictionary or a specified word alignment. Song et al. (2019) also propose a data augmentation approach that uses constraints along the source as input during the model training. Concurrently to our work, Bergmanis and Pinnis (2021) modify Dinu et al. (2019) approach by providing lemmatized word factors associated to random tokens in the source sentence. With the lemmatized factors, they force the model to learn the correct inflection of the word in the translation. The main difference between our work and most of the existing approaches is the use of lemmatized constraints to allow the model to correctly inflect them to agree with the output context. The 4020 concurrent work by Bergmanis and Pinnis (2021) presents a very similar idea. They also use lemmatized forms of the constraints and let the model itself to generate"
2021.acl-long.311,W17-4716,0,0.0189429,"nslation of text with formal mark-up and its interaction with content (Hanneman and Dinu, 2020). 2.2 Constrained decoding An alternative way of adding constraints to the final translation is by manipulating the beam search decoding process. Anderson et al. (2017) use a finite state machine (FSM) that recognizes target sentence with constraint patterns. Each state of the FSM has its own beam and only hypotheses in beams that are in accepting states can be finished. Hasler et al. (2018) improve upon this work by utilizing encoder-decoder attention weights to guide the placement of a constraint. Chatterjee et al. (2017) also use attention weights and beam search look-ahead to choose constraint positions. Hokamp and Liu (2017) present Grid Beam Search, which extends the usual beam search (Och and Ney, 2004) with a mechanism to ensure the coverage of all constrains. Post and Vilar (2018) propose a similar but more efficient algorithm. By dynamically reallocating the beam capacity, an arbitrary number of constraints can be processed within a constant width of the beam. One shortcoming of the above methods is the slower inference compared to unmodified beam search models. This issue is in large part solved by ef"
2021.acl-long.311,P16-1162,0,0.0419562,"where words that should or should not appear in the output are known upfront. Common use cases include integration of domain-specific terminology and translation of named entities or rare words using a dictionary. Such functionality was previously implemented in phrase-based systems (Okuma et al., 2008), like Moses (Koehn et al., 2007). In NMT, this task is not yet definitely solved, since the translation process is hard to interpret and influence. 2.1 Output post-processing In order to enforce the presence of specific terms, some approaches post-process the output. Prior to subword handling (Sennrich et al., 2016; Kudo and Richardson, 2018), unknown words were corrected by replacing them with word translation pairs from a bilingual dictionary (Luong et al., 2015). Crego et al. (2016) use placeholders to translate numbers and named entities. Placeholders have also been found useful for translation of text with formal mark-up and its interaction with content (Hanneman and Dinu, 2020). 2.2 Constrained decoding An alternative way of adding constraints to the final translation is by manipulating the beam search decoding process. Anderson et al. (2017) use a finite state machine (FSM) that recognizes target"
2021.acl-long.311,N19-1044,0,0.0913053,"lation. 1 Input (EN) Likud party has merged with an even more hawkish lot under Avigdor Lieberman. No constraint translation (CS) Strana Likud se spojila s ještě jestřábím losem pod Avigdorem Liebermanem. Surface form model output radikální Strana Likud se spojila s ještě radikální partou pod vedením Avigdora Liebermana radikální Strana Likud se spojila s ještě radikálnější partií pod vedením Avigdora Liebermana. (CS) Lemmatized model output (CS) Figure 1: Comparison between constrained translations from English to Czech. Introduction In Neural Machine Translation (NMT), lexical constraining (Song et al., 2019; Hokamp and Liu, 2017; Post and Vilar, 2018) involves changing the translation process in a way that desired terms appear in the model’s output. Translation constraints are useful in domain adaptation, interactive machine translation or named entities translation. Current approaches focus either on manipulating beam search decoding (Hokamp and Liu, 2017; Post and Vilar, 2018; Hu et al., 2019) or training an NMT model using constraints alongside the input (Dinu et al., 2019; Song et al., 2019; Chen et al., 2020). In inflected languages, constraints from both source and target sides may appear"
2021.acl-long.311,K17-3009,0,0.0131113,"al, open-domain rare words using dictionary. 4.1 Data We train English-Czech NMT models for our experiments. Czech has a high degree of inflection with seven cases and three genders for nouns and adjectives. We train our models on CzEng 2.0 (Kocmi et al., 2020) using all authentic parallel sentences (61M), as well as back-translated Czech monolingual sentences (51M). Newstest-2019 (Barrault et al., 2019) is used as a validation set and newstest2020 (Barrault et al., 2020) as a test set. We break the text into subwords using SentencePiece (Kudo and Richardson, 2018) and lemmatize using UDPipe (Straka and Straková, 2017). BLEU scores are computed using SacreBLEU (Post, 2018).2 For experiments mentioning dictionaries, we extracted pairs of terms from English and Czech Wiktionary3 and a large commercial dictionary. In appendix B.2 we show that using Wiktionary also improves performance upon baseline, but the commercial dictionary offers better coverage of the expressions and thus provides better overall results. For this reason, all the experimets shown further are based on the commercial dictionary data. We use the Czech government database for EU terminology4 to evaluate integration of domainspecific terminol"
2021.acl-long.311,2020.wmt-1.41,1,0.815147,"Missing"
2021.acl-long.311,W18-6319,0,0.014121,"-Czech NMT models for our experiments. Czech has a high degree of inflection with seven cases and three genders for nouns and adjectives. We train our models on CzEng 2.0 (Kocmi et al., 2020) using all authentic parallel sentences (61M), as well as back-translated Czech monolingual sentences (51M). Newstest-2019 (Barrault et al., 2019) is used as a validation set and newstest2020 (Barrault et al., 2020) as a test set. We break the text into subwords using SentencePiece (Kudo and Richardson, 2018) and lemmatize using UDPipe (Straka and Straková, 2017). BLEU scores are computed using SacreBLEU (Post, 2018).2 For experiments mentioning dictionaries, we extracted pairs of terms from English and Czech Wiktionary3 and a large commercial dictionary. In appendix B.2 we show that using Wiktionary also improves performance upon baseline, but the commercial dictionary offers better coverage of the expressions and thus provides better overall results. For this reason, all the experimets shown further are based on the commercial dictionary data. We use the Czech government database for EU terminology4 to evaluate integration of domainspecific terminology through constraints. We select all Czech terms and"
2021.eacl-demos.32,P19-1126,0,0.361622,"Missing"
2021.eacl-demos.32,D14-1140,0,0.362092,"Missing"
2021.eacl-demos.32,2020.iwslt-1.27,0,0.152889,"among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quali"
2021.eacl-demos.32,E17-1099,0,0.269847,"rts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) propose"
2021.eacl-demos.32,N12-1048,0,0.134304,"m the secured networks of the labs so it usually does not run into firewall issues. tions of the EU and nearby countries. Experimentally, we include also other languages based on available systems among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speec"
2021.eacl-demos.32,2020.eamt-1.53,1,0.493793,"Missing"
2021.eacl-demos.32,C18-2020,1,0.876378,"Missing"
2021.eacl-demos.32,2020.iwslt-1.25,1,0.823962,"Missing"
2021.eacl-demos.32,N16-3017,1,0.800534,"Missing"
2021.eacl-demos.32,2020.acl-main.148,1,0.8252,"tem in end-to-end fashion and face engineering problems and technical issues on all layers from sound acquisition through network connections, worker configuration to subtitle presentation. • We are currently running a user study with nonGerman speakers watching German videos with our online subtitles, see Section 7.1. We aim to measure the comprehension loss caused by different subtitling options, latency or flicker. 42 languages (Johnson et al., 2017). The models are mostly Transformers (Vaswani et al., 2017) but we improve their performance in massively multilingual setting by extra depth (Zhang et al., 2020). 5.3 Interplay of ASR and MT Connecting ASR and MT systems is not straightforward because MT systems assume input in the form of complete sentences. We follow the strategy of Niehues et al. (2016), first inserting punctuation into the stream of tokens coming from ASR (Tilk and Alum¨ae, 2016), breaking it up at full stops and sending individual sentences to MT, either as unfinished sentence prefixes, or complete sentences. We are using re-translation, as ASR or punctuation updates are received. Currently, the main problem is that punctuation prediction does not have access to the sound any mor"
2021.eacl-demos.32,D19-1137,0,0.29802,"product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) proposed a new approach with a delay-based heuristic. The model decides to read more input (or wait for it) or wri"
2021.eacl-demos.32,2020.findings-emnlp.349,0,0.0269839,"text available, the user does not see sufficient number of words to let the brain “make up” or reconstruct the original meaning from pieces. The short-term memory of recently processed text does not seem to be sufficient for this type recovery, while seeing the words in larger context gives the user a better chance. The last step in an SLT system is the delivery of the translated content to the user. Our goal stops at the textual representation, i.e. we do not include speech synthesis and delivery of the sound, which would bring yet another set of design decisions and open problems, see e.g. Zheng et al. (2020). We experiment with two different views for our text output, both implemented as web applications. The “subtitle view” is optimized toward minimal use of screen space. Only two lines of text are available which leaves room either for e.g. a streamed video of the session or the slides, or for many languages displayed at once, if the screen is intended for a multi-lingual audience. The “paragraph view” provides more textual context to the user. 7.1 Subtitle View The subtitle view offers a simple interface with a HLS stream of the video or slides and one or more subtitles streams. Section 7.1 pr"
2021.eacl-demos.32,W19-5337,1,0.807801,"latency and hypotheses updates, as in KIT Lecture Translator (M¨uller et al., 2016). We use the hybrid ASR models based on Janus from KIT Lecture Translator, for German and English, as well as recent neural sequence-to-sequence ASR models trained on the same data (Nguyen et al., 2020). For Czech ASR, we use a Kaldi hybrid model trained on a Corpus of Czech Parliament Plenary Hearings (Kratochv´ıl et al., 2019). Czech sequence-to-sequence ASR is a work in progress. 5.2 MT Systems in ELITR We use bilingual NMT models for some high resource and well-studied language pairs e.g. for English-Czech (Popel et al., 2019; Wetesko et al., 2019). For other targets, we use multi-target models, e.g. an English-centric universal model for ELITR Flexible Architecture We always strive for the best performance for each considered language pair. With the perpetual com272 Index Name auto-iwslt2020-antrecorp(ASR) auto-iwslt2020-antrecorp(MT) auto-iwslt2020-antrecorp(MT) auto-asr-english-auditing(ASR) auto-asr-english-auditing(MT) auto-asr-english-auditing(MT) auto-iwslt2020-khanacademy(ASR) Worker en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-en to 41 en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-e"
2021.eacl-demos.9,2020.eamt-1.53,1,0.78716,"Missing"
2021.eacl-demos.9,2021.eacl-demos.32,1,0.748319,"Missing"
2021.eacl-demos.9,P19-1126,0,0.026489,"Missing"
2021.eacl-demos.9,W08-0509,0,0.0767354,"e preceding “Unternehmen” (company) was available only at 1062. For “unser”, SLT EV selects the expected time as the maximum between 895 (its expectation time under proportional delay) and 961 (the time that its aligned source word “our” appeared) . In other words, SLT EV gives more time to the SLT system to display the “unser” because its aligned word is output a bit later than the proportional expectation of “unser”. Under the alignment-based delay, we do not expect that the word will be output earlier than its alignment indicates. Technically, we rely on automatic word alignments by MGIZA (Gao and Vogel, 2008) which is a multi-threaded version of GIZA++ (Och and Ney, 2003), aligning the completed segments of the golden source transcript and the reference translation. The effect of alignment errors on the reliability of the evaluation is yet to be explored. 4.2.3 Multi-Reference Delay Calculation With multiple references, we create a separate table T for each and calculate the delay of each segment individually, taking the minimum across all references. The final delay is the sum of these minima. We use this strategy for both delay calculation methods and both segmentation strategies introduced abov"
2021.eacl-demos.9,D14-1140,0,0.0387859,"Missing"
2021.eacl-demos.9,N12-1048,0,0.0274606,"defined measures with a normalized Flicker in our work. We also propose a new averaging of older measures. A preliminary version of SLT EV was used in the IWSLT 2020 SHARED TASK. Moreover, a growing collection of test datasets directly accessible by SLT EV are provided for system evaluation comparable across papers. 1 Introduction Spoken Language Translation (SLT), i.e. translation of human speech across languages, is an application at least as important as Machine Translation (MT). Many approaches have been examined so far, ranging from translation of transcript chunks (F¨ugen et al., 2008; Bangalore et al., 2012) to fully end-to-end, speech-to-speech neural systems, (Jia et al., 2019). In recent years, simultaneous translation systems aim at behavior similar to human interpreters, digesting and producing an infinite sequence of words. Some systems (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019b; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019) do not consider any revision of their outputs and can be evaluated 2 Related Work SLT EV is designed to be versatile enough to score automatic SLT as well as transcribed human interpretation. Shimizu et al. (20"
2021.eacl-demos.9,E17-1099,0,0.0611745,"ion comparable across papers. 1 Introduction Spoken Language Translation (SLT), i.e. translation of human speech across languages, is an application at least as important as Machine Translation (MT). Many approaches have been examined so far, ranging from translation of transcript chunks (F¨ugen et al., 2008; Bangalore et al., 2012) to fully end-to-end, speech-to-speech neural systems, (Jia et al., 2019). In recent years, simultaneous translation systems aim at behavior similar to human interpreters, digesting and producing an infinite sequence of words. Some systems (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019b; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019) do not consider any revision of their outputs and can be evaluated 2 Related Work SLT EV is designed to be versatile enough to score automatic SLT as well as transcribed human interpretation. Shimizu et al. (2014) are probably the first to score human interpretation with automatic 1 https://github.com/ELITR/SLTev 71 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 71–79 April 19 - 23, 2021. ©2021 As"
2021.eacl-demos.9,J03-1002,0,0.0437663,"r “unser”, SLT EV selects the expected time as the maximum between 895 (its expectation time under proportional delay) and 961 (the time that its aligned source word “our” appeared) . In other words, SLT EV gives more time to the SLT system to display the “unser” because its aligned word is output a bit later than the proportional expectation of “unser”. Under the alignment-based delay, we do not expect that the word will be output earlier than its alignment indicates. Technically, we rely on automatic word alignments by MGIZA (Gao and Vogel, 2008) which is a multi-threaded version of GIZA++ (Och and Ney, 2003), aligning the completed segments of the golden source transcript and the reference translation. The effect of alignment errors on the reliability of the evaluation is yet to be explored. 4.2.3 Multi-Reference Delay Calculation With multiple references, we create a separate table T for each and calculate the delay of each segment individually, taking the minimum across all references. The final delay is the sum of these minima. We use this strategy for both delay calculation methods and both segmentation strategies introduced above. 4.3 Flicker to Assess Stability For systems that revise their"
2021.eacl-demos.9,P04-1077,0,0.119574,"(i.e., the number of corrections) by measuring the overlap between consecutive updates. As soon as a word is changed, all the following words are counted as updated, suggesting than any word change forces the user to reread all the rest. Gu et al. (2017) consider two versions of delay when assessing their reinforcement learning based simultaneous SLT model: Average Proportion (of waiting compared to producing words) and Consecutive Wait (the silence duration so far), and prescribe a target value for each of them to steer the learning, also balancing it with quality estimated by smoothed BLEU (Lin and Och, 2004). Since their model does not allow corrections, they do not require a measure of stability. The delay measures of (Gu et al., 2017) were criticised by Ma et al. (2019) when they introduced their wait-k model. They defined a measure Average Lag (AL), which measures how far, in words, the translation is behind an ideal wait-k model. Since wait-k does not allow corrections, they do not need a stability measure. AL was improved by Cherry and Foster (2019), with Differentiable Average Lag (DAL), which not only is differentiable, but fixes some undesirable behaviour of AL around sentence boundaries."
2021.eacl-demos.9,P02-1040,0,0.124861,"50 Good 65 Good mor 119 Good morning how 195 Good morning. How are you? 102 Good morning. 218 How are you? I 195 How are you? 239 I am ... ... (a) Time-stamped transcript P P P P C P C P ... 60 80 130 201 201 220 220 245 ... 0 0 0 0 0 102 102 195 ... 50 Gut 65 Guten Morgen! 119 Guten wie morgen 195 Guten Morgen! Wie geht es dir? 102 Guten Morgen! 218 Wie geht es dir? Ich 195 Wie geht es dir? 239 Ich bin ... ... (b) SLT candidate output Figure 1: Example of SLT EV file formats. All timestamps in centiseconds. measures but they segment the output manually and assess only the quality using BLEU (Papineni et al., 2002), WER (Matusov et al., 2005), TER (Snover et al., 2006), and RIBES (Isozaki et al., 2010). Most SLT evaluations require sentence segmentation of the candidate to match the reference one. Using mwerSegmenter (Matusov et al., 2005), they re-segment the candidate automatically, minimizing WER against the reference. We complement this approach with time-based segmentation. Niehues et al. (2016) introduced the retranslation approach to simultaneous SLT, and define latency based on the time between a word expected and actually displayed, considering only the final version of the word, not early revi"
2021.eacl-demos.9,W18-6319,0,0.0232859,"nt of revision. Trading these qualities for one another is again possible: It is obvious that if a system creates the translations with a longer Delay or revises them more (higher Flicker), the quality of the final translation (i.e., the output text) can be better. Given the existence of three evaluation criteria and a multitude of possible definitions for each of them, the need for some robust and standard metrics to evaluate SLT is inevitable. Recently, the MT community tackled a similar problem (i.e., the inconsistency in the reporting of BLEU scores) by introducing a tool named sacreBLEU (Post, 2018) with a canonical implementation of the widely user metric. In this work, we propose SLT EV,1 an open-source tool to calculate the quality of SLT systems based on three different criteria: translation quality, latency, and stability, in a standardized way. Furthermore, we complement SLT EV with a growing collection of freelyavailable test sets for Automatic Speech Recognition (ASR), MT and SLT for a number of languages, so that these technologies can be evaluated in comparable settings, similarly to what the WMT news test sets (Barrault et al., 2020) offer in MT. Automatic evaluation of Machin"
2021.eacl-demos.9,shimizu-etal-2014-collection,0,0.0219904,"lore et al., 2012) to fully end-to-end, speech-to-speech neural systems, (Jia et al., 2019). In recent years, simultaneous translation systems aim at behavior similar to human interpreters, digesting and producing an infinite sequence of words. Some systems (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019b; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019) do not consider any revision of their outputs and can be evaluated 2 Related Work SLT EV is designed to be versatile enough to score automatic SLT as well as transcribed human interpretation. Shimizu et al. (2014) are probably the first to score human interpretation with automatic 1 https://github.com/ELITR/SLTev 71 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 71–79 April 19 - 23, 2021. ©2021 Association for Computational Linguistics P P P P C P C P ... 0 0 0 0 0 102 102 195 ... 50 Good 65 Good mor 119 Good morning how 195 Good morning. How are you? 102 Good morning. 218 How are you? I 195 How are you? 239 I am ... ... (a) Time-stamped transcript P P P P C P C P ... 60 80 130 201 201 220 220 245 ... 0 0 0 0 0 1"
2021.eacl-demos.9,2020.emnlp-demos.19,0,0.0488099,"ability. For evaluation, they check the output of ASR and the output of the MT system as recorded over time in their simple logging system. They assess the quality, latency, and stability (i.e., Flicker). The quality is estimated using BLEU after mwerSegmenter re-segmentation. For the assessment of latency (translation lag) and stability (the number of erased tokens in temporary translations per final target token, “Normalized Erasure” in the paper), they do not use any segmentation at all and instead calculate the scores for ten-minute long audio chunks. The closest to our work is SIMULEVAL (Ma et al., 2020), a client-server toolkit measuring the latency of SLT including any network effects between the evaluated system (client) and the mock user (SIMULEVAL server). SIMULEVAL offers a nice visualization interface but the required clientserver approach may be unsuitable for research prototypes solving SLT only partially. Most importantly, updates of output (Flicker) are not supported and no test set for reproducible scoring is provided. 3 Input Formats SLT EV can evaluate separate ASR and MT systems as well as cascaded and end-to-end SLT systems. We focus on SLT here. Three input files are used for"
2021.eacl-demos.9,2006.amta-papers.25,0,0.205111,". How are you? 102 Good morning. 218 How are you? I 195 How are you? 239 I am ... ... (a) Time-stamped transcript P P P P C P C P ... 60 80 130 201 201 220 220 245 ... 0 0 0 0 0 102 102 195 ... 50 Gut 65 Guten Morgen! 119 Guten wie morgen 195 Guten Morgen! Wie geht es dir? 102 Guten Morgen! 218 Wie geht es dir? Ich 195 Wie geht es dir? 239 Ich bin ... ... (b) SLT candidate output Figure 1: Example of SLT EV file formats. All timestamps in centiseconds. measures but they segment the output manually and assess only the quality using BLEU (Papineni et al., 2002), WER (Matusov et al., 2005), TER (Snover et al., 2006), and RIBES (Isozaki et al., 2010). Most SLT evaluations require sentence segmentation of the candidate to match the reference one. Using mwerSegmenter (Matusov et al., 2005), they re-segment the candidate automatically, minimizing WER against the reference. We complement this approach with time-based segmentation. Niehues et al. (2016) introduced the retranslation approach to simultaneous SLT, and define latency based on the time between a word expected and actually displayed, considering only the final version of the word, not early revisions. They did not provide any evaluation of stability"
2021.eacl-demos.9,2005.iwslt-1.19,0,0.116909,"morning how 195 Good morning. How are you? 102 Good morning. 218 How are you? I 195 How are you? 239 I am ... ... (a) Time-stamped transcript P P P P C P C P ... 60 80 130 201 201 220 220 245 ... 0 0 0 0 0 102 102 195 ... 50 Gut 65 Guten Morgen! 119 Guten wie morgen 195 Guten Morgen! Wie geht es dir? 102 Guten Morgen! 218 Wie geht es dir? Ich 195 Wie geht es dir? 239 Ich bin ... ... (b) SLT candidate output Figure 1: Example of SLT EV file formats. All timestamps in centiseconds. measures but they segment the output manually and assess only the quality using BLEU (Papineni et al., 2002), WER (Matusov et al., 2005), TER (Snover et al., 2006), and RIBES (Isozaki et al., 2010). Most SLT evaluations require sentence segmentation of the candidate to match the reference one. Using mwerSegmenter (Matusov et al., 2005), they re-segment the candidate automatically, minimizing WER against the reference. We complement this approach with time-based segmentation. Niehues et al. (2016) introduced the retranslation approach to simultaneous SLT, and define latency based on the time between a word expected and actually displayed, considering only the final version of the word, not early revisions. They did not provide"
2021.eacl-demos.9,N16-3017,0,0.0588397,"Missing"
2021.eacl-demos.9,D19-1137,0,0.0111475,"n speech across languages, is an application at least as important as Machine Translation (MT). Many approaches have been examined so far, ranging from translation of transcript chunks (F¨ugen et al., 2008; Bangalore et al., 2012) to fully end-to-end, speech-to-speech neural systems, (Jia et al., 2019). In recent years, simultaneous translation systems aim at behavior similar to human interpreters, digesting and producing an infinite sequence of words. Some systems (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019b; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019) do not consider any revision of their outputs and can be evaluated 2 Related Work SLT EV is designed to be versatile enough to score automatic SLT as well as transcribed human interpretation. Shimizu et al. (2014) are probably the first to score human interpretation with automatic 1 https://github.com/ELITR/SLTev 71 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 71–79 April 19 - 23, 2021. ©2021 Association for Computational Linguistics P P P P C P C P ... 0 0 0 0 0 102 102 195 ... 50 Good 65 Good mor 11"
2021.emnlp-main.650,P11-2031,0,0.13472,"Missing"
2021.emnlp-main.650,N19-1423,0,0.0181702,"w the Transformer-based sequence-to-sequence models fail when the target sequence lengths of the training and validation data do not match. We show that this holds not only for very long test sequences but can be observed even with short sequences if they are omitted from the training data. Furthermore, we show that we can artificially improve the test performance on longer sequences by only using shorter training sequences and concatenating them into longer training examples. Current state-of-the-art Transformer-based sequence generation models, either fine-tuned for chosen downstream tasks (Devlin et al., 2019), or trained from scratch for specific tasks such as maWe do not argue about Transformer’s (in)ability chine translation (Vaswani et al., 2017) or speech to handle long-distance dependencies, but our rerecognition (Pham et al., 2019), more and more sults suggest that a considerably simpler reason of often achieve performance comparable to that of mismatching sequence length can also contribute humans (Hassan et al., 2018; Popel et al., 2020; to the performance drop. We think that our findNguyen et al., 2020). However, such models fre- ings can lead to better understanding of the Transquently r"
2021.emnlp-main.650,2020.findings-emnlp.301,0,0.034144,"of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence. 1 Introduction tokens) to reach such performance (Brown et al., 2020). The good performance on held-out test sets seems to confirm the good generalization power of these models, although the inherent strong biases, sometimes leading to the use of a foul and toxic language, preserving stereotypes, etc., are well acknowledged (Gehman et al., 2020). Brown et al. (2020) claim that their Transformer model is also capable of simple arithmetics, however, it is yet to be validated whether the model truly learns the arithmetic algorithms or simply encodes a lookup table for a subset of specific examples. In this paper, we argue that the assumed generalization power of the current state-of-the-art Transformer-based language generators does not come from the architecture itself but rather from the sheer volume of training data and the model’s ability to exploit the similarities between the training and validation data. We demonstrate how the Tr"
2021.emnlp-main.650,P19-1267,0,0.0264569,"However, these models still got outperformed on long sequences by phrasebased models (Koehn and Knowles, 2017). This problem was not resolved with the introduction of Transformers (Vaswani et al., 2017). Surprisingly, even though there were previous studies explaining the weaknesses of RNNs with respect to long sequence modeling (Hochreiter and Schmidhuber, 1997; Hochreiter, 1998), similar analyses are yet to be done for Transformers which are fundamentally different from RNNs. There is an ongoing debate about the proper way of splitting the available data to training and evaluation subsets. Gorman and Bedrick (2019) show that using only standard dataset splits can lead to a biased evaluation resulting in overestimating the generalization ability of the model. Furthermore, Søgaard et al. (2020) argue that even using randomly sampled dataset splits does not solve the overestimation problem. They instead suggest using multiple test sets possibly of an adversarial nature to properly evaluate the generalization ability of the model. In the following experiments, we evaluate vanilla Transformer on such adversarial splits created with respect to the lengths of the modeled sequences. Although similar analyses we"
2021.emnlp-main.650,P02-1040,0,0.110187,"encoding (Sennrich et al., 2016) on our training data, to create subword segmentation of size 30k.6 We split all tokenized and BPE-segmented datasets into buckets of sizes 1-10, 11-20, ..., 91-100 (labeled as 10, 20, ..., 100 respectively) based on the number of tokens on the target side. Table 3 shows the sizes of the respective training corpora. We train a separate model for each training bucket. Details on the model hyper-parameters are available in Appendix A.2. We evaluate how the length of the training data affects the performance with respect to the length of the test data using BLEU (Papineni et al., 2002), namely the SacreBLEU implementation (Post, 2018).7 Figure 1 (Top) shows that regardless of the training bucket, the model performs best when presented with data of target-side length similar to the length of the training data. This confirms that the model overfits to the length of the training data, affecting its performance even on shorter sentences. The performance further decreases with increasing train-test length difference, although it needs to be noted that the BLEU scores between different testset buckets are not directly comparable due to the nature of the scoring metric and the fac"
2021.emnlp-main.650,W18-6319,0,0.0126942,"eate subword segmentation of size 30k.6 We split all tokenized and BPE-segmented datasets into buckets of sizes 1-10, 11-20, ..., 91-100 (labeled as 10, 20, ..., 100 respectively) based on the number of tokens on the target side. Table 3 shows the sizes of the respective training corpora. We train a separate model for each training bucket. Details on the model hyper-parameters are available in Appendix A.2. We evaluate how the length of the training data affects the performance with respect to the length of the test data using BLEU (Papineni et al., 2002), namely the SacreBLEU implementation (Post, 2018).7 Figure 1 (Top) shows that regardless of the training bucket, the model performs best when presented with data of target-side length similar to the length of the training data. This confirms that the model overfits to the length of the training data, affecting its performance even on shorter sentences. The performance further decreases with increasing train-test length difference, although it needs to be noted that the BLEU scores between different testset buckets are not directly comparable due to the nature of the scoring metric and the fact that each testset bucket contains different test"
2021.emnlp-main.650,P16-1162,0,0.0593604,"he arguments for unshift and push are sampled from a Bernoulli distribution with 0 character having p = 0.5. 3.2 Machine Translation To see whether our findings within the string editing tasks also hold for natural language which has more complex structure, we perform similar experiments on English-Czech translation. We use CzEng 2.03 (Kocmi et al., 2020) as our training corpus, a concatenation of WMT2020 (Barrault et al., 2020) newstest13-16 as held-out test set and a concatenation of newstest17-20 for final evaluation.4 We tokenize our data using Moses tokenizer.5 We use byte-pair encoding (Sennrich et al., 2016) on our training data, to create subword segmentation of size 30k.6 We split all tokenized and BPE-segmented datasets into buckets of sizes 1-10, 11-20, ..., 91-100 (labeled as 10, 20, ..., 100 respectively) based on the number of tokens on the target side. Table 3 shows the sizes of the respective training corpora. We train a separate model for each training bucket. Details on the model hyper-parameters are available in Appendix A.2. We evaluate how the length of the training data affects the performance with respect to the length of the test data using BLEU (Papineni et al., 2002), namely th"
2021.emnlp-main.650,N18-2074,0,0.0257886,"70 and 80 buckets migth be due to significantly smaller size of training data (< 1M sentence pairs). In Appendix B, we also provide a case study of the models trained on various length buckets. The length-controlled experiment results presented by Neishi and Yoshinaga (2019), while not directly focused on exploring the target-side length overfitting phenomenon, point to a similar behavior of vanilla Transformers with regards to both longer and shorter test sentences. Based on their results, the replacement of the absolute positional embeddings with a variation of relative-position embeddings (Shaw et al., 2018; Neishi and Yoshinaga, 2019) seems like a promising approach towards alleviating the length overfitting problem. training strategy using the synthetic data to see how strongly can the length of the training examples (although artificial) affect the model performance on the test examples of similar length. Figure 2 shows that the simple concatenation of shorter training sentence pairs can lead to a performance similar to the model trained on the genuinely longer sentences. Only the performance of the model trained on the concatenation of very short sentences (the line “TrainBucket.Concat=10” i"
2021.emnlp-main.801,W14-0307,0,0.0593636,"Missing"
2021.emnlp-main.801,W17-3204,0,0.0599099,"Missing"
2021.emnlp-main.801,2013.mtsummit-wptp.1,0,0.0657736,"Missing"
2021.emnlp-main.801,2020.eamt-1.13,0,0.0876269,"Missing"
2021.emnlp-main.801,W19-6626,0,0.0261339,"Missing"
2021.emnlp-main.801,2013.mtsummit-wptp.5,0,0.0528047,"Missing"
2021.emnlp-main.801,2020.wmt-1.28,1,0.703983,"ith clearly marked document boundaries. For clarity, we will refer to the whole set simply as “file” and the individual parts as “documents”. 3.2 cloud.google.com/translate azure.microsoft.com/en-us/services/cognitiveservices/translator/ 6 We also experimented with BERTScore but its Pearson correlation with BLEU is 0.9939. This would lead to the same observations and conclusions. 5 Model M01 M02 M03 M04 M05 M06 M07 M08 M09 M10 M11 Google Microsoft Machine Translation Models In total, we used 13 MT models of various quality. Models M01–M11 are based on the setup, training procedure and data of Popel (2020). We chose this particular approach because it has been reported to reach human translation quality (Popel et al., 2020). For our purposes, we reproduce the training, stopping it at various stages of the training process. All MT systems translate sentences in isolation, with the exception of M11, which is a document-level system (replicating CUNI-DocTransformer in Popel (2020)). Systems MT01–MT10 differ only in the number of training steps, which affects also the ratio of authentic- and synthetic- data checkpoints in the hourly checkpoint averaging (Popel et al., 2020): the best devset BLEU wa"
2021.emnlp-main.801,W15-3049,0,0.0606843,"Missing"
2021.emnlp-main.801,W18-6319,0,0.0252958,"4.65).6 Most of the systems are concentrated in the upper half of the range. 4 This better reflects realistic scenarios in localization workflows where users can typically decide among several engines of comparable but not identical performance. TER BLEU Steps [k] 0.729 0.678 0.655 0.648 0.622 0.624 0.604 0.600 0.603 0.600 0.601 0.623 0.632 25.35 31.61 33.09 33.63 35.22 35.68 36.58 36.41 37.40 37.44 37.37 37.56 33.06 25.4 29.0 29.3 33.0 72.8 997.1 1015.2 1022.4 1055.0 1058.6 698.5 – – ACh 8 8 8 8 6 0 5 6 8 6 5 – – Table 1: Overview of MT systems used. TER and BLEU were measured by SacreBLEU7 (Post, 2018). Steps mark the number of training steps in thousands. ACh is the number of authentic-data-trained checkpoints in an average of 8 checkpoints. 3.3 Translation Process We carried out the translation in two stages: MT post-editing stage and final revision stage. For both stages, we used Memsource as the computerassisted translation (CAT) tool. (1) Post-editing The documents were first translated by all 13 MT systems. In addition, we included a variant with no translation (”Source”) and with a pre-existing reference translation (”Reference”).8 The translated files were shuffled at document bound"
2021.emnlp-main.801,2016.amta-researchers.2,0,0.103117,"ss to the original text as well. Finally, Koponen (2013) comments on the high variance of post-editors, which is a common problem in postediting research (Koponen, 2016). Interactive MT is an alternative use case of computer-assisted translation and it is possible that effort or behavioural patterns in interactive MT could be used as a different proxy extrinsic measure for MT quality. Post-editor productivity has also been measured in contrast to interactive translation prediction by Sanchis-Trilles et al. (2014). Additionally, our focus is state-of-the-art NMT systems, which was not true for Sanchez-Torron and Koehn (2016), who constructed 9 artificially severely degraded statistical phrase-based MT systems. The experiment by Koehn and Germann (2014) used only 4 MT systems. Our focus is motivated by the industry’s direct application: Considering the cost of skilled staff and model training, what are the practical benefits of improving MT performance? In contrast to the previous setups, we evaluate two additional settings: post-editing human reference and translating from scratch, corresponding to a theoretical2 BLEU of 100 and 0, respectively. We also consider the quality of the PE output and not only the proce"
2021.emnlp-main.801,2009.mtsummit-posters.20,0,0.208863,"Missing"
2021.emnlp-main.801,tiedemann-2012-parallel,0,0.0132279,"the translation process. NMT system quality and PE effort is not a simple one and that older results based on statistical MT 2 In fact, humans never produce the same translation, so may not directly carry over to NMT. The first of BLEU of 100 is unattainable, and the source text often conthe six challenges listed by Koehn and Knowles tains some tokens appearing also in the output, so not translating can reach BLEU scores of e.g. 3 or 4. (2017) suggests that fluency over adequacy can be 3 Document-level BLEU of 19.3 on miscellaneous a critical issue: NMT systems have lower quality FI→SV OPUS (Tiedemann, 2012) data. Current state of the out of domain, to the point that they completely ¨ benchmark (Tiedemann et al., art is 29.5 on the FIKSMO sacrifice adequacy for the sake of fluency. 2020). 10205 3.1 Documents In total, we used 99 source lines (segments) of 8 different parallel English documents for which Czech human reference translations were available. One line can contain more than one sentence, which is reflected by the rather high average sentence length of 25 words. We chose the domains to mirror common use-cases in localization: 36 lines of news texts (WMT19 News testset), 29 lines from a l"
2021.emnlp-main.801,2020.lrec-1.470,0,0.0240506,"Missing"
2021.emnlp-main.801,2020.wmt-1.41,1,0.825737,"Missing"
2021.humeval-1.13,2020.emnlp-main.5,0,0.0403366,"Missing"
2021.humeval-1.13,W13-2305,0,0.0101524,"he 130 documents, using the RankME evaluation (Novikova et al., 2018) following the methodology of Popel et al. (2020). In this RankME evaluation, fluency, adequacy and overall quality are evaluated in a source-based sentencelevel document-aware fashion, on a 0–10 scale, where all the evaluated translations are shown on the same screen, allowing thus better reliability in comparisons; see Section 5 for details. 3 Automatic analysis of references Table 1 shows the translation quality of the three references and two selected MT systems according to two manual evaluations, DA (Direct Assessment, Graham et al., 2013) and RankME, and four types of BLEU scores. The first three types use http://www.statmt.org/wmt06 till wmt20 2 The additional references R EF 2 and R EF 3 were not available before our RankME evaluation started. We plan to evaluate them in future. 114 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 114–119 Online, April 19, 2021. ©2021 Association for Computational Linguistics manual system REF 1 REF 2 REF 3 CUNI-D OC T RANSFORMER O NLINE -B BLEU DA RankME R EF 1 R EF 2 R EF 3 R EF 2+3 85.6 – – 82.8 70.5 8.17 – – 7.39 5.62 – 28.90 24.20 35.88 41.11 28.91 – 26.45"
2021.humeval-1.13,N18-2012,0,0.0118233,"-G and O NLINE -Z. We focused on three translations: the official reference, R EF 1; the best-performing MT system (according to the official WMT manual evaluation), CUNI-D OC T RANSFORMER (Popel, 2020); and the best-performing online system, O NLINE -B. We hired two professional translators (native Czech speakers) to translate the whole WMT20 test set, thus creating additional references R EF 2 and R EF 3. We also hired 18 annotators to judge the translation quality of R EF 1, CUNID OC T RANSFORMER and O NLINE -B.2 The annotators assessed 90 of the 130 documents, using the RankME evaluation (Novikova et al., 2018) following the methodology of Popel et al. (2020). In this RankME evaluation, fluency, adequacy and overall quality are evaluated in a source-based sentencelevel document-aware fashion, on a 0–10 scale, where all the evaluated translations are shown on the same screen, allowing thus better reliability in comparisons; see Section 5 for details. 3 Automatic analysis of references Table 1 shows the translation quality of the three references and two selected MT systems according to two manual evaluations, DA (Direct Assessment, Graham et al., 2013) and RankME, and four types of BLEU scores. The f"
2021.humeval-1.13,2020.wmt-1.28,1,0.692658,"nally written 1 in English – news stories downloaded from web. The test set comes with an official reference translation into Czech (R EF 1) provided by the WMT organizers and done by a professional translation agency. There are also 8 machine translations submitted by the participants of the WMT news translation shared task and 4 translations by online systems anonymized as O NLINE -A, O NLINE -B, O NLINE -G and O NLINE -Z. We focused on three translations: the official reference, R EF 1; the best-performing MT system (according to the official WMT manual evaluation), CUNI-D OC T RANSFORMER (Popel, 2020); and the best-performing online system, O NLINE -B. We hired two professional translators (native Czech speakers) to translate the whole WMT20 test set, thus creating additional references R EF 2 and R EF 3. We also hired 18 annotators to judge the translation quality of R EF 1, CUNID OC T RANSFORMER and O NLINE -B.2 The annotators assessed 90 of the 130 documents, using the RankME evaluation (Novikova et al., 2018) following the methodology of Popel et al. (2020). In this RankME evaluation, fluency, adequacy and overall quality are evaluated in a source-based sentencelevel document-aware fas"
2021.humeval-1.13,W15-3049,0,0.0792015,"Missing"
2021.humeval-1.13,W18-6319,0,0.0273676,"pril 19, 2021. ©2021 Association for Computational Linguistics manual system REF 1 REF 2 REF 3 CUNI-D OC T RANSFORMER O NLINE -B BLEU DA RankME R EF 1 R EF 2 R EF 3 R EF 2+3 85.6 – – 82.8 70.5 8.17 – – 7.39 5.62 – 28.90 24.20 35.88 41.11 28.91 – 26.45 36.50 31.08 24.18 26.43 – 30.17 26.39 37.22 – – 47.59 41.00 Table 1: Manual and automatic evaluation scores of the systems in our study. DA is the source-based Direct Assessment average score (un-normalized). RankME is the average Overall quality score over all 90 documents (not sentences) evaluated in our study. BLEU is computed with SacreBLEU (Post, 2018) with signature BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.4.13. The best score in each column is in bold. R EF 1, R EF 2 and R EF 3, respectively, as the reference translation in BLEU. The fourth type uses BLEU with two reference translations: R EF 2+3. While both manual evaluations, DA and RankME, agree that R EF 1 is better than both CUNI-D OC T RANSFORMER and O NLINE -B, the automatic metric BLEU evaluates one of the two MT systems as better than R EF 1.3 For brevity, we report only BLEU, but we confirmed this with several other automatic metrics, e.g. chrF (Popovi´c, 2015). Th"
2021.iwslt-1.1,2020.lrec-1.520,0,0.0759372,"Missing"
2021.iwslt-1.1,2020.iwslt-1.3,0,0.0592828,"Missing"
2021.iwslt-1.1,2021.iwslt-1.5,0,0.0628415,"Missing"
2021.iwslt-1.1,N19-1202,1,0.897776,"Missing"
2021.iwslt-1.1,2021.acl-long.224,1,0.769761,"sh-German section of the MuST-C V2 corpus7 and include training, dev, and test (Test Common), in the same structure of the MuST-C V1 corpus (Cattoni et al., 2021) used last year. Since the 2021 test set was processed using the same pipeline applied to create MuST-C V2, the use of the new training resource was strongly recommended. The main differences with respect to MuST-C v1 are: gap with respect to the traditional cascade approach (integrating ASR and MT components in a pipelined architecture). In light of last year’s IWSLT results (Ansari et al., 2020) and of the findings of recent works (Bentivogli et al., 2021) attesting that the gap between the two paradigms has substantially closed, also this year a key element of the evaluation was to set up a shared framework for their comparison. For this reason, and to reliably measure progress with respect to the past rounds, the general evaluation setting was kept unchanged. This stability mainly concerns two aspects: the allowed architectures and the test set provision. On the architecture side, participation was allowed both with cascade and end-to-end (also known as direct) systems. In the latter case, valid submissions had to be obtained by models that:"
2021.iwslt-1.1,2021.iwslt-1.22,0,0.082468,"Missing"
2021.iwslt-1.1,2021.iwslt-1.27,1,0.721453,"to use the same training and development data as in the Offline Speech Translation track. More details are available in §3.2. For the English-Japanese text-to-text track, participants could use the parallel data and monolingual data available for the English-Japanese WMT20 news task (Barrault et al., 2020). For development, participants could use the IWSLT 2017 development sets,2 the IWSLT 2021 development set3 and the simultaneous interpretation transcripts for the IWSLT 2021 development set.4 The simultaneous interpretation was recorded as a part of NAIST Simultaneous Interpretation Corpus (Doi et al., 2021). Systems were evaluated with respect to quality and latency. Quality was evaluated with the standard BLEU metric (Papineni et al., 2002a). Latency was evaluated with metrics developed for simultaneous machine translation, including average proportion (AP), average lagging (AL) and differentiable average lagging (DAL, Cherry and Foster 2019), and later extended to the task of simultaneous speech translation (Ma et al., 2020b). The evaluation was run with the S IMUL E VAL toolkit (Ma et al., 2020a). For the latency measurement of speech input systems, we contrasted computation-aware and non com"
2021.iwslt-1.1,2012.eamt-1.60,1,0.698226,"rently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun,"
2021.iwslt-1.1,2020.iwslt-1.8,1,0.838546,"Missing"
2021.iwslt-1.1,2021.iwslt-1.12,0,0.0248123,"a et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two S"
2021.iwslt-1.1,L18-1001,0,0.0652124,"Missing"
2021.iwslt-1.1,2021.acl-long.68,1,0.787992,"Missing"
2021.iwslt-1.1,L18-1275,0,0.0286278,"ng two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guidelines.19 This makes them less literal compared to standard, unconstrained translations; • OpenSubtitles 2018 (Lison et al., 2018); • Augmented LibriSpeech et al., 2018)16 (Kocabiyikoglu • Mozilla Common Voice17 ; • Unconstrained translations. These references were created from scratch20 by adhering to the usual translation guidelines. They are hence exact (more literal) translations, without paraphrasing and with proper punctuation. • LibriSpeech ASR corpus (Panayotov et al., 2015). The list of allowed development data includes the dev set from IWSLT 2010, as well as the test sets used for the 2010, 2013, 2014, 2015 and 2018 IWSLT campaigns. Using other training/development resources was allowed but, in this case, parti"
2021.iwslt-1.1,2020.iwslt-1.9,0,0.0594925,"Missing"
2021.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0742394,"Missing"
2021.iwslt-1.1,2021.iwslt-1.4,0,0.134226,"essler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and acces"
2021.iwslt-1.1,N18-2074,0,0.0499123,"Missing"
2021.iwslt-1.1,2006.amta-papers.25,0,0.370434,"Missing"
2021.iwslt-1.1,W14-3354,0,0.0745687,"Missing"
2021.iwslt-1.1,W18-6319,0,0.0144137,"cipants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guideli"
2021.iwslt-1.1,2021.iwslt-1.19,0,0.0280723,", 2021) Fondazione Bruno Kessler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; comm"
2021.iwslt-1.1,2020.findings-emnlp.230,0,0.0691772,"Missing"
2021.iwslt-1.1,2021.iwslt-1.7,0,0.0794497,"Missing"
2021.iwslt-1.1,2021.iwslt-1.16,0,0.0332503,"nyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two Swahili varieties (Congolese and Co"
2021.iwslt-1.1,2020.lrec-1.517,1,0.846095,"s were finally converted from two (stereo) to one (mono) channel and downsampled from 48 to 16 kHz, using FFmpeg.10 Upon inspection of the spectrograms of the same talks in the two versions of MuST-C, it clearly emerges that the upper limit band in the audios used in MuST-C V1 is 5 kHz, while it is at 8 kHz in the latest version, coherently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 thi"
2021.iwslt-1.1,2021.iwslt-1.6,0,0.156379,"2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource spe"
2021.mtsummit-asltrw.3,2020.iwltp-1.7,1,0.815336,"Missing"
2021.mtsummit-asltrw.3,P19-1289,0,0.0279532,"Missing"
2021.mtsummit-asltrw.3,tiedemann-2012-parallel,0,0.0899064,"Missing"
2021.naacl-main.14,C04-1046,0,0.235102,"ment algorithm,5 enables us to identify the source words that have led to those translation errors. QE suggestions are presented by red word highlighting (see Figure 1). We note that word-level error annotation is a hard and costly task. Thus, available data for building systems to predict word-level errors is scarce. To circumvent this issue we relied on a feature-based approach which exploited information from the neural MT system (i.e. a glass-box approach to QE) and did not require large amounts of data for training. Glass-box features have been successfully used for QE of statistical MT (Blatz et al., 2004; Specia et al., 2013) and have been recently shown to be effective for sentence-level QE of neural MT systems (Fomicheva et al., 2020). To accommodate for the different types of MT models used in this work, including a student model Czech 2, we did not use the full set of features from Fomicheva et al. (2020) but instead relied on simple subset of log-probability based features: • Log-probability of the word • Log-prob. of the previous word • Log-prob. of the next word • Average log-prob. of the translated sentence • Number of characters in the word We build a binary gradient boosting classif"
2021.naacl-main.14,N13-1073,0,0.0967535,"Missing"
2021.naacl-main.14,D19-5503,0,0.0406336,"Missing"
2021.naacl-main.14,2020.wmt-1.17,0,0.16429,"31.61 Table 1: Performance of utilized MT systems in BLEU score evaluated on WMT18 test set; higher is better. Machine Translation. We used three MT systems for Czech (differing in speed and training data size) and one for Estonian. All of the systems were trained in both directions: the forward systems translate from English, whereas the opposite direction is used as a backward translation cue. All the MT systems follow the Transformer model architecture (Vaswani et al., 2017) design, though student systems make use of the simplified simple recurrent unit and other modifications described in Germann et al. (2020). Table 1 shows how the MT systems performed in terms of BLEU score (Papineni et al., 2002) on the test set of WMT18 News task (Bojar et al., 2018). The Czech 3 system is the winning MT model of Czech–English News Translation in WMT 2019 (Popel et al., 2019), having been trained on 58M authentic sentence pairs and 65M backtranslated monolingual sentences.4 The training proposed by Germann et al. (2020) was used for a CPU-optimized student model Czech 2. It was created by the knowledge distillation (Kim and Rush, 2016) method on translations generated by Czech 3. Although it has been trained so"
2021.naacl-main.14,D16-1139,0,0.0245682,"the simplified simple recurrent unit and other modifications described in Germann et al. (2020). Table 1 shows how the MT systems performed in terms of BLEU score (Papineni et al., 2002) on the test set of WMT18 News task (Bojar et al., 2018). The Czech 3 system is the winning MT model of Czech–English News Translation in WMT 2019 (Popel et al., 2019), having been trained on 58M authentic sentence pairs and 65M backtranslated monolingual sentences.4 The training proposed by Germann et al. (2020) was used for a CPU-optimized student model Czech 2. It was created by the knowledge distillation (Kim and Rush, 2016) method on translations generated by Czech 3. Although it has been trained solely on synthetic data, its performance in the news domain falls behind the teacher only by 0.5 to 3.0 BLEU points, depending on the translation direction. We included it mainly due to its speed as shown in Section 4. The design of the Czech 1 system is identical to Czech 3. The only difference is that the former was trained only on a subsample of 5M sentence pairs from CzEng 1.7 (Bojar et al., 2016). This system was chosen to simulate performance on less resourceful language pairs. The Estonian system uses the same c"
2021.naacl-main.14,2020.iwslt-1.25,1,0.687222,"Missing"
2021.naacl-main.14,P02-1040,0,0.109419,"t; higher is better. Machine Translation. We used three MT systems for Czech (differing in speed and training data size) and one for Estonian. All of the systems were trained in both directions: the forward systems translate from English, whereas the opposite direction is used as a backward translation cue. All the MT systems follow the Transformer model architecture (Vaswani et al., 2017) design, though student systems make use of the simplified simple recurrent unit and other modifications described in Germann et al. (2020). Table 1 shows how the MT systems performed in terms of BLEU score (Papineni et al., 2002) on the test set of WMT18 News task (Bojar et al., 2018). The Czech 3 system is the winning MT model of Czech–English News Translation in WMT 2019 (Popel et al., 2019), having been trained on 58M authentic sentence pairs and 65M backtranslated monolingual sentences.4 The training proposed by Germann et al. (2020) was used for a CPU-optimized student model Czech 2. It was created by the knowledge distillation (Kim and Rush, 2016) method on translations generated by Czech 3. Although it has been trained solely on synthetic data, its performance in the news domain falls behind the teacher only by"
2021.naacl-main.14,W19-5337,1,0.901334,"Missing"
2021.naacl-main.14,underwood-etal-2014-evaluating,0,0.0267014,"to human professionals in specific settings (Hassan et al., 2018; Popel et al., 2020), it is far from reasonable to blindly believe that the output of MT systems is perfectly accurate. It should thus not be simply included in an email or another message without some means of verification. Feedback in this scenario is needed, which would tell users if the translation is correct and ideally even give instructions on how to improve it. A related area of interactive machine translation (IMT) focuses mainly on either post-editor scenarios (Martínez-Gómez et al., 2012; Sanchis-Trilles et al., 2014; Underwood et al., 2014; Alabau et al., 2016) or generally scenarios in which users are able to produce the translation themselves and the system only aims to speed it up or improve it (Santy et al., 2019). Outbound translation differs from common IMT scenarios by the fact that the user does not speak the target language, and hence operates on the MT result only in a limited way. The first work to deal with this task by Zouhar and Bojar (2020) focused on working with CzechGerman MT in context of asking and reformulating questions. A preliminary experiment on the effect of translation cues has been carried out by Zou"
2021.naacl-main.14,2020.lrec-1.860,1,0.894876,"responsibility to create the content in the way that it is correctly interpreted by a recipient lies on the authors of the message. The main issue is that the target language might be entirely unknown to them. Prototypically it is communication by email, filling in foreign language forms, or involving some other kind of interactive medium. The focus in this scenario is placed not only on producing high-quality translations but also on reassuring the author that the MT output is correct. One of the approaches to improving both quality and authors’ confidence, first employed in this scenario by Zouhar and Bojar (2020), is to provide cues that indicate the quality of MT output as well as suggest possible rephrasing of the source. They may include backward translation to the source language, highlighting of the potentially problematic parts of the input, or suggesting paraphrases. Except for preliminary work by Zouhar and Novák (2020), the impact of individual cues has not yet been properly explored. In this paper, we present the results of a new experiment on outbound translation. Building on the previous works, the focus was expanded to investigate the influence of different levels of performance of the un"
2021.naacl-main.14,D19-3018,0,0.0174502,"It should thus not be simply included in an email or another message without some means of verification. Feedback in this scenario is needed, which would tell users if the translation is correct and ideally even give instructions on how to improve it. A related area of interactive machine translation (IMT) focuses mainly on either post-editor scenarios (Martínez-Gómez et al., 2012; Sanchis-Trilles et al., 2014; Underwood et al., 2014; Alabau et al., 2016) or generally scenarios in which users are able to produce the translation themselves and the system only aims to speed it up or improve it (Santy et al., 2019). Outbound translation differs from common IMT scenarios by the fact that the user does not speak the target language, and hence operates on the MT result only in a limited way. The first work to deal with this task by Zouhar and Bojar (2020) focused on working with CzechGerman MT in context of asking and reformulating questions. A preliminary experiment on the effect of translation cues has been carried out by Zouhar and Novák (2020), but it was conducted on a much smaller scale both in terms of participants and annotators and with non-native speakers of English. This may have affected the re"
2021.naacl-main.14,P13-4014,1,0.685445,"Missing"
2021.naacl-main.14,tiedemann-2012-parallel,0,0.0914444,"Missing"
2021.wat-1.1,2020.acl-main.703,0,0.187967,"news-commentary corpus.14 This year we also encouraged participants to use any corpora from WMT 202015 and WMT 202116 involving Japanese, Russian, and English as long as it did not belong to the news commentary domain to prevent any test set sentences from being unintentionally seen during training. 2,049 2,050 Table 5: The NICT-SAP task corpora splits. The corpora belong to two domains: wikinews (ALT) and software documentation (IT). The Wikinews corpora are Nway parallel. also encouraged the use of monolingual corpora expecting that it would be for pre-trained NMT models such as BART/MBART (Lewis et al., 2020; Liu et al., 2020). In Table 5 we give statistics of the aforementioned corpora which we used for the organizer’s baselines. Note that the evaluation corpora for both domains are created from documents and thus contain document level meta-data. Participants were encouraged to use document level approaches. Note that we do not exhaustively list6 all available corpora here and participants were not restricted from using any corpora as long as they are freely available. 2.7 Partition train development test train development test train development test 8 http://www.phontron.com/kftt/ https://data"
2021.wat-1.1,C18-2019,0,0.0200002,"task and 4 systems for the Japanese→ English.74 On the whole, all the submitted systems are basically lexical-constraint-aware NMT models with lexically constrained decoding method, where the restricted target vocabulary is concatenated into source sentences and, during the beam search at inference time, the models generate translation outputs containing the target vocabulary. We observed that these techniques boost the final translation performance of the NMT models in the restricted translation task. For human evaluation, we conducted the sourcebased direct assessment (Cettolo et al., 2017; Federmann, 2018) and source-based contrastive assessment (Sakaguchi and Van Durme, 2018; Federmann, 2018), to have the top-ranked systems of each team appraised by bilingual human annotators. In the human evaluation campaign, we also include the human reference data. Table 20 reports the final automatic evaluation score and the human evaluation results. In both tasks, the systems from the team “NTT” are the most highly evaluated in all the submitted systems in the final score and the human evaluation, consistently. We also note that our designed automation metric is well correlated Flickr30kEnt-JP Japanese↔En"
2021.wat-1.1,W18-1819,0,0.0552279,"Missing"
2021.wat-1.1,2007.mtsummit-papers.63,0,0.0610664,"om OPUS. We Test set II : A pair of test and reference sentences and context data that are articles including test sentences. The references were automatically extracted from English newswire sentences and manually selected. Therefore, the quality of the references of test set II is better than that of test set I. The statistics of JIJI Corpus are shown in Table 2. The definition of data use is shown in Table 3. Participants submit the translation results of one or more of the test data. The sentence pairs in each data are identified in the same manner as that for ASPEC using the method from (Utiyama and Isahara, 2007). 2.5 ALT and UCSY Corpus The parallel data for Myanmar-English translation tasks at WAT2021 consists of two corpora, the ALT corpus and UCSY corpus. • The ALT corpus is one part from the Asian Language Treebank (ALT) project (Riza et al., 2016), consisting of twenty thousand Myanmar-English parallel sentences from news articles. • The UCSY corpus (Yi Mon Shwe Sin and Khin Mar Soe, 2018) is constructed by the NLP Lab, University of Computer Studies, 3 http://opus.nlpl.eu/ http://www.statmt.org/wmt20/ 5 Software Domain Evaluation Splits 4 3 Task Use Training Test set I Japanese to English Test"
2021.wat-1.16,2020.coling-tutorials.3,0,0.0168009,"re f with the dimensionality of the sum of both feature vectors are then fed to the LSTM-based decoder. LSTM Decoder: In the proposed approach, the encoder module is not trainable, it only extracts the image features however the LSTM decoder is trainable. We used LSTM decoder using the image features for caption generation using greedy search approach (Soh). We used the cross-entropy loss during decoding (Yu et al., 2019). 149 3.3 Indic Multilingual Translation Sharing parameters across multiple languages, particularly low-resource Indic languages, results in gains in translation performance (Dabre et al., 2020). Motivated by this finding, we train neural MT models with shared parameters across multiple languages for the Indic multilingual translation task. We additionally apply transfer learning where we train a neural MT model in two phases (Kocmi and Bojar, 2018). The first phase consists of Figure 3: Architecture for Indic Multilingual translation. We show here the setup in which both the source and the target language tags are used. training a multilingual translation model on training pairs drawn from one of the following options: (a) any Indic language from the dataset as the source and corres"
2021.wat-1.16,W18-6325,1,0.8344,"We used LSTM decoder using the image features for caption generation using greedy search approach (Soh). We used the cross-entropy loss during decoding (Yu et al., 2019). 149 3.3 Indic Multilingual Translation Sharing parameters across multiple languages, particularly low-resource Indic languages, results in gains in translation performance (Dabre et al., 2020). Motivated by this finding, we train neural MT models with shared parameters across multiple languages for the Indic multilingual translation task. We additionally apply transfer learning where we train a neural MT model in two phases (Kocmi and Bojar, 2018). The first phase consists of Figure 3: Architecture for Indic Multilingual translation. We show here the setup in which both the source and the target language tags are used. training a multilingual translation model on training pairs drawn from one of the following options: (a) any Indic language from the dataset as the source and corresponding English target; (b) English as the source and any corresponding Indic language as the target; and (c) combination of (a) and (b), that is, the model is trained to enable translation from any Indic language to English and also English to any Indic lang"
2021.wat-1.16,D18-2012,0,0.0732595,"rimental details of the tasks we participated in. 3.1 EN-HI and EN-ML text-only translation For the HVG text-only translation track, we train a Transformer model (Vaswani et al., 2017) using the concatenation of IIT-B training data and HVG training data (see Table 1). Similar to the two-phase approach outlined in Section 3.3, we continue the training using only the HVG training data to obtain the final checkpoint. For the MVG text-only translation track, we train a Transformer model using only the MVG training data. For both EN-HI and EN-ML translation, we trained SentencePiece subword units (Kudo and Richardson, 2018) setting maximum vocabulary size to 8k. The vocabulary was learned jointly on the source and target sentences of HVG and IIT-B for EN-HI and of MVG for EN-ML. The number of encoder and decoder layers was set to 3 each; while the number of heads was set to 8. We have set the hidden size to 128, along with the dropout value of 0.1. We initialized the model parameters using Xavier initialization (Glorot and Bengio, 2010) and used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 5e−4 for optimizing model parameters. Gradient clipping was used to clip gradients greater than 1. The t"
2021.wat-1.16,E17-2035,0,0.0122238,"are shown in Table 1. Tokens Set Sentences English Hindi Malayalam Train 28930 143164 145448 107126 D-Test 998 4922 4978 3619 E-Test 1595 7853 7852 5689 C-Test 1400 8186 8639 6044 IITB Train 1.5 M 20.6 M 22.1 M – Table 1: Statistics of our data used in the English→Hindi and English→Malayalam Multimodal task: the number of sentences and tokens. Task 2: English→Malayalam Multimodal Translation For this task, the organizers provided MalayalamVisualGenome 1.0 dataset7 (MVG for short). MVG is an extension of the HVG dataset for supporting Malayalam, which belongs to the Dravidian language family (Kumar et al., 2017). The dataset size and images are the same as HVG. While HVG contains bilingual (English and Hindi) segments, MVG contains bilingual (English and Malayalam) segments, with the English shared across HVG and MVG, see Table 1. 5 https://lindat.mff.cuni.cz/repository/ xmlui/handle/11234/1-3267 6 http://www.cfilt.iitb.ac.in/iitb_parallel/ 7 https://lindat.mff.cuni.cz/repository/ xmlui/handle/11234/1-3533 Task 3: Indic Multilingual Translation For this task, the organizers provided a training corpus that comprises in total 11 million sentence pairs collected from several corpora. The evaluation (dev"
2021.wat-1.16,P16-1168,0,0.0542321,"Missing"
2021.wat-1.16,D19-5224,0,0.0287504,"y the conclusion in Section 5. 2 https://ufal.mff.cuni. cz/malayalam-visual-genome/ wat2021-english-malayalam-multi 3 http://lotus.kuee.kyoto-u.ac.jp/WAT/ indic-multilingual/ 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2021/index.html 146 Proceedings of the 8th Workshop on Asian Translation, pages 146–154 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics 2 Dataset We have used the oﬀicial datasets provided by the WAT2021 organizers for the tasks. Task 1: English→Hindi Multimodal Translation For this task, the organizers provided HindiVisualGenome 1.1 (Parida et al., 2019)5 dataset (HVG for short). The training part consists of 29k English and Hindi short captions of rectangular areas in photos of various scenes and it is complemented by three test sets: development (D-Test), evaluation (E-Test) and challenge test set (C-Test). Our WAT submissions were for E-Test (denoted “EV” in WAT oﬀicial tables) and CTest (denoted “CH” in WAT tables). Additionally, we used the IITB Corpus6 which is supposedly the largest publicly available EnglishHindi parallel corpus (Kunchukuttan et al., 2017). This corpus contains 1.59 million parallel segments and it was found very effe"
2021.wat-1.16,W18-6319,0,0.0605898,"B training data. The drop in perplexity midway for EN-HI is because of the change of training data from IIT-B + HVG to only HVG after the first phase of the training converges. Upon evaluating the translations using the development set, we obtained the following scores for Hindi translations. The BLEU score was 46.7 upon using HVG + IIT-B training data. In comparison, we observed that the BLEU score was 39.9 upon using only the HVG training data (without IIT-B training data). For Malayalam translations, the BLEU score on the development set was 31.3. BLEU scores were computed using sacreBLEU (Post, 2018). 3.2 Image Caption Generation This task in WAT 2021 is formulated as generating a caption in Hindi and Malayalam for a specific region in the given image. Most existing research in the area of image captioning refers to generating a textual description for the entire image (Yang and Okazaki, 2020; Yang et al., 2017; Lindh et al., 2018; Staniūtė and Šešok, 2019; Miyazaki and Shimizu, 2016; Wu et al., 2017). However, a naive approach of using only a specified region (as defined by the rectangular bounding box) as an input to the generic image caption generation system often does not yield meani"
2021.wat-1.16,2020.coling-main.176,0,0.180753,"tions. The BLEU score was 46.7 upon using HVG + IIT-B training data. In comparison, we observed that the BLEU score was 39.9 upon using only the HVG training data (without IIT-B training data). For Malayalam translations, the BLEU score on the development set was 31.3. BLEU scores were computed using sacreBLEU (Post, 2018). 3.2 Image Caption Generation This task in WAT 2021 is formulated as generating a caption in Hindi and Malayalam for a specific region in the given image. Most existing research in the area of image captioning refers to generating a textual description for the entire image (Yang and Okazaki, 2020; Yang et al., 2017; Lindh et al., 2018; Staniūtė and Šešok, 2019; Miyazaki and Shimizu, 2016; Wu et al., 2017). However, a naive approach of using only a specified region (as defined by the rectangular bounding box) as an input to the generic image caption generation system often does not yield meaningful results. When a small region of the image with few objects is considered for captioning, it lacks the context English Text: The snow is white. Hindi Text: बफर सफेद है Malayalam Text: മഞഞ െവളുതതതാണ Gloss: Snow is white Figure 1: Sample image with speciﬁc region and its description for ca"
berka-etal-2012-automatic,niessen-etal-2000-evaluation,0,\N,Missing
berka-etal-2012-automatic,C08-1141,0,\N,Missing
berka-etal-2012-automatic,J03-1002,0,\N,Missing
berka-etal-2012-automatic,fishel-etal-2012-terra,1,\N,Missing
berka-etal-2012-automatic,vilar-etal-2006-error,0,\N,Missing
berka-etal-2012-automatic,W11-2107,0,\N,Missing
bojar-etal-2008-czeng,steinberger-etal-2006-jrc,0,\N,Missing
bojar-etal-2008-czeng,P02-1040,0,\N,Missing
bojar-etal-2008-czeng,P07-2045,1,\N,Missing
bojar-etal-2008-czeng,W04-3250,0,\N,Missing
bojar-etal-2010-data,P02-1040,0,\N,Missing
bojar-etal-2010-data,P07-2045,1,\N,Missing
bojar-etal-2010-data,N03-1017,0,\N,Missing
bojar-etal-2010-data,J03-1002,0,\N,Missing
bojar-etal-2010-data,J07-2003,0,\N,Missing
bojar-etal-2010-data,P03-1021,0,\N,Missing
bojar-etal-2010-evaluating,steinberger-etal-2006-jrc,0,\N,Missing
bojar-etal-2010-evaluating,J03-1002,0,\N,Missing
bojar-etal-2012-joy,bojar-etal-2010-evaluating,1,\N,Missing
bojar-etal-2012-joy,C00-2163,0,\N,Missing
bojar-etal-2012-joy,W07-1709,0,\N,Missing
bojar-etal-2012-joy,P02-1040,0,\N,Missing
bojar-etal-2012-joy,H05-1066,0,\N,Missing
bojar-etal-2012-joy,P07-2045,1,\N,Missing
bojar-etal-2012-joy,W10-1703,0,\N,Missing
bojar-etal-2012-joy,W09-3939,1,\N,Missing
bojar-etal-2014-hindencorp,bojar-etal-2010-data,1,\N,Missing
bojar-etal-2014-hindencorp,W12-3152,0,\N,Missing
bojar-etal-2014-hindencorp,W11-4624,0,\N,Missing
bojar-etal-2014-hindencorp,W07-1709,0,\N,Missing
bojar-etal-2014-hindencorp,C10-2010,0,\N,Missing
bojar-etal-2014-hindencorp,majlis-zabokrtsky-2012-language,0,\N,Missing
bojar-etal-2014-hindencorp,bojar-etal-2012-joy,1,\N,Missing
bojar-prokopova-2006-czech,W05-0809,0,\N,Missing
bojar-prokopova-2006-czech,W03-0301,0,\N,Missing
bojar-prokopova-2006-czech,J03-1002,0,\N,Missing
bojar-prokopova-2006-czech,W05-0806,0,\N,Missing
D16-1134,W13-2322,0,0.335895,"Missing"
D16-1134,W05-0909,0,0.252262,"Missing"
D16-1134,W13-2203,1,0.896683,"Missing"
D16-1134,W12-4204,1,0.908186,"Missing"
D16-1134,W11-2101,1,0.914645,"Missing"
D16-1134,W14-4005,0,0.0250442,"Missing"
D16-1134,P16-2013,0,0.0371161,"Missing"
D16-1134,W07-0738,0,0.0835328,"Missing"
D16-1134,N15-1124,0,0.125815,"Missing"
D16-1134,P15-1174,0,0.0311734,"Missing"
D16-1134,P07-2045,1,0.0114599,"Missing"
D16-1134,W05-0904,0,0.093592,"Missing"
D16-1134,W11-1002,0,0.0373567,"Missing"
D16-1134,lo-wu-2014-reliability,0,0.0405899,"Missing"
D16-1134,oepen-lonning-2006-discriminant,0,0.0919776,"Missing"
D16-1134,2006.amta-papers.25,0,0.300368,"Missing"
D16-1134,W15-3502,1,0.478779,"Missing"
D16-1134,P02-1040,0,\N,Missing
D19-5201,E06-1031,0,0.0437063,"ion web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 http://lotus.kuee.kyoto-u.ac.jp/WAT/ km-en-d"
D19-5201,W16-4601,1,0.763733,"Missing"
D19-5201,W19-6613,1,0.928331,"Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation results for each particJaRuNC Corpus For the Russian↔Japanese task we asked participants to use the JaRuNC corpus5 (Imankulova et al., 2019) which belongs to the news commentary domain. This dataset was manually aligned and cleaned and is trilingual. It can be used to evaluate Russian↔English 6 http://www.phontron.com/kftt/ https://datarepository.wolframcloud.com/ resources/Japanese-English-Subtitle-Corpus 8 https://wit3.fbk.eu/ 9 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 10 https://cms.unov.org/UNCorpus/ 11 https://translate.yandex.ru/corpus?lang=en 12 http://lotus.kuee.kyoto-u.ac.jp/WAT/ News-Commentary/news-commentary-v14.en-ru. filtered.tar.gz 7 4 http://ufal.mff.cuni.cz/~ramasamy/parallel/ html/ 5 https://github.com/aizhanti/JaR"
D19-5201,W17-5701,1,0.779618,"Missing"
D19-5201,D10-1092,0,0.105912,"t of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for each task were calculated using the corresponding reference translations. The automatic evaluation system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provid"
D19-5201,W14-7001,1,0.794275,"t 400 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. WAT receives submissions at any time; i.e., there is no submission deadline of translation results w.r.t automatic evaluation of translation quality. Introduction The Workshop on Asian Translation (WAT) is an open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2018 (Nakazawa et al., 2014, 2015, 2016, 2017, 2018), WAT2019 brings together machine 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/ 3 https://ufal.mff.cuni.cz/hindi-visual-genome/ wat-2019-multimodal-task 1 One paper was withdrawn post acceptance and hence only 6 papers will be in the proceedings. 1 Proceedings of the 6th Workshop on Asian Translation, pages 1–35 Hong Kong, China, November 4, 2019. ©2019 Association for Computational Linguistics Lang JE JC Train 3,008,500 672,315 Dev 1,790 2,090 DevTest 1,784 2,148 Lang zh-ja ko-ja en-ja Test 1,812 2,107 Lang zh-ja ko-ja en-ja Table 1: Statistics for ASPEC Dataset"
D19-5201,W15-5001,1,0.855752,"Missing"
D19-5201,P17-4012,0,0.147541,"MT RBMT Other Other ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ja-hi ✓ ✓ ✓ ✓ EnTam ta-en en-ta ✓ ✓ IITB en-hi hi-ja ✓ ✓ ✓ ✓ TDDC ja-en ✓ hi-en ✓ ✓ NewsCommentary ru-ja ja-ru JIJI ja-en en-ja ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ALT en-my km-en ✓ ✓ Multimodal en-hi ✓ my-en ✓ ✓ en-km ✓ model) for Chinese segmentation. • The Moses toolkit for English and Indonesian tokenization. • Mecab-ko16 for Korean segmentation. • Indic NLP Library17 for Indic language segmentation. • The tools included in the ALT corpus for Myanmar and Khmer segmentation. • subword-nmt18 for all languages. 3.3.1 NMT with Attention We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems of NMT with attention (System ID: NMT). We used the following OpenNMT configuration. • • • • • • • • When we built BPE-codes, we merged source and target sentences and we used 100,000 for s option. We used 10 for vocabulary-threshold when subword-nmt applied BPE. 3.2.2 The default values were used for the other system parameters. For EnTam, News Commentary • The Moses toolkit for English and Russian only for the News Commentary data. 3.3.2 Transformer (Tensor2Tensor) For the News Commentary and English↔Tamil tasks, we used tensor2tensor’s20 im"
D19-5201,W18-1819,0,0.026575,"multilingual model. As for the English↔Tamil task, we train separate baseline models for each translation direction with 32,000 separate sub-word vocabularies. • Mecab19 for Japanese segmentation. • The EnTam corpus is not tokenized by any external toolkits. • Both corpora are further processed by tensor2tensor’s internal pre/postprocessing which includes sub-word segmentation. 3.2.3 For Multi-Modal Task • Hindi Visual Genome comes untokenized and we did not use or recommend any specific external tokenizer. 3.3.3 Transformer (OpenNMT-py) For the Multimodal task, we used the Transformer model (Vaswani et al., 2018) as implemented in OpenNMT-py (Klein et al., 2017) and used the “base” model with default parameters for the multi-modal task baseline. We have generated the vocabulary of 32k subword types jointly for both the source and target languages. The vocabulary is shared between the encoder and decoder. • The standard OpenNMT-py sub-word segmentation was used for pre/postprocessing for the baseline system and each participant used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_freque"
D19-5201,P11-2093,0,0.0158555,"tion system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each submission: • Human Evaluation: whether or not they submit the results for human evaluation; Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model23 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.24 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.25 For Korean segmentation, we used mecab-ko.26 For Myanmar and Khmer segmentations, we used myseg.py27 and kmseg.py28 . For English and Russian tokenizations, we used tokenizer.perl29 in the Moses toolkit. For Hindi and Tamil tokenizations, we used Indic NLP Library.30 The detailed procedu"
D19-5201,P02-1040,0,0.106619,"used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 Baseline NMT Methods We used the following NMT with attention for most of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for e"
D19-5201,W16-2342,0,0.0144454,"on results that participants permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyot"
D19-5201,J82-2005,0,0.731235,"Missing"
D19-5201,D19-5224,0,0.0224104,"Missing"
D19-5201,W15-3049,0,0.0158524,"s permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-da"
D19-5201,W12-5611,1,0.827974,"/ 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k translation quality as well but this is beyond the scope of this years sub-task. Refer to Table 10 for the statistics of the in-domain parallel corpora. In addition we encouraged the participants to use out-of-domain parallel corpora from various sources such as KFTT,6 JESC,7 TED,8 ASPEC,9 UN,10 Yandex11 and Russian↔English news-commentary corpus.12 EnTam Corpus For Tamil↔English translation task we asked the participants to use the publicly available EnTam mixed domain corpus4 (Ramasamy et al., 2012). This corpus contains training, development and test sentences mostly from the news-domain. The other domains are Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation r"
D19-5201,2006.amta-papers.25,0,0.0857626,"hed are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 htt"
D19-5201,2007.mtsummit-papers.63,0,0.678495,",000 2,000 Dev 2,000 2,000 2,000 Table 2: Statistics for JPC • Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Korean↔Japanese language pairs. In the future, we will add more Asian languages such as Vietnamese, Thai and so on. 2 Train 1,000,000 1,000,000 1,000,000 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million Japanese-English scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from Utiyama and Isahara (2007). Each sentence pair is accompanied by a similarity score calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the 2.2 JPC JPO Patent Corpus (JPC) for the patent tasks was constructed by the Japan Patent Office (JPO) in collaboration with NICT. The corpus consists of Chinese-Japanese, KoreanJapanese and English-Japanese patent descriptions whose International Patent Classi2 Disclosure Period 2016-01-01 to 2017-12-31 2018-01-01 to 2018-06-30 Train 1,089,346 (614,817) 314,649 (218,495) Dev Texts Items 1,153 2,"
D19-5223,W17-4757,1,0.844667,"ed the step 165K where E-Test reached its maximum score. In other words, the choice of the model for the C-Test was based on E-Test serving as a validation set. 4 on C-Test). Manual judgments also indicate that our translations are better than those of the best multi-modal system, but here the comparison has to be taken with a grain of salt. The root of the trouble is that the manual evaluation for the text-only and multimodal tracks ran separately. While the underlying method (Direct Assessment, DA, Graham et al., 2013) in principle scores sentences in absolute terms, it has been observed by Bojar et al. (2017) that DA scores from independent runs are not reliably comparable. We indicate this by the additional horizontal lines in Table 2. Figure 2 illustrates of our translation output. Official Results 5 Discussion We report the official automatic as well as manual evaluation results of our models for the evaluation and challenge test dataset here in Table 2. All the scores are available on the WAT 2019 website3 and in the WAT overview paper (Nakazawa et al., 2019). According to both automatic and manual scores, our submissions were the best in the text-only task (MM**TEXT), see the tables in Nakaza"
D19-5223,N19-1422,0,0.0261374,"ion in Section 6. Introduction In recent years, significant research has been done to address problems that require joint modelling of language and vision (Specia et al., 2016). The popular applications involving Natural Language Processing (NLP) and Computer Vision (CV) include image description generation (Bernardi et al., 2016), video captioning (Li et al., 2019), or visual question answering (Antol et al., 2015). In the past few decades, multi-modality has received critical attention in translation studies, although the benefit of visual modality in machine translation is still in debate (Caglayan et al., 2019). The main motivation in multi-modal research in machine translation is the intuition that information from other modalities could help to find the correct sense of ambiguous words in the source sentence, which could potentially lead to more accurate translations (Lala and Specia, 2018). ∗ Sentences 28932 1.4 M 998 1595 1400 2 Dataset The official training set was provided by the task organizers: Hindi Visual Genome (HVG for short, Parida et al., 2019a,b). The training part consists of 29k English and Hindi short captions of rectangular areas in photos of various scenes and it is complemented"
D19-5223,W18-3405,0,0.0191298,"aluation and challenge test sets. Our submission tops in its track among the competitors in terms of both automatic and manual evaluation. Based on automatic scores, our text-only submission also outperforms systems that consider visual information in the “multimodal translation” task. 1 Tokens English Hindi 143178 136722 20.6 M 22.1 M 4922 4695 7852 7535 8185 8665 Table 1: Statistics of our data: the number of sentences and tokens. Despite the lack of multi-modal datasets, there is a visible interest in using image features even for machine translation for lowresource language. For instance, Chowdhury et al. (2018) train a multi-modal neural MT system for Hindi→English using synthetic parallel data only. In this system description paper, we explain how we used additional resources in the textonly track of WAT 2019 Multi-Modal Translation Task. Section 2 describes the datasets used in our experiment. Section 3 presents the model and experimental setups used in our approach. Section 4 provides the official evaluation results of WAT 2019 followed by the conclusion in Section 6. Introduction In recent years, significant research has been done to address problems that require joint modelling of language and"
D19-5223,W13-2305,0,0.0330876,"ns of C-Test for the WAT official evaluation, we already knew the full training run and selected the step 165K where E-Test reached its maximum score. In other words, the choice of the model for the C-Test was based on E-Test serving as a validation set. 4 on C-Test). Manual judgments also indicate that our translations are better than those of the best multi-modal system, but here the comparison has to be taken with a grain of salt. The root of the trouble is that the manual evaluation for the text-only and multimodal tracks ran separately. While the underlying method (Direct Assessment, DA, Graham et al., 2013) in principle scores sentences in absolute terms, it has been observed by Bojar et al. (2017) that DA scores from independent runs are not reliably comparable. We indicate this by the additional horizontal lines in Table 2. Figure 2 illustrates of our translation output. Official Results 5 Discussion We report the official automatic as well as manual evaluation results of our models for the evaluation and challenge test dataset here in Table 2. All the scores are available on the WAT 2019 website3 and in the WAT overview paper (Nakazawa et al., 2019). According to both automatic and manual sco"
D19-5223,P17-4012,0,0.0506253,"ttp://opennmt.net/OpenNMT-py/quickstart. html 2 https://nvidia.github.io/OpenSeq2Seq/html/ api-docs/optimizers.html Additionally, we used the IITB Corpus (Kunchukuttan et al., 2017) which is supposedly the largest publicly available EnglishHindi parallel corpus. This corpus contains 1.49 million parallel segments and it was found very effective for English-Hindi translation (Parida and Bojar, 2018). The statistics of the datasets are shown in Table 1. 3 Experiments We focussed only on the text translation task. We used the Transformer model (Vaswani et al., 2018) as implemented in OpenNMT-py (Klein et al., 2017).1 3.1 Tokenization and Vocabulary Subword units were constructed using the word pieces algorithm (Johnson et al., 2017). Tokenization is handled automatically as part of the pre-processing pipeline of word pieces. We generated the vocabulary of 32k subword types jointly for both the source and target languages. The vocabulary is shared between the encoder and decoder. 3.2 Training 176 System and WAT Task Label Our MMEVTEXTen-hi Best competitor in MMEVMMen-hi Our MMCHTEXTen-hi Best competitor in MMCHMMen-hi WAT BLEU 41.32 40.55 30.94 20.37 Our sacreBLEU 41.1 – 30.7 – Our Moses BLEU 52.18 – 40."
D19-5223,kocmi-bojar-2017-curriculum,1,0.832729,"rtheless good, indicating that reasonably clean data and baseline settings of the Transformer architecture deliver good translations. The specifics of the task have to be taken into account. The “sentences” in Hindi Visual Genome are quite short, only 4.7 Hindi and 4.9 English tokens per sentence. This is substantially less than the IITB corpus where the average number of tokens is 15.8 (Hindi) and 14.7 (English). With IITB mixed in the training data, the model gets a significant advantage, not only because of the better coverage of words and phrases but also due to the length. As observed by Kocmi and Bojar (2017) and Popel and Bojar (2018), NMT models struggle to produce outputs longer than the training data was. Our situation is the reverse, so our model “operates within its comfortable zone”. 3 http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation/ 177 English Input: Translated Output: Gloss: gold religious cross on top of golden ball सोने क गद के शीष पर वण धा मक ॉस . Gold religious cross on top of golden ball English Input: Translated Output: Gloss: a blue wall beside tennis court टे नस कोट के पास एक नीली दीवार ह । Blue wall near the tennis court English Input: Translated Output: Gloss: the tennis court"
D19-5223,W16-2346,0,0.0493525,"Missing"
D19-5223,W18-1819,0,0.0606578,"ining itself is not affected by them in any way. 1 http://opennmt.net/OpenNMT-py/quickstart. html 2 https://nvidia.github.io/OpenSeq2Seq/html/ api-docs/optimizers.html Additionally, we used the IITB Corpus (Kunchukuttan et al., 2017) which is supposedly the largest publicly available EnglishHindi parallel corpus. This corpus contains 1.49 million parallel segments and it was found very effective for English-Hindi translation (Parida and Bojar, 2018). The statistics of the datasets are shown in Table 1. 3 Experiments We focussed only on the text translation task. We used the Transformer model (Vaswani et al., 2018) as implemented in OpenNMT-py (Klein et al., 2017).1 3.1 Tokenization and Vocabulary Subword units were constructed using the word pieces algorithm (Johnson et al., 2017). Tokenization is handled automatically as part of the pre-processing pipeline of word pieces. We generated the vocabulary of 32k subword types jointly for both the source and target languages. The vocabulary is shared between the encoder and decoder. 3.2 Training 176 System and WAT Task Label Our MMEVTEXTen-hi Best competitor in MMEVMMen-hi Our MMCHTEXTen-hi Best competitor in MMCHMMen-hi WAT BLEU 41.32 40.55 30.94 20.37 Our"
D19-5223,L18-1602,0,0.0239275,"escription generation (Bernardi et al., 2016), video captioning (Li et al., 2019), or visual question answering (Antol et al., 2015). In the past few decades, multi-modality has received critical attention in translation studies, although the benefit of visual modality in machine translation is still in debate (Caglayan et al., 2019). The main motivation in multi-modal research in machine translation is the intuition that information from other modalities could help to find the correct sense of ambiguous words in the source sentence, which could potentially lead to more accurate translations (Lala and Specia, 2018). ∗ Sentences 28932 1.4 M 998 1595 1400 2 Dataset The official training set was provided by the task organizers: Hindi Visual Genome (HVG for short, Parida et al., 2019a,b). The training part consists of 29k English and Hindi short captions of rectangular areas in photos of various scenes and it is complemented by three test sets: development (D-Test), evaluation (E-Test) and challenge test set (C-Test). We did not make any use of the images. Our WAT submissions were for E-Test (denoted “EV” in WAT official tables) and C-Test (denoted “CH” in WAT tables). Corresponding author 175 Proceedings o"
D19-5223,P02-1040,0,0.103612,"d followed the standard “Noam” learning rate decay,2 see Vaswani et al. (2017) or Popel and Bojar (2018) for more details. Our starting learning rate was 0.2 and we used 8000 warm up steps. We ran only one training run. We concatenated HVG and IITB training data and shuffled it at the level of sentences. We let the model train for up to 200K steps, interrupted a few times due to GPU queueing limitations of our cluster. Following the recommendation of Popel and Bojar (2018), we present the full learning curves on D-Test, ETest and C-Test in Figure 1. We observed a huge difference between BLEU (Papineni et al., 2002) scores as implemented in the Moses toolkit (Koehn et al., 2007) and the newer implementation in sacreBLEU (Post, 2018). The discrepancy is very likely caused by different tokenization but the best choice in terms of linguistic plausibility still has to be made. In Figure 1, we show both implementations and see that the Moses implementation gives scores higher by 10 (!) points absolute. More importantly, it is a little less peaked, which we see as evidence for better robustness and thus hopefully the linguistic adequacy. All of the test sets (D-, E- and C-Test) are independent of the training"
D19-5223,D19-5224,0,0.119661,"Missing"
D19-5223,J82-2005,0,0.698832,"Missing"
D19-5223,W18-6319,0,0.017101,"rting learning rate was 0.2 and we used 8000 warm up steps. We ran only one training run. We concatenated HVG and IITB training data and shuffled it at the level of sentences. We let the model train for up to 200K steps, interrupted a few times due to GPU queueing limitations of our cluster. Following the recommendation of Popel and Bojar (2018), we present the full learning curves on D-Test, ETest and C-Test in Figure 1. We observed a huge difference between BLEU (Papineni et al., 2002) scores as implemented in the Moses toolkit (Koehn et al., 2007) and the newer implementation in sacreBLEU (Post, 2018). The discrepancy is very likely caused by different tokenization but the best choice in terms of linguistic plausibility still has to be made. In Figure 1, we show both implementations and see that the Moses implementation gives scores higher by 10 (!) points absolute. More importantly, it is a little less peaked, which we see as evidence for better robustness and thus hopefully the linguistic adequacy. All of the test sets (D-, E- and C-Test) are independent of the training data and the training itself is not affected by them in any way. 1 http://opennmt.net/OpenNMT-py/quickstart. html 2 htt"
E17-1087,baker-etal-2002-emille,0,0.0271386,"ach direction. The main model was trained for over 530,000 steps (each step is the processing of one batch of inputs) on a single core of the GeForce GTX Titan Z GPU. The training took around 5 days. The stopping criterion for the training was the error on a development set. 4 Tatoeba6 is a collection of simple sentences for language learners. Tatoeba contains sentences in 307 languages, but for most languages it has only a few hundred sentences. Leipzig corpora collection (Quasthoff et al., 2006) covers 220 languages with newspaper text and other various texts collected from the web. EMILLE (Baker et al., 2002) contains texts in 14 Indian languages and English. Haitian Creole training data (Callison-Burch et al., 2011) were prepared by the organizers of WMT11 shared task on machine translation of SMS messages sent to an emergency hotline in the aftermath of the 2010 Haitian earthquake. We used only the official documents from the training data, not the actual SMS messages because they contained a lot of noise. Training Data Our goal is to develop an off-the-shelf language recognizer, with no need for retraining by the user and covering as many languages as possible. Finding suitable training data is"
E17-1087,N10-1027,0,0.0284133,"most 10M characters in the training set, the second group was capped at 5M characters and the third group was allowed only 1M characters per language at most.7 In total, our final training set includes 131 + 1 (HTML) languages, see Figure 2. We divide the corpus into non-overlapping training, development and test sections. We re5 Monolingual Language Identification Most of related research is focused on monolingual language identification, i.e. recognizing the single language of an input document. We compare our method in this setting with several other algorithms on the dataset presented by Baldwin and Lui (2010). The dataset consists of 3 different test sets, each containing a different number of languages, styles and document lengths collected from different sources, see Table 1 for details: EuroGov contains texts in Western European languages from European government resources. TCL was extracted by the Thai Computational Linguistics Laboratory in 2005 from online news sources and the test set also contains multiple file encodings. Since our method assumes Unicode input, we converted TCL to Unicode encoding. 7 Higher-quality sources such as Tatoeba are generally smaller and since we mixed the source"
E17-1087,W11-2103,0,0.013384,"batch of inputs) on a single core of the GeForce GTX Titan Z GPU. The training took around 5 days. The stopping criterion for the training was the error on a development set. 4 Tatoeba6 is a collection of simple sentences for language learners. Tatoeba contains sentences in 307 languages, but for most languages it has only a few hundred sentences. Leipzig corpora collection (Quasthoff et al., 2006) covers 220 languages with newspaper text and other various texts collected from the web. EMILLE (Baker et al., 2002) contains texts in 14 Indian languages and English. Haitian Creole training data (Callison-Burch et al., 2011) were prepared by the organizers of WMT11 shared task on machine translation of SMS messages sent to an emergency hotline in the aftermath of the 2010 Haitian earthquake. We used only the official documents from the training data, not the actual SMS messages because they contained a lot of noise. Training Data Our goal is to develop an off-the-shelf language recognizer, with no need for retraining by the user and covering as many languages as possible. Finding suitable training data is thus an important part 3 5 We set the learning rate to 0.0001 and train with the batch size of 64 windows. 4"
E17-1087,D14-1179,0,0.00943817,"Missing"
E17-1087,hughes-etal-2006-reconsidering,0,0.341411,"ally data generated by web users. As more people get access to the web, more languages and dialects start to appear and need to be processed. In order to be able to use such data for further natural language processing (NLP) tasks, we need to know in which languages they were written. Language identification is thus a key component for both building various NLP resources from the web and also for running many web services. Techniques of language identification can rely on handcrafted rules, usually of high precision but 2 Related Work Of the many possible approaches to language identification Hughes et al. (2006), character ngram statistics are among the most popular ones. Cavnar et al. (1994) were probably the first; they used the 300 most frequent character n-grams (with n ranging from 1 to 5, as is also typically used in other works). All the n-gram-based ap927 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 927–936, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics proaches differ primarily in the calculation of the distance between the n-gram profile of the training and test"
E17-1087,W14-3907,0,0.0881762,"Missing"
E17-1087,W16-6212,0,0.0534589,"oCorasick string matching. To our knowledge, and also according to the survey by Garg et al. (2014), neural networks have not been used often for language identification so far. One exception is Al-Dubaee et al. (2010), who combine a feed-forward network classifier with wavelet transforms of feature vectors to identify English and Arabic from the Unicode representation of words, sentences or whole documents. The benefit of NN in this setting is not very clear to us because English and Arabic can be distinguished by the script. During writing of this paper, we have found a new pre-print paper (Jaech et al., 2016) which handles language identification with NN. Specifically, they employ Convolutional Neural Networks followed by Recurrent Neural Networks. Their approach labels text on the word level, which is problematic in languages without clear word delimiters. In comparison with our model, they need to pre-process the data and break long words into smaller chunks, whereas we simply use text without any preprocessing. In practice, several tools are often used at once, 1 with some form of majority voting. For example, Twitter internal language detector uses their inhouse tool along with CLD2 and Langid"
E17-1087,N13-1131,0,0.207075,"r models in this criterion (Fµ of .965) and so does our nonadapted version, reaching Fµ of .941. 6.2 not present in the test set, the score gets on par with the adapted version, see the line labelled “restricted” in Table 5. 7 Text Partitioning Figure 3 illustrates the behaviour of our model on text with mixed languages. We have selected very short (50–130 characters) and challenging segments where the languages mostly share the same script. Finding the boundary between languages written in different scripts is quite easy, as illustrated by the first example. Only too late, we discovered that King and Abney (2013) provide a test set for word-level identification for 30 languages. We thus have to leave the evaluation of our model on this dataset for future. WikipediaMulti WikipediaMulti (Lui et al., 2014) is a dataset of artificially prepared multilingual documents, mixed from monolingual Wikipedia articles from 44 languages. Each of the artificial documents contains texts in 1 ≤ k ≤ 5 randomly selected languages. The average document length is 5500 bytes. The training set consists of 5000 monolingual documents, the development set consists of 5000 multilingual documents and test set consists of 1000 do"
E17-1087,I11-1062,0,0.0531056,"Missing"
E17-1087,P12-3005,0,0.440623,"in the tested document of the most frequent n-gram types extracted from training documents for each language. The highestscoring language is then returned. Hughes et al. (2006) mention a number of freely available tools at that time. Since then, one aspect of the tools became also important: the number of languages covered. The language identification tool CLD21 by Google detects 80 languages and uses a Naive Bayes classifier, treating specifically unambiguous scripts such as Greek and using either character unigrams (Han and similar scripts) or fourgrams. Another popular tool is Langid.py by Lui and Baldwin (2012), covering 97 languages out of the box. Langid.py relies on Naive Bayes classifier with a multinominal event model and mixture of byte n-grams for training. The tool includes tokenization and fast feature extraction using AhoCorasick string matching. To our knowledge, and also according to the survey by Garg et al. (2014), neural networks have not been used often for language identification so far. One exception is Al-Dubaee et al. (2010), who combine a feed-forward network classifier with wavelet transforms of feature vectors to identify English and Arabic from the Unicode representation of w"
E17-1087,Q14-1003,0,0.399918,"only references to the actual tweets and most of them are no longer available. We thus have to rely on our own test set, as described in Section 4. Results on short texts are reported in Table 3. The two other systems, Langid.py and CLD2 cover fewer languages and they were trained on texts unrelated to our collection of data. It is there9 We should mention that LangDetect used EuroGov as a validation set, so its score on this test set is not reliable. 10 https://blog.twitter.com/2015/ evaluating-language-identificationperformance 932 System VRL (2010) * ALTW2010 winner * SEGLANG * LINGUINI * Lui et al. (2014) * Lui et al. (2014) our retrain Our model Our model Training set ALTW2010 ALTW2010 ALTW2010 - mono ALTW2010 - mono ALTW2010 - mono ALTW2010 - mono ALTW2010 - mono Our dataset PM .497 .718 .801 .616 .753 .768 .819 .709 RM .467 .703 .810 .535 .771 .716 .764 .714 FM .464 .699 .784 .513 .748 .724 .779 .695 Pµ .833 .932 .866 .713 .945 .968 .966 .941 Rµ .826 .931 .946 .688 .922 .896 .964 .941 Fµ .829 .932 .905 .700 .933 .931 .965 .941 Table 4: Results of multilingual language identification on the ALTW2010 test set. * As reported by Lui et al. (2014) System SEGLANG * LINGUINI * Lui et al. (2014) *"
E17-1087,majlis-zabokrtsky-2012-language,0,0.070618,"Missing"
E17-1087,quasthoff-etal-2006-corpus,0,0.0531438,"same size. We set e, the size of the embedding layer, to 200. The BiRNN uses a single hidden layer of 500 GRU cells for each direction. The main model was trained for over 530,000 steps (each step is the processing of one batch of inputs) on a single core of the GeForce GTX Titan Z GPU. The training took around 5 days. The stopping criterion for the training was the error on a development set. 4 Tatoeba6 is a collection of simple sentences for language learners. Tatoeba contains sentences in 307 languages, but for most languages it has only a few hundred sentences. Leipzig corpora collection (Quasthoff et al., 2006) covers 220 languages with newspaper text and other various texts collected from the web. EMILLE (Baker et al., 2002) contains texts in 14 Indian languages and English. Haitian Creole training data (Callison-Burch et al., 2011) were prepared by the organizers of WMT11 shared task on machine translation of SMS messages sent to an emergency hotline in the aftermath of the 2010 Haitian earthquake. We used only the official documents from the training data, not the actual SMS messages because they contained a lot of noise. Training Data Our goal is to develop an off-the-shelf language recognizer,"
E17-2059,N12-1047,0,0.0199491,"ne. We choose English→Czech as a task that is representative for machine translation from a morphologically underspecified language into a morphologically rich language. 5.1 11.8 12.7 50K pre-pruned by applying a minimum score threshold of 0.0001 on the source-to-target phrase translation probability, and the decoder loads a maximum of 100 best translation options per distinct source side. We use cube pruning in decoding. Pop limit and stack limit for cube pruning are set to 1000 for tuning and to 5000 for testing. The distortion limit is 6. Weights are tuned on newstest2013 with k-best MIRA (Cherry and Foster, 2012) over 200-best lists for 25 iterations. Translation quality is measured in B LEU (Papineni et al., 2002) on three different test sets, newstest2014, newstest2015, and newstest2016.3 Our training data amounts to around 50 million bilingual sentences overall, but we conduct sets of experiments with systems trained using different fractions of this data (50K, 500K, 5M, 50M). Whereas English→Czech has good coverage in terms of training corpora, we simulate lowand medium-resource conditions for the purpose of drawing more general conclusions. Irrespective of this, we utilize the same large LMs in a"
E17-2059,N13-1001,1,0.853824,"s using 5M training sentence pairs. Table 6: English→Czech experimental results using 50M training sentence pairs. In our baseline systems, we already draw on lemmas and morphosyntactic tags as factors on the target side, in addition to word surface forms.1 The additional target-side factors allow us to integrate features that independently model word sense (in terms of the lemma) and morphological attributes (in terms of the morphosyntactic tag). All our translation engines (cf. Section 5) incorporate ngram LMs over lemmas and over morphosyntactic tags, and an operation sequence model (OSM) (Durrani et al., 2013) with lemmas on the target side. These models counteract sparsity, and where models over surface forms fail for unseen variants, they still assign scores which are based on reliable probability estimates. When enhancing a system with synthesized phrase table entries, we add further features. Since the usual phrase translation and lexical translation log-probabilities over surface forms cannot be estimated for unseen morphological variants, but all new variants are generated from existing lemmas, we utilize the corresponding log-probabilities over target lemmas. Those can be extracted from the"
E17-2059,W10-1705,1,0.905261,"low-resource conditions, but the problem persists even with larger amounts of parallel training data. When translating into the morphologically rich language, the system fails at producing the unseen morphological variants, leading to major translation errors. Consider the Czech example in Table 1. A small parallel corpus of 50K English-Czech sentences contains only a single variant of the morphological 2 Related Work Translation into morphologically rich languages is often tackled through “two-step”, i.e., separate modules for morphological prediction and generation (Toutanova et al., 2008; Bojar and Kos, 2010; 369 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 369–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Fraser et al., 2012; Burlot et al., 2016). An important problem is that lexical choice (of the lemma) is carried out in a separate step from morphological prediction. Factored machine translation with separate translation and generation models represents a different approach, operating with a singlestep search. However, too many options in decoding cause a blow"
E17-2059,N13-1073,0,0.0371886,"t no separate generation model). The target factors are the word surface form, lemma, and a morphosyntactic tag. We use the Czech positional tagset (Hajiˇc and Hladká, 1998) which fully describes the word’s morphological attributes. On the source side we use only surface forms, except for the discriminative classifier, which includes the features as shown in Table 2. We employ corpora that have been provided for the English→Czech News translation shared task at WMT16 (Bojar et al., 2016b), including the CzEng parallel corpus (Bojar et al., 2016a). Word alignments are created using fast_align (Dyer et al., 2013) and symmetrized. We extract phrases up to a maximum length of 7. The phrase table is 3 We evaluate case-sensitive with mteval-v13a.pl -c, comparing post-processed hypotheses against the raw reference. 372 input: now , six in 10 Republicans have a favorable view of Donald Trump . baseline: ted’ , šest v 10 republikáni mají pˇríznivý výhled Donald Trump . now, six inlocation 10 Republicansnom have a_favorable outlook Donaldnom Trumpnom . + synthetic (mtu) + morph-vw: ted’ , šest do deseti republikán˚u má pˇríznivý názor na Donalda Trumpa . now, six into tengen Republicansgen have a_favorable op"
E17-2059,W11-2138,1,0.887605,"et al., 2012; Burlot et al., 2016). An important problem is that lexical choice (of the lemma) is carried out in a separate step from morphological prediction. Factored machine translation with separate translation and generation models represents a different approach, operating with a singlestep search. However, too many options in decoding cause a blow-up of the search space; and useful information is dropped when modeling source_word→target_lemma and target_lemma→target_word separately. Word forms not seen in parallel data are sometimes still available in monolingual data. Backtranslation (Bojar and Tamchyna, 2011) takes advantage of this. The monolingual target language data is lemmatized, automatically translated to the source language, and the translations are aligned with the original, inflected target corpus to produce supplementary training data. Disadvantages are both the computational expense and that the back-translated text may contain errors. Previous work on synthetic phrases by Chahuneau et al. (2013) is most similar to our work. They commit to generation of a single candidate inflection of a lemma prior to decoding, chosen only based on a hierarchical rule and source-side information, a si"
E17-2059,E12-1068,1,0.915146,"rors. Consider the Czech example in Table 1. A small parallel corpus of 50K English-Czech sentences contains only a single variant of the morphological 2 Related Work Translation into morphologically rich languages is often tackled through “two-step”, i.e., separate modules for morphological prediction and generation (Toutanova et al., 2008; Bojar and Kos, 2010; 369 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 369–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Fraser et al., 2012; Burlot et al., 2016). An important problem is that lexical choice (of the lemma) is carried out in a separate step from morphological prediction. Factored machine translation with separate translation and generation models represents a different approach, operating with a singlestep search. However, too many options in decoding cause a blow-up of the search space; and useful information is dropped when modeling source_word→target_lemma and target_lemma→target_word separately. Word forms not seen in parallel data are sometimes still available in monolingual data. Backtranslation (Bojar and Ta"
E17-2059,P98-1080,0,0.622887,"Missing"
E17-2059,D07-1091,0,0.389039,"morphological forms that are not known as translations of the source side of the phrase, we add these morphological variants as new translation options. We consider two settings: Scoring Unseen Morphological Variants Assigning dependable model scores to synthesized morphological forms is a primary challenge. During decoding, the artificially added phrase table entries compete with baseline phrases that had been directly extracted from the parallel training data. The correct choice has to be determined in search based on model scores. A phrase-based model with linguistically motivated factors (Koehn and Hoang, 2007) enables us to achieve better generalization capabilities when translating into a morphologically rich language. 370 newstest 2014 B LEU 2015 B LEU 2016 B LEU 12.4 12.2 10.8 10.6 11.8 11.8 + synthetic (word) + morph-vw-50K 13.4 13.4 11.3 11.4 12.5 12.7 + synthetic (word?) + morph-vw-50K 13.3 13.3 11.3 11.3 + synthetic (mtu) + morph-vw-50K 13.5 13.4 + synthetic (mtu?) + morph-vw-50K 13.4 13.5 system baseline 50K + morph-vw-50K 2015 B LEU 2016 B LEU 17.7 17.6 14.4 14.4 16.1 16.5 + synthetic (word) + morph-vw-500K 18.1 18.4 14.7 15.2 16.4 17.3 12.5 12.7 + synthetic (word?) + morph-vw-500K 18.0 18"
E17-2059,N03-1017,0,0.0265226,"emmas. We design techniques for generating and scoring unseen morphological variants fully integrated into phrase-based search, with the decoder being able to choose freely amongst all possible morphological variants. Empirically, we observe considerable gains in translation quality especially under medium- to low-resource conditions. Introduction Morphologically rich languages exhibit a large amount of inflected word surface forms for most lemmas, which poses difficulties to current statistical machine translation (SMT) technology. SMT systems, such as phrase-based translation (PBT) engines (Koehn et al., 2003), are trained on parallel corpora and can learn the vocabulary that is observed in the data. After training, the decoder can output words which have been seen on the target side of the corpus, but no unseen words. Sparsity of morphological variants leads to many linguistically valid morphological word forms remaining unseen in practical scenarios. This is a substantial issue under low-resource conditions, but the problem persists even with larger amounts of parallel training data. When translating into the morphologically rich language, the system fails at producing the unseen morphological va"
E17-2059,P07-2045,1,0.0312554,"f a known word, these two independent components should still be able to assign meaningful scores to the translation. Note that the features require lemmatization and tagging on both sides and a dependency parse of the source side. 5 16.1 17.3 500K 18.9 19.0 5M 20.5 20.8 50M 0 5 10 15 20 B LEU Figure 1: Visualization of the English→Czech translation quality on newstest2016, showing the benefit of our approach under different training resource conditions (50K/500K/5M/50M). Empirical Evaluation For an empirical evaluation of our technique, we build baseline phrase-based SMT engines using Moses (Koehn et al., 2007). We then enrich these baselines with linguistically motivated morphological variants that are unseen in the parallel training data, and we augment the model with the discriminative classifier to guide morphological selection during decoding. Different flavors of synthetic morphological variants are compared, each either combined with the discriminative classifier or standalone. We choose English→Czech as a task that is representative for machine translation from a morphologically underspecified language into a morphologically rich language. 5.1 11.8 12.7 50K pre-pruned by applying a minimum s"
E17-2059,P02-1040,0,0.101706,"y underspecified language into a morphologically rich language. 5.1 11.8 12.7 50K pre-pruned by applying a minimum score threshold of 0.0001 on the source-to-target phrase translation probability, and the decoder loads a maximum of 100 best translation options per distinct source side. We use cube pruning in decoding. Pop limit and stack limit for cube pruning are set to 1000 for tuning and to 5000 for testing. The distortion limit is 6. Weights are tuned on newstest2013 with k-best MIRA (Cherry and Foster, 2012) over 200-best lists for 25 iterations. Translation quality is measured in B LEU (Papineni et al., 2002) on three different test sets, newstest2014, newstest2015, and newstest2016.3 Our training data amounts to around 50 million bilingual sentences overall, but we conduct sets of experiments with systems trained using different fractions of this data (50K, 500K, 5M, 50M). Whereas English→Czech has good coverage in terms of training corpora, we simulate lowand medium-resource conditions for the purpose of drawing more general conclusions. Irrespective of this, we utilize the same large LMs in all setups, assuming that proper amounts of target language monolingual data can often be gathered, even"
E17-2059,P14-5003,0,0.110248,"Missing"
E17-2059,P16-1161,1,0.749918,"entries. For baseline phrase table entries, we retain their four baseline phrase translation and lexical translation features, meaning that features over target lemmas score synthesized entries and features over surface forms score baseline entries. The features have separate weights in the model combination. Furthermore, a binary indicator distinguishes baseline phrases from synthesized phrases. The final key to our approach is using a discriminative classifier (morph-vw, Vowpal Wabbit2 for Morphology) which can take context from both the source side and the target side into account, as in (Tamchyna et al., 2016). We design feature templates for the classifier that generalize to unseen morphological variants, as listed in Table 2. “Indicator” features are concatenations of words inside 1 But note that our factored systems operate without a division into separate translation and generation models. 2 371 https://hunch.net/~vw/ baseline + synthetic (mtu) + morph-vw the phrase, “internal” features represent each word in the phrase separately. Context features on the source side capture a fixed-sized window around the phrase. Target-side context is only to the left of the current phrase. The feature set is"
E17-2059,P08-1059,0,0.170483,"substantial issue under low-resource conditions, but the problem persists even with larger amounts of parallel training data. When translating into the morphologically rich language, the system fails at producing the unseen morphological variants, leading to major translation errors. Consider the Czech example in Table 1. A small parallel corpus of 50K English-Czech sentences contains only a single variant of the morphological 2 Related Work Translation into morphologically rich languages is often tackled through “two-step”, i.e., separate modules for morphological prediction and generation (Toutanova et al., 2008; Bojar and Kos, 2010; 369 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 369–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Fraser et al., 2012; Burlot et al., 2016). An important problem is that lexical choice (of the lemma) is carried out in a separate step from morphological prediction. Factored machine translation with separate translation and generation models represents a different approach, operating with a singlestep search. However, too many options in"
fishel-etal-2012-terra,specia-etal-2010-dataset,0,\N,Missing
fishel-etal-2012-terra,W10-1755,0,\N,Missing
fishel-etal-2012-terra,J11-4002,1,\N,Missing
fishel-etal-2012-terra,P02-1040,0,\N,Missing
fishel-etal-2012-terra,W10-1738,0,\N,Missing
fishel-etal-2012-terra,P11-1103,0,\N,Missing
fishel-etal-2012-terra,W09-0401,0,\N,Missing
fishel-etal-2012-terra,P07-2045,1,\N,Missing
fishel-etal-2012-terra,P11-1022,0,\N,Missing
fishel-etal-2012-terra,C08-1141,0,\N,Missing
fishel-etal-2012-terra,P11-4010,0,\N,Missing
fishel-etal-2012-terra,2011.eamt-1.12,0,\N,Missing
fishel-etal-2012-terra,vilar-etal-2006-error,0,\N,Missing
fishel-etal-2012-terra,2010.eamt-1.12,0,\N,Missing
hajic-etal-2012-announcing,W04-2705,0,\N,Missing
hajic-etal-2012-announcing,C00-2163,0,\N,Missing
hajic-etal-2012-announcing,P07-1031,0,\N,Missing
I05-2031,2004.eamt-1.3,0,0.0456147,"jective followed by a noun get the same structure: the noun governs the preceeding adjective. For the remaining entries (with very varied POS patterns), we employ a corpus-based search similar to the automatic procedure of identifying morphological constraints. 4.3 Named entity recognition module Named entities (NE) are atomic units such as proper names, temporal expressions (e.g., dates) and quantities (e.g., monetary expressions). They occur quite often in various texts and carry important information. Hence, proper analysis of NEs and their translation has an enormous impact on MT quality (Babych and Hartley, 2004). In our system they are extremely important due to the nature of input texts. The Wall Street Journal section of PennTreebank shows much higher density of named entities than ordinary texts. Their correct recognition therefore has a tremendous impact on the performance of the whole system, especially if the evaluation of the translation quality is based on golden standard translations. NE translation involves both semantic translation and phonetic transliteration. Each type of NE is handled in a different way. For instance, person names do not undergo semantic translation (only transliteratio"
I05-2031,2003.mtsummit-papers.21,1,0.811712,"Missing"
jawaid-bojar-2014-two,2005.iwslt-1.18,0,\N,Missing
jawaid-bojar-2014-two,W12-3130,1,\N,Missing
jawaid-bojar-2014-two,W09-0422,1,\N,Missing
jawaid-bojar-2014-two,C00-2163,0,\N,Missing
jawaid-bojar-2014-two,W07-1709,0,\N,Missing
jawaid-bojar-2014-two,P02-1040,0,\N,Missing
jawaid-bojar-2014-two,D07-1091,0,\N,Missing
jawaid-bojar-2014-two,P08-1059,0,\N,Missing
jawaid-bojar-2014-two,P07-2045,1,\N,Missing
jawaid-bojar-2014-two,W07-0720,0,\N,Missing
jawaid-bojar-2014-two,P07-1017,0,\N,Missing
jawaid-bojar-2014-two,P05-1033,0,\N,Missing
jawaid-bojar-2014-two,P08-1115,0,\N,Missing
jawaid-bojar-2014-two,J07-2003,0,\N,Missing
jawaid-bojar-2014-two,W10-1705,1,\N,Missing
jawaid-etal-2014-tagged,baker-etal-2002-emille,0,\N,Missing
jawaid-etal-2014-tagged,E09-1079,0,\N,Missing
jawaid-etal-2014-tagged,W12-5011,1,\N,Missing
jawaid-etal-2014-tagged,gimenez-marquez-2004-svmtool,0,\N,Missing
kocmi-bojar-2017-curriculum,P02-1040,0,\N,Missing
kocmi-bojar-2017-curriculum,D16-1248,0,\N,Missing
kocmi-bojar-2017-curriculum,E17-3017,0,\N,Missing
kocmi-bojar-2017-curriculum,P16-1162,0,\N,Missing
N19-1070,I13-1062,0,0.122196,"sed ones. 1 Introduction A valuable concept for searching and categorizing scientific papers in digital libraries is the keyphrase (we use keyphrase and keyword interchangeably), a short set of one or few words that represent concepts. Scientific articles are commonly annotated with keyphrases based on taxonomies of concepts and the authors’ judgment. Finding keyphrases that best describe the contents of a document is thus essential and rewarding. Most of the proposed keyphrase extraction solutions tend to be unsupervised (Florescu and Caragea, 2017; Nguyen and Luong, 2010; Rose et al., 2010; Bougouin et al., 2013; Campos et al., 2018) and generate terms by selecting the most appropriate candidates, ranking the candidates based on several features and finally returning the top N . Another way is to utilize datasets of texts and keywords for training supervised models with linguistic or other features to predict if candidates 1 http://hdl.handle.net/11234/1-2943 666 Proceedings of NAACL-HLT 2019, pages 666–672 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Attribute Records Keyphrases Title tokens Abstract tokens Av. Keyphrase Av. Title Av. Abstract train"
N19-1070,K16-1028,0,0.0278191,"arge dataset of article titles, abstracts, and keywords that can be used for keyword generation, text summarization or similar purposes. Finally, we evaluated the performance of different neural network architectures on summarization of article keyword strings, comparing them with popular unsupervised methods. 2 Train 2M 12M 24M 441M 6 12.1 220 Val 100K 575K 1.3M 21M 5.8 12.8 211 Test 100K 870K 1.6M 37M 8.7 15.9 372 Fullset 2.2M 13.4M 27M 499M 6.1 12.3 227 Table 1: Statistics of OAGK dataset used for testing, English Gigaword (Napoles et al., 2012), CNN/Daily Mail described in Section 4.3 of (Nallapati et al., 2016) and Newsroom, a heterogeneous bundle of news articles described in Grusky et al. (2018). These datasets are frequently used for the task of predicting titles from abstracts or short stories. However, no keyphrases are provided; they do not serve to our purpose. ArnetMiner is a recent attempt to crawl scientific paper data from academic networks (Tang et al., 2008). The system extracts profiles of researchers from digital resources and integrates their data in a common network. A spin-off is the Open Academic Graph (OAG) data collection (Sinha et al., 2015). To produce a usable collection for"
N19-1070,W12-3018,0,0.0352423,"Missing"
N19-1070,P17-1102,0,0.330961,"yphrases than the simpler unsupervised methods, or the existing supervised ones. 1 Introduction A valuable concept for searching and categorizing scientific papers in digital libraries is the keyphrase (we use keyphrase and keyword interchangeably), a short set of one or few words that represent concepts. Scientific articles are commonly annotated with keyphrases based on taxonomies of concepts and the authors’ judgment. Finding keyphrases that best describe the contents of a document is thus essential and rewarding. Most of the proposed keyphrase extraction solutions tend to be unsupervised (Florescu and Caragea, 2017; Nguyen and Luong, 2010; Rose et al., 2010; Bougouin et al., 2013; Campos et al., 2018) and generate terms by selecting the most appropriate candidates, ranking the candidates based on several features and finally returning the top N . Another way is to utilize datasets of texts and keywords for training supervised models with linguistic or other features to predict if candidates 1 http://hdl.handle.net/11234/1-2943 666 Proceedings of NAACL-HLT 2019, pages 666–672 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Attribute Records Keyphrases Title"
N19-1070,S10-1035,0,0.809431,"supervised methods, or the existing supervised ones. 1 Introduction A valuable concept for searching and categorizing scientific papers in digital libraries is the keyphrase (we use keyphrase and keyword interchangeably), a short set of one or few words that represent concepts. Scientific articles are commonly annotated with keyphrases based on taxonomies of concepts and the authors’ judgment. Finding keyphrases that best describe the contents of a document is thus essential and rewarding. Most of the proposed keyphrase extraction solutions tend to be unsupervised (Florescu and Caragea, 2017; Nguyen and Luong, 2010; Rose et al., 2010; Bougouin et al., 2013; Campos et al., 2018) and generate terms by selecting the most appropriate candidates, ranking the candidates based on several features and finally returning the top N . Another way is to utilize datasets of texts and keywords for training supervised models with linguistic or other features to predict if candidates 1 http://hdl.handle.net/11234/1-2943 666 Proceedings of NAACL-HLT 2019, pages 666–672 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Attribute Records Keyphrases Title tokens Abstract tokens"
N19-1070,N18-1065,0,0.0367776,"Missing"
N19-1070,D15-1044,0,0.274815,"alue for N and require it as a preset parameter. Furthermore, semantic and syntactic properties of article phrases are analyzed separately. The meaning of paragraphs, sections or entire document is thus missed. Lastly, only phrases that do appear in the article are returned. Meng et al. (2017) recently proposed a deep supervised keyphrase generation solution trained on a big dataset. It successfully solves the last two problems above, but not the first one. Motivated by recent advances in neural machine translation and abstractive text summarization (Vaswani et al., 2017; Foster et al., 2018; Rush et al., 2015; See et al., 2017), in this paper, we explore the possibility of considering keyphrase generation as an abstractive text summarization task. Instead of generating keywords one by one and linking them to form the keyphrase string, we consider the later as an abstractive summary of the concatenated paper title and abstract. Different recently-proposed text summarization architectures are tried on four test datasets of article keyphrases (Tanti et al., 2017; Rush et al., 2015; See et al., 2017). We trained them with a newly created dataset of 2.2 million article titles, abstracts and keyphrase s"
N19-1070,P14-1119,0,0.0960049,"l Library, ScienceDirect, and Web of Science. In Hulth (2003), we found a collection of 2000 (1500 for train/val and 500 for testing) abstracts in English, together with titles and authors’ keywords. The corresponding articles were published from 1998 to 2002 and belong to the discipline of Information Technology. Furthermore, Krapivin et al. (2010) released a dataset of 2000 (1600 for train/val and 400 for testing) full articles published by ACM from 2003 to 2005 in Computer Science domain. More information about similar keyphrase data collections or other available resources can be found in Hasan and Ng (2014) and in online repositories.2 Regarding text summarization, some of the most popular datasets are: DUC-2004 3 mainly 3 3.1 Keyphrase Extraction Strategies Unsupervised and Supervised Methods T OPIC R ANK is an extractive method that creates topic clusters using the graph of terms and phrases (Bougouin et al., 2013). Obtained topics are then ranked according to their importance in the document. Finally, keyphrases are extracted by pick2 https://github.com/LIAAD/ KeywordExtractor-Datasets 3 https://duc.nist.gov/duc2004/ 667 article abstracts and titles as sources and keyword strings as targets."
N19-1070,P17-1099,0,0.202348,"ire it as a preset parameter. Furthermore, semantic and syntactic properties of article phrases are analyzed separately. The meaning of paragraphs, sections or entire document is thus missed. Lastly, only phrases that do appear in the article are returned. Meng et al. (2017) recently proposed a deep supervised keyphrase generation solution trained on a big dataset. It successfully solves the last two problems above, but not the first one. Motivated by recent advances in neural machine translation and abstractive text summarization (Vaswani et al., 2017; Foster et al., 2018; Rush et al., 2015; See et al., 2017), in this paper, we explore the possibility of considering keyphrase generation as an abstractive text summarization task. Instead of generating keywords one by one and linking them to form the keyphrase string, we consider the later as an abstractive summary of the concatenated paper title and abstract. Different recently-proposed text summarization architectures are tried on four test datasets of article keyphrases (Tanti et al., 2017; Rush et al., 2015; See et al., 2017). We trained them with a newly created dataset of 2.2 million article titles, abstracts and keyphrase strings that we proc"
N19-1070,W03-1028,0,0.406516,"sks. Some rounded measures about each set of released data are presented in Table 1. Scientific Paper Datasets Because of the open source and open data initiatives, many public datasets from various domains can be found online (C ¸ ano and Morisio, 2015). Among the several collections of scientific articles, some of them have gained considerable popularity in research literature. In Meng et al. (2017), we found a recent and big collection of 20K paper abstracts and keyphrases. These metadata belong to articles of computer science from ACM Digital Library, ScienceDirect, and Web of Science. In Hulth (2003), we found a collection of 2000 (1500 for train/val and 500 for testing) abstracts in English, together with titles and authors’ keywords. The corresponding articles were published from 1998 to 2002 and belong to the discipline of Information Technology. Furthermore, Krapivin et al. (2010) released a dataset of 2000 (1600 for train/val and 400 for testing) full articles published by ACM from 2003 to 2005 in Computer Science domain. More information about similar keyphrase data collections or other available resources can be found in Hasan and Ng (2014) and in online repositories.2 Regarding te"
N19-1070,W04-1013,0,0.153762,"enerating keywords one by one and linking them to form the keyphrase string, we consider the later as an abstractive summary of the concatenated paper title and abstract. Different recently-proposed text summarization architectures are tried on four test datasets of article keyphrases (Tanti et al., 2017; Rush et al., 2015; See et al., 2017). We trained them with a newly created dataset of 2.2 million article titles, abstracts and keyphrase strings that we processed and released.1 The selected text summarization models are compared with popular unsupervised and supervised methods using ROUGE (Lin, 2004) and fullmatch F1 metrics. The results show that though Authors’ keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. In this paper, we explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract. First, we collect, process and release a large dataset of scientific paper metadata that contains 2.2 million records. Then we experiment wit"
N19-1070,W17-3506,0,0.0520699,"ne. Motivated by recent advances in neural machine translation and abstractive text summarization (Vaswani et al., 2017; Foster et al., 2018; Rush et al., 2015; See et al., 2017), in this paper, we explore the possibility of considering keyphrase generation as an abstractive text summarization task. Instead of generating keywords one by one and linking them to form the keyphrase string, we consider the later as an abstractive summary of the concatenated paper title and abstract. Different recently-proposed text summarization architectures are tried on four test datasets of article keyphrases (Tanti et al., 2017; Rush et al., 2015; See et al., 2017). We trained them with a newly created dataset of 2.2 million article titles, abstracts and keyphrase strings that we processed and released.1 The selected text summarization models are compared with popular unsupervised and supervised methods using ROUGE (Lin, 2004) and fullmatch F1 metrics. The results show that though Authors’ keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable bu"
N19-1070,P16-1008,0,0.0163243,"ilar to A BS. It is composed of an attention-based encoder that produces the context vector. The decoder is extended with a pointer-generator model that computes a probability pgen from the context vector, the decoder states, and the decoder output. That probability is used as a switch to decide if the next word is to be generated or copied from the input. This model is thus a compromise between abstractive and extractive (copying words from input) models. Another extension is the coverage mechanism for avoiding word repetitions in the summary, a common problem of encoder-decoder summarizers (Tu et al., 2016). ing one candidate from each of the most important topics. A more recent, unsupervised and featurebased method for keyphrase extraction is YAKE ! (Campos et al., 2018). It heuristically combines features like casing, word position or word frequency to generate an aggregate score for each phrase and uses it to select the best candidates. One of the first supervised methods is K EA described by Witten et al. (1999). It extracts those candidate phrases from the document that have good chances to be keywords. Several features like TF-IDF are computed for each candidate phrase during training. In"
N19-1070,P18-1008,0,\N,Missing
P07-2045,N03-2002,0,0.152204,"nfusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input."
P07-2045,koen-2004-pharaoh,0,0.148177,"to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems. Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004). Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding. The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic. These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps. Moses also integrates confusion network decoding, which allows the translation of amb"
P07-2045,D07-1091,1,0.158367,"Missing"
P07-2045,N03-1017,1,0.161374,"informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 07aec_2@williams.edu. 8 evh4@cornell.edu 2 Abstract We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 1 Motivation Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research. However, until now, most work in this field has been carried out on proprietary and in-house research systems. This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is co"
P07-2045,P03-1021,0,0.176468,"ent data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customiz"
P07-2045,J03-1002,0,0.164868,"L 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Moses has an active research community and has reached over 1000 downloads as of 1st March 2007. The main online pre"
P07-2045,P02-1040,0,0.148118,"d language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Mo"
P07-2045,N07-1062,1,0.152186,"up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed. Moses implements an efficient representation of the phrase translation table. Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder. For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more"
P07-2045,D08-1076,0,\N,Missing
P07-2045,2006.iwslt-evaluation.8,1,\N,Missing
P10-2016,W08-0309,0,0.065204,"based on the deep syntactic representation of the sentence performing very well for Czech as the target language. Aside from including dependency and n-gram relations in the scoring, we also apply and evaluate SemPOS for English. Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1 , WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2 . The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. 2 Problems of BLEU BLEU (Papineni et al., 2002) is"
P10-2016,W09-0401,0,0.0561799,"representation of the sentence performing very well for Czech as the target language. Aside from including dependency and n-gram relations in the scoring, we also apply and evaluate SemPOS for English. Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1 , WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2 . The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. 2 Problems of BLEU BLEU (Papineni et al., 2002) is an established language-indep"
P10-2016,2009.mtsummit-papers.3,0,0.368676,"o if the part of speech does not occur in the reference or the candidate set and otherwise it is computed as given in Equation 1 below. ˇ We use TectoMT (Zabokrtsk´ y and Bojar, 2008), http://ufal.mff.cuni.cz/tectomt/, for the linguistic pre-processing. While both our implementation of SemPOS as well as TectoMT are in principle freely available, a stable public version has yet to be released. Our plans include experiments with approximating the deep syntactic analysis with a simple tagger, which would also decrease the installation burden and computation costs, at the expense of accuracy. 5 3 Condon et al. (2009) identify similar issues when evaluating translation to Arabic and employ rule-based normalization of MT output to improve the correlation. It is beyond the scope of this paper to describe the rather different nature of morphological richness in Czech, Arabic and also other languages, e.g. German or Finnish. 4 The dataset with manually flagged errors is available at http://ufal.mff.cuni.cz/euromatrixplus/ 87 SRC REF cu-bojar pctrans Prague Stock Market falls to minus by the end of the trading day praˇzsk´a burza se ke konci obchodov´an´ı propadla do minusu praha stock market klesne k minus na"
P10-2016,W07-0738,0,0.0873662,"Missing"
P10-2016,P07-2045,1,0.010071,". for other languages) were found later, usually employing language-specific tools, see e.g. Przybocki et al. (2008) or Callison-Burch et al. (2009). The unbeaten advantage of BLEU is its simplicity. Figure 1 illustrates a very low correlation to human judgments when translating to Czech. We plot the official BLEU score against the rank established as the percentage of sentences where a system ranked no worse than all its competitors (Callison-Burch et al., 2009). The systems developed at Charles University (cu-) are described in Bojar et al. (2009), uedin is a vanilla configuration of Moses (Koehn et al., 2007) and the remaining ones are commercial MT systems. In a manual analysis, we identified the reasons for the low correlation: BLEU is overly sensitive to sequences and forms in the hypothesis matching ∗ This work has been supported by the grants EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of the Czech Republic), FP7-ICT-2009-4-247762 (Faust), GA201/09/H057, GAUK 1163/2010, and MSM 0021620838. We are grateful to the anonymous reviewers for further research suggestions. 1 http://nist.gov/speech/tests /metricsmatr/2008/results/ 2 http://www.statmt.org/wmt08 and wmt09 86 Proceedings"
P10-2016,P02-1040,0,0.0811214,"e also apply and evaluate SemPOS for English. Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1 , WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2 . The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. 2 Problems of BLEU BLEU (Papineni et al., 2002) is an established language-independent MT metric. Its correlation to human judgments was originally deemed high (for English) but better correlating metrics (esp. for other languages"
P10-2016,W09-0422,1,\N,Missing
P16-1161,D07-1007,0,0.312333,"econd issue has to do with morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder. This enables our classifier to extract features not only from the full source sentence but also from a limited targetside context. This allows the model to not only help with semantics but also to improve morphological and syntactic coherence. For sense disambiguation, source context is the main source of information, as has been shown in previous work (Vickrey et al., 2005), (Carpuat and Wu, 2007), (Gimpel and Smith, 2008) inter alia. Consider the first set of examples in Figure 1, produced by a strong baseline PBMT system. The English word “shooting” has multiple senses when translated into Czech: it may either be the act of firing a weapon or making a film. When the cue word “film” is close, the phrase-based model is able to use it in one phrase with the ambiguous “shooting”, disambiguating correctly the translation. When we add a single word in between, the model fails to capture the relationship and the most frequent sense is selected instead. Wider source context information is re"
P16-1161,2012.eamt-1.60,0,0.0164676,"test set and we test on the WMT14 set. We use TreeTagger (Schmid, 1994) to lemmatize and tag the German data. English-Polish has not been included in WMT shared tasks so far, but was present as a language pair for several IWSLT editions which concentrate on TED talk translation. Full test sets are only available for 2010, 2011, and 2012. The references for 2013 and 2014 were not made public. We use the development set and test set from 2010 as development data for parameter tuning. The remaining two test sets (2011, 2012) are our test data. We train on the concatenation of Europarl and WIT3 (Cettolo et al., 2012), ca. 750 thousand sentence pairs. The Polish half has been tagged using WCRFT (Radziszewski, 2013) which produces full morphological tags compatible with the NKJP tagset (Przepi´orkowski, 2009). English-Romanian was added in WMT16. We train our system using the available parallel data – Europarl and SETIMES2 (Tiedemann, 2009), roughly 600 thousand sentence pairs. We tune the English-Romanian system on the official development set and we test on the WMT16 test set. We use the online tagger by Tufis et al. (2008) to preprocess the data. Table 3 shows the obtained results. Similarly to English-C"
P16-1161,P11-2031,0,0.0533509,"– our classifier with source-context features only, • +target – our classifier with both sourcecontext and target-context features. For each of these settings, we vary the size of the training data for our classifier, the phrase table and the LM. We experiment with three different sizes: small (200 thousand sentence pairs), medium (5 million sentence pairs), and full (the whole CzEng corpus, over 14.8 million sentence pairs). For each setting, we run system weight optimization (tuning) using minimum error rate training (Och, 2003) five times and report the average BLEU score. We use MultEval (Clark et al., 2011) to compare the systems and to determine whether the differences in results are statistically significant. We always compare the baseline with +source and +source with +target. Table 2 shows the obtained results. Statistically significant differences (α=0.01) are marked in bold. The source-context model does not help in the small data setting but brings a substantial improvement of 0.7-0.8 BLEU points for the medium and full data settings, which is an encouraging result. Target-side context information allows our model to push the translation quality further: even for the small data setting, i"
P16-1161,P14-1129,0,0.036134,"We look at phrase counts and cooccurrence counts in the training data, we subtract one from the number of occurrences for the current source phrase, target phrase and the phrase pair. If the count goes to zero, we skip the training example. Without this technique, the classifier might learn to simply trust very long phrase pairs which were extracted from the same training sentence. For target-side context features, we simply use the true (gold) target context. This leads to training which is similar to language model estimation; this model is somewhat similar to the neural joint model for MT (Devlin et al., 2014), but in our case implemented using a linear (maximumentropy-like) model. 2.4 Training We use Vowpal Wabbit in the --csoaa ldf mc setting which reduces our multi-class problem to one-against-all binary classification. We use the logistic loss as our objective. We experimented with various settings of L2 regularization but were not able to get an improvement over not using regularization at all. We train each model with 10 iterations over the data. We evaluate all of our models on a held-out set. We use the same dataset as for MT system tuning because it closely matches the domain of our test s"
P16-1161,W08-0302,0,0.0657345,"th morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder. This enables our classifier to extract features not only from the full source sentence but also from a limited targetside context. This allows the model to not only help with semantics but also to improve morphological and syntactic coherence. For sense disambiguation, source context is the main source of information, as has been shown in previous work (Vickrey et al., 2005), (Carpuat and Wu, 2007), (Gimpel and Smith, 2008) inter alia. Consider the first set of examples in Figure 1, produced by a strong baseline PBMT system. The English word “shooting” has multiple senses when translated into Czech: it may either be the act of firing a weapon or making a film. When the cue word “film” is close, the phrase-based model is able to use it in one phrase with the ambiguous “shooting”, disambiguating correctly the translation. When we add a single word in between, the model fails to capture the relationship and the most frequent sense is selected instead. Wider source context information is required for correct disambi"
P16-1161,2010.amta-papers.33,0,0.107227,"nt phrases. They used a strong feature set originally developed for word sense disambiguation. Gimpel and Smith (2008) also used wider source-context information but did not train a classifier; instead, the features were included directly in the log-linear model of the decoder. Mauser et al. (2009) introduced the “discriminative word lexicon” and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features. Subotin (2011) included target-side context information in a maximum-entropy model for the prediction of morphology. The work was done within the paradigm of hierarchical PBMT and assumes that cube pruning is used in decoding. Their algorithm was tailored to the specific problem of passing non-local information about morphological agreement required by individual rules (such as explicit rules enforcing subject-ve"
P16-1161,D07-1091,0,0.151245,"perimented with various settings of L2 regularization but were not able to get an improvement over not using regularization at all. We train each model with 10 iterations over the data. We evaluate all of our models on a held-out set. We use the same dataset as for MT system tuning because it closely matches the domain of our test set. We evaluate model accuracy after each pass over the training data to detect over-fitting and we select the model with the highest held-out accuracy. 2.5 Feature Set Our feature set requires some linguistic processing of the data. We use the factored MT setting (Koehn and Hoang, 2007) and we represent each type of information as an individual factor. On the source side, we use the word surface form, its lemma, morphological tag, analytical function (such as Subj for subjects) and the lemma of the parent node in the dependency parse tree. On the target side, we only use word lemmas and morphological tags. Table 1 lists our feature sets for each language pair. We implemented indicator features for both the source and target side; these are simply concatenations of the words in the current phrase into a single feature. Internal features describe words within the current phras"
P16-1161,2005.mtsummit-papers.11,0,0.0967476,"ur model. A baseline which always chooses the most frequent phrasal translation obtains accuracy of 51.5. For the sourcecontext model, the held-out accuracy was 66.3, while the target context model achieved accuracy of 74.8. Note that this high difference is somewhat misleading because in this setting, the targetcontext model has access to the true target context (i.e., it is cheating). 4.2 Additional Language Pairs We experiment with translation from English into German, Polish, and Romanian. Our English-German system is trained on the data available for the WMT14 translation task: Europarl (Koehn, 2005) and the Common Crawl corpus,3 roughly 4.3 million sentence pairs altogether. We tune the system on the WMT13 test set and we test on the WMT14 set. We use TreeTagger (Schmid, 1994) to lemmatize and tag the German data. English-Polish has not been included in WMT shared tasks so far, but was present as a language pair for several IWSLT editions which concentrate on TED talk translation. Full test sets are only available for 2010, 2011, and 2012. The references for 2013 and 2014 were not made public. We use the development set and test set from 2010 as development data for parameter tuning. The"
P16-1161,D09-1022,0,0.0271419,"have been proposed before. Carpuat and Wu (2007) trained a maximum entropy classifier for each source phrase type which used source context information to disambiguate its translations. The models did not capture target-side information and they were independent; no parameters were shared between classifiers for different phrases. They used a strong feature set originally developed for word sense disambiguation. Gimpel and Smith (2008) also used wider source-context information but did not train a classifier; instead, the features were included directly in the log-linear model of the decoder. Mauser et al. (2009) introduced the “discriminative word lexicon” and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features. Subotin (2011) included target-side context information in a maximum-entropy model for the predi"
P16-1161,W11-2124,0,0.0242953,"mplemented indicator features for both the source and target side; these are simply concatenations of the words in the current phrase into a single feature. Internal features describe words within the current phrase. Context features are extracted either from a window of a fixed size around the current phrase (on the source side) or from a limited left-hand side context (on the target side). Bilingual context features are concatenations of target-side context words and their sourceside counterparts (according to word alignment); these features are similar to bilingual tokens in bilingual LMs (Niehues et al., 2011). Each of our feature types can be configured to look at any individual factors or their combinations. The features in Table 1 are divided into three sets. The first set contains label-independent (=shared) features which only depend on the source sentence. The second set contains shared features which depend on target-side context; these can only be used when VW is applied during decoding. We use target context size two in all our experiments.2 Finally, the third set contains label-dependent features which describe the currently predicted phrasal translation. 2 In preliminary experiments we f"
P16-1161,P03-1021,0,0.038966,"English to Czech translation. To verify that our method is 1709 • +source – our classifier with source-context features only, • +target – our classifier with both sourcecontext and target-context features. For each of these settings, we vary the size of the training data for our classifier, the phrase table and the LM. We experiment with three different sizes: small (200 thousand sentence pairs), medium (5 million sentence pairs), and full (the whole CzEng corpus, over 14.8 million sentence pairs). For each setting, we run system weight optimization (tuning) using minimum error rate training (Och, 2003) five times and report the average BLEU score. We use MultEval (Clark et al., 2011) to compare the systems and to determine whether the differences in results are statistically significant. We always compare the baseline with +source and +source with +target. Table 2 shows the obtained results. Statistically significant differences (α=0.01) are marked in bold. The source-context model does not help in the small data setting but brings a substantial improvement of 0.7-0.8 BLEU points for the medium and full data settings, which is an encouraging result. Target-side context information allows ou"
P16-1161,P14-5003,0,0.0928089,"Missing"
P16-1161,P11-1024,0,0.0179208,"the features were included directly in the log-linear model of the decoder. Mauser et al. (2009) introduced the “discriminative word lexicon” and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features. Subotin (2011) included target-side context information in a maximum-entropy model for the prediction of morphology. The work was done within the paradigm of hierarchical PBMT and assumes that cube pruning is used in decoding. Their algorithm was tailored to the specific problem of passing non-local information about morphological agreement required by individual rules (such as explicit rules enforcing subject-verb agreement). Our algorithm only assumes that hypotheses are constructed left to right and provides a general way for including target context information in the classifier, regardless of the type"
P16-1161,tufis-etal-2008-racais,0,0.132569,"11, 2012) are our test data. We train on the concatenation of Europarl and WIT3 (Cettolo et al., 2012), ca. 750 thousand sentence pairs. The Polish half has been tagged using WCRFT (Radziszewski, 2013) which produces full morphological tags compatible with the NKJP tagset (Przepi´orkowski, 2009). English-Romanian was added in WMT16. We train our system using the available parallel data – Europarl and SETIMES2 (Tiedemann, 2009), roughly 600 thousand sentence pairs. We tune the English-Romanian system on the official development set and we test on the WMT16 test set. We use the online tagger by Tufis et al. (2008) to preprocess the data. Table 3 shows the obtained results. Similarly to English-Czech experiments, BLEU scores are av1710 3 http://commoncrawl.org/ input: baseline: +source: +target: the most intensive mining took place there from 1953 to 1962 . nejv´ıce intenzivn´ı tˇezˇ ba doˇslo tam z roku 1953 , aby 1962 . the most intensive miningnom there occurred there from 1953 , in order to 1962 . nejv´ıce intenzivn´ı tˇezˇ by m´ısto tam z roku 1953 do roku 1962 . the most intensive mininggen place there from year 1953 until year 1962 . nejv´ıce intenzivn´ı tˇezˇ ba prob´ıhala od roku 1953 do roku 1"
P16-1161,H05-1097,0,0.0554915,"iminative lexicon. The second issue has to do with morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder. This enables our classifier to extract features not only from the full source sentence but also from a limited targetside context. This allows the model to not only help with semantics but also to improve morphological and syntactic coherence. For sense disambiguation, source context is the main source of information, as has been shown in previous work (Vickrey et al., 2005), (Carpuat and Wu, 2007), (Gimpel and Smith, 2008) inter alia. Consider the first set of examples in Figure 1, produced by a strong baseline PBMT system. The English word “shooting” has multiple senses when translated into Czech: it may either be the act of firing a weapon or making a film. When the cue word “film” is close, the phrase-based model is able to use it in one phrase with the ambiguous “shooting”, disambiguating correctly the translation. When we add a single word in between, the model fails to capture the relationship and the most frequent sense is selected instead. Wider source c"
P16-1161,W14-3302,1,\N,Missing
P18-1126,D17-1070,0,0.368996,"i-head attention on the previous layer and a position-wise feed-forward network. In order to introduce a ﬁxed-size sentence representation into the model, we modify it by adding inner attention after the last encoder layer. The attention in the decoder then operates on the components of this representation (i.e. the rows of the matrix M ). This variation on the Transformer model corresponds to the ATTN - ATTN column in Table 1 and is therefore denoted TRF - ATTN - ATTN. 4 Representation Evaluation Continuous sentence representations can be evaluated in many ways, see e.g. Kiros et al. (2015), Conneau et al. (2017) or the RepEval workshops.2 We evaluate our learned representations with classiﬁcation and similarity tasks from SentEval (Section 4.1) and by examining clusters of sentence paraphrase representations (Section 4.2). SentEval We perform evaluation on 10 classiﬁcation and 7 similarity tasks using the SentEval3 (Conneau et al., 2017) evaluation tool. This is a superset of the tasks from Kiros et al. (2015). Table 2 describes the classiﬁcation tasks (number of classes, data size, task type and an example) and Table 3 lists the similarity tasks. The similarity (relatedness) datasets contain pairs o"
P18-1126,C04-1051,0,0.234617,"the classiﬁcation tasks (number of classes, data size, task type and an example) and Table 3 lists the similarity tasks. The similarity (relatedness) datasets contain pairs of sentences labeled with a real-valued similarity score. A given sentence representation model is evaluated either by learning to directly predict this score given the respective sentence embeddings (“regression”), or by computing the cosine similarity of the embeddings (“similarity”) without the need of any training. In both cases, Pearson and Spearman correlation of the predictions with the gold ratings is reported. See Dolan et al. (2004) for details on MRPC and Hill et al. (2016) for the remaining tasks. 4.2 Paraphrases We also evaluate the representation of paraphrases. We use two paraphrase sources for this purpose: COCO and HyTER Networks. COCO (Common Objects in Context; Lin et al., 2014) is an object recognition and image captioning dataset, containing 5 captions for each image. We extracted the captions from its validation set to form a set of 5 × 5k = 25k sentences grouped by the source image. The average sentence length is 11 tokens and the vocabulary size is 9k types. HyTER Networks (Dreyer and Marcu, 2014) are large"
P18-1126,W16-3210,0,0.0299806,"Missing"
P18-1126,N16-1162,0,0.512124,"e sentence to other sentences (e.g. semantic similarity, paraphrasing, entailment). On the other hand, we can aim at “universal” sentence representations, that is representations performing reasonably well in a range of such tasks. Regardless the evaluation criterion, the representations can be learned either in an unsupervised way (from simple, unannotated texts) or supervised, relying on manually constructed training sets of sentences equipped with annotations of the appropriate type. A different approach is to obtain sentence representations from training neural machine translation models (Hill et al., 2016). Since Hill et al. (2016), NMT has seen substantial advances in translation quality and it is thus natural to ask how these improvements affect the learned representations. One of the key technological changes was the introduction of “attention” (Bahdanau et al., 2014), making it even the very central component in the network (Vaswani et al., 2017). Attention allows the NMT system to dynamically choose which parts of the source are most important when deciding on the current output token. As a consequence, there is no longer a static vector representation of the sentence available in the syst"
P18-1126,P02-1040,0,0.100733,"f translation quality. (We use 1k randomly selected sentence pairs from CzEng 1.7 dtest as a development set. For evaluation, we use the entire etest.) We also evaluate the InferSent model6 (Conneau et al., 2017) as pre-trained on the natural language inference (NLI) task. InferSent has been shown to achieve state-of-the-art results on the SentEval tasks. We also include a bag-ofwords baseline (GloVe-BOW) obtained by averaging GloVe7 word vectors (Pennington et al., 2014). 5.1 Translation Quality We estimate translation quality of the various models using single-reference case-sensitive BLEU (Papineni et al., 2002) as implemented in Neural Monkey (the reference implementation is mteval-v13a.pl from NIST or Moses). Tables 4 and 5 provide the results on the two datasets. The cs dataset is much larger and the training takes much longer. We were thus able to experiment with only a subset of the possible model conﬁgurations. 5 http://ufal.mff.cuni.cz/czeng/czeng17 https://github.com/facebookresearch/ InferSent 7 https://nlp.stanford.edu/projects/ glove/ 1366 6 Model Size Heads de-ATTN de-TRF de-ATTN - ATTN de-ATTN - ATTN de-ATTN - ATTN de-ATTN - ATTN de-ATTN - ATTN de-ATTN - ATTN de-TRF - ATTN - ATTN de-ATTN"
P18-1126,D14-1162,0,0.0877606,"s). The datasets for both de and cs models come with their respective development and test sets of sentence pairs, which we use for the evaluation of translation quality. (We use 1k randomly selected sentence pairs from CzEng 1.7 dtest as a development set. For evaluation, we use the entire etest.) We also evaluate the InferSent model6 (Conneau et al., 2017) as pre-trained on the natural language inference (NLI) task. InferSent has been shown to achieve state-of-the-art results on the SentEval tasks. We also include a bag-ofwords baseline (GloVe-BOW) obtained by averaging GloVe7 word vectors (Pennington et al., 2014). 5.1 Translation Quality We estimate translation quality of the various models using single-reference case-sensitive BLEU (Papineni et al., 2002) as implemented in Neural Monkey (the reference implementation is mteval-v13a.pl from NIST or Moses). Tables 4 and 5 provide the results on the two datasets. The cs dataset is much larger and the training takes much longer. We were thus able to experiment with only a subset of the possible model conﬁgurations. 5 http://ufal.mff.cuni.cz/czeng/czeng17 https://github.com/facebookresearch/ InferSent 7 https://nlp.stanford.edu/projects/ glove/ 1366 6 Mode"
P18-1126,W17-2619,0,0.283059,"posed architecture: compound attention, described here in Section 3.1. sT � + β31 β32 decoder encoder Shi et al. (2016) investigate the syntactic properties of representations learned by NMT systems by predicting sentence- and word-level syntactic labels (e.g. tense, part of speech) and by generating syntax trees from these representations. 3 s3 c3 Hill et al. (2016) perform a systematic evaluation of sentence representation in different models, including NMT, by applying them to various sentence classiﬁcation tasks and by relating semantic similarity to closeness in the representation space. Schwenk and Douze (2017) aim to learn language-independent sentence representations using NMT systems with multiple source and target languages. They do not consider the attention mechanism and evaluate primarily by similarity scores of the learned representations for similar sentences (within or across languages). s2 β33 β34 M1 M2 M3 M4 = M � + α21 ← − h1 − → h1 x1 α22 α23 ← − h2 − → h2 x2 α2T ← − hT − → =H hT ... ← − h3 − → h3 x3 ... xT Figure 1: An illustration of compound attention with 4 attention heads. The ﬁgure shows the computations that result in the decoder state s3 (in addition, each state si depends on t"
P18-1126,P16-1009,0,0.0287227,"lower the DB index, the better the separation. To match with the rest of our metrics, we take its 1 inverse: iDB = DB . 5 Experimental Results We trained English-to-German and English-toCzech NMT models using Neural Monkey4 (Helcl and Libovick´y, 2017a). In the following, we distinguish these models using the code of the target language, i.e. de or cs. The de models were trained on the Multi30K multilingual image caption dataset (Elliott et al., 4 https://github.com/ufal/neuralmonkey 2016), extended by Helcl and Libovick´y (2017b), who acquired additional parallel data using backtranslation (Sennrich et al., 2016) and perplexitybased selection (Yasuda et al., 2008). This extended dataset contains 410k sentence pairs, with the average sentence length of 12 ± 4 tokens in English. We train each model for 20 epochs with the batch size of 32. We truecased the training data as well as all data we evaluate on. For German, we employed Neural Monkey’s reversible pre-processing scheme, which expands contractions and performs morphological segmentation of determiners. We used a vocabulary of at most 30k tokens for each language (no subword units). The cs models were trained on CzEng 1.7 (Bojar et al., 2016).5 We"
P18-1126,E17-3017,0,0.0567331,"Missing"
P18-1126,D16-1159,0,0.0611468,"1 Model Architectures Our proposed model architectures differ in (a) which encoder states are considered in subsequent processing, (b) how they are combined, and (c) how they are used in the decoder. Table 1 summarizes all the examined conﬁgurations of RNN-based models. The ﬁrst three (ATTN, FINAL, FINAL - CTX) correspond roughly to the standard sequence-to-sequence models, Bahdanau et al. (2014), Sutskever et al. (2014) and Cho et al. (2014), resp. The last column (ATTN ATTN ) is our main proposed architecture: compound attention, described here in Section 3.1. sT � + β31 β32 decoder encoder Shi et al. (2016) investigate the syntactic properties of representations learned by NMT systems by predicting sentence- and word-level syntactic labels (e.g. tense, part of speech) and by generating syntax trees from these representations. 3 s3 c3 Hill et al. (2016) perform a systematic evaluation of sentence representation in different models, including NMT, by applying them to various sentence classiﬁcation tasks and by relating semantic similarity to closeness in the representation space. Schwenk and Douze (2017) aim to learn language-independent sentence representations using NMT systems with multiple sou"
P18-1126,I08-2088,0,0.0349995,"ch with the rest of our metrics, we take its 1 inverse: iDB = DB . 5 Experimental Results We trained English-to-German and English-toCzech NMT models using Neural Monkey4 (Helcl and Libovick´y, 2017a). In the following, we distinguish these models using the code of the target language, i.e. de or cs. The de models were trained on the Multi30K multilingual image caption dataset (Elliott et al., 4 https://github.com/ufal/neuralmonkey 2016), extended by Helcl and Libovick´y (2017b), who acquired additional parallel data using backtranslation (Sennrich et al., 2016) and perplexitybased selection (Yasuda et al., 2008). This extended dataset contains 410k sentence pairs, with the average sentence length of 12 ± 4 tokens in English. We train each model for 20 epochs with the batch size of 32. We truecased the training data as well as all data we evaluate on. For German, we employed Neural Monkey’s reversible pre-processing scheme, which expands contractions and performs morphological segmentation of determiners. We used a vocabulary of at most 30k tokens for each language (no subword units). The cs models were trained on CzEng 1.7 (Bojar et al., 2016).5 We used byte-pair encoding (BPE) with a vocabulary of 3"
P19-2017,D18-1549,0,0.0214303,"performance to the backtranslation approach. Recently, Lample and Conneau (2019) suggested using a single cross-lingual language model trained on multiple monolingual corpora as an initialization for various NLP tasks, including machine translation. While our work focuses strictly on a monolingual language model pretraining, we believe that our work can further benefit from using cross-lingual language models. Another possible approach is to introduce an additional reordering (Zhang and Zong, 2016) or de-noising objectives, the latter being recently employed in the unsupervised NMT scenarios (Artetxe et al., 2018; Lample et al., 2017). These approaches try to force the NMT model to learn useful features by presenting it with either shuffled or noisy sentences teaching it to reconstruct the original input. Furthermore, Khayrallah et al. (2018) show how to prevent catastrophic forgeting during domain adaptation scenarios. They fine-tune the generaldomain NMT model using in-domain data adding p(θ|Dmt ∪ Dsrc ∪ Dtgt ) = p(Dmt |θ)p(θ|Dsrc ∪ Dtgt ) p(Dmt ) (1) Equation 1 holds, assuming datasets Dmt , Dsrc and Dtgt being mutually exclusive. The probability p(Dmt |θ) is the negative of the MT loss function an"
P19-2017,W18-2705,0,0.0346684,"achine translation. While our work focuses strictly on a monolingual language model pretraining, we believe that our work can further benefit from using cross-lingual language models. Another possible approach is to introduce an additional reordering (Zhang and Zong, 2016) or de-noising objectives, the latter being recently employed in the unsupervised NMT scenarios (Artetxe et al., 2018; Lample et al., 2017). These approaches try to force the NMT model to learn useful features by presenting it with either shuffled or noisy sentences teaching it to reconstruct the original input. Furthermore, Khayrallah et al. (2018) show how to prevent catastrophic forgeting during domain adaptation scenarios. They fine-tune the generaldomain NMT model using in-domain data adding p(θ|Dmt ∪ Dsrc ∪ Dtgt ) = p(Dmt |θ)p(θ|Dsrc ∪ Dtgt ) p(Dmt ) (1) Equation 1 holds, assuming datasets Dmt , Dsrc and Dtgt being mutually exclusive. The probability p(Dmt |θ) is the negative of the MT loss function and p(θ|Dsrc ∪ Dtgt ) is the result of the unsupervised pretraining. We can assume that during the unsupervised pretraining, the parameters θsrc of the encoder are independent of the parameters θtgt of the decoder. Furthermore, we assum"
P19-2017,D16-1160,0,0.0177053,". Additionally, Ramachandran et al. (2017) showed that the unsupervised pretraining approach reaches at least similar performance to the backtranslation approach. Recently, Lample and Conneau (2019) suggested using a single cross-lingual language model trained on multiple monolingual corpora as an initialization for various NLP tasks, including machine translation. While our work focuses strictly on a monolingual language model pretraining, we believe that our work can further benefit from using cross-lingual language models. Another possible approach is to introduce an additional reordering (Zhang and Zong, 2016) or de-noising objectives, the latter being recently employed in the unsupervised NMT scenarios (Artetxe et al., 2018; Lample et al., 2017). These approaches try to force the NMT model to learn useful features by presenting it with either shuffled or noisy sentences teaching it to reconstruct the original input. Furthermore, Khayrallah et al. (2018) show how to prevent catastrophic forgeting during domain adaptation scenarios. They fine-tune the generaldomain NMT model using in-domain data adding p(θ|Dmt ∪ Dsrc ∪ Dtgt ) = p(Dmt |θ)p(θ|Dsrc ∪ Dtgt ) p(Dmt ) (1) Equation 1 holds, assuming datase"
P19-2017,W18-6424,0,0.0201377,"ffective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-toright language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context. 1 Introduction Neural machine translation (NMT) using sequence to sequence architectures (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017) has become the dominant approach to automatic machine translation. While being able to approach human-level performance (Popel, 2018), it still requires a huge amount of parallel data, otherwise it can easily overfit. Such data, however, might not always be available. At the same time, it is generally much easier to gather large amounts of monolingual data, and therefore, it is interesting to find ways of making use of such data. The simplest 130 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 130–135 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ization. Their approach has two main drawbacks: first, durin"
P19-2017,E17-2025,0,0.0356917,"mber of training examples to converge. 131 the work of Kirkpatrick et al. (2017), we can estimate p(θsrc |Dsrc ) as Gaussian distribution using Laplace approximation (MacKay, 1992), with mean given by the pretrained parameters θsrc and variance given by a diagonal of the Fisher information matrix Fsrc . Then, we can add the following regularization term to our loss function: X Lewc−src (θ) = i,θi ⊂θsrc for source and target languages, each containing around 32k subwords. We use separate embeddings in the encoder and decoder. In the decoder, we tie the embeddings with the output softmax layer (Press and Wolf, 2017). During both pretraining and fine tuning, we use Adam optimizer (Kingma and Ba, 2014) and gradient clipping. We set the initial learning rate to 3.1, use a linear warm-up for 33500 training steps and then decay the learning rate exponentially. We set the training batch size to a maximum of 2048 tokens per batch together with sentence bucketing for more efficient training. We set dropout to 0.1. During the final evaluation, we use beam search decoding with beam size of 8 and length normalization set to 1.0. When pretraining the encoder and decoder, we use identical network parameters. We train"
P19-2017,D17-1039,0,0.108889,"rst, we initialize both encoder and decoder with source and target language models respectively. Then, we fine-tune the NMT model using the parallel data. To prevent the encoder and decoder from forgetting the original language modeling (LM) task, we regularize their weights individually using Elastic Weight Consolidation based on their importance to that task. Our hypothesis is the following: by forcing the network to remember the original LM tasks we can reduce overfitting of the NMT model on the limited parallel data. We also provide a comparison of our approach with the method proposed by Ramachandran et al. (2017). They also suggest initialization of the encoder and decoder with a language model. However, during the fine-tuning phase they use the original language modeling objectives as an additional training loss in place of model regularThis work presents our ongoing research of unsupervised pretraining in neural machine translation (NMT). In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation (EWC) to avoid forgetting of the original language m"
P19-2017,P16-1009,0,0.0274045,"er constraints, Kirkpatrick et al. (2017) approach the neural network training as a Bayesian inference problem. To put it into the context of NMT, we would like to find the most probable network parameters θ, given a parallel data Dmt and monolingual data Dsrc and Dtgt for source and target languages, respectively: Related Work Several other approaches towards exploiting the available monolingual data for NMT have been previously proposed. Currently, the most common method is creating synthetic parallel data by backtranslating the target language monolingual corpora using machine translation (Sennrich et al., 2016). While being consistently beneficial, this method requires a pretrained model to prepare the backtranslations. Additionally, Ramachandran et al. (2017) showed that the unsupervised pretraining approach reaches at least similar performance to the backtranslation approach. Recently, Lample and Conneau (2019) suggested using a single cross-lingual language model trained on multiple monolingual corpora as an initialization for various NLP tasks, including machine translation. While our work focuses strictly on a monolingual language model pretraining, we believe that our work can further benefit"
P19-2017,K17-3009,0,0.0647771,"Missing"
S15-2059,D07-1007,0,0.0492676,"our system in the sub-task which aims at monolingual all-words disambiguation and entity linking. Aside from system description, we pay closer attention to the evaluation of system outputs. 1 2 Introduction Word sense disambiguation (WSD, i.e. picking the right sense for a given word from a fixed inventory) and entity linking (EL, i.e. identifying a particular named entity listed in a database given its mention in a text) are among the fashionable tasks in computational linguistics and natural language processing these days. WSD has been, after some debate, shown to help machine translation (Carpuat and Wu, 2007), other applications include knowledge discovery or machine reading in general (Etzioni et al., 2006; Schubert, 2006). WSD and EL are usually applied with large and rich context available (Navigli, 2009), but the arguably harder setting of short context has a wider range of applications, including text similarity measurements (Abdalgader and Skabar, 2011), Named Entities Extraction and Named Entities Disambiguation (Habib and Keulen, 2012) ∗ This research was supported by the grants FP7-ICT-201310-610516 (QTLeap). This research was partially supported by SVV project number 260 224. This work h"
S15-2059,S15-2049,0,0.125808,"t context has a wider range of applications, including text similarity measurements (Abdalgader and Skabar, 2011), Named Entities Extraction and Named Entities Disambiguation (Habib and Keulen, 2012) ∗ This research was supported by the grants FP7-ICT-201310-610516 (QTLeap). This research was partially supported by SVV project number 260 224. This work has been using language resources developed, stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2010013). Task Description As participants of SemEval-2015 Task 13 (Moro and Navigli, 2015), we were given only a very brief instructions, effectively just one example of a POStagged sentence: The/X European/J/european Medicines/N/medicine Agency/N/agency (/X EMA/N/ema )/X is/V/be ,. . . We were expected to provide such input with labels indicating that e.g. the words “European Medicines Agency” refer to the entity described in the English Wikipedia under the title European Medicines Agency (“wiki:European Medicines Agency”), the word “Medicines” refers to the BabelNet concept 00054128n etc. The repertoire of word sense and entities came from BabelNet 2.5 which included: Wikipedia p"
S15-2059,2014.tc-1.29,1,0.736977,"Missing"
sindlerova-bojar-2010-building,J05-1004,0,\N,Missing
sindlerova-bojar-2010-building,2008.eamt-1.16,0,\N,Missing
W07-0735,N03-2002,0,0.0339025,"actor explicitly highlighting the Verb (lexicalized) its Modifiers (case and the lemma of the preposition, if present) and boundary symbols such as punctuation or conjunctions and using a dummy token for all other words did not bring any improvement over the baseline. A possible reason is that we employed only a standard 7-gram language model to this factor. A more appropriate treatment is to disregard the dummy tokens in the language model at all and use an n-gram language model that looks at last n − 1 non-dummy items. 8 Related Research Class-based LMs (Brown et al., 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario. Given the small differences in all T+. . . scenarios’ performance, class-based LM might bring equivalent improvement. Yang and Kirchhoff (2006) have recently documented minor BLEU improvement using factored LMs in singlefactored SMT to English. The multi-factored approach to SMT of Moses is however more general. Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovi´c and Ney, 2004)) or machine translation quality (Nießen and Ney (2001), Koehn and Knight (2003), Zollmann et al. (2006), among others, for vario"
W07-0735,J92-4003,0,0.0355786,"pendency parse trees to construct a factor explicitly highlighting the Verb (lexicalized) its Modifiers (case and the lemma of the preposition, if present) and boundary symbols such as punctuation or conjunctions and using a dummy token for all other words did not bring any improvement over the baseline. A possible reason is that we employed only a standard 7-gram language model to this factor. A more appropriate treatment is to disregard the dummy tokens in the language model at all and use an n-gram language model that looks at last n − 1 non-dummy items. 8 Related Research Class-based LMs (Brown et al., 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario. Given the small differences in all T+. . . scenarios’ performance, class-based LM might bring equivalent improvement. Yang and Kirchhoff (2006) have recently documented minor BLEU improvement using factored LMs in singlefactored SMT to English. The multi-factored approach to SMT of Moses is however more general. Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovi´c and Ney, 2004)) or machine translation quality (Nießen and Ney (2001), Koehn and Knight (2003), Zo"
W07-0735,P98-1035,0,0.0368632,"en target verb forms. The T+T+G scenario allows a similar extension if the described generation step is replaced by a (probabilistic) morphological generator. Nguyen and Shimazu (2006) translate from English to Vietnamese but the morphological richness of Vietnamese is comparable to English. In fact the Vietnamese vocabulary size is even smaller than English vocabulary size in one of their corpora. The observed improvement due to explicit modelling of morphology might not scale up beyond small-data setting. As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve 238 clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages. 9 Conclusion We experimented with multi-factored phrase-based translation aimed at improving morphological coherence in MT output. We varied the setup of additional factors (translation scenario) and the level of detail in morphological tags. Our results on English-to-Czech translation demonstrate significant improvement in BLEU scores by explicit modelling of mo"
W07-0735,P05-1033,0,0.0112488,"n and Shimazu (2006) translate from English to Vietnamese but the morphological richness of Vietnamese is comparable to English. In fact the Vietnamese vocabulary size is even smaller than English vocabulary size in one of their corpora. The observed improvement due to explicit modelling of morphology might not scale up beyond small-data setting. As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve 238 clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages. 9 Conclusion We experimented with multi-factored phrase-based translation aimed at improving morphological coherence in MT output. We varied the setup of additional factors (translation scenario) and the level of detail in morphological tags. Our results on English-to-Czech translation demonstrate significant improvement in BLEU scores by explicit modelling of morphology and using a separate morphological language model to ensure the coherence. To our knowledge, this is one of the first experiments showing the a"
W07-0735,cmejrek-etal-2004-prague,0,0.0400936,"Missing"
W07-0735,W06-3102,0,0.010838,"recently documented minor BLEU improvement using factored LMs in singlefactored SMT to English. The multi-factored approach to SMT of Moses is however more general. Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovi´c and Ney, 2004)) or machine translation quality (Nießen and Ney (2001), Koehn and Knight (2003), Zollmann et al. (2006), among others, for various languages; Goldwater and McClosky (2005), Bojar et al. (2006) and Talbot and Osborne (2006) for Czech), however, they focus on translating from the highly inflectional language. Durgar El-Kahlout and Oflazer (2006) report preliminary experiments in English to Turkish singlefactored phrase-based translation, gaining significant improvements by splitting root words and their morphemes into a sequence of tokens. In might be interesting to explore multi-factored scenarios for different Turkish morphology representation suggested the paper. de Gispert et al. (2005) generalize over verb forms and generate phrase translations even for unseen target verb forms. The T+T+G scenario allows a similar extension if the described generation step is replaced by a (probabilistic) morphological generator. Nguyen and Shim"
W07-0735,P03-2041,0,0.0219834,"enerator. Nguyen and Shimazu (2006) translate from English to Vietnamese but the morphological richness of Vietnamese is comparable to English. In fact the Vietnamese vocabulary size is even smaller than English vocabulary size in one of their corpora. The observed improvement due to explicit modelling of morphology might not scale up beyond small-data setting. As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve 238 clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages. 9 Conclusion We experimented with multi-factored phrase-based translation aimed at improving morphological coherence in MT output. We varied the setup of additional factors (translation scenario) and the level of detail in morphological tags. Our results on English-to-Czech translation demonstrate significant improvement in BLEU scores by explicit modelling of morphology and using a separate morphological language model to ensure the coherence. To our knowledge, this is one of the first experiment"
W07-0735,H05-1085,0,0.0841942,"lar to our T+C scenario. Given the small differences in all T+. . . scenarios’ performance, class-based LM might bring equivalent improvement. Yang and Kirchhoff (2006) have recently documented minor BLEU improvement using factored LMs in singlefactored SMT to English. The multi-factored approach to SMT of Moses is however more general. Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovi´c and Ney, 2004)) or machine translation quality (Nießen and Ney (2001), Koehn and Knight (2003), Zollmann et al. (2006), among others, for various languages; Goldwater and McClosky (2005), Bojar et al. (2006) and Talbot and Osborne (2006) for Czech), however, they focus on translating from the highly inflectional language. Durgar El-Kahlout and Oflazer (2006) report preliminary experiments in English to Turkish singlefactored phrase-based translation, gaining significant improvements by splitting root words and their morphemes into a sequence of tokens. In might be interesting to explore multi-factored scenarios for different Turkish morphology representation suggested the paper. de Gispert et al. (2005) generalize over verb forms and generate phrase translations even for unse"
W07-0735,P98-1080,0,0.077361,"Missing"
W07-0735,E03-1076,0,0.0300543,"ed LMs (Brown et al., 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario. Given the small differences in all T+. . . scenarios’ performance, class-based LM might bring equivalent improvement. Yang and Kirchhoff (2006) have recently documented minor BLEU improvement using factored LMs in singlefactored SMT to English. The multi-factored approach to SMT of Moses is however more general. Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovi´c and Ney, 2004)) or machine translation quality (Nießen and Ney (2001), Koehn and Knight (2003), Zollmann et al. (2006), among others, for various languages; Goldwater and McClosky (2005), Bojar et al. (2006) and Talbot and Osborne (2006) for Czech), however, they focus on translating from the highly inflectional language. Durgar El-Kahlout and Oflazer (2006) report preliminary experiments in English to Turkish singlefactored phrase-based translation, gaining significant improvements by splitting root words and their morphemes into a sequence of tokens. In might be interesting to explore multi-factored scenarios for different Turkish morphology representation suggested the paper. de Gis"
W07-0735,koen-2004-pharaoh,0,0.405644,"he maximum entropy principle or optimized with respect to the final translation quality measure. Most of our features are phrase-based and we require all such features to operate synchronously on the segmentation sK 1 and independently of neighbouring segments. In other words, we restrict the form of phrase-based features to: hm (cI1 , f1J , sK 1 )= K X ˜ m (˜ h ck , f˜k ) steps produce conflicting values in an output factor are discarded. A MAPPING step from a subset of source factors S ⊆ {1 . . . F } to a subset of target factors T ⊆ {1 . . . C} is the standard phrase-based model (see e.g. (Koehn, 2004a)) and introduces a feature in the following form: ˜ map:S→T (˜ cTk ) h ck , f˜k ) = log p(f˜kS |˜ m The conditional probability of f˜kS , i.e. the phrase ˜ fk restricted to factors S, given c˜Tk , i.e. the phrase c˜k restricted to factors T is estimated from relative cT ) where cTk ) = N (f˜S , c˜T )/N (˜ frequencies: p(f˜kS |˜ S T N (f˜ , c˜ ) denotes the number of co-occurrences of a phrase pair (f˜S , c˜T ) that are consistent with the word alignment. The marginal count N (˜ cT ) is the number of occurrences of the target phrase c˜T in the training corpus. For each mapping step, the model"
W07-0735,W04-3250,0,0.411438,"he maximum entropy principle or optimized with respect to the final translation quality measure. Most of our features are phrase-based and we require all such features to operate synchronously on the segmentation sK 1 and independently of neighbouring segments. In other words, we restrict the form of phrase-based features to: hm (cI1 , f1J , sK 1 )= K X ˜ m (˜ h ck , f˜k ) steps produce conflicting values in an output factor are discarded. A MAPPING step from a subset of source factors S ⊆ {1 . . . F } to a subset of target factors T ⊆ {1 . . . C} is the standard phrase-based model (see e.g. (Koehn, 2004a)) and introduces a feature in the following form: ˜ map:S→T (˜ cTk ) h ck , f˜k ) = log p(f˜kS |˜ m The conditional probability of f˜kS , i.e. the phrase ˜ fk restricted to factors S, given c˜Tk , i.e. the phrase c˜k restricted to factors T is estimated from relative cT ) where cTk ) = N (f˜S , c˜T )/N (˜ frequencies: p(f˜kS |˜ S T N (f˜ , c˜ ) denotes the number of co-occurrences of a phrase pair (f˜S , c˜T ) that are consistent with the word alignment. The marginal count N (˜ cT ) is the number of occurrences of the target phrase c˜T in the training corpus. For each mapping step, the model"
W07-0735,2005.mtsummit-papers.11,0,0.0137332,"n. Additional annotation of input and output tokens (multiple factors) is used to explicitly model morphology. We vary the translation scenario (the setup of multiple factors) and the amount of information in the morphological tags. Experimental results demonstrate significant improvement of translation quality in terms of BLEU. 1.1 1 Introduction Statistical phrase-based machine translation (SMT) systems currently achieve top performing results.1 Known limitations of phrase-based SMT include worse quality when translating to morphologically rich languages as opposed to translating from them (Koehn, 2005). One of the teams at the 2006 summer engineering workshop at Johns Hopkins University2 attempted to tackle these problems by introducing separate FACTORS in SMT input and/or output to allow explicit modelling of the underlying language structure. The support for factored translation models was incorporated into the Moses opensource SMT system3 . In this paper, we report on experiments with English-to-Czech multi-factor translation. After a brief overview of factored SMT and our data (Sections 2 and 3), we summarize some possible translating scenarios in Section 4. Section 5 studies the 1 http"
W07-0735,W01-1407,0,0.110562,"ated Research Class-based LMs (Brown et al., 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario. Given the small differences in all T+. . . scenarios’ performance, class-based LM might bring equivalent improvement. Yang and Kirchhoff (2006) have recently documented minor BLEU improvement using factored LMs in singlefactored SMT to English. The multi-factored approach to SMT of Moses is however more general. Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovi´c and Ney, 2004)) or machine translation quality (Nießen and Ney (2001), Koehn and Knight (2003), Zollmann et al. (2006), among others, for various languages; Goldwater and McClosky (2005), Bojar et al. (2006) and Talbot and Osborne (2006) for Czech), however, they focus on translating from the highly inflectional language. Durgar El-Kahlout and Oflazer (2006) report preliminary experiments in English to Turkish singlefactored phrase-based translation, gaining significant improvements by splitting root words and their morphemes into a sequence of tokens. In might be interesting to explore multi-factored scenarios for different Turkish morphology representation su"
W07-0735,J03-1002,0,0.0015075,"for the SMT workshop4 of the ACL 2007 conference.5 The Czech part of the corpus was tagged and lemmatized using the tool by Hajiˇc and Hladk´a (1998), the English part was tagged MXPOST (Ratnaparkhi, 1996) and lemmatized using the Morpha tool (Minnen et al., 2001). After some final cleanup, the corpus consists of 55,676 pairs of sentences (1.1M Czech tokens and 1.2M English tokens). We use the designated additional tuning and evaluation sections consisting of 1023, resp. 964 sentences. In all experiments, word alignment was obtained using the grow-diag-final heuristic for symmetrizing GIZA++ (Och and Ney, 2003) alignments. To reduce data sparseness, the English text was lowercased and Czech was lemmatized for alignment estimation. Language models are based on the target 4 http://www.statmt.org/wmt07/ Our preliminary experiments with the Prague Czechˇ English Dependency Treebank, PCEDT v.1.0 (Cmejrek et al., 2004), 20k sentences, gave similar results, although with a lower level of significance due to a smaller evaluation set. 5 234 3.1 Evaluation Measure and MERT We evaluate our experiments using the (lowercase, tokenized) BLEU metric and estimate the empirical confidence using the bootstrapping met"
W07-0735,P03-1021,0,0.0354242,"Missing"
W07-0735,P02-1040,0,0.0857028,"/moses/ 2 Motivation for Improving Morphology Czech is a Slavic language with very rich morphology and relatively free word order. The Czech morphological system (Hajiˇc, 2004) defines 4,000 tags in theory and 2,000 were actually seen in a big tagged corpus. (For comparison, the English Penn Treebank tagset contains just about 50 tags.) In our parallel corpus (see Section 3 below), the English vocabulary size is 35k distinct token types but more than twice as big in Czech, 83k distinct token types. To further emphasize the importance of morphology in MT to Czech, we compare the standard BLEU (Papineni et al., 2002) of a baseline phrasebased translation with BLEU which disregards word forms (lemmatized MT output is compared to lemmatized reference translation). The theoretical margin for improving MT quality is about 9 BLEU points: the same MT output scores 12 points in standard BLEU and 21 points in lemmatized BLEU. 2 Overview of Factored SMT In statistical MT, the goal is to translate a source (foreign) language sentence f1J = f1 . . . fj . . . fJ into a target language (Czech) sentence cI1 = c1 . . . cj . . . cI . In phrase-based SMT, the assumption is made that the target sentence can be constructed"
W07-0735,C04-1045,0,0.129126,"Missing"
W07-0735,P06-1122,0,0.0488644,"all T+. . . scenarios’ performance, class-based LM might bring equivalent improvement. Yang and Kirchhoff (2006) have recently documented minor BLEU improvement using factored LMs in singlefactored SMT to English. The multi-factored approach to SMT of Moses is however more general. Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovi´c and Ney, 2004)) or machine translation quality (Nießen and Ney (2001), Koehn and Knight (2003), Zollmann et al. (2006), among others, for various languages; Goldwater and McClosky (2005), Bojar et al. (2006) and Talbot and Osborne (2006) for Czech), however, they focus on translating from the highly inflectional language. Durgar El-Kahlout and Oflazer (2006) report preliminary experiments in English to Turkish singlefactored phrase-based translation, gaining significant improvements by splitting root words and their morphemes into a sequence of tokens. In might be interesting to explore multi-factored scenarios for different Turkish morphology representation suggested the paper. de Gispert et al. (2005) generalize over verb forms and generate phrase translations even for unseen target verb forms. The T+T+G scenario allows a s"
W07-0735,P02-1039,0,0.0252937,"abilistic) morphological generator. Nguyen and Shimazu (2006) translate from English to Vietnamese but the morphological richness of Vietnamese is comparable to English. In fact the Vietnamese vocabulary size is even smaller than English vocabulary size in one of their corpora. The observed improvement due to explicit modelling of morphology might not scale up beyond small-data setting. As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve 238 clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages. 9 Conclusion We experimented with multi-factored phrase-based translation aimed at improving morphological coherence in MT output. We varied the setup of additional factors (translation scenario) and the level of detail in morphological tags. Our results on English-to-Czech translation demonstrate significant improvement in BLEU scores by explicit modelling of morphology and using a separate morphological language model to ensure the coherence. To our knowledge, this is one of the f"
W07-0735,E06-1006,0,0.0408663,"g a dummy token for all other words did not bring any improvement over the baseline. A possible reason is that we employed only a standard 7-gram language model to this factor. A more appropriate treatment is to disregard the dummy tokens in the language model at all and use an n-gram language model that looks at last n − 1 non-dummy items. 8 Related Research Class-based LMs (Brown et al., 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario. Given the small differences in all T+. . . scenarios’ performance, class-based LM might bring equivalent improvement. Yang and Kirchhoff (2006) have recently documented minor BLEU improvement using factored LMs in singlefactored SMT to English. The multi-factored approach to SMT of Moses is however more general. Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovi´c and Ney, 2004)) or machine translation quality (Nießen and Ney (2001), Koehn and Knight (2003), Zollmann et al. (2006), among others, for various languages; Goldwater and McClosky (2005), Bojar et al. (2006) and Talbot and Osborne (2006) for Czech), however, they focus on translating from the highly inflectional language. D"
W07-0735,N06-2051,0,0.0172625,"2) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario. Given the small differences in all T+. . . scenarios’ performance, class-based LM might bring equivalent improvement. Yang and Kirchhoff (2006) have recently documented minor BLEU improvement using factored LMs in singlefactored SMT to English. The multi-factored approach to SMT of Moses is however more general. Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovi´c and Ney, 2004)) or machine translation quality (Nießen and Ney (2001), Koehn and Knight (2003), Zollmann et al. (2006), among others, for various languages; Goldwater and McClosky (2005), Bojar et al. (2006) and Talbot and Osborne (2006) for Czech), however, they focus on translating from the highly inflectional language. Durgar El-Kahlout and Oflazer (2006) report preliminary experiments in English to Turkish singlefactored phrase-based translation, gaining significant improvements by splitting root words and their morphemes into a sequence of tokens. In might be interesting to explore multi-factored scenarios for different Turkish morphology representation suggested the paper. de Gispert et al. (2005) gener"
W07-0735,W96-0213,0,\N,Missing
W07-0735,C98-1035,0,\N,Missing
W07-0735,C98-1077,0,\N,Missing
W07-0735,2006.amta-papers.16,0,\N,Missing
W08-0319,bojar-etal-2008-czeng,1,0.701467,"Missing"
W08-0319,W07-0735,1,0.91216,"ectly to produce a grammatical sentence and preserve the expressed relations between elements in the sentence, e.g. verbs and their modifiers. This year, we have taken two radically different approaches to English-to-Czech MT. Section 2 describes our setup of the phrase-based system Moses (Koehn et al., 2007) and Section 3 focuses on a system with probabilistic tree transfer employed at a deep syntactic layer and the new challenges this approach brings. ∗ The work on this project was supported by the grants FP6ˇ IST-5-034291-STP (EuroMatrix), MSM0021620838, MSMT ˇ LC536, and GA405/06/0589. CR Bojar (2007) describes various experiments with factored translation to Czech aimed at improving target-side morphology. We use essentially the same setup with some cleanup and significantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We bas"
W08-0319,A00-1031,0,0.013451,"and significantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We based the word alignment on Czech and English lemmas (base forms of words) as provided by the combination of taggers and lemmatizers by Hajiˇc (2004) for Czech and Brants (2000) followed by Minnen et al. (2001) for English. We symmetrized the two GIZA++ runs using grow-diag-final heuristic. Truecasing. We attempted to preserve meaningbearing case distinctions. The Czech lemmatizer produces case-sensitive lemmas and thus makes it easy to cast the capitalization of the lemma back on the word form.1 For English we approximate the same effect by a two-step procedure.2 1 We change the capitalization of the form to match the lemma in cases where the lemma is lowercase, capitalized (ucfirst) or all-caps. For mixed-case lemmas, we keep the form intact. 2 We first collect a l"
W08-0319,J92-4003,0,0.0509682,"Missing"
W08-0319,P96-1025,0,0.0155031,"sis, both formally captured as labelled ordered dependency trees: the ANALYTICAL (a-, surface syntax) representation bears a 1-1 correspondence between tokens in the sentence and nodes in the tree; the TECTOGRAMMATICAL (t-, deep syntax) representation contains nodes only for autosemantic words and adds nodes for elements not expressed on the surface but required by the grammar (e.g. dropped pronouns). We use the following tools to automatically annoˇ ska, tate plaintext up to the t-layer: (1) TextSeg (Ceˇ 2006) for tokenization, (2) tagging and lemmatization see above, (3) parsing to a-layer: Collins (1996) followed by head-selection rules for English, McDonald and others (2005) for Czech, (4) parsing to tˇ layer: Zabokrtsk´ y (2008) for English, Klimeˇs (2006) for Czech. 3.2 Probabilistic Tree Transfer The transfer step is based on Synchronous Tree Subˇ stitution Grammars (STSG), see Bojar and Cmejrek (2007) for a detailed explanation. The essence is a log-linear model to search for the most likely synchronous derivation δˆ of the source T1 and target T2 dependency trees: δˆ = exp argmax 3 MT with a Deep Syntactic Transfer Theoretical Background said Figure 1: Sample treelet pair, a-layer. δ s."
W08-0319,P05-1067,0,0.0442,"ses, either. Lack of n-gram LM in the (deterministic) generation procedures from a t-tree. While we support final LM-based rescoring, there is too little variance in n-best lists due to the explosion mentioned above. Too many model parameters given our stack limit. We use identical MERT implementation to optimize λm s but in the large space of hypotheses, MERT does not converge. 3.3.2 Related Research Our approach should not be confused with the ˇ TectoMT submission by Zdenˇek Zabokrtsk´ y with a deterministic transfer: heuristics fully exploiting the similarity of English and Czech t-layers. Ding and Palmer (2005) improve over word-based MT baseline with a formalism very similar to STSG. Though not explicitly stated, they seem not to encode frontiers in the treelets and allow for adjunction (adding siblings), like Quirk et al. (2005), which significantly reduces data sparseness. Riezler and III (2006) report an improvement in MT grammaticality on a very restricted test set: short sentences parsable by an LFG grammar without back-off rules. 4 Conclusion We have presented our best-performing factored phrase-based English-to-Czech translation and a highly experimental complex system with treebased transfe"
W08-0319,2006.amta-papers.8,0,0.016828,"based MT on WMT07 DevTest. Moses Moses, CzEng data only etct, TectoMT annotation WMT07 DevTest 14.9±0.9 13.9±0.9 4.7±0.5 WMT08 NC Test News Test 16.4±0.6 12.3±0.6 15.2±0.6 10.0±0.5 4.9±0.3 3.3±0.3 Table 2: WMT08 shared task BLEU scores. rules for t-layer parsing and generation instead of ˇ Klimeˇs (2006) and (Pt´acˇ ek and Zabokrtsk´ y, 2006). 3.3.1 Discussion Our syntax-based approach does not reach scores of phrase-based MT due to the following reasons: Cumulation of errors at every step of analysis. Data loss due to incompatible parses and node alignment. Unlike e.g. Quirk et al. (2005) or Huang et al. (2006) who parse only one side and project the structure, we parse both languages independently. Natural divergence and random errors in either of the parses and/or the alignment prevent us from extracting many treelet pairs. Combinatorial explosion in target node attributes. Currently, treelet options are fully built in advance. Uncertainty in the many t-node attributes leads to too many insignificant variations while e.g. different lexical choices are pushed off the stack. While vital for final sentence generation (see Table 1), fine-grained t-node attributes should be produced only once all key s"
W08-0319,D07-1091,0,0.0615314,"d right away or a binode model promoting likely combinations of the governor g(e) and the child c(e) of an edge e ∈ T2 : hbinode (δ) = log Y p(c(e) |g(e)) (3) e∈T2 The probabilistic dictionary of aligned treelet pairs is extracted from node-aligned (GIZA++ on linearized trees) parallel automatic treebank as in Moses’ training: all treelet pairs compatible with the node alignment. 3.2.1 Factored Treelet Translation Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes representing various linguistic features.3 We can consider the attributes as individual factors (Koehn and Hoang, 2007). This allows us to condition the translation choice on a subset of source factors only. In order to generate a value for each target-side factor, we use a sequence of mapping steps similar to Koehn and Hoang (2007). For technical reasons, our current implementation allows to generate factored targetside only when translating a single node to a single node, i.e. preserving the tree structure. In our experiments we used 8 source (English) tnode attributes and 14 target (Czech) attributes. 3.3 Recent Experimental Results Table 1 shows BLEU scores for various configurations of our decoder. The ab"
W08-0319,P07-2045,1,0.0205542,"d right away or a binode model promoting likely combinations of the governor g(e) and the child c(e) of an edge e ∈ T2 : hbinode (δ) = log Y p(c(e) |g(e)) (3) e∈T2 The probabilistic dictionary of aligned treelet pairs is extracted from node-aligned (GIZA++ on linearized trees) parallel automatic treebank as in Moses’ training: all treelet pairs compatible with the node alignment. 3.2.1 Factored Treelet Translation Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes representing various linguistic features.3 We can consider the attributes as individual factors (Koehn and Hoang, 2007). This allows us to condition the translation choice on a subset of source factors only. In order to generate a value for each target-side factor, we use a sequence of mapping steps similar to Koehn and Hoang (2007). For technical reasons, our current implementation allows to generate factored targetside only when translating a single node to a single node, i.e. preserving the tree structure. In our experiments we used 8 source (English) tnode attributes and 14 target (Czech) attributes. 3.3 Recent Experimental Results Table 1 shows BLEU scores for various configurations of our decoder. The ab"
W08-0319,H05-1066,1,0.769471,"Missing"
W08-0319,C00-2163,0,0.0100666,"ˇ IST-5-034291-STP (EuroMatrix), MSM0021620838, MSMT ˇ LC536, and GA405/06/0589. CR Bojar (2007) describes various experiments with factored translation to Czech aimed at improving target-side morphology. We use essentially the same setup with some cleanup and significantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We based the word alignment on Czech and English lemmas (base forms of words) as provided by the combination of taggers and lemmatizers by Hajiˇc (2004) for Czech and Brants (2000) followed by Minnen et al. (2001) for English. We symmetrized the two GIZA++ runs using grow-diag-final heuristic. Truecasing. We attempted to preserve meaningbearing case distinctions. The Czech lemmatizer produces case-sensitive lemmas and thus makes it easy to cast the capitalization of the lemma back on the word form.1 For English we approxim"
W08-0319,P03-1021,0,0.0173091,"weighted language models for the target (Czech) side: • 3-grams of word forms based on all CzEng 0.7 data, 15M tokens, • 3-grams of word forms in Project Syndicate section of CzEng (in-domain for WMT07 and WMT08 NC-test set), 1.8M tokens, • 4-grams of word forms based on Czech National Corpus (Kocek et al., 2000), version SYN2006, 365M tokens, • three models of 7-grams of morphological tags from the same sources. Lexicalized reordering using the monotone/swap/discontinuous bidirectional model based on both source and target word forms. MERT. We use the minimum-error rate training procedure by Och (2003) as implemented in the Moses toolkit to set the weights of the various translation and language models, optimizing for BLEU. Final detokenization is a simple rule-based procedure based on Czech typographical conventions. Finally, we capitalize the beginnings of sentences. See BLEU scores in Table 2 below. Pred VP = Sb uvedla , zˇ e Pred Czech has a well-established theory of linguistic analysis called Functional Generative Description (Sgall et al., 1986) supported by a big treebanking enterprise (Hajiˇc and others, 2006) and on-going adaptations for other languages including English (Cinkov´a"
W08-0319,P05-1034,0,0.258943,"BLEU scores for syntax-based MT on WMT07 DevTest. Moses Moses, CzEng data only etct, TectoMT annotation WMT07 DevTest 14.9±0.9 13.9±0.9 4.7±0.5 WMT08 NC Test News Test 16.4±0.6 12.3±0.6 15.2±0.6 10.0±0.5 4.9±0.3 3.3±0.3 Table 2: WMT08 shared task BLEU scores. rules for t-layer parsing and generation instead of ˇ Klimeˇs (2006) and (Pt´acˇ ek and Zabokrtsk´ y, 2006). 3.3.1 Discussion Our syntax-based approach does not reach scores of phrase-based MT due to the following reasons: Cumulation of errors at every step of analysis. Data loss due to incompatible parses and node alignment. Unlike e.g. Quirk et al. (2005) or Huang et al. (2006) who parse only one side and project the structure, we parse both languages independently. Natural divergence and random errors in either of the parses and/or the alignment prevent us from extracting many treelet pairs. Combinatorial explosion in target node attributes. Currently, treelet options are fully built in advance. Uncertainty in the many t-node attributes leads to too many insignificant variations while e.g. different lexical choices are pushed off the stack. While vital for final sentence generation (see Table 1), fine-grained t-node attributes should be produ"
W08-0319,N06-1032,0,0.311055,"Missing"
W09-0422,bojar-etal-2008-czeng,1,0.888991,"Missing"
W09-0422,W08-0309,0,0.0625391,"ion of prepositional group would be difficult otherwise. After the capitalization of the beginning of each sentence (and each named entity instance), we obtain the final translation by flattening the surface tree. Table 3 reports lowercase BLEU and NIST scores and preliminary manual ranks of our submissions in contrast with other systems participating in English→Czech translation, as evaluated on the official WMT09 unseen test set. Note that automatic metrics are known to correlate quite poorly with human judgements, see the best ranking but “lower scoring” PC Translator this year and also in Callison-Burch et al. (2008). System BLEU Moses T 14.24 Moses T+C 13.86 Google 13.59 U. of Edinburgh 13.55 Moses T+C+C&T+T+G 84k 10.01 Eurotran XP 09.51 PC Translator 09.42 TectoMT 07.29 NIST 5.175 5.110 4.964 5.039 4.360 4.381 4.335 4.173 Rank -3.02 (4) – -2.82 (3) -3.24 (5) -2.81 (2) -2.77 (1) -3.35 (6) Table 3: Automatic scores and preliminary human rank for English→Czech translation. Systems in italics are provided for comparison only. Best results in bold. Unfortunately, this preliminary evaluation suggests that simpler models perform better, partly 4.4 Preliminary Error Analysis because it is easier to tune them pr"
W09-0422,P05-1045,0,0.0043121,"ons: 15 33 43 . 5 Later, we found out that the grow-diag-final-and heuristic provides insignificantly superior results. 4 ˇ In some previous experiments (e.g.Zabokrtsk´ y et al. (2008)), we used phrase-structure parser Collins (1999) with subsequent constituency-dependency conversion. 2 126 with the option to resort to (2) an independent translation of lemma→lemma and tag→tag finished by a generation step that combines target-side lemma and tag to produce the final target-side form. One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005). The nodes in the English t-layer are grouped according to the detected named entities and they are assigned the type of entity (location, person, or organization). This information is preserved in the transfer of the deep English trees to the deep Czech trees to allow for the appropriate capitalization of the Czech translation. We use three language models in this setup (3-grams of forms, 3-grams of lemmas, and 10-grams of tags). Due to the increased complexity of the setup, we were able to train this model on 84k parallel sentences only (the Commentaries section) and we use the target-side"
W09-0422,P07-2045,1,0.010104,"naming conventions. However, we were unable to reliably determine the series number and the episode number from the file names. We employed a two-step procedure to automatically pair the TV series subtitle files. For every TV series: We describe two systems for English-toCzech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach. 1 Introduction We participated in WMT09 with two very different systems: (1) a phrase-based MT based on Moses (Koehn et al., 2007) and tuned for English→Czech translation, and (2) a complex ˇ system in the TectoMT platform (Zabokrtsk´ y et al., 2008). 1. We clustered the files on both sides to remove duplicates 2. We found the best matching using a provisional translation dictionary. This proved to be a successful technique on a small sample of manually paired test data. The process was facilitated by the fact that the correct pairs of episodes usually share some named entities which the human translator chose to keep in the original English form. 2 Data 2.1 Monolingual Data Our Czech monolingual data consist of (1) the"
W09-0422,J03-1002,0,0.00228104,"0.4 Table 2: Czech-English data sizes and sources. ∗ The work on this project was supported by the grants MSM0021620838, 1ET201120505, 1ET101120503, GAUK ˇ ˇ LC536 and FP6-IST-5-034291-STP 52408/2008, MSMT CR (EuroMatrix). 1 www.opensubtitles.org and titulky.com Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125–129, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 125 2.3 Data Preprocessing using TectoMT platform: Analysis and Alignment also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och and Ney (2003). We use acquired aligned tectogrammatical trees for training some models for the transfer. As analysis of such amounts of data is obviously computationally very demanding, we run it in parallel using Sun Grid Engine3 cluster of 40 4-CPU computers. For this purpose, we implemented a rather generic tool that submits any TectoMT pipeline to the cluster. As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data. The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0"
W09-0422,W07-1709,0,0.170323,"Missing"
W09-0422,W08-0325,1,0.861517,"emented a rather generic tool that submits any TectoMT pipeline to the cluster. As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data. The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0, Hajiˇc and others (2006)), i.e. we used three layers of sentence representation: morphological layer, surface-syntax layer (called analytical (a-) layer), and deep-syntax layer (called tectogrammatical (t-) layer). The analysis was implemented using TectoMT, ˇ (Zabokrtsk´y et al., 2008). TectoMT is a highly modular software framework aimed at creating MT systems (focused, but by far not limited to translation using tectogrammatical transfer) and other NLP applications. Numerous existing NLP tools such as taggers, parsers, and named entity recognizers are already integrated in TectoMT, especially for (but again, not limited to) English and Czech. During the analysis of the large Czech monolingual data, we used Jan Hajiˇc’s Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set ˇ of features as described in Nov´ak and Zabokrt"
W09-0422,J03-4003,0,\N,Missing
W09-0422,H05-1066,0,\N,Missing
W09-0422,W08-0319,1,\N,Missing
W09-0422,2008.eamt-1.16,1,\N,Missing
W10-1705,W07-0734,0,0.0261528,"gments of MT quality. Kos and Bojar (2009) introduced SemPOS, a metric that performs much better in terms of correlation to human judgments when translating to Czech. Naturally, we wanted to optimize towards SemPOS. SemPOS computes the overlapping of autosemantic (content-bearing) word lemmas in the candidate and reference translations given a finegrained semantic part of speech (sempos9 ), as defined in Hajiˇc et al. (2006), and outputs average overlapping score over all sempos types. The SemPOS metric outperformed common metrics as BLEU, TER (Snover et al., 2006) or an adaptation of Meteor (Lavie and Agarwal, 2007) for Czech on test sets from WMT08 (CallisonBurch et al., 2008). 4.1 BLEU 10.11±0.40 9.53±0.39 9.46±0.37 8.20±0.37 6.96±0.33 MERT loop: • indeed apply TectoMT processing to the n-best list at each iteration (parallelized to 15 CPUs), • apply TectoMT to the training data, express the (deep) lemma and sempos as additional factors using a blank value for auxiliary words, and using Moses factored translation to translate from English forms to triplets of Czech form, deep lemma and sempos. Table 7 lists several ZMERT runs when optimizing a simple form→form phrase-based model (small data setting) to"
W10-1705,P03-1021,0,0.0367222,"than in the simple model. 63 be still improved, and more importantly, the second phase of monotone decoding could be handled by a more appropriate model capable of including more additional (source) context features.8 TectoMT in MERT Factored translation 4 Optimizing towards SemPOS SemPOS 29.69 29.69 29.36 29.68 27.79 Iters 20 10 10 9 Time 2d12.0h 1d12.0h 2.4h 1.7h Table 7: Five independent MERT runs optimizing towards SemPOS with semantic parts of speech and lemmas provided either by TectoMT on the fly or by Moses factored translation. In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. In the standard implementation in Moses, BLEU (Papineni et al., 2002) is used as the objective function, despite its rather disputable correlation with human judgments of MT quality. Kos and Bojar (2009) introduced SemPOS, a metric that performs much better in terms of correlation to human judgments when translating to Czech. Naturally, we wanted to optimize towards SemPOS. SemPOS computes the overlapping of autosemantic (content-bearing) word lemmas in the candidate and reference translations given a finegrained semantic part of speech (sempos9 ), as"
W10-1705,P02-1040,0,0.086078,"decoding could be handled by a more appropriate model capable of including more additional (source) context features.8 TectoMT in MERT Factored translation 4 Optimizing towards SemPOS SemPOS 29.69 29.69 29.36 29.68 27.79 Iters 20 10 10 9 Time 2d12.0h 1d12.0h 2.4h 1.7h Table 7: Five independent MERT runs optimizing towards SemPOS with semantic parts of speech and lemmas provided either by TectoMT on the fly or by Moses factored translation. In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. In the standard implementation in Moses, BLEU (Papineni et al., 2002) is used as the objective function, despite its rather disputable correlation with human judgments of MT quality. Kos and Bojar (2009) introduced SemPOS, a metric that performs much better in terms of correlation to human judgments when translating to Czech. Naturally, we wanted to optimize towards SemPOS. SemPOS computes the overlapping of autosemantic (content-bearing) word lemmas in the candidate and reference translations given a finegrained semantic part of speech (sempos9 ), as defined in Hajiˇc et al. (2006), and outputs average overlapping score over all sempos types. The SemPOS metric"
W10-1705,W09-0422,1,0.86434,"Missing"
W10-1705,2008.amta-srw.6,0,0.0347558,"Missing"
W10-1705,W07-0735,1,0.88458,"for your training, the figures still underpin the issue of sparse data in Czech-English translation. 3.1 Two-Step Translation In order to avoid the explosion of the translation options6 , we experimented with two-step translation. The first step translates from English to lemmatized Czech augmented to preserve important semantic properties known from the source phrase. The second step is a monotone translation from the lemmas to fully inflected Czech. The idea behind the delimitation is that all the morphological properties of Czech words that can be established 3 Targetting Czech Word Forms Bojar (2007) experimented with several translation scenarios, including what we will call MorphG, i.e. the independent translation of lemma to lemma and tag to tag followed by a generation step to produce target-side word form. With the small training set available then, the MorphG model performed equally well as a simpler direct translation followed by target-side tagging and an additional n-gram model over morphological tags. Koehn and Hoang (2007) reports even a large loss with MorphG for German-to-English if the alternative 6 and also motivated when we noticed that reading MT output to lemmatized Czec"
W10-1705,2006.amta-papers.25,0,0.0469969,"its rather disputable correlation with human judgments of MT quality. Kos and Bojar (2009) introduced SemPOS, a metric that performs much better in terms of correlation to human judgments when translating to Czech. Naturally, we wanted to optimize towards SemPOS. SemPOS computes the overlapping of autosemantic (content-bearing) word lemmas in the candidate and reference translations given a finegrained semantic part of speech (sempos9 ), as defined in Hajiˇc et al. (2006), and outputs average overlapping score over all sempos types. The SemPOS metric outperformed common metrics as BLEU, TER (Snover et al., 2006) or an adaptation of Meteor (Lavie and Agarwal, 2007) for Czech on test sets from WMT08 (CallisonBurch et al., 2008). 4.1 BLEU 10.11±0.40 9.53±0.39 9.46±0.37 8.20±0.37 6.96±0.33 MERT loop: • indeed apply TectoMT processing to the n-best list at each iteration (parallelized to 15 CPUs), • apply TectoMT to the training data, express the (deep) lemma and sempos as additional factors using a blank value for auxiliary words, and using Moses factored translation to translate from English forms to triplets of Czech form, deep lemma and sempos. Table 7 lists several ZMERT runs when optimizing a simple"
W10-1705,W08-0309,0,0.0948923,"Missing"
W10-1705,D07-1103,0,0.0260003,"ven the little or no improvements achieved by the many configurations we tried, our English-toCzech primary submission is rather simple: • Standard GIZA++ word alignment based on both source and target lemmas. • Two alternative decoding paths; forms always truecased: form+tag→form & form→form. The first path is more specific and helps to preserve core syntactic elements in the sentence. Without the tag, ambiguous English words could often all translate as e.g. nouns, leading to no verb in the Czech sentence. The default path serves as a back-off. • Significance filtering of the phrase tables (Johnson et al., 2007) implemented for Moses by Chris Dyer; default settings of filter value a+e and the cut-off 30. • Two separate 5-gram Czech LMs of truecased forms each of which interpolates models trained on the following datasets; the interpolation weights were set automatically using SRILM (Stolcke, 2002) based on the target side of 11 The subsequent MERT training using the same development test may suffer from overestimating the language model weights, but we did not observe the issue, possibly due to only moderate overlap of the datasets. 12 We attempted to use a second LM trained on English Gigaword by Ch"
W10-1705,D07-1091,0,0.6836,"ut they were able to use only 84k sentences. For the full training set of 2.2M sentences, the model was too big to fit in reasonable disk limits. More importantly, already in the small data setting, the complex model suffered from little stability due to abundance of features (5 features per phrasetable plus tree features for three LMs), so nearly the same performance on the development set gave largely varying quality on the independent test set. The most important issue of the MorphG setup, however, is the explosion of translation options. Due to the “synchronous factors” approach of Moses (Koehn and Hoang, 2007), all translation options have to be fully constructed before the main search begins. The MorphG model however licenses too many possible combinations of lemmas, tags and final word forms, so the pruning of translation options strikes hard, causing search errors. For more details, see Bojar et al. (2009a) where a similar issue occurs for treeletbased translation. 40 0.4 2.0 3.5 5.6 8.6 9.4 Table 4: Percentage of sentences reachable in Czech-to-English large setting, two alternative decoding paths to translate from Czech lemma if the form is not available in the translation table (BLEU score 18"
W10-1705,W04-3250,0,0.232953,"Missing"
W11-2101,J08-4004,0,0.0118721,"e lengths. We see that the interannotator agreement (when excluding references) is reasonably high only for sentences up to 10 words in length – according to Landis and Koch (1977), and as cited by the WMT overview paper, not even ‘moderate’ agreement can be assumed if κ is less than 0.4. Another popular (and controversial) rule of thumb (Krippendorff, 1980) is more strict and says that κ < 0.67 is not suitable even for tentative conclusions. 4 2.2.2 Estimating P (E), the Expected Agreement by Chance Several agreement measures (usually called kappas) were designed based on the Equation 1 (see Artstein and Poesio (2008) and Eugenio and Glass (2004) for an overview and a discussion). Those measures differ from each other in how to define the individual components of Equation 2, and hence differ in what the expected agreement by chance (P (E)) would be:8 • The S measure (Bennett et al., 1954) assumes a uniform distribution over the categories. • Scott’s π (Scott, 1955) estimates the distribution empirically from actual annotation. • Cohen’s κ (Cohen, 1960) estimates the distribution empirically as well, and further assumes a separate distribution for each annotator. Given that the WMT10 overview paper assumes"
W11-2101,W10-1703,1,0.863735,"Missing"
W11-2101,2010.eamt-1.12,0,0.0147419,"Missing"
W11-2101,vilar-etal-2006-error,0,0.00766891,"Missing"
W11-2108,W10-1705,1,0.840589,"ings in English. We see that the best correlation arises when no words are thrown away. One possible explanation is that auxiliary words are recognized by the morphological tag well enough anyway and stopwords lists remove also important content words, decreasing the overall accuracy of the overlapping. 6 Tunable Metric WMT11 Shared Task The goal of the tunable metric task in WMT11 was to use the custom metric in MERT optimization (Och, 2003). The target language was English. We choose APPROX + CAP - MICRO since this combination correlates best with human judgments. Based on the experience of Bojar and Kos (2010), we combine this metric with BLEU. In our opinion, the SemPOS metric and its variants alone are are good at comparing systems’ outputs where sentence fluency has been already ensured. On the other hand, they fail in ranking sentences in n-best lists 97 in MERT optimization because they observe only t-lemmas and don’t penalize wrong morphological forms of words. We thus use BLEU to establish sentence fluency and our metrics to prefer sentences with correctly translated content words. We have tried several weights for the linear combination of BLEU and the chosen approximation. See Table 7 for"
W11-2108,P10-2016,1,0.892114,"Missing"
W11-2108,W08-0309,0,0.165866,"Missing"
W11-2108,W10-1703,0,0.0572115,"WMT11). 2 Method of Evaluation Our primary objective is to create a good metric for automatic MT evaluation and possibly also tuning. We are not interested much in how close is our proposed approximation to the (automatic or manual) semposes and t-lemmas. Therefore, we evaluate only how well do our metrics (the pair of a chosen approximation and a chosen formula for the overlapping) correlate with human judgments. 2.1 Test Data We use the data collected during three Workshops on Statistical Machine Translation: WMT08 (CallisonBurch et al., 2008), WMT09 (Callison-Burch et al., 2009) and WMT10 (Callison-Burch et al., 2010). So far, we study only Czech and English as the target languages. Our test sets are summarized in Table 1: we have four sets with Czech as the target language and 16 sets with English as the target language. Each testset in each translation direction gives us for each sentence one hypothesis for each participating MT system. Human judges (repeatedly) ranked subsets of these hypotheses comparing at most 5 hypotheses at once and indicating some ordering of the hypotheses. The ordering may include ties. In WMT, these 5-fold rankings are interpreted as “simulated pairwise comparisons”: all pairwi"
W11-2108,W02-1001,0,0.0437609,"tion of the overlapping with human judgments and some lower it. We therefore try one more variant of the approximation which considers only (language-specific) subset of semposes. The approximation called APPROX - RESTR considers only these sempos tags in Czech: v, n.denot, adj.denot, n.pron.def.pers, n.pron.def.demon, adv.denot.ngrad.nneg, adv.denot.grad.nneg. The considered sempos tags for English are: v, n.denot, adj.denot, n.pron.indef. 3.4 T-lemma and Sempos Tagging Our last approximation method differs a lot from the previous three approximations. We use the sequence labeling algorithm (Collins, 2002) as implemented in Featurama2 to choose the t-lemma and sempos ˇ tag. The CzEng corpus (Bojar and Zabokrtsk´ y, 2009) serves to train two taggers: one for Czech and 2 http://sourceforge.net/projects/ featurama/ Tag v n.denot adj.denot n.pron.indef n.quant.def n.pron.def.pers adv.pron.indef adv.denot.grad.neg R. Fr. 0.236 0.506 0.124 0.019 0.039 0.068 0.005 0.003 Min. 0.403 0.189 0.264 0.224 -0.084 -0.500 -0.382 -1.000 Max. 1.000 1.000 0.964 1.000 0.893 0.975 1.000 0.904 Avg. 0.735 0.728 0.720 0.639 0.495 0.493 0.432 0.413 cial t-lemmas for negation and personal pronouns (“#Neg”, “#PersPron”)."
W11-2108,W07-0738,0,0.0999485,"Missing"
W11-2108,P03-1021,0,0.0206328,"arious stopwords list lengths for the approximation APPROX - STOPWORDS. Figure 5.1 shows the dependency of the correlation on stopwords list length for all overlappings in English. We see that the best correlation arises when no words are thrown away. One possible explanation is that auxiliary words are recognized by the morphological tag well enough anyway and stopwords lists remove also important content words, decreasing the overall accuracy of the overlapping. 6 Tunable Metric WMT11 Shared Task The goal of the tunable metric task in WMT11 was to use the custom metric in MERT optimization (Och, 2003). The target language was English. We choose APPROX + CAP - MICRO since this combination correlates best with human judgments. Based on the experience of Bojar and Kos (2010), we combine this metric with BLEU. In our opinion, the SemPOS metric and its variants alone are are good at comparing systems’ outputs where sentence fluency has been already ensured. On the other hand, they fail in ranking sentences in n-best lists 97 in MERT optimization because they observe only t-lemmas and don’t penalize wrong morphological forms of words. We thus use BLEU to establish sentence fluency and our metric"
W11-2108,P02-1040,0,0.0899321,"d: BOOST- MICRO (Equation 3), CAP MACRO (Equation 5), and CAP - MICRO (Equation 6). 5 Experiments Table 5 shows the results for English as the target language. The first two columns denote the combination of an approximation method and an overlapping formula. For conciseness, we report only the minimum, maximum and average value among correlations of all test sets. To compare metrics to original SemPOS, the table includes non-approximated variant ORIG where the t-lemmas and semposes are assigned by the TectoMT framework. For the purposes of comparison, we also report the correlations of BLEU (Papineni et al., 2002) and a linear combination of AP - 96 the name SEMPOS - BLEU since this metric was used in Tunable Metric Task (Section 6). The best performing metric is the combination of approximation APPROX and overlapping CAP MICRO . It actually slightly outperforms all nonapproximated metrics. In general, the reductions APPROX and ORIG combined with CAP - MICRO or CAP - MACRO perform very well. Reductions APPROX - STOPWORDS and APPROX - RESTR do not improve on APPROX. The TAGGER approximation correlates similarly to ORIG when micro-average is used. Table 6 contains the results for Czech as the target lang"
W11-2108,W08-0325,0,0.0934417,"Missing"
W11-2108,W09-0401,0,\N,Missing
W11-2138,W09-0432,0,0.0577992,"ss of surface forms, an issue typical for morphologically rich languages. When MT systems translate into such languages, the limited size of parallel data often causes the situation where the output should include a word form never observed in the training data. Even though the parallel data do contain the desired word ∗ This work has been supported by the grants EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of the Czech Republic), P406/10/P259, and MSM 0021620838. The idea of using monolingual data for improving the translation model has been explored in several previous works. Bertoldi and Federico (2009) used monolingual data for adapting existing translation models to translation of data from different domains. In their experiments, the most effective approach was to train a new translation model from “fake” parallel data consisting of target-side monolingual data and their machine translation into the source language by a baseline system. Ueffing et al. (2007) used a boot-strapping technique to extend translation models using monolingual data. They gradually translated additional source-side sentences and selectively incorporated them and their translations in the model. Our technique also"
W11-2138,W11-2123,0,0.0536074,"Missing"
W11-2138,D07-1091,0,0.0412273,"mall parallel corpus and large target-side monolingual texts. data and learn these forms statistically. We are therefore not limited to verbs, but our system is only able to generate surface forms observed in the target-side monolingual data. 3 Reverse Self-Training Figure 1 illustrates the core of the method. Using available parallel data, we first train an MT system to translate from the target to the source language. Since we want to gather new word forms from the monolingual data, this reverse model needs the ability to translate them. For that purpose we use a factored translation model (Koehn and Hoang, 2007) with two alternative decoding paths: form→form and back-off→form. We experimented with several options for the back-off (simple stemming by truncation or full lemmatization), see Section 4.4. The decoder can thus use a less sparse representation of words if their exact forms are not available in the parallel data. We use this reverse model to translate (much larger) target-side monolingual data into the source language. We preserve the word alignments of the phrases as used in the decoding so we directly obtain the word alignment in the new “parallel” corpus. This gives us enough information"
W11-2138,2009.mtsummit-papers.7,0,0.0241671,"Missing"
W11-2138,W04-3250,0,0.145361,"Missing"
W11-2138,P00-1056,0,0.185585,"Missing"
W11-2138,P03-1021,0,0.0141943,"vailable in the parallel data. We use this reverse model to translate (much larger) target-side monolingual data into the source language. We preserve the word alignments of the phrases as used in the decoding so we directly obtain the word alignment in the new “parallel” corpus. This gives us enough information to proceed with the standard MT system training – we extract and score the phrases consistent with the constructed word alignment and create the phrase table. We combine this enlarged translation model with a model trained on the true parallel data and use Minimum Error Rate Training (Och, 2003) to find the balance between the two models. The final model has four separate components – two language models (one trained on parallel and one on monolingual data) and the two translation models. We do not expect the translation quality to im331 prove simply because more data is included in training – by adding translations generated using known data, the model could gain only new combinations of known words. However, by using a back-off to less sparse units (e.g. lemmas) in the factored target→source translation, we enable the decoder to produce previously unseen surface forms. These transl"
W11-2138,P02-1040,0,0.079168,"Missing"
W11-2138,steinberger-etal-2006-jrc,0,\N,Missing
W11-2138,P07-2045,1,\N,Missing
W11-2152,W10-1705,1,0.760025,"decimal and thousand separators in numbers. While there are language-specific conventions, they are not always followed and the normalization can in such cases confuse the order of magnitude by 3. 427 the output based on non-normalized test sets as our primary English-to-Czech submission. We invested much less effort into the submission called CU - BOJAR for Czech-to-English. The only interesting feature there is the use of alternative decoding paths to translate either from the Czech form or from the Czech lemma equipped with meaningbearing morphological properties, e.g. the number of nouns. Bojar and Kos (2010) used the same setup with simple lemmas in the fallback decoding path. The enriched lemmas perform marginally better. 2.3 Two-step translation Our two-step translation is essentially the same setup as detailed by Bojar and Kos (2010): (1) the English source is translated to simplified Czech, and (2) the simplified Czech is monotonically translated to fully inflected Czech. Both steps are simple phrase-based models. Instead of word forms, the simplified Czech uses lemmas enriched by a subset of morphological features selected manually to encode only properties overt both in English and Czech su"
W11-2152,H05-1066,0,0.0673478,"Missing"
W11-2152,J03-1002,0,0.00353942,"ical example of a correction is the agreement between the subject and the predicate: they should share the morphological number and gender. If they do not, we simply change the number and gender of the predicate in agreement with the subject.4 An example of such a changed predicate is in Figure 1. Apart from the dependency tree of the target sentence, we can also use the dependency tree of the source sentence. Source sentences are grammatically correct and the accuracy of the tagger and the parser is accordingly higher there. Words in the source and target sentences are aligned using GIZA++5 (Och and Ney, 2003) but verbose outputs of the original MT systems would be possibly a better option. The rules for fixing grammatical agreement between words can thus consider also the dependency relations and morphological caregories of their English counterparts in the input sentence. 4 In this case, we suppose that the number of the subject has a much higher chance to be correct. 5 GIZA++ was run on lemmatized texts in both directions and intersection symmetrization was used. 428 . AuxK came Pred people pl Sb Some pl Atr later Adv . AuxK přišel lidé Někteří Pred sg, m Sb pl Atr později Adv přišli Pred pl Fig"
W11-2152,W07-1709,0,0.109407,"Missing"
W11-2152,N07-1064,0,\N,Missing
W12-3105,W10-1703,0,0.0513929,"Missing"
W12-3105,W11-2106,0,0.0901512,"Missing"
W12-3105,fishel-etal-2012-terra,1,0.86613,"Missing"
W12-3105,N06-1014,0,0.0173292,"Zeman et al., 2011) and Hjerson (Popovi´c and Ney, 2011) use different methods for automatic error analysis. Addicter explicitly aligns the hypothesis and reference translations and induces error categories based on the alignment coverage while Hjerson compares words encompassed in the WER (word error rate) and PER (position-independent word error rate) scores to the same end. Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al., 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair. Both tools use word lemmas for their analysis; we used TreeTagger (Schmid, 1995) for analyzing English, Spanish, German and French and Morˇce (Spoustov´a et al., 2007) to analyze Czech. The same tools are used for PoS-tagging in some experiments. 2.2 HYP -1 Binary Classification Pairwise comparison of sentence pairs is achieved with a binary SVM classifier, trained via sequential minimal optimization (Platt, 1998), implemented in Weka (Hall et al., 2009). The input feature vectors are comp"
W12-3105,P02-1040,0,0.0842937,"Missing"
W12-3105,W11-2111,0,0.0174685,"e scores remain to be checked against the human 69 judgments from WMT’12. The introduced TerrorCat metric has certain dependencies. For one thing, in order to apply it to new languages, a training set of manual rankings is required – although this can be viewed as an advantage, since it enables the user to tune the metric to his/her own preference. Additionally, the metric depends on lemmatization and PoS-tagging. There is a number of directions to explore in the future. For one, both Addicter and Hjerson report MT errors related more to adequacy than fluency, although it was shown last year (Parton et al., 2011) that fluency is an important component in rating translation quality. It is also important to test how well the metric performs if lemmatization and PoStagging are not available. For this year’s competition, training data was taken separately for every language pair; it remains to be tested whether combining human judgements with the same target language and different source languages leads to better or worse performance. To conclude, we have described TerrorCat, one of the submissions to the metrics shared task of WMT’12. TerrorCat is rather demanding to apply on one hand, having more requir"
W12-3105,J11-4002,1,0.872172,"Missing"
W12-3105,W07-1709,0,0.0315439,"Missing"
W12-3130,P08-1087,0,0.0139404,"and CU - POOR - COMB. 1 Nickname Direct Single-Step Two-Step Complex Figure 1: A taxonomy of factored phrase-based models. Introduction Koehn and Hoang (2007) introduced “factors” to phrase-based MT to explicitly capture arbitrary features in the phrase-based model. In essence, input and output tokens are no longer atomic units but rather vectors of atomic values encoding e.g. the lexical and morphological information separately. Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a. (Bojar, 2007; Avramidis and Koehn, 2008; Stymne, 2008; Badr et al., 2008; Ramanathan et al., 2009; Koehn et al., 2010; Yeniterzi and Oflazer, 2010). On the other hand, it happens quite frequently, that the factored setup causes a loss compared to the phrase-based baseline. The underlying reason is the complexity of the search space which gets boosted when the model explicitly includes detailed information, see e.g. Bojar and Kos (2010) or Toutanova et al. (2008). ∗ This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czech Science Foundation grants"
W12-3130,P08-2039,0,0.0184658,"ingle-Step Two-Step Complex Figure 1: A taxonomy of factored phrase-based models. Introduction Koehn and Hoang (2007) introduced “factors” to phrase-based MT to explicitly capture arbitrary features in the phrase-based model. In essence, input and output tokens are no longer atomic units but rather vectors of atomic values encoding e.g. the lexical and morphological information separately. Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a. (Bojar, 2007; Avramidis and Koehn, 2008; Stymne, 2008; Badr et al., 2008; Ramanathan et al., 2009; Koehn et al., 2010; Yeniterzi and Oflazer, 2010). On the other hand, it happens quite frequently, that the factored setup causes a loss compared to the phrase-based baseline. The underlying reason is the complexity of the search space which gets boosted when the model explicitly includes detailed information, see e.g. Bojar and Kos (2010) or Toutanova et al. (2008). ∗ This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czech Science Foundation grants P406/11/1499 and P406/10/P259. W"
W12-3130,W07-0702,0,0.0512234,"are denoted with gY-Z, where both Y and Z are targetside factors. Individual mapping steps are combined with a plus, while individual source or target factors are combined with an “a”. As a simple example, tF-F denotes the direct translation from source form (F ) to the target form. A linguistically motivated scenario with one search can be written as tL-L+tT-T+gLaT-F : translate (1) the lemma (L) to lemma, (2) the morphological tag (T) to tag independently and (3) finally generate the target form from the lemma and the tag. We use two more operators: “:” delimits alternative decoding paths (Birch et al., 2007) used within one search and “=” delimits two independent searches. A plausible setup is e.g. tF-LaT=tLaTF:tL-F motivated as follows: the source word form is translated to the lemma and tag in the target language. Then a second search (whose translation tables can be trained on larger monolingual data) consists of two alternative decoding paths: either the pair of L and T is translated into the target form, or as a fallback, the tag is disregarded and the target form is guessed only from the lemma (and the context as scored by the language model). The example also illustrated the priorities of"
W12-3130,W10-1705,1,0.935652,"ical information separately. Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a. (Bojar, 2007; Avramidis and Koehn, 2008; Stymne, 2008; Badr et al., 2008; Ramanathan et al., 2009; Koehn et al., 2010; Yeniterzi and Oflazer, 2010). On the other hand, it happens quite frequently, that the factored setup causes a loss compared to the phrase-based baseline. The underlying reason is the complexity of the search space which gets boosted when the model explicitly includes detailed information, see e.g. Bojar and Kos (2010) or Toutanova et al. (2008). ∗ This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czech Science Foundation grants P406/11/1499 and P406/10/P259. We are grateful for reviewers’ comments but we have to obey the 6 page limit. Thanks also to Aleˇs Tamchyna for supplementary material on MERT. In this paper, we first provide a taxonomy of (phrase-based) translation setups and then we examine a range of sample configurations in this taxonomy. We don’t state universal rules, because the applicability of each of the"
W12-3130,W11-2138,1,0.846265,"lary issues (due to insufficient generalization) on either side. Single-step scenarios have 253 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 253–260, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics a very high risk of combinatorial explosion of translation options (think cartesian product of all target side factors) and/or of spurious ambiguity (several derivations leading to the same output). Such added ambiguity can lead to n-best lists with way fewer unique items than the given n, which in turn renders MERT unstable, see also Bojar and Tamchyna (2011). Serially connected setups (two as our TwoStep or more) can lose relevant candidates between the searches, unless some ambiguous representation like lattices is passed between the steps. An independent axis on which Moses setups can be organized consists of the number and function of factors on the source and the target side. We use a very succint notation for the setups except the “complex” one: tX-Y denotes a translation step between the factors X in the source language and Y in the target language. Generation steps are denoted with gY-Z, where both Y and Z are targetside factors. Individua"
W12-3130,W07-0735,1,0.869459,"s CU - BOJAR and CU - POOR - COMB. 1 Nickname Direct Single-Step Two-Step Complex Figure 1: A taxonomy of factored phrase-based models. Introduction Koehn and Hoang (2007) introduced “factors” to phrase-based MT to explicitly capture arbitrary features in the phrase-based model. In essence, input and output tokens are no longer atomic units but rather vectors of atomic values encoding e.g. the lexical and morphological information separately. Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a. (Bojar, 2007; Avramidis and Koehn, 2008; Stymne, 2008; Badr et al., 2008; Ramanathan et al., 2009; Koehn et al., 2010; Yeniterzi and Oflazer, 2010). On the other hand, it happens quite frequently, that the factored setup causes a loss compared to the phrase-based baseline. The underlying reason is the complexity of the search space which gets boosted when the model explicitly includes detailed information, see e.g. Bojar and Kos (2010) or Toutanova et al. (2008). ∗ This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czec"
W12-3130,P11-2031,0,0.0232721,"with the same language models. 5 Single-Step Experiments Single-step scenarios consist of more than one translation steps within a single search. We do not distinguish whether all the translation steps belong to the same decoding path or to alternative decoding paths. Table 3 lists several single-step configurations (and three direct translations for a comparison). The single-step configurations always include the linguistically-motivated tL-L+tT-T+gLaT-F with varying language models and optionally with an alternative decoding path to serve as the fallback. Aware of the low stability of MERT (Clark et al., 2011), we run MERT three times and report the average BLEU score including the standard deviation. The last column in Table 3 lists the average number of distinct candidates per sentence in the nbest lists during MERT, dubbed “effective n-best list size”. Unless stated otherwise, we used 100-best lists. We see that due to spurious ambiguity, e.g. various segmentations of the input into phrases, the effective size does not reach even a half of the limit. 255 We make three observations here: (1) In this small data setting with a very morphologically rich language, the complex setup tL-L+tTT+gLaT-F do"
W12-3130,P08-1115,0,0.0381709,"better in the two-factor “|” setup that allows to disregard the MOT, while MOT1 works better in the direct translation “+”. Overall, we see no improvement over the tF-F 257 A Complex Moses Setup Obviously, many setups fall under the “complex” category of our taxonomy, including also some system combination approaches. We tried to combine three Moses systems: (1) CU - BOJAR as described below, (2) same setup like CU - BOJAR but optimized towards 1-TER (Snover et al., 2006), and (3) a largedata two-step setup.3 The system combination is performed using a fourth Moses search that gets a lattice (Dyer et al., 2008) of individual systems’ outputs, performs an identity translation and scores the candidates by language models and other features. The lattice is created from the individual system outputs in the ROVER style (Matusov et al., 2008) utilizing the source-to-hypothesis word alignments as produced by the individual systems. We use our simple implementation for constructing the confusion networks and converting them to the lattices. The “combination Moses” was tuned on the WMT11 test set towards BLEU. The resulting system is called CU - POOR - COMB , because we felt it underperformed the individual"
W12-3130,E12-1068,0,0.0520756,"should check the real BLEU and continue or simply rerun the optimization if the iteration limit was reached. 6 Two-Step Experiments The linguistically motivated setups used in the previous sections are prohibitively expensive for large data, see also Bojar et al. (2009). A number of researchers have thus tried diving the complexity of search into two independent phases: (1) translation and reordering, and (2) conjugation and declination. The most promising results were obtained with the second step predicting individual morphological features using a specialized tool (Toutanova et al., 2008; Fraser et al., 2012). Here, we simply use one more Moses search as Bojar and Kos (2010). In the first step, source English gets translated to a simplified Czech and in the second step, the simplified Czech gets fully inflected. 6.1 Factors in Two-Step Setups Two-step setups can use factors in the source, middle or the target language. We experiment with factors only in the middle language (affecting both the first and the second search) and use only the form in both 256 source and target sides. In the middle language, we experiment with one or two factors. For presentation purposes, we always speak about two fact"
W12-3130,D07-1091,0,0.464476,"5, Praha 1, CZ-118 00, Czech Republic {bojar,jawaid,kamran}@ufal.mff.cuni.cz Number of Number of Translation Independent Structure Steps Searches of Searches One One – One – Several Serial Several Complex Abstract We introduce a taxonomy of factored phrasebased translation scenarios and conduct a range of experiments in this taxonomy. We point out several common pitfalls when designing factored setups. The paper also describes our WMT12 submissions CU - BOJAR and CU - POOR - COMB. 1 Nickname Direct Single-Step Two-Step Complex Figure 1: A taxonomy of factored phrase-based models. Introduction Koehn and Hoang (2007) introduced “factors” to phrase-based MT to explicitly capture arbitrary features in the phrase-based model. In essence, input and output tokens are no longer atomic units but rather vectors of atomic values encoding e.g. the lexical and morphological information separately. Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a. (Bojar, 2007; Avramidis and Koehn, 2008; Stymne, 2008; Badr et al., 2008; Ramanathan et al., 2009; Koehn et al., 2010; Yeniterzi and Oflazer, 2010). On the other hand, it h"
W12-3130,P07-2045,1,0.0120095,"o-2011, wmtmono-2012. Except where stated otherwise, we tune on the official WMT10 test set and report BLEU (Papineni et al., 2002) scores on the WMT11 test set. 4 Direct Setups Table 2 lists our experiments with direct translation, various factors and language models in our notation. http://ufal.mff.cuni.cz/treex/ We did not include the parallel en-cs data made available by the WMT12 organizers. This probably explains our loss compared to UEDIN but allows a direct comparison with CU T ECTO MT, a deep syntactic MT based on the same data. 2 Throughout the experiments, we use the Moses toolkit (Koehn et al., 2007) and GIZA++ (Och 254 BLEU 13.05±0.44 13.01±0.44 12.99±0.44 12.42±0.44 12.19±0.44 12.08±0.45 Table 2: Direct en→cs translation (a single search with one translation step only). 1 3 Language Models form + lemma + tag form + tag form + tag form form form Decoding Paths tL-L+tT-T+gLaT-F:tF-FaLaT tL-L+tT-T+gLaT-F tL-L+tT-T+gLaT-F tL-L+tT-T+gLaT-F:tF-FaLaT, 200-best-list tF-FaLaT tL-L+tT-T+gLaT-F:tF-FaLaT tF-FaT tL-L+tT-T+gLaT-F:tF-FaT tF-F (baseline) tL-L+tT-T+gLaT-F:tF-F LMs F+L+T F+L+T F+T F+L+T F+L+T L+T F+T F+T F F Avg. BLEU 13.31±0.06 13.30±0.05 13.17±0.01 13.15±0.24 13.13±0.06 13.09±0.06 13.0"
W12-3130,W10-1715,0,0.0122492,"nomy of factored phrase-based models. Introduction Koehn and Hoang (2007) introduced “factors” to phrase-based MT to explicitly capture arbitrary features in the phrase-based model. In essence, input and output tokens are no longer atomic units but rather vectors of atomic values encoding e.g. the lexical and morphological information separately. Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a. (Bojar, 2007; Avramidis and Koehn, 2008; Stymne, 2008; Badr et al., 2008; Ramanathan et al., 2009; Koehn et al., 2010; Yeniterzi and Oflazer, 2010). On the other hand, it happens quite frequently, that the factored setup causes a loss compared to the phrase-based baseline. The underlying reason is the complexity of the search space which gets boosted when the model explicitly includes detailed information, see e.g. Bojar and Kos (2010) or Toutanova et al. (2008). ∗ This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czech Science Foundation grants P406/11/1499 and P406/10/P259. We are grateful for reviewers’ comments but we"
W12-3130,C00-2163,0,0.109615,"Missing"
W12-3130,P02-1040,0,0.0872071,"ported here, we use the Small dataset only. The language model (LM) for these experiments is a 5-gram one based on the target-side of Small only. Our WMT12 submissions are based on the Large and Mono data. The language model for the large experiments uses 6-grams of forms and optionally 8-grams of morphological tags. As in previous years, the language models are interpolated (towards the best cross entropy on WMT08 dataset) from domain-specific LMs, e.g. czeng-news, czengtechdoc, wmtmono-2011, wmtmono-2012. Except where stated otherwise, we tune on the official WMT10 test set and report BLEU (Papineni et al., 2002) scores on the WMT11 test set. 4 Direct Setups Table 2 lists our experiments with direct translation, various factors and language models in our notation. http://ufal.mff.cuni.cz/treex/ We did not include the parallel en-cs data made available by the WMT12 organizers. This probably explains our loss compared to UEDIN but allows a direct comparison with CU T ECTO MT, a deep syntactic MT based on the same data. 2 Throughout the experiments, we use the Moses toolkit (Koehn et al., 2007) and GIZA++ (Och 254 BLEU 13.05±0.44 13.01±0.44 12.99±0.44 12.42±0.44 12.19±0.44 12.08±0.45 Table 2: Direct en→c"
W12-3130,P09-1090,0,0.045529,"Complex Figure 1: A taxonomy of factored phrase-based models. Introduction Koehn and Hoang (2007) introduced “factors” to phrase-based MT to explicitly capture arbitrary features in the phrase-based model. In essence, input and output tokens are no longer atomic units but rather vectors of atomic values encoding e.g. the lexical and morphological information separately. Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a. (Bojar, 2007; Avramidis and Koehn, 2008; Stymne, 2008; Badr et al., 2008; Ramanathan et al., 2009; Koehn et al., 2010; Yeniterzi and Oflazer, 2010). On the other hand, it happens quite frequently, that the factored setup causes a loss compared to the phrase-based baseline. The underlying reason is the complexity of the search space which gets boosted when the model explicitly includes detailed information, see e.g. Bojar and Kos (2010) or Toutanova et al. (2008). ∗ This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czech Science Foundation grants P406/11/1499 and P406/10/P259. We are grateful for review"
W12-3130,2006.amta-papers.25,0,0.0226115,"le language and the type of the LOF and MOT. We see an interesting difference between MOT1 and MOT0 or 2 . The more fine-grained MOT0 or 2 work better in the two-factor “|” setup that allows to disregard the MOT, while MOT1 works better in the direct translation “+”. Overall, we see no improvement over the tF-F 257 A Complex Moses Setup Obviously, many setups fall under the “complex” category of our taxonomy, including also some system combination approaches. We tried to combine three Moses systems: (1) CU - BOJAR as described below, (2) same setup like CU - BOJAR but optimized towards 1-TER (Snover et al., 2006), and (3) a largedata two-step setup.3 The system combination is performed using a fourth Moses search that gets a lattice (Dyer et al., 2008) of individual systems’ outputs, performs an identity translation and scores the candidates by language models and other features. The lattice is created from the individual system outputs in the ROVER style (Matusov et al., 2008) utilizing the source-to-hypothesis word alignments as produced by the individual systems. We use our simple implementation for constructing the confusion networks and converting them to the lattices. The “combination Moses” was"
W12-3130,W07-1709,0,0.086213,"Missing"
W12-3130,P08-1059,0,0.497901,"ely. Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a. (Bojar, 2007; Avramidis and Koehn, 2008; Stymne, 2008; Badr et al., 2008; Ramanathan et al., 2009; Koehn et al., 2010; Yeniterzi and Oflazer, 2010). On the other hand, it happens quite frequently, that the factored setup causes a loss compared to the phrase-based baseline. The underlying reason is the complexity of the search space which gets boosted when the model explicitly includes detailed information, see e.g. Bojar and Kos (2010) or Toutanova et al. (2008). ∗ This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czech Science Foundation grants P406/11/1499 and P406/10/P259. We are grateful for reviewers’ comments but we have to obey the 6 page limit. Thanks also to Aleˇs Tamchyna for supplementary material on MERT. In this paper, we first provide a taxonomy of (phrase-based) translation setups and then we examine a range of sample configurations in this taxonomy. We don’t state universal rules, because the applicability of each of the setups depends very much on"
W12-3130,P10-1047,0,0.0323185,"ase-based models. Introduction Koehn and Hoang (2007) introduced “factors” to phrase-based MT to explicitly capture arbitrary features in the phrase-based model. In essence, input and output tokens are no longer atomic units but rather vectors of atomic values encoding e.g. the lexical and morphological information separately. Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a. (Bojar, 2007; Avramidis and Koehn, 2008; Stymne, 2008; Badr et al., 2008; Ramanathan et al., 2009; Koehn et al., 2010; Yeniterzi and Oflazer, 2010). On the other hand, it happens quite frequently, that the factored setup causes a loss compared to the phrase-based baseline. The underlying reason is the complexity of the search space which gets boosted when the model explicitly includes detailed information, see e.g. Bojar and Kos (2010) or Toutanova et al. (2008). ∗ This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czech Science Foundation grants P406/11/1499 and P406/10/P259. We are grateful for reviewers’ comments but we have to obey the 6 page limit"
W12-3130,W09-0422,1,\N,Missing
W12-3148,D11-1033,0,0.0399979,"ly small: the news section of CzEng 1.0 (197k sentence pairs, 4.2M Czech words, 4.8M English words). We tuned the models on WMT09 test set and evaluated on WMT11 test set. The techniques examined here rely on a large monolingual corpus to select data from. We used all the monolingual data provided by the organizers of WMT11 (18.3M sentences, 316M words). 5.1 Tailoring the Language Model Our first attempt was to tailor the language model to the test set. Our approach is similar to Zhao et al. (2004). In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al. (2011) extend their ideas and apply them to MT. Naturally, we only used the source side of the test set. First we translated the test set using a baseline translation system. Lucene indexer was then used to select sentences similar to the translated ones in the large target-side monolingual corpus. Finally, a new language model was created from the selected sentences. The weight of the new LM has to reflect the importance of the language model during both MERT tuning as well as final application on (a different) test set. If the new LM were based only on the final test set, MERT would underestimate"
W12-3148,W09-0432,0,0.0282342,"(Koehn, 2004). 3 Selected per Trans. — — 3 100 1000 All Sents Sel. Sents Total 0 16k – rand. sel. 16k 502k 3.8M 18.3M Avg BLEU±std 12.39±0.06 12.18±0.06 12.73±0.04 14.21±0.11 15.12±0.08 15.55±0.11 Table 4: Results of experiments with Lucene, language model adapted. Table 3: BLEU scores on WMT12 test set when tuning on WMT11 test set towards one or more references. 4 Used Models None LM LM LM LM LM http://ufal.ms.mff.cuni.cz/treex/ 376 5 Experiments with Domain Adaptation Domain adaptation is widely recognized as a technique which can significantly improve translation quality (Wu et al., 2008; Bertoldi and Federico, 2009; Daum´e and Jagarlamudi, 2011). In our experiments we tried to select sentences close to the source side of the test set and use them to improve the final translation. The parallel data used in this section are only small: the news section of CzEng 1.0 (197k sentence pairs, 4.2M Czech words, 4.8M English words). We tuned the models on WMT09 test set and evaluated on WMT11 test set. The techniques examined here rely on a large monolingual corpus to select data from. We used all the monolingual data provided by the organizers of WMT11 (18.3M sentences, 316M words). 5.1 Tailoring the Language Mo"
W12-3148,W10-1705,1,0.84195,"score the remaining candidates. In the normal IR scenario, the query is usually small. However, for domain adaptation a query can be a whole corpus. Lucene does not allow such big queries. This problem is resolved by taking the query corpus sentence by sentence and searching many times. The final score of a sentence in the index is calculated as the average of the scores from the sentence-level queries. Methods that make use of this functionality are discussed in Section 5. 3 Reducing OOV by Relaxing Alignments Out-of-vocabulary (OOV) rate has been shown to increase during phrase extraction (Bojar and Kos, 2010). This is due to unfortunate alignment of some words—no consistent phrase pair that includes them can be extracted. This issue can be partially overcome by adding translations of these “lost” words (according to Giza++ word alignment) to the extracted phrase table. This is not our original technique, it was suggested by Mermer and Saraclar (2011), though it is not included in the published abstract. The extraction of phrases in the (hierarchical) decoder Jane (Stein et al., 2011) offers a range of similar heuristics. Tinsley et al. (2009) also observes gains when extending the set of phrases c"
W12-3148,W12-3130,1,0.78639,"ty and we evaluate it against the previous version to show whether the effect for MT is positive. Out-of-vocabulary rate is another problem related to data selection. We present a simple technique to reduce it by including words that became spurious OOVs during phrase extraction. ∗ This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czech Science Foundation grants P406/11/1499 and P406/10/P259. Data and Tools Comparison of CzEng 1.0 and 0.9 As this year’s WMT is the first to include the new version of CzEng (Bojar et al., 2012b), we carried out a few experiments to compare its suitability for MT with its predecessor, CzEng 0.9. Apart from size (which has almost doubled), there are important differences between the two versions. In CzEng 0.9, the largest portion by far came from movie subtitles (a data source of varying quality), followed by EU legislation and technical manuals. On the other hand, CzEng 1.0 has over 4 million sentence pairs from fiction and nearly the same amount of data from EU legislation. Roughly 3 million sentence pairs come from movie subtitles. This proportion of domains suggests a higher qual"
W12-3148,P05-1032,0,0.0667485,"Missing"
W12-3148,P11-2031,0,0.0221207,"lection is done by choosing the most similar sentences to the source side of the final test set. WMT10 system is 378 System Baseline Lucene WMT10 Perfect selection Bad selection avg BLEU±std 11.41±0.25 12.31±0.01 12.37±0.02 12.64±0.02 6.37±0.64 Table 6: Results of tuning with different corpora tuned on 2489 sentence pairs of WMT10 test set. To identify an upper bound, we also include a Perfect selection system which is tuned on the final WMT11 test set. Naturally, this is not a fair competitor. In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al., 2011). Lucene and the WMT10 system were tuned 3 times while baseline system was tuned 9 times because of randomness in selection of tuning corpora (3 different tuning corpora each tuned 3 times). The results are shown in Table 6. Even though the variance of the baseline system is high (because we randomly selected corpora 3 times), the difference in scores between baseline and Lucene system is high enough to conclude that tuning on Lucene-selected corpus helps translation quality. Still it does not give better BLEU score than system tuned on WMT10 corpus. One possible reason is that the whole CzEng"
W12-3148,P11-2071,0,0.0579479,"Missing"
W12-3148,W11-2139,0,0.0659804,"Missing"
W12-3148,W04-3250,0,0.0709704,"er we achieved no improvement at all. For the second experiment we used 3 translations of WMT11 test set. One is the true reference distributed for the shared task and two were translated manually from the German version of the data into Czech. We achieved a small improvement in final BLEU score when training on a small data set. On the complete constrained training data for WMT12, there was no improvement—in fact, the BLEU score as evaluated on the WMT12 test set was negligibly lower. Table 3 summarizes our results. The ± sign denotes the confidence bounds estimated via bootstrap resampling (Koehn, 2004). 3 Selected per Trans. — — 3 100 1000 All Sents Sel. Sents Total 0 16k – rand. sel. 16k 502k 3.8M 18.3M Avg BLEU±std 12.39±0.06 12.18±0.06 12.73±0.04 14.21±0.11 15.12±0.08 15.55±0.11 Table 4: Results of experiments with Lucene, language model adapted. Table 3: BLEU scores on WMT12 test set when tuning on WMT11 test set towards one or more references. 4 Used Models None LM LM LM LM LM http://ufal.ms.mff.cuni.cz/treex/ 376 5 Experiments with Domain Adaptation Domain adaptation is widely recognized as a technique which can significantly improve translation quality (Wu et al., 2008; Bertoldi and"
W12-3148,W11-2132,0,0.0143182,". Note that when selecting the sentences, we used lemmas instead of word forms to reduce data sparseness. So Lucene was actually indexing the lemmatized version of the monolingual data and the baseline translation translated English lemmas to Czech lemmas when creating the “query corpus”. The final models were created from the original sentences, not their lemmatized versions. 5.2 Tailoring the Translation Model Reverse self-training is a trick that allows to improve the translation model using (target-side) monolingual data and can lead to a performance improvement (Bojar and Tamchyna, 2011; Lambert et al., 2011). In our scenario, we translated the selected sentences (in the opposite direction, i.e. from the target into the source language). Then we created a new translation model (in the original direction) based on the alignment of selected sentences and their reverse translation. This new model is finally combined with the baseline model and weighted by MERT. The whole scenario is shown in Figure 1. The results of our experiments are in Table 5. We ran the experiment with translation model adaptation for 100 most similar sentences selected by Lucene. 377 Each experiment was again performed five tim"
W12-3148,N10-1062,0,0.0165529,"available that could tackle this problem. Wuebker et al. (2010) store phrase pair counts per sentence when extracting phrases and thus they can reestimate the probabilities when a sentence has to be excluded from the phrase tables. For large parallel corpora, suffix arrays (CallisonBurch et al., 2005) have been used. Suffix arrays allow for a quick retrieval of relevant sentence pairs, the phrase extraction is postponed and performed on the fly for each input sentence. It is trivial to filter out sentences belonging to the tuning set during this delayed extraction. With dynamic suffix arrays (Levenberg et al., 2010), one could even simply remove the tuning sentences from the suffix array. 379 6 Submitted Systems This paper covers the submissions CU - TAMCH - BOJ. We translated from English into Czech. Our setup was very similar to CU - BOJAR (Bojar et al., 2012a), but our primary submission is tuned on multiple reference translations as described in Section 4. Apart from the additional references, this is a constrained setup. CzEng 1.0 were the only parallel data used in training. We used a factored model to translate the combination of English surface form and part-of-speech tag into Czech form+POS. We"
W12-3148,P10-2041,0,0.0407829,"and use them to improve the final translation. The parallel data used in this section are only small: the news section of CzEng 1.0 (197k sentence pairs, 4.2M Czech words, 4.8M English words). We tuned the models on WMT09 test set and evaluated on WMT11 test set. The techniques examined here rely on a large monolingual corpus to select data from. We used all the monolingual data provided by the organizers of WMT11 (18.3M sentences, 316M words). 5.1 Tailoring the Language Model Our first attempt was to tailor the language model to the test set. Our approach is similar to Zhao et al. (2004). In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al. (2011) extend their ideas and apply them to MT. Naturally, we only used the source side of the test set. First we translated the test set using a baseline translation system. Lucene indexer was then used to select sentences similar to the translated ones in the large target-side monolingual corpus. Finally, a new language model was created from the selected sentences. The weight of the new LM has to reflect the importance of the language model during both MERT tuning as well as final application on (a differen"
W12-3148,C08-1125,0,0.0638965,"Missing"
W12-3148,P10-1049,0,0.0300875,"tences, the distortion and language model get an extremely low weight compared to the weights of translation model. This is because they are not useful in translation of tuning data which was already seen during training. Instead of reordering two short phrases A and B, system already knows the translation of the phrase A B so no distortion is needed. On unseen sentences, such weights lead to poor results. This amplifies a drawback of our approach: source texts have to be known prior to system tuning or even before phrase extraction. There are methods available that could tackle this problem. Wuebker et al. (2010) store phrase pair counts per sentence when extracting phrases and thus they can reestimate the probabilities when a sentence has to be excluded from the phrase tables. For large parallel corpora, suffix arrays (CallisonBurch et al., 2005) have been used. Suffix arrays allow for a quick retrieval of relevant sentence pairs, the phrase extraction is postponed and performed on the fly for each input sentence. It is trivial to filter out sentences belonging to the tuning set during this delayed extraction. With dynamic suffix arrays (Levenberg et al., 2010), one could even simply remove the tunin"
W12-3148,C04-1059,0,0.0278019,"e side of the test set and use them to improve the final translation. The parallel data used in this section are only small: the news section of CzEng 1.0 (197k sentence pairs, 4.2M Czech words, 4.8M English words). We tuned the models on WMT09 test set and evaluated on WMT11 test set. The techniques examined here rely on a large monolingual corpus to select data from. We used all the monolingual data provided by the organizers of WMT11 (18.3M sentences, 316M words). 5.1 Tailoring the Language Model Our first attempt was to tailor the language model to the test set. Our approach is similar to Zhao et al. (2004). In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al. (2011) extend their ideas and apply them to MT. Naturally, we only used the source side of the test set. First we translated the test set using a baseline translation system. Lucene indexer was then used to select sentences similar to the translated ones in the large target-side monolingual corpus. Finally, a new language model was created from the selected sentences. The weight of the new LM has to reflect the importance of the language model during both MERT tuning as well as final"
W12-3148,C10-1075,0,\N,Missing
W12-4204,2004.iwslt-papers.1,0,0.0315707,"esources for interpreting and automating HMEANT. We apply HMEANT to a new language, Czech in particular, by evaluating a set of Englishto-Czech MT systems. HMEANT proves to correlate with manual rankings at the sentence level better than a range of automatic metrics. However, the main contribution of this paper is the identification of several issues of HMEANT annotation and our proposal on how to resolve them. 1 Introduction Manual evaluation of machine translation output is a tricky enterprise. It has been long recognized that different evaluation techniques lead to different outcomes, e.g. Blanchon et al. (2004) mention an evaluation carried out in 1972 where the very same Russian-to-English MT outputs were scored 4.5 out of the maximum 5 points by prospective users of the system but only 1 out of 5 by teachers of English. Throughout the years, many techniques were explored with more or less of a success. The two-scale scoring for adequacy and fluency used in NIST evaluation has been abandoned by some evaluation campaigns, most notably the WMT shared task series, see Koehn and Monz (2006) through Callison-Burch et al. (2012)1 . Since 2008, WMT uses a simple relative ranking of MT outputs as its prima"
W12-4204,W11-2101,1,0.865595,"Missing"
W12-4204,W07-0718,0,0.0354432,"ome evaluation campaigns, most notably the WMT shared task series, see Koehn and Monz (2006) through Callison-Burch et al. (2012)1 . Since 2008, WMT uses a simple relative ranking of MT outputs as its primary manual evaluation technique: the annotator is presented with up to 5 MT outputs for a given input sentence and the task is to rank them from best to worst (ties allowed) on whatever criteria he or she deems appropriate. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter- and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al., 2007), the technique and its evaluation are still far from satisfactory. Bojar et al. (2011) observe several discrepancies in the interpretation of the rankings, partly due to the high load on human annotators (the comparison of several long sentences at once, among other issues) but partly also due to technicalities of the calculation. Lo and Wu (2011a) present an interesting evaluation technique called MEANT (or HMEANT if carried out by humans), the core of which lies in assessing whether the key elements in the predicateargument structure of the sentence have been preserved. In other words, lay"
W12-4204,W12-3102,0,0.0678334,"Missing"
W12-4204,hajic-etal-2012-announcing,1,0.867193,"Missing"
W12-4204,W06-3114,0,0.0381749,"ky enterprise. It has been long recognized that different evaluation techniques lead to different outcomes, e.g. Blanchon et al. (2004) mention an evaluation carried out in 1972 where the very same Russian-to-English MT outputs were scored 4.5 out of the maximum 5 points by prospective users of the system but only 1 out of 5 by teachers of English. Throughout the years, many techniques were explored with more or less of a success. The two-scale scoring for adequacy and fluency used in NIST evaluation has been abandoned by some evaluation campaigns, most notably the WMT shared task series, see Koehn and Monz (2006) through Callison-Burch et al. (2012)1 . Since 2008, WMT uses a simple relative ranking of MT outputs as its primary manual evaluation technique: the annotator is presented with up to 5 MT outputs for a given input sentence and the task is to rank them from best to worst (ties allowed) on whatever criteria he or she deems appropriate. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter- and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al., 2007), the technique and its evaluation are still far from satisfacto"
W12-4204,P11-1023,1,0.910768,"t (ties allowed) on whatever criteria he or she deems appropriate. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter- and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al., 2007), the technique and its evaluation are still far from satisfactory. Bojar et al. (2011) observe several discrepancies in the interpretation of the rankings, partly due to the high load on human annotators (the comparison of several long sentences at once, among other issues) but partly also due to technicalities of the calculation. Lo and Wu (2011a) present an interesting evaluation technique called MEANT (or HMEANT if carried out by humans), the core of which lies in assessing whether the key elements in the predicateargument structure of the sentence have been preserved. In other words, lay annotators are checking, if they recognize who did what to whom, when, where and why from the MT outputs and whether the respective role fillers convey the same meaning as in the reference translation. HMEANT has been shown to correlate reasonably well with manual adequacy and ranking evaluations. It is relatively fast and should lend itself to fu"
W12-4204,W11-1002,1,0.876808,"t (ties allowed) on whatever criteria he or she deems appropriate. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter- and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al., 2007), the technique and its evaluation are still far from satisfactory. Bojar et al. (2011) observe several discrepancies in the interpretation of the rankings, partly due to the high load on human annotators (the comparison of several long sentences at once, among other issues) but partly also due to technicalities of the calculation. Lo and Wu (2011a) present an interesting evaluation technique called MEANT (or HMEANT if carried out by humans), the core of which lies in assessing whether the key elements in the predicateargument structure of the sentence have been preserved. In other words, lay annotators are checking, if they recognize who did what to whom, when, where and why from the MT outputs and whether the respective role fillers convey the same meaning as in the reference translation. HMEANT has been shown to correlate reasonably well with manual adequacy and ranking evaluations. It is relatively fast and should lend itself to fu"
W12-4204,J05-1004,0,0.0225999,"rgument (how) You may consider the Action predicate to be the central event, while the other roles modify the Action to give a more detailed description of the event. Each semantic frame contains exactly one Action and any number of other roles. Please note that the Action predicate must be exactly ONE single word. There may be multiple semantic frames in one sentence, because a sentence may be constructed to describe multiple events and each semantic frame captures only one event. Functional Generative Description The core ideas of HMEANT follow the case grammar (Fillmore, 1968) or PropBank (Palmer et al., 2005) and can be also directly related to an established linguistic theory which was primarily devel31 oped for Czech, namely the Functional Generative Description (Sgall, 1967; Sgall et al., 1986). The theory defines so-called “tectogrammatical” layer (tlayer). At the t-layer, each sentence is represented as a dependency tree with just content words as separate nodes. All auxiliary words are “hidden” into attributes of the corresponding t-nodes. Moreover, ellipsis is restored to some extent, so e.g. dropped subject pronouns do have a corresponding t-node. An important element of FGD is the valency"
W12-4204,W12-3146,0,0.0428927,"Missing"
W12-4204,sindlerova-bojar-2010-building,1,0.844744,"g the development of the Prague Dependency Treebank (Hajiˇc et al., 2006)2 and the parallel Prague Czech-English Dependency Treebank (Hajiˇc 2 http://ufal.mff.cuni.cz/pdt2.0/ et al., 2012)3 . Note that the latter is a translation of all the 49k sentences of the Penn Treebank WSJ section. Both English and Czech sentences are manually annotated at the tectogrammatical layer, where the English layer is based on the Penn annotation and manually adapted for t-layer. Both languages include their respective valency lexicons and the work on a bilingual valency lexicon is being developed ˇ (Sindlerov´ a and Bojar, 2010). A range of automatic tools to convert plain text up to the t-layer exist for both English and Czech. Most of them are now part of the Treex platform (Popel ˇ and Zabokrtsk´ y, 2010)4 and they were successfully used in automatic annotation of 15 million parallel sentences (Bojar et al., 2012)5 as well as other NLP tasks including English-to-Czech MT. Recently, significant effort was also invested in parsing not quite correct output of MT systems into Czech for the purposes of rule-based grammar correction (Rosa et al., 2012). Establishing the automatic pipeline for MEANT should be relatively"
W12-4204,bojar-etal-2012-joy,1,\N,Missing
W12-5011,gimenez-marquez-2004-svmtool,0,0.146457,"RB and I in CRULP tagset map to ADV. When designing the mapping rules, we considered available documentation and also data tagged with the tagset. 3.3 Mapping HUM analyzer and SH parser outputs Before mapping test set tagged by HUM analyzer and SH parser on Sajjad’s tagset, we drop all the morphological information and preserve only the set of proposed POS tags. A sample test sentence tagged by HUM analyzer and SH parser is shown in Table 4. Again, “|” delimits words from their tags. The mapping to Sajjad’s tagset can again introduce ambiguity. We delimit ambiguous tags with “-”. 4 Our tagger Giménez and Màrquez (2004) introduced and made publicly available a multi-purpose tagger called SVM Tool. SVM Tool performed better than state-of-the-art taggers and Sajjad and Schmid (2009) conﬁrmed this for Urdu. We follow up on this work and train SVM Tool on CRULP manually tagged data (Section 2.2). SVM Tool oﬀers ﬁve diﬀerent kind of models for training a learner. We use ‘model 4’ with tagging direction from right-to-left. Model 4 boosts identiﬁcation of unkown words during the learning time by artiﬁcially marking some of the words as unknown 138 Sajjad’s Tagset A AA AD ADJ ADV AKP AP CA CC DATE EXP FR G GR I INT"
W12-5011,E09-1079,0,0.467006,"/wiki/Urdu http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_parser.php 136 represented in Shakti Standard Format (SSF)3 . Detailed description of SSF and a brief overview of tagset is available online4 . 2.2 Data Besides the tools mentioned above, there is a freely available POS tagged corpus developed by CRULP (Center for Research in Urdu Language Processing)5 . The underlying tagset is also available online6 . The actual text of the corpus is a translation of portion of the Wall Street Journal’s section of the Penn Treebank. We use this data to train our tagger, see Section 4. Sajjad and Schmid (2009) manually tagged 110K tokens from a news corpus (www.jang.com.pk) but their data is not freely available7 . In addition to 110K tokens, they also tagged 8K tokens8 from BBC News (www.bbc.co.uk/urdu/) and made it freely available online9 . Sajjad and Schmid (2009) have designed their own tagset10 . that is purely syntactic in nature, we call this tagset Sajjad’s tagset. Note that Sajjad’s tagset is more coarse-grained than the one used by CRULP. For instance, Sajjad tags proper noun by PN whereas CRULP distinguishes between the ﬁrst proper noun in a multi-word name (NNP) and the following ones"
W12-5611,W11-2101,1,0.850569,"Missing"
W12-5611,W10-1703,0,0.0197028,"Missing"
W12-5611,P05-1033,0,0.0246605,"t Dev News 108,332 12,037 1000 Cinema 34,690 3854 1000 Bible 26,884 2987 1000 All 171,706 19,078 1000 Corpus Training data English Tamil 2.9M 2.1M 3.4M 3.8M 529K 353K 605K 610K 668K 352K 733K 731K 4.1M 2.9M 4.8M 5.3M Test data English Tamil 328K 247K 386K 447K 60K 40K 68K 69K 94K 50K 103K 103K 459K 318K 534K 586K Dev data English Tamil 27K 20K 32K 37K 15K 10K 18K 18K 22K 12K 24K 24K 23K 16K 27K 30K Table 1: Corpus statistics. For each corpus, the upper and lower row correspond to the number of tokens before and after the suffix splitting. 4.2 Systems Used We use phrase-based and hierarchical (Chiang, 2005) MT systems as implemented by Koehn et al. (2007) for our experiments. We use the default system settings for all experiments and report results for individual datasets as well as for the entire training data, All. 4.3 Examined Configurations Our experiments consist of the following settings for both phrase based and hierarchical systems: • baseline: The default, no suffix splitting. • targetmor : No change in English side of the data. Our suffix splitter is run on Tamil. • source+targetmor : Both the English and Tamil suffix splitters are run on the respective sides of the data. For each sett"
W12-5611,W01-1409,0,0.579788,"propose morphological processing aimed at reducing data sparsity. In Section 4, we describe our English-Tamil parallel corpora collection and the system configurations we use. In Section 5, we report the results and analyze them in Section 6. 2 Related Work Research into SMT involving Tamil language is not very common, the main reason perhaps being the lack of parallel corpora. Nevertheless there have been efforts for other Indian languages such as Hindi (Udupa U. and Faruquie, 2004), (Ramanathan et al., 2008) and (Bojar et al., 2008). The earliest work that appeared on English-Tamil SMT was (Germann, 2001) which described building a small English-Tamil parallel corpus as well as an SMT system. So far, the efforts for building English-Tamil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose steps that improve the performance of SMT systems. Applying morphological processing to SMT is not new"
W12-5611,P07-2045,1,0.0102404,"0 3854 1000 Bible 26,884 2987 1000 All 171,706 19,078 1000 Corpus Training data English Tamil 2.9M 2.1M 3.4M 3.8M 529K 353K 605K 610K 668K 352K 733K 731K 4.1M 2.9M 4.8M 5.3M Test data English Tamil 328K 247K 386K 447K 60K 40K 68K 69K 94K 50K 103K 103K 459K 318K 534K 586K Dev data English Tamil 27K 20K 32K 37K 15K 10K 18K 18K 22K 12K 24K 24K 23K 16K 27K 30K Table 1: Corpus statistics. For each corpus, the upper and lower row correspond to the number of tokens before and after the suffix splitting. 4.2 Systems Used We use phrase-based and hierarchical (Chiang, 2005) MT systems as implemented by Koehn et al. (2007) for our experiments. We use the default system settings for all experiments and report results for individual datasets as well as for the entire training data, All. 4.3 Examined Configurations Our experiments consist of the following settings for both phrase based and hierarchical systems: • baseline: The default, no suffix splitting. • targetmor : No change in English side of the data. Our suffix splitter is run on Tamil. • source+targetmor : Both the English and Tamil suffix splitters are run on the respective sides of the data. For each settings, we report BLEU (Papineni et al., 2002) scor"
W12-5611,N04-4015,0,0.0325478,"a small English-Tamil parallel corpus as well as an SMT system. So far, the efforts for building English-Tamil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose steps that improve the performance of SMT systems. Applying morphological processing to SMT is not new, the idea goes back to (Lee, 2004) for Arabic-English or (Nießen and Ney, 2004) for German-English. (Ramanathan et al., 2008) and (Ramanathan et al., 2009) are the first to experiment an Indian language, namely in English-Hindi translation. We apply similar techniques to English-Tamil pair. 3 Suffix Splitting English and Tamil morphologies follow different inflectional patterns. While English morphology can be adequately described with a few morphological suffixes, thousands of wordforms can be built from a single root in Tamil. As expected, verbs and nouns are the main productive parts of speech in Tamil. For example, a Tamil"
W12-5611,J04-2003,0,0.0355256,"rpus as well as an SMT system. So far, the efforts for building English-Tamil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose steps that improve the performance of SMT systems. Applying morphological processing to SMT is not new, the idea goes back to (Lee, 2004) for Arabic-English or (Nießen and Ney, 2004) for German-English. (Ramanathan et al., 2008) and (Ramanathan et al., 2009) are the first to experiment an Indian language, namely in English-Hindi translation. We apply similar techniques to English-Tamil pair. 3 Suffix Splitting English and Tamil morphologies follow different inflectional patterns. While English morphology can be adequately described with a few morphological suffixes, thousands of wordforms can be built from a single root in Tamil. As expected, verbs and nouns are the main productive parts of speech in Tamil. For example, a Tamil verb, in addition to the root bearing the le"
W12-5611,P02-1040,0,0.0834182,"nted by Koehn et al. (2007) for our experiments. We use the default system settings for all experiments and report results for individual datasets as well as for the entire training data, All. 4.3 Examined Configurations Our experiments consist of the following settings for both phrase based and hierarchical systems: • baseline: The default, no suffix splitting. • targetmor : No change in English side of the data. Our suffix splitter is run on Tamil. • source+targetmor : Both the English and Tamil suffix splitters are run on the respective sides of the data. For each settings, we report BLEU (Papineni et al., 2002) scores in three variations: BLEUsu f f _sep , BLEUsu f f _r e j and BLEUst em_onl y . In the case of BLEUsu f f _sep evaluation, both the reference and hypothesis translations are suffixseparated before the evaluation, allowing a better match with the reference but also risking more false positives. The BLEUsu f f _r e j evaluation corresponds to what Tamil readers would like to see: the suffixes are rejoined (if they were separated) prior to evaluation. BLEUst em_onl y ignores suffixes altogether, both hypothesis and reference translations contain only stem words. Manual sentence level ranki"
W12-5611,W12-3152,0,0.0497214,"Missing"
W12-5611,I08-1067,0,0.026118,"l differences contribute to data sparsity. We attempt to address both issues in this paper. In Section 3, we propose morphological processing aimed at reducing data sparsity. In Section 4, we describe our English-Tamil parallel corpora collection and the system configurations we use. In Section 5, we report the results and analyze them in Section 6. 2 Related Work Research into SMT involving Tamil language is not very common, the main reason perhaps being the lack of parallel corpora. Nevertheless there have been efforts for other Indian languages such as Hindi (Udupa U. and Faruquie, 2004), (Ramanathan et al., 2008) and (Bojar et al., 2008). The earliest work that appeared on English-Tamil SMT was (Germann, 2001) which described building a small English-Tamil parallel corpus as well as an SMT system. So far, the efforts for building English-Tamil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose ste"
W12-5611,P09-1090,0,0.0146616,"amil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose steps that improve the performance of SMT systems. Applying morphological processing to SMT is not new, the idea goes back to (Lee, 2004) for Arabic-English or (Nießen and Ney, 2004) for German-English. (Ramanathan et al., 2008) and (Ramanathan et al., 2009) are the first to experiment an Indian language, namely in English-Hindi translation. We apply similar techniques to English-Tamil pair. 3 Suffix Splitting English and Tamil morphologies follow different inflectional patterns. While English morphology can be adequately described with a few morphological suffixes, thousands of wordforms can be built from a single root in Tamil. As expected, verbs and nouns are the main productive parts of speech in Tamil. For example, a Tamil verb, in addition to the root bearing the lexical information, can include suffixes corresponding to person, number, gen"
W12-5611,2009.mtsummit-papers.7,0,\N,Missing
W13-2201,W13-2205,0,0.0583032,"Missing"
W13-2201,S13-1034,0,0.0277746,"Missing"
W13-2201,C12-1008,0,0.0091484,"ignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011), and pseudo-reference METEOR score; the most successful set, Feature Set 33 combines those 24 features with the 17 baseline features. For English-Spanish, LogReg was used with L2 Regularisation (Lin et al., 2007) and"
W13-2201,W13-2206,0,0.0913052,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2240,0,0.0186069,"Missing"
W13-2201,W13-2242,0,0.220429,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2207,0,0.0357036,"Missing"
W13-2201,W11-2104,0,0.00810596,"Missing"
W13-2201,P10-2016,1,0.826191,"Missing"
W13-2201,W13-2208,1,0.743011,"Missing"
W13-2201,P11-1022,0,0.112625,"the intended application. In Task 2 we tested binary word-level classification in a post-editing setting. If such annotation is presented through a user interface we imagine that words marked as incorrect would be hidden from the editor, highlighted as possibly wrong or that a list of alternatives would we generated. With respect to the poor improvements over trivial baselines, we consider that the results for word-level prediction could be mostly connected to limitations of the datasets provided, which are very small for word-level prediction, as compared to successful previous work such as (Bach et al., 2011). Despite the limited amount of training data, several systems were able to predict dubious words (binary variant of the task), showing that this can be a promising task. Extending the granularity even further by predicting the actual editing action necessary for a word yielded less positive results than the binary setting. We cannot directly compare sentence- and word-level results. However, since sentence-level predictions can benefit from more information available and therefore more signal on which the prediction is based, the natural conclusion is that, if there is a choice in the predict"
W13-2201,W13-2209,0,0.034542,"Missing"
W13-2201,W13-2241,1,0.0862471,"Missing"
W13-2201,W07-0718,1,0.697635,"to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated both automatically and manually."
W13-2201,W11-2103,1,0.5768,"Missing"
W13-2201,W13-2243,1,0.754887,"Missing"
W13-2201,W13-2213,0,0.0301468,"Missing"
W13-2201,P05-1022,0,0.0401846,"parses of the source and target sentence, the positions of the phrases with the lowest and highest probability and future cost estimate in the translation, the counts of phrases in the decoding graph whose probability or whether the future cost estimate is higher/lower than their standard deviation, counts of verbs and determiners, etc. The second submission (pls8) was trained with Partial Least Squares regression (Stone and Brooks, 1990) including more glass-box features. The following NLP tools were used in feature extraction: the Brown English Wall-StreetJournal-trained statistical parser (Charniak and Johnson, 2005), a Lexical Functional Grammar parser (XLE), together with a hand-crafted Lexical Functional Grammar, the English ParGram grammar (Kaplan et al., 2004), and the TreeTagger part-of-speech tagger (Schmidt, 1994) with off-the-shelf publicly available pre-trained tagging models for English and Spanish. For pseudoreference features, the Bing, Moses and Systran translation systems were used. The Mallet toolkit (McCallum, 2002) was used to build the topic models and features based on a grammar checker were extracted with LanguageTool.16 FBK-Uedin (T1.1, T1.3): The submissions explored features built"
W13-2201,W13-2210,0,0.0366902,"Missing"
W13-2201,W13-2212,1,0.1814,"Missing"
W13-2201,W13-2214,0,0.0302242,"Missing"
W13-2201,W13-2217,0,0.0313645,"Missing"
W13-2201,W13-2215,0,0.0174795,"Missing"
W13-2201,W13-2253,0,0.055006,"Missing"
W13-2201,W13-2246,0,0.0627442,"Missing"
W13-2201,N06-1058,0,0.0556187,"of the official test set size, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run ma"
W13-2201,W13-2248,0,0.0488662,"Missing"
W13-2201,2012.iwslt-papers.5,1,0.674636,"ts are somewhat artificially more diverse; in narrow domains, source sentences can repeat and even appear verbatim in the training data, and in natural test sets with multiple references, short sentences can receive several identical translations. For each probe, we measure the Spearman’s rank correlation coefficient ρ of the ranks proposed by BLEU or NIST and the manual ranks. We use the same implementation as applied in the WMT13 Shared Metrics Task (Mach´acˇ ek and Bojar, 2013). Note that the WMT13 metrics task still uses the WMT12 evaluation method ignoring ties, not the expected wins. As Koehn (2012) shows, the two methods do not differ much. Overall, the correlation is strongly impacted by Figure 5: Correlation of BLEU and WMT13 manual ranks for English→Czech translation Figure 6: Correlation of NIST and WMT13 manual ranks for English→Czech translation the particular choice of test sentences and reference translations. By picking sentences randomly, similarly or equally sized test sets can reach different correlations. Indeed, e.g. for a test set of about 1500 distinct sentences selected from the 3000-sentence official test set (1 reference translation), we obtain correlations for BLEU b"
W13-2201,W13-2202,1,0.761984,"Missing"
W13-2201,W06-3114,1,0.635019,"ntence, ranking of up to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated bot"
W13-2201,W13-2219,0,0.0360343,"Missing"
W13-2201,W13-2220,0,0.017834,"Missing"
W13-2201,W13-2255,0,0.0136612,"Missing"
W13-2201,W12-3113,0,0.012942,"Missing"
W13-2201,W13-2247,0,0.0208658,"Missing"
W13-2201,W13-2221,1,0.762444,"Missing"
W13-2201,W13-2228,0,0.0270572,"Missing"
W13-2201,W13-2224,0,0.0611782,"Missing"
W13-2201,2013.mtsummit-papers.21,1,0.577491,"ve learning to reduce the training set size (and therefore the annotation effort). The initial set features contains all black box and glass box features available within the Q U E ST framework (Specia et al., 2013) for the dataset at hand (160 in total for Task 1.1, and 80 for Task 1.3). The query selection strategy for active learning is based on the informativeness of the instances using Information Density, a measure that leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is. To perform feature selection, following (Shah et al., 2013) features are ranked by the Gaussian Process 23 algorithm according to their learned length scales, which can be interpreted as the relevance of such feature for the model. This information was used for feature selection by discarding the lowest ranked (least useful) ones. based on empirical results found in (Shah et al., 2013), the top 25 features for both models were selected and used to retrain the same regression algorithm. Semantic Roles could bring marginally better accuracy. TCD-CNGL (T1.1) and TCD-DCU-CNGL (T1.3): The system is based on features which are commonly used for style classi"
W13-2201,2012.amta-papers.13,0,0.0208135,"Missing"
W13-2201,W13-2250,0,0.0320711,"Missing"
W13-2201,W13-2225,0,0.0376682,"Missing"
W13-2201,P13-1135,1,0.207781,"Missing"
W13-2201,W13-2226,1,0.759686,"Missing"
W13-2201,2006.amta-papers.25,0,0.295541,"ation Based on the data of Task 1.3, we define Task 2, a word-level annotation task for which participants are asked to produce a label for each token that indicates whether the word should be changed by a post-editor or kept in the final translation. We consider the following two sets of labels for prediction: Sentence-level Quality Estimation Task 1.1 Predicting Post-editing Distance This task is similar to the quality estimation task in WMT12, but with one important difference in the scoring variant: instead of using the post-editing effort scores in the [1, 2, 3, 4, 5] range, we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). Two variants of the results could be submitted in the shared task: • Binary classification: a keep/change label, the latter meaning that the token should be corrected in the post-editing process. • Multi-class classification: a label specifying the edit action that should be performed on the token (keep as is, delete, or substitute). 6.3 Datasets Ta"
W13-2201,W13-2227,0,0.0389834,"Missing"
W13-2201,W09-0441,0,0.0271208,"ze, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly p"
W13-2201,W13-2230,0,0.0166145,"Missing"
W13-2201,W12-3118,1,0.609118,"1.1, T1.3): The submissions explored features built on MT engine resources including automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011"
W13-2201,P13-2135,0,0.148019,"the difference between selecting the best or the worse translation. 19 ID CMU CNGL DCU DCU-SYMC DFKI FBK-UEdin LIG LIMSI LORIA SHEF TCD-CNGL TCD-DCU-CNGL UMAC UPC Participating team Carnegie Mellon University, USA (Hildebrand and Vogel, 2013) Centre for Next Generation Localization, Ireland (Bicici, 2013b) Dublin City University, Ireland (Almaghout and Specia, 2013) Dublin City University & Symantec, Ireland (Rubino et al., 2013b) German Research Centre for Artificial Intelligence, Germany (Avramidis and Popovic, 2013) Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de Souza et al., 2013) Laboratoire d’Informatique Grenoble, France (Luong et al., 2013) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Singh et al., 2013) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois and Smaili, 2013) University of Sheffield, UK (Beck et al., 2013) Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013) Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and Rubino, 2013) University of Macau, China (Han et al., 2013) Universitat Politecnica de Catalunya, Spain (Formiga et al., 2013b) Ta"
W13-2201,2011.eamt-1.12,1,0.741789,"ar (CCG) features: CCG supertag language model perplexity and log probability, the number of maximal CCG constituents in the translation output which are the highestprobability minimum number of CCG constituents that span the translation output, the percentage of CCG argument mismatches between each subsequent CCG supertags, the percentage of CCG argument mismatches between each subsequent CCG maximal categories and the minimum number of phrases detected in the translation output. A second submission uses the aforementioned CCG features combined with 80 features from Q U E ST as described in (Specia, 2011). For the CCG features, the C&C parser was used to parse the translation output. Moses was used to build the phrase table from the SMT training corpus with maximum phrase length set to 7. The language model of supertags was built using the SRILM toolkit. As learning algorithm, Logistic Regression as provided by the SCIKIT- LEARN toolkit was used. The training data was prepared by converting each ranking of translation outputs to a set of pairwise comparisons according to the approach proposed by Avramidis et al. (2011). The rankings were generated back from pairwise comparisons predicted by th"
W13-2201,P13-4014,1,0.118757,"Missing"
W13-2201,W13-2229,0,0.0371208,"Missing"
W13-2201,tantug-etal-2008-bleu,0,0.0131442,"a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly picking the test set size (number of distinct sentences) and the number of distinct references per sentence. Note that such test sets are s"
W13-2201,W10-1751,0,\N,Missing
W13-2201,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2201,W13-2249,0,\N,Missing
W13-2201,N04-1013,0,\N,Missing
W13-2201,W02-1001,0,\N,Missing
W13-2201,W12-3102,1,\N,Missing
W13-2201,P12-3024,0,\N,Missing
W13-2201,W09-0401,1,\N,Missing
W13-2201,W13-2222,0,\N,Missing
W13-2201,W10-1711,0,\N,Missing
W13-2201,2010.iwslt-evaluation.22,0,\N,Missing
W13-2201,W10-1703,1,\N,Missing
W13-2201,W08-0309,1,\N,Missing
W13-2201,W13-2218,0,\N,Missing
W13-2201,P13-1004,1,\N,Missing
W13-2201,2013.mtsummit-papers.9,0,\N,Missing
W13-2201,W13-2216,1,\N,Missing
W13-2201,W13-2244,0,\N,Missing
W13-2202,W12-3102,0,0.128312,"Missing"
W13-2202,W11-2107,0,0.243788,"Missing"
W13-2202,W13-2251,0,0.036342,"Missing"
W13-2202,W13-2252,0,0.0864116,"Missing"
W13-2202,E06-1031,0,0.573676,"Macau (Han et al., 2013) Idiap Research Institute (Hajlaoui, 2013) (Hajlaoui and Popescu-Belis, 2013) Dublin City University (Wu et al., 2013) University of Shefield (Song et al., 2013) Hong Kong University of Science and Technology (Lo and Wu, 2013) German Research Center for Artificial Intelligence (Fishel, 2013) DFKI (Avramidis and Popovi´c, 2013) Table 1: Participants of WMT13 Metrics Shared Task In addition to that we have computed the following two groups of standard metrics as baselines: • Moses Scorer. Metrics BLEU (Papineni et al., 2002), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences we used the standard tokenizer script as available in Moses Toolkit. In this paper we use the suffix *- MOSES to label these metrics. • Mteval. Metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl 1 which is used in OpenMT Evaluation Campaign and includes its own tokenization. We use *- MTEVAL suffix to label these metrics. By default, mteval assumes the text is in ASCII, causing poor tokenization around curly quotes. We run mteval"
W13-2202,P02-1040,0,0.111352,"Carnegie Mellon University (Denkowski and Lavie, 2011) University of Macau (Han et al., 2013) Idiap Research Institute (Hajlaoui, 2013) (Hajlaoui and Popescu-Belis, 2013) Dublin City University (Wu et al., 2013) University of Shefield (Song et al., 2013) Hong Kong University of Science and Technology (Lo and Wu, 2013) German Research Center for Artificial Intelligence (Fishel, 2013) DFKI (Avramidis and Popovi´c, 2013) Table 1: Participants of WMT13 Metrics Shared Task In addition to that we have computed the following two groups of standard metrics as baselines: • Moses Scorer. Metrics BLEU (Papineni et al., 2002), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences we used the standard tokenizer script as available in Moses Toolkit. In this paper we use the suffix *- MOSES to label these metrics. • Mteval. Metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl 1 which is used in OpenMT Evaluation Campaign and includes its own tokenization. We use *- MTEVAL suffix to label these metrics. By default, mteval assumes the text is in"
W13-2202,2006.amta-papers.25,0,0.314238,"Denkowski and Lavie, 2011) University of Macau (Han et al., 2013) Idiap Research Institute (Hajlaoui, 2013) (Hajlaoui and Popescu-Belis, 2013) Dublin City University (Wu et al., 2013) University of Shefield (Song et al., 2013) Hong Kong University of Science and Technology (Lo and Wu, 2013) German Research Center for Artificial Intelligence (Fishel, 2013) DFKI (Avramidis and Popovi´c, 2013) Table 1: Participants of WMT13 Metrics Shared Task In addition to that we have computed the following two groups of standard metrics as baselines: • Moses Scorer. Metrics BLEU (Papineni et al., 2002), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences we used the standard tokenizer script as available in Moses Toolkit. In this paper we use the suffix *- MOSES to label these metrics. • Mteval. Metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl 1 which is used in OpenMT Evaluation Campaign and includes its own tokenization. We use *- MTEVAL suffix to label these metrics. By default, mteval assumes the text is in ASCII, causing poor tokeni"
W13-2202,W13-2253,0,\N,Missing
W13-2202,W13-2256,0,\N,Missing
W13-2202,W13-2201,1,\N,Missing
W13-2208,W12-3139,0,0.0143246,"rain a 4gram language model using KenLM (Heafield et al., 2013). Unfortunately, we did not manage to use a model of higher order. The model file (even in the binarized trie format with probability quantization) was so large that we ran out of memory in decoding.5 We also tried pruning these larger models but we did not have enough RAM. To cater for a longer-range coherence, we trained a 7-gram language model only on the News Crawl corpus (concatenation of all years). In this case, we used SRILM (Stolcke, 2002) and pruned n-grams so that (training set) model perplexity 2.1.5 Bigger Tuning Sets Koehn and Haddow (2012) report benefits from tuning on a larger set of sentences. We experimented with a down-scaled MT system to compare a couple of options for our tuning set: the default 3003 sentences of newstest2011, the default and three more Czech references that were created by translating from German, the default and two more references that were created by postediting a variant of our last year’s Moses system and also a larger single-reference set consisting of several newstest years. The preliminary results were highly inconclusive: negligibly higher BLEU scores obtained lower manual scores. Unable to pic"
W13-2208,W07-0702,0,0.0218649,"h and in Czech, such as: – a subject in English is marked by being a left modifier of the predicate, while in Czech a subject is marked by the nominative morphological case – English marks possessiveness by the preposition ’of’, while Czech uses the genitive morphological case – negation can be marked in various ways in English and Czech • verb-noun and noun-noun valency—see (Rosa et al., 2013) Depfix first performs a complex lingustic anal6 Using K-best batch MIRA (Cherry and Foster, 2012) did not work any better in our setup. 7 We are aware of the fact that Moses alternative decoding paths (Birch and Osborne, 2007) with similar phrase tables clutter n-best lists with identical items, making MERT less stable (Eisele et al., 2008; Bojar and Tamchyna, 2011). The issue was not severe in our case, CU - BOJAR needed 10 iterations compared to 3 iterations needed for PLAIN. 95 System CU - TECTOMT CU - BOJAR CU - DEPFIX PLAIN Moses G OOGLE T R . BLEU TER 14.7 20.1 20.0 19.5 – 0.741 0.696 0.693 0.713 – WMT Ranking Appraise MTurk 0.455 0.491 0.637 0.555 0.664 0.542 – – 0.618 0.526 System All Moses TectoMT Other System None Moses TectoMT Both CU - BOJAR ysis of both the source English sentence and its translation t"
W13-2208,2005.mtsummit-papers.11,0,0.005624,"hus less prone to errors in local morphological agreement. Table 1 summarizes the final (case-sensitive!) BLEU scores for four setups. The standard approach is to train SMT lowercase and apply a recaser, e.g. the Moses one, on the output. Another option (denoted “lc→form”) is to lowercase only the source side of the parallel data. This more or less makes the translation model responsible for identifying names and the language model for identifying beginnings of sentences. 2.1.3 Large Parallel Data The main source of our parallel data was CzEng 1.0 (Bojar et al., 2012b). We also used Europarl (Koehn, 2005) as made available by WMT13 organizers.2 The English-Czech part of the new Common Crawl corpus was quite small and very noisy, so we did not include it in our training data. Table 2 provides basic statistics of the data. Processing large parallel data can be challenging in terms of time and computational resources required. The main bottlenecks are word alignment and phrase extraction. GIZA++ (Och and Ney, 2000) has been the standard tool for computing word alignment in phrase-based MT. A multi-threaded version exists (Gao and Vogel, 2008), which also supports incremental extensions of paralle"
W13-2208,W11-2138,1,0.834334,"e nominative morphological case – English marks possessiveness by the preposition ’of’, while Czech uses the genitive morphological case – negation can be marked in various ways in English and Czech • verb-noun and noun-noun valency—see (Rosa et al., 2013) Depfix first performs a complex lingustic anal6 Using K-best batch MIRA (Cherry and Foster, 2012) did not work any better in our setup. 7 We are aware of the fact that Moses alternative decoding paths (Birch and Osborne, 2007) with similar phrase tables clutter n-best lists with identical items, making MERT less stable (Eisele et al., 2008; Bojar and Tamchyna, 2011). The issue was not severe in our case, CU - BOJAR needed 10 iterations compared to 3 iterations needed for PLAIN. 95 System CU - TECTOMT CU - BOJAR CU - DEPFIX PLAIN Moses G OOGLE T R . BLEU TER 14.7 20.1 20.0 19.5 – 0.741 0.696 0.693 0.713 – WMT Ranking Appraise MTurk 0.455 0.491 0.637 0.555 0.664 0.542 – – 0.618 0.526 System All Moses TectoMT Other System None Moses TectoMT Both CU - BOJAR ysis of both the source English sentence and its translation to Czech by CU - BOJAR. The analysis includes tagging, word-alignment, and dependency parsing both to shallow-syntax (“analytical”) and deep-sy"
W13-2208,W11-2101,1,0.892202,"Missing"
W13-2208,P00-1056,0,0.1175,"mes and the language model for identifying beginnings of sentences. 2.1.3 Large Parallel Data The main source of our parallel data was CzEng 1.0 (Bojar et al., 2012b). We also used Europarl (Koehn, 2005) as made available by WMT13 organizers.2 The English-Czech part of the new Common Crawl corpus was quite small and very noisy, so we did not include it in our training data. Table 2 provides basic statistics of the data. Processing large parallel data can be challenging in terms of time and computational resources required. The main bottlenecks are word alignment and phrase extraction. GIZA++ (Och and Ney, 2000) has been the standard tool for computing word alignment in phrase-based MT. A multi-threaded version exists (Gao and Vogel, 2008), which also supports incremental extensions of parallel data by applying a saved model on a new sentence pair. We evaluated these tools and measured their wall-clock time3 as well as the final BLEU score of a full MT system. Surprisingly, single-threaded GIZA++ was considerably faster than single-threaded MGIZA. Using 12 threads, MGIZA outperformed GIZA++ but the difference was smaller than we expected. Table 3 summarizes the results. We checked the difference in B"
W13-2208,W12-3130,1,0.916037,"gure 1. Each of the intermediate stages of processing has been submitted as a separate primary system for the WMT manual evalution, allowing for a more thorough analysis. Instead of an off-the-shelf system combination technique, we use TectoMT output as synthetic training data for Moses as described in Section 2.1 and finally we process its output using rule-based corrections of Depfix (Section 2.2). All steps directly use the source sentence. 2.1 Moses Setup for CU - BOJAR We ran a couple of probes with reduced training data around the setup of Moses that proved successful in previous years (Bojar et al., 2012a). 2.1.1 Pre-processing We use a stable pre-processing pipeline that includes normalization of quotation marks,1 tokenization, tagging and lemmatization with tools 1 We do not simply convert them to unpaired ASCII quotes but rather balance them and use other heuristics to convert most cases to the typographically correct form. 92 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 92–98, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics Case BLEU recaser 9.05 lc→form 9.13 utc 9.70 stc 9.81 Corpus CzEng 1.0 Europarl Common Crawl Table 1"
W13-2208,W12-3146,1,0.783173,"Missing"
W13-2208,N12-1047,0,0.026653,"person, if applicable • transfer of meaning in cases where the same meaning is expressed by different grammatical means in English and in Czech, such as: – a subject in English is marked by being a left modifier of the predicate, while in Czech a subject is marked by the nominative morphological case – English marks possessiveness by the preposition ’of’, while Czech uses the genitive morphological case – negation can be marked in various ways in English and Czech • verb-noun and noun-noun valency—see (Rosa et al., 2013) Depfix first performs a complex lingustic anal6 Using K-best batch MIRA (Cherry and Foster, 2012) did not work any better in our setup. 7 We are aware of the fact that Moses alternative decoding paths (Birch and Osborne, 2007) with similar phrase tables clutter n-best lists with identical items, making MERT less stable (Eisele et al., 2008; Bojar and Tamchyna, 2011). The issue was not severe in our case, CU - BOJAR needed 10 iterations compared to 3 iterations needed for PLAIN. 95 System CU - TECTOMT CU - BOJAR CU - DEPFIX PLAIN Moses G OOGLE T R . BLEU TER 14.7 20.1 20.0 19.5 – 0.741 0.696 0.693 0.713 – WMT Ranking Appraise MTurk 0.455 0.491 0.637 0.555 0.664 0.542 – – 0.618 0.526 System"
W13-2208,P13-3025,1,0.887247,"Missing"
W13-2208,P11-2031,0,0.0255261,"for computing word alignment in phrase-based MT. A multi-threaded version exists (Gao and Vogel, 2008), which also supports incremental extensions of parallel data by applying a saved model on a new sentence pair. We evaluated these tools and measured their wall-clock time3 as well as the final BLEU score of a full MT system. Surprisingly, single-threaded GIZA++ was considerably faster than single-threaded MGIZA. Using 12 threads, MGIZA outperformed GIZA++ but the difference was smaller than we expected. Table 3 summarizes the results. We checked the difference in BLEU using the procedure by Clark et al. (2011) and GIZA++ alignments were indeed The final two approaches attempt at “truecasing” the data, i.e. the ideal lowercasing of everything except names. Our simple unsupervised truecaser (“utc”) uses a model trained on monolingual data (1 million sentences in this case, same as the parallel training data used in this experiment) to identify the most frequent “casing shape” of each token type when it appears within a sentence and then converts its occurrences at the beginnings of sentences to this shape. Our supervised truecaser (“stc”) casts the case of the lemma on the form, because our lemmatize"
W13-2208,W08-0328,0,0.0307767,"bject is marked by the nominative morphological case – English marks possessiveness by the preposition ’of’, while Czech uses the genitive morphological case – negation can be marked in various ways in English and Czech • verb-noun and noun-noun valency—see (Rosa et al., 2013) Depfix first performs a complex lingustic anal6 Using K-best batch MIRA (Cherry and Foster, 2012) did not work any better in our setup. 7 We are aware of the fact that Moses alternative decoding paths (Birch and Osborne, 2007) with similar phrase tables clutter n-best lists with identical items, making MERT less stable (Eisele et al., 2008; Bojar and Tamchyna, 2011). The issue was not severe in our case, CU - BOJAR needed 10 iterations compared to 3 iterations needed for PLAIN. 95 System CU - TECTOMT CU - BOJAR CU - DEPFIX PLAIN Moses G OOGLE T R . BLEU TER 14.7 20.1 20.0 19.5 – 0.741 0.696 0.693 0.713 – WMT Ranking Appraise MTurk 0.455 0.491 0.637 0.555 0.664 0.542 – – 0.618 0.526 System All Moses TectoMT Other System None Moses TectoMT Both CU - BOJAR ysis of both the source English sentence and its translation to Czech by CU - BOJAR. The analysis includes tagging, word-alignment, and dependency parsing both to shallow-syntax"
W13-2208,spoustova-spousta-2012-high,0,0.0280766,"Missing"
W13-2208,E12-1068,0,0.0139164,"matic rule-based correction of frequent grammatical and meaning errors. We do not use any off-the-shelf systemcombination method. 1 2 System Description Input TectoMT Moses Depﬁx Introduction cu-tectomt cu-bojar cu-depﬁx = Chimera Figure 1: C HIMERA: three systems combined. Targeting Czech in statistical machine translation (SMT) is notoriously difficult due to the large number of possible word forms and complex agreement rules. Previous attempts to resolve these issues include specific probabilistic models (Subotin, 2011) or leaving the morphological generation to a separate processing step (Fraser et al., 2012; Mareˇcek et al., 2011). TectoMT (CU - TECTOMT, Galuˇscˇ a´ kov´a et al. (2013)) is a hybrid (rule-based and statistical) MT system that closely follows the analysis-transfersynthesis pipeline. As such, it suffers from many issues but generating word forms in proper agreements with their neighbourhood as well as the translation of some diverging syntactic structures are handled well. Overall, TectoMT sometimes even ties with a highly tuned Moses configuration in manual evaluations, see Bojar et al. (2011). Finally, Rosa et al. (2012) describes Depfix, a rule-based system for post-processing ("
W13-2208,P11-1024,0,0.0163434,"vel of representation, factored phrase-based translation using Moses, and finally automatic rule-based correction of frequent grammatical and meaning errors. We do not use any off-the-shelf systemcombination method. 1 2 System Description Input TectoMT Moses Depﬁx Introduction cu-tectomt cu-bojar cu-depﬁx = Chimera Figure 1: C HIMERA: three systems combined. Targeting Czech in statistical machine translation (SMT) is notoriously difficult due to the large number of possible word forms and complex agreement rules. Previous attempts to resolve these issues include specific probabilistic models (Subotin, 2011) or leaving the morphological generation to a separate processing step (Fraser et al., 2012; Mareˇcek et al., 2011). TectoMT (CU - TECTOMT, Galuˇscˇ a´ kov´a et al. (2013)) is a hybrid (rule-based and statistical) MT system that closely follows the analysis-transfersynthesis pipeline. As such, it suffers from many issues but generating word forms in proper agreements with their neighbourhood as well as the translation of some diverging syntactic structures are handled well. Overall, TectoMT sometimes even ties with a highly tuned Moses configuration in manual evaluations, see Bojar et al. (201"
W13-2208,W13-2216,1,0.598857,"Missing"
W13-2208,W08-0509,0,0.0189865,"ta was CzEng 1.0 (Bojar et al., 2012b). We also used Europarl (Koehn, 2005) as made available by WMT13 organizers.2 The English-Czech part of the new Common Crawl corpus was quite small and very noisy, so we did not include it in our training data. Table 2 provides basic statistics of the data. Processing large parallel data can be challenging in terms of time and computational resources required. The main bottlenecks are word alignment and phrase extraction. GIZA++ (Och and Ney, 2000) has been the standard tool for computing word alignment in phrase-based MT. A multi-threaded version exists (Gao and Vogel, 2008), which also supports incremental extensions of parallel data by applying a saved model on a new sentence pair. We evaluated these tools and measured their wall-clock time3 as well as the final BLEU score of a full MT system. Surprisingly, single-threaded GIZA++ was considerably faster than single-threaded MGIZA. Using 12 threads, MGIZA outperformed GIZA++ but the difference was smaller than we expected. Table 3 summarizes the results. We checked the difference in BLEU using the procedure by Clark et al. (2011) and GIZA++ alignments were indeed The final two approaches attempt at “truecasing”"
W13-2208,P13-2121,0,0.0341719,"Missing"
W13-2208,W11-2152,1,\N,Missing
W13-2208,bojar-etal-2012-joy,1,\N,Missing
W13-2216,2011.mtsummit-papers.35,0,0.0473899,"Missing"
W13-2216,W11-2101,1,0.872402,"Missing"
W13-2216,W09-0405,0,0.0153382,"in this paper. 2.1 Statistical Post-Editing Statistical post-editing (SPE, see e.g. Simard et al. (2007), Dugast et al. (2009)) is a popular method 1 2 If more reference translations are available, it would be beneficial to choose such references for training SPE which are most similar to the first-stage outputs. However, in our experiments only one reference is available. http://www.statmt.org/wmt13 141 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 141–147, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics (Eisele et al., 2008; Chen et al., 2009). Another issue of phrase table combination is that the same output can be achieved with phrases from several phrase tables, leading to spurious ambiguity and thus less diversity in n-best lists of a given size (see Chen et al. (2009) for one possible solution). CComb does not suffer from the spurious ambiguity issue, but it does not allow to tune special features for the individual first-stage systems. serves well-formed syntactic sentence structures, and the SPE (Moses) fixes low fluency wordings. 2.2 MT Output Combination An SPE system is trained to improve the output of a single first-stag"
W13-2216,W09-0419,0,0.0745508,"nd P HRASE F IX – statistical post-editing of T ECTO MT using Moses (Koehn et al., 2007). We also report on experiments with another hybrid method where T EC TO MT is used to produce additional (so-called synthetic) parallel training data for Moses. This method was used in CU-B OJAR and CU-D EPFIX submissions, see Bojar et al. (2013). 2 Overview of Related Work The number of approaches to system combination is enormous. We very briefly survey those that form the basis of our work reported in this paper. 2.1 Statistical Post-Editing Statistical post-editing (SPE, see e.g. Simard et al. (2007), Dugast et al. (2009)) is a popular method 1 2 If more reference translations are available, it would be beneficial to choose such references for training SPE which are most similar to the first-stage outputs. However, in our experiments only one reference is available. http://www.statmt.org/wmt13 141 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 141–147, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics (Eisele et al., 2008; Chen et al., 2009). Another issue of phrase table combination is that the same output can be achieved with phrases from several"
W13-2216,W08-0328,0,0.125383,"of our work reported in this paper. 2.1 Statistical Post-Editing Statistical post-editing (SPE, see e.g. Simard et al. (2007), Dugast et al. (2009)) is a popular method 1 2 If more reference translations are available, it would be beneficial to choose such references for training SPE which are most similar to the first-stage outputs. However, in our experiments only one reference is available. http://www.statmt.org/wmt13 141 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 141–147, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics (Eisele et al., 2008; Chen et al., 2009). Another issue of phrase table combination is that the same output can be achieved with phrases from several phrase tables, leading to spurious ambiguity and thus less diversity in n-best lists of a given size (see Chen et al. (2009) for one possible solution). CComb does not suffer from the spurious ambiguity issue, but it does not allow to tune special features for the individual first-stage systems. serves well-formed syntactic sentence structures, and the SPE (Moses) fixes low fluency wordings. 2.2 MT Output Combination An SPE system is trained to improve the output of"
W13-2216,N07-1029,0,0.022578,"slation of each sentence is a combination of phrases from several systems. In both approaches, the systems are treated as black boxes, so only their outputs are needed. In the simplest setting, all systems are supposed to be equally good/reliable, and the final output is selected by voting, based on the number of shared ngrams or language model scores. The number and the identity of the systems to be combined therefore do not need to be known in advance. More sophisticated methods learn parameters/weights specific for the individual systems. These methods are based e.g. on confusion networks (Rosti et al., 2007; Matusov et al., 2008) and joint optimization of word alignment, word order and lexical choice (He and Toutanova, 2009). 2.3 In our experiments, we use both CComb and PTComb approaches. In PTComb, we use T EC TO MT as the only first-stage system and Moses as the second-stage system. We use the two phrase tables separately (the merging is not needed; 5 · 2 is still a reasonable number of features in MERT). In CComb, we concatenate English↔Czech parallel corpus with English↔“synthetic Czech” corpus translated from English using T ECTO MT. A single phrase table is created from the concatenated c"
W13-2216,D09-1125,0,0.0152484,"ted as black boxes, so only their outputs are needed. In the simplest setting, all systems are supposed to be equally good/reliable, and the final output is selected by voting, based on the number of shared ngrams or language model scores. The number and the identity of the systems to be combined therefore do not need to be known in advance. More sophisticated methods learn parameters/weights specific for the individual systems. These methods are based e.g. on confusion networks (Rosti et al., 2007; Matusov et al., 2008) and joint optimization of word alignment, word order and lexical choice (He and Toutanova, 2009). 2.3 In our experiments, we use both CComb and PTComb approaches. In PTComb, we use T EC TO MT as the only first-stage system and Moses as the second-stage system. We use the two phrase tables separately (the merging is not needed; 5 · 2 is still a reasonable number of features in MERT). In CComb, we concatenate English↔Czech parallel corpus with English↔“synthetic Czech” corpus translated from English using T ECTO MT. A single phrase table is created from the concatenated corpus. 3 T ECTO MT T ECTO MT is a linguistically-motivated tree-totree deep-syntactic translation system with transfer b"
W13-2216,W07-0733,0,0.0297807,"to use the n firststage systems to prepare synthetic parallel data and include them in the training data for the SMT. Corpus Combination (CComb) The easiest method is to use these n newly created parallel corpora as additional training data, i.e. train Moses on a concatenation of the original parallel sentences (with human-translated references) and the new parallel sentences (with machinetranslated pseudo-references). Phrase Table Combination (PTComb) Another method is to extract n phrase tables in addition to the original phrase table and exploit the Moses option of multiple phrase tables (Koehn and Schroeder, 2007). This means that given the usual five features (forward/backward phrase/lexical log probability and phrase penalty), we need to tune 5 · (n + 1) features. Because such MERT (Och, 2003) tuning may be unstable for higher n, several methods were proposed where the n+1 phrase tables are merged into a single one 142 Corpus Sents CzEng tmt (CzEng) Czech Web Corpus WMT News Crawl 15M 15M 37M 25M Tokens Czech English 205M 236M 197M 236M 627M – 445M – T ECTO MT P HRASE F IX Filtering Mark Reliable Phr. Mark Identities Table 1: Statistics of used data. 4 BLEU 14.71±0.53 17.73±0.54 14.68±0.50 17.87±0.55"
W13-2216,N07-1064,0,0.349038,"ed MT because of the complementary nature of weaknesses and advantages of rule-based and statistical approaches. SPE is usually done with an off-the-shelf SMT system (e.g. Moses) which is trained on output of the first-stage system aligned with reference translations of the original source text. The goal of SPE is to produce translations that are better than both the first-stage system alone and the second-stage SMT trained on the original training data. Most SPE approaches use the reference translations from the original training parallel corpus to train the second-stage system. In contrast, Simard et al. (2007) use human-post-edited firststage system outputs instead. Intuitively, the latter approach achieves better results because the human-post-edited translations are closer to the first-stage output than the original reference translations. Therefore, SPE learns to perform the changes which are needed the most. However, creating human-post-edited translations is laborious and must be done again for each new (version of the) first-stage system in order to preserve its full advantage over using the original references.2 Rosa et al. (2013) have applied SPE on English→Czech SMT outputs. They have used"
W13-2216,P07-2045,1,0.0124533,"e put SPE in context with other system combination techniques and evaluate SPE vs. another simple system combination technique: using synthetic parallel data from T ECTO MT to train a statistical MT system (SMT). We confirm that P HRASE F IX (SPE) improves the output of T ECTO MT, and we use this to analyze errors in T EC TO MT. However, we also show that extending data for SMT is more effective. 1 Introduction This paper describes two submissions to the WMT 2013 shared task:1 T ECTO MT – a deepsyntactic tree-to-tree system and P HRASE F IX – statistical post-editing of T ECTO MT using Moses (Koehn et al., 2007). We also report on experiments with another hybrid method where T EC TO MT is used to produce additional (so-called synthetic) parallel training data for Moses. This method was used in CU-B OJAR and CU-D EPFIX submissions, see Bojar et al. (2013). 2 Overview of Related Work The number of approaches to system combination is enormous. We very briefly survey those that form the basis of our work reported in this paper. 2.1 Statistical Post-Editing Statistical post-editing (SPE, see e.g. Simard et al. (2007), Dugast et al. (2009)) is a popular method 1 2 If more reference translations are availab"
W13-2216,2006.amta-papers.25,0,0.03865,"etic Czech) lemmas, which could be acquired directly from the T ECTO MT output. For the rest of the experiments, we approximated lemmas with just the first four lowercase characters of each (English and Czech) token. 4.2 5 SPE Experiments We trained a base SPE system as described in Section 2.1 and dubbed it P HRASE F IX. First two rows of Table 2 show that the firststage T ECTO MT system (serving here as the baseline) was significantly improved in terms of BLEU (Papineni et al., 2002) by P HRASE F IX (p &lt; 0.001 according to the paired bootstrap test (Koehn, 2004)), but the difference in TER (Snover et al., 2006) is not significant.5 The preliminary results of WMT 2013 manual evaluation show only a minor improvement: T ECTO MT=0.476 vs. P HRASE F IX=0.484 (higher means better, for details on the ranking see Callison-Burch et al. (2012)). Language Models In all our experiments, we used three language models on truecased forms: News Crawl as provided by WMT organizers,4 the Czech side of CzEng and the Articles section of the Czech Web 5 The BLEU and TER results reported here slightly differ from the results shown at http://matrix.statmt. org/matrix/systems_list/1720 because of different tokenization and"
W13-2216,spoustova-spousta-2012-high,0,0.0943894,"Missing"
W13-2216,W04-3250,0,0.0861943,"ed to base alignment on (genuine and synthetic Czech) lemmas, which could be acquired directly from the T ECTO MT output. For the rest of the experiments, we approximated lemmas with just the first four lowercase characters of each (English and Czech) token. 4.2 5 SPE Experiments We trained a base SPE system as described in Section 2.1 and dubbed it P HRASE F IX. First two rows of Table 2 show that the firststage T ECTO MT system (serving here as the baseline) was significantly improved in terms of BLEU (Papineni et al., 2002) by P HRASE F IX (p &lt; 0.001 according to the paired bootstrap test (Koehn, 2004)), but the difference in TER (Snover et al., 2006) is not significant.5 The preliminary results of WMT 2013 manual evaluation show only a minor improvement: T ECTO MT=0.476 vs. P HRASE F IX=0.484 (higher means better, for details on the ranking see Callison-Burch et al. (2012)). Language Models In all our experiments, we used three language models on truecased forms: News Crawl as provided by WMT organizers,4 the Czech side of CzEng and the Articles section of the Czech Web 5 The BLEU and TER results reported here slightly differ from the results shown at http://matrix.statmt. org/matrix/syste"
W13-2216,W10-1730,1,0.910175,"Missing"
W13-2216,P09-2037,1,0.839154,"age system and Moses as the second-stage system. We use the two phrase tables separately (the merging is not needed; 5 · 2 is still a reasonable number of features in MERT). In CComb, we concatenate English↔Czech parallel corpus with English↔“synthetic Czech” corpus translated from English using T ECTO MT. A single phrase table is created from the concatenated corpus. 3 T ECTO MT T ECTO MT is a linguistically-motivated tree-totree deep-syntactic translation system with transfer based on Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Tree Markov Models (Žabokrtský and Popel, 2009). It employs some rule-based components, but the most important tasks in the analysistransfer-synthesis pipeline are based on statistics and machine learning. There are three main reasons why it is a suitable candidate for SPE and other hybrid methods. • T ECTO MT has quite different distribution and characteristics of errors compared to standard SMT (Bojar et al., 2011). • T ECTO MT is not tuned for BLEU using MERT (its development is rather driven by human inspection of the errors although different setups are regularly evaluated with BLEU as an additional guidance). • T ECTO MT uses deep-sy"
W13-2216,J03-1002,0,0.00478822,"omitted), see Table 1 for statistics. We translated the English side of CzEng with T ECTO MT to obtain “synthetic Czech”. This way we obtained a new parallel corpus, denoted tmt (CzEng), with English ↔ synthetic Czech sentences. Analogically, we translated the WMT 2013 test set (newstest2013) with T ECTO MT and obtained tmt (newstest2013). Our baseline SMT system (Moses) trained on CzEng corpus only was then also used for WMT 2013 test set translation, and we obtained smt (newstest2013). For all MERT tuning, newstest2011 was used. 4.1 Alignment All our parallel data were aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the “grow-diag-final-and” heuristics. This applies also to the synthetic corpora tmt (CzEng), tmt (newstest2013),3 and smt (newstest2013). For the SPE experiments, we decided to base alignment on (genuine and synthetic Czech) lemmas, which could be acquired directly from the T ECTO MT output. For the rest of the experiments, we approximated lemmas with just the first four lowercase characters of each (English and Czech) token. 4.2 5 SPE Experiments We trained a base SPE system as described in Section 2.1 and dubbed it P HRASE F IX. First two rows of Table 2 show that the"
W13-2216,P03-1021,0,0.00803066,"llel corpora as additional training data, i.e. train Moses on a concatenation of the original parallel sentences (with human-translated references) and the new parallel sentences (with machinetranslated pseudo-references). Phrase Table Combination (PTComb) Another method is to extract n phrase tables in addition to the original phrase table and exploit the Moses option of multiple phrase tables (Koehn and Schroeder, 2007). This means that given the usual five features (forward/backward phrase/lexical log probability and phrase penalty), we need to tune 5 · (n + 1) features. Because such MERT (Och, 2003) tuning may be unstable for higher n, several methods were proposed where the n+1 phrase tables are merged into a single one 142 Corpus Sents CzEng tmt (CzEng) Czech Web Corpus WMT News Crawl 15M 15M 37M 25M Tokens Czech English 205M 236M 197M 236M 627M – 445M – T ECTO MT P HRASE F IX Filtering Mark Reliable Phr. Mark Identities Table 1: Statistics of used data. 4 BLEU 14.71±0.53 17.73±0.54 14.68±0.50 17.87±0.55 17.87±0.57 1-TER 35.61±0.60 35.63±0.65 35.47±0.57 35.57±0.66 35.85±0.68 Table 2: Comparison of several strategies of SPE. Best results are in bold. Common Experimental Setup Corpus (Sp"
W13-2216,P02-1040,0,0.0911933,"mt (CzEng), tmt (newstest2013),3 and smt (newstest2013). For the SPE experiments, we decided to base alignment on (genuine and synthetic Czech) lemmas, which could be acquired directly from the T ECTO MT output. For the rest of the experiments, we approximated lemmas with just the first four lowercase characters of each (English and Czech) token. 4.2 5 SPE Experiments We trained a base SPE system as described in Section 2.1 and dubbed it P HRASE F IX. First two rows of Table 2 show that the firststage T ECTO MT system (serving here as the baseline) was significantly improved in terms of BLEU (Papineni et al., 2002) by P HRASE F IX (p &lt; 0.001 according to the paired bootstrap test (Koehn, 2004)), but the difference in TER (Snover et al., 2006) is not significant.5 The preliminary results of WMT 2013 manual evaluation show only a minor improvement: T ECTO MT=0.476 vs. P HRASE F IX=0.484 (higher means better, for details on the ranking see Callison-Burch et al. (2012)). Language Models In all our experiments, we used three language models on truecased forms: News Crawl as provided by WMT organizers,4 the Czech side of CzEng and the Articles section of the Czech Web 5 The BLEU and TER results reported here"
W13-2216,W12-4205,1,0.900973,"Missing"
W13-2216,P13-3025,0,0.277699,"Missing"
W13-2216,W12-3102,0,\N,Missing
W13-2216,W07-0704,0,\N,Missing
W14-3302,bojar-etal-2014-hindencorp,1,0.801715,"Missing"
W14-3302,W13-2242,0,0.189283,"Missing"
W14-3302,W14-3305,0,0.0227897,"Missing"
W14-3302,W14-3326,1,0.729814,"Missing"
W14-3302,W14-3313,0,0.0328356,"Missing"
W14-3302,W14-3342,0,0.0770851,"Missing"
W14-3302,2012.iwslt-papers.5,1,0.897779,"Each system SJ in the pool {Sj } is represented by an associated relative ability µj and a variance σa2 (fixed across all systems) which serve as the parameters of a Gaussian distribution. Samples from this distribution represent the quality of sentence translations, with higher quality samples having higher values. Pairwise annotations (S1 , S2 , π) are generated according to the following process: Method 1: Expected Wins (EW) Introduced for WMT13, the E XPECTED W INS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j 6= i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, 19 1. Select two systems S1 and S2 from the pool of systems {Sj } This score is then used to sort the systems and produce the ranking. 2. Draw two “translations”, adding random 2 to simulate Gaussian noise with variance σobs the subjectivity of the task and the differences among annotators: 3.4 Method Selection We have three methods which, pr"
W14-3302,W06-3114,1,0.176114,"estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag"
W14-3302,W12-3101,0,0.0505246,"Missing"
W14-3302,P02-1040,0,0.10401,"inuous-space language model is also used in a post-processing step for each system. POSTECH submitted a phrase-based SMT system and query translation system for the DE–EN language pair in both subtasks. They analysed three types of query formation, generated query translation candidates using term-to-term dictionaries and a phrase-based system, and then scored them using a co-occurrence word frequency measure to select the best candidate. UEDIN applied the Moses phrase-based system to 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original ID BLEU normalized truecased normalized lowercased 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech→English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1."
W14-3302,W14-3328,0,0.0383178,"Missing"
W14-3302,W14-3301,1,0.485672,"guage pair by translating newspaper articles and provided training data. 2.1 2.2 As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Europarl4 , United Nations, French-English 109 corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU), some were updated (Russian-English parallel data provided by Yandex, News Commentary, monolingual data), and a new corpus was added (HindiEnglish corpus, Bojar et al. (2014)), Hindi-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure 1. Test data The test data for this year’s task was selected from news stories from online sources, as before. However, we changed our method to create the test sets. In previous years, we took equal amounts of source sentences from all six languages involved (around 500 sentences each), and translated them into all other languages. While this produced a multi-parallel test corpus that could be also used for language pairs (such as Czech-Russian) that we did not include in the evaluati"
W14-3302,W14-3330,0,0.0464303,"Missing"
W14-3302,W14-3320,0,0.0387654,"Missing"
W14-3302,W14-3317,0,0.0337314,"Missing"
W14-3302,W14-3343,1,0.726267,"Finally, with respect to out-of-domain (different 41 It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 23 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments. (Shah and Specia, 2014) applied the models trained"
W14-3302,W02-1001,0,\N,Missing
W14-3302,E06-1031,0,\N,Missing
W14-3302,W12-3102,1,\N,Missing
W14-3302,P13-2135,0,\N,Missing
W14-3302,W14-3311,0,\N,Missing
W14-3302,W14-3314,0,\N,Missing
W14-3302,W09-0401,1,\N,Missing
W14-3302,W14-3319,0,\N,Missing
W14-3302,W14-3344,0,\N,Missing
W14-3302,W14-3327,0,\N,Missing
W14-3302,W07-0718,1,\N,Missing
W14-3302,P11-1132,0,\N,Missing
W14-3302,W13-2248,0,\N,Missing
W14-3302,W14-3307,0,\N,Missing
W14-3302,W10-1703,1,\N,Missing
W14-3302,W14-3323,0,\N,Missing
W14-3302,P13-4014,1,\N,Missing
W14-3302,W08-0309,1,\N,Missing
W14-3302,W14-3340,1,\N,Missing
W14-3302,W14-3312,0,\N,Missing
W14-3302,P13-1139,0,\N,Missing
W14-3302,W14-3310,1,\N,Missing
W14-3302,P13-1004,1,\N,Missing
W14-3302,W14-3304,0,\N,Missing
W14-3302,W14-3321,0,\N,Missing
W14-3302,W14-3308,0,\N,Missing
W14-3302,uresova-etal-2014-multilingual,1,\N,Missing
W14-3302,W14-3336,1,\N,Missing
W14-3302,W14-3331,0,\N,Missing
W14-3302,W14-3332,0,\N,Missing
W14-3302,W14-3325,0,\N,Missing
W14-3302,W14-3329,0,\N,Missing
W14-3302,W14-3315,0,\N,Missing
W14-3302,W13-2201,1,\N,Missing
W14-3302,W14-3339,0,\N,Missing
W14-3302,W14-3322,1,\N,Missing
W14-3302,W14-3303,0,\N,Missing
W14-3302,W14-3318,0,\N,Missing
W14-3302,W14-3341,0,\N,Missing
W14-3302,W11-2101,1,\N,Missing
W14-3302,W14-3338,1,\N,Missing
W14-3322,W11-2138,1,0.918214,"Missing"
W14-3322,W13-2208,1,0.864737,"DEPFIX, Depfix post-processing is added; and CU - FUNKY also employs documentspecific language models. Introduction 2 TectoMT (§2.4) Factored Moses (§2.1) Adapted LM (§2.2) Document-specific LMs (§2.3) Depfix (§2.5) Y NK FI X -D -FU CU EP R -BO JA CU CU -TE CT OM T In this paper, we describe translation systems submitted by Charles University (CU or CUNI) to the Translation task of the Ninth Workshop on Statistical Machine Translation (WMT) 2014. In §2, we present our English→Czech systems, CU - TECTOMT, CU - BOJAR , CU - DEPFIX and CU FUNKY . The systems are very similar to our submissions (Bojar et al., 2013) from last year, the main novelty being our experiments with domainspecific and document-specific language models. In §3, we describe our experiments with English→Hindi translation, which is a translation pair new both to us and to WMT. We unsuccessfully experimented with reverse self-training and a morphological-tags-based language model, and so our final submission, CU - MOSES, is only a basic instance of Moses. CU 1 Abstract D D D D D D D D D D D D D Table 1: EN→CS systems submitted to WMT. 2.1 Our Baseline Factored Moses System Our baseline translation system (denoted “Baseline” in the fol"
W14-3322,P13-2121,0,0.043073,"Missing"
W14-3322,2005.mtsummit-papers.11,0,0.0931675,"our experiments with English→Hindi translation, which is a translation pair new both to us and to WMT. We unsuccessfully experimented with reverse self-training and a morphological-tags-based language model, and so our final submission, CU - MOSES, is only a basic instance of Moses. CU 1 Abstract D D D D D D D D D D D D D Table 1: EN→CS systems submitted to WMT. 2.1 Our Baseline Factored Moses System Our baseline translation system (denoted “Baseline” in the following) is similar to last year – we trained a factored Moses model on the concatenation of CzEng (Bojar et al., 2012) and Europarl (Koehn, 2005), see Table 2. We use two factors: tag, which is the part-of-speech tag, and stc, which is “supervised truecasing”, i.e. the surface form with letter case set according to the lemma; see (Bojar et al., 2013). Our factored Moses system translates from English stc to Czech stc |tag in one translation step. Our basic language models are identical to last year’s submission. We added an adapted language English→Czech Our submissions for English→Czech build upon last year’s successful C HIMERA system (Bojar et al., 2013). We combine several different approaches: • factored phrase-based Moses model ("
W14-3322,W10-1730,1,0.935053,"Missing"
W14-3322,P00-1056,0,0.191162,"on of vocabulary reported in the paper is to roughly one half. In our case, the vocabulary is reduced much more, so we opted for a more conservative back-off, namely “nosuf2”. Baseline System The baseline system was eventually our bestperforming one. Its design is completely straightforward – it uses one phrase table trained on all parallel data (we translate from “supervisedtruecased” English into Hindi forms) and one 5gram language model trained on all monolingual data. We used KenLM (Heafield et al., 2013) for estimating the model as the data was rather large (see Table 6). We used GIZA++ (Och and Ney, 2000) as our word alignment tool. We experimented with several coarser representations to make the final alignment more reliable. Table 7 shows the results. The factor “stem4” refers to simply taking the first four characters of each word. For lemmas, we used the outputs of the tools mentioned above. However, lemmas as output by the Hindi tagger were not much coarser than surface forms – the ratio between the number of types is merely 1.11 – so we also tried “stemming” the lemmas (lemma4). Of these variants, stem4-stem4 alignment worked best and we used it for the rest of our experiments. 3.2 BLEU"
W14-3322,P02-1040,0,0.0888957,"we do not use any stopwords or keyword detection methods, and also pretending that each sentence in our monolingual corpus is a “document” for the information retrieval system is far from ideal. We also evaluated a version of CU - BOJAR which uses not only the adapted LM but also an additional LM trained on the full 2013 News Crawl data (see “CU - BOJAR +full 2013 news” in Table 5) but found no improvement compared to using just the adapted model (trained on a subset of the data). Results 3 We report scores of automatic metrics as shown in the submission system,3 namely (case-sensitive) BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results, summarized in Table 5, show that CU - FUNKY is the most successful of our systems according to BLEU, while the simpler CU - DEPFIX wins in TER. The results of manual evaluation suggest that CU - DEPFIX (dubbed C HIMERA) remains the best performing English→Czech system. In comparison to other English→Czech systems submitted to WMT 2014, CU - FUNKY ranked as the second in BLEU, and CU - DEPFIX ranked 3 BLEU 21.1 21.6 20.9 21.2 20.2 15.2 20.7 English→Hindi English-Hindi is a new language pair this year. We submitted an unconstrained system for English→"
W14-3322,2006.amta-papers.25,0,0.0448398,"eyword detection methods, and also pretending that each sentence in our monolingual corpus is a “document” for the information retrieval system is far from ideal. We also evaluated a version of CU - BOJAR which uses not only the adapted LM but also an additional LM trained on the full 2013 News Crawl data (see “CU - BOJAR +full 2013 news” in Table 5) but found no improvement compared to using just the adapted model (trained on a subset of the data). Results 3 We report scores of automatic metrics as shown in the submission system,3 namely (case-sensitive) BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results, summarized in Table 5, show that CU - FUNKY is the most successful of our systems according to BLEU, while the simpler CU - DEPFIX wins in TER. The results of manual evaluation suggest that CU - DEPFIX (dubbed C HIMERA) remains the best performing English→Czech system. In comparison to other English→Czech systems submitted to WMT 2014, CU - FUNKY ranked as the second in BLEU, and CU - DEPFIX ranked 3 BLEU 21.1 21.6 20.9 21.2 20.2 15.2 20.7 English→Hindi English-Hindi is a new language pair this year. We submitted an unconstrained system for English→Hindi translation. We used Hin"
W14-3322,W07-1709,0,0.0300845,"Missing"
W14-3322,P09-2037,1,0.836354,"er General General General News stc stc tag stc 4 7 10 6 2.4 Sents Tokens ARPA.gz Trie [M] [M] [GB] [GB] 201.31 3430.92 28.2 11.8 24.91 444.84 13.1 8.1 14.83 205.17 7.2 3.0 0.25 4.73 0.2 – Table 4: Czech LMs used in CU - BOJAR. The last small model is described in §2.2. 1 Document-Specific Language Models TectoMT2 was one of the three key components in last year’s C HIMERA. It is a linguisticallymotivated tree-to-tree deep-syntactic translation system with transfer based on Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Tree Markov Models ˇ (Zabokrtsk´ y and Popel, 2009). It is trained on the WMT-provided data: CzEng 1.0 (parallel data) and News Crawl (2007–2012 Czech monolingual sets). We maintain the same approach to combining TectoMT with Moses as last year – we translate WMT test sets from years 2007–2014 and use them as additional synthetic parallel training data – a corpus consisting of the test set source side (English) and TectoMT output (synthetic Czech). We then use the standard extraction pipeline to create 2 http://lucene.apache.org 196 TectoMT Deep-Syntactic MT System http://ufal.mff.cuni.cz/tectomt/ an additional phrase table from this corpus. T"
W14-3322,W12-3148,1,\N,Missing
W14-3322,bojar-etal-2014-hindencorp,1,\N,Missing
W14-3336,W14-3302,1,0.115085,"tion (how often a metric agrees with humans in comparing two translations of a particular sentence). 1 2 We used the translations of MT systems involved in WMT14 Shared Translation Task together with reference translations as the test set for the Metrics Task. This dataset consists of 110 systems’ outputs and 10 reference translations in 10 translation directions (English from and into Czech, French, German, Hindi and Russian). For most of the translation directions each system’s output and the reference translation contain 3003 sentences. For more details please see the WMT14 overview paper (Bojar et al., 2014). 2.1 Manual MT Quality Judgements During the WMT14 Translation Task, a large scale manual annotation was conducted to compare the systems. We used these collected human judgements for the evalution of the automatic metrics. The participants in the manual annotation were asked to evaluate system outputs by ranking translated sentences relative to each other. For each source segment that was included in the procedure, the annotator was shown the outputs of five systems to which he or she was supposed to assign ranks. Ties were allowed. These collected rank labels for each five-tuple of systems"
W14-3336,W12-3102,0,0.02619,"06 ± .039 o .809 ± .036 o .823 ± .037 .799 ± .041 o .805 ± .039 o .809 ± .039 .768 ± .036 .745 ± .035 .696 ± .037 .915 ± .048 .785 ± .050 o .962 ± .038 .962 ± .038 .713 ± .040 Table 3: System-level correlations of automatic evaluation metrics and the official WMT human scores when translating out of English. The symbol “o” indicates where the Spearman’s ρ average is out of sequence compared to the main Pearson average. Correlation coefficient Direction Considered Systems NIST CDER AMBER M ETEOR BLEU PER APAC T BLEU BLEU NRC ELEXR TER WER PARMESAN UPC-IPA REDS YS S ENT REDS YS UPC-STOUT Tasks (Callison-Burch et al. (2012) and earlier), comparisons with human ties were considered as discordant. To easily see which pairs are counted as concordant and which as discordant, we have developed the following tabular notation. This is for example the WMT12 method: Metric WMT12 &lt; = &gt; &lt; 1 -1 -1 X X X = &gt; -1 -1 1 Human cases and assigning lots of ties. A natural solution is to count the metrics ties also in denominator to avoid the problem. We will denote this variant as WMT14: Metric WMT14 &lt; = &gt; &lt; 1 0 -1 = X X X &gt; -1 0 1 Human The WMT14 variant does not allow for gaming the scoring like the WMT13 variant does. Compared t"
W14-3336,W14-3346,0,0.0205604,"Missing"
W14-3336,W14-3347,0,0.0351421,"Missing"
W14-3336,W14-3348,0,0.139844,"Missing"
W14-3336,W14-3353,0,0.0667494,"Missing"
W14-3336,W13-2202,1,0.398635,"Missing"
W14-3336,W14-3349,0,0.0273695,"Missing"
W14-3336,P02-1040,0,0.101216,"d BLEU in system level correlation. In into-English directions, metric D ISCOTK- PARTYTUNED has the highest correlation in two language directions and it is also the best correlated metric on average according to both Pearson and Spearman’s coefficients. The second best correlated metric on average (according to Pearson) is LAYERED which is also the single best metric in Hindi-to-English direction. Metrics REDS YS and REDS YS S ENT are quite unstable, they win in French-to-English and Czech-to-English directions respectively but they perform very poorly in r = qP n • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl2 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences we used the standard tokenizer script as available in Moses toolkit. We have normalized all metrics’ scores such that"
W14-3336,2006.amta-papers.25,0,0.591212,"Missing"
W14-3336,W14-3351,0,0.0488426,"Missing"
W14-3336,W14-3352,0,0.131071,"Missing"
W14-3336,W14-3355,0,0.0656081,"Missing"
W14-3336,W06-3114,0,0.139007,"n each system a score that reflects how high that system was usually ranked by the annotators. Please see the WMT14 overview paper for details on how this score is computed. You can also find inter- and intra-annotator agreement estimates there. Introduction Automatic machine translation metrics play a very important role in the development of MT systems and their evaluation. There are many different metrics of diverse nature and one would like to assess their quality. For this reason, the Metrics Shared Task is held annually at the Workshop of Statistical Machine Translation1 , starting with Koehn and Monz (2006) and following up to Bojar et al. (2014). In this task, we asked metrics developers to score the outputs of WMT14 Shared Translation Task (Bojar et al., 2014). We have collected the computed metrics’ scores and use them to evaluate quality of the metrics. The systems’ outputs, human judgements and evaluated metrics are described in Section 2. The quality of the metrics in terms of system level correlation is reported in Section 3. Segment level correlation with a detailed discussion and a slight 1 Data 2.2 Participants of the Metrics Shared Task Table 1 lists the participants of WMT14 Shared M"
W14-5505,baker-etal-2002-emille,0,0.783649,"Missing"
W14-5505,P05-1033,0,0.0648682,"as the latter model makes the reordering cost (paid when picking source phrases out of sequence) dependent only on the length of the jump. The distance-based model is suited well for local reordering but it is fairly weak in capturing any long distance reorderings. The syntax-based model (SBMT) builds upon Synchronous Context-Free Grammar (SCFG) that synchronously generates source and target sentences. The grammar rules can either consist of linguistically motivated non-terminals such as NP, VP etc. or the generic non-terminal “X” in which case the model is called “hierarchical phrase-based” (Chiang, 2005; Chiang, 2007). In either case, the model is capable of capturing long-distance reordering much better than the lexicalized reordering of PBMT. 2.2 Data Processing and MT Training For the training of our en-ur translation systems, the standard training pipeline of Moses is used along with the GIZA++ (Och and Ney, 2000) alignment toolkit and a 5-gram SRILM language model (Stolcke, 2002). The source texts were processed using the Treex platform (Popel and Žabokrtský, 2010)2 , which included tokenization and lemmatization. The target side of the corpus is tokenized using a simple tokenization sc"
W14-5505,J07-2003,0,0.021675,"model makes the reordering cost (paid when picking source phrases out of sequence) dependent only on the length of the jump. The distance-based model is suited well for local reordering but it is fairly weak in capturing any long distance reorderings. The syntax-based model (SBMT) builds upon Synchronous Context-Free Grammar (SCFG) that synchronously generates source and target sentences. The grammar rules can either consist of linguistically motivated non-terminals such as NP, VP etc. or the generic non-terminal “X” in which case the model is called “hierarchical phrase-based” (Chiang, 2005; Chiang, 2007). In either case, the model is capable of capturing long-distance reordering much better than the lexicalized reordering of PBMT. 2.2 Data Processing and MT Training For the training of our en-ur translation systems, the standard training pipeline of Moses is used along with the GIZA++ (Och and Ney, 2000) alignment toolkit and a 5-gram SRILM language model (Stolcke, 2002). The source texts were processed using the Treex platform (Popel and Žabokrtský, 2010)2 , which included tokenization and lemmatization. The target side of the corpus is tokenized using a simple tokenization script3 by Dan Ze"
W14-5505,W13-4709,0,0.0589873,"o-European languages, e.g. by having inﬂectional morphological system. To the best of our knowledge, the research on English-to-Urdu machine translation has been very much fragmented, preventing the authors to build upon the works of others. Our underlying motivation for this paper is to establish a common ground and provide a concise summary of available data resources and set up reproducible baseline results of several available test sets. With this basis, future Urdu MT research should be able to stepwise improve the state of the art, in contrast with the scattered experiments done so far (Khan et al., 2013; Ali et al., 2013; Ali and Malik, 2010). In Section 2, the experimental setup and data processing tools are described. Existing corpora are introduced in Section 3, automatic results are reported in Section 4 and manual evaluation is discussed in Section 5. 2 Experimental Setup This section brieﬂy introduces the selection of SMT models that are used to build the baseline English-Urdu SMT system and also explains the processing of parallel data before passing it to the MT system. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings"
W14-5505,P07-2045,1,0.0173265,"that are used to build the baseline English-Urdu SMT system and also explains the processing of parallel data before passing it to the MT system. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4. 0/ 37 Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 37–42, Dublin, Ireland, August 23-29 2014. 2.1 Two Models of SMT The state-of-the-art MT toolkit Moses1 (Koehn et al., 2007), oﬀers two mainstream models of SMT: phrase-based (PBMT) and syntax-based (SBMT) that includes the hierarchical model. The PBMT model operates only on mapping of source phrases (short sequences of words) to target phrases. For dealing with word order diﬀerences, two rather weak models are available: lexicalized and distance-based. The lexicalized reordering models (Tillmann, 2004) are considered more advanced as they condition reordering on the actual phrases, whereas the latter model makes the reordering cost (paid when picking source phrases out of sequence) dependent only on the length of"
W14-5505,J93-2004,0,0.0482335,"n a separate MTurk task, the Turkers voted which of the English translations is the best one. The oﬃcial training, dev and devtest sets is ﬁrst merged and afterwards the voting list is used to retrieve only the winning English sentence ignoring sentences with no votes altogether. The oﬃcial testset is left unaltered to report our ﬁnal results on this data. • Quran: The publicly available parallel English and Urdu translation of Quranic data6 is used, which is collected by Jawaid and Zeman (2011) in their work. The data consists of 6K aligned parallel sentences. • Penn Treebank: Penn Treebank (Marcus et al., 1993) is an annotated corpus of around 4.5 million words originating from Wall Street Journal (WSJ), Brown corpus, Switchboard and ATIS. The entire treebank in English is released by the Linguistic Data Consortium (LDC). A subset of the WSJ section whose Urdu translations are provided by Center for Language Engineering (CLE)7 is used. Out of 2,499 WSJ stories in the Treebank, only 317 are available in Urdu. • Afrl: Afrl, the largest of the parallel resources we were able to get, is not publicly available. The corpus originally consists of 87K sentences coming from mix of several domains mainly news"
W14-5505,C00-2163,0,0.090718,"nous Context-Free Grammar (SCFG) that synchronously generates source and target sentences. The grammar rules can either consist of linguistically motivated non-terminals such as NP, VP etc. or the generic non-terminal “X” in which case the model is called “hierarchical phrase-based” (Chiang, 2005; Chiang, 2007). In either case, the model is capable of capturing long-distance reordering much better than the lexicalized reordering of PBMT. 2.2 Data Processing and MT Training For the training of our en-ur translation systems, the standard training pipeline of Moses is used along with the GIZA++ (Och and Ney, 2000) alignment toolkit and a 5-gram SRILM language model (Stolcke, 2002). The source texts were processed using the Treex platform (Popel and Žabokrtský, 2010)2 , which included tokenization and lemmatization. The target side of the corpus is tokenized using a simple tokenization script3 by Dan Zeman and it is lemmatized using the Urdu Shallow Parser4 developed by Language Technologies Research Center of IIIT Hyderabad. The alignments are learnt from the lemmatized version of the corpus. In all other cases, word forms (i.e. no morphological decomposition) in their true case (i.e. names capitalized"
W14-5505,P02-1040,0,0.0874075,"Missing"
W14-5505,W12-3152,0,0.0632007,"rdu-English part are documents produced by the British Departments of Health, Social Services, Education and Skills, and Transport, Local Government and the Regions of British government translated into Urdu. In this work, the manually sentence aligned version of English-Urdu Emille corpus Jawaid and Zeman (2011) is used. 1 http://statmt.org/moses/ http://ufal.mff.cuni.cz/treex/ 3 The tokenization script can be downloaded from: http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5 4 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_parser.php 2 38 • IPC: The Indic Parallel Corpus (Post et al., 2012)5 is a collection of Wikipedia documents of six Indian sub-continent languages translated into English through crowdsourcing in the Amazon Mechanical Turk (MTurk) platform. The English-Urdu part generally contains four (in some cases three) English translations for each Urdu sentence. In a separate MTurk task, the Turkers voted which of the English translations is the best one. The oﬃcial training, dev and devtest sets is ﬁrst merged and afterwards the voting list is used to retrieve only the winning English sentence ignoring sentences with no votes altogether. The oﬃcial testset is left unalt"
W14-5505,N04-4026,0,0.0241497,"rkshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 37–42, Dublin, Ireland, August 23-29 2014. 2.1 Two Models of SMT The state-of-the-art MT toolkit Moses1 (Koehn et al., 2007), oﬀers two mainstream models of SMT: phrase-based (PBMT) and syntax-based (SBMT) that includes the hierarchical model. The PBMT model operates only on mapping of source phrases (short sequences of words) to target phrases. For dealing with word order diﬀerences, two rather weak models are available: lexicalized and distance-based. The lexicalized reordering models (Tillmann, 2004) are considered more advanced as they condition reordering on the actual phrases, whereas the latter model makes the reordering cost (paid when picking source phrases out of sequence) dependent only on the length of the jump. The distance-based model is suited well for local reordering but it is fairly weak in capturing any long distance reorderings. The syntax-based model (SBMT) builds upon Synchronous Context-Free Grammar (SCFG) that synchronously generates source and target sentences. The grammar rules can either consist of linguistically motivated non-terminals such as NP, VP etc. or the g"
W14-5505,jawaid-etal-2014-tagged,1,\N,Missing
W14-5808,Q13-1034,1,0.823033,"Missing"
W14-5808,A00-2018,0,0.0661509,"Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (H"
W14-5808,J02-3001,0,0.013808,"nnotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create"
W14-5808,J93-2004,0,0.0452856,"also present quantitative comparison on 100 parallel sentences, for all the aforementioned categories and some of their subtypes. We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Ro"
W14-5808,W04-2705,0,0.050433,"first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al.,"
W14-5808,J05-1004,0,0.0825818,"ome of their subtypes. We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue an"
W14-5808,N07-1051,0,0.0137229,"anks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and"
W14-5808,N04-1030,0,0.050937,"opBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create a unified representati"
W14-5808,prasad-etal-2008-penn,0,0.0548286,"resent the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are a"
W14-5808,sindlerova-etal-2014-resources,1,0.82705,"nk leaves out ARG0 e.g. for unaccusative verbs (for example The window.ARG1 broke vs. Okno.ARG0←ACT/window se/itself rozbilo/broke.). Finally, some differences are due to some arguments not being considered arguments at all in the other language, in which case some other AMR label is used instead (for example, We could have spent 400M.ARG3 ... elsewhere vs. ... mohli/could utratit/spend 400M.extent ... jinde/elsewhere). These differences could possibly be consolidated (only) by carefully linking the two lexicons (with AMR guidelines intact). This is in fact being performed in another project (Sindlerova et al., 2014), but it is a daunting manual task, since the underlying theories behind PropBank and PDT-Vallex/EngVallex differ. However, one has to ask if it does make sense to do so, because with enough parallel data available, the mappings can be learned relatively easily: in most cases, no structural differences are involved and there will be a simple one-to-one mapping between the labels (conditioned on the particular verb sense). 4.4 Structural Differences Local differences can be safely ignored, since they will be in most cases resolved during the assumed process of wikification, i.e., linking to an"
W14-5808,Q13-1019,0,0.0125518,"ibution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create a unified representation which would cover at least a whole sentence, or even a continuous text (Hajiˇc et al., 2003; Srikumar and Roth, 2013), and currently the Abstract Meaning Representation represents an attempt to provide a common ground for truly semantic and fully covering annotation representation. An Abstract Meaning Representation is a rooted, directional and labeled graph that represents the meaning of a sentence and it abstracts away from such syntactic notions as word category (verbs and nouns), word order, morphological variation etc. Instead, it focuses on semantic relations between concepts and makes heavy use of predicate-argument structures as defined in PropBank (for English). As a result, the word order in the se"
W14-5808,W04-3212,0,0.0235348,"2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create a unified representation which would cover a"
W14-5808,xue-etal-2014-interlingua,1,0.234271,"ions into Czech, annotated manually by AMRs, with the goal to describe the differences and if possible, to classify them into two main categories: those which are merely convention differences and thus can be unified by changing such conventions in the AMR annotation guidelines, and those which are so deeply rooted in the language structure that the level of abstraction which is inherent in the current AMR scheme does not allow for such unification. 1 Introduction In this paper, we follow on a previous first exploratory investigation of differences in AMR annotation among different languages (Xue et al., 2014), which has classified the similarities and differences into four categories: (a) no difference, (b) local difference only (such as multiword expressions vs. single word terms), (c) reconcilable difference due to AMR conventions, and (d) deep differences which cannot be unified in the AMR guidelines. In this paper, we would like to elaborate especially on the (b) and (c) types, which have been only exemplified in the previous work. In this paper, we would like to not only go deeper, but also present quantitative comparison on 100 parallel sentences, for all the aforementioned categories and so"
W14-5808,J03-4003,0,\N,Missing
W14-5808,W09-1201,1,\N,Missing
W14-5808,W13-2322,0,\N,Missing
W15-3001,W05-0909,0,0.0473932,"asks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13 (Bojar et al., 2013). These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor (Banerjee and Lavie, 2005). Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scor"
W15-3001,2011.mtsummit-papers.35,0,0.348896,"Missing"
W15-3001,W13-2241,1,0.851171,"Missing"
W15-3001,P13-2097,1,0.801912,"Missing"
W15-3001,W13-2242,0,0.0386206,"Missing"
W15-3001,W14-3339,0,0.0452066,"Missing"
W15-3001,W15-3035,0,0.054515,"Missing"
W15-3001,W11-2103,1,0.524514,"Missing"
W15-3001,W13-2201,1,0.499374,"Missing"
W15-3001,W14-3302,1,0.498006,"Missing"
W15-3001,W14-3340,1,0.54686,"Missing"
W15-3001,W15-3007,0,0.0166876,"Missing"
W15-3001,W15-3006,1,0.827804,"Missing"
W15-3001,W07-0718,1,0.664541,"om 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (Stanojevi´c et al., 2015a,b)."
W15-3001,2014.amta-researchers.13,0,0.0200349,"Missing"
W15-3001,W08-0309,1,0.406319,"2. machine translation and automatic evaluation or prediction of translation quality. 2 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in 2008 and 2009 (Callison-Burch et al., 2008, 2009). We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 2.1 2.3 We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table 2; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained"
W15-3001,W15-3025,1,0.914094,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W10-1703,1,0.163282,"Missing"
W15-3001,P15-2026,1,0.797338,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W12-3102,1,0.571688,"ator agreement, both for inter- and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English–German, e.g., PROMT- RULE). The same is true to a lesser degree for statistical syntax-based systems (see English–German, UEDIN - SYNTAX ) and online systems that were not tuned to the shared task (see Czech–English, CU TECTO vs. the cluster of tuning task systems TT*). 4 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014), with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year’s shared task were: • Advance work on sentence- and wordlevel quality estimation by providing larger datasets. • Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. Quality Estimation Task • Explore differences between sentence-level and document-level prediction. The fourth edition of the WMT shared task on quality estimation (QE) of mac"
W15-3001,W15-3008,0,0.0135172,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W09-0401,1,0.251315,"Missing"
W15-3001,W15-3009,0,0.0460438,"Missing"
W15-3001,W15-3010,0,0.035772,"Missing"
W15-3001,P10-4002,0,0.00861255,"t sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features:"
W15-3001,W15-3011,0,0.0373266,"Missing"
W15-3001,W15-3036,0,0.0738999,"Missing"
W15-3001,W15-3012,0,0.0168435,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W15-4903,0,0.0623526,"Missing"
W15-3001,W15-3013,1,0.843414,"Missing"
W15-3001,W11-2123,0,0.0240468,"27,101 5,966 8,816 SRC 13,701 3,765 5,307 Lemmas TGT PE 7,624 7,689 2,810 2,819 3,778 3,814 Table 18: Data statistics. and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee e"
W15-3001,W08-0509,0,0.0268564,"ere collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it sui"
W15-3001,W15-3014,0,0.0344469,"Missing"
W15-3001,P15-1174,0,0.0105686,"415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: exact match, tokenised and case insensitive (Gim´enez and M`arquez, 2010). guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in (Graham, 2015), Table 16 shows an alternative ranking of systems considering Pearson’s r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTMDCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISCBO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 4.6 Discussion In what follows, we discuss the main findings of this year’s shared task based on the goals we had previously identified for it."
W15-3001,W04-3250,1,0.485885,"f the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible"
W15-3001,2005.mtsummit-papers.11,1,0.0759255,"(Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features: – The order of the highest order n-gram which starts or ends with the target token. – Backoff behaviour of the n-grams (ti−2 , ti−1 , ti ), (ti−1 , ti , ti+1 ), (ti , ti+1 , ti+2 ), where"
W15-3001,W15-3039,1,0.80659,"Missing"
W15-3001,J10-4005,0,0.0619684,"Missing"
W15-3001,J82-2005,0,0.818658,"Missing"
W15-3001,W14-3342,0,0.0353204,"t although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 25 features: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST7 (Specia et al., 2013) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. • Number of tokens in the source and target sentences. • Ave"
W15-3001,W13-2248,0,0.0824189,"Missing"
W15-3001,W06-3114,1,0.427665,"translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (S"
W15-3001,W15-3015,0,0.0443946,"Missing"
W15-3001,W15-3016,0,0.0442638,"Missing"
W15-3001,W15-3037,0,0.0800739,"Missing"
W15-3001,P03-1021,0,0.0587969,"utomatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara"
W15-3001,padro-stanilovsky-2012-freeling,0,0.011024,"o rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (∼ 0.7 TER difference) measured for the two baselines. USAAR-SAPE. The USAAR-SAPE system (Pal et al., 2015b) is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling (Padr and Stanilovsky, 2012). The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the 31 ID Baseline FBK Primary LIMSI Primary USAAR-SAPE LIMSI Contrastive Abu-MaTran Primary FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Avg. TER 22.913 23.228 23.331 23.426 23.573 23.639 23.649 23.839 24.715 ID Baseline LIMSI Primary FBK Primary USAAR-SAPE Abu-MaTran Primary LIMSI Contrastive FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Table 20: Official results for the WMT15 Automatic Post-editing task – average TER (↓) case sensitive. Table 21: Official"
W15-3001,W15-3038,0,0.0668224,"Missing"
W15-3001,W13-2814,0,0.0155219,"ject (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniews"
W15-3001,W07-0734,0,0.0277649,"Abu-MaTran FBK LIMSI USAAR-SAPE Participating team Abu-MaTran Project (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of"
W15-3001,W15-3017,0,0.0328586,"Missing"
W15-3001,W15-3026,0,0.0488223,"Missing"
W15-3001,W15-3040,1,0.889327,"HIDDEN Participating team Dublin City University, Ireland and University of Sheffield, UK (Logacheva et al., 2015) Heidelberg University, Germany (Kreutzer et al., 2015) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) Dublin City University, Ireland (Bicici et al., 2015) Shenyang Aerospace University, China (Shang et al., 2015) University of Sheffield Team 1, UK (Shah et al., 2015) Alicant University, Spain (Espl`a-Gomis et al., 2015a) Ghent University, Belgium (Tezcan et al., 2015) University of Sheffield, UK and Saarland University, Germany (Scarton et al., 2015a) University of Sheffield, UK (Scarton et al., 2015a) Undisclosed Table 7: Participants in the WMT15 quality estimation shared task. one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. and fine-tuned for the quality estimation classification task by back-propagat"
W15-3001,W15-4916,1,0.80858,"Missing"
W15-3001,P02-1040,0,0.108957,"ndia (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniewski et al., 2015). The first one is based on the approach of Simard et al. (2007) and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in imp"
W15-3001,2012.eamt-1.34,0,0.0799883,"Missing"
W15-3001,W15-3023,0,0.0267499,"Missing"
W15-3001,W15-3018,0,0.0412931,"Missing"
W15-3001,W15-3041,1,0.847113,"Missing"
W15-3001,potet-etal-2012-collection,0,0.011237,"Missing"
W15-3001,W15-3042,0,0.0858502,"Missing"
W15-3001,W15-3019,0,0.0362545,"Missing"
W15-3001,N07-1064,0,0.644928,"nsitive) are reported in Tables 20 and 21. • The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; • The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specifi"
W15-3001,W15-3022,0,0.0629217,"Missing"
W15-3001,P15-4020,1,0.0689693,"://github.com/lspecia/quest 14 http://scikit-learn.org/ https://github.com/qe-team/marmot • Target token, its left and right contexts of one word. Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boole"
W15-3001,W15-3027,0,0.0625354,"Missing"
W15-3001,P13-4014,1,0.762876,"Missing"
W15-3001,2013.mtsummit-papers.15,0,0.055239,"these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by Wisniewski et al. (2013), results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 5.3 Results The official results achieved by the participating systems are reported in Tables 20 and 21. The seven runs submitted are sorted based on the average TER they achieve on test data. Table 20 shows the results computed in case sensitive mode, while Table 21 provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22"
W15-3001,W15-3032,1,0.810291,"Missing"
W15-3001,W15-3031,1,0.376914,"Missing"
W15-3001,W15-3020,1,0.808362,"Missing"
W15-3001,W15-3043,0,0.0443554,"Missing"
W15-3001,W15-3021,0,0.038175,"Missing"
W15-3001,P07-2045,1,\N,Missing
W15-3001,W15-3004,0,\N,Missing
W15-3001,2015.eamt-1.4,0,\N,Missing
W15-3001,N06-1014,0,\N,Missing
W15-3001,2015.eamt-1.17,1,\N,Missing
W15-3001,2012.iwslt-papers.12,0,\N,Missing
W15-3006,W14-3322,1,0.725436,"itute of Formal and Applied Linguistics Malostransk´e n´amˇest´ı 25, Prague, Czech Republic surname@ufal.mff.cuni.cz Abstract In the following, we provide various details of the setup. We leave Depfix aside, since we simply applied it as a post-processing step and the relevant analysis of its rules was published previously (Bojar et al., 2013). This paper describes our WMT15 system submission for the translation task, a hybrid system for English-to-Czech translation. We repeat the successful setup from the previous two years. 1 2 Introduction Chimera in WMT15 2.1 C HIMERA (Bojar et al., 2013; Tamchyna et al., 2014) is our English-to-Czech MT system designed as a combination of three very different components: Factored Setup We use our established setup, translating from English word form in one translation step to the Czech word form and morphological tag. This allows us to use language models over morphological tags, see §2.5 below. Our word forms are in truecase, i.e. the words at sentence beginnings are lowercased, unless they are names. We rely on Czech and English lemmatizers2 to select the true case. Otherwise, our setup is fairly standard. We do not use any models of reordering, relying on basic"
W15-3006,W11-2123,0,0.0222644,"ns 235.19M 17.62M 291.38M 93.44M 24330 4092 # cs tokens 206.05M 15.00M 237.61M 84.81M 23106 41591 Constrained? D D - Table 1: Summary of parallel data used in our constrained and full setup. Czech Press CWC articles CzEng news RSS WMT mono # sents 305.41M 38.42M 0.20M 4.81M 44.08M # tokens 4852.59M 627.97M 4.22M 73.68M 738.88M long - big D D D D D D D Full morph - D D D longmorph - D D D long - D Constrained morph longmorph - D D D D Table 2: Monolingual data sources and LMs. 2.4 Long is a 7-gram model based on our truecased word forms. While the remaining LMs are trained directly with KenLM (Heafield, 2011), this 7-gram LMs is interpolated with SRILM from separate (KenLM) ARPA files estimated from each of the years separately. The lambdas for the interpolation are set to optimize the perplexity on WMT newstest2012. This approach allows us to use the relatively high order of the model and probably serves also as a kind of smoothing, distributing more probability mass to n-grams that are important across several years. Monolingual Data Table 2 summarizes the monolingual data that we use in the full and in the constrained setup. Czech Press is a very large collection of news texts acquired in 2012."
W15-3006,P07-2045,1,0.0087363,"e use our established setup, translating from English word form in one translation step to the Czech word form and morphological tag. This allows us to use language models over morphological tags, see §2.5 below. Our word forms are in truecase, i.e. the words at sentence beginnings are lowercased, unless they are names. We rely on Czech and English lemmatizers2 to select the true case. Otherwise, our setup is fairly standard. We do not use any models of reordering, relying on basic distortion penalty. ˇ • TectoMT (Popel and Zabokrtsk´ y, 2010), a deep-syntactic transfer-based system, • Moses (Koehn et al., 2007), where we use a factored phrase-based setup with large language models, • Depfix (Rosa et al., 2012), an automatic postediting system, aimed at correcting mainly errors in morphological agreement but successful also in semantic corrections, esp. recovery of lost negation. 2.2 Our System Combination The first two components of C HIMERA, TectoMT (which appears in WMT evaluations as CU TECTOMT ) and Moses are independent MT systems on their own. C HIMERA combines them in a way remotely similar to standard system combination techniques (Matusov et al., 2008) and adds the third component, Depfix,"
W15-3006,W12-3146,0,0.234855,"Missing"
W15-3006,W15-4103,1,0.868584,"Missing"
W15-3031,W06-3114,1,0.788111,"system outputs by ranking translated sentences relative to each other. For each source segment that was included in the procedure, the annotator was shown five different outputs to which he or she was supposed to assign ranks. Ties were allowed. Introduction Automatic machine translation metrics play a very important role in the development of MT systems and their evaluation. There are many different metrics of diverse nature and one would like to assess their quality. For this reason, the Metrics Shared Task is held annually at the Workshop of Statistical Machine Translation1 , starting with Koehn and Monz (2006) and following up to Mach´acˇ ek and Bojar (2014). The systems’ outputs, human judgements and evaluated metrics are described in Section 2. The quality of the metrics in terms of system level correlation is reported in Section 3. Section 4 is devoted to segment level correlation. 2 These collected rank labels for each five-tuple of outputs were then interpreted as pairwise comparisons of systems and used to assign each system a score that reflects how high that system was usually ranked by the annotators. Several methods have been tested in the past for the exact score calculation and WMT15 ha"
W15-3031,W15-3032,1,0.838791,"Missing"
W15-3031,E06-1031,0,0.0403601,"e flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 2.2.8 U OW- LSTM U OW- LSTM uses dependency-tree recursive neural network to represent both the hypothesis and the reference with a dense vector. The final score is obtained from a neural network trained on judgements from previous years converted to similarity scores, taking into account both the distance and angle of the two representations. U OW- LSTM tied for the best place in fr-en system-level evaluation with DPMF. • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences, we used the standard tokenizer script as available in Moses toolkit. 2.2.9 UPF - COBALT UPF - COBALT pays an increased attention to syntactic context (for example arguments, complements, modifiers etc.) both in aligning the words of the hypothesis and reference as well as in scoring of the matched words. It relies on additional resources including stemmers, WordNet synsets, paraphrase databases and distributed word representations. UPF - COBALT system-level score was calculated by taking"
W15-3031,W15-3051,0,0.0746818,"Missing"
W15-3031,W15-3052,0,0.0615378,"Missing"
W15-3031,W15-3053,0,0.092353,"Missing"
W15-3031,W15-3046,0,\N,Missing
W15-3031,W15-3044,0,\N,Missing
W15-3031,W15-3001,1,\N,Missing
W15-3031,W13-2202,1,\N,Missing
W15-3031,W11-2101,1,\N,Missing
W15-3031,D15-1124,0,\N,Missing
W15-3031,W15-3047,0,\N,Missing
W15-3032,N12-1047,0,0.0384564,"able 2: Out of vocabulary word counts aspects of model optimization: (1) the set of features in the model, (2) optimization algorithm, and (3) MT quality metric used in optimization. For (1), we provide a fixed set of “dense” features and also allow participants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Hoang et al., 2009) implementation of hierarchical phrase-based model (Chiang, 2005). The language models were 5-gram models with Kneser-Ney smoothing (Kneser and Ney, 1995) built using KenLM (Heafield et al., 2013). For word alignments, we used Mgiza++ (Gao and Vogel, 2008). The parallel data used for training translation models consisted of the Europarl v7, News"
W15-3032,P05-1033,0,0.111809,"gorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Hoang et al., 2009) implementation of hierarchical phrase-based model (Chiang, 2005). The language models were 5-gram models with Kneser-Ney smoothing (Kneser and Ney, 1995) built using KenLM (Heafield et al., 2013). For word alignments, we used Mgiza++ (Gao and Vogel, 2008). The parallel data used for training translation models consisted of the Europarl v7, News Commentary data (parallel-nc-v9) and CommonCrawl, as released for WMT14.3 We excluded CzEng because we wanted to keep the task small and accessible to more groups. Since the test set (newstest2015) and the development set (newstest2014) are in the news domain, we opted to exclude Europarl from the language model dat"
W15-3032,W11-2107,0,0.140585,"e allowed to add any sparse features implemented in Moses Release 3.0 (corresponds to Github commit 5244a7b607) and/or to use any optimization algorithm and evaluation metric. Fully manual 3 http://www.statmt.org/wmt14/ translation-task.html 275 System BLEU -* AFRL DCU HKUST ILLC-U VA METEOR-CMU USAAR-T UNA Participant baselines United States Air Force Research Laboratory (Erdmann and Gwinnup, 2015) Dublin City University (Li et al., 2015) Hong Kong University of Science and Technology (Lo et al., 2015) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) Carnegie Mellon University (Denkowski and Lavie, 2011) Saarland University (Liling Tan and Mihaela Vela; no corresponding paper) Table 3: Participants of WMT15 Tuning Shared Task the WMT15 Translation Task (Bojar et al., 2015). We leave all decoder settings (n-best list size, pruning limits etc.) at their default values. While the participants may have used different limits during tuning, the final test run was performed at our site with the default values. It is indeed only the feature weights that differ. 3 mantics into MEANT and handling failures of the underlying semantic parser. The submission of HKUST contained a bug that was discovered aft"
W15-3032,W15-3054,0,0.0914258,"odel consists of a rule table extracted from the parallel corpus, the default glue grammar and the language model extracted from the monolingual data. As such, this defines a fixed set of dense features. The participants were allowed to add any sparse features implemented in Moses Release 3.0 (corresponds to Github commit 5244a7b607) and/or to use any optimization algorithm and evaluation metric. Fully manual 3 http://www.statmt.org/wmt14/ translation-task.html 275 System BLEU -* AFRL DCU HKUST ILLC-U VA METEOR-CMU USAAR-T UNA Participant baselines United States Air Force Research Laboratory (Erdmann and Gwinnup, 2015) Dublin City University (Li et al., 2015) Hong Kong University of Science and Technology (Lo et al., 2015) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) Carnegie Mellon University (Denkowski and Lavie, 2011) Saarland University (Liling Tan and Mihaela Vela; no corresponding paper) Table 3: Participants of WMT15 Tuning Shared Task the WMT15 Translation Task (Bojar et al., 2015). We leave all decoder settings (n-best list size, pruning limits etc.) at their default values. While the participants may have used different limits during tuning, the final test run was performed at ou"
W15-3032,W08-0509,0,0.0281934,"dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Hoang et al., 2009) implementation of hierarchical phrase-based model (Chiang, 2005). The language models were 5-gram models with Kneser-Ney smoothing (Kneser and Ney, 1995) built using KenLM (Heafield et al., 2013). For word alignments, we used Mgiza++ (Gao and Vogel, 2008). The parallel data used for training translation models consisted of the Europarl v7, News Commentary data (parallel-nc-v9) and CommonCrawl, as released for WMT14.3 We excluded CzEng because we wanted to keep the task small and accessible to more groups. Since the test set (newstest2015) and the development set (newstest2014) are in the news domain, we opted to exclude Europarl from the language model data. We did not add any monolingual news on top of News Commentary, which are quite close to the news domain. In retrospect, we should have added also some of the monolingual news data as relea"
W15-3032,2014.iwslt-evaluation.4,0,0.086516,"Missing"
W15-3032,P13-2121,0,0.0480516,"Missing"
W15-3032,2009.iwslt-papers.4,0,0.0787001,"pants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Hoang et al., 2009) implementation of hierarchical phrase-based model (Chiang, 2005). The language models were 5-gram models with Kneser-Ney smoothing (Kneser and Ney, 1995) built using KenLM (Heafield et al., 2013). For word alignments, we used Mgiza++ (Gao and Vogel, 2008). The parallel data used for training translation models consisted of the Europarl v7, News Commentary data (parallel-nc-v9) and CommonCrawl, as released for WMT14.3 We excluded CzEng because we wanted to keep the task small and accessible to more groups. Since the test set (newstest2015) and the development set (newstest2014) are in the news"
W15-3032,D11-1125,0,0.0572143,"word counts aspects of model optimization: (1) the set of features in the model, (2) optimization algorithm, and (3) MT quality metric used in optimization. For (1), we provide a fixed set of “dense” features and also allow participants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Hoang et al., 2009) implementation of hierarchical phrase-based model (Chiang, 2005). The language models were 5-gram models with Kneser-Ney smoothing (Kneser and Ney, 1995) built using KenLM (Heafield et al., 2013). For word alignments, we used Mgiza++ (Gao and Vogel, 2008). The parallel data used for training translation models consisted of the Europarl v7, News Commentary data (parall"
W15-3032,N10-1080,0,0.307115,"s using some evaluation measure (traditionally called “metric” in the MT field). In short, the optimizer tunes model weights so that the final combined model score correlates with the metric score. The metric score, in turn, is designed to correlate well with human judgements of translation quality, see Stanojevi´c et al. (2015) and the previous papers summarizing WMT metrics tasks. However, a metric that correlates well with humans on final output quality may not be usable in weight optimization for various technical reasons. BLEU (Papineni et al., 2002) was shown to be very hard to surpass (Cer et al., 2010) and this is also confirmed by the results of the invitation-only WMT11 Tunable Metrics Task (Callison-Burch et al., 2010)1 . Note however, that some metrics have been successfully used for system tuning (Liu et al., 2011; Beloucif et al., 2014). The aim of the WMT15 Tuning Task2 is to attract attention to the exploration of all the three This paper presents the results of the WMT15 Tuning Shared Task. We provided the participants of this task with a complete machine translation system and asked them to tune its internal parameters (feature weights). The tuned systems were used to translate th"
W15-3032,D11-1035,0,0.168814,"n, is designed to correlate well with human judgements of translation quality, see Stanojevi´c et al. (2015) and the previous papers summarizing WMT metrics tasks. However, a metric that correlates well with humans on final output quality may not be usable in weight optimization for various technical reasons. BLEU (Papineni et al., 2002) was shown to be very hard to surpass (Cer et al., 2010) and this is also confirmed by the results of the invitation-only WMT11 Tunable Metrics Task (Callison-Burch et al., 2010)1 . Note however, that some metrics have been successfully used for system tuning (Liu et al., 2011; Beloucif et al., 2014). The aim of the WMT15 Tuning Task2 is to attract attention to the exploration of all the three This paper presents the results of the WMT15 Tuning Shared Task. We provided the participants of this task with a complete machine translation system and asked them to tune its internal parameters (feature weights). The tuned systems were used to translate the test set and the outputs were manually ranked for translation quality. We received 4 submissions in the English-Czech and 6 in the Czech-English translation direction. In addition, we ran 3 baseline setups, tuning the p"
W15-3032,W15-3056,0,0.222921,"tracted from the monolingual data. As such, this defines a fixed set of dense features. The participants were allowed to add any sparse features implemented in Moses Release 3.0 (corresponds to Github commit 5244a7b607) and/or to use any optimization algorithm and evaluation metric. Fully manual 3 http://www.statmt.org/wmt14/ translation-task.html 275 System BLEU -* AFRL DCU HKUST ILLC-U VA METEOR-CMU USAAR-T UNA Participant baselines United States Air Force Research Laboratory (Erdmann and Gwinnup, 2015) Dublin City University (Li et al., 2015) Hong Kong University of Science and Technology (Lo et al., 2015) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) Carnegie Mellon University (Denkowski and Lavie, 2011) Saarland University (Liling Tan and Mihaela Vela; no corresponding paper) Table 3: Participants of WMT15 Tuning Shared Task the WMT15 Translation Task (Bojar et al., 2015). We leave all decoder settings (n-best list size, pruning limits etc.) at their default values. While the participants may have used different limits during tuning, the final test run was performed at our site with the default values. It is indeed only the feature weights that differ. 3 mantics into MEANT an"
W15-3032,P03-1021,0,0.0603998,"sion in the tuning task consisted of the configuration of the MT system, i.e. the additional sparse features (if any) and the values of all the feature weights, λm . Table 2: Out of vocabulary word counts aspects of model optimization: (1) the set of features in the model, (2) optimization algorithm, and (3) MT quality metric used in optimization. For (1), we provide a fixed set of “dense” features and also allow participants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Hoang et al., 2009) implementation of hierarchical phrase-based model (Chiang, 2005). The language models were 5-gram models with Kneser-Ney smoothing (Kneser and Ney, 1995) built using"
W15-3032,P02-1040,0,0.101622,"uced translations automatically against reference translations using some evaluation measure (traditionally called “metric” in the MT field). In short, the optimizer tunes model weights so that the final combined model score correlates with the metric score. The metric score, in turn, is designed to correlate well with human judgements of translation quality, see Stanojevi´c et al. (2015) and the previous papers summarizing WMT metrics tasks. However, a metric that correlates well with humans on final output quality may not be usable in weight optimization for various technical reasons. BLEU (Papineni et al., 2002) was shown to be very hard to surpass (Cer et al., 2010) and this is also confirmed by the results of the invitation-only WMT11 Tunable Metrics Task (Callison-Burch et al., 2010)1 . Note however, that some metrics have been successfully used for system tuning (Liu et al., 2011; Beloucif et al., 2014). The aim of the WMT15 Tuning Task2 is to attract attention to the exploration of all the three This paper presents the results of the WMT15 Tuning Shared Task. We provided the participants of this task with a complete machine translation system and asked them to tune its internal parameters (featu"
W15-3032,W15-3050,1,0.848835,"Missing"
W15-3032,W15-3031,1,0.847493,"Missing"
W15-3032,N09-2006,0,0.185358,"tuning task consisted of the configuration of the MT system, i.e. the additional sparse features (if any) and the values of all the feature weights, λm . Table 2: Out of vocabulary word counts aspects of model optimization: (1) the set of features in the model, (2) optimization algorithm, and (3) MT quality metric used in optimization. For (1), we provide a fixed set of “dense” features and also allow participants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Hoang et al., 2009) implementation of hierarchical phrase-based model (Chiang, 2005). The language models were 5-gram models with Kneser-Ney smoothing (Kneser and Ney, 1995) built using KenLM (Heafield et al"
W15-3032,W10-1703,0,\N,Missing
W15-3032,W15-3055,0,\N,Missing
W15-4103,P11-1105,0,0.0190879,"n many occassions, we were surprised by the low quality of CH’s translations. We considered this system a rather strong baseline, given the LMs trained on billions of tokens and the factored scheme, which specifically targets morphological coherence. Yet we observed many obvious errors both in lexical choice and morphological agreement, which were well within the scope of the phrase length limit and n-gram order. We believe that more sophisticated statistical models, such as discriminative classifiers which take source context into account (Carpuat and Wu, 2007) or operation sequence models (Durrani et al., 2011), could be applied to further improve CH. 5.2 Conclusion Practical Considerations As he have shown, our approach to system combination has some unique properties and can certainly be an interesting alternative. Yet it can be viewed as impractical – the models (the TectoMT phrase table, specifically) actually require the input to be known in advance. In this section, we outline a possible solution which would allow for using the system in an on-line setting. The synthetic parallel data consist of the dev set and test set. Our development data can be fixed in advance so re-tuning the system par"
W15-4103,W08-0509,0,0.0115384,"line a possible solution which would allow for using the system in an on-line setting. The synthetic parallel data consist of the dev set and test set. Our development data can be fixed in advance so re-tuning the system parameters is not required for new inputs. The only remaining issue is ensuring that the second phrase table contains the TectoMT translation of the input. We propose to first translate the input sentence using TectoMT. Then for word alignment, we can either use the alignment information directly from TectoMT or apply a pretrained word-alignment model, provided e.g. by MGiza (Gao and Vogel, 2008). Phrase extraction and scoring can be done quickly on the fly. Phrase scores should ideally be combined with the dev-set part of the phrase table. Moses has support for dynamic updating of its phrase tables (Bertoldi, 2014), so changing the scores or adding new phrase pairs is possible at very little cost. With pre-trained word alignment and dynamic updating of the phrase table, we believe that our approach could be readily deployed in practice. Acknowledgements This research was supported by the grants H2020ICT-2014-1-645452 (QT21), H2020-ICT-20141-644402 (HimL), H2020-ICT-2014-1-644753 (KCo"
W15-4103,W11-2138,1,0.859732,"phological variants of words are considered at all. Phrase Table Limit Until recently we did not pay much attention to the maximum number of different translation options considered per source phrase (the parameter table-limit), assuming that the good phrase pairs are scored high and will be present in the list. This year, we set table-limit to 100 instead of the default 20 and found that while it indeed made little or no difference in CH, it affected the system combination in CH. It is known that multiple phrase tables clutter the search space with different derivations of the same output (Bojar and Tamchyna, 2011), demanding a relaxation of pruning during the search (e.g. stack-limit or the various limits of cube pruning). From this point of view, increasing the table-limit actually makes the situation worse by bringing in more options. We leave the search pruning limits at their default values, increase only the table-limit, and yet observe a gain. Table 4 shows the average testset BLEU score (incl. the standard deviation) obtained in three independent runs of MERT when setting the table-limit to 20 or 100 for one or both 3 Contribution of Individual Components Table 5 breaks n-grams from the referenc"
W15-4103,P10-2016,1,0.901066,"Missing"
W15-4103,P07-2045,1,0.0119551,"Missing"
W15-4103,W13-2208,1,0.883153,"s significantly better translations than either component by itself. 1 2 Chimera Overview Chimera is a system combination of a phrasebased Moses system (Koehn et al., 2007) with Tecˇ toMT (Popel and Zabokrtsk´ y, 2010), finally processed with Depfix (Rosa et al., 2012), an automatic correction of morphological and some semantic errors (reversed negation). Chimera thus does not quite fit in the classification of hybrid MT systems suggested by Costa-juss`a and Fonollosa (2015). Figure 1 provides a graphical summary of the simple system combination technique dubbed “poor man’s”, as introduced by Bojar et al. (2013b). The system combination does not need any dedicated tool, e.g. those by Matusov et al. (2008), Barrault (2010), or Heafield and Lavie (2010). Instead, it directly includes the output of the transfer-based system into the main phrasebased search. Introduction Chimera (Bojar et al., 2013b; Tamchyna et al., 2014) is a hybrid English-to-Czech MT system which has repeatedly won in the WMT shared translation task (Bojar et al., 2013a; Bojar et al., 2014). It combines a statistical phrase-based system (Moses, in a factored setting), a deep-transfer ˇ hybrid system TectoMT (Popel and Zabokrtsk´ y,"
W15-4103,W14-3302,1,0.774586,"ss`a and Fonollosa (2015). Figure 1 provides a graphical summary of the simple system combination technique dubbed “poor man’s”, as introduced by Bojar et al. (2013b). The system combination does not need any dedicated tool, e.g. those by Matusov et al. (2008), Barrault (2010), or Heafield and Lavie (2010). Instead, it directly includes the output of the transfer-based system into the main phrasebased search. Introduction Chimera (Bojar et al., 2013b; Tamchyna et al., 2014) is a hybrid English-to-Czech MT system which has repeatedly won in the WMT shared translation task (Bojar et al., 2013a; Bojar et al., 2014). It combines a statistical phrase-based system (Moses, in a factored setting), a deep-transfer ˇ hybrid system TectoMT (Popel and Zabokrtsk´ y, 2010) and a rule-based post-editing tool Depfix (Rosa et al., 2012). Empirical results show that each of the components contributes significantly to the translation 11 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 11–20, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Dev set (En) Test set (En) WMT13 Parallel training data Moses CH0 Synthetic ttable Moses CH1 Depﬁx WMT15"
W15-4103,P03-1021,0,0.0206923,"actored) Moses system with two phrase tables. The first is a standard phrase table extracted from English-Czech parallel data. The second phrase table is tailored to the input data and comes from a synthetic parallel corpus provided by TectoMT: the source sides of the dev and test sets are first translated with CU - TECTOMT. Following the standardard word alignment on the source side and the translation, phrases are extracted from this synthetic corpus and added as a separate phrase table to the combined system (CH). The relative importance of this phrase table is estimated in standard MERT (Och, 2003). The final translation of the test set is produced by Moses (enriched with this additional phrase table) and additionally post-processed by Depfix. Note that all components of this combination have direct access to the source side which prevents the cumulation of errors. For brevity, we will use the following names: CH  to denote the plain Moses, CH  to denote the Moses combining the two phrase tables (one from CH  and one from CU - TECTOMT), and CH  to denote the final C HIMERA. In this paper, we focus on the first two components, leaving CH aside. The rest of this section summarizes Ch"
W15-4103,D07-1007,0,0.0426157,"tion for several years in a row. Weaknesses of CH On many occassions, we were surprised by the low quality of CH’s translations. We considered this system a rather strong baseline, given the LMs trained on billions of tokens and the factored scheme, which specifically targets morphological coherence. Yet we observed many obvious errors both in lexical choice and morphological agreement, which were well within the scope of the phrase length limit and n-gram order. We believe that more sophisticated statistical models, such as discriminative classifiers which take source context into account (Carpuat and Wu, 2007) or operation sequence models (Durrani et al., 2011), could be applied to further improve CH. 5.2 Conclusion Practical Considerations As he have shown, our approach to system combination has some unique properties and can certainly be an interesting alternative. Yet it can be viewed as impractical – the models (the TectoMT phrase table, specifically) actually require the input to be known in advance. In this section, we outline a possible solution which would allow for using the system in an on-line setting. The synthetic parallel data consist of the dev set and test set. Our development data"
W15-4103,P11-2031,0,0.0734448,"Missing"
W15-4103,W12-3146,0,0.223375,"Missing"
W15-4103,W14-3322,1,0.510919,"l and some semantic errors (reversed negation). Chimera thus does not quite fit in the classification of hybrid MT systems suggested by Costa-juss`a and Fonollosa (2015). Figure 1 provides a graphical summary of the simple system combination technique dubbed “poor man’s”, as introduced by Bojar et al. (2013b). The system combination does not need any dedicated tool, e.g. those by Matusov et al. (2008), Barrault (2010), or Heafield and Lavie (2010). Instead, it directly includes the output of the transfer-based system into the main phrasebased search. Introduction Chimera (Bojar et al., 2013b; Tamchyna et al., 2014) is a hybrid English-to-Czech MT system which has repeatedly won in the WMT shared translation task (Bojar et al., 2013a; Bojar et al., 2014). It combines a statistical phrase-based system (Moses, in a factored setting), a deep-transfer ˇ hybrid system TectoMT (Popel and Zabokrtsk´ y, 2010) and a rule-based post-editing tool Depfix (Rosa et al., 2012). Empirical results show that each of the components contributes significantly to the translation 11 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 11–20, c Beijing, China, July 31, 2015. 2015 Associ"
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2302,P13-1023,0,0.0135154,"ng algorithm is improved (linear SVM instead of logistic regression) and some features that are relatively slow to compute are removed (paraphrasing, syntax and permutation trees) which resulted in a very large speed-up. BEER is usually trained for ranking but in this case there was a compromise: the initial model is trained for ranking (RR) with ranking SVM and then the output from SVM is scaled using trained regression model to approximate absolute judgment (DA). HUME The HUME metric (Birch et al., 2016) is a novel human evaluation measure that decomposes over the UCCA semantic units. UCCA (Abend and Rappoport, 2013) is an appealing candidate for semantic analysis, due to its crosslinguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and adjectival argument structures and their interrelations. HUME operates by aggregating human assessments of the translation quality of individual semantic units in the source sentence. We thus avoid the semantic annotation of machinegenerated text, which is often garbled or seman2.4.2 C HARAC T ER C HARAC T ER (Wang et al., 2016) is a novel character-level metric inspired by the commonly applied"
W16-2302,D16-1134,1,0.845309,"and permutation trees. BEER has participated in previous years of the evaluation task. This year the learning algorithm is improved (linear SVM instead of logistic regression) and some features that are relatively slow to compute are removed (paraphrasing, syntax and permutation trees) which resulted in a very large speed-up. BEER is usually trained for ranking but in this case there was a compromise: the initial model is trained for ranking (RR) with ranking SVM and then the output from SVM is scaled using trained regression model to approximate absolute judgment (DA). HUME The HUME metric (Birch et al., 2016) is a novel human evaluation measure that decomposes over the UCCA semantic units. UCCA (Abend and Rappoport, 2013) is an appealing candidate for semantic analysis, due to its crosslinguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and adjectival argument structures and their interrelations. HUME operates by aggregating human assessments of the translation quality of individual semantic units in the source sentence. We thus avoid the semantic annotation of machinegenerated text, which is often garbled or seman2.4"
W16-2302,W11-2101,1,0.910889,"Missing"
W16-2302,1999.mtsummit-1.31,0,0.193921,"Missing"
W16-2302,W15-3047,0,0.172925,"Missing"
W16-2302,D15-1124,0,0.0419862,"Missing"
W16-2302,W16-2339,0,0.102063,"T10 T10 T10 T9,F8 T9,F8 T9,F8 T9,F8 T9,F8 T9,F8 T10,F9 T11,F10 T11,F10 T11,F10 T11,F10 cs de T5,F4,T6 T5,F5 T8,F4 T8,F5 Table 3: Overview of tables (T) and figures (F) reporting results of the individual “tracks” and language pairs. 2.4.9 UPF-C OBALT, C OBALT F and M ETRICS F optimization. To tokenize the sentences, we used the standard tokenizer script as available in Moses toolkit. Since Moses scorer is versioned on Github, we strongly encourage authors of high-performing metrics to add them to Moses scorer, as this will ensure that their metric can be included in future tasks. UPF-C OBALT (Fomicheva et al., 2016) is an alignment-based metric that examines the syntactic contexts of lexically similar candidate and reference words in order to distinguish meaningpreserving variations from the differences indicative of MT errors. This year the metric was improved by explicitly addressing MT fluency. The new version of the metric, C OBALT F, combines various components of UPF-C OBALT with a number of fine-grained features intended to capture the number and scale of disfluent fragments contained in MT sentences. M ETRICS F is a combination of three evaluation systems, BLEU, M ETEOR and UPF-C OBALT, with the"
W16-2302,W16-2303,1,0.878639,"Missing"
W16-2302,W06-3114,0,0.0357039,"rovision of more conclusive system-level metric rankings. 1 • Relative Ranking (RR) of up to 5 different translation candidates at a time, as collected in WMT in the past, Introduction • Direct Assessment (DA) evaluating the adequacy of a translation candidate on an absolute scale in isolation from other translations, Automatic evaluation of machine translation quality is essential in the development and selection of machine translation systems. Many different automatic MT quality metrics are available and the Metrics Shared Task1 is held annually at WMT to assess their quality, starting with Koehn and Monz (2006) and following up to Stanojevi´c et al. (2015). • HUME, a composite segment-level score aggregated over manual judgments of translation quality of semantic units of the source sentence. Additional changes to the task evaluation include a change in the way we compute confidence 1 http://www.statmt.org/wmt16/ metrics-task/ 199 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 199–231, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics Track RRsysNews RRsysIT DAsysNews RRsegNews DAsegNews HUMEseg Test set newstest201"
W16-2302,D14-1020,1,0.744969,"ores with human assessment variant RR + TT, i.e. standard WMT relative ranking including tuning task systems. In previous years, we reported empirical confidence intervals of system-level correlations obtained by bootstrap resampling human assessments data and computing confidence intervals for individual correlations with human assessment. Such confidence intervals reflect the variance due to particular sentences and assessors involved in the evaluation but lead to over-estimation of significant differences if employed to conclude which metrics outperform others. This year, as recommended by Graham and Baldwin (2014), instead we employ Williams significance test (Williams, 1959). Williams test is a test of significance of a difference in dependent correlations and therefore suitable for evaluation of metrics. Correlations not significantly outperformed by any other are highlighted in bold in Tables 4 and 5. Since RR is the official method of evaluation for this year’s metrics task, bolded correlations under RR comprise official winners of the news domain portion of the system-level metrics task. DA results are included for comparison and are investigatory only. a significant increase in correlation with h"
W16-2302,E06-1031,0,0.00991093,"MTEVAL BLEU (Papineni et al., 2002) and MTEVAL NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 3.1 System-Level Results for News Task As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics, as follows: • Moses Scorer. The metrics MOSES BLEU, MOSES TER (Snover et al., 2006), MOSES WER, MOSES PER and MOSE CDER (Leusch et al., 2006) were produced by the Moses scorer which is used in Moses model Pn − H)(Mi − M ) qP n 2 2 (H − H) i i=1 i=1 (Mi − M ) r = qP n i=1 (Hi (1) where H are human assessment scores of all systems in a given translation direction, M are corresponding scores as predicted by a given metric. H and M are their means respectively. 7 http://www.itl.nist.gov/iad/mig/ tools/ 205 Since some metrics, such as BLEU, for example, aim to achieve a strong positive correlation with human assessment, while error metrics, such as TER aim for a strong negative correlation, after computation of r for metrics, we compare"
W16-2302,N16-1001,1,0.834545,". Correlations not significantly outperformed by any other are highlighted in bold in Tables 4 and 5. Since RR is the official method of evaluation for this year’s metrics task, bolded correlations under RR comprise official winners of the news domain portion of the system-level metrics task. DA results are included for comparison and are investigatory only. a significant increase in correlation with human assessment over that of BLEU, where a green cell in the column denotes outperformance of BLEU by the metric in that row. For investigatory purposes only, we also include hybrid-supersample (Graham and Liu, 2016) results for system-level metrics. 10K hybrid systems were created per language pair, with corresponding DA human assessment scores, by sampling pairs of systems from WMT16 translation task and creating a hybrid system by combining translations from each system to create new hybrid output test set documents, each with a corresponding DA human assessment score. Not all metrics participating in the system-level metrics shared task submitted metric scores for the large set of hybrid systems, possibly due to the increased time required to run metrics on the large set of 10K systems. In this respec"
W16-2302,W14-3336,1,0.563594,"oncordant |+ |Discordant| (2) where Concordant is the set of all human comparisons for which a given metric suggests the same order and Discordant is the set of all human comparisons for which a given metric disagrees. The formula is not specific with respect to ties, i.e. cases where the annotation says that the two outputs are equally good. The way in which ties (both in human and metric judgment) were incorporated in computing Kendall τ has changed across the years of WMT metrics tasks. Here we adopt the version from WMT14 and WMT15. For a detailed discussion on other options, see Mach´acˇ ek and Bojar (2014). The method is formally described using the following matrix: Given such a matrix Ch,m where h, m ∈ {&lt;, = , &gt;}9 and a metric, we compute the Kendall’s τ for the metric the following way: Segment-level HUME evaluation The evaluation of segment-level metrics with reference to HUME scores operates in a similar way to DA, by computing the Pearson correlation of HUME evaluation scores for individual translations with metric scores. Williams test is also applied to test for significant differences in metric performance. Kendall’s Tau-like Formulation We measure the quality of metrics’ segment-level"
W16-2302,W13-2305,1,0.696366,"rporate DA fluency into future metric evaluations, however. Finally, although it is common to apply a sentence length restriction in WMT human evaluation, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT16. Direct Assessment (DA) In addition to the standard relative ranking (RR) manual evaluation employed to yield official system rankings in WMT16 translation task, this year the translation task also trialed a new method of human evaluation, monolingual direct assessment (DA) of translation fluency (Graham et al., 2013) and adequacy (Graham et al., 2014; Graham et al., 2016). For investigatory purposes, therefore, we also include evaluation of metrics with reference to the newly trialed human assessment method. Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment for both fluency and adequacy. Furthermore, DA avoids bias that has been problematic in previous evaluations introduced by simultaneous assessment of several alternate"
W16-2302,W13-2202,1,0.663833,"st of the paper. We discuss systemlevel results for news task systems (including tuning task systems) in Section 3.1. The system-level results for the IT domain are discussed in Section 3.2. The segment-level results are in Section 3.3. We end with discussion in Section 3.4. • Mteval. The metrics MTEVAL BLEU (Papineni et al., 2002) and MTEVAL NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 3.1 System-Level Results for News Task As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics, as follows: • Moses Scorer. The metrics MOSES BLEU, MOSES TER (Snover et al., 2006), MOSES WER, MOSES PER and MOSE CDER (Leusch et al., 2006) were produced by the Moses scorer which is used in Moses model Pn − H)(Mi − M ) qP n 2 2 (H − H) i i=1 i=1 (Mi − M ) r = qP n i=1 (Hi (1) where H are human assessment scores of all systems in a given translation direction, M are corresponding scores as predicted by a given metric. H and M are their"
W16-2302,W16-2340,0,0.0785443,"Metric BEER C HARAC T ER CHR F1,2,3, WORD F1,2,3 D EP C HECK DPMF COMB - WITHOUT-RED MPEDA U OW.R E VAL UPF - COBALT, C OBALT F, M ETRICS F DTED Participant ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) RWTH Aachen University (Wang et al., 2016) Humboldt University of Berlin (Popovi´c, 2016) Charles University, no corresponding paper Chinese Academy of Sciences and Dublin City University (Yu et al., 2015) Jiangxi Normal University (Zhang et al., 2016) University of Wolverhampton (Gupta et al., 2015b) Universitat Pompeu Fabra (Fomicheva et al., 2016) University of St Andrews, (McCaffery and Nederhof, 2016) Table 2: Participants of WMT16 Metrics Shared Task tically unclear. This also allows the re-use of the source semantic annotation for measuring the quality of different translations of the same source sentence, and avoids reliance on possibly suboptimal reference translations. HUME shows good inter-annotator agreement, and reasonable correlation with Direct Assessment (Graham et al., 2015). Segment-level DA adequacy scores were collected as in system-level DA, described in Section 2.3.1, again with strict quality control and score standardization applied. To achieve accurate segment-level sco"
W16-2302,E14-1047,1,0.869993,"ic evaluations, however. Finally, although it is common to apply a sentence length restriction in WMT human evaluation, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT16. Direct Assessment (DA) In addition to the standard relative ranking (RR) manual evaluation employed to yield official system rankings in WMT16 translation task, this year the translation task also trialed a new method of human evaluation, monolingual direct assessment (DA) of translation fluency (Graham et al., 2013) and adequacy (Graham et al., 2014; Graham et al., 2016). For investigatory purposes, therefore, we also include evaluation of metrics with reference to the newly trialed human assessment method. Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment for both fluency and adequacy. Furthermore, DA avoids bias that has been problematic in previous evaluations introduced by simultaneous assessment of several alternate translations of a given single so"
W16-2302,P02-1040,0,0.11901,"al. (2016a), metrics task occasionally suffers from “loss of knowledge” when successful metrics participate only in one year. We attempt to avoid this by regularly evaluating also a range of “baseline metrics”: 3 Results Table 3 provides an overview of all the tables and figures in the rest of the paper. We discuss systemlevel results for news task systems (including tuning task systems) in Section 3.1. The system-level results for the IT domain are discussed in Section 3.2. The segment-level results are in Section 3.3. We end with discussion in Section 3.4. • Mteval. The metrics MTEVAL BLEU (Papineni et al., 2002) and MTEVAL NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 3.1 System-Level Results for News Task As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics, as follows: • Moses Scorer. The metrics MOSES BLEU, MOSES TER (Snover et al., 2006), MOSES WER, MOSES PER and MOSE CDER (Leusch et al., 2006) were produced"
W16-2302,N15-1124,1,0.922262,"rsity (Yu et al., 2015) Jiangxi Normal University (Zhang et al., 2016) University of Wolverhampton (Gupta et al., 2015b) Universitat Pompeu Fabra (Fomicheva et al., 2016) University of St Andrews, (McCaffery and Nederhof, 2016) Table 2: Participants of WMT16 Metrics Shared Task tically unclear. This also allows the re-use of the source semantic annotation for measuring the quality of different translations of the same source sentence, and avoids reliance on possibly suboptimal reference translations. HUME shows good inter-annotator agreement, and reasonable correlation with Direct Assessment (Graham et al., 2015). Segment-level DA adequacy scores were collected as in system-level DA, described in Section 2.3.1, again with strict quality control and score standardization applied. To achieve accurate segment-level scores for translations, a human assessment of each translation was collected from 15 distinct human assessors before combination into a mean adequacy score for each individual translation. Although in general agreement in human assessment of MT has been difficult to achieve, segment-level DA scores employing a minimum of 15 repeat assessments have been shown to be almost perfectly replicable."
W16-2302,W16-2341,0,0.054072,"Missing"
W16-2302,2006.amta-papers.25,0,0.140134,"end with discussion in Section 3.4. • Mteval. The metrics MTEVAL BLEU (Papineni et al., 2002) and MTEVAL NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 3.1 System-Level Results for News Task As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics, as follows: • Moses Scorer. The metrics MOSES BLEU, MOSES TER (Snover et al., 2006), MOSES WER, MOSES PER and MOSE CDER (Leusch et al., 2006) were produced by the Moses scorer which is used in Moses model Pn − H)(Mi − M ) qP n 2 2 (H − H) i i=1 i=1 (Mi − M ) r = qP n i=1 (Hi (1) where H are human assessment scores of all systems in a given translation direction, M are corresponding scores as predicted by a given metric. H and M are their means respectively. 7 http://www.itl.nist.gov/iad/mig/ tools/ 205 Since some metrics, such as BLEU, for example, aim to achieve a strong positive correlation with human assessment, while error metrics, such as TER aim for a strong negative c"
W16-2302,W15-3050,1,0.8645,"Missing"
W16-2302,W15-3031,1,0.543137,"Missing"
W16-2302,W16-2342,0,0.0636751,"easure that decomposes over the UCCA semantic units. UCCA (Abend and Rappoport, 2013) is an appealing candidate for semantic analysis, due to its crosslinguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and adjectival argument structures and their interrelations. HUME operates by aggregating human assessments of the translation quality of individual semantic units in the source sentence. We thus avoid the semantic annotation of machinegenerated text, which is often garbled or seman2.4.2 C HARAC T ER C HARAC T ER (Wang et al., 2016) is a novel character-level metric inspired by the commonly applied translation edit rate (TER). It is defined as the minimum number of character edits required to adjust a hypothesis, until it completely matches 203 rank, using training data from the English-targeted language pairs from WMT12 to WMT14. In the results DPMF COMB - WITHOUT-RED is represented as DPMF COMB for brevity. the reference, normalized by the length of the hypothesis sentence. C HARAC T ER calculates the character-level edit distance while performing the shift edit on word level. Unlike the strict matching criterion in TE"
W16-2302,W15-3053,0,0.0759192,"W.R E VAL U OW.R E VAL (Gupta et al., 2015b) uses dependency-tree Long Short Term Memory (LSTM) network to represent both the hypothesis and the reference with a dense vector. Training is performed using the judgements from WMT13 (Bojar et al., 2013) converted to similarity scores. The final score at the system level is obtained by averaging the segment level scores obtained from a neural network which takes into account both distance and Hadamard product of the two representations. DPMF COMB - WITHOUT-RED The authors of DPMF COMB - WITHOUT-RED follow the work on last year’s metric DPMF COMB (Yu et al., 2015), but modify it with two main differences. Firstly, they use the ‘case insensitive’ instead of ‘case sensitive’ option when using Asiya. Secondly, RED P are not used. Thus, DPMF COMB - WITHOUT-RED is a combined metric including 57 single metrics. Weights of the individual metrics are trained with SVMU OW.R E VAL is the same as U OW LSTM (Gupta et al., 2015a) that participated in the WMT15 task except that LSTM vector dimension is 150 for UoW.ReVal instead of 300. 204 Track RRsysNews RRsysIT DAsysNews RRsegNews DAsegNews HUMEseg cs de ro fi into-English T4,F1 T4,F1 ru tr English into ro fi ru t"
W16-2302,W16-2343,0,0.0217744,"TED evaluates only the word order. and WORD F WORD F1,2,3 (Popovi´c, 2016) calculate a simple F-score combination of the precision and recall of word n-grams of maximal length 4 with different setting for the β parameter (β = 1, 2, or 3). Precision and recall that are used in computation of the F-score are arithmetic averages of precisions and recalls, respectively, for the different n-gram orders. CHR F1,2,3 calculate the F-score of character n-grams of maximal length 6. β parameter gives β times weight to recall: β = 1 implies equal weights for precision and recall. 2.4.4 2.4.7 MPEDA MPEDA (Zhang et al., 2016) is developed on the basis of the METEOR metric. In order to accurately match words or phrases with the same or similar meaning, it extracts a domain-specific paraphrase table from the monolingual corpus and applies that paraphrase table to the METEOR metric to replace the general one. Unlike traditional paraphrase extraction approaches, it first filters out a domain-specific sub-corpus from a large general monolingual corpus and then extracts domain-specific paraphrase table from the sub-corpus by Markov Network model. Since the proposed paraphrase extraction approach can be used in all langu"
W16-2302,W13-2201,1,\N,Missing
W16-2303,N10-1080,0,0.14962,"of translation quality, see Bojar et al. (2016c) and the previous papers summarizing WMT metrics tasks. However, a metric that correlates well with humans on final output quality may not be usable in weight optimization for various technical reasons. Many metrics that have very high correlation with human judgment achieve that by using complex models that are very slow so they might present a bottle-neck in the tuning process when the chosen evaluation metric needs to evaluate a huge number of translations in the n-best lists. BLEU (Papineni et al., 2002) was shown to be very hard to surpass (Cer et al., 2010) as a tuning metric and this is also confirmed by the previous WMT15 Tuning Task results (Stanojevi´c et al., 2015) and by the results of the invitation-only WMT11 Tunable Metrics Task (Callison-Burch et al., 2010)1 . Note however, that some metrics have been successfully used for system tuning (Liu et al., 2011; Beloucif et al., 2014). The aim of the WMT16 Tuning Task2 is (just like in WMT15 Tuning Task) to attract attention This paper presents the results of the WMT16 Tuning Shared Task. We provided the participants of this task with a complete machine translation system and asked them to tu"
W16-2303,N12-1047,0,0.0267091,"s to the exploration of all the three aspects of model optimization: (1) the set of features in the model, (2) optimization algorithm, and (3) MT quality metric used in optimization. For (1), we provide a fixed set of “dense” features and also allow participants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Koehn et al., 2007) implementation of phrase-based model. The language models were 5-gram models built using KenLM (Heafield et al., 2013) with modified Kneser-Ney smoothing (James, 2000) without pruning. For word alignments, we used fast-align toolkit (Dyer et al., 2013). Alignments are computed in both directions and symmetrized using grow-diag-final-and heurist"
W16-2303,N13-1073,0,0.019652,"a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Koehn et al., 2007) implementation of phrase-based model. The language models were 5-gram models built using KenLM (Heafield et al., 2013) with modified Kneser-Ney smoothing (James, 2000) without pruning. For word alignments, we used fast-align toolkit (Dyer et al., 2013). Alignments are computed in both directions and symmetrized using grow-diag-final-and heuristic. We use CzEng 1.6pre3 (Bojar et al., 2016b) parallel data for the extraction of translation models. We train two language models for each translation direction: the first model is trained on CzEng 1.6pre target data and the second model is trained on concatenation of Europarl v7, News Commentary data (parallel-nc-v11), news data (2007-2013, 2014-v2, 2015) and additionally news discussion v1 (for English language model only), as released for WMT164 . We excluded CommonCrawl data because we wanted to"
W16-2303,2014.iwslt-evaluation.4,0,0.120321,"Missing"
W16-2303,W15-3054,0,0.0668205,"ubmitted systems are shown in Table 3. We provide a brief summary of each evaluated optimization method in the rest of this section, concluding with baseline approaches (Section 3.7). DCU (Li et al., 2015) is tuned with RED, an evaluation metric based on matching of dependency ngrams. As tuning algorithm the authors have used KBMIRA . 3.1 3.4 3.3 ILLC-U VA-BEER ILLC-U VA-BEER (Stanojevi´c and Sima’an, 2015) was tuned using PRO (Hopkins and May, 2011) learning algorithm with new version of BEER evaluation metric. The authors claim that DCU AFRL1 and AFRL2 As in the previous year’s submissions (Erdmann and Gwinnup, 2015), the AFRL systems used Drem, which is a derivative-free optimization algorithm that interpolates n-best lists returned by 234 the decoder. Methodology for the current tuning task is nearly identical, since recent changes to Drem mostly relate to improving treatment of n-best list rescoring techniques (Gwinnup et al., 2016). The objective function used within Drem is the same for cs-en AFRL and en-cs system AFRL1: Since all the submissions including the baselines were subject to manual evaluation, we did not run the MERT or MIRA optimizations more than once (as is the common practice for estim"
W16-2303,W16-2313,0,0.0491492,"s into Czech for free to ease the participation for teams without any access to speakers of Czech. A complete model consists of a phrase table extracted from the parallel corpus, two lexicalized reordering tables and the two language model extracted from the monolingual data. As such, this defines a fixed set of dense features which is big3 http://ufal.mff.cuni.cz/czeng/ czeng16pre 4 http://www.statmt.org/wmt16/ translation-task.html 233 System BLEU -MIRA, BLEU -MERT AFRL DCU FJFI-PSO ILLC-U VA-BEER NRC-MEANT, NRC-NNBLEU USAAR Participant baselines United States Air Force Research Laboratory (Gwinnup et al., 2016) Dublin City University (Li et al., 2015) Czech Technical University in Prague (Kocur and Bojar, 2016) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) National Research Council Canada (Lo et al., 2015) Saarland University (Liling Tan; no corresponding paper) Table 3: Participants of WMT16 Tuning Shared Task models were extracted using code readily available in Moses. One of the models is word-based (Koehn et al., 2005) and the other is hierarchical (Galley and Manning, 2008). Both reordering models use msd orientation in both forward and backward direction, with model conditione"
W16-2303,D08-1089,0,0.0366115,"ILLC-U VA-BEER NRC-MEANT, NRC-NNBLEU USAAR Participant baselines United States Air Force Research Laboratory (Gwinnup et al., 2016) Dublin City University (Li et al., 2015) Czech Technical University in Prague (Kocur and Bojar, 2016) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) National Research Council Canada (Lo et al., 2015) Saarland University (Liling Tan; no corresponding paper) Table 3: Participants of WMT16 Tuning Shared Task models were extracted using code readily available in Moses. One of the models is word-based (Koehn et al., 2005) and the other is hierarchical (Galley and Manning, 2008). Both reordering models use msd orientation in both forward and backward direction, with model conditioned on both the source and target languages (msdbidirectional-fe). Before any further processing, the data was pretokenized and tokenized (using standard Moses scripts) and lowercased. We also removed parallel sentences longer than 60 words or shorter than 4 words, no data cleaning was performed for monolingual data. Table 1 summarizes the final dataset sizes and Table 2 provides details on outof-vocabulary items. Aside from the dev set provided, the participants were free to use any other d"
W16-2303,P13-2121,0,0.0341664,"Missing"
W16-2303,D11-1125,0,0.263418,"ll the three aspects of model optimization: (1) the set of features in the model, (2) optimization algorithm, and (3) MT quality metric used in optimization. For (1), we provide a fixed set of “dense” features and also allow participants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Koehn et al., 2007) implementation of phrase-based model. The language models were 5-gram models built using KenLM (Heafield et al., 2013) with modified Kneser-Ney smoothing (James, 2000) without pruning. For word alignments, we used fast-align toolkit (Dyer et al., 2013). Alignments are computed in both directions and symmetrized using grow-diag-final-and heuristic. We use CzEng 1.6pre3"
W16-2303,W16-2344,1,0.849786,"complete model consists of a phrase table extracted from the parallel corpus, two lexicalized reordering tables and the two language model extracted from the monolingual data. As such, this defines a fixed set of dense features which is big3 http://ufal.mff.cuni.cz/czeng/ czeng16pre 4 http://www.statmt.org/wmt16/ translation-task.html 233 System BLEU -MIRA, BLEU -MERT AFRL DCU FJFI-PSO ILLC-U VA-BEER NRC-MEANT, NRC-NNBLEU USAAR Participant baselines United States Air Force Research Laboratory (Gwinnup et al., 2016) Dublin City University (Li et al., 2015) Czech Technical University in Prague (Kocur and Bojar, 2016) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) National Research Council Canada (Lo et al., 2015) Saarland University (Liling Tan; no corresponding paper) Table 3: Participants of WMT16 Tuning Shared Task models were extracted using code readily available in Moses. One of the models is word-based (Koehn et al., 2005) and the other is hierarchical (Galley and Manning, 2008). Both reordering models use msd orientation in both forward and backward direction, with model conditioned on both the source and target languages (msdbidirectional-fe). Before any further processing, the da"
W16-2303,W15-3050,1,0.832068,"Missing"
W16-2303,2005.iwslt-1.8,0,0.0474051,"33 System BLEU -MIRA, BLEU -MERT AFRL DCU FJFI-PSO ILLC-U VA-BEER NRC-MEANT, NRC-NNBLEU USAAR Participant baselines United States Air Force Research Laboratory (Gwinnup et al., 2016) Dublin City University (Li et al., 2015) Czech Technical University in Prague (Kocur and Bojar, 2016) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) National Research Council Canada (Lo et al., 2015) Saarland University (Liling Tan; no corresponding paper) Table 3: Participants of WMT16 Tuning Shared Task models were extracted using code readily available in Moses. One of the models is word-based (Koehn et al., 2005) and the other is hierarchical (Galley and Manning, 2008). Both reordering models use msd orientation in both forward and backward direction, with model conditioned on both the source and target languages (msdbidirectional-fe). Before any further processing, the data was pretokenized and tokenized (using standard Moses scripts) and lowercased. We also removed parallel sentences longer than 60 words or shorter than 4 words, no data cleaning was performed for monolingual data. Table 1 summarizes the final dataset sizes and Table 2 provides details on outof-vocabulary items. Aside from the dev se"
W16-2303,W15-3032,1,0.356964,"Missing"
W16-2303,N09-2006,0,0.0249069,"onfiguration of the MT system, i.e. the additional sparse features (if any) and the values of all the feature weights. Table 2: Out of vocabulary word counts to the exploration of all the three aspects of model optimization: (1) the set of features in the model, (2) optimization algorithm, and (3) MT quality metric used in optimization. For (1), we provide a fixed set of “dense” features and also allow participants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Koehn et al., 2007) implementation of phrase-based model. The language models were 5-gram models built using KenLM (Heafield et al., 2013) with modified Kneser-Ney smoothing (James, 2000) without pruning. Fo"
W16-2303,P07-2045,1,0.0208268,"pants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Koehn et al., 2007) implementation of phrase-based model. The language models were 5-gram models built using KenLM (Heafield et al., 2013) with modified Kneser-Ney smoothing (James, 2000) without pruning. For word alignments, we used fast-align toolkit (Dyer et al., 2013). Alignments are computed in both directions and symmetrized using grow-diag-final-and heuristic. We use CzEng 1.6pre3 (Bojar et al., 2016b) parallel data for the extraction of translation models. We train two language models for each translation direction: the first model is trained on CzEng 1.6pre target data and the second model is trained on"
W16-2303,W15-3055,0,0.0381521,"Missing"
W16-2303,D11-1035,0,0.0591207,"t achieve that by using complex models that are very slow so they might present a bottle-neck in the tuning process when the chosen evaluation metric needs to evaluate a huge number of translations in the n-best lists. BLEU (Papineni et al., 2002) was shown to be very hard to surpass (Cer et al., 2010) as a tuning metric and this is also confirmed by the previous WMT15 Tuning Task results (Stanojevi´c et al., 2015) and by the results of the invitation-only WMT11 Tunable Metrics Task (Callison-Burch et al., 2010)1 . Note however, that some metrics have been successfully used for system tuning (Liu et al., 2011; Beloucif et al., 2014). The aim of the WMT16 Tuning Task2 is (just like in WMT15 Tuning Task) to attract attention This paper presents the results of the WMT16 Tuning Shared Task. We provided the participants of this task with a complete machine translation system and asked them to tune its internal parameters (feature weights). The tuned systems were used to translate the test set and the outputs were manually ranked for translation quality. We received 4 submissions in the Czech-English and 8 in the English-Czech translation direction. In addition, we ran 2 baseline setups, tuning the para"
W16-2303,W15-3056,0,0.12047,"two language model extracted from the monolingual data. As such, this defines a fixed set of dense features which is big3 http://ufal.mff.cuni.cz/czeng/ czeng16pre 4 http://www.statmt.org/wmt16/ translation-task.html 233 System BLEU -MIRA, BLEU -MERT AFRL DCU FJFI-PSO ILLC-U VA-BEER NRC-MEANT, NRC-NNBLEU USAAR Participant baselines United States Air Force Research Laboratory (Gwinnup et al., 2016) Dublin City University (Li et al., 2015) Czech Technical University in Prague (Kocur and Bojar, 2016) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) National Research Council Canada (Lo et al., 2015) Saarland University (Liling Tan; no corresponding paper) Table 3: Participants of WMT16 Tuning Shared Task models were extracted using code readily available in Moses. One of the models is word-based (Koehn et al., 2005) and the other is hierarchical (Galley and Manning, 2008). Both reordering models use msd orientation in both forward and backward direction, with model conditioned on both the source and target languages (msdbidirectional-fe). Before any further processing, the data was pretokenized and tokenized (using standard Moses scripts) and lowercased. We also removed parallel sentence"
W16-2303,P03-1021,0,0.0520879,"ed of the configuration of the MT system, i.e. the additional sparse features (if any) and the values of all the feature weights. Table 2: Out of vocabulary word counts to the exploration of all the three aspects of model optimization: (1) the set of features in the model, (2) optimization algorithm, and (3) MT quality metric used in optimization. For (1), we provide a fixed set of “dense” features and also allow participants to add additional “sparse” features. For (2), the optimization algorithm, task participants are free to use one of the available algorithms for direct loss optimization (Och, 2003; Zhao and Chen, 2009), which are usually capable of optimizing only a dozen of features, or one of the optimizers handling also very large sets of features (Cherry and Foster, 2012; Hopkins and May, 2011), or a custom algorithm. And finally for (3), participants can use any established evaluation metric or a custom one. 2 Details of Systems Tuned The systems that were distributed for tuning are based on Moses (Koehn et al., 2007) implementation of phrase-based model. The language models were 5-gram models built using KenLM (Heafield et al., 2013) with modified Kneser-Ney smoothing (James, 200"
W16-2303,P02-1040,0,0.103808,"are usually designed to correlate well with human judgments of translation quality, see Bojar et al. (2016c) and the previous papers summarizing WMT metrics tasks. However, a metric that correlates well with humans on final output quality may not be usable in weight optimization for various technical reasons. Many metrics that have very high correlation with human judgment achieve that by using complex models that are very slow so they might present a bottle-neck in the tuning process when the chosen evaluation metric needs to evaluate a huge number of translations in the n-best lists. BLEU (Papineni et al., 2002) was shown to be very hard to surpass (Cer et al., 2010) as a tuning metric and this is also confirmed by the previous WMT15 Tuning Task results (Stanojevi´c et al., 2015) and by the results of the invitation-only WMT11 Tunable Metrics Task (Callison-Burch et al., 2010)1 . Note however, that some metrics have been successfully used for system tuning (Liu et al., 2011; Beloucif et al., 2014). The aim of the WMT16 Tuning Task2 is (just like in WMT15 Tuning Task) to attract attention This paper presents the results of the WMT16 Tuning Shared Task. We provided the participants of this task with a"
W16-2303,W15-3049,0,0.0612564,"Missing"
W16-2303,W10-1703,0,\N,Missing
W16-2303,W16-2302,1,\N,Missing
W16-2303,W16-2301,1,\N,Missing
W16-2320,W16-2304,1,0.833347,"factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. T"
W16-2320,W05-0909,0,0.583183,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W16-2320,P13-2071,1,0.873371,"odel trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. The first part was used as development set while t"
W16-2320,D15-1129,1,0.847985,"training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based loc"
W16-2320,N13-1073,0,0.034805,"preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ ques"
W16-2320,J04-2004,0,0.0194677,"en systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probabilit"
W16-2320,2011.mtsummit-papers.30,0,0.020392,"-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 201"
W16-2320,N12-1047,0,0.591808,"rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard p"
W16-2320,E14-2008,1,0.72015,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,P05-1033,0,0.151933,"ative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combin"
W16-2320,J07-2003,0,0.558191,"this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the bou"
W16-2320,W14-3310,1,0.909876,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D14-1179,0,0.0138582,"Missing"
W16-2320,2014.iwslt-evaluation.7,1,0.925781,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D08-1089,0,0.0211464,", built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. T"
W16-2320,N15-1105,0,0.046766,"Missing"
W16-2320,W08-0509,0,0.06624,"probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via B LEU on a validation set, and perform early stopping on B LEU. Decoding is performed with beam search with a beam size of 12. A more detailed description of the system, and more experimental results, can be found in (Sennrich et al., 2016a). 3.10 3.11 USFD’s phrase-based system is built using the Moses toolkit, with MGIZA (Gao and Vogel, 2008) for word alignment and KenLM (Heafield et al., 2013) for language model training. We use all available parallel data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the featur"
W16-2320,W16-2315,1,0.820309,"vs et al., 2012) that features language-specific data filtering and cleaning modules. Tilde’s system was trained on all available parallel data. Two language models are trained using KenLM (Heafield, 2011): 1) a 5-gram model using the Europarl and SETimes2 corpora, and 2) a 3-gram model using the Common Crawl corpus. We also apply a custom tokenization tool that takes into account specifics of the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural Sy"
W16-2320,D07-1103,0,0.032902,"bbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT ("
W16-2320,W14-3360,0,0.0191919,"NMT system on newsdev2016/2, but lags behind on newstest2016. Removing the by itself weakest system shows a slight degradation on newsdev2016/2 and newstest2016, hinting that it still provides valuable information. Table 2 shows a comparison between all systems by scoring the translation output against each other in T ER and B LEU. We see that the neural networks outputs differ the most from all the other systems. Figure 1: System A: the large building; System B: the large home; System C: a big house; System D: a huge house; Reference: the big house. classes were generated using the method of Green et al. (2014). 4 System Combination System combination produces consensus translations from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alig"
W16-2320,P07-2045,1,0.010375,", 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based"
W16-2320,P13-2121,0,0.0645203,"Missing"
W16-2320,W11-2123,0,0.124165,"nslation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phras"
W16-2320,N04-1022,0,0.037676,"f the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpu"
W16-2320,2015.iwslt-papers.3,1,0.744353,"g. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn e"
W16-2320,J06-4004,0,0.106202,"Missing"
W16-2320,2009.iwslt-papers.4,0,0.0984189,"target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SETimes2) directly into the log-linear combination of the system and let MIRA (Cherry and Foster, 2012) optimize their weights along with all other features in tuning, rather than relying on a single linearly interpolated language model. We add another background language model estimated over a concatenation of all Ro"
W16-2320,P10-2041,0,0.0238386,"ontaining the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using"
W16-2320,P07-1019,0,0.236745,"grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language"
W16-2320,2012.amta-papers.19,1,0.778005,"common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set usin"
W16-2320,W11-2211,1,0.902733,"ty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system provided by the RWTH is an attention-based recurrent neural network similar to (Bahdanau et al., 2015). The implementation is based on Blocks (van Merri¨enboer et al., 2015) and Theano (Bergstra et al., 20"
W16-2320,W13-2264,1,0.852517,"stic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additio"
W16-2320,W13-2258,1,0.869431,"extraction, we impose less strict extraction constraints than the Moses defaults. We extract more hierarchical rules by allowing for a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system prov"
W16-2320,E99-1010,0,0.040797,"ion 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and th"
W16-2320,P03-1021,0,0.501814,". To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SE"
W16-2320,D14-1003,1,0.932306,"with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule appli"
W16-2320,P06-1055,0,0.0121776,"anslation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but als"
W16-2320,2007.tmi-papers.21,0,0.0230136,"n 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add t"
W16-2320,P16-1161,1,0.729045,"omanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard phrasebased setup is the addition of a feature-rich discriminative translation model which is conditioned on both source- and target-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-"
W16-2320,tufis-etal-2008-racais,0,0.107217,"Missing"
W16-2320,P16-1009,1,0.78198,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,P12-3008,0,0.0597108,"Missing"
W16-2320,P16-1162,1,0.259658,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,W10-1738,1,0.885055,"rget-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical ph"
W16-2320,P15-4020,1,0.815128,"data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the feature set with the 17 baseline black-box features from sentencelevel Quality Estimation (QE) produced with Quest++4 (Specia et al., 2015). The 1000-best lists are then reranked and the top-best hypothesis extracted using the nbest rescorer available within the Moses toolkit. 3.12 UvA We use a phrase-based machine translation system (Moses) with a distortion limit of 6 and lexicalized reordering. Before translation, the English source side is preordered using the neural preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in"
W16-2320,W16-2327,1,0.84726,"Missing"
W16-2320,D13-1138,1,0.859072,"(Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integra"
W16-2320,2002.tmi-tutorials.2,0,0.0608664,"omanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all"
W16-2325,2005.iwslt-1.8,0,0.152572,"Missing"
W16-2325,P07-2045,1,0.0213385,"p is fairly complex, following Bojar et al. (2013). The key components of C HIMERA are: This paper describes the phrase-based systems jointly submitted by CUNI and LMU to English-Czech and English-Romanian News translation tasks of WMT16. In contrast to previous years, we strictly limited our training data to the constraint datasets, to allow for a reliable comparison with other research systems. We experiment with using several additional models in our system, including a feature-rich discriminative model of phrasal translation. 1 English-Czech System • Moses, a phrase-based factored system (Koehn et al., 2007). • TectoMT, a deep-syntactic transfer-based ˇ system (Popel and Zabokrtsk´ y, 2010). • Depfix, a rule-based post-processing system (Rosa et al., 2012). The core of the system is Moses. We combine it with TectoMT in a simple way which we refer to as “poor man’s” system combination: we translate our development and test data with TectoMT first and then add the source sentences and their translations as additional (synthetic) parallel data to the Moses system. This new corpus is used to train a separate phrase table. At test time, we run Moses which uses both phrase tables and we correct its out"
W16-2325,W13-2208,1,0.840484,"Popel and Zabokrtsk´ y, 2010). • Depfix, a rule-based post-processing system (Rosa et al., 2012). The core of the system is Moses. We combine it with TectoMT in a simple way which we refer to as “poor man’s” system combination: we translate our development and test data with TectoMT first and then add the source sentences and their translations as additional (synthetic) parallel data to the Moses system. This new corpus is used to train a separate phrase table. At test time, we run Moses which uses both phrase tables and we correct its output using Depfix. The system is described in detail in Bojar et al. (2013). Our subsequent analysis in Tamchyna and Bojar (2015) shows that the contribution of TectoMT is essential for the performance of C HIMERA. In particular, TectoMT provides new translations which are otherwise not available to the phrase-based system and it also improves the morphological and syntactic coherence of translations. Introduction We have a long-term experience with English-toCzech machine translation and over the years, our systems have grown together from rather diverse set of system types to a single system combination called C HIMERA (Bojar et al., 2013). This system has been suc"
W16-2325,W15-3006,1,0.864481,"ectoMT is essential for the performance of C HIMERA. In particular, TectoMT provides new translations which are otherwise not available to the phrase-based system and it also improves the morphological and syntactic coherence of translations. Introduction We have a long-term experience with English-toCzech machine translation and over the years, our systems have grown together from rather diverse set of system types to a single system combination called C HIMERA (Bojar et al., 2013). This system has been successful in the previous three years of WMT (Bojar et al., 2013; Tamchyna et al., 2014; Bojar and Tamchyna, 2015) and we follow a similar design this year. Unlike previous years, we only use constrained data in system training, to allow for a more meaningful comparison with the competing systems. The gains thanks to the additional data in contrast to the gains thanks the system combination have been evaluated in terms of BLEU in Bojar and Tamchyna (2015). The details of our English-to-Czech system are in Section 2. In this work, we also present our system submission for English-Romanian translation. This system uses a factored setting similar to C HIMERA but lacks its two key components: the deepsyntacti"
W16-2325,P03-1021,0,0.0682765,"HIMERA. We have added the discriminative model which conditions both on the source and target context to the system and obtained a small but significant improvement in BLEU. Discriminative Translation Model We utilize the same discriminative model as for C HIMERA. For English-Romanian, we also use dependency parses of the source sentences and target-side context features as additional source of information in our official submission. 3.6 BLEU 26.2 26.6 28.0 28.1 28.3 Results Table 2 lists BLEU scores of various system settings. Each BLEU score is an average over 5 runs of system tuning (MERT, Och, 2003). The table shows how BLEU score develops as we add the individual components to the system: the 7gram morphological LM (“tagLM”), the 4-gram LM from Common Crawl (“ccrawl”), the lexicalized reordering (“RR”) and finally the discriminative translation model (“discTM”). We test for statistical significance using MultEval (Clark et al., 2011); we test each new component against the system without it (i.e., +tagLM is compared to baseline, +ccrawl is tested against +tagLM etc.). When the p-value is lower than 0.05, we mark the result in bold. 5 Acknowledgement This work has received funding from t"
W16-2325,W14-3363,0,0.0253533,"t increasing the phrase table limit (the maximum number of possible translations per source phrase) is necessary to obtain good performance. Our input is also factored (though the phrase tables do not condition on these additional factors) and contains the form, lemma and morphological tag. We use these factors to extract rich features for our discriminative context model. Linearly interpolated translation models. There is some evidence that when dealing with heterogeneous domains, it might be beneficial to construct the final TM as a linear, uniform interpolation of many small phrase tables (Carpuat et al., 2014). We experiment with splitting the data into 20 parts (without any domain selection, simply a random shuffle) and using linear interpolation to combine the partial models. The added benefit is that phrase extraction for all these parts can run in parallel (2h25m per part on average). The merging of these parts took 16h12m, which is still substantially faster than the single extraction (53h7m). 2.2 Discriminative Translation Model We add a feature-rich, discriminative model of phrasal translation to our system (Tamchyna et al., 2016). This classifier produces a single phrase translation probabi"
W16-2325,P11-2031,0,0.0511747,"ntences and target-side context features as additional source of information in our official submission. 3.6 BLEU 26.2 26.6 28.0 28.1 28.3 Results Table 2 lists BLEU scores of various system settings. Each BLEU score is an average over 5 runs of system tuning (MERT, Och, 2003). The table shows how BLEU score develops as we add the individual components to the system: the 7gram morphological LM (“tagLM”), the 4-gram LM from Common Crawl (“ccrawl”), the lexicalized reordering (“RR”) and finally the discriminative translation model (“discTM”). We test for statistical significance using MultEval (Clark et al., 2011); we test each new component against the system without it (i.e., +tagLM is compared to baseline, +ccrawl is tested against +tagLM etc.). When the p-value is lower than 0.05, we mark the result in bold. 5 Acknowledgement This work has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements no. 644402 (HimL) and no. 645452 (QT21). This work has been using language resources stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2015071). This work was partially s"
W16-2325,P13-2071,1,0.867673,"nd uses surface forms, lemmas and tags. On the target side, the model has access to limited context (similarly to an LM) and uses target surface forms, lemmas and tags. However, our English-Czech submission to WMT16 does not use target-context information due to time constraints. 2.4 Lexicalized Reordering and OSM We experiment with using a lexicalized reordering model (Koehn et al., 2005) in the common setting: model monotone/swap/discontinuous reordering, word-based extraction, bidirectional, conditioned both on the source and target language. We also train an operation sequence model (OSM, Durrani et al., 2013), which is a generative model that sees the translation process as a linear sequence of operations which generate a source and target sentence in parallel. The probability of a sequence of operations is defined according to an n-gram model, that is, the probability of an operation depends on the n − 1 preceding operations. We have trained our 5-gram model on surface forms, using the CzEng16pre corpus. Language Models Our LM configuration is based on the successful setting from previous years, however all LMs are trained using the constrained data; this is a major difference from our previous s"
W16-2325,2005.mtsummit-papers.11,0,0.0101213,"up in the same cluster, ranking #4 and #5 among all systems for this language pair. The hacked system • seems negligibly better (0.302 TrueSkill) than the one with the discriminative model (∗, reaching 0.299 TrueSkill). 3 English-Romanian System We also submitted a constrained phrase-based system for English→Romanian translation which is loosely inspired by the basic components of C HIMERA. Additionally, our submission uses the source- and target-context discriminative translation model as well. 3.1 Data and Pre-Processing We use all the data available to constrained submissions: Europarl v8 (Koehn, 2005) and SETIMES2 (Tiedemann, 2009) parallel corpora and News 2015 and Common Crawl monolingual corpora.1 We split the official development set into two halves; we use the first part for system tuning and the second part serves as our test set. Data pre-processing differs between English and Romanian. For English, we use Treex (Popel ˇ and Zabokrtsk´ y, 2010) to obtain morphological tags, lemmas and dependency parses of the sentences. For Romanian, we use the online tagger by Tufis et al. (2008) as run by our colleagues at LIMSI-CNRS for the joint QT21 Romanian system (Peter et al., 2016). 3.2 Fac"
W16-2325,W12-3146,0,0.156313,"Missing"
W16-2325,P16-1161,1,0.84112,"s a linear, uniform interpolation of many small phrase tables (Carpuat et al., 2014). We experiment with splitting the data into 20 parts (without any domain selection, simply a random shuffle) and using linear interpolation to combine the partial models. The added benefit is that phrase extraction for all these parts can run in parallel (2h25m per part on average). The merging of these parts took 16h12m, which is still substantially faster than the single extraction (53h7m). 2.2 Discriminative Translation Model We add a feature-rich, discriminative model of phrasal translation to our system (Tamchyna et al., 2016). This classifier produces a single phrase translation probability which is additionally conditioned on the full source sentence and limited left-hand-side target context. The probability is added as an additional feature to Moses’ log-linear model. The motivation for adding the context model is to improve lexical choice (which can be better inferred thanks to full source-context information) and morphological coherence. The model uses a rich feature set on both sides: In the source, the model has access to the full input sentence and uses surface forms, lemmas and tags. On the target side, th"
W16-2325,W15-4103,1,0.784163,"based post-processing system (Rosa et al., 2012). The core of the system is Moses. We combine it with TectoMT in a simple way which we refer to as “poor man’s” system combination: we translate our development and test data with TectoMT first and then add the source sentences and their translations as additional (synthetic) parallel data to the Moses system. This new corpus is used to train a separate phrase table. At test time, we run Moses which uses both phrase tables and we correct its output using Depfix. The system is described in detail in Bojar et al. (2013). Our subsequent analysis in Tamchyna and Bojar (2015) shows that the contribution of TectoMT is essential for the performance of C HIMERA. In particular, TectoMT provides new translations which are otherwise not available to the phrase-based system and it also improves the morphological and syntactic coherence of translations. Introduction We have a long-term experience with English-toCzech machine translation and over the years, our systems have grown together from rather diverse set of system types to a single system combination called C HIMERA (Bojar et al., 2013). This system has been successful in the previous three years of WMT (Bojar et a"
W16-2325,W14-3322,1,0.860386,"t the contribution of TectoMT is essential for the performance of C HIMERA. In particular, TectoMT provides new translations which are otherwise not available to the phrase-based system and it also improves the morphological and syntactic coherence of translations. Introduction We have a long-term experience with English-toCzech machine translation and over the years, our systems have grown together from rather diverse set of system types to a single system combination called C HIMERA (Bojar et al., 2013). This system has been successful in the previous three years of WMT (Bojar et al., 2013; Tamchyna et al., 2014; Bojar and Tamchyna, 2015) and we follow a similar design this year. Unlike previous years, we only use constrained data in system training, to allow for a more meaningful comparison with the competing systems. The gains thanks to the additional data in contrast to the gains thanks the system combination have been evaluated in terms of BLEU in Bojar and Tamchyna (2015). The details of our English-to-Czech system are in Section 2. In this work, we also present our system submission for English-Romanian translation. This system uses a factored setting similar to C HIMERA but lacks its two key c"
W16-2325,tufis-etal-2008-racais,0,0.0121309,"el as well. 3.1 Data and Pre-Processing We use all the data available to constrained submissions: Europarl v8 (Koehn, 2005) and SETIMES2 (Tiedemann, 2009) parallel corpora and News 2015 and Common Crawl monolingual corpora.1 We split the official development set into two halves; we use the first part for system tuning and the second part serves as our test set. Data pre-processing differs between English and Romanian. For English, we use Treex (Popel ˇ and Zabokrtsk´ y, 2010) to obtain morphological tags, lemmas and dependency parses of the sentences. For Romanian, we use the online tagger by Tufis et al. (2008) as run by our colleagues at LIMSI-CNRS for the joint QT21 Romanian system (Peter et al., 2016). 3.2 Factored Translation Similarly to C HIMERA, we train a factored phrase table which translates source surface forms to tuples (form, lemma, tag). Our input is factored and contains the form, lemma, morphological tag, 1 387 http://commoncrawl.org/ Setting baseline +tagLM +ccrawl +RM +discTM lemma of dependency parent and analytical function (“surface” syntactic role, e.g. Subj for subjects). These additional source-side factors are again not used by the phrase table and serve only as information"
W16-2325,W16-2320,1,\N,Missing
W16-2327,J07-2003,0,0.787531,"dual monolingual corpora, we first used lmplz (Heafield et al., 2013) to train count-based 5-gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then used the SRILM toolkit (Stolcke, 2002) to linearly interpolate the models http://ufal.mff.cuni.cz/treex 399 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 399–410, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2.6 using weights tuned to minimize perplexity on the development set. In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based"
W16-2327,P11-2072,0,0.0157943,"ubject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 4.5 Word Alignment Baseline Features Our core set of string-to-tree feature functions is unchanged from previous years. It includes the ngram language model’s log probability for the target string, the target word count, the rule count, and several pre-computed rule-specific scores. The rule-specific scores were: the direct and indirect translation probabilities; the direct and indirect lexical weights (Koeh"
W16-2327,P05-1066,0,0.077238,"s representing the decoding search space) on a concatenation of newssyscomb2009 and newstest2008–2012. BLEU 26.8 26.2 25.6 26.4 26.6 26.5 3.5 Table 2: Effect of each of the language models used in the English→Romanian system. The experiments are not cumulative, so we first try pruning the “all” language model, then go back to the unpruned version and remove each LM in turn, observing the effect. The submitted system used all four LMs, and the scores shown are uncased B LEU scores on newstest2016. 3.4 German→English For phrase-based translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. The operation sequence model for the German→English phrase-based system was unpruned. We integrated three language models: an unpruned LM over all English data except the CommonCrawl monolingual corpus; a pruned LM over CommonCrawl; and a pruned LM over the monolingual News Crawl 2015 corpus. In addition to lexical smoothing with the standard lexicon models, we utilized a source-to-target IBM Model 1 (Brown et al., 1993) for sentence-level lexical scoring in a similar manner as described by Huck et al."
W16-2327,P14-1129,0,0.035129,"on limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English Co"
W16-2327,P13-2071,0,0.020975,"a weighted linear combination of features. The core features of our model are a 5-gram LM score (i.e. log probability), phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with Good-Turing smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and right-to-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 2.5 Decoding Tuning Since our feature set (generally around 500 to 1000 features) was too large for MERT, we used k-best batch MIRA for tuning (Cherry and Foster, 2012). To speed up tuning we applied threshold pruning to th"
W16-2327,J93-2003,0,0.052624,"Missing"
W16-2327,D14-1082,0,0.163978,"ram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), which was ranked (joint) first at WMT 15. Our improvements this year stem from particle verb restructuring (Sennrich and Haddow, 2015), and the use of the new monolingual News Crawl 2015 corpus for In 4 cases, the system with constraints delivered much better translation, and three of those were overall improvement of the sentence structure. In 41 cases, the area was better for various reasons. Most frequently (16 cases), this was ind"
W16-2327,N13-1073,0,0.099487,"Missing"
W16-2327,N12-1047,0,0.356006,"rection. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 2.5 Decoding Tuning Since our feature set (generally around 500 to 1000 features) was too large for MERT, we used k-best batch MIRA for tuning (Cherry and Foster, 2012). To speed up tuning we applied threshold pruning to the phrase table, based on the direct translation model probability. 400 are quite understandable, e.g. source yös Intian on sanottu olevan kiinnostunut puolustusyhteistyösopimuksesta Japanin kanssa. base India is also said to be interested in puolustusyhteistyösopimuksesta with Japan. bpe India is also said to be interested in defence cooperation agreement with Japan. reference India is also reportedly hoping for a deal on defence collaboration between the two nations. However applying BPE to Finnish can also result in some rather odd trans"
W16-2327,W06-1607,0,0.0250361,"ts here, we used 100,000 BPE merges to create the model. Applying BPE to Finnish→English was clearly effective at addressing the unknown word problem, and in many cases the resulting translations Baseline Features We follow the standard approach to SMT of scoring translation hypotheses using a weighted linear combination of features. The core features of our model are a 5-gram LM score (i.e. log probability), phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with Good-Turing smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and right-to-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features"
W16-2327,D10-1063,0,0.0144921,"fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 4.5 Word Alignment Baseline Features Our core set of string-to-tree feature functions is unchanged from previous years. It includes the ngram language model’s log probability for the target string, the target word count, the rule count, and several pre-computed rule-specific scores. The rule-specific scores were: the direct and indirect translation probabilities; the direct and indirect lexical weights (Koehn et al., 2003); the monolingual PCFG probability of the tree fra"
W16-2327,P07-1019,0,0.0831252,"For individual monolingual corpora, we first used lmplz (Heafield et al., 2013) to train count-based 5-gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then used the SRILM toolkit (Stolcke, 2002) to linearly interpolate the models http://ufal.mff.cuni.cz/treex 399 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 399–410, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2.6 using weights tuned to minimize perplexity on the development set. In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based"
W16-2327,P06-1121,0,0.0809512,"Missing"
W16-2327,2015.iwslt-evaluation.4,1,0.850731,"ms are given in Table 3. English→German For the English→German phrase-based system, we exploited several translation factors in addition to word surface forms, in particular: Och clusters (with 50 classes) and part-of-speech tags (Ratnaparkhi, 1996) on the English side, as well as Och clusters (50 classes), morphological tags, and part-of-speech tags on the German side (Schmid, 2000). Recent experiments for our IWSLT 2015 phrase-based system have reconfirmed that English→German translation quality can benefit from these factors when supplementary models over factored representations are used (Huck and Birch, 2015). For WMT16, we utilized the factors in the translation model, in operation sequence models, and in language models (for linearly interpolated 7-gram LMs over Och clusters and morphological tags). Sparse source word deletion, target word insertion, and word translation features were integrated over the top 200 word surface forms and over selected factors (source and target Och clusters, source part-of-speech tags and target morphological tags). An unpruned 5-gram LM over words that was trained on all German data except the CommonCrawl monolingual corpus was supplemented by a separate pruned LM"
W16-2327,N04-1035,0,0.0743322,"pus was made up of three parts: all the English monolingual medical data from WMT14 medical, WMT16 biomedical and EMEA (11M sentences); all the English LDC GigaWord data (180M sentences); and all the English general domain data from WMT16 (240M sentences). We used the monolingual data to build three different language models which were then linearly interpolated. System tuning was with the SCIELO development data provided for the biomedical task. 4 4.4 SCFG rules were extracted from the word-aligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004, 2006). Minimal GHKM rules were composed into larger rules subject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexic"
W16-2327,W14-4018,1,0.849099,"s, improving overall sentence structure on average. Crazy Reordering 3 Table 7: Manual evaluation of translations as proposed by the English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), w"
W16-2327,D08-1089,0,0.179545,"Missing"
W16-2327,W08-0509,0,0.143531,"Missing"
W16-2327,2011.iwslt-papers.1,1,0.855699,"al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. The operation sequence model for the German→English phrase-based system was unpruned. We integrated three language models: an unpruned LM over all English data except the CommonCrawl monolingual corpus; a pruned LM over CommonCrawl; and a pruned LM over the monolingual News Crawl 2015 corpus. In addition to lexical smoothing with the standard lexicon models, we utilized a source-to-target IBM Model 1 (Brown et al., 1993) for sentence-level lexical scoring in a similar manner as described by Huck et al. (2011) for hierarchical systems. We tuned on the concatenation of newssyscomb2009 and newstest2008–2012. Unlike last year’s system (Haddow et al., 2015)—and different from the inverse translation direction (English→German)—we refrained from using any factors and instead set up a system that operates over surface form word representations only. In relation to last year’s system, we were able to maintain high translation quality as measured in B LEU despite the abandonment of factors. However, we suspect that human judgment scores may suffer a bit from the abandonment of a factored model. We decided t"
W16-2327,N10-1129,0,0.0414698,"Missing"
W16-2327,W15-3013,1,0.881832,"Missing"
W16-2327,E03-1076,0,0.0923236,"concatenation of newssyscomb2009 and newstest2008–2012. BLEU 26.8 26.2 25.6 26.4 26.6 26.5 3.5 Table 2: Effect of each of the language models used in the English→Romanian system. The experiments are not cumulative, so we first try pruning the “all” language model, then go back to the unpruned version and remove each LM in turn, observing the effect. The submitted system used all four LMs, and the scores shown are uncased B LEU scores on newstest2016. 3.4 German→English For phrase-based translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. The operation sequence model for the German→English phrase-based system was unpruned. We integrated three language models: an unpruned LM over all English data except the CommonCrawl monolingual corpus; a pruned LM over CommonCrawl; and a pruned LM over the monolingual News Crawl 2015 corpus. In addition to lexical smoothing with the standard lexicon models, we utilized a source-to-target IBM Model 1 (Brown et al., 1993) for sentence-level lexical scoring in a similar manner as described by Huck et al. (2011) for hierarchical systems. We tuned on th"
W16-2327,N03-1017,0,0.0439096,"011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 4.5 Word Alignment Baseline Features Our core set of string-to-tree feature functions is unchanged from previous years. It includes the ngram language model’s log probability for the target string, the target word count, the rule count, and several pre-computed rule-specific scores. The rule-specific scores were: the direct and indirect translation probabilities; the direct and indirect lexical weights (Koehn et al., 2003); the monolingual PCFG probability of the tree fragment from which the rule was extracted; and a rule As in the phrase-based models, we used fast_align for word alignment and the grow-diag-final-and heuristic for symmetrization. 4.3 unbinarized 5 20 5 Table 4: Parameter settings for rule composition. The parameters were relaxed for systems that used binarization to allow for the increase in tree node density. Except for English-Czech, which we describe separately in Section 5.1, preprocessing was similar to the phrase-based systems (Section 2.3). To parse the target-side of the training data,"
W16-2327,N04-1022,0,0.115351,"anguage models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then used the SRILM toolkit (Stolcke, 2002) to linearly interpolate the models http://ufal.mff.cuni.cz/treex 399 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 399–410, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2.6 using weights tuned to minimize perplexity on the development set. In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams on"
W16-2327,N12-1005,0,0.0147426,"t translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English CommonCrawl corpus."
W16-2327,W05-0904,0,0.0750355,"e English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), which was ranked (joint) first at WMT 15. Our improvements this year stem from particle verb restructuring (Sennrich and Haddow, 2015)"
W16-2327,H05-1066,0,0.0224782,"s. We used randomly-chosen subsets of the previous years’ test data to speed up decoding. 5 5.1 system baseline + constraints HimL1 23.3 23.6 B LEU HimL2 Khresmoi 18.6 20.4 18.8 20.7 Table 5: Translation results on the development system for English→Czech with unification-based constraints. Cased B LEU scores are shown. They are averaged over three tuning runs (note that baseline weights are reused in the experiments with constraints). Syntax-based Experiments English→Czech For English→Czech, we used Treex to preprocess and parse the Czech-side of the training data. Treex uses the MST parser (McDonald et al., 2005), which produces dependency graphs with non-projective arcs. In order to extract SCFG rules, we first applied the following conversion process: i) the dependency graphs were projectivized using the Malt Parser, which implements the method described in Nivre and Nilsson (2005) (we used the ‘Head’ encoding scheme); ii) the projective dependency graphs were converted to CFG trees. In addition, we reduced the complex positional tags to simple POS tags by discarding the morphological attributes. The CFG trees were not binarized. We also experimented with unificationbased agreement and case governme"
W16-2327,W13-2221,1,0.819391,"e used the monolingual data to build three different language models which were then linearly interpolated. System tuning was with the SCIELO development data provided for the biomedical task. 4 4.4 SCFG rules were extracted from the word-aligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004, 2006). Minimal GHKM rules were composed into larger rules subject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binar"
W16-2327,W14-4011,1,0.900492,"Missing"
W16-2327,P05-1013,0,0.487034,"ased constraints. Cased B LEU scores are shown. They are averaged over three tuning runs (note that baseline weights are reused in the experiments with constraints). Syntax-based Experiments English→Czech For English→Czech, we used Treex to preprocess and parse the Czech-side of the training data. Treex uses the MST parser (McDonald et al., 2005), which produces dependency graphs with non-projective arcs. In order to extract SCFG rules, we first applied the following conversion process: i) the dependency graphs were projectivized using the Malt Parser, which implements the method described in Nivre and Nilsson (2005) (we used the ‘Head’ encoding scheme); ii) the projective dependency graphs were converted to CFG trees. In addition, we reduced the complex positional tags to simple POS tags by discarding the morphological attributes. The CFG trees were not binarized. We also experimented with unificationbased agreement and case government constraints (Williams and Koehn, 2011; Williams, 2014). Specifically, our constraints were designed to enforce: i) case, gender, and number agreement between nouns and pre-nominal adjectival modifiers; ii) number and person agreement between subjects and verbs; iii) case a"
W16-2327,Q15-1013,1,0.838097,"hypothesis better in a surprisingly larger span of words, improving overall sentence structure on average. Crazy Reordering 3 Table 7: Manual evaluation of translations as proposed by the English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We repo"
W16-2327,P03-1021,0,0.0414024,"translation task. We used two test sets from the HimL project and the Khresmoi test set. Results with and without constraints are shown in Table 5. We used hard constraints and reused the baseline weights (re-tuning did not appear to give additional gains). rareness penalty. 4.6 Decoding Decoding for the string-to-tree models is based on Sennrich’s (2014) recursive variant of the CYK+ parsing algorithm combined with LM integration via cube pruning (Chiang, 2007). 4.7 Tuning The feature weights for the English→Czech and Finnish→English systems were tuned using the Moses implementation of MERT (Och, 2003). For the remaining systems we used k-best MIRA (Cherry and Foster, 2012) due to the use of sparse features. We used randomly-chosen subsets of the previous years’ test data to speed up decoding. 5 5.1 system baseline + constraints HimL1 23.3 23.6 B LEU HimL2 Khresmoi 18.6 20.4 18.8 20.7 Table 5: Translation results on the development system for English→Czech with unification-based constraints. Cased B LEU scores are shown. They are averaged over three tuning runs (note that baseline weights are reused in the experiments with constraints). Syntax-based Experiments English→Czech For English→Cze"
W16-2327,D15-1248,1,0.87645,"for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), which was ranked (joint) first at WMT 15. Our improvements this year stem from particle verb restructuring (Sennrich and Haddow, 2015), and the use of the new monolingual News Crawl 2015 corpus for In 4 cases, the system with constraints delivered much better translation, and three of thos"
W16-2327,P06-1055,0,0.0370971,"ility of the tree fragment from which the rule was extracted; and a rule As in the phrase-based models, we used fast_align for word alignment and the grow-diag-final-and heuristic for symmetrization. 4.3 unbinarized 5 20 5 Table 4: Parameter settings for rule composition. The parameters were relaxed for systems that used binarization to allow for the increase in tree node density. Except for English-Czech, which we describe separately in Section 5.1, preprocessing was similar to the phrase-based systems (Section 2.3). To parse the target-side of the training data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, and the ParZu dependency parser (Sennrich et al., 2013) for German. Except where stated otherwise, we right-binarized the trees after parsing to increase rule coverage. 4.2 Rule Extraction Language Models As in the phrase-based systems (Section 2.3), we used linearly-interpolated language models as standard, with some systems adding Common403 In preliminary experiments we used a smaller training set, comprising 2 million sentence pairs sampled from OPUS and monolingual data from last year’s WMT translation task. We used two test sets from the HimL project"
W16-2327,P16-1162,1,0.597202,"rm any corpus filtering other than the standard Moses method, which removes sentence pairs with extreme length ratios, and sentences longer than 80 tokens. Introduction Edinburgh’s submissions to the WMT 2016 news translation task fall into two distinct groups: neural translation systems and statistical translation systems. In this paper, we describe the statistical systems, which includes a mix of phrase-based and syntax-based approaches. We also include a brief description of our phrase-based submission to the WMT16 biomedical translation task. Our neural systems are described separately in Sennrich et al. (2016a). In most cases, our statistical systems build on last year’s, incorporating recent modelling refinements and adding this year’s new training data. For Romanian—a new language this year—we paid particular attention to language-specific processing of diacritics. For English→Czech, we experimented with a string-to-tree system, first using Treex1 (formerly TectoMT; Popel and Žabokrtský, 2010) to produce Czech dependency parses, then converting them to constituency representation and extracting GHKM rules. In the next two sections, we describe the phrasebased systems, first describing the core s"
W16-2327,W11-2126,1,0.846339,"dependency graphs with non-projective arcs. In order to extract SCFG rules, we first applied the following conversion process: i) the dependency graphs were projectivized using the Malt Parser, which implements the method described in Nivre and Nilsson (2005) (we used the ‘Head’ encoding scheme); ii) the projective dependency graphs were converted to CFG trees. In addition, we reduced the complex positional tags to simple POS tags by discarding the morphological attributes. The CFG trees were not binarized. We also experimented with unificationbased agreement and case government constraints (Williams and Koehn, 2011; Williams, 2014). Specifically, our constraints were designed to enforce: i) case, gender, and number agreement between nouns and pre-nominal adjectival modifiers; ii) number and person agreement between subjects and verbs; iii) case agreement between prepositions and nouns; iv) use of nominative case for subject nouns. For every Czech word in the training data, we obtained a set of morphological analyses using MorphoDiTa (Straková et al., 2014). From these analyses, we constructed a lexicon of feature structures. For constraint extraction, we used handwritten rules along the lines of those d"
W16-2327,R13-1079,1,0.855359,"ased models, we used fast_align for word alignment and the grow-diag-final-and heuristic for symmetrization. 4.3 unbinarized 5 20 5 Table 4: Parameter settings for rule composition. The parameters were relaxed for systems that used binarization to allow for the increase in tree node density. Except for English-Czech, which we describe separately in Section 5.1, preprocessing was similar to the phrase-based systems (Section 2.3). To parse the target-side of the training data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, and the ParZu dependency parser (Sennrich et al., 2013) for German. Except where stated otherwise, we right-binarized the trees after parsing to increase rule coverage. 4.2 Rule Extraction Language Models As in the phrase-based systems (Section 2.3), we used linearly-interpolated language models as standard, with some systems adding Common403 In preliminary experiments we used a smaller training set, comprising 2 million sentence pairs sampled from OPUS and monolingual data from last year’s WMT translation task. We used two test sets from the HimL project and the Khresmoi test set. Results with and without constraints are shown in Table 5. We used"
W16-2327,W12-3150,1,0.844228,"M sentences of parallel data. Our monolingual corpus was made up of three parts: all the English monolingual medical data from WMT14 medical, WMT16 biomedical and EMEA (11M sentences); all the English LDC GigaWord data (180M sentences); and all the English general domain data from WMT16 (240M sentences). We used the monolingual data to build three different language models which were then linearly interpolated. System tuning was with the SCIELO development data provided for the biomedical task. 4 4.4 SCFG rules were extracted from the word-aligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004, 2006). Minimal GHKM rules were composed into larger rules subject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restri"
W16-2327,W14-3324,1,0.800794,"Missing"
W16-2327,W15-3024,1,0.879589,"rder. Since the hard unification constraints effectively only avoid some of the possible translations (i.e. reduce the search space), we conclude that having to obey mere agreement constraints helps to select a hypothesis better in a surprisingly larger span of words, improving overall sentence structure on average. Crazy Reordering 3 Table 7: Manual evaluation of translations as proposed by the English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we a"
W16-2327,P14-5003,0,0.0285107,"Missing"
W16-2327,tiedemann-2012-parallel,0,0.108338,". 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English CommonCrawl corpus. We used newsdev2015 for tuning, and newsdev2015 for testing during system development. One clear problem that we noted with our submission from last year was the large number of OOVs, which were then copied directly into the English output. This is undoubtedly due to the agglutinative nature of Finnish, and probably was the cause of our system being poorly judged by human evaluators,"
W16-2327,D13-1140,0,0.194779,"(Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English CommonCrawl corpus. We used newsdev2015 for tuning, and newsdev2015 for testing during system d"
W16-2327,N07-1051,0,\N,Missing
W16-2327,W09-0429,1,\N,Missing
W16-2327,P05-1045,0,\N,Missing
W16-2327,P13-2121,0,\N,Missing
W16-2334,W16-2332,1,0.730263,"f performance of the forced non-translations from the evaluation section: we simply always apply it in the TectoMT system, but never in the Moses system.9 4 Moses 4.2 TectoMT TectoMT is a hybrid MT system, combining statistical and rule-based Treex blocks to perform translation with transfer on the layer of tectogrammatical (deep) syntax. MT Systems We use two systems, Moses (Koehn et al., 2007) ˇ and TectoMT (Zabokrtsk´ y et al., 2008), as well as their combination Chimera (Bojar et al., 2013); see also a more detailed description of the Moses and TectoMT systems within the QTLeap project by Gaudio et al. (2016) in these proceedings. 10 W2A::ResegmentSentences W2A::TokenizeMoses 12 W2A::TokenizeMorphoDiTa for EN→CS 13 W2W::NormalizeEnglishSentence 14 W2A::EscapeMoses 15 W2A::TruecaseMoses 16 A2A::ProjectCase 17 A2W::CapitalizeSentStart 11 9 This holds even for the Chimera combination, i.e. this method is applied in its TectoMT component but not in the Moses component. 452 System Annotations (not adapted) XXX XML (not adapted) XXX XML →ES 22.23 23.61 24.22 26.01 26.89 27.40 →NL 23.40 24.89 25.41 21.82 23.52 23.26 →PT 14.01 15.47 15.58 13.11 14.19 14.21 We use TectoMT’s translation model interpolation"
W16-2334,P07-2045,1,0.0394348,"rained MT systems with no further retraining. We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination. In all settings, our method improves the translation quality. Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one. before and after processing them by the MT system, and provide both system-specific and systemindependent approaches. We employ the MT systems used and further developed by us and our partners within the QTLeap project, namely Moses (Koehn et al., 2007), Tecˇ toMT (Zabokrtsk´ y et al., 2008), and their combination Chimera (Bojar et al., 2013). We briefly describe the systems in § 4. In § 5, we evaluate our domain-adaptation methods (as well as the standard method of retraining the system with all available data) applied to these MT systems for translation from English to Czech (EN→CS), Spanish (EN→ES), Dutch (EN→NL), and Portuguese (EN→PT). Introduction 2 In this paper, we describe our work on domain adaptation of machine translation systems, performed in close collaboration with numerous partners within the QTLeap project.1 The project focu"
W16-2334,P03-1021,0,0.0324255,"ems domain-adapted: they are trained and tuned on the Batch1 and Batch2 parts of the in-domain training data, as described below. Thus, even without the domain adaptation through in-domain lexicons (which were also provided by the task organizers), the systems constitute strong baselines within the IT domain. Still, the lexicons were not used to train nor tune the systems. 3.5 4.1 Forced non-translations Moses is a standard phrase-based statistical machine translation system. We train Moses on general-domain training data and tune it on the Batch2 part from in-domain training data using MERT (Och, 2003). We perform domain adaptation of Moses using either XXX placeholders or XML annotations. EN→CS uses factored translation (with part-of-speech tags as additional target-side factors), which is not compatible with the XML annotations, and thus only XXX placeholders are used for EN→CS. We apply some rather standard pre- and postprocessing steps (implemented as Treex blocks). Preprocessing: • segmentation into sentences10 • tokenization11,12 • normalization of quotes, dashes and contracted forms (for EN→CS)13 • entity escaping14 • truecasing (for EN→NL)15 /lowercasing Postprocessing: • projection"
W16-2334,P02-1040,0,0.100304,"of the in-domain training data. Unlike Moses, TectoMT does not support automatic tuning of parameters; however, some parameters were tuned manually using Batch2 from in-domain training data. We only experiment with domain adaptation of TectoMT via Treex wild attributes (§ 3.4). Table 1: BLEU evaluation of two forced translation styles for Moses: XXX placeholders and XML markup. For comparison, the non-adapted system is also included. 4.3 5 Moses Chimera Chimera We use the WMT16 IT task test set (i.e. Batch3 from the QTLeap corpus19 ) to evaluate our experiments using (case-insensitive) BLEU (Papineni et al., 2002). First, in Table 1, we compare the two annotation styles we can use for Moses. In general, the XML annotations perform better, in half of the cases leading to a result better by about +0.5 BLEU than that of the XXX placeholders while performing worse only once. Although the documentation in the Moses manual is not very detailed in this respect, we believe that the XML annotations are more palatable to the language model, which can then make meaningful decisions at the boundaries of the force-translated entities, while the XXX placeholders simply constitute out-ofvocabulary items for the langu"
W16-2334,W12-3146,1,0.930914,"Missing"
W16-2334,W15-5711,1,0.848136,"Missing"
W16-2334,W08-0325,0,0.736805,"g. We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination. In all settings, our method improves the translation quality. Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one. before and after processing them by the MT system, and provide both system-specific and systemindependent approaches. We employ the MT systems used and further developed by us and our partners within the QTLeap project, namely Moses (Koehn et al., 2007), Tecˇ toMT (Zabokrtsk´ y et al., 2008), and their combination Chimera (Bojar et al., 2013). We briefly describe the systems in § 4. In § 5, we evaluate our domain-adaptation methods (as well as the standard method of retraining the system with all available data) applied to these MT systems for translation from English to Czech (EN→CS), Spanish (EN→ES), Dutch (EN→NL), and Portuguese (EN→PT). Introduction 2 In this paper, we describe our work on domain adaptation of machine translation systems, performed in close collaboration with numerous partners within the QTLeap project.1 The project focuses on high-quality translation for the"
W16-2334,P07-1033,0,\N,Missing
W16-2334,W13-2208,1,\N,Missing
W16-2344,P07-2045,1,0.0310978,"the model. Tuning within SMT refers to the process of finding the optimal weights for these features on a given tuning set. This paper describes our submission to WMT16 Tuning Task1 , a shared task where all the SMT model components and the tuning set are given and task participants are expected to provide only the weight settings. We took part only in English-to-Czech system tuning. Our solution is based on the standard tuning method of Minimum Error-Rate Training (MERT, Och, 2003). The MERT algorithm described in Bertoldi et al. (2009) is the default tuning method in the Moses SMT toolkit (Koehn et al., 2007). The inner loop of the algorithm performs optimization on a space of weight vectors with a given 2 MERT The basic goal of MERT is to find optimal weights for various numerical features of an SMT system. The weights are considered optimal if they minimize an automated error metric which compares the machine translation to a human translation for a certain tuning (development) set. Formally, each feature provides a score (sometimes a probability) that a given sentence e in goal language is the translation of the foreign sentence f . Given a weight for each such feature, it is possible to combin"
W16-2344,P03-1021,0,0.13283,"s or scores to possible translations. These are then combined in a weighted sum to determine the best translation given by the model. Tuning within SMT refers to the process of finding the optimal weights for these features on a given tuning set. This paper describes our submission to WMT16 Tuning Task1 , a shared task where all the SMT model components and the tuning set are given and task participants are expected to provide only the weight settings. We took part only in English-to-Czech system tuning. Our solution is based on the standard tuning method of Minimum Error-Rate Training (MERT, Och, 2003). The MERT algorithm described in Bertoldi et al. (2009) is the default tuning method in the Moses SMT toolkit (Koehn et al., 2007). The inner loop of the algorithm performs optimization on a space of weight vectors with a given 2 MERT The basic goal of MERT is to find optimal weights for various numerical features of an SMT system. The weights are considered optimal if they minimize an automated error metric which compares the machine translation to a human translation for a certain tuning (development) set. Formally, each feature provides a score (sometimes a probability) that a given senten"
W16-2344,I11-1073,0,0.0162574,"article Swarm Optimization Submission for WMT16 Tuning Task Viktor Kocur CTU in Prague FNSPE kocurvik@fjfi.cvut.cz Ondˇrej Bojar Charles University in Prague ´ MFF UFAL bojar@ufal.mff.cuni.cz Abstract translation metric2 . The standard optimization is a variant of grid search and in our work, we replace it with the Particle Swarm Optimization (PSO, Eberhart et al., 1995) algorithm. Particle Swarm Optimization is a good candidate for an efficient implementation of the inner loop of MERT due to the nature of the optimization space. The so-called Traditional PSO (TPSO) has already been tested by Suzuki et al. (2011), with a success. Improved versions of the PSO algorithm, known as Standard PSO (SPSO), have been summarized in Clerc (2012). In this paper, we test a modified version of the latest SPSO2011 algorithm within the Moses toolkit and compare its results and computational costs with the standard Moses implementation of MERT. This paper describes our submission to the Tuning Task of WMT16. We replace the grid search implemented as part of standard minimum-error rate training (MERT) in the Moses toolkit with a search based on particle swarm optimization (PSO). An older variant of PSO has been previou"
W16-2361,J92-4003,0,0.391299,"Missing"
W16-2361,W14-4012,0,0.287119,"Missing"
W16-2361,W11-2107,0,0.036755,"ways in which the image annotation were collected also lead to two sub-tasks. The first one is called Multimodal Translation and its goal is to generate a translation of an image caption to the target language given the caption in source language and the image itself. The second task is the Cross-Lingual Image Captioning. In this setting, the system is provided five captions in the source language and it should generate one caption in target language given both sourcelanguage captions and the image itself. Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011). The translation task is evaluated against a single reference sentence which is the direct human translation of the source sentence. The cross-lingual captioning task is evaluated against the five reference captions in the target language created independently of the source captions. 4.2 Phrase-Based System For the translation task, we trained Moses SMT (Koehn et al., 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word 649 system Multimodal translation BLEU METEOR Cross-lingual ca"
W16-2361,W16-3210,0,0.125555,"Missing"
W16-2361,P16-1154,0,0.16715,"es are extracted from the 4096-dimensional penultimate layer (fc7) of the VGG-16 Imagenet network Simonyan and Zisserman (2014) before applying non-linearity. We keep the weights of the convolutional network fixed during the training. We do not use attention over the image features, so the image information is fed to the network only via the initial state. We also try a system combination and add an encoder for the phrase-based output. The SMT encoder shares the vocabulary and word embeddings with the decoder. For the combination with SMT output, we experimented with the CopyNet architecture (Gu et al., 2016) and with encoding the sequence the way as in the APE task (see Section 3.2). Since neither of these variations seems to have any effect on the performance, we report only the results of the simple encoder combina650 Source Reference Moses 2 Errors: MMMT Gloss: CLC Gloss: Source Reference Moses MMMT CLC A group of men are loading cotton onto a truck Eine Gruppe von M¨annern l¨adt Baumwolle auf einen Lastwagen eine Gruppe von M¨annern l¨adt cotton auf einen Lkw untranslated “cotton” and capitalization of “LKW” Eine Gruppe von M¨annern l¨adt etwas auf einem Lkw. ::::: A group of men are loading"
W16-2361,D15-1293,0,0.0314596,"s’ ability to learn a dense representation of the input in the form of a real-valued vector recently allowed researchers to combine machine vision and natural language processing into tasks believed to be extremely difficult only few years ago. The distributed representations of words, sentences and images can be understood as a kind of common data type for language and images within the models. This is then used in tasks like automatic image captioning (Vinyals et al., 2015; Xu et al., 2015), visual question answering (Antol et al., 2015) or in attempts to ground lexical semantics in vision (Kiela and Clark, 2015). Model Description We use the neural translation model with attention (Bahdanau et al., 2014) and extend it to include multiple encoders, see Figure 1 for an illustration. Each input sentence enters the system simultaneously in several representations xi . An encoder used for the i-th representation Xi = (x1i , . . . , xki ) of k words, each stored as a one-hot vector xji , is a bidirectional RNN implementing a function f (Xi ) = Hi = (h1i , . . . , hki ) (1) where the states hji are concatenations of the outputs of the forward and backward networks after processing the j-th token in the resp"
W16-2361,D15-1044,0,0.0336949,"es the architecture of the networks we have used. Section 3 summarizes related work on the task of automatic post-editing of machine translation output and describes our submission to the Workshop of Machine Translation (WMT) competition. In a similar fashion, Section 4 refers to the task of multimodal translation. Conclusions and ideas for further work are given in Section 5. Introduction 2 Neural sequence to sequence models are currently used for variety of tasks in Natural Language Processing including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), text summarization (Rush et al., 2015), natural language generation (Wen et al., 2015), and others. This was enabled by the capability of recurrent neural networks to model temporal structure in data, including the long-distance dependencies in case of gated networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014). The deep learning models’ ability to learn a dense representation of the input in the form of a real-valued vector recently allowed researchers to combine machine vision and natural language processing into tasks believed to be extremely difficult only few years ago. The distributed representations of words, senten"
W16-2361,P07-2045,1,0.0121854,"source language and it should generate one caption in target language given both sourcelanguage captions and the image itself. Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011). The translation task is evaluated against a single reference sentence which is the direct human translation of the source sentence. The cross-lingual captioning task is evaluated against the five reference captions in the target language created independently of the source captions. 4.2 Phrase-Based System For the translation task, we trained Moses SMT (Koehn et al., 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word 649 system Multimodal translation BLEU METEOR Cross-lingual captioning BLEU METEOR Moses baseline MM baseline 32.2 54.4 27.2 11.3 33.8 32.6 tuned Moses NMT NMT + Moses NMT + image NMT + Moses + image — ” —, submitted 36.8 37.1 36.5 34.0 37.3 31.9 57.4 54.6 54.3 51.6 55.2 49.6 12.3 13.6 13.7 13.3 13.6 13.0 35.0 34.6 35.1 34.4 34.9 33.5 9.1 22.7 24.6 14.0 25.3 38.5 39.3 31.6 captioning only 5 en captions 5 en captions + image — ” —, subm"
W16-2361,W07-0728,0,0.0666219,"LEU score. The system was able to deal very well with the frequent error of keeping a word from the source in the translated sentence. Although neural sequential models usually learn the basic output structure very quickly, in this case it made a lot of errors in pairing parentheses correctly. We ascribe this to the edit-operation notation which obfuscated the basic orthographic patterns in the target sentences. Related Work In the previous year’s competition (Bojar et al., 2015), most of the systems were based on the phrase-base statistical machine translation (SMT) in a monolingual setting (Simard et al., 2007). There were also several rule-based post-editing systems benefiting from the fact that errors introduced by statistical and rule-based systems are of a different type (Rosa, 2014; Mohaghegh et al., 2013). Although the use of neural sequential model is very straightforward in this case, to the best of our knowledge, there have not been experiments with RNNs for this task. 3.2 method 4 Experiments & Results Multimodal Translation The goal of the multimodal translation task is to generate an image caption in a target language (German) given the image itself and one or more captions in the source"
W16-2361,W13-4703,0,0.0288696,"tructure very quickly, in this case it made a lot of errors in pairing parentheses correctly. We ascribe this to the edit-operation notation which obfuscated the basic orthographic patterns in the target sentences. Related Work In the previous year’s competition (Bojar et al., 2015), most of the systems were based on the phrase-base statistical machine translation (SMT) in a monolingual setting (Simard et al., 2007). There were also several rule-based post-editing systems benefiting from the fact that errors introduced by statistical and rule-based systems are of a different type (Rosa, 2014; Mohaghegh et al., 2013). Although the use of neural sequential model is very straightforward in this case, to the best of our knowledge, there have not been experiments with RNNs for this task. 3.2 method 4 Experiments & Results Multimodal Translation The goal of the multimodal translation task is to generate an image caption in a target language (German) given the image itself and one or more captions in the source language (English). The input sentence is fed to our system in a form of multiple input sequences without explicitly telling which sentence is the source one and which one 648 Source Choose Uncached Refr"
W16-2361,2006.amta-papers.25,0,0.13055,"k, the organizers provided tokenized data from the IT domain (Turchi et al., 2016). The training data consist of 12,000 triplets of the source sentence, its automatic translation and a reference sentence. The reference sentences are manually post-edited automatic translations. Additional 1,000 sentences were provided for validation, and another 2,000 sentences for final evaluation. Throughout the paper, we report scores on the validation set; reference sentences for final evaluation were not released for obvious reasons. The performance of the systems is measured using Translation Error Rate (Snover et al., 2006) from the manually post-edited sentences. We thus call the score HTER. This means that the goal of the task is more to simulate manual post-editing, rather than to reconstruct the original unknown reference sentence. 3.1 HTER BLEU baseline edit operations edit operations+ .2481 .2438 .2436 62.29 62.70 62.62 Table 1: Results of experiments on the APE task on the validation data. The ‘+’ sign indicates the additional regular-expression rules – the system that has been submitted. is the MT output. It is up to the network to discover their best use when producing the (single) target sequence. The"
W16-2361,P02-1040,0,0.0965578,"ages and a test set with 1,000 images. The two ways in which the image annotation were collected also lead to two sub-tasks. The first one is called Multimodal Translation and its goal is to generate a translation of an image caption to the target language given the caption in source language and the image itself. The second task is the Cross-Lingual Image Captioning. In this setting, the system is provided five captions in the source language and it should generate one caption in target language given both sourcelanguage captions and the image itself. Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011). The translation task is evaluated against a single reference sentence which is the direct human translation of the source sentence. The cross-lingual captioning task is evaluated against the five reference captions in the target language created independently of the source captions. 4.2 Phrase-Based System For the translation task, we trained Moses SMT (Koehn et al., 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word 649 system"
W16-2361,2014.amta-researchers.3,0,0.359423,"he image itself. Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011). The translation task is evaluated against a single reference sentence which is the direct human translation of the source sentence. The cross-lingual captioning task is evaluated against the five reference captions in the target language created independently of the source captions. 4.2 Phrase-Based System For the translation task, we trained Moses SMT (Koehn et al., 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word 649 system Multimodal translation BLEU METEOR Cross-lingual captioning BLEU METEOR Moses baseline MM baseline 32.2 54.4 27.2 11.3 33.8 32.6 tuned Moses NMT NMT + Moses NMT + image NMT + Moses + image — ” —, submitted 36.8 37.1 36.5 34.0 37.3 31.9 57.4 54.6 54.3 51.6 55.2 49.6 12.3 13.6 13.7 13.3 13.6 13.0 35.0 34.6 35.1 34.4 34.9 33.5 9.1 22.7 24.6 14.0 25.3 38.5 39.3 31.6 captioning only 5 en captions 5 en captions + image — ” —, submitted Table 2: Results of experiments with the multimodal translation task on the validation data. At the time of"
W16-2361,P16-5005,0,0.0205225,"is a distribution estimated as  T m αm i = softmax v · tanh(s + WHi Hi ) (3) with sm being the hidden state of the decoder in time m. Vector v and matrix WHi are learned parameters for projecting the encoder states. The probability of the decoder emitting the word ym in the j-th step, denoted as P (ym |H1 , . . . , Hn , Y0..m−1 ), is proportional to ! n X exp Wo sj + (4) Wai aji We experimented with recently published improvements of neural sequence to sequence learning: scheduled sampling (Bengio et al., 2015), noisy activation function (G¨ulc¸ehre et al., 2016), linguistic coverage model (Tu et al., 2016). None of them were able to improve the systems’ performance, so we do not include them in our submissions. i=1 where Hi are hidden states from the i-th encoder and Y0..m−1 is the already decoded target sentence (represented as matrix, one-hot vector for each produced word). Matrices Wo and Wai are learned parameters; Wo determines the recurrent dependence on the decoder’s state and Wai determine the dependence on the (attention-weighted) encoders’ states. For image captioning, we do not use the attention model because of its high computational demands and rely on the basic model by Vinyals Si"
W16-2361,D15-1199,0,0.0540366,"Missing"
W16-2361,W15-3001,1,\N,Missing
W16-2371,P00-1037,0,0.204594,"sourceto-target or vice versa). With documents on both sides converted to one language, we then treat the task as a noisy channel problem, similarly to many works of information retrieval based on language modelling techniques (Ponte and Croft, 1998; Zhai and Lafferty, 2001; Xu et al., 2001). Specifically, we assume that the observed output is the source page S , damaged by noisy transfer of some target page T . Through decoding, we want to find the target page T that most likely lead to the observed output S. The process is visualized in Figure 1. Therefore, like in the noisy channel model (Brill and Moore, 2000), to decode the input T , we estimate the probability of T given the output observation S, P (T |S). Following Bayes’ rule, the problem is characterized by Equation 2: max(c) )· ct Nt X lS − |piSt − piTt | lS Language model-based approach (UFAL-2) (1) i Here S, T are the source and target documents, respectively, S ∩ T is the set containing all terms which occurs in both documents, Nt = min(|St |, |Tt |) where St , Tt is the number of occurrences of term t in the respective document. The length of the source document is denoted lS . piSt is the position of i-th occurrence of the term t in the"
W16-2371,W11-2125,0,0.0225105,"sk rely on document metadata (e.g. the similarity of document URLs or language tags within URLs), some emphasize more the actual content of the documents. Previous work (Rapp, 1999; Ma and Liberman, 1999) focused on document alignment by counting word co-occurrences between source and target documents in a fixed-size window. More recently, methods from cross-lingual information retrieval (CLIR) have been used (Snover et al., 2008; Abdul Rauf and Schwenk, 2011), ranking lists of target documents given a source document by a probabilistic model. Locality sensitive hashing has also been applied (Krstovski and Smith, 2011). Introduction Parallel data play an essential role in training of statistical machine translation (MT) systems. While big collections have been already created, e.g. the corpus OPUS (Tiedemann, 2012), the World Wide Web remains a largely underexploited source. That is the motivation for the shared task “Bilingual Document Alignment” of the ACL 2016 workshop First Conference on Machine Translation (WMT16) which requires participants to align web page in one language to their translation counterparts in another language. In this paper, we describe our attempt. The rest of the paper is organized"
W16-2371,1999.mtsummit-1.79,0,0.800346,"position similarity between candidate document pairs. The second method requires automatically translated versions of the target text, and matches them with the candidates. The third and fourth methods try to overcome some of the challenges presented by the nature of the corpus, by considering the string similarity of source URL and candidate URL, and combining the first two approaches. 1 Some approaches to the task rely on document metadata (e.g. the similarity of document URLs or language tags within URLs), some emphasize more the actual content of the documents. Previous work (Rapp, 1999; Ma and Liberman, 1999) focused on document alignment by counting word co-occurrences between source and target documents in a fixed-size window. More recently, methods from cross-lingual information retrieval (CLIR) have been used (Snover et al., 2008; Abdul Rauf and Schwenk, 2011), ranking lists of target documents given a source document by a probabilistic model. Locality sensitive hashing has also been applied (Krstovski and Smith, 2011). Introduction Parallel data play an essential role in training of statistical machine translation (MT) systems. While big collections have been already created, e.g. the corpus"
W16-2371,P99-1067,0,0.624039,"ses the term position similarity between candidate document pairs. The second method requires automatically translated versions of the target text, and matches them with the candidates. The third and fourth methods try to overcome some of the challenges presented by the nature of the corpus, by considering the string similarity of source URL and candidate URL, and combining the first two approaches. 1 Some approaches to the task rely on document metadata (e.g. the similarity of document URLs or language tags within URLs), some emphasize more the actual content of the documents. Previous work (Rapp, 1999; Ma and Liberman, 1999) focused on document alignment by counting word co-occurrences between source and target documents in a fixed-size window. More recently, methods from cross-lingual information retrieval (CLIR) have been used (Snover et al., 2008; Abdul Rauf and Schwenk, 2011), ranking lists of target documents given a source document by a probabilistic model. Locality sensitive hashing has also been applied (Krstovski and Smith, 2011). Introduction Parallel data play an essential role in training of statistical machine translation (MT) systems. While big collections have been already c"
W16-2371,D08-1090,0,0.0313657,"hallenges presented by the nature of the corpus, by considering the string similarity of source URL and candidate URL, and combining the first two approaches. 1 Some approaches to the task rely on document metadata (e.g. the similarity of document URLs or language tags within URLs), some emphasize more the actual content of the documents. Previous work (Rapp, 1999; Ma and Liberman, 1999) focused on document alignment by counting word co-occurrences between source and target documents in a fixed-size window. More recently, methods from cross-lingual information retrieval (CLIR) have been used (Snover et al., 2008; Abdul Rauf and Schwenk, 2011), ranking lists of target documents given a source document by a probabilistic model. Locality sensitive hashing has also been applied (Krstovski and Smith, 2011). Introduction Parallel data play an essential role in training of statistical machine translation (MT) systems. While big collections have been already created, e.g. the corpus OPUS (Tiedemann, 2012), the World Wide Web remains a largely underexploited source. That is the motivation for the shared task “Bilingual Document Alignment” of the ACL 2016 workshop First Conference on Machine Translation (WMT16"
W16-2371,tiedemann-2012-parallel,0,0.0167385,"ed on document alignment by counting word co-occurrences between source and target documents in a fixed-size window. More recently, methods from cross-lingual information retrieval (CLIR) have been used (Snover et al., 2008; Abdul Rauf and Schwenk, 2011), ranking lists of target documents given a source document by a probabilistic model. Locality sensitive hashing has also been applied (Krstovski and Smith, 2011). Introduction Parallel data play an essential role in training of statistical machine translation (MT) systems. While big collections have been already created, e.g. the corpus OPUS (Tiedemann, 2012), the World Wide Web remains a largely underexploited source. That is the motivation for the shared task “Bilingual Document Alignment” of the ACL 2016 workshop First Conference on Machine Translation (WMT16) which requires participants to align web page in one language to their translation counterparts in another language. In this paper, we describe our attempt. The rest of the paper is organized as follows: In Section 2, we describe the methods we used in our four submitted systems. Section 3 describes our experimental setup and compares the results of the proposed methods. We conclude the p"
W16-2371,N13-1073,0,\N,Missing
W16-2380,C04-1046,0,0.059671,"out low quality translations to avoid spending time post-editing them, or by providing end-users with an estimate on how good or bad the translation is. In 2012, WMT established the first sentencelevel quality estimation shared task (CallisonBurch et al., 2012). Since then, new sub-tasks, language pairs and datasets in different domains were introduced every year (Bojar et al., 2013, 2014, 2015). In contrast to automatic evaluation (the “metrics task”), QE task aims to develop systems that provide predictions on the quality of machine translated text without access to reference translations (Blatz et al., 2004; Specia et al., 2009). 2 Bilingual Distributed Representations Word embeddings have shown a great potential in tackling various NLP tasks recently, including multilingual tasks. However, there is a major problem with using word embeddings in a multilingual setting because models are trained independently for each of the languages and the resulting 764 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 764–771, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics α and β are used to balance the influence of the monol"
W16-2380,W13-2243,0,0.379303,"Missing"
W16-2380,S15-2046,0,0.0614813,"Missing"
W16-2380,P14-1006,0,0.0504903,"Missing"
W16-2380,W15-1521,0,0.30635,"ored by participating systems, including lexical, syntactic, semantic, embeddingbased features (Shah et al., 2015), as well as features dependent on any details the particular MT systems may provide (Soricut et al., 2012; Camargo de Souza et al., 2013). In our model, we try to exploit the power of bilingual distributed representations combined with word alignment information to boost the performance of translation quality estimation. For this purpose, we use the implementation provided by the Multivec tool (B´erard et al., 2016) for the bilingual distributed representation model, described by Luong et al. (2015) and the GIZA++ word alignment model (Och and Ney, 2003). The rest of this paper is organized as follows. In Sections 2 and 3, we give an overview of the bilingual distributional model and word alignment for our purposes. Section 4 gives a detailed description of our feature set, including the features derived from manual post-edits of other sentences. Section 5 describes the datasets and resources we used to build our model. Section 6 discusses the experiments conducted and the official results. The final Section 7 concludes the paper. This paper describes our submission UFAL MULTIVEC to the"
W16-2380,P13-4014,0,0.0521544,"Missing"
W16-2380,2009.eamt-1.5,0,0.0286548,"nslations to avoid spending time post-editing them, or by providing end-users with an estimate on how good or bad the translation is. In 2012, WMT established the first sentencelevel quality estimation shared task (CallisonBurch et al., 2012). Since then, new sub-tasks, language pairs and datasets in different domains were introduced every year (Bojar et al., 2013, 2014, 2015). In contrast to automatic evaluation (the “metrics task”), QE task aims to develop systems that provide predictions on the quality of machine translated text without access to reference translations (Blatz et al., 2004; Specia et al., 2009). 2 Bilingual Distributed Representations Word embeddings have shown a great potential in tackling various NLP tasks recently, including multilingual tasks. However, there is a major problem with using word embeddings in a multilingual setting because models are trained independently for each of the languages and the resulting 764 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 764–771, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics α and β are used to balance the influence of the monolingual components over"
W16-2380,S15-2027,0,0.0290017,"re-trained embeddings of the source language when learning target representations. A bilingual constraint (such as unsupervised word alignments derived from a parallel corpus; Zou et al., 2013) ensures that semantically similar words across languages end up with embeddings similar in the learned vector space. 3 For cross-lingual semantic similarity, a word alignment model is an important component. According to the evaluation of the semantic textual similarity task in SemEval 2015, the best performing systems in both the English and Spanish subtasks relied mainly on word alignment techniques (Sultan et al., 2015; H¨anig et al., 2015). Inspired by these results, we add features based on word alignment to the QE system. According to Specia et al. (2015), alignmentbased features are used for word-level QE only and there is no alignment-based features included in the baseline feature set for sentence-level QE. We use GIZA++ (Och and Ney, 2003) to obtain the alignments. By default, GIZA++ alignments are not symmetric. We symmetrize them by taking the intersection of the two directions, leading to high-precision alignments. For pre-processing, we lowercase and stem words (naively taking just the first four"
W16-2380,J03-1002,0,0.0281044,"ic, semantic, embeddingbased features (Shah et al., 2015), as well as features dependent on any details the particular MT systems may provide (Soricut et al., 2012; Camargo de Souza et al., 2013). In our model, we try to exploit the power of bilingual distributed representations combined with word alignment information to boost the performance of translation quality estimation. For this purpose, we use the implementation provided by the Multivec tool (B´erard et al., 2016) for the bilingual distributed representation model, described by Luong et al. (2015) and the GIZA++ word alignment model (Och and Ney, 2003). The rest of this paper is organized as follows. In Sections 2 and 3, we give an overview of the bilingual distributional model and word alignment for our purposes. Section 4 gives a detailed description of our feature set, including the features derived from manual post-edits of other sentences. Section 5 describes the datasets and resources we used to build our model. Section 6 discusses the experiments conducted and the official results. The final Section 7 concludes the paper. This paper describes our submission UFAL MULTIVEC to the WMT16 Quality Estimation Shared Task, for EnglishGerman"
W16-2380,N03-1033,0,0.0224651,"based features were introduced to estimate translation quality of each source-target sentence pair with the help of their POS tags. In our experiments, we restrict the range of POS tags used to produce our features to only nouns, verbs, adverbs, and adjectives. The POS tags for both English and German come again from Stanford POS Tagger. The two introduced features are: NounSim is similar to WordSim, but instead of taking all alignment links, we compute the average cosine similarity of only the links where the source (English) word is a noun. The POS tags were produced by Stanford POS Tagger (Toutanova et al., 2003). Alignment-Based Features We propose several features based on automatic word alignments as obtained in Section 3. 1 m X i=1 In our submission, we use three features derived from bilingual embeddings: 4.3 Alignment Quality Score Number of correctly matched tags represents the number of source words that are aligned to target words with the same POS tag. http://www.quest.dcs.shef.ac.uk/ 766 Number of wrongly matched tags represents the number of source words that are aligned to target words with a different POS tag. 5 Since the alignments are needed for the source and candidate translation, th"
W16-2380,W15-3041,0,0.0173432,"mation Amal Abdelsalam∗ , Ondˇrej Bojar∗∗ , Samhaa El-Beltagy∗ ∗ Nile University in Egypt, Center of Informatics Science, Text Mining Research Group am.mahmoud@nu.edu.eg, samhaa@computer.org ∗∗ Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics bojar@ufal.mff.cuni.cz Abstract Sentence-level QE is the most popular track in the WMT QE shared task, due to its presence in all editions of the task since the beginning. Many features have been explored by participating systems, including lexical, syntactic, semantic, embeddingbased features (Shah et al., 2015), as well as features dependent on any details the particular MT systems may provide (Soricut et al., 2012; Camargo de Souza et al., 2013). In our model, we try to exploit the power of bilingual distributed representations combined with word alignment information to boost the performance of translation quality estimation. For this purpose, we use the implementation provided by the Multivec tool (B´erard et al., 2016) for the bilingual distributed representation model, described by Luong et al. (2015) and the GIZA++ word alignment model (Och and Ney, 2003). The rest of this paper is organized a"
W16-2380,D13-1141,0,0.0182079,"rs of ws ; Predict neighbors of wt ; Use ws to predict neighbors of wt ; Use wt to predict neighbors of ws ; end end Algorithm 1: BiSkip learning algorithm by Luong et al. (2015) Bilingual Mapping , where word representations are trained for each language independently and a linear mapping is then learned to transform representations from one language to another (Mikolov et al., 2013a). Monolingual Adaptation relies on pre-trained embeddings of the source language when learning target representations. A bilingual constraint (such as unsupervised word alignments derived from a parallel corpus; Zou et al., 2013) ensures that semantically similar words across languages end up with embeddings similar in the learned vector space. 3 For cross-lingual semantic similarity, a word alignment model is an important component. According to the evaluation of the semantic textual similarity task in SemEval 2015, the best performing systems in both the English and Spanish subtasks relied mainly on word alignment techniques (Sultan et al., 2015; H¨anig et al., 2015). Inspired by these results, we add features based on word alignment to the QE system. According to Specia et al. (2015), alignmentbased features are us"
W16-2380,2006.amta-papers.25,0,0.024419,"rongly matched tags represents the number of source words that are aligned to target words with a different POS tag. 5 Since the alignments are needed for the source and candidate translation, they come from Run-2. QEcorpus (our name) denotes the EnglishGerman corpus released by the WMT16 QE task organizers. It is the first time when this language pair appears in the segmentlevel QE. QEcorpus consist of 15k source sentences in the IT domain, divided into 12k training, 1k development and 2k testing segments. Source sentences are provided with their machine translations, post-editions and HTER (Snover et al., 2006) as post-editing effort scores. 4.4 Our experiments use the following corpora: Post-Edited N -grams As mentioned earlier, in quality estimation, there is no access to reference translation. However, the QE task organizers provided the participants with training data (called “QEcorpus” in Section 5) consisting of 12k training segments and 1k development segments machine-translated and manually post-edited. To benefit from this valuable resource, we introduce another set of features representing the most frequent bigrams in translation text that were changed through the post-editing. The list of"
W16-2380,W12-3118,0,0.0252062,"ormatics Science, Text Mining Research Group am.mahmoud@nu.edu.eg, samhaa@computer.org ∗∗ Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics bojar@ufal.mff.cuni.cz Abstract Sentence-level QE is the most popular track in the WMT QE shared task, due to its presence in all editions of the task since the beginning. Many features have been explored by participating systems, including lexical, syntactic, semantic, embeddingbased features (Shah et al., 2015), as well as features dependent on any details the particular MT systems may provide (Soricut et al., 2012; Camargo de Souza et al., 2013). In our model, we try to exploit the power of bilingual distributed representations combined with word alignment information to boost the performance of translation quality estimation. For this purpose, we use the implementation provided by the Multivec tool (B´erard et al., 2016) for the bilingual distributed representation model, described by Luong et al. (2015) and the GIZA++ word alignment model (Och and Ney, 2003). The rest of this paper is organized as follows. In Sections 2 and 3, we give an overview of the bilingual distributional model and word alignme"
W16-2380,P15-4020,0,0.280216,"ts derived from a parallel corpus; Zou et al., 2013) ensures that semantically similar words across languages end up with embeddings similar in the learned vector space. 3 For cross-lingual semantic similarity, a word alignment model is an important component. According to the evaluation of the semantic textual similarity task in SemEval 2015, the best performing systems in both the English and Spanish subtasks relied mainly on word alignment techniques (Sultan et al., 2015; H¨anig et al., 2015). Inspired by these results, we add features based on word alignment to the QE system. According to Specia et al. (2015), alignmentbased features are used for word-level QE only and there is no alignment-based features included in the baseline feature set for sentence-level QE. We use GIZA++ (Och and Ney, 2003) to obtain the alignments. By default, GIZA++ alignments are not symmetric. We symmetrize them by taking the intersection of the two directions, leading to high-precision alignments. For pre-processing, we lowercase and stem words (naively taking just the first four letters) on both sides of the input. Some of our features rely on the alignments of our training data (the ITcorpus and the training part of"
W16-2380,J93-2003,0,\N,Missing
W16-2380,W15-3035,0,\N,Missing
W16-2380,L16-1662,0,\N,Missing
W16-3706,P08-1087,0,0.0248375,"n target side are discussed below: The method of source pseudo-words insertion to generate the target words is not novel. We build upon the work of Kamran (2011) who exploited the use of pseudo-words for generating the target case markers for the English-Urdu language pair. Kamran (2011) used preliminary set of linguistic rules to add case markers for subject, object, indirect object and additionally for verb auxiliaries. We refine the oversimplified linguistic rules for adding pseudo-words by first identifying the various syntactic and morphological features such as transitivity and animacy. Avramidis and Koehn (2008) model case agreement phenomenon for English-to-Greek by adding case information as factor on source side. This approach uses source CFG parses to identify the grammatical roles of words, whereas we use the dependency parses. Also, due to the fact that Greek noun inflections depend on their role, information is added in the form of factors, whereas we use the single-factored setup with the assumption that pseudo-words will play a role in the selection of the correct noun forms. Goldwater and McClosky (2005) aim at overcoming the data sparseness issue by increasing the similarity between langua"
W16-3706,W07-0702,0,0.0357542,"eek noun inflections depend on their role, information is added in the form of factors, whereas we use the single-factored setup with the assumption that pseudo-words will play a role in the selection of the correct noun forms. Goldwater and McClosky (2005) aim at overcoming the data sparseness issue by increasing the similarity between languages using source morphological analysis for Czech-to-English MT. In this approach, the source input is first lemmatized and then extra tokens are added for the information that is stripped off during the lemmatization process, such as for negation words. Birch et al. (2007) have shown the use of Combinatorial Categorial Grammar (CCG) supertags on source sentence, for German-to-English translation, in an attempt to capture the syntactic structure of the source language in factored SMT models. Recently, Dungarwal et al. (2014) have used CCG supertags as an additional factor on source for English-to-Hindi SMT system. 3 Enriching Source 3.1 Stanford Parser Stanford parser1 is a toolkit that contains java implementation for both probabilistic contextfree grammar (PCFG) and dependency parsers. The dependency parser extracts the typed dependency parse (de Marneffe et a"
W16-3706,de-marneffe-etal-2006-generating,0,0.140154,"Missing"
W16-3706,W14-3308,0,0.0197067,"2005) aim at overcoming the data sparseness issue by increasing the similarity between languages using source morphological analysis for Czech-to-English MT. In this approach, the source input is first lemmatized and then extra tokens are added for the information that is stripped off during the lemmatization process, such as for negation words. Birch et al. (2007) have shown the use of Combinatorial Categorial Grammar (CCG) supertags on source sentence, for German-to-English translation, in an attempt to capture the syntactic structure of the source language in factored SMT models. Recently, Dungarwal et al. (2014) have used CCG supertags as an additional factor on source for English-to-Hindi SMT system. 3 Enriching Source 3.1 Stanford Parser Stanford parser1 is a toolkit that contains java implementation for both probabilistic contextfree grammar (PCFG) and dependency parsers. The dependency parser extracts the typed dependency parse (de Marneffe et al., 2006) using the phrase structure parse of the sentence. Typed dependencies – such as subject, direct object etc – represent the grammatical relations between the individual words. The Stanford dependencies are represented as triplets consist of the nam"
W16-3706,H05-1085,0,0.0538892,"tifying the various syntactic and morphological features such as transitivity and animacy. Avramidis and Koehn (2008) model case agreement phenomenon for English-to-Greek by adding case information as factor on source side. This approach uses source CFG parses to identify the grammatical roles of words, whereas we use the dependency parses. Also, due to the fact that Greek noun inflections depend on their role, information is added in the form of factors, whereas we use the single-factored setup with the assumption that pseudo-words will play a role in the selection of the correct noun forms. Goldwater and McClosky (2005) aim at overcoming the data sparseness issue by increasing the similarity between languages using source morphological analysis for Czech-to-English MT. In this approach, the source input is first lemmatized and then extra tokens are added for the information that is stripped off during the lemmatization process, such as for negation words. Birch et al. (2007) have shown the use of Combinatorial Categorial Grammar (CCG) supertags on source sentence, for German-to-English translation, in an attempt to capture the syntactic structure of the source language in factored SMT models. Recently, Dunga"
W16-3706,W14-5505,1,0.661996,"rcased) are used. The lexicalized word-based reordering model (Koehn et al., 2005) is trained using msd orientation in both forward and backward direction, with model conditioned on both the source and the target languages (msd-bidirectional-fe). 3 http://nlp.stanford.edu:8080/parser/index.jsp http://ufal.mff.cuni.cz/treex/ 5 The tokenization script can be downloaded from: http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5 6 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_parser.php 4 56 The parallel and monolingual data is summarized in Table 2. The parallel data reported in Jawaid et al. (2014a) (called “ALL”) is used for training, development and test with the similar data splits. Jawaid et al. (2014b) released large plain and annotated Urdu monolingual data from mix of several domains. The plain text monolingual data is used to build the language model. Parallel Mono Dataset Train Dev Test - Sents (en/ur) 74.9k 2K 2K 5.4M Tokens (en/ur) 1.5M/1.7M 41.5K/45.2K 41.8K/45.6K 95.4M Table 2: Summary of training data. Final BLEU scores (Papineni et al., 2002) are reported on the test set called “PTEST” in the following and also on the three independent official test sets briefly explaine"
W16-3706,jawaid-etal-2014-tagged,1,0.837255,"rcased) are used. The lexicalized word-based reordering model (Koehn et al., 2005) is trained using msd orientation in both forward and backward direction, with model conditioned on both the source and the target languages (msd-bidirectional-fe). 3 http://nlp.stanford.edu:8080/parser/index.jsp http://ufal.mff.cuni.cz/treex/ 5 The tokenization script can be downloaded from: http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5 6 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_parser.php 4 56 The parallel and monolingual data is summarized in Table 2. The parallel data reported in Jawaid et al. (2014a) (called “ALL”) is used for training, development and test with the similar data splits. Jawaid et al. (2014b) released large plain and annotated Urdu monolingual data from mix of several domains. The plain text monolingual data is used to build the language model. Parallel Mono Dataset Train Dev Test - Sents (en/ur) 74.9k 2K 2K 5.4M Tokens (en/ur) 1.5M/1.7M 41.5K/45.2K 41.8K/45.6K 95.4M Table 2: Summary of training data. Final BLEU scores (Papineni et al., 2002) are reported on the test set called “PTEST” in the following and also on the three independent official test sets briefly explaine"
W16-3706,2005.iwslt-1.8,0,0.019636,"using the Treex platform (Popel and Žabokrtský, 2010)4 , which included tokenization and lemmatization. The target side of the corpus is tokenized using a simple tokenization script5 by Dan Zeman and it is lemmatized using the Urdu Shallow Parser6 developed by Language Technologies Research Center of IIIT Hyderabad. The alignments are learnt from the lemmatized version of the corpus. For the rest of the SMT pipeline, word forms (i.e. no morphological decomposition) in their true case (i.e. names capitalized but sentence starts lowercased) are used. The lexicalized word-based reordering model (Koehn et al., 2005) is trained using msd orientation in both forward and backward direction, with model conditioned on both the source and the target languages (msd-bidirectional-fe). 3 http://nlp.stanford.edu:8080/parser/index.jsp http://ufal.mff.cuni.cz/treex/ 5 The tokenization script can be downloaded from: http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5 6 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_parser.php 4 56 The parallel and monolingual data is summarized in Table 2. The parallel data reported in Jawaid et al. (2014a) (called “ALL”) is used for training, development and test wi"
W16-3706,P14-5010,0,0.00409485,"d Parser Stanford parser1 is a toolkit that contains java implementation for both probabilistic contextfree grammar (PCFG) and dependency parsers. The dependency parser extracts the typed dependency parse (de Marneffe et al., 2006) using the phrase structure parse of the sentence. Typed dependencies – such as subject, direct object etc – represent the grammatical relations between the individual words. The Stanford dependencies are represented as triplets consist of the name of the dependency relation, the dependent and the governor (also known as the “head”). The Stanford CoreNLP framework2 (Manning et al., 2014) is used for applying the NLP pipeline on the input sentence. The framework uses “annotators” for linguistic processing of input text. We use following annotators to process a sentence: tokenize, ssplit, pos, lemma, ner, parse and dcoref. Additionally, we set splitting of sentence (ssplit) to one sentence per input and tokenization is restricted to white space only. 1 2 http://nlp.stanford.edu/software/lex-parser.shtml http://nlp.stanford.edu/software/corenlp.shtml 55 Stanford CoreNLP provides the dependency parse in three graphical representations: basic, collapsed and cc-processed (collapsed"
W16-3706,C00-2163,0,0.107816,"ccusative ko obj Dative ko subj/ind. obj Instrumental se subj/obl/adjunct Genitive ksubj/specifier Locative m˜ e/par/tak/ϕ obl/adjunct Table 1: Case Markers in Urdu Absence of marker with subject or object roles marks the nominative case, while accusative and dative share the marker “ko”. Due to the fact that nominative lacks the marker, we only add pseudo-words for ergative, accusative and dative markers. Rest of the three cases are not considered in this work. 4 Common Settings For the training of our translation system, the standard training pipeline of Moses is used along with the GIZA++ (Och and Ney, 2000) alignment toolkit and a 5-gram SRILM language model (Stolcke, 2002). The source texts were processed using the Treex platform (Popel and Žabokrtský, 2010)4 , which included tokenization and lemmatization. The target side of the corpus is tokenized using a simple tokenization script5 by Dan Zeman and it is lemmatized using the Urdu Shallow Parser6 developed by Language Technologies Research Center of IIIT Hyderabad. The alignments are learnt from the lemmatized version of the corpus. For the rest of the SMT pipeline, word forms (i.e. no morphological decomposition) in their true case (i.e. nam"
W16-3706,P02-1040,0,0.0983966,"?filename=downloads/shallow_parser.php 4 56 The parallel and monolingual data is summarized in Table 2. The parallel data reported in Jawaid et al. (2014a) (called “ALL”) is used for training, development and test with the similar data splits. Jawaid et al. (2014b) released large plain and annotated Urdu monolingual data from mix of several domains. The plain text monolingual data is used to build the language model. Parallel Mono Dataset Train Dev Test - Sents (en/ur) 74.9k 2K 2K 5.4M Tokens (en/ur) 1.5M/1.7M 41.5K/45.2K 41.8K/45.6K 95.4M Table 2: Summary of training data. Final BLEU scores (Papineni et al., 2002) are reported on the test set called “PTEST” in the following and also on the three independent official test sets briefly explained by Jawaid et al. (2014a). 5 Experiments The experiments are conducted with the insertion of pseudo-words on the un-preordered source side as well as after preordering the source corpus. For preordering of the English corpus, we use the transformation module of Jawaid and Zeman (2011) that utilizes the Stanford PCFG parse trees to first parse the input sentences and afterwards applies the hand-written rules to transform the English sentences to closely match the s"
W16-4506,W15-1006,0,0.0619694,"ach, but in manual evaluation, both WSD methods bring improvements. 1 Introduction The possibility of using word sense disambiguation (WSD) systems in machine translation (MT) has recently been investigated in several ways: Output of WSD systems has been incorporated into MT to improve translation quality — at the decoding step of a phrase-based statistical machine translation (PBSMT) system (Chan et al., 2007) or as contextual features in maximum entropy (MaxEnt) models (Neale et al., 2015) and (Neale et al., 2016). In addition, WSD has also been used in MT evaluation, for example in METEOR (Apidianaki et al., 2015). These works indicate that WSD can be beneficial to different MT tasks, in case of using senses as contextual features for MaxEnt models Neale et al. (2016) achieve statistically significant improvement over the baseline for English-to-Portuguese translation. And Apidianaki et al. (2015) report that usage of WSD can establish better sense correspondences and improve its correlation with human judgments of translation quality. In this research, we have investigated the possibilities of integrating two different approaches to verbal WSD into a PB-SMT system – verb patterns based on corpus patte"
W16-4506,W07-0735,1,0.766205,"• Form+Sense→Form – two source factors (surface word form and verb sense ID, if applicable) are translated to the target-side word forms. This is technically identical to appending the verb sense ID to the source words. • Form→Form+Tag – the source word form is translated to two factors on the target side: word form and morphological tag (part-of-speech tag with morphological categories of Czech, such as case, number, gender, or tense). This allows us to use an additional language model trained on morphological tags only. This setup is known to perform well for morphologically rich languages (Bojar, 2007) and thus was selected as a baseline for all comparisons. • Form+Sense→Form+Tag – a combination of the two setups above: two source and two target-side factors, for better handling of source verb meaning and target morphological coherence. • Form→Form+Tag + Form+Sense→Form+Tag – a combination of previous two models as two separate phrase tables. For all configurations, we trained a 4-gram language model on word forms of the sentences from the training set. This LM was pruned: we discarded all singleton n-grams (apart from unigrams). In addition, for configurations which generated morphological"
W16-4506,cinkova-etal-2012-database,1,0.873109,"Missing"
W16-4506,cinkova-2006-propbank,0,0.0449843,"Missing"
W16-4506,P11-2031,0,0.0327178,"Missing"
W16-4506,W15-2111,1,0.895548,"Missing"
W16-4506,hajic-etal-2012-announcing,1,0.881793,"Missing"
W16-4506,C12-1073,1,0.903554,"Missing"
W16-4506,D07-1091,0,0.027397,"meaning of the sentence (Healy and Miller, 1970) and thus accurate translation of the verb is critical for the understanding of the translation. Therefore, improvement of the translation of verbs can lead to overall increase of the translation quality. Therefore, improvement of the translation of verbs can lead to an overall increase of translation quality. The outputs of automatic verb sense disambiguation systems using both CPA and valency frames were integrated into Moses statistical machine translation system(Koehn et al., 2007). Both kinds of verb senses were added as additional factors (Koehn and Hoang, 2007). Section 4.1 shows that we obtain statistically significant improvement in terms of BLEU scores (Papineni et al., 2002) and manual evaluation of translations validated that. The novelty of this work lies not only in our focus only on verbs senses, but also in the fact that we are comparing the impact of two WSD approaches on the statistical machine translation. The following Section 2 describes the initial setup of our experiments. Section 3 and Section 4 depict the idea behind corpus pattern analysis and verb valency frames representations and show evaluation results of incorporation of thes"
W16-4506,P07-2045,1,0.0305017,"focus on verbs was motivated by the ideas that verbs carry a crucial part of the meaning of the sentence (Healy and Miller, 1970) and thus accurate translation of the verb is critical for the understanding of the translation. Therefore, improvement of the translation of verbs can lead to overall increase of the translation quality. Therefore, improvement of the translation of verbs can lead to an overall increase of translation quality. The outputs of automatic verb sense disambiguation systems using both CPA and valency frames were integrated into Moses statistical machine translation system(Koehn et al., 2007). Both kinds of verb senses were added as additional factors (Koehn and Hoang, 2007). Section 4.1 shows that we obtain statistically significant improvement in terms of BLEU scores (Papineni et al., 2002) and manual evaluation of translations validated that. The novelty of this work lies not only in our focus only on verbs senses, but also in the fact that we are comparing the impact of two WSD approaches on the statistical machine translation. The following Section 2 describes the initial setup of our experiments. Section 3 and Section 4 depict the idea behind corpus pattern analysis and verb"
W16-4506,W04-3250,0,0.159584,"orm+Tag Avg 25.0 24.9 25.0 22.6 22.5 22.6 62.2 62.4 62.2 ssel 0.9 0.9 0.9 0.4 0.4 0.4 0.7 0.7 0.7 sTest 0.1 0.1 0.1 0.0 0.0 0.1 0.2 0.1 0.2 p-value 0.00 0.16 0.00 0.22 0.00 0.61 Table 4: Multeval results for corpus pattern analysis, based on 36 MERT runs We also performed a more detailed analysis with pairwise comparisons of the following configurations: • Form→Form vs. Form+Sense→Form • Form→Form+Tag vs. Form+Sense→Form+Tag • Form→Form+Tag vs. Form→Form+Tag + Form+Sense→Form+Tag 3.1.1 Form→Form vs. Form+Sense→Form The comparison provided by MT-ComparEval based on paired bootstrap resampling (Koehn, 2004) of best MERT runs for both configurations showed that Form→Form is significantly better (p-value=0.022) than Form+Sense→Form. The sentence-by-sentence comparison explains this: On the positive side, 8 examples out of the top 10 sentences where Form+Sense→Form output was better than Form→Form profited from using additional information about the verb sense. On the negative side, the model with verb senses made a lot of errors due to badly extracted phrase tables, even leaving some verbs untranslated. 3.1.2 Form→Form+Tag vs. Form+Sense→Form+Tag In this case the same paired bootstrap resampling o"
W16-4506,W15-5708,0,0.0317026,"s show a statistically significant translation quality improvement in terms of the BLEU metric for the valency frames approach, but in manual evaluation, both WSD methods bring improvements. 1 Introduction The possibility of using word sense disambiguation (WSD) systems in machine translation (MT) has recently been investigated in several ways: Output of WSD systems has been incorporated into MT to improve translation quality — at the decoding step of a phrase-based statistical machine translation (PBSMT) system (Chan et al., 2007) or as contextual features in maximum entropy (MaxEnt) models (Neale et al., 2015) and (Neale et al., 2016). In addition, WSD has also been used in MT evaluation, for example in METEOR (Apidianaki et al., 2015). These works indicate that WSD can be beneficial to different MT tasks, in case of using senses as contextual features for MaxEnt models Neale et al. (2016) achieve statistically significant improvement over the baseline for English-to-Portuguese translation. And Apidianaki et al. (2015) report that usage of WSD can establish better sense correspondences and improve its correlation with human judgments of translation quality. In this research, we have investigated th"
W16-4506,P03-1021,0,0.0100485,"respective numbers of sentences and tokens in each of training, development and test sets are shown in Table 1. For our experiments, 28 different English verbs were selected and automatically annotated with corpus pattern analysis senses, and 3,306 verbs annotated using valency frames. The subset has been selected to include verbs annotated with CPA, so the effect of WSD would be visible. All the experiments were carried out in the Eman experiment management system (Bojar and Tamchyna, 2013) using the Moses PB-SMT system (Koehn et al., 2007) as the core and minimum error rate training (MERT, (Och, 2003)) to optimize the decoder feature weights on the development set. The evaluation was performed using the BLEU score (Papineni et al., 2002), but the results of each setup were then thoroughly examined and verified using the MT-ComparEval system (Aranberri et al., 2016)1 . Set Training Development Test Number of sentences 649,605 10,115 2,707 Tokens CS 10,759,546 187,478 59,446 Tokens EN 12,073,130 167,788 67,336 Table 1: Data set composition 2.2 MT configurations As we have mentioned in Section 1 the main goal of the experiments was to explore whether verb senses as additional factors in the s"
W16-4506,J05-1004,0,0.0598681,"Missing"
W16-4506,P02-1040,0,0.100568,"ding of the translation. Therefore, improvement of the translation of verbs can lead to overall increase of the translation quality. Therefore, improvement of the translation of verbs can lead to an overall increase of translation quality. The outputs of automatic verb sense disambiguation systems using both CPA and valency frames were integrated into Moses statistical machine translation system(Koehn et al., 2007). Both kinds of verb senses were added as additional factors (Koehn and Hoang, 2007). Section 4.1 shows that we obtain statistically significant improvement in terms of BLEU scores (Papineni et al., 2002) and manual evaluation of translations validated that. The novelty of this work lies not only in our focus only on verbs senses, but also in the fact that we are comparing the impact of two WSD approaches on the statistical machine translation. The following Section 2 describes the initial setup of our experiments. Section 3 and Section 4 depict the idea behind corpus pattern analysis and verb valency frames representations and show evaluation results of incorporation of these sense to phrase-based statistical machine translation. The next section (Section 5) is devoted to the discussion of re"
W16-4506,L16-1296,0,\N,Missing
W16-6401,W10-1705,1,0.929434,"oses + Moses post-editing, simple Moses + Moses post-editing, TwoStep Google Translate + TectoMT post-editing Moses + TectoMT post-editing § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setup"
W16-6401,W15-3006,1,0.779365,"nslation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been"
W16-6401,W12-3130,1,0.860797,"s, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd Deep Machine Translation Workshop (DMTW 2016), pages 1–10, Lisbon, Portugal, 21 October 2016. Figure 1: TectoMT 2.1.1 Factored Moses In the more recent experiments that we report, the Moses system used is actually the Factored Moses of Bojar et al. (2012). It translates the source English text into a factored representation of Czech, where each word is represented by a tuple of a word form and a corresponding part-of-speech (PoS) tag. This enables Moses to use an additional language model which operates on PoS tags instead of word forms. This helps overcome data sparsity issues of the word-based language model and thus leads to a higher output quality, especially to its better grammaticality. Factored Moses is trained on parallel corpora pre-analyzed by Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically"
W16-6401,W13-2208,1,0.933672,"to perform forced translation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at"
W16-6401,W15-3009,1,0.853769,"y Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically motivated NLP framework. It consists of a large number of smaller components performing a specific NLP-task (blocks), both Treex-specific as well as Treex-wrapped external tools, which can be flexibly combined into processing pipelines. Sentences are represented by surface and deep syntactic dependency trees, richly annotated with numerous linguistic attributes, similarly to the Prague Dependency Treebank (Hajiˇc, 1998). 2.2.1 TectoMT The main application of Treex is TectoMT3 (Žabokrtský et al., 2008; Dušek et al., 2015), a linguistically motivated hybrid machine translation system. Its pipieline consists of three main steps: analysis of each source sentence up to t-layer (a deep syntactic representation of the sentence in a labelled dependency t-tree), transfer of the source t-tree to the target t-tree (i.e., the translation per se), and generation of the target sentence from the target t-tree (see Figure 1). The transfer is performed by copying the t-tree structure and grammatemes4 (attributes describing grammatical meaning) from source, and predicting target lemmas and formemes5 (deep morphosyntactic attri"
W16-6401,W12-3132,1,0.90614,"Missing"
W16-6401,W13-2216,1,0.89877,"Missing"
W16-6401,P07-2045,1,0.0127177,"rtcomings cancel out. In our paper, we review a set of such attempts, performed with Moses, a prominent representative of the PB-SMT systems, and Treex, a linguistically motivated NLP framework, featuring, among other, a full-fledged deep syntactic MT system, TectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to othe"
W16-6401,W09-0424,0,0.0237791,"u-plain Moses output downloaded from http://matrix.statmt.org/systems/show/2807, test set downloaded from http://matrix.statmt.org/test_sets/list. 10 Figure 8: Moses with TectoMT pre- and post-processing 6 Figure 9: Two-headed Chimera Zeman (2010) used several pre-processing steps to make the source English text more similar to Czech, such as removing articles, marking subjects by artificial suffixes (“/Sb”), and reordering auxiliary verbs to neighbor their main verbs. Of course, the SMT system was also trained on texts preprocessed in that way; in these experiments, the Joshua PB-SMT system (Li et al., 2009) was used instead of Moses. This approach may seem too aggressive, prone to making the input noisier as well as being potentially lossy. However, the author showed that with careful selection and tuning of the pre-processing steps, a significant improvement of translation quality can be achieved; moreover, this was also confirmed on English-to-Hindi translation. Rosa et al. (2016) successfully apply Treex pre-processing and post-processing to Moses, but this time with the main objective being an adaptation of Moses trained on general-domain data to a specific domain (namely the domain of Infor"
W16-6401,J03-1002,0,0.0116228,"ectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd De"
W16-6401,W07-0704,0,0.0827717,"Missing"
W16-6401,W12-3146,1,0.888935,"Missing"
W16-6401,W16-2334,1,0.910577,"ng § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setups have been properly described and evaluated in a peer-reviewed publication, others, especially some of the unsuccessful ones, were ne"
W16-6401,N07-1064,0,0.0327481,"Majliš (2009) Zeman (2010) Table 2: Base systems. Figure 3: PhraseFix translating one t-node with two or more t-nodes or deleting some t-nodes.8 It also uses MERT tuning and it should scale with more training data. In the experiments with two factors (Popel, 2013), two language models were used: one for lemmas and one for formemes. Unfortunately, the TectoMoses experiment brought negative results, presumably due to additional noise introduced by the added transformations. 3.2 PhraseFix: TectoMT with Moses Post-editing The PhraseFix system of Galušˇcáková et al. (2013) is based on the work of Simard et al. (2007), who introduced the idea of automatically post-editing a first-stage MT system by a second-stage MT system, trained to “translate” the output of the first-stage system into a reference translation. This has been shown to be particularly beneficial for conceptually different MT systems. In PhraseFix, the source English side of the CzEng parallel corpus of Bojar and Žabokrtský (2009) is translated by TectoMT into Czech, and Moses is then trained in a monolingual setting to translate the TectoMT-Czech into reference-Czech, i.e., the target side of CzEng (see Figure 3). Evaluation shows that this"
W16-6401,W07-1709,0,0.0726314,"Missing"
W16-6401,W16-2325,1,0.836879,"parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been shown to have a positive effect, e.g., in choosing the correct inflection of a word when the language model encounters an unknown context, or in generating a translation for a word that constitutes an out-of-vocabulary item for Moses (as TectoMT can abstract from word forms to lemmas and beyond, which Moses cannot). 3.8 Chimera: Moses with Additional TectoMT Phrase-table and Depfix Post-editing The Three-headed Chimera, or simply Chimera (Bojar et al., 2013b; Tamchyna et al., 2016), is a combination of TectoMT and Moses, as in Section 3.7, complemented by a final post-editing step performed by Depfix, as in Section 3.5 (see Figure 10). It has been repeatedly confirmed as the best system by both automatic and manual evaluations, not only among the ones reported in this paper, but also in general, 12 http://www.statmt.org/moses/?n=Advanced.Hybrid 7 Figure 10: Three-headed Chimera being the winner of the WMT English-to-Czech translation task in the years 2013, 2014 and 2015 (Bojar et al., 2013a; Bojar et al., 2014; Bojar et al., 2015). 4 Conclusion We reviewed a range of e"
W16-6401,W08-0325,0,0.0651825,"Missing"
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W17-4719,L16-1470,1,0.868974,"Missing"
W17-4719,C16-2064,0,0.0199749,"ach language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectur"
W17-4719,W16-4616,0,0.0207529,"training settings for each language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use o"
W17-4719,W17-4754,0,0.123112,"Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domai"
W17-4719,federmann-2010-appraise,0,0.0291939,"run2 LMU PJIIT run1 PJIIT run2 PJIIT run3 uedin-nmt run1 uedin-nmt run2 UHH run1 UHH run2 UHH run3 cs 15.93* 22.79* - de 20.45* 27.57* 26.79 29.46* 21.88* 33.06* 18.71 19.80 19.66* fr 22.99* 31.79 31.89 33.36* pl 14.09* 14.32 10.75 14.34* 23.15* 19.87 - es 40.97 41.20 41.22* ro 10.56* 18.10* 29.32* 27.32 - Table 7: Results for the NHS test sets. * indicates the primary run as informed by the participants. native speakers of the languages and were either members of the participating teams or colleagues from the research community. The validation task was carried out using the Appraise tool15 (Federmann, 2010). For each pairwise comparison, we validated a total of 100 randomly-chosen sentence pairs. The validation consisted of reading the two sentences (A and B), i.e., translations from two systems or from the reference, and choosing one of the options below: The manual validation for the Scielo test sets is presented in Table 8, for the comparison of the only participating team (UHH) to the reference translation. For en2es, the automatic translation scored lower than the reference one in 53 out of 100 pairs, but could still beat the reference translation in 23 pairs. For en2pt, the automatic trans"
W17-4719,W17-4730,0,0.0242129,"uilt BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectures, layer normalization, and more compact models due to weight-tying and improvements in BPE segmentations. Lilt (Lilt Inc.). The system from the Lilt Inc.13 uses an in-house implementation of a sequenceto-sequence model with Bahdanau-style attention. The final submissions are ensembles between models fine-tuned on different parts of the available data. LMU (Ludwig Maximilian University of Munich). LMU Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokeni"
W17-4719,W16-2337,0,0.097915,". All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domain data14 and out-ofdomain data. However, we did not perform SOUL re-scoring. and Kneser-Ney discounting were used to estimate 5-gram language models (LM). For word alignment, GIZA++ with the default grow-diag-finaland alignment symmetrization method was used. Tuning of the SMT systems was performed with MERT. Commoncrawl and Wikipedia were used as general domain data for all language pairs except for EN/PT, where no Commoncrawl data was provided by WMT. As for the in-domain corpo"
W17-4719,W17-4739,1,\N,Missing
W17-4719,W17-4743,0,\N,Missing
W17-4720,P11-2031,0,0.0359814,"or the future to be able to draw better conclusions. 5.3 • Neural Monkey – the output of the system described in Section 4.2 using greedy decoding, • Neural Monkey 1 – decoding with beam search of 50 and taking only the first candidate translation to the phrase table, • Neural Monkey 50 – decoding with beam search of 50 and taking all 50 candidate translations to the phrase table, All combinations we have experimented with are shown in Table 4. The last column “Average BLEU” was calculated the same way as it was done in Section 4.3. Also the same 5 MERT runs were used for MultEval evaluation (Clark et al., 2011). Basically, Table 4 confirms the well-know saying “more data helps”. Using translations from different systems as additional phrase tables gave on average a 2.5 BLEU score boost, if we compare rows 1 or 2 and row 14. We also see that using more than three phrase tables might lead to a lower BLEU score: Consider the system in the row 7 with four separate phrase tables (Avg. BLEU 23.7) and the system in the row 3 where three of the tables were first merged into one (Avg. BLEU 23.9). Moreover, Multeval comparison showed no significant difference between systems from rows 7 and 8, despite the eff"
W17-4720,P13-2071,0,0.0171315,"he standard encoder-decoder architecture with attention as preposed by Bahdanau et al. (2015). (Attempts to combine MT systems with Neural Monkey are described in Section 5.2 below.) We use the following model parameters which fit into 8GB GPU memory of NVIDIA GeForce GTX 1080. The encoder uses embeddings of size 600 and the hidden state of size 600. • Synthetic phrase table extracted from the main training data, ie. either or both of NematusNews and MosesNews as listed in Table 1. • In-domain phrase table extracted from either or both of XenCNews and XenCMonoNews. • Operation Sequence Model (Durrani et al., 2013) trained on the NematusNews corpus. 8 While dropout is useful for small datasets, Sennrich et al. (2016a) observed no gain from dropout with 8M training sentence pairs. Our training data is more than 7× larger. 9 In contrast to what Tu et al. (2017, Table 1) observe for other implementations of the Bahdanau et al. (2015) model, Neural Monkey does not exhibit degradation of the quality of the top candidate with increasing beam size. We have thus no reason to keep beam size as small as usual. 6 http://data.statmt.org/rsennrich/ wmt16_systems 7 http://ufal.mff.cuni.cz/neuralmonkey 250 Phrase Tabl"
W17-4720,P07-2045,1,0.0105599,"ased on a simple perplexity computation utilizing only one side of the corpora so that monolingual corpora are sufficient and the second mode is based on the bilingual crossentropy difference as described by Axelrod et al. (2011). We took two different corpora as our in-domain data: 334 322k 103 193k 55k 67k Table 1: Datasets 3.1 Back-Translated Data To create back-translated data, we used the CzEng 1.6 Czech-English parallel corpus (Bojar et al., 2016) and the Czech News Crawl articles released for WMT20171 (called “mononews” for short). We used two different back-translation systems: Moses (Koehn et al., 2007) trained by ourselves, and Marian2 (known as AmuNMT before it included NMT training; Junczys-Dowmunt et al., 2016) using the pretrained Nematus (Sennrich et al., 2017) models3 from WMT16 News Task.4 We used only the non-ensembled left-to-right run (i.e. no right-to-left rescoring as done by Sennrich et al., 2016a) with beam size of 5,5 taking just the single-best output. The Moses-based system used only a single phrase table translating from word form to word forms and twelve 10-gram language models built on individual years of English mononews. We took all Czech mononews corpora available thi"
W17-4720,D11-1033,0,0.0298072,"MT16 submission (Tamchyna et al., 2016). Instead, we used the XenC toolkit (Rousseau, 2013) to extract domain-specific data from the whole corpus (referred to as “out-of-domain”, in the following). We used two modes of XenC. Both of these modes estimate two language models from in-domain and out-of-domain corpora, using SRILM toolkit (Stolcke, 2002). The first mode is a filtering process based on a simple perplexity computation utilizing only one side of the corpora so that monolingual corpora are sufficient and the second mode is based on the bilingual crossentropy difference as described by Axelrod et al. (2011). We took two different corpora as our in-domain data: 334 322k 103 193k 55k 67k Table 1: Datasets 3.1 Back-Translated Data To create back-translated data, we used the CzEng 1.6 Czech-English parallel corpus (Bojar et al., 2016) and the Czech News Crawl articles released for WMT20171 (called “mononews” for short). We used two different back-translation systems: Moses (Koehn et al., 2007) trained by ourselves, and Marian2 (known as AmuNMT before it included NMT training; Junczys-Dowmunt et al., 2016) using the pretrained Nematus (Sennrich et al., 2017) models3 from WMT16 News Task.4 We used onl"
W17-4720,Q17-1026,0,0.0588988,"Missing"
W17-4720,P17-2031,0,0.0633694,"Missing"
W17-4720,P14-5003,0,0.143231,"Missing"
W17-4720,W16-2325,1,0.900635,"1. Use available monolingual data and last year’s systems to prepare a synthetic parallel corpus using “back translation” (Section 3). 2. Train “individual forward systems” on this synthetic corpus (Section 4). Introduction 3. Apply individual forward systems to the source side of the genuine parallel data. The paper describes CUNI submissions for English-to-Czech WMT 2017 News Translation Task. We experimented with several neural machine translation (NMT) systems and we further developed our phrase-based statistical machine translation system Chimera, which was our primary system last year (Tamchyna et al., 2016). This year, we planned our setup in a way that would allow us to experiment with neural system combination. To this end, we reserved the provided English-Czech parallel data for the training of the system combination and trained our “individual forward systems” on almost only synthetic data. The structure of the paper is the following. In Section 2, we provide an overview of the relatively complex setup. Section 3 details how the training data for all the systems were prepared, including the description of MT systems used for backtranslation. Section 4 is devoted to our individual forward tra"
W17-4720,C16-1172,0,0.0483656,"Missing"
W17-4720,P03-1021,0,0.00938203,"ce side of the development and test sets. to 20M sentence pairs instead of 59M synthetic sentences. Selecting the genuine parallel sentences both bilingually and monolingually (XenCNews) works usually better than selecting them only monolingually (XenCMonoNews), but there is a significant difference in corpus size so the numbers are not directly comparable. The common components for all the tested systems are language models, which were taken from CUNI’s last year submission. For some experiments we have used up to 4 phrase tables separately as Moses alternative decoding paths, trusting MERT (Och, 2003) to estimate weights. Alternatively (or when the number of the phrase tables would be even higher), we used the standard Moses phrase table mixing technique with uniform weights. Phrase tables mixed into one before MERT are listed as “Mix(table1, table2, ...)” in the following. MERT was done using the WMT2015 test set, and our internal evaluation was performed on WMT2016 test set, but with a different tokenization so the scores reported here are not directly comparable to the results at http://matrix. statmt.org/. We report the results in Table 2, listing the used phrase tables and optionally"
W17-4720,W12-3146,1,0.904593,"Missing"
W17-4720,W08-0325,0,0.045346,"Missing"
W17-4720,P17-2060,0,0.0446737,"Missing"
W17-4720,E17-3017,0,0.050928,"Missing"
W17-4720,P16-1162,0,0.751947,"able 1: Datasets 3.1 Back-Translated Data To create back-translated data, we used the CzEng 1.6 Czech-English parallel corpus (Bojar et al., 2016) and the Czech News Crawl articles released for WMT20171 (called “mononews” for short). We used two different back-translation systems: Moses (Koehn et al., 2007) trained by ourselves, and Marian2 (known as AmuNMT before it included NMT training; Junczys-Dowmunt et al., 2016) using the pretrained Nematus (Sennrich et al., 2017) models3 from WMT16 News Task.4 We used only the non-ensembled left-to-right run (i.e. no right-to-left rescoring as done by Sennrich et al., 2016a) with beam size of 5,5 taking just the single-best output. The Moses-based system used only a single phrase table translating from word form to word forms and twelve 10-gram language models built on individual years of English mononews. We took all Czech mononews corpora available this year, concatenated and translated them using both systems described above and thus created two back-translated corpora on which we planned to train our forward systems. The “Synthetic corpora” section of Table 1 shows the numbers of sentences and tokens of the resulting corpora. Despite having started with the"
W17-4734,W05-0909,0,0.158301,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W17-4734,P09-1064,0,0.0328843,"translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER"
W17-4734,D17-1209,1,0.891192,"Missing"
W17-4734,E14-2008,1,0.856971,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W16-2302,1,0.832947,". Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER (Stanojevic and Simaan, 2014) or C HR F (Popovic, 2015). The entire list of MT candidates is then entirely re-ranked according to the averaged score of each candidate. Different from most re-ranking approaches which make use of additional information usually treated as new model components and combined with the existing ones, we here focus only on the MT candidates. The difference between the consensus-based n-best list selection and an oracle translation is the absence Since only one development set was provided we split the given development set into two parts: newsdev2017/1 a"
W17-4734,W14-3310,1,0.870459,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4703,1,0.884283,"Missing"
W17-4734,2014.iwslt-evaluation.7,1,0.873477,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4737,1,0.831634,"Missing"
W17-4734,W11-2123,0,0.0435093,"o this end, k-best hypothesis from the dictionary were generated, as well as the n-best hypothesis 3.4 Tilde The Tilde system is a Moses phrase-based SMT system that was trained on the Tilde MT platform (Vasil¸jevs et al., 2012). The system was trained using all available parallel data - 1.74 million unique sentence pairs after filtering, and 3 million unique sentence pairs that were acquired by re-translating a random selection of indomain monolingual sentences with a neural machine translation system (Pinnis et al., 2017). The system has a 5-gram language model that was trained using KenLM (Heafield, 2011) on all available monolingual data (27.83 million unique sentences). 3.5 UEDIN The University of Edinburgh’s system is an attentional encoder-decoder (Bahdanau et al., 2015), trained using the Nematus toolkit (Sennrich et al., 2017c). As training data, we used all parallel and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden la"
W17-4734,W15-3049,0,0.0536506,"Missing"
W17-4734,E17-2025,0,0.0291776,"l and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden layers of size 1024, with the size of the source and target network vocabularies fixed to the size of the respective BPE vocabularies. In order to reduce the size of the models, the target-side embedding weights were tied with the transpose of 350 the output weight matrix (Press and Wolf, 2017). We used a deep transition architecture inspired by the one proposed by Zilly et al. (2016) for language modelling. In experiments conducted during feature development, we found that this gave consistent improvements across multiple language pairs. We also applied layer normalisation (Ba et al., 2016) to all recurrent and feed-forward layers, except for layers that are followed by a softmax. In preliminary experiments, we found that using layer normalisation led to faster convergence and resulted in slightly better performance. We trained the models with adam (Kingma and Ba, 2015), using a le"
W17-4734,E17-3017,0,0.0486901,"Missing"
W17-4734,P17-4012,0,0.0301124,"stem for the WMT 2017 shared task for machine translation of news 1 are seven individual 1 http://www.statmt.org/wmt17/ translation-task.html 348 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 348–357 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tool. The number of sentences being removed is approximately 50000. in Neural Monkey. Instead, the translations were generated using greedy search. 3 3.2 Translation Systems The neural machine translation models from KIT are built with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for p"
W17-4734,D17-1159,0,0.0716203,"Missing"
W17-4734,D16-1096,0,0.0289091,"rom backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about the effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of"
W17-4734,P03-1021,0,0.0379532,"re case-sensitive. of reference translation: each translation hypothesis is scored against all the other hypotheses used as references while in an oracle translation each translation hypothesis is scored against a single reference. This results in obtaining as best translation hypothesis the candidate that is most similar to the most likely translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varyi"
W17-4734,P16-1162,0,0.11892,"he effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of the system was built using Neural Monkey2 (Helcl and Libovick´y, 2017), a flexible sequence-to-sequence toolkit implementing primarily the Bahdanau et al. (2015) model but useful also in multi-modal translation and multi-task training. We used essentially the baseline setup of the system as released for the WMT17 NMT Training Task3 (Bojar et al., 2017) for an 8GB GPU card. This involves BPE (Sennrich et al., 2016) with 30k merges, maximum sentence length for both source and target limited to 50 (BPE) tokens, no dropout and embeddings (both source and target) of 600, vocabulary shared between encoder and decoder, attention and conditional GRU (Firat and Cho, 2016). We experimented with the RNN size of the encoder and decoder and increased them to 800 instead of 600, at the expense of reducing batch size to 10. The batch size of 30 with this enlarged model would still fit into our GPU card but this run was prematurely interrupted due to a hardware failure and we noticed that it converges slower in terms"
W17-4734,W14-3354,0,0.0640118,"Missing"
W17-4734,P16-5005,0,0.0206781,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P16-1008,0,0.0235141,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P12-3008,0,0.0243602,"Missing"
W17-4755,W17-4757,1,0.870414,"Missing"
W17-4755,P13-1023,0,0.00853037,"Missing"
W17-4755,W17-4766,0,0.0543842,"L NGRAM 2 VEC T REE AGGREG UHH TSKM Seg-level Sys-level Hybrids • • • • • − • • • • • • • • ⊘ ⊘ ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ − • ⊘ ⊘ ⊘ ⊘ ⊘ − ⊘ ⊘ Participant Charles University (Mareˇcek et al., 2017) Charles University (Mareˇcek et al., 2017) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) ICTCAS-DCU (Ma et al., 2017) University of Tartu (T¨attar and Fishel, 2017) RWTH Aachen University (Wang et al., 2016) (Popovi´c, 2015) (Popovi´c, 2017) (Popovi´c, 2017) NRC (Lo, 2017) NRC (Lo, 2017) University of Tartu (T¨attar and Fishel, 2017) Charles University (Mareˇcek et al., 2017) (Duma and Menzel, 2017) Table 2: Participants of WMT17 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. AUTO DA incl. TECTO BEER B LEND BLEU 2 VEC SEP C HARAC T ER CHR F, incl. + and ++ MEANT 2.0 incl. NOSRL NGRAM 2 VEC T REE AGGREG http://github.com/ufal/auto-hume Baselines: BLEU, NIST CDER, PER, TER, WER SENT BLEU http://github.com/moses-smt/mosesdecoder http://gith"
W17-4755,D14-1020,1,0.396051,"Missing"
W17-4755,D16-1134,1,0.213117,"ximately 300 sentences for each of the four language pairs (from English into Czech, German, Polish and Romanian) coming from both WMT16 news translation task as well as from HimL test sets 2015,3 which are sentences from health-related texts by Cochrane and NHS 24. The reference translations are the standard WMT16 references for the news domain and post-edits of phrase-based MT for the Cochrane and NHS 24 sentences. No document structure has been preserved in this dataset. evaluations are now a little closer: both humans and metrics compare the MT output with the reference. • The HUME score (Birch et al., 2016) is a segment-level score aggregated over manual judgements of translation quality of semantic units of the source sentence. In contrast to previous years, the official method of evaluation changes, moving from “relative ranking” (RR, evaluating up to five system outputs on an annotation screen relative to each other) to DA and employing the Pearson correlation r in most cases. Due to difficulties in obtaining sufficient number of judgements for segment-level evaluation of some language pairs, we re-interpret DA judgements for these language pairs as relative comparisons and use Kendall’s τ as"
W17-4755,N16-1001,1,0.858563,"training task organizers on newstest2017, see Bojar et al. (2017b) for more details. All training task systems can be thus seen as regular submissions to the news translation task, with additional constraints in place. While one would expect these systems to produce outputs more similar to each other than the remaining news task systems, this is not the case, see Table 3 in Findings 2017. Based on the manual evaluation, training task systems however perform similarly, occupying the lower half of the ranking. the aim of providing a larger set of systems against which to evaluate metrics, as in Graham and Liu (2016). Hybrid systems were created separately for newstest2017 and himltest2017 by randomly alternating sentences from the outputs of pairs of systems of the given dataset. In short, we create 10K hybrid MT systems for each language pair. Excluding the hybrid systems, we ended up with 166 system outputs across 16 language pairs and 3 test sets. 2.3 Manual MT Quality Judgments HUME Test Set Round 2 Systems are the MT systems translating himltest2017. For each language pair, three different MT systems are provided. The translations were run by the EU project HimL and the systems cover major MT system"
W17-4755,W11-2101,1,0.897453,"Missing"
W17-4755,W13-2305,1,0.850645,"stem were to a large part included in the HUME track last year and thus leaked to the training data we provided to metrics task participants this year. The affected test set file is himltest2017a.Year1.en-pl with 324 sentences out of 340 included in the training data. The file himltest2017a.PBMT.en-pl also contains 16 known sentences, probably due to identical translation. The performance of trained metrics for en-pl evaluation have the potential to be inflated therefore. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. Furthermore, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on one screen, where Hybrid Systems are created automatically with 4 http://www.himl.eu/files/D5.4_Second_ Evaluation_Report.pdf 5 491 https://www.mturk.com scores for translations"
W17-4755,1999.mtsummit-1.31,0,0.441529,"Missing"
W17-4755,E14-1047,1,0.39054,"part included in the HUME track last year and thus leaked to the training data we provided to metrics task participants this year. The affected test set file is himltest2017a.Year1.en-pl with 324 sentences out of 340 included in the training data. The file himltest2017a.PBMT.en-pl also contains 16 known sentences, probably due to identical translation. The performance of trained metrics for en-pl evaluation have the potential to be inflated therefore. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. Furthermore, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on one screen, where Hybrid Systems are created automatically with 4 http://www.himl.eu/files/D5.4_Second_ Evaluation_Report.pdf 5 491 https://www.mturk.com scores for translations were unfairly penali"
W17-4755,N15-1124,1,0.70619,"nts. 1 Introduction Evaluating the quality of machine translation (MT) is critical for developers of MT systems to monitor progress as well as for MT users to select among available MT engines for their language pair of interest. Manual evaluation is however costly and difficult to reproduce. Automatic MT evaluation can resolve these issues, if it matches manual evaluation. The Metrics Shared Task1 of WMT annually evaluates the performance of automatic machine translation metrics in their ability to provide a substitute for human assessment of translation quality. • In Direct Assessment (DA) (Graham et al., 2015), humans assess the quality of a given MT output translation by comparison with a reference translation (but not the source). DA is the new standard used in WMT news translation task evaluation, requiring only monolingual evaluators. The added benefit for the metrics task is that the manual and automatic 1 http://www.statmt.org/wmt17/ metrics-task.html, starting with Koehn and Monz (2006) up to Bojar et al. (2016b) 489 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 489–513 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computat"
W17-4755,W17-4769,1,0.897756,"Missing"
W17-4755,L16-1262,0,0.0253718,"Missing"
W17-4755,P02-1040,0,0.115662,"Missing"
W17-4755,W06-3114,0,0.0825673,"hared Task1 of WMT annually evaluates the performance of automatic machine translation metrics in their ability to provide a substitute for human assessment of translation quality. • In Direct Assessment (DA) (Graham et al., 2015), humans assess the quality of a given MT output translation by comparison with a reference translation (but not the source). DA is the new standard used in WMT news translation task evaluation, requiring only monolingual evaluators. The added benefit for the metrics task is that the manual and automatic 1 http://www.statmt.org/wmt17/ metrics-task.html, starting with Koehn and Monz (2006) up to Bojar et al. (2016b) 489 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 489–513 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics available in Deliverable D5.4 of the project.2 Out selection contains approximately 300 sentences for each of the four language pairs (from English into Czech, German, Polish and Romanian) coming from both WMT16 news translation task as well as from HimL test sets 2015,3 which are sentences from health-related texts by Cochrane and NHS 24. The reference translations are"
W17-4755,W15-3049,0,0.0885115,"Missing"
W17-4755,E06-1031,0,0.16166,"Missing"
W17-4755,W17-4770,0,0.0439896,"Missing"
W17-4755,W15-3056,0,0.0246205,"Missing"
W17-4755,2006.amta-papers.25,0,0.29098,"Missing"
W17-4755,W17-4767,0,0.310193,"Missing"
W17-4755,W15-3050,0,0.209272,"Missing"
W17-4755,W17-4768,1,0.563357,"Missing"
W17-4755,W17-4771,0,0.145209,"Missing"
W17-4755,W14-3336,1,0.627786,"Missing"
W17-4755,W16-2342,0,0.124039,"han“ even for metrics where the better system receives a higher score. 494 Metric AUTO DA AUTO DA. TECTO BEER B LEND BLEU 2 VEC SEP C HARAC T ER CHR F CHR F+ CHR F++ MEANT 2.0 MEANT 2.0- NOSRL NGRAM 2 VEC T REE AGGREG UHH TSKM Seg-level Sys-level Hybrids • • • • • − • • • • • • • • ⊘ ⊘ ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ − • ⊘ ⊘ ⊘ ⊘ ⊘ − ⊘ ⊘ Participant Charles University (Mareˇcek et al., 2017) Charles University (Mareˇcek et al., 2017) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) ICTCAS-DCU (Ma et al., 2017) University of Tartu (T¨attar and Fishel, 2017) RWTH Aachen University (Wang et al., 2016) (Popovi´c, 2015) (Popovi´c, 2017) (Popovi´c, 2017) NRC (Lo, 2017) NRC (Lo, 2017) University of Tartu (T¨attar and Fishel, 2017) Charles University (Mareˇcek et al., 2017) (Duma and Menzel, 2017) Table 2: Participants of WMT17 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. AUTO DA incl. TECTO BEER B LEND BLEU 2 VEC SEP C HARAC T ER CHR F, incl"
W17-4755,W15-3053,0,0.10151,"Missing"
W17-4769,D16-1134,1,0.825917,"rst one works only on Czech and uses many semantic features based of rich Czech tectogrammatical annotation (B¨ohmov´a et al., 2003). The second one uses much fewer features, however, it is language universal and needs only a dependency parsing model available. 2.1 AutoDA Using Czech Tectogrammatics This metric automatically parses the Czech translation candidate and the reference translation and uses various semantic features to compute the final score. 1. AutoDA: A linear regression model using semantic features trained on WMT Direct Assessment scores (Bojar et al., 2016) or HUMEseg scores (Birch et al., 2016). 2.1.1 Word Alignment AutoDA relies on automatic alignment between the translation candidate and the reference trans2. TreeAggreg: N-gram based metric computed over aligned syntactic structures instead of 604 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 604–611 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Method AutoDA TreeAggreg NMTScorer Resource Type Monolingual/Bilingual* Monolingual Bilingual Trainable Yes No Yes Metric Type Segment-level Linear Regression Tree Segment-level ChrF** Segment-le"
W17-4769,L16-1262,0,0.102773,"Missing"
W17-4769,C00-2163,0,0.158333,"sible. TreeAggreg can use any string-level metric for score computation instead of ChrF (**). Dataset WMT16 DAseg WMT15 DAseg WMT16 HUMEseg Source TR/FI/CS/RO/RU/DE EN DE/RU/FI/CS EN EN Target EN RU EN RU CS/DE/PL/RO # Sentences 560 500 ∼350 Table 2: Overview of the available data for training AutoDA. • functor: the semantic value of the syntactic dependency relation. Functors express the functions of individual modifications in the sentence, e.g. ACT (Actor), PAT (Patient), ADDR (Addressee), LOC (Location), MANN (Manner), lation. The easiest way of obtaining word alignments is to run GIZA++ (Och and Ney, 2000) on the set of sentence pairs. GIZA++ was designed to align documents in two languages and it can obviously also align documents in a single language, although it does not benefit in any way from the fact that many words are identical in the aligned sentences. GIZA++ works well if the input corpus is sufficiently large, to allow for extraction of reliable word co-occurrence statistics. While the test sets alone are too small, we have a corpus of paraphrases for Czech (Bojar et al., 2013). We thus run GIZA++ on all possible paraphrase combinations together with the reference-translation pairs w"
W17-4769,W16-2301,1,0.86698,"Missing"
W17-4769,P02-1040,0,0.114911,"ranslation “Jako kofeinov´y n´apoj, alkohol v tˇele zabraˇnuje vstˇreb´av´an´ı kalcia z potravy.” metric aligned-tnode-tlemma-exact-match aligned-tnode-formeme-match aligned-tnode-functor-match aligned-tnode-sempos-match lexrf-form-exact-match lexrf-lemma-exact-match BLEU on forms BLEU on lemmas chrF3 AutoDA (87 features) AutoDA (selected 23 features) 2.1.4 Linear Regression Training We collect 83 various features based on matching tectogrammatical attributes computed on all nodes or a subsets defined by particular semantic part-of-speech tags. To this set of features, we add two BLEU scores (Papineni et al., 2002) computed on forms and on lemmas and two chrF3 scores (Popovic, 2015) computed on trigrams and sixgrams, so we have 87 features in total. We train a linear regression model to obtain a weighted mix of features that fits best the WMT16 HUMEseg scores. Since the amount of annotated data available is low, we use the jackknife strategy: en-cs 0.449 0.429 0.391 0.416 0.372 0.436 0.361 0.395 0.540 0.625 0.659 Table 3: Selected Czech deep-syntactic features and their correlation against WMT16 HUMEseg dataset. Comparison with BLEU, chrF3, and our trainable AutoDA (using chrF3 as well). • We split the"
W17-4769,W15-3049,0,0.139982,"Missing"
W17-4769,W12-4205,1,0.901962,"Missing"
W17-4769,L16-1680,0,0.0491449,"Missing"
W17-4777,W04-3250,0,0.205278,"sent in the training data, the model learns to use special tokens “<keep&gt;” and “<delete&gt;”, or to normally produce characters present in the training data, to indicate the modifications needed for the MT output. We used the same network parameters and data (including the synthetic dataset) for the model with and without BPE. 3.4 BLEU 62.09 (±1.04) 50.86 (±3.96) 66.04 (±1.16) 62.08 (±1.05) Table 2: Automatic evaluation of the final 8GB APE setups. The score of the original MT output is shown for comparison. The ± values are empirical confidence intervals reflecting the variance in the test set (Koehn, 2004). BLEU of 66.04. We chose this system as our primary submission for the WMT16 APE task. Evaluation We were a little surprised that there was no improvement when using model that learned to generate post-editing operations (“Synth+editops”). When we manually examined the generated output, we found out that the system took the safer path of keeping most of the machine translation output because it probably resulted in fewer errors than trying to change it. This could be probably avoided by discouraging the model from keeping the whole MT output unchanged and we plan investigating this approach i"
W17-4777,P16-1160,0,0.422712,"s reported that 661 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 661–666 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Figure 1: Illustration of a multiple-encoder sequence-to-sequence architecture as illustrated in Libovick´y et al. (2016). proach in its basic form results in much longer sequences that are generally much harder to learn for the underlying recurrent neural network (RNN, Pascanu et al., 2012). Another downside is the increased training and inference time for each sentence. Recently, Lee et al. (2016) presented an encoder architecture that uses RNN over the output of several hundreds convolutional filters that are applied on the character-level embeddings, combining the benefits of both convolutional and recurrent approaches. (through the attention mechanism) the decoder is able to attend to relevant parts of the concatenated sentences when generating output. As an alternative option, we also tried using two separate encoders, one for the source sentence and one for the MT output (Libovick´y et al., 2016) as shown in Figure 1. In this case, both encoders encode their corresponding input se"
W17-4777,P17-2031,0,0.0368583,"Missing"
W17-4777,W16-2361,1,0.56112,"Missing"
W17-4777,C16-1172,0,0.287904,"Missing"
W17-4777,P16-2046,0,0.0850807,"Missing"
W17-4777,P02-1040,0,0.108365,"ined. The model size was downscaled to 5GB due to the limited computation resources. • shared character-level vocabulary size: 500 • encoder RNN size: 256 • input embedding size: 300 The experiments were carried out in Neural Monkey1 (Helcl and Libovick´y, 2017), a framework for sequence-to-sequence modeling. Most of the required neural network components together with necessary preprocessing and postprocessing were already implemented in the framework. We added the RNN over convolutional encoder in this work. We used 12k sentences WMT16 APE training dataset for training and we computed BLEU (Papineni et al., 2002) on WMT16 APE development dataset to compare the baselines. The evaluation was performed during training. We thus did not use beam search and simply greedily chose the most probable output at each decoding step to get the validation output. The best results for each architecture are shown in Table 1. We can see that the character-level post-editing models outperform the subword-level models. However, the training was done using only a small dataset which may possibly indicate that the character level architecture is able to better exploit the training data. Nevertheless, we chose the character"
W17-4777,2011.mtsummit-papers.35,0,0.0729381,"Missing"
W17-4777,W16-2378,0,0.195216,"ing. This can be done manually, or as the automatic post-editing (APE) task expects, automatically. When phrase-based machine translation (PBMT) was the indisputable state of the art, some automatic post-editing (APE) systems were based on the PBMT techniques (Simard et al., 2007). With source-sentence information (B´echara et al., 2011), post-editing results were quite promising. It is therefore not surprising that with the rise of the neural machine translation, neural APE systems based on the findings in NMT research were built (Pal et al., 2016) and even won last year’s WMT16 Shared Task (Junczys-Dowmunt and Grundkiewicz, 2016). 2.1.1 Multi-Source Input All our experiments use both the source sentence and the MT output to be corrected. As far as encoding the input is concerned, we examined two basic approaches. We tried using a single encoder that received the concatenation of the source sentence and the corresponding MT output as suggested by Niehues et al. (2016). The resulting input sequence becomes longer and it may thus be more difficult to encode, but it was reported that 661 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 661–666 c Copenhagen, Denmark, September"
W17-4777,N07-1064,0,0.242237,"nd the MT output (multi-source input), and (2) whether to use subword units or individual characters. Introduction Even with the recent substantial improvements of the machine translation (MT) quality mainly thanks to the increasingly popular neural models (neural MT, NMT), many errors still remain in the output require further post-editing. This can be done manually, or as the automatic post-editing (APE) task expects, automatically. When phrase-based machine translation (PBMT) was the indisputable state of the art, some automatic post-editing (APE) systems were based on the PBMT techniques (Simard et al., 2007). With source-sentence information (B´echara et al., 2011), post-editing results were quite promising. It is therefore not surprising that with the rise of the neural machine translation, neural APE systems based on the findings in NMT research were built (Pal et al., 2016) and even won last year’s WMT16 Shared Task (Junczys-Dowmunt and Grundkiewicz, 2016). 2.1.1 Multi-Source Input All our experiments use both the source sentence and the MT output to be corrected. As far as encoding the input is concerned, we examined two basic approaches. We tried using a single encoder that received the conc"
W17-4777,Q17-1026,0,\N,Missing
W17-4777,P16-1162,0,\N,Missing
W17-4780,W15-1521,0,0.0697943,"Missing"
W17-4780,P16-1162,0,0.192923,"sion to the WMT 2017 Neural MT Training Task. We modified the provided NMT system in order to allow for interrupting and continuing the training of models. This allowed mid-training batch size decrementation and incrementation at variable rates. In addition to the models with variable batch size, we tried different setups with pre-trained word2vec embeddings. Aside from batch size incrementation, all our experiments performed below the baseline. 1 1.1 The Baseline System Our baseline model was trained using the provided NMT system and the provided data, including the given word splits of BPE (Sennrich, Haddow, and Birch, 2016). Of the two available configurations, we selected the 4GB one for most experiments to fit the limits of GPU cards available at MetaCentrum.1 This configuration uses a maximum sentence length of 50, word embeddings of size 300, hidden layers of size 350, and clips the gradient norm to 1.0. We used a mini-batch size of 60 for this model. Due to resource limitations at MetaCentrum, the training had to be interrupted after a week of training. We modified Neural Monkey to enable training continuation by saving and loading the model and we always submitted the continued training as a new job. When"
W17-5715,D15-1166,0,0.10508,"Missing"
W17-5715,D17-1151,0,0.0359414,"Missing"
W17-5715,P16-1009,0,0.400653,"In this section we describe the methods we used for preprocessing both Japanese and English. Due to Japanese being an unsegmented language with no clear definition of word boundaries, proper text segmentation is essential. We used MeCab2 (Kudo et al., 2004) with the UniDic3 dictionary to perform the tokenization. For English, we used morphological analyser MorphoDiTa4 (Strakov´a et al., 2014) to tokenize English training sentences. Based on the generated lemmas, we also performed truecasing of the target side of the training data. To reduce the vocabulary size, we use byte pair encoding (BPE; Sennrich et al., 2016c) which breaks all words into subword units. The vocabulary is initialized with all alphabet characters Introduction With neural machine translation (NMT) currently becoming the leading paradigm in the field of machine translation, many novel NMT architectures with state-of-the-art results are being proposed. In the past, there were reports on large scale evaluation (Britz et al., 2017), however, the experiments were performed on a limited number of language pairs from related language families (English→German, English→French) or focused on a subset of possible NMT architectures, leaving room"
W17-5715,P17-1175,0,0.0491915,"Missing"
W17-5715,P16-1162,0,0.55565,"In this section we describe the methods we used for preprocessing both Japanese and English. Due to Japanese being an unsegmented language with no clear definition of word boundaries, proper text segmentation is essential. We used MeCab2 (Kudo et al., 2004) with the UniDic3 dictionary to perform the tokenization. For English, we used morphological analyser MorphoDiTa4 (Strakov´a et al., 2014) to tokenize English training sentences. Based on the generated lemmas, we also performed truecasing of the target side of the training data. To reduce the vocabulary size, we use byte pair encoding (BPE; Sennrich et al., 2016c) which breaks all words into subword units. The vocabulary is initialized with all alphabet characters Introduction With neural machine translation (NMT) currently becoming the leading paradigm in the field of machine translation, many novel NMT architectures with state-of-the-art results are being proposed. In the past, there were reports on large scale evaluation (Britz et al., 2017), however, the experiments were performed on a limited number of language pairs from related language families (English→German, English→French) or focused on a subset of possible NMT architectures, leaving room"
W17-5715,W14-4012,0,0.0542259,"Missing"
W17-5715,P14-5003,0,0.0658526,"Missing"
W17-5715,W04-3230,0,0.211586,"Missing"
W17-5715,P16-1100,0,0.0440675,"Missing"
W17-7508,P14-5004,0,0.182637,". Note that the performance on this test set is affected by the vocabulary overlap between the test set and the vocabulary of the embeddings; questions containing out-of-vocabulary words cannot be evaluated. This is the main reason, why we trained all tasks on the same training set and with the same vocabulary, so that their performance in lexical relations can be directly compared. Another lexical relation benchmark is the word similarity. The idea is that similar words such as ‘football’ and ‘soccer’ should have vectors close together. There exist many datasets dealing with word similarity. Faruqui and Dyer (2014) have 61 extracted words similarity pairs from 12 different Initialization N (0, 10) N (0, 1) N (0, 0.1) N (0, 0.01) N (0, 0.001) Zeros Ones He init. Xavier init. Word2Vec on trainset* Word2Vec official* GloVe official* en-cs 0.0; 0.3 0.0; 0.4 1.2; 23.5 2.0; 29.9 2.1; 31.4 1.6; 29.5 0.5; 16.6 1.4; 28.9 1.5; 29.5 MT LM LEM 0.0; 0.3 1.4; 3.5 5.5; 15.2 6.9; 19.4 6.7; 18.2 6.0; 17.5 5.3; 9.3 7.7; 18.3 7.4; 18.2 22.3; 48.9 81.3; 70.7 12.3; 60.1 0.0; 0.3 0.0; 0.3 0.0; 0.8 0.1; 32.7 0.3; 33.3 0.2; 31.1 0.1; 31.0 0.1; 32.6 0.1; 32.7 Table 3: The accuracy in percent on the (semantic; morphosyntactic) q"
W17-7508,P16-1158,0,0.0186295,"ll zeros leaves the neural network free choice over the utilization of the embedding space. We support our hypothesis as follows. • We examine the embedding space on the performance in lexical relations between words, If our hypothesis is plausible, low-variance embeddings will perform better at representing these relations. • We run an experiment with non-trainable fixed random initialization to demonstrate the ability of the neural network to overcome broken embeddings and to learn the information about words in its deeper hidden layers. 5.1 Lexical relations Recent work on word embeddings (Vylomova et al., 2016; Mikolov et al., 2013) has shown that simple vector operations over the embeddings are surprisingly effective at capturing various semantic and morphosyntactic relations, despite lacking explicit supervision in these respects. The testset by Mikolov et al. (2013) contains “questions” defined as v(X) − v(Y ) + v(A) ∼ v(B). The well-known example involves predicting a vector for word ‘queen’ from the vector combination of v(king) − v(man) + v(woman). This example is a part of “semantic relations” in the test set, called opposite-gender. The dataset contain another 4 semantic relations and 9 mor"
W17-7508,N16-1030,0,0.0159209,"Word2Vec was trained on 100 billion words Google News S Bandyopadhyay, D S Sharma and R Sangal. Proc. of the 14th Intl. Conference on Natural Language Processing, pages 56–64, c Kolkata, India. December 2017. 2016 NLP Association of India (NLPAI) dataset1 and GloVe embeddings were trained on 6 billion words from the Wikipedia. Sometimes, they are used as a fixed mapping for a better robustness of the system (Kenter and De Rijke, 2015), but they are more often used to seed the embeddings in a system and they are further trained in the particular end-to-end application (Collobert et al., 2011; Lample et al., 2016). In practice, random initialization of embeddings is still more common than using pretrained embeddings and it should be noted that pretrained embeddings are not always better than random initialization (Dhingra et al., 2017). We are not aware of any study of the effects of various random embeddings initializations on the training performance. In the first part of the paper, we explore various English word embeddings initializations in four tasks: neural machine translation (denoted MT in the following for short), language modeling (LM), part-of-speech tagging (TAG) and lemmatization (LEM), c"
W17-7508,P16-1160,0,0.0223043,"e any significant difference between various methods of random initialization, as long as the variance is kept reasonably low. High-variance initialization prevents the network to use the space of embeddings and forces it to use other free parameters to accomplish the task. We support this hypothesis by observing the performance in learning lexical relations and by the fact that the network can learn to perform reasonably in its task even with fixed random embeddings. 1 Introduction Embeddings or lookup tables (Bengio et al., 2003) are used for units of different granularity, from characters (Lee et al., 2016) to subword units (Sennrich et al., 2016; Wu et al., 2016) up to words. In this paper, we focus solely on word embeddings (embeddings attached to individual token types in the text). In their highly dimensional vector space, word embeddings are capable of representing many aspects of similarities between words: semantic relations or morphological 56 properties (Mikolov et al., 2013; Kocmi and Bojar, 2016) in one language or cross-lingually (Luong et al., 2015). Embeddings are trained for a task. In other words, the vectors that embeddings assign to each word type are almost never provided manu"
W17-7508,W15-1521,0,0.0680389,"Missing"
W17-7508,P02-1040,0,0.101987,"sionality of the available pretrained Word2Vec and GloVe embeddings. All tasks are trained using the Adam (Kingma and Ba, 2014) optimization algorithm. We are using 4GB machine translation setup (MT) as described in Bojar et al. (2017) with increased encoder and decoder RNN sizes. The setup is the encoder-decoder architecture with attention mechanism as proposed by Bahdanau et al. (2014). We use encoder RNN with 500 GRU cells for each direction (forward and backward), decoder RNN with 450 conditional GRU cells, maximal length of 50 words and no dropout. We evaluate the performance using BLEU (Papineni et al., 2002). Because our aim is not to surpass the state-of-the-art MT performance, we omit common extensions like beam search or ensembling. Pretrained embeddings also prevent us from using subword units (Sennrich et al., 2016) or a larger embedding size, as customary in NMT. We experiment only with English-to-Czech MT and when using pretrained embeddings we modify only the 58 source-side (encoder) embeddings, because there are no pretrained embeddings available for Czech. The goal of the language model (LM) is to predict the next word based on the history of previous words. Language modeling can be thu"
W17-7508,D14-1162,0,0.0859859,"Missing"
W17-7508,P16-1162,0,0.12597,"various methods of random initialization, as long as the variance is kept reasonably low. High-variance initialization prevents the network to use the space of embeddings and forces it to use other free parameters to accomplish the task. We support this hypothesis by observing the performance in learning lexical relations and by the fact that the network can learn to perform reasonably in its task even with fixed random embeddings. 1 Introduction Embeddings or lookup tables (Bengio et al., 2003) are used for units of different granularity, from characters (Lee et al., 2016) to subword units (Sennrich et al., 2016; Wu et al., 2016) up to words. In this paper, we focus solely on word embeddings (embeddings attached to individual token types in the text). In their highly dimensional vector space, word embeddings are capable of representing many aspects of similarities between words: semantic relations or morphological 56 properties (Mikolov et al., 2013; Kocmi and Bojar, 2016) in one language or cross-lingually (Luong et al., 2015). Embeddings are trained for a task. In other words, the vectors that embeddings assign to each word type are almost never provided manually but always discovered automatically"
W18-1816,D17-1209,0,0.0233415,"tion, we describe a selection of these contributions. Kreutzer et al. (2017) used Neural Monkey for domain adaptation using bandit learning with simulated user feedback. This technique brings a signiﬁcant improvement over both the out-of-domain model and other domain adaptation techniques. Libovick´y and Helcl (2017) published interpretable attention combination strategies for multi-source sequence-to-sequence learning tasks, such as multimodal translation. These combination strategies allow to explicitly model the different importance of the source sequences in each step during the decoding. Bastings et al. (2017) used Neural Monkey to develop a new convolutional architecture for encoding the input sentences using dependency trees. This syntax-aware source language representation brings a consistent improvement over the attentive sequence-to-sequence model baseline. Kocmi and Bojar (2017) published a thorough examination of various embedding initialization strategies, both random and pre-trained. They conclude that improvements from pretrained embeddings are not always beneﬁcial, and that high-quality embeddings can be trained using an initialization with a normal distribution with a small variance or"
W18-1816,W17-4717,1,0.892012,"Missing"
W18-1816,W17-4757,1,0.611972,"Missing"
W18-1816,W14-4012,0,0.0822584,"Missing"
W18-1816,P17-1012,0,0.0254664,"twodimensional (spatial) set of states. • Recurrent Sequence Encoder allows encoding input sequence using stacked recurrent neural networks (RNNs). In addition to embedded symbolic input (Sutskever et al., 2014), we allow also a numerical input which can be used for instance for speech recognition. • RNN over Character-Level Convolutional Neural Network (Lee et al., 2017), a technique that applies convolutional neural network (CNN) on the unsegmented character-level input. To produce the sequence of states, it uses a recurrent sequence encoder on the CNN outputs. • Deep Convolutional Encoder (Gehring et al., 2017), an encoder for sequence-to-sequence learning with stacked convolutional layers and positional embeddings. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 170 • Transformer Encoder is the encoder part of the Transformer (Vaswani et al., 2017) architecture which repeatedly uses the self-attention mechanism over the input representation. • ConvNet for Sentence Classiﬁcation. We implement the convolutional network with maxpooling for sentence classiﬁcation (Kim, 2014). It is a fast baseline method for sentence classiﬁcation such as sentiment analysis. • Self"
W18-1816,W17-4749,1,0.895582,"Missing"
W18-1816,E17-3017,0,0.022014,"., 2017; Bojar et al., 2017b). Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 172 5 Comparison to Other Toolkits Due to its goals, Neural Monkey differs from other open-source packages used for sequenceto-sequence learning.3 There are several production-oriented tools. They aim to be easy to use and work well in an online setup. They also aim to reach the best result possible as long as it does not intervene with its practical usability. Tools like this are most importantly OpenNMT (Klein et al., 2017), Marian (Junczys-Dowmunt et al., 2016), Sockeye NMT (Hieber et al., 2017), and Tensor2Tensor (Kaiser et al., 2017). All of them contain several well-tuned pre-made architectures that can be used out of the box for sequence-to-sequence learning tasks. Tools like Nematus (Sennrich et al., 2017b) and nmtpy (Caglayan et al., 2017) built on Theano (Al-Rfou et al., 2016) or xnmt based on DyNet (Neubig et al., 2017) are more research oriented. For instance, Nematus implements several techniques which improve the performance of the system in the ofﬂine setup but which would be prohibitively slow in the online use. Using these techniques (such as model ensembling or right-t"
W18-1816,D14-1181,0,0.00548686,"s a recurrent sequence encoder on the CNN outputs. • Deep Convolutional Encoder (Gehring et al., 2017), an encoder for sequence-to-sequence learning with stacked convolutional layers and positional embeddings. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 170 • Transformer Encoder is the encoder part of the Transformer (Vaswani et al., 2017) architecture which repeatedly uses the self-attention mechanism over the input representation. • ConvNet for Sentence Classiﬁcation. We implement the convolutional network with maxpooling for sentence classiﬁcation (Kim, 2014). It is a fast baseline method for sentence classiﬁcation such as sentiment analysis. • Self-Attentive Sentence Embedding. As a more advanced technique for sentence classiﬁcation, we use sentence encoding by Lin et al. (2017) which uses attention to produce a ﬁxed-size structured representation of the input sequence. • Wrapper for ImageNet Networks. Tasks combining language and vision usually use image features from deep convolutional networks for image classiﬁcation. Neural Monkey provides wrappers for the inclusion of these models as implemented in TensorFlow Slim library.2 In particular, we"
W18-1816,P17-4012,0,0.0332938,"as baselines in bandit learning and neural training tasks. (Sokolov et al., 2017; Bojar et al., 2017b). Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 172 5 Comparison to Other Toolkits Due to its goals, Neural Monkey differs from other open-source packages used for sequenceto-sequence learning.3 There are several production-oriented tools. They aim to be easy to use and work well in an online setup. They also aim to reach the best result possible as long as it does not intervene with its practical usability. Tools like this are most importantly OpenNMT (Klein et al., 2017), Marian (Junczys-Dowmunt et al., 2016), Sockeye NMT (Hieber et al., 2017), and Tensor2Tensor (Kaiser et al., 2017). All of them contain several well-tuned pre-made architectures that can be used out of the box for sequence-to-sequence learning tasks. Tools like Nematus (Sennrich et al., 2017b) and nmtpy (Caglayan et al., 2017) built on Theano (Al-Rfou et al., 2016) or xnmt based on DyNet (Neubig et al., 2017) are more research oriented. For instance, Nematus implements several techniques which improve the performance of the system in the ofﬂine setup but which would be prohibitively slow in t"
W18-1816,W17-7508,1,0.784248,"echniques. Libovick´y and Helcl (2017) published interpretable attention combination strategies for multi-source sequence-to-sequence learning tasks, such as multimodal translation. These combination strategies allow to explicitly model the different importance of the source sequences in each step during the decoding. Bastings et al. (2017) used Neural Monkey to develop a new convolutional architecture for encoding the input sentences using dependency trees. This syntax-aware source language representation brings a consistent improvement over the attentive sequence-to-sequence model baseline. Kocmi and Bojar (2017) published a thorough examination of various embedding initialization strategies, both random and pre-trained. They conclude that improvements from pretrained embeddings are not always beneﬁcial, and that high-quality embeddings can be trained using an initialization with a normal distribution with a small variance or even with zeros. Neural Monkey has also been used in various submissions to shared tasks at the Conference on Machine Translation (WMT). In 2016 and 2017, it was used in submissions for multimodal translation task (Libovick´y et al., 2016; Helcl and Libovick´y, 2017). It was also"
W18-1816,P17-1138,0,0.0193038,"useful features available in Neural Monkey. • Beam Search and Model Ensembles. • Independent Training, Saving and Loading Parts of the Model. • Multi-Task Learning. Experiment conﬁgurations can include deﬁnition of multiple decoders and specify which contribute to the training loss. • Loading Nematus Models. Models that have been trained with Nematus (Sennrich et al., 2017b) can be loaded into Neural Monkey. This can be used either for ﬁne-tuning, domain adaptation, or multi-task learning. • Bandit Learning. Neural Monkey can be also used for bandit learning using expected loss minimization (Kreutzer et al., 2017). In this setup, a pre-trained model is tuned using either a simulated or real user feedback. • Detailed Logging and Visualization. We implement several evaluation metrics that can be used for continuous validation of the models. We also plot the loss, norms of the parameters, and histograms of the parameter gradients to TensorBoard. It can also be used for embeddings visualization. There is also a standalone tool for attention visualization (Rikters et al., 2017). 4 Research Contributions The development of Neural Monkey began in spring 2016 and since that time it has been used in various res"
W18-1816,P17-2031,1,0.854406,"Missing"
W18-1816,W16-2361,1,0.844112,"Missing"
W18-1816,W17-4739,0,0.0838206,"for classiﬁcation or regression. 2 https://github.com/tensorﬂow/models/tree/master/research/slim Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 171 3.3 Additional Features This section summarizes additional useful features available in Neural Monkey. • Beam Search and Model Ensembles. • Independent Training, Saving and Loading Parts of the Model. • Multi-Task Learning. Experiment conﬁgurations can include deﬁnition of multiple decoders and specify which contribute to the training loss. • Loading Nematus Models. Models that have been trained with Nematus (Sennrich et al., 2017b) can be loaded into Neural Monkey. This can be used either for ﬁne-tuning, domain adaptation, or multi-task learning. • Bandit Learning. Neural Monkey can be also used for bandit learning using expected loss minimization (Kreutzer et al., 2017). In this setup, a pre-trained model is tuned using either a simulated or real user feedback. • Detailed Logging and Visualization. We implement several evaluation metrics that can be used for continuous validation of the models. We also plot the loss, norms of the parameters, and histograms of the parameter gradients to TensorBoard. It can also be use"
W18-1816,W17-4756,0,0.0237367,"Missing"
W18-1816,W17-4720,1,0.646194,"Missing"
W18-6325,P07-1108,0,\N,Missing
W18-6325,N16-1101,0,\N,Missing
W18-6325,D16-1163,0,\N,Missing
W18-6325,L16-1561,0,\N,Missing
W18-6325,W17-3204,0,\N,Missing
W18-6325,W17-4733,0,\N,Missing
W18-6325,I17-2050,0,\N,Missing
W18-6325,W17-4715,0,\N,Missing
W18-6325,D18-1549,0,\N,Missing
W18-6325,Y17-1038,0,\N,Missing
W18-6325,W17-4739,0,\N,Missing
W18-6325,P16-1162,0,\N,Missing
W18-6325,P16-1009,0,\N,Missing
W18-6325,W16-3408,0,\N,Missing
W18-6401,W18-6432,1,0.779979,"Missing"
W18-6401,W07-0718,1,0.492999,"Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation. 1 Introduction The Third Conference on Machine Translation (WMT) held at EMNLP 20181 host a number of shared tasks on various aspects of machine translation. This conference builds on twelve previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants we"
W18-6401,W11-2101,1,0.731598,"Missing"
W18-6401,W08-0309,1,0.64499,"Missing"
W18-6401,W10-1703,1,0.498736,"Missing"
W18-6401,W12-3102,1,0.647494,"Missing"
W18-6401,E14-2008,0,0.0242419,"n the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated data. At inference time, translations which are copies of the source are filtered out, replacing them with the output of a very small news-commentary only"
W18-6401,E17-2058,0,0.0576994,"Missing"
W18-6401,W18-6406,0,0.107817,"t (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous y"
W18-6401,W13-2305,1,0.876146,"017 (English) and news 2011 (Chinese). Subwords (BPE) are used for both English and Chinese sentences. 3 Human Evaluation A human evaluation campaign is run each year to assess translation quality and to determine the final ranking of systems taking part in the competition. This section describes how preparation of evaluation data, collection of human assessments, and computation of the official results of the shared task was carried out this year. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and two years ago the evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with relative ranking (RR) and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established in 2016 (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for"
W18-6401,E14-1047,1,0.89975,"Missing"
W18-6401,W18-6407,1,0.888041,"ns Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered pa"
W18-6401,W18-6410,0,0.0609282,"ions, organized into 35 teams are listed in Table 2 and detailed in the rest of this section. Each system did not necessarily appear in all translation tasks. We also included 39 online MT systems (originating from 5 services), which we anonymized as ONLINE -A,B,F,G. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations. 2.3.1 A ALTO (Grönroos et al., 2018) Aalto participated in the constrained condition of the multi-lingual subtrack, with a single system trained to translate from English to both Finnish 3 http://www.yandex.com/ Estonian Research Council institutional research grant IUT20-56: “Computational models of the Estonian Language” 4 5 As of Fall 2011, the proceedings of the European Parliament are no longer translated into all official languages. 273 Europarl Parallel Corpus German ↔ English Czech ↔ English Finnish ↔ English Estonian ↔ English Sentences 1,920,209 646,605 1,926,114 652,944 Words 50,486,398 53,008,851 14,946,399 17,376,43"
W18-6401,D18-1045,0,0.0609466,"Missing"
W18-6401,W11-2123,0,0.0087119,"t sets. The second is a Marian (Junczys-Dowmunt et al., 2018) system ensembling 5 Univ. Edinburgh “bi-deep” and 6 transformer models all trained on the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated da"
W18-6401,E17-3017,1,0.773822,"eriment, right-toleft reranking does not help. Another focus is 277 (SMT) submission for the Finnish morphology test suite (Burlot et al., 2018). given to data filtering through rules, translation model and language model including parallel data and monolingual data. The language model is based the Transformer architecture as well. The final system is trained with four different seeds and mixed data. 2.3.8 2.3.9 JHU (Koehn et al., 2018a) The JHU systems are the result of two relatively independent efforts on German–English language directions and Russian–English, using the Marian and Sockeye (Hieber et al., 2017) neural machine translation toolkits, respectively. The novel contributions are iterative back-translation (for German) and fine-tuning on test sets from prior years (for both languages). HY (Raganato et al., 2018; Hurskainen and Tiedemann, 2017) The University of Helsinki (HY) submitted four systems: HY-AH, HY-NMT, HY-NMT-2 STEP and HY-SMT. 2.3.10 JUCBNMT (Mahata et al., 2018) JUCBNMT is an encoder-decoder sequence-tosequence NMT model with character level encoding. The submission uses preprocessing like tokenization, truecasing and corpus cleaning. Both encoder and decoder use a single LSTM"
W18-6401,P17-4012,0,0.0266435,"ted paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered parallel and filtered back-translated monolingual data. The main contribution is a novel cross-lingual Morfessor (Virpioja et al., 2013) segmentation using cognates extracted from the parallel data. The aim is to improve the consistency of the morphological segmentation. Aalto decode using an ensemble of 3 (et) or 8 (fi) models. 2.3.4 2.3.2 2.3.5 AFRL The CUNI-KOCMI submission focuses on the low-resource language neural machine translation (NMT). The final submission uses a method of transfer learning: the model is pretrained on a related high-resource language (her"
W18-6401,W18-6413,0,0.01703,"he constrained systems, however, the data, taking into account its relatively large size, was not factored. T ENCENT (Wang et al., 2018a) T ENCENT- ENSEMBLE (called TenTrans) is an improved NMT system on Transformer based on self-attention mechanism. In addition to the basic settings of Transformer training, T ENCENTENSEMBLE uses multi-model fusion techniques, multiple features reranking, different segmentation models and joint learning. Additionally, data selection strategies were adopted to fine-tune the trained system, achieving a stable performance improvement. An additional system paper (Hu et al., 2018) describes a non-primary submission. 2.3.29 TILDE 2.3.30 U BIQUS The U BIQUS -NMT system is probably developed by the Ubiqus company (www.ubiqus.com). No further information is available. 2.3.31 UCAM (Stahlberg et al., 2018) UCAM is a generalization of previous work (de Gispert et al., 2017) to multiple architectures. It is a system combination of two Transformer-like models, a recurrent model, a convolutional model, and a phrase-based SMT system. The output is probably dominated by the Transformer, and to some extend by the SMT system. (Pinnis et al., 2018) submitted four systems: TILDE - C -"
W18-6401,W18-6416,1,0.791378,"Missing"
W18-6401,W17-4730,0,0.0118952,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6417,1,0.821949,"as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We held 1 2 http://www.statmt.org/wmt18/ 272 http://statmt.org/wmt18/results.html Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 272–303 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64028 tions are also available for interactive visualization and comparison of diff"
W18-6401,W17-4706,0,0.0132191,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6430,0,0.299738,"of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Part"
W18-6401,W18-6427,0,0.0533796,"Missing"
W18-6401,W18-6428,0,0.0966755,"D U NISOUND U NSUP TARTU Institution Aalto University (Grönroos et al., 2018) Air Force Research Laboratory (Gwinnup et al., 2018) Alibaba Group (Deng et al., 2018) Charles University (Kocmi et al., 2018) Charles University (Popel, 2018) Facebook AI Research (Edunov et al., 2018) Global Tone Communication Technology (Bei et al., 2018) University of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Ca"
W18-6401,W18-6431,0,0.0938702,"nications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implement"
W18-6401,W16-2326,0,0.0278975,"Missing"
W18-6433,W15-1844,0,0.0116739,". We collected sentences containing a coreference link involving a personal pronoun (it) or a relative pronoun (that, which, who, whom, whose). The base sentence remains unchanged. In order to generate the variant, the antecedent noun of the pronoun is then changed to a synonym using WordNet (Miller, 1995): • Czech: MorphoDiTa (Strakov´a et al., 2014) • German: SMOR (Schmid et al., 2004) • Personal pronoun: This cat is cute and I love it. → This dog is cute and I love it. • Finnish: The finnish-analyze-words script7 provided by the Language Bank of Finland8 and based on the Omorfi morphology (Pirinen, 2015) and the HFST toolkit (Lind´en et al., 2011) • Relative pronoun: The woman who left was angry. → The man who left was angry. In the output of the MT system, we are then able to locate the antecedent of the pronoun by looking for the only noun that differs between the base and variant translations (namely, the translation of cat/woman in the base and dog/man in the variant). Finally, we check whether the noun and personal • English: MorphoDiTa (Strakov´a et al., 2014) 7 http://urn.fi/urn:nbn:fi: lb-2018041701 8 https://www.kielipankki.fi/ 549 pronoun bear the same gender.9 We also check number"
W18-6433,W17-4705,1,0.757878,".fi francois.yvon@limsi.fr Abstract about systems performance than just one overall number (even if it correlates well with human judgement). Evaluation metrics that focus on various aspects of the translation, such as syntax or morphology, rather than on general translation quality, have thus seen renewed interest. This interest has spurred the inclusion of additional test suites into the WMT 2018 news translation task. Progress in the quality of machine translation output calls for new automatic evaluation procedures and metrics. In this paper, we extend the Morpheval protocol introduced by Burlot and Yvon (2017) for the English-toCzech and English-to-Latvian translation directions to three additional language pairs, and report its use to analyze the results of WMT 2018’s participants for these language pairs. Considering additional, typologically varied source and target languages also enables us to draw some generalizations regarding this morphology-oriented evaluation procedure. 1 Burlot and Yvon (2017, B&Y in the following) present a test suite for evaluating the morphological competence of machine translation systems. They provide a set of sentence pairs in the source language that differ by one"
W18-6433,schmid-etal-2004-smor,0,0.0605379,"Missing"
W18-6433,P16-1162,0,0.017792,"th feature types, the variants are created through some type of transformation that is supposed to be invariant with respect to target morphology. For the consistency features, this transformation is semantic (based on the hyponymy relation), whereas it is morphological for the stability features. Rare word features In the early days of NMT, translation of out-ofvocabulary words was virtually impossible and hampered the performance when compared with SMT. In recent years however, most systems have adopted an approach in which rare words are split into “subwords” during preprocessing (see e.g. Sennrich et al., 2016), such that any unknown word can be composed of various subword chunks during test time. Several subword chunking algorithms with various parameter settings can be used, but their respective performance differences are hard to assess as they typically concern low-frequency words with low impact on general translation quality. Therefore, we introduce two features that specifically deal with low-frequency items. These features are language-independent and do not require the use of a morphological analyzer. For the first feature, we identify large numbers (at least 3 digits) in the English source"
W18-6433,P05-1045,0,0.0339138,"set of contrasts that can be triggered in the source language and evaluated in the target language; As source corpora, we use the English News2007 and 2008 corpora (for EN-CS and ENDE), the English News-2007 corpus (for ENFI), and SETIMES2 (for TR-EN). In order to detect the source features, the corpora are annotated using TreeTagger (Schmid, 1994) and/or CoreNLP (Manning et al., 2014) (for English), or an Apertium (Forcada et al., 2011) morphological analyser (for Turkish). For the named entities feature used in EN-FI, we additionally annotate the source corpora with the Stanford NER tagger (Finkel et al., 2005). • a procedure to generate contrast pairs from a monolingual source language corpus; • and a procedure to score the target language translations of the contrast pairs. B&Y describe three types of contrasts. Type A contrasts resemble paradigm completion tasks, in which one single morphological feature (number, gender, tense, etc.) is evaluated. The two sentences of a contrast pair only differ in one word (or phrase) and across one feature at a time. Type B contrasts contain somewhat more complicated substitutions that are mainly evaluated in terms of agreement. For example, a contrast pair con"
W18-6433,P14-5003,0,0.0577653,"Missing"
W18-6433,tiedemann-2012-parallel,0,0.0291093,"For instance, the English expression apple juice in the base translates into the German compound Apfelsaft. We modify the word apple and obtain orange juice, which translates into Orangensaft. In the MT output we finally compare both compounds Apfelsaft and Orangensaft and report a success if they have at least one morpheme in common. Here, the common morpheme is -saft. For the test suite generation, we needed a translation dictionary containing compounds on the German side and multi-word expressions on the English side. We gathered all the English-German parallel data we could find on OPUS (Tiedemann, 2012) and removed the data available at the WMT18 News Translation shared task. This resulted in nearly 40M parallel sentences. We obtained a phrase table out of this data using the Moses toolkit (Koehn et al., 2007). We finally extracted from this phrase table a dictionary containing a compound on the German side and several multi-word expressions on the English side (removing punctuation and other noisy tokens). Verb position The test suite is generated by locating complex sentences where (a) the principal clause can be omitted and (b) the subordinate clause leads to a German translation where th"
W18-6433,P07-2045,1,0.0112429,"ut we finally compare both compounds Apfelsaft and Orangensaft and report a success if they have at least one morpheme in common. Here, the common morpheme is -saft. For the test suite generation, we needed a translation dictionary containing compounds on the German side and multi-word expressions on the English side. We gathered all the English-German parallel data we could find on OPUS (Tiedemann, 2012) and removed the data available at the WMT18 News Translation shared task. This resulted in nearly 40M parallel sentences. We obtained a phrase table out of this data using the Moses toolkit (Koehn et al., 2007). We finally extracted from this phrase table a dictionary containing a compound on the German side and several multi-word expressions on the English side (removing punctuation and other noisy tokens). Verb position The test suite is generated by locating complex sentences where (a) the principal clause can be omitted and (b) the subordinate clause leads to a German translation where the verb should be located at the end of the clause. Using CoreNLP annotations, we focus on specific English conjunctions that lead to a verb shift in German, like that → dass, because → weil, etc. In order to gen"
W18-6433,P14-5010,0,0.0375358,"generation: The Morpheval test suites 1. Collect a large number of short sentences (length &lt; 15 words) containing a source feature of interest. A Morpheval test suite according to B&Y consists of three aspects: • the definition of a set of contrasts that can be triggered in the source language and evaluated in the target language; As source corpora, we use the English News2007 and 2008 corpora (for EN-CS and ENDE), the English News-2007 corpus (for ENFI), and SETIMES2 (for TR-EN). In order to detect the source features, the corpora are annotated using TreeTagger (Schmid, 1994) and/or CoreNLP (Manning et al., 2014) (for English), or an Apertium (Forcada et al., 2011) morphological analyser (for Turkish). For the named entities feature used in EN-FI, we additionally annotate the source corpora with the Stanford NER tagger (Finkel et al., 2005). • a procedure to generate contrast pairs from a monolingual source language corpus; • and a procedure to score the target language translations of the contrast pairs. B&Y describe three types of contrasts. Type A contrasts resemble paradigm completion tasks, in which one single morphological feature (number, gender, tense, etc.) is evaluated. The two sentences of"
W18-6450,W11-2101,1,0.77912,"ent (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014a; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. In addition, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on a single screen, where scores for translations had been unfairly penalized if often compared to high quality translations (Bojar et al., 2011). DA therefore employs assessment of individual translations in isolation from other outputs. Translation adequacy is structured as a monolingual assessment of similarity of meaning where the target language reference translation and the MT output are displayed to the human assessor. Assessors rate a given translation by how adequately it expresses the meaning of the reference translation on an analogue scale corresponding to an underlying 0-100 rating scale.5 Large numbers of DA human assessments of translations for all 14 language pairs included in the News Translation Task were collected fr"
W18-6450,W17-4755,1,0.502389,"metric carries out a comparison of MT system output translations and human-produced reference translations to produce a single overall • In Direct Assessment (DA) (Graham et al., 2016), humans assess the quality of a given MT output translation by comparison with a reference translation (as opposed to the source and reference). DA is the new standard used in WMT 1 The availability of a reference translation is the key difference between our task and MT quality estimation, where no reference is assumed. 2 http://www.statmt.org/wmt18/metrics-task. html, starting with Koehn and Monz (2006) up to Bojar et al. (2017) 671 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 671–688 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64077 News Translation Task evaluation, requiring only monolingual evaluators. translations. If the task includes a wide range of systems of varying quality, however, or systems are quite different in nature, this could in some way make the task easier for metrics, with metrics that are more sensitive to certain aspects of MT output performing better."
W18-6450,W14-3333,1,0.881849,"riction in WMT human evaluation prior to the introduction of DA, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT18. In the system-level evaluation, the goal is to assess the quality of translation of an MT system for the whole test set. Our manual scoring method, DA, nevertheless proceeds sentence by sentence, aggregating the final score as described below. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014a; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. In addition, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on a single screen, where scores for translations had been unfairly penalized if often compared to high quality translations (Bojar et al., 2011). DA therefore employs assessment of individual transla"
W18-6450,W18-6432,1,0.880138,"uality. The Metrics Shared Task2 of WMT annually evaluates the performance of automatic machine translation metrics in their ability to provide a substitute for human assessment of translation quality. Again, we keep the two main types of metric evaluation unchanged from the previous years. In system-level evaluation, each metric provides a quality score for the whole translated test set (usually a set of documents, in fact). In segment-level evaluation, a score is assigned by a given metric to every individual sentence. The underlying texts and MT systems come from the News Translation Task (Bojar et al., 2018, denoted as Findings 2018 in the following). The texts were drawn from the news domain and involve translations to/from Chinese (zh), Czech (cs), German (de), Estonian (et), Finnish (fi), Russian (ru), and Turkish (tr), each paired with English, making a total of 14 language pairs. A single form of golden truth of translation quality judgement is used this year: Abstract This paper presents the results of the WMT18 Metrics Shared Task. We asked participants of this task to score the outputs of the MT systems involved in the WMT18 News Translation Task with automatic metrics. We collected scor"
W18-6450,N15-1124,1,0.936055,"Missing"
W18-6450,W14-3348,0,0.0229664,"task. Some details are provided in the system description paper (?). YiSi-1 measures the relative lexical semantic similarity (weighted word embeddings cosine similarity aggregated into n-grams similarity) of the candidate and reference translations, optionally taking the shallow semantic structure (“srl”) into account. YiSi-0 is a degenerate resource-free version using the longest common character substring, instead of word embeddings cosine similarity, to measure the word similarity of the candidate and reference translations. meteor++ meteor++ (Guo et al., 2018) is metric based on Meteor (Denkowski and Lavie, 2014), adding explicing treatment of “copy-words”, i.e. words that are likely to be preserved across all paraphrases of a sentence in a given language. 2.4.6 2.4.9 Baseline Metrics As mentioned by Bojar et al. (2016), Metrics Task occasionally suffers from “loss of knowledge” when successful metrics participate only in one year. We attempt to avoid this by regularly evaluating also a range of “baseline metrics”: RUSE RUSE (Shimanaka et al., 2018) is a perceptron regressor based on three types of sentence embeddings: Infersent, Quick-Thought and Universal Sentence Encoder, designed with the aim to u"
W18-6450,W18-6454,0,0.0421465,"as having statistically significant difference in performance. 6 Due to an error in the write-up for WMT17 (errata to follow), this second change was not properly reflected in the paper, only in the evaluation scripts. 675 Metric Seg-level Sys-level Hybrids • • • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • ⋆ ⊘ ⊘ ⊘ ⊘ BEER BLEND CharacTer ITER meteor++ RUSE UHH_TSKM YiSi-* Participant ILLC – University of Amsterdam (Stanojević and Sima’an, 2015) Tencent-MIG-AI Evaluation & Test Lab (Ma et al., 2017) RWTH Aachen University (Wang et al., 2016a) Jadavpur University (Panja and Naskar, 2018) Peking University (Guo et al., 2018) Tokyo Metropolitan University (Shimanaka et al., 2018) (Duma and Menzel, 2017) NRC (Lo, 2018) Table 2: Participants of WMT18 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. “⋆” indicates that the original ITER system-level scores should be calculated as the micro-average of segment-level scores but we calculate them as simple macro-averaged fo"
W18-6450,W17-4766,0,0.167842,"n error in the write-up for WMT17 (errata to follow), this second change was not properly reflected in the paper, only in the evaluation scripts. 675 Metric Seg-level Sys-level Hybrids • • • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • ⋆ ⊘ ⊘ ⊘ ⊘ BEER BLEND CharacTer ITER meteor++ RUSE UHH_TSKM YiSi-* Participant ILLC – University of Amsterdam (Stanojević and Sima’an, 2015) Tencent-MIG-AI Evaluation & Test Lab (Ma et al., 2017) RWTH Aachen University (Wang et al., 2016a) Jadavpur University (Panja and Naskar, 2018) Peking University (Guo et al., 2018) Tokyo Metropolitan University (Shimanaka et al., 2018) (Duma and Menzel, 2017) NRC (Lo, 2018) Table 2: Participants of WMT18 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. “⋆” indicates that the original ITER system-level scores should be calculated as the micro-average of segment-level scores but we calculate them as simple macro-averaged for the hybrid systems. See the ITER paper for more details. BEER BLEND CharacTer"
W18-6450,W06-3114,0,0.125502,"usual set-up, an automatic metric carries out a comparison of MT system output translations and human-produced reference translations to produce a single overall • In Direct Assessment (DA) (Graham et al., 2016), humans assess the quality of a given MT output translation by comparison with a reference translation (as opposed to the source and reference). DA is the new standard used in WMT 1 The availability of a reference translation is the key difference between our task and MT quality estimation, where no reference is assumed. 2 http://www.statmt.org/wmt18/metrics-task. html, starting with Koehn and Monz (2006) up to Bojar et al. (2017) 671 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 671–688 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64077 News Translation Task evaluation, requiring only monolingual evaluators. translations. If the task includes a wide range of systems of varying quality, however, or systems are quite different in nature, this could in some way make the task easier for metrics, with metrics that are more sensitive to certain aspects of MT"
W18-6450,D14-1020,1,0.8977,"r computation of r for metrics, we compare metrics via the absolute value of a given metric’s correlation with human assessment. Table 4 provides the system-level correlations of metrics evaluating translation of newstest2018 into English while Table 5 provides the same for out-of-English language pairs. The underlying texts are part of the WMT18 News Translation test set (newstest2018) and the underlying MT systems are all MT systems participating in the WMT18 News Translation Task with the exception of a single tr-en system not included in the initial human evaluation run. As recommended by Graham and Baldwin (2014), we employ Williams significance test (Williams, 1959) to identify differences in correlation that are statistically significant. Williams test is a test of significance of a difference in dependent correlations and therefore suitable for evaluation of metrics. Correlations not significantly outperformed by any other metric for the given language pair are highlighted in bold in Tables 4 and 5. Since pairwise comparisons of metrics may be also of interest, e.g. to learn which metrics We run chrF++.py with the parameters -nw 0 -b 3 to obtain the chrF score and with -nw 0 -b 1 to obtain the chrF"
W18-6450,W04-3250,0,0.366364,"Missing"
W18-6450,N16-1001,1,0.948707,", see below for details and references. Section 2 describes our datasets, i.e. the sets of underlying sentences, system outputs, human judgements of translation quality and also participating metrics. Sections 3.1 and 3.2 then provide the results of system and segment-level metric evaluation, respectively. We discuss the results in Section 4. 2 News Task Systems are machine translation systems participating in the WMT18 News Translation Task (see Findings 2018).3 Hybrid Systems are created automatically with the aim of providing a larger set of systems against which to evaluate metrics, as in Graham and Liu (2016). Hybrid systems were created for newstest2018 by randomly selecting a pair of MT systems from all systems taking part in that language pair and producing a single output document by randomly selecting sentences from either of the two systems. In short, we create 10K hybrid MT systems for each language pair. Data This year, we provided the task participants with one test set along with reference translations and outputs of MT systems. Participants were free to choose which language pairs they wanted to participate and whether they reported system-level, segment-level scores or both. 2.1 Exclud"
W18-6450,E06-1031,0,0.123079,"UHH_TSKM (Duma and Menzel, 2017) is a non-trained metric utilizing kernel functions, i.e. methods for efficient calculation of overlap of substructures between the candidate and the reference translations. The metric uses both sequence kernels, applied on the tokenized input data, together with tree kernels, that exploit the syntactic structure of the sentences. Optionally, the match can also be performed for the candidate and a pseudoreference (i.e. a translation by another MT system) or for the source sentence and the • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. To tokenize the sentences, we used the standard tokenizer script as available in Moses toolkit. When tokenizing, we also convert all outputs to lowercase. 7 677 http://www.itl.nist.gov/iad/mig/tools/ 3 Results Since Moses scorer is versioned on Github, we strongly encourage authors of highperforming metrics to add them to Moses scorer, as this will ensure that their metric can be easily included in future tasks. We discuss system-level results for news task systems in Section 3.1. The segment-level results are in Se"
W18-6450,W13-2305,1,0.82848,"sentence length restriction in WMT human evaluation prior to the introduction of DA, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT18. In the system-level evaluation, the goal is to assess the quality of translation of an MT system for the whole test set. Our manual scoring method, DA, nevertheless proceeds sentence by sentence, aggregating the final score as described below. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014a; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. In addition, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on a single screen, where scores for translations had been unfairly penalized if often compared to high quality translations (Bojar et al., 2011). DA therefore employs assessment"
W18-6450,W17-4768,1,0.855089,"Missing"
W18-6450,E14-1047,1,0.86335,"riction in WMT human evaluation prior to the introduction of DA, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT18. In the system-level evaluation, the goal is to assess the quality of translation of an MT system for the whole test set. Our manual scoring method, DA, nevertheless proceeds sentence by sentence, aggregating the final score as described below. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014a; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. In addition, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on a single screen, where scores for translations had been unfairly penalized if often compared to high quality translations (Bojar et al., 2011). DA therefore employs assessment of individual transla"
W18-6450,W16-2342,0,0.312471,"sue that shorter translations normally achieve lower TER. Similarly to other character-level metrics, CharacTer is applied to non-tokenized outputs and references, which also holds for this year’s submission. This year tokenization was carried out for en-ru hypotheses and reference before calculating the scores, since this results in large improvements in terms of correlations. For other language pairs a tokenizer was not used for pre-processing. A python library was used for calculating the Levenshtein distance, so that the metric is now about 7 times faster than before. CharacTer CharacTer (Wang et al., 2016b; Wang et al., 2016a), identical to the 2016 setup, is a character-level metric inspired by the commonly applied translation edit rate (TER). It is defined as the minimum number of character edits required to adjust a hypothesis, until it completely matches the reference, normalized by the length of the hypothesis sentence. CharacTer calculates the characterlevel edit distance while performing the shift edit on word level. Unlike the strict matching criterion in TER, a hypothesis word is considered to match a reference word and could be shifted, if the edit distance between them is below a th"
W18-6450,W13-2202,1,0.624348,"Sentence Encoder, designed with the aim to utilize global sentence information that cannot be captured by local features based on character or word n-grams. The sentence embeddings come from pre-trained models and the regression itself is trained on past manual judgements in WMT shared tasks. 2.4.7 • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 that is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Macháček and Bojar, 2013). UHH_TSKM UHH_TSKM (Duma and Menzel, 2017) is a non-trained metric utilizing kernel functions, i.e. methods for efficient calculation of overlap of substructures between the candidate and the reference translations. The metric uses both sequence kernels, applied on the tokenized input data, together with tree kernels, that exploit the syntactic structure of the sentences. Optionally, the match can also be performed for the candidate and a pseudoreference (i.e. a translation by another MT system) or for the source sentence and the • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER"
W18-6450,W18-6455,0,0.047048,"R). It is defined as the minimum number of character edits required to adjust a hypothesis, until it completely matches the reference, normalized by the length of the hypothesis sentence. CharacTer calculates the characterlevel edit distance while performing the shift edit on word level. Unlike the strict matching criterion in TER, a hypothesis word is considered to match a reference word and could be shifted, if the edit distance between them is below a threshold value. The Levenshtein distance between the reference and 676 2.4.4 ITER candidate back-translated into the source language. ITER (Panja and Naskar, 2018) is an improved Translation Edit/Error Rate (TER) metric. In addition to the basic edit operations in TER (insertion, deletion, substitution and shift), ITER also allows stem matching and uses optimizable edit costs and better normalization. Note that for segment-level evaluation, we reverse the sign of the score, so that better translations get higher scores. For systemlevel confidence, we calculate the system-level scores for hybrids systems slightly differently than the original ITER definition would require. We use the unweighted arithmetic average of segment-level scores (macro-average) w"
W18-6450,P02-1040,0,0.104437,"ful metrics participate only in one year. We attempt to avoid this by regularly evaluating also a range of “baseline metrics”: RUSE RUSE (Shimanaka et al., 2018) is a perceptron regressor based on three types of sentence embeddings: Infersent, Quick-Thought and Universal Sentence Encoder, designed with the aim to utilize global sentence information that cannot be captured by local features based on character or word n-grams. The sentence embeddings come from pre-trained models and the regression itself is trained on past manual judgements in WMT shared tasks. 2.4.7 • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 that is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Macháček and Bojar, 2013). UHH_TSKM UHH_TSKM (Duma and Menzel, 2017) is a non-trained metric utilizing kernel functions, i.e. methods for efficient calculation of overlap of substructures between the candidate and the reference translations. The metric uses both sequence kernels, applied on the tokenized input data, together with tree kernels, that"
W18-6450,W15-3049,0,0.255195,"Missing"
W18-6450,W17-4770,0,0.11381,"Missing"
W18-6450,W18-6456,0,0.216819,"n performance. 6 Due to an error in the write-up for WMT17 (errata to follow), this second change was not properly reflected in the paper, only in the evaluation scripts. 675 Metric Seg-level Sys-level Hybrids • • • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • ⋆ ⊘ ⊘ ⊘ ⊘ BEER BLEND CharacTer ITER meteor++ RUSE UHH_TSKM YiSi-* Participant ILLC – University of Amsterdam (Stanojević and Sima’an, 2015) Tencent-MIG-AI Evaluation & Test Lab (Ma et al., 2017) RWTH Aachen University (Wang et al., 2016a) Jadavpur University (Panja and Naskar, 2018) Peking University (Guo et al., 2018) Tokyo Metropolitan University (Shimanaka et al., 2018) (Duma and Menzel, 2017) NRC (Lo, 2018) Table 2: Participants of WMT18 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. “⋆” indicates that the original ITER system-level scores should be calculated as the micro-average of segment-level scores but we calculate them as simple macro-averaged for the hybrid systems. See the ITER paper for more detai"
W18-6450,2006.amta-papers.25,0,0.103061,"ter (Macháček and Bojar, 2013). UHH_TSKM UHH_TSKM (Duma and Menzel, 2017) is a non-trained metric utilizing kernel functions, i.e. methods for efficient calculation of overlap of substructures between the candidate and the reference translations. The metric uses both sequence kernels, applied on the tokenized input data, together with tree kernels, that exploit the syntactic structure of the sentences. Optionally, the match can also be performed for the candidate and a pseudoreference (i.e. a translation by another MT system) or for the source sentence and the • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. To tokenize the sentences, we used the standard tokenizer script as available in Moses toolkit. When tokenizing, we also convert all outputs to lowercase. 7 677 http://www.itl.nist.gov/iad/mig/tools/ 3 Results Since Moses scorer is versioned on Github, we strongly encourage authors of highperforming metrics to add them to Moses scorer, as this will ensure that their metric can be easily included in future tasks. We discuss system-level results for news task systems in Section"
W18-6450,W15-3050,0,0.0885999,"Missing"
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-5302,W17-4755,1,0.837587,"al”) for some language pairs and reference-free (or “bilingual”) for others.3 Due to these different types of golden truth collection, reference-based language pairs are in a closer match with the standard referencebased metrics, while the reference-free language pairs are better fit for the “QE as a metric” subtask. Note that system-level manual scores are different than those of the segment-level. Since for segment-level evaluation, collecting enough DA judgements for each segment is infeasible, so we resort to converting DA judgements to 2.3.2 Segment-level Golden Truth: daRR Starting from Bojar et al. (2017), when WMT fully switched to DA, we had to come up with a solid golden standard for segment-level judgements. Standard DA scores are reliable only when averaged over sufficient number of judgments.4 Fortunately, when we have at least two DA scores for translations of the same source input, it is possible to convert those DA scores into a relative ranking judgement, if the difference in DA scores allows conclusion that one translation is better than the other. In the following, we denote these re-interpreted DA judgements as “daRR”, to distinguish it clearly from the relative ranking (“RR”) gol"
W19-5302,P17-1152,0,0.0220899,"metric, which builds upon CDER. It is defined as the minimum number of operations of an extension to the conventional edit distance containing a “jump” operation. The edit distance operations (insertions, deletions and substitutions) are performed at the character level and jumps are performed when a blank space is reached. Furthermore, the coverage of multiple characters in the hypothesis is penalised by the introduction of a coverage penalty. The sum of the length of the reference and the coverage penalty is used as the normalisation term. 4.5 ESIM Enhanced Sequential Inference Model (ESIM; Chen et al., 2017; Mathur et al., 2019) is a neural model proposed for Natural Language Inference that has been adapted for MT evaluation. It uses cross-sentence attention and sentence matching heuristics to generate a representation of the translation and the reference, which is fed to a feedforward regressor. The metric is trained on singly-annotated Direct Assessment data that has been collected for evaluating WMT systems: all WMT 2018 toEnglish data for the to-English language pairs, and all WMT 2018 data for all other language pairs. 4.7 Meteor++_2.0 (syntax), Meteor++_2.0 (syntax+copy) Meteor++ 2.0 (Guo"
W19-5302,W19-5356,0,0.253083,", 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − http://github.com/kokeman/PReP − − http://github.com/poethan/LEPOR http://github.com/poethan/LEPOR http://github.com/nitikam/mteval-in-context http://github.com/rwth-i6/ExtendedEditDistance http://github.com/rwth-i6/CharacTER http://github.com/nitikam/mteval-in-context http://github.com/stanojevic/beer http://github.com/mjpost/sa"
W19-5302,W14-3348,0,0.0631558,"oposed for Natural Language Inference that has been adapted for MT evaluation. It uses cross-sentence attention and sentence matching heuristics to generate a representation of the translation and the reference, which is fed to a feedforward regressor. The metric is trained on singly-annotated Direct Assessment data that has been collected for evaluating WMT systems: all WMT 2018 toEnglish data for the to-English language pairs, and all WMT 2018 data for all other language pairs. 4.7 Meteor++_2.0 (syntax), Meteor++_2.0 (syntax+copy) Meteor++ 2.0 (Guo and Hu, 2019) is a metric based on Meteor (Denkowski and Lavie, 2014) that takes syntactic-level paraphrase knowledge into consideration, where paraphrases may sometimes be skip-grams. i.e. (protect...from, protect...against). As the original Meteor-based metrics only pay attention to consecutive string matching, 68 they perform badly when reference-hypothesis pairs contain skip n-gram paraphrases. Meteor++ 2.0 extracts the knowledge from the Paraphrase Database (PPDB; Bannard and Callison-Burch, 2005) and integrates it into Meteor-based metrics. 4.8 aggregating the idf-weighted lexical semantic similarities based on the contextual embeddings extracted from BER"
W19-5302,N19-1423,0,0.0185514,"ntextual embeddings extracted from BERT to evaluate the crosslingual lexical semantic similarity between the input and MT output. Like YiSi-1, YiSi-2 can exploit shallow semantic structures as well (denoted as YiSi-2_srl). PReP PReP (Yoshimura et al., 2019) is a method for filtering pseudo-references to achieve a good match with a gold reference. At the beginning, the source sentence is translated with some off-the-shelf MT systems to create a set of pseudo-references. (Here the MT systems were Google Translate and Microsoft Bing Translator.) The pseudoreferences are then filtered using BERT (Devlin et al., 2019) fine-tuned on the MPRC corpus (Dolan and Brockett, 2005), estimating the probability of the paraphrase between gold reference and pseudo-references. Thanks to the high quality of the underlying MT systems, a large portion of their outputs is indeed considered as a valid paraphrase. The final metric score is calculated simply with SentBLEU with these multiple references. 4.9 4.11 QE Systems In addition to the submitted standard metrics, 10 quality estimation systems were submitted to the “QE as a Metric” track. The submitted QE systems are evaluated in the same settings as metrics to facilitat"
W19-5302,P05-1074,0,0.0809195,", and all WMT 2018 data for all other language pairs. 4.7 Meteor++_2.0 (syntax), Meteor++_2.0 (syntax+copy) Meteor++ 2.0 (Guo and Hu, 2019) is a metric based on Meteor (Denkowski and Lavie, 2014) that takes syntactic-level paraphrase knowledge into consideration, where paraphrases may sometimes be skip-grams. i.e. (protect...from, protect...against). As the original Meteor-based metrics only pay attention to consecutive string matching, 68 they perform badly when reference-hypothesis pairs contain skip n-gram paraphrases. Meteor++ 2.0 extracts the knowledge from the Paraphrase Database (PPDB; Bannard and Callison-Burch, 2005) and integrates it into Meteor-based metrics. 4.8 aggregating the idf-weighted lexical semantic similarities based on the contextual embeddings extracted from BERT and optionally incorporating shallow semantic structures (denoted as YiSi-1_srl). YiSi-0 is the degenerate version of YiSi-1 that is ready-to-deploy to any language. It uses longest common character substring to measure the lexical similarity. YiSi-2 is the bilingual, reference-less version for MT quality estimation, which uses the contextual embeddings extracted from BERT to evaluate the crosslingual lexical semantic similarity bet"
W19-5302,I05-5002,0,0.0979701,"the crosslingual lexical semantic similarity between the input and MT output. Like YiSi-1, YiSi-2 can exploit shallow semantic structures as well (denoted as YiSi-2_srl). PReP PReP (Yoshimura et al., 2019) is a method for filtering pseudo-references to achieve a good match with a gold reference. At the beginning, the source sentence is translated with some off-the-shelf MT systems to create a set of pseudo-references. (Here the MT systems were Google Translate and Microsoft Bing Translator.) The pseudoreferences are then filtered using BERT (Devlin et al., 2019) fine-tuned on the MPRC corpus (Dolan and Brockett, 2005), estimating the probability of the paraphrase between gold reference and pseudo-references. Thanks to the high quality of the underlying MT systems, a large portion of their outputs is indeed considered as a valid paraphrase. The final metric score is calculated simply with SentBLEU with these multiple references. 4.9 4.11 QE Systems In addition to the submitted standard metrics, 10 quality estimation systems were submitted to the “QE as a Metric” track. The submitted QE systems are evaluated in the same settings as metrics to facilitate comparison. Their descriptions can be found in the Find"
W19-5302,C12-2044,0,0.187067,"Missing"
W19-5302,D14-1020,1,0.886024,"sment. 5.1.1 an adaptation of the conventional Kendall’s Tau coefficient. Since we do not have a total order ranking of all translations, it is not possible to apply conventional Kendall’s Tau (Graham et al., 2015). Our Kendall’s Tau-like formulation, τ , is as follows: System-Level Results Tables 3, 4 and 5 provide the system-level correlations of metrics evaluating translation of newstest2019. The underlying texts are part of the WMT19 News Translation test set (newstest2019) and the underlying MT systems are all MT systems participating in the WMT19 News Translation Task. As recommended by Graham and Baldwin (2014), we employ Williams significance test (Williams, 1959) to identify differences in correlation that are statistically significant. Williams test is a test of significance of a difference in dependent correlations and therefore suitable for evaluation of metrics. Correlations not significantly outperformed by any other metric for the given language pair are highlighted in bold in Tables 3, 4 and 5. Since pairwise comparisons of metrics may be also of interest, e.g. to learn which metrics significantly outperform the most widely employed metric BLEU, we include significance test results for ever"
W19-5302,2013.mtsummit-posters.3,0,0.0255629,"ystem-level and not at segmentlevel. In this submitted baseline version, hLEPOR_baseline was not tuned for each language pair separately but the default weights were applied across all submitted language pairs. Further improvements can be achieved by tuning the weights according to the development data, adding morphological information and applying n-gram factor scores into it (e.g. part-of-speech, n-gram precision and n-gram recall that were added into LEPOR in WMT13.). The basic model factors and further development with parameters setting were described in the paper (Han et al., 2012) and (Han et al., 2013). For sentence-level score, only hLEPORa_baseline was submitted with scores calculated as the weighted harmonic mean of all the designed factors using default parameters. For system-level score, both hLEPORa_baseline and hLEPORb_baseline were submitted, where hLEPORa_baseline is the the average score of all sentence-level scores, and hLEPORb_baseline is calculated via the same sentence-level hLEPOR equation but replacing each factor value with its system-level counterpart. EED EED (Stanchev et al., 2019) is a characterbased metric, which builds upon CDER. It is defined as the minimum number of"
W19-5302,W13-2305,1,0.868874,"and segment-level evaluation (Section 5.2). 2.3.1 System-level Golden Truth: DA Overall, the results are based on 233 systems across 18 language pairs.2 2.3 For the system-level evaluation, the collected continuous DA scores, standardized for each annotator, are averaged across all assessed segments for each MT system to produce a scalar rating for the system’s performance. The underlying set of assessed segments is different for each system. Thanks to the fact that the system-level DA score is an average over many judgments, mean scores are consistent and have been found to be reproducible (Graham et al., 2013). For more details see Findings 2019. Manual Quality Assessment Direct Assessment (DA, Graham et al., 2013, 2014a, 2016) was employed as the source of the “golden truth” to evaluate metrics again this year. The details of this method of human evaluation are provided in Findings 2019. The basis of DA is to collect a large number of quality assessments (a number on a scale of 1–100, i.e. effectively a continuous scale) for the outputs of all MT systems. These scores are then standardized per annotator. In the past years, the underlying manual scores were reference-based (human judges had access"
W19-5302,W04-3250,0,0.292959,"Missing"
W19-5302,E14-1047,1,0.934062,"Missing"
W19-5302,W06-3114,0,0.318343,"Missing"
W19-5302,2003.mtsummit-papers.32,0,0.308746,"Missing"
W19-5302,N16-1001,1,0.928052,"tandard DA scores are reliable only when averaged over sufficient number of judgments.4 Fortunately, when we have at least two DA scores for translations of the same source input, it is possible to convert those DA scores into a relative ranking judgement, if the difference in DA scores allows conclusion that one translation is better than the other. In the following, we denote these re-interpreted DA judgements as “daRR”, to distinguish it clearly from the relative ranking (“RR”) golden truth used in the past years.5 2 4 This year, we do not use the artificially constructed “hybrid systems” (Graham and Liu, 2016) because the confidence on the ranking of system-level metrics is sufficient even without hybrids. 3 Specifically, the reference-based language pairs were those where the anticipated translation quality was lower or where the manual judgements were obtained with the help of anonymous crowdsourcing. Most of these cases were translations into English (fien, gu-en, kk-en, lt-en, ru-en and zh-en) and then the language pairs not involving English (de-cs, de-fr and fr-de). The reference-less (bilingual) evaluations were those where mainly MT researchers themselves were involved in the annotations: e"
W19-5302,E06-1031,0,0.112399,"same source input sentence; “DA pairs” is the number of all possible pairs of translations of the same source input resulting from “DA>1”; and “daRR” is the number of DA pairs with an absolute difference in DA scores greater than the 25 percentage point margin. BLEU and NIST The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using mteval-v13a.pl8 from the OpenMT Evaluation Campaign. The tool includes its own tokenization. We run mteval with the flag --international-tokenization.9 TER, WER, PER and CDER. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. We used the standard tokenizer script as available in Moses toolkit for tokenization. From the complete set of human assessments collected for the News Translation Task, all possible pairs of DA judgements attributed to distinct translations of the same source were converted into daRR better/worse judgements. Distinct translations of the same source input whose DA scores fell within 25 percentage points (which could have been deemed equal quality) were omitted from the evaluation of segment-level metrics. Conversion"
W19-5302,W19-5358,0,0.455314,"Missing"
W19-5302,W14-3333,1,0.926699,"Missing"
W19-5302,W18-6450,1,0.687061,"Missing"
W19-5302,N15-1124,1,0.828388,"ct translations of the same source input, s1 and s2 , is counted as a concordant (Conc) or disconcordant (Disc) pair is defined by the following matrix: s1 &lt; s2 s1 = s2 s1 > s2 s1 &lt; s2 Conc − Disc Metric s1 = s2 Disc − Disc s1 > s2 Disc − Conc In the notation of Macháček and Bojar (2014), this corresponds to the setup used in WMT12 (with a different underlying method of manual judgements, RR): Segment-Level Evaluation Segment-level evaluation relies on the manual judgements collected in the News Translation Task evaluation. This year, again we were unable to follow the methodology outlined in Graham et al. (2015) for evaluation of segment-level metrics because the sampling of sentences did not provide sufficient number of assessments of the same segment. We therefore convert pairs of DA scores for competing translations to daRR better/worse preferences as described in Section 2.3.2. We measure the quality of metrics’ segmentlevel scores against the daRR golden truth using a Kendall’s Tau-like formulation, which is Human WMT12 &lt; = > &lt; 1 X -1 Metric = > -1 -1 X X -1 1 The key differences between the evaluation used in WMT14–WMT16 and evaluation used in WMT17–WMT19 were (1) the move from RR to daRR and ("
W19-5302,W14-3336,1,0.714999,"|Discordant| (2) Human where Concordant is the set of all human comparisons for which a given metric suggests the same order and Discordant is the set of all human comparisons for which a given metric disagrees. The formula is not specific with respect to ties, i.e. cases where the annotation says that the two outputs are equally good. The way in which ties (both in human and metric judgement) were incorporated in computing Kendall τ has changed across the years of WMT Metrics Tasks. Here we adopt the version used in WMT17 daRR evaluation. For a detailed discussion on other options, see also Macháček and Bojar (2014). Whether or not a given comparison of a pair of distinct translations of the same source input, s1 and s2 , is counted as a concordant (Conc) or disconcordant (Disc) pair is defined by the following matrix: s1 &lt; s2 s1 = s2 s1 > s2 s1 &lt; s2 Conc − Disc Metric s1 = s2 Disc − Disc s1 > s2 Disc − Conc In the notation of Macháček and Bojar (2014), this corresponds to the setup used in WMT12 (with a different underlying method of manual judgements, RR): Segment-Level Evaluation Segment-level evaluation relies on the manual judgements collected in the News Translation Task evaluation. This year, agai"
W19-5302,W19-5357,0,0.153853,"Missing"
W19-5302,W13-2202,1,0.696592,"xtracting daRR judgements from all possible pairs of translations of the same source input. We see that only German-French and esp. French-German can suffer from insufficient number of these simulated pairwise comparisons. The daRR judgements serve as the golden standard for segment-level evaluation in WMT19. 3 Baseline Metrics In addition to validating popular metrics, including baselines metrics serves as comparison and prevents “loss of knowledge” as mentioned by Bojar et al. (2016). Moses scorer6 is one of the MT evaluation tools that aggregated several useful metrics over the time. Since Macháček and Bojar (2013), we have been using Moses scorer to provide most of the baseline metrics and kept encouraging authors of well-performing MT metrics to include them in Moses scorer.7 The baselines we report are: newstest2019 Table 1: Number of judgements for DA converted to daRR data; “DA>1” is the number of source input sentences in the manual evaluation where at least two translations of that same source input segment received a DA judgement; “Ave” is the average number of translations with at least one DA judgement available for the same source input sentence; “DA pairs” is the number of all possible pairs"
W19-5302,W16-2342,0,0.157687,"usch et al. (2003) Leusch et al. (2006) Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi htt"
W19-5302,P19-1269,0,0.298995,"ch et al. (2006) Snover et al. (2006) Leusch et al. (2003) Leusch et al. (2006) Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiS"
W19-5302,P02-1040,0,0.118027,"mber of judgements for DA converted to daRR data; “DA>1” is the number of source input sentences in the manual evaluation where at least two translations of that same source input segment received a DA judgement; “Ave” is the average number of translations with at least one DA judgement available for the same source input sentence; “DA pairs” is the number of all possible pairs of translations of the same source input resulting from “DA>1”; and “daRR” is the number of DA pairs with an absolute difference in DA scores greater than the 25 percentage point margin. BLEU and NIST The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using mteval-v13a.pl8 from the OpenMT Evaluation Campaign. The tool includes its own tokenization. We run mteval with the flag --international-tokenization.9 TER, WER, PER and CDER. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. We used the standard tokenizer script as available in Moses toolkit for tokenization. From the complete set of human assessments collected for the News Translation Task, all possible pairs of DA judgements attributed t"
W19-5302,W19-5410,0,0.0820879,"dit distance, edit types edit distance, edit types edit distance, edit types character n-grams character n-grams n-grams n-grams Features yes yes ? ? ? ? yes yes Learned? Papineni et al. (2002) Doddington (2002) Leusch et al. (2006) Snover et al. (2006) Leusch et al. (2003) Leusch et al. (2006) Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et"
W19-5302,W12-3116,0,0.224116,"similarity semantic similarity n-grams n-grams n-grams Levenshtein distance edit distance, edit types edit distance, edit types edit distance, edit types character n-grams character n-grams n-grams n-grams Features yes yes ? ? ? ? yes yes Learned? Papineni et al. (2002) Doddington (2002) Leusch et al. (2006) Snover et al. (2006) Leusch et al. (2003) Leusch et al. (2006) Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu,"
W19-5302,W19-5360,0,0.174611,"Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − http://github.com/kokeman/PReP − − http://github.com/poethan/LEPOR http://github.com/poethan/LEPOR http://github.com/nitikam/mteval-in-context http://github.com/rwth-i6/ExtendedEditDistance http://github.com/rwth-i6/CharacTER http://github.com/nitikam/mteval-in-context http://github.com/"
W19-5302,W15-3049,0,0.119189,"etrics smoothed version of BLEU for scoring at the segment-level. We used the standard tokenizer script as available in Moses toolkit for tokenization. Table 2 lists the participants of the WMT19 Shared Metrics Task, along with their metrics and links to the source code where available. We have collected 24 metrics from a total of 13 research groups, with 10 reference-less “metrics” submitted to the joint task “QE as a Metrich” with WMT19 Quality Estimation Task. The rest of this section provides a brief summary of all the metrics that participated. chrF and chrF+. The metrics chrF and chrF+ (Popović, 2015, 2017) are computed using their original Python implementation, see Table 2. We ran chrF++.py with the parameters -nw 0 -b 3 to obtain the chrF score and with -nw 1 -b 3 to obtain the chrF+ score. Note that chrF intentionally removes all spaces before matching the n-grams, detokenizing the segments but also concatenating words.10 4.1 BEER BEER (Stanojević and Sima’an, 2015) is a trained evaluation metric with a linear model that combines sub-word feature indicators (character n-grams) and global word order features (skip bigrams) to achieve a language agnostic and fast to compute evaluation m"
W19-5302,W17-4770,0,0.0695998,"Missing"
W19-5302,W18-6319,0,0.0768405,"ame source were converted into daRR better/worse judgements. Distinct translations of the same source input whose DA scores fell within 25 percentage points (which could have been deemed equal quality) were omitted from the evaluation of segment-level metrics. Conversion of scores in this way produced a large set of daRR judgements for all language pairs, sentBLEU. The metric sentBLEU is computed using the script sentence-bleu, a part of the Moses toolkit. It is a 6 https://github.com/moses-smt/mosesdecoder/ blob/master/mert/evaluator.cpp 7 If you prefer standard BLEU, we recommend sacreBLEU (Post, 2018a), found at https://github.com/ mjpost/sacreBLEU. 8 http://www.itl.nist.gov/iad/mig/tools/ 9 International tokenization is found to perform slightly better (Macháček and Bojar, 2013). rely on judgements collected from known-reliable volunteers and crowd-sourced workers who passed DA’s quality control mechanism. Any inconsistency that could arise from reliance on DA judgements collected from low quality crowd-sourcing is thus prevented. 65 66 Baselines Metrics IBM1-morpheme IBM1-pos4gram LP LASIM UNI UNI+ USFD USFD-TL YiSi-2 YiSi-2_srl BEER BERTr characTER EED ESIM LEPORa LEPORb Meteor++_2.0 ("
W19-5302,2006.amta-papers.25,0,0.285519,"least one DA judgement available for the same source input sentence; “DA pairs” is the number of all possible pairs of translations of the same source input resulting from “DA>1”; and “daRR” is the number of DA pairs with an absolute difference in DA scores greater than the 25 percentage point margin. BLEU and NIST The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using mteval-v13a.pl8 from the OpenMT Evaluation Campaign. The tool includes its own tokenization. We run mteval with the flag --international-tokenization.9 TER, WER, PER and CDER. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. We used the standard tokenizer script as available in Moses toolkit for tokenization. From the complete set of human assessments collected for the News Translation Task, all possible pairs of DA judgements attributed to distinct translations of the same source were converted into daRR better/worse judgements. Distinct translations of the same source input whose DA scores fell within 25 percentage points (which could have been deemed equal quality) were omitted from the evalua"
W19-5302,W19-5359,0,0.144887,"Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − http://github"
W19-5302,W15-3050,0,0.0508599,"Missing"
W19-5323,P07-2045,1,\N,Missing
W19-5323,J03-1002,0,\N,Missing
W19-5323,P12-3005,0,\N,Missing
W19-5323,2005.mtsummit-papers.11,0,\N,Missing
W19-5323,D14-1025,0,\N,Missing
W19-5323,P14-5003,0,\N,Missing
W19-5323,W16-2342,0,\N,Missing
W19-5323,D18-1399,0,\N,Missing
W19-5323,P16-1009,0,\N,Missing
W19-5323,tiedemann-2012-parallel,0,\N,Missing
W19-5323,W11-2123,0,\N,Missing
W19-5323,N13-1073,0,\N,Missing
W19-5337,W19-5301,1,0.839454,"Missing"
W19-5337,N13-1073,0,0.0558634,"ed T2T system achieved an insignificant improvement (+0.1 BLEU) over our last year’s sentence-level T2T system, but applying this system on sentences led to a significant worsening (−0.6 BLEU). We hypothesized that by providing the translation model with larger attendable context, the resulting translations display larger lexical consistency. We could demonstrate it by finding less examples where an English polysemous word is translated to two or more Czech non-synonymous lemmata within one document. To evaluate the hypothesis, we word-aligned the source and target sentences using fast_align (Dyer et al., 2013).5 We then lemmatized the aligned words (both English and Czech) using MorphoDiTa (Straková et al., 2014) and considered all instances where a single English lemma was aligned to at least two Czech lemmata in a single document. Since our focus was on evaluating the difference between non-context and document-level models, we selected only the English lemmata with different number of aligned Czech lemmata in the two types of systems. Two pairs of models were compared: “DocTransformer T2T” vs. “Transformer T2T 2019” and “DocTransformer Marian” vs. “Transformer Marian”. The final pool of examples"
W19-5337,P18-4020,0,0.0759104,"Missing"
W19-5337,D18-2012,0,0.0320416,"mup 20000 --lr-decay-inv-sqrt 20000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --label-smoothing 0.1 --learn-rate 0.0002 --exponential-smoothing 3 Document-Level Systems Our document-level models were created by training on the context-augmented data described in Section 2.2. We used different strategies for document-level decoding in Marian and in T2T. We used the same learning rate as T2T and estimated the number of warmup training steps so the model consumed approximately the same number of sentences as T2T in warmup. Instead of T2T’s default SubwordTextEncoder, we used SentencePiece (Kudo and Richardson, 2018) with its default parameters to obtain a shared vocabulary of 32,000 entries from untokenized training data. We set the maximal sentence length to 150 and decoded with beam size 4. We could not use Adafactor (Shazeer and Stern, 2018) optimizer as in T2T, because it is not implemented in Marian. We used Adam instead. We did not set the batch size manually, but used the --mini-batch-fit parameter to determine the mini-batch size automatically based on sentence lengths to fit the available memory. We estimated the workspace memory to 13,900 MB as the largest possible on our hardware. We shuffled"
W19-5337,D18-1512,0,0.0561563,"Missing"
W19-5337,W18-6424,1,0.812702,"coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we cannot draw any conclusions from this week evidence. 1 Since assessing the performance of documentlevel systems is one of the goals of WMT19 (Barrault et al., 2019), we decided to build NMT systems trained for translation of longer segments than single sentences. In this paper, we describe our five NMT systems submitted to WMT19 English→Czech news translation task (see Table 1). They are based on the Transformer model (Vaswani et al., 2017) and on our submission from WMT18 (Popel, 2018). Our new contributions are (i) adaptation of the baseline single-sentence models to translate multiple adjacent sentences in a document at once, so the Transformer can attend to inter-sentence relations and achieve better document-level translation quality, as was already showed to be effective by Jean et al. (2017); and (ii) reimplementation of our last year’s submission in the Marian framework (Junczys-Dowmunt et al., 2018). Introduction Neural machine translation has reached a point, where the quality of automatic translation measured on isolated sentences is similar on average to the qual"
W19-5337,W18-6319,0,0.0236236,"6 29.20 28.13 0.5474 29.00 27.89 0.5516 Table 4: Automatic evaluation on newstest2019. Significantly different BLEU scores (p < 0.05 bootstrap resampling) are separated by a horizontal line. „lower and upper“. This is considered as standard in Czech formal texts. For Marian, we applied only the conversion of quotation symbols. 4 Results 4.1 Automatic Evaluation Table 4 reports the automatic metrics of our English→Czech systems submitted to WMT2019, plus the best other system – UEdin (Marian system trained by University of Edinburgh). The automatic metrics are calculated using sacreBLEU 1.3.2 (Post, 2018) and their signatures are: • pre-context: sentences which are ignored in the translation and serve only as a context for better translation of the main content, • BLEU+case.mixed+lang.encs+numrefs.1+smooth.exp+tok.13a, • main content: sentences which are used for the final translation, • BLEU+case.lc+lang.encs+numrefs.1+smooth.exp+tok.intl and • post-context: sentences which are ignored, similarly to the pre-context. • chrF2+case.mixed+lang.encs+numchars.6+numrefs.1+space.False. Based on a small dev-set BLEU hyperparameter search, we selected the following length limits: pre-context of up to 2"
W19-5337,P14-5003,0,0.0292515,"Missing"
W19-5337,W18-6312,0,0.0177422,"f our last year’s submission in the Marian framework (Junczys-Dowmunt et al., 2018). Introduction Neural machine translation has reached a point, where the quality of automatic translation measured on isolated sentences is similar on average to the quality of professional human translations. Hassan et al. (2018) report achieving a “human parity” on Chinese→English news translation. Bojar et al. (2018, p. 291) report that our last year’s English→Czech system (Popel, 2018) was evaluated as significantly better (p < 0.05) than the human reference. However, it has been shown (Läubli et al., 2018; Toral et al., 2018) that evaluating the quality of translation of news articles on isolated sentences without the context of the whole document is not sufficient. It can bias the evaluation results because systems that ignore the context are not penalized in the evaluation for these context-related errors; and vice versa: sysThis paper is organized as follows: In Section 2, we describe our training data and its augmentation to overlapping multi-sentence sequences. We describe also the hyper-parameters of our models in the two frameworks. Section 3 follows with a description of the document-level decoding strateg"
W19-5337,W18-1819,0,0.0615439,"use only the data allowed in WMT2018, which does not include CS NewsCrawl 2018 and WikiTitles. All the data were preprocessed, filtered and backtranslated by the same process as in Popel (2018). We selected the originally English part of newstest2016 for validation, following the idea of CZ/nonCZ tuning in Popel (2018), but excluding the CZ tuning because the WMT2019 test set was announced to contain only original English sentences and no translationese. 2.2 2.3 2.3.1 Model Hyper-parameters Tensor2Tensor Our three systems with “T2T” in the name are implemented in the Tensor2Tensor framework (Vaswani et al., 2018), version 1.6.0. The model and training parameters this year are identical to our last year’s (WMT18) submission (Popel, 2018), with just two exceptions: First, we trained on 10 GPUs instead of 8 GPUs, thus using the effective batch size of 29k subwords instead of 23k subwords. Second, we used max_length=200 instead of 150. This means we discard all training sequences longer than 200 subwords. With our 32k joint subword vocabulary, a word contains on average 1.5 subwords. Thus effectively, the sequence-length limit used in T2T training was in most cases lower than 1000 characters – on average"
W19-5337,W18-6401,1,\N,Missing
W19-5352,W16-2354,0,0.0604893,"Missing"
W19-5352,W18-6424,0,0.123633,"dependent unit of human communication. 1 This work does not address in detail errors in coreference, pronoun and gender translation, as these phenomena have been already widely accounted for, e.g. Guillou et al. (2016); Nov´ak (2016). 455 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 455–463 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Data and trained on parallel and back-translated monolingual data. It translates one sentence at a time. CUNI-DocTransf-T2T is a Transformer model following Popel (2018), but trained on WMT19 document-level parallel and monolingual data. During decoding, each document was split into overlapping multi-sentence segments, where only the “middle” sentences in each segment are used for the final translation. CUNI-Transf-T2T is the same system as CUNI-DocTransf-T2T, just applied on separate sentences during decoding. CUNI-DocTransf-Marian is documentlevel trained Transformer in Marian framework following Popel (2018), but finetuned on document-level parallel and monolingual data by translating triples of adjacent sentences at once. If possible, only the middle sent"
W19-5352,C10-2118,0,0.0191112,"oˇze prˇs´ı.). Connectives of adverbial origin have looser positions in some cases;5 they can occur e.g. in the first and second position in the sentence (For me it is easier to not lose a game than to win it, thus I produce better results in stronger tournaments. Both umpires claimed that they were unsighted, and were thus forced to give Somny the benefit of the doubt. / Pro mˇe je snazˇs´ı neztratit 5 6.1.3 Alternative lexicalizations of discourse connectives (AltLexes) In addition to discourse connectives, discourse relations can also be expressed by their alternatives called AltLexes, see Prasad et al. (2010). Alternative lexicalizations of connectives are often multiword phrases such as for this reason. Since these cohesive structures often have an idiomatic character and they generally do not achieve such degree of grammaticalization as connectives, their forms in languages may vary to a large extent. For example, the AltLex for this reason is not translated into Czech literary as pro tento d˚uvod ‘lit. for this reason’, but as z tohoto d˚uvodu ‘lit. from this reason’. Other examples of English AltLexes are that’s all, that’s largely due to, attributed that to, it will cause etc. A list of AltLe"
W19-5352,W16-2345,0,0.0262679,"rmation structure), discourse connectives and alternative lexicalizations of connectives.1 We assume that translation systems might have difficulties with these phenomena, as they are related to the previous context and go beyond (or are affected by the phenomena across) the sentence boundary. In this way, they contribute to the overall coherence of the text that should (as a whole) function as an independent unit of human communication. 1 This work does not address in detail errors in coreference, pronoun and gender translation, as these phenomena have been already widely accounted for, e.g. Guillou et al. (2016); Nov´ak (2016). 455 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 455–463 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Data and trained on parallel and back-translated monolingual data. It translates one sentence at a time. CUNI-DocTransf-T2T is a Transformer model following Popel (2018), but trained on WMT19 document-level parallel and monolingual data. During decoding, each document was split into overlapping multi-sentence segments, where only the “middle” sentences in each segment are"
W19-5352,P09-1076,0,0.0300635,"cal structures and anaphoric relations. In addition, the Czech part is automatically tagged and parsed as surface-syntactic dependency trees on the analytical layer. The English part also preserves the original phrase-structure annotation of the Penn Treebank. Also, the annotation of discourse relations, connectives and Altlexes from the Penn Discourse Treebank was extracted and added to our PCEDT dataset. 3 4 Annotation Design The 101 PCEDT documents selected for translation and manual evaluation belong to the “essay” and “letter” genre labels according to the classification of PDTB given in Webber (2009). At the same time, the selected texts have a length of 20– 50 sentences. These documents were submitted as an additional test suite for Machine Translation of News shared task at the WMT 2019. Because we are interested in document-level translation and the effect of context on the translation, we only selected documents with cross-sentence discourse relations. We have created a simple annotation interface (see Figure 1), which allows the annotator to mark the items that were translated correctly. Specifically, several types of cross-sentence discourse relations are considered on the source si"
W19-5352,J93-2004,0,0.0657277,"on a selection of 101 documents from the parallel Prague Czech-English Dependency Treebank (PCEDT, Hajiˇc et al. (2012)), and we also used discourse annotations of the same texts in the Penn Discourse Treebank 3.0 (PDTB, for details see Webber et al. (2019)). 2.1 Prague Czech-English Dependency Treebank The Prague Czech-English Dependency Treebank is a parallel corpus consisting of English original texts and their Czech translations. The PCEDT contains 1.2 million running words in almost 50,000 sentences in each part. The English texts come from the Penn Treebank (Wall Street Journal Section; Marcus et al., 1993). They were manually translated into Czech by trained linguists without any support of MT and proofread. The PCEDT is manually annotated on the tectogrammatical (deep-syntactic) layer in both languages. The sentences are represented by dependency structures of content words. The nodes in the tree structures are provided with syntactico-semantic labels as, e.g., predicate, actor, patiens, addressee or locative. Also, the valency frames of verbs (argument structure) are captured, as well as elliptical structures and anaphoric relations. In addition, the Czech part is automatically tagged and par"
W19-5352,hajic-etal-2012-announcing,1,\N,Missing
W19-5352,W18-6401,1,\N,Missing
W19-5355,W16-2342,0,0.0700587,"ed only on monolingual source and target texts, optionally using a small parallel development set of a few thousand sentence pairs. Our evaluation also includes several anonymized online systems (“online-. . . ”) the internals of which are not known. These online systems could in principle include our test suite as part of their training data. The number of evaluated documents and MT systems for each examined language pair is in Table 2. 3 Automatic Evaluation For automatic evaluation, we use several of common MT evaluation metrics (Papineni et al., 2002; Popovi´c, 2015; Leusch and Ney, 2008; Wang et al., 2016; Snover et al., 2006). Metrics listed with the prefix “n” are reversed (1 − score) so that higher numbers indicate a better translation in all the figures we report. We calculate the score for each of the documents in our test suite separately and report the 482 average score and the standard deviation. The scores are detailed in Tables 3 to 7. In the subsequent tables, we sometimes abbreviate system names for typesetting reasons. The main observation across the tables is that all the scores heavily vary across individual documents. The typical standard deviation is 3–5 for BLEU and similarly"
W19-5355,P02-1040,0,0.112561,"e Czech↔German research systems were unsupervised, i.e. trained only on monolingual source and target texts, optionally using a small parallel development set of a few thousand sentence pairs. Our evaluation also includes several anonymized online systems (“online-. . . ”) the internals of which are not known. These online systems could in principle include our test suite as part of their training data. The number of evaluated documents and MT systems for each examined language pair is in Table 2. 3 Automatic Evaluation For automatic evaluation, we use several of common MT evaluation metrics (Papineni et al., 2002; Popovi´c, 2015; Leusch and Ney, 2008; Wang et al., 2016; Snover et al., 2006). Metrics listed with the prefix “n” are reversed (1 − score) so that higher numbers indicate a better translation in all the figures we report. We calculate the score for each of the documents in our test suite separately and report the 482 average score and the standard deviation. The scores are detailed in Tables 3 to 7. In the subsequent tables, we sometimes abbreviate system names for typesetting reasons. The main observation across the tables is that all the scores heavily vary across individual documents. The"
W19-5355,W19-5337,1,0.878261,"Missing"
W19-5355,W15-3049,0,0.0678947,"Missing"
W19-5355,2006.amta-papers.25,0,0.16204,"ual source and target texts, optionally using a small parallel development set of a few thousand sentence pairs. Our evaluation also includes several anonymized online systems (“online-. . . ”) the internals of which are not known. These online systems could in principle include our test suite as part of their training data. The number of evaluated documents and MT systems for each examined language pair is in Table 2. 3 Automatic Evaluation For automatic evaluation, we use several of common MT evaluation metrics (Papineni et al., 2002; Popovi´c, 2015; Leusch and Ney, 2008; Wang et al., 2016; Snover et al., 2006). Metrics listed with the prefix “n” are reversed (1 − score) so that higher numbers indicate a better translation in all the figures we report. We calculate the score for each of the documents in our test suite separately and report the 482 average score and the standard deviation. The scores are detailed in Tables 3 to 7. In the subsequent tables, we sometimes abbreviate system names for typesetting reasons. The main observation across the tables is that all the scores heavily vary across individual documents. The typical standard deviation is 3–5 for BLEU and similarly for other metrics. Th"
W19-5355,W18-6401,1,\N,Missing
W19-8630,P18-1063,0,0.0674786,"ociation for Computational Linguistics stead of reporting only the latter. The proposed scheme and the metrics can be used for a more detailed evaluation of supervised learning models. Using them, we examine various recently proposed methods in two tasks: text summarization using the popular CNNDM (CNN/Daily Mail, Nallapati et al., 2016) dataset and title generation of scientific articles using OAGS, a novel dataset of abstract-title pairs that we processed and released.2 According to our results, the bestperforming and fastest methods in the two datasets are those of Paulus et al. (2017) and Chen and Bansal (2018). Regarding score and time efficiency, Transformer (Vaswani et al., 2017) is distinctly superior. In the future, we will examine the Transformer model on more data with different parameter setups. Applying our evaluation scheme to related tasks such as MT (Machine Translation) could also be beneficial. Overall, this work brings the following main contributions: (i) We define and propose three data efficiency metrics and a simple evaluation scheme that uses them for a more comprehensive evaluation of data-driven learning methods. (ii) We use the scheme and metrics to benchmark some of the most"
W19-8630,D12-1052,0,0.021941,"rks to represent and process variable-length sequences has created a tradition of applying them on sequenceto-sequence tasks such as ATS or MT. In the case of ATS, the goal is to process the source text producing a target text that is shorter but still meaningful and easy to read. Rush et al. (2015) were probably the first to implement attention in a network dedicated to ATS. Their model (A BS in the following) uses an encoder that learns a soft alignment (attention) between the source and the target sequences producing the context vector. In the decoding phase, it uses a beam-search decoder (Dahlmeier and Ng, 2012) with a window of 10 candidate words in each target position. There are 256 and 128 dimensions in the hidden layer and word embedding layer respectively. The authors reported state-ofhttps://duc.nist.gov/duc2004/ 233 ing is added to word embeddings to preserve the order of the input and output sequences. T RANS is the biggest model we tried, with four layers in both encoder and decoder, 512 dimensions in each layer, including the embedding layers, 200K training steps and 8000 warm-up steps. Two observed problems in the encoder-decoder framework are the exposure bias and train/test inconsistenc"
W19-8630,N18-1065,0,0.106242,"Missing"
W19-8630,P16-1154,0,0.0443271,"performed. They were learned during the training of each model. Adam optimizer (Kingma and Ba, 2014) was used with α = 0.001, β1 = 0.9, β2 = 0.999 and  = 10−8 . We chose mini-batches of size 16 in most of the cases (8 for G LOB E N and T RANS to avoid memory errors). All experiments were conducted on two NVIDIA GTX 1080Ti GPUs. the-art results in the DUC-2004 testing dataset. See et al. (2017) proposed Pointer-Generator (P COV), a model that implements an attentionbased encoder for producing the context vector. The decoder is extended with a pointing/copying mechanism (Gulcehre et al., 2016; Gu et al., 2016) that is used in each step to compute a generation probability pgen from the context vector, the decoder states, and the decoder output in that step. This generation probability is used as a switch to decide if the next word should be predicted or copied from the input. Another extension is the coverage mechanism (keeping track of decoder outputs) for avoiding word repetitions in the summary, a chronic problem of encoder-decoder summarizers (Tu et al., 2016). The method was implemented with word embeddings and hidden layer of sizes 128 and 256 respectively. Lin et al. (2018) tried a partial us"
W19-8630,K16-1028,0,0.212075,", 2014) which has been 1 We use “performance” solely for the output quality, not the time needed to train the model or obtain the output. 229 Proceedings of The 12th International Conference on Natural Language Generation, pages 229–239, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics stead of reporting only the latter. The proposed scheme and the metrics can be used for a more detailed evaluation of supervised learning models. Using them, we examine various recently proposed methods in two tasks: text summarization using the popular CNNDM (CNN/Daily Mail, Nallapati et al., 2016) dataset and title generation of scientific articles using OAGS, a novel dataset of abstract-title pairs that we processed and released.2 According to our results, the bestperforming and fastest methods in the two datasets are those of Paulus et al. (2017) and Chen and Bansal (2018). Regarding score and time efficiency, Transformer (Vaswani et al., 2017) is distinctly superior. In the future, we will examine the Transformer model on more data with different parameter setups. Applying our evaluation scheme to related tasks such as MT (Machine Translation) could also be beneficial. Overall, this"
W19-8630,P16-1014,0,0.0606436,"of word embeddings was performed. They were learned during the training of each model. Adam optimizer (Kingma and Ba, 2014) was used with α = 0.001, β1 = 0.9, β2 = 0.999 and  = 10−8 . We chose mini-batches of size 16 in most of the cases (8 for G LOB E N and T RANS to avoid memory errors). All experiments were conducted on two NVIDIA GTX 1080Ti GPUs. the-art results in the DUC-2004 testing dataset. See et al. (2017) proposed Pointer-Generator (P COV), a model that implements an attentionbased encoder for producing the context vector. The decoder is extended with a pointing/copying mechanism (Gulcehre et al., 2016; Gu et al., 2016) that is used in each step to compute a generation probability pgen from the context vector, the decoder states, and the decoder output in that step. This generation probability is used as a switch to decide if the next word should be predicted or copied from the input. Another extension is the coverage mechanism (keeping track of decoder outputs) for avoiding word repetitions in the summary, a chronic problem of encoder-decoder summarizers (Tu et al., 2016). The method was implemented with word embeddings and hidden layer of sizes 128 and 256 respectively. Lin et al. (2018)"
W19-8630,W12-3018,0,0.0675465,"Missing"
W19-8630,P18-2124,0,0.0848057,"Missing"
W19-8630,W04-1013,0,0.0859288,"Missing"
W19-8630,D15-1044,0,0.460595,"n conclude that: A and B perform (almost) the same on D, but A will probably perform better than B if trained on more data. The scheme can be used to evaluate data-driven methods with different scores, on different tasks. In Section 5 we show the results we obtained by applying it to assess several advanced ATS methods. 4 Text Summarization Datasets The tendency towards data-driven methods based on neural networks has encouraged experiments with large text collections for various tasks. In the case of ATS, one of the first big datasets was the annotated English Gigaword (Napoles et al., 2012; Rush et al., 2015) with over nine million news articles and headlines processed using CoreNLP of 232 CNNDM OAGS Split Train1 Train2 Train3 Valid Test Train1 Train2 Train3 Valid Test Rec 96K 192K 287K 13K 11K 500K 1M 1.5M 10K 10K SrcL 784 780 786 769 787 183 205 211 231 237 TgtL 54 57 55 61 58 9 10 11 13 12 Voc 380K 555K 690K – – 1.2M 2.1M 2.8M – – Used 49K 49K 49K – – 98K 98K 98K – – title, and language fields were extracted from each record where they were available. In many cases, abstract language did not match the language field. We ignored the latter and used a language identifier to remove records that we"
W19-8630,P18-2027,0,0.0117939,"cehre et al., 2016; Gu et al., 2016) that is used in each step to compute a generation probability pgen from the context vector, the decoder states, and the decoder output in that step. This generation probability is used as a switch to decide if the next word should be predicted or copied from the input. Another extension is the coverage mechanism (keeping track of decoder outputs) for avoiding word repetitions in the summary, a chronic problem of encoder-decoder summarizers (Tu et al., 2016). The method was implemented with word embeddings and hidden layer of sizes 128 and 256 respectively. Lin et al. (2018) tried a partial use of convolutions in their model (G LOB E N) to avoid word repetitions and semantic irrelevance in the summaries. They couple the encoder with a convolutional gated unit which performs global encoding of the source context and uses it to filter certain ngram features and refine the output of the encoder in each time step. G LOB E N is a very big network (about 68M parameters on CNNDM) with three layers in the encoder and other three in the decoder, each of 512 dimensions. A taxonomy of the above (and more) sequenceto-sequence methods and added mechanisms can be found in Shi"
W19-8630,P17-1099,0,0.0423068,"together the extractor and abstractor networks. Same as most models above, FAST R L uses 256 and 128 dimensions for the recurrent layer and the word embeddings. In every experiment, no pretraining of word embeddings was performed. They were learned during the training of each model. Adam optimizer (Kingma and Ba, 2014) was used with α = 0.001, β1 = 0.9, β2 = 0.999 and  = 10−8 . We chose mini-batches of size 16 in most of the cases (8 for G LOB E N and T RANS to avoid memory errors). All experiments were conducted on two NVIDIA GTX 1080Ti GPUs. the-art results in the DUC-2004 testing dataset. See et al. (2017) proposed Pointer-Generator (P COV), a model that implements an attentionbased encoder for producing the context vector. The decoder is extended with a pointing/copying mechanism (Gulcehre et al., 2016; Gu et al., 2016) that is used in each step to compute a generation probability pgen from the context vector, the decoder states, and the decoder output in that step. This generation probability is used as a switch to decide if the next word should be predicted or copied from the input. Another extension is the coverage mechanism (keeping track of decoder outputs) for avoiding word repetitions i"
W19-8630,P14-5010,0,0.0047929,"bout social sciences, psychology, economics or engineering disciplines. Given its huge size and the topical richness, the value of OAGS is twofold: (i) It can be used to supplement existing datasets on title generation tasks when more training data are needed. (ii) It can be used for creating byproducts of specific scientific disciplines or domains. Table 1: Statistics of used datasets. For each split, it shows the number of records (Rec), average length of source and target texts in tokens (SrcL, TgtL), total vocabulary size (Voc), and the number of most frequent words that were used (Used). Manning et al. (2014). Each headline was paired with the first sentence of the corresponding article to create the training base for the experiments. DUC-2004 is another dataset4 , mostly used as an evaluation baseline, given its small size. It consists of 500 document-summary pairs curated by human experts. Newsroom is a recent and heterogeneous bundle of about 1.3 million news articles (Grusky et al., 2018). CNNDM has become the most popular dataset for text summarization (Nallapati et al., 2016). It provides a large set of news articles and the corresponding multi-sentence summaries, unlike the three above that"
W19-8630,P17-1054,0,0.0770164,"r text summarization (Nallapati et al., 2016). It provides a large set of news articles and the corresponding multi-sentence summaries, unlike the three above that contain one-sentence summaries only. It is thus more suitable for training and testing summarization models of longer texts. Title generation task, on the other hand, requires data samples of shorter texts and one-sentence titles. Collections of abstracts and titles from scientific articles are well suited for exploring it. KP20k is a collection of 20K records of scientific paper metadata (title, abstract and keywords) presented by Meng et al. (2017). The metadata belong to articles of computer science from ACM Digital Library, ScienceDirect, and Web of Science. The demand for more and more data has motivated initiatives that mine research articles from academic networks. One of them is ArnetMiner, a system that extracts researcher profiles from the Web and integrates the data into a unified network (Tang et al., 2008). A byproduct of that work is the OAG (Open Academic Graph) collection (Sinha et al., 2015). To produce a big title generation dataset for our experiments, we started from OAG. First, abstract, 4 5 Text Summarization Evaluat"
W19-8630,P16-1008,0,0.0286963,"ttentionbased encoder for producing the context vector. The decoder is extended with a pointing/copying mechanism (Gulcehre et al., 2016; Gu et al., 2016) that is used in each step to compute a generation probability pgen from the context vector, the decoder states, and the decoder output in that step. This generation probability is used as a switch to decide if the next word should be predicted or copied from the input. Another extension is the coverage mechanism (keeping track of decoder outputs) for avoiding word repetitions in the summary, a chronic problem of encoder-decoder summarizers (Tu et al., 2016). The method was implemented with word embeddings and hidden layer of sizes 128 and 256 respectively. Lin et al. (2018) tried a partial use of convolutions in their model (G LOB E N) to avoid word repetitions and semantic irrelevance in the summaries. They couple the encoder with a convolutional gated unit which performs global encoding of the source context and uses it to filter certain ngram features and refine the output of the encoder in each time step. G LOB E N is a very big network (about 68M parameters on CNNDM) with three layers in the encoder and other three in the decoder, each of 5"
xue-etal-2014-interlingua,N07-1051,0,\N,Missing
xue-etal-2014-interlingua,W04-2705,0,\N,Missing
xue-etal-2014-interlingua,A00-2018,0,\N,Missing
xue-etal-2014-interlingua,J93-2004,0,\N,Missing
xue-etal-2014-interlingua,C12-1083,0,\N,Missing
xue-etal-2014-interlingua,W04-3212,1,\N,Missing
xue-etal-2014-interlingua,J03-4003,0,\N,Missing
xue-etal-2014-interlingua,Q13-1034,1,\N,Missing
xue-etal-2014-interlingua,W09-1201,1,\N,Missing
xue-etal-2014-interlingua,J02-3001,0,\N,Missing
xue-etal-2014-interlingua,J05-1004,1,\N,Missing
xue-etal-2014-interlingua,Q13-1019,0,\N,Missing
xue-etal-2014-interlingua,prasad-etal-2008-penn,0,\N,Missing
xue-etal-2014-interlingua,W13-2322,1,\N,Missing
xue-etal-2014-interlingua,N04-1030,0,\N,Missing
