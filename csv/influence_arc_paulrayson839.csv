2020.lrec-1.383,L16-1569,0,0.0309522,"size of corpora that they can index. Other systems, such as Corpuscle6 are targetted at corpora of the order of one billion words. SketchEngine and the open-source cut-down version incorporating Manatee (corpus management tool) and Bonito (graphical interface) (Rychlý, 2007) has an apparent maximum allowed corpus size of 231 − 2 according to the Manatee change log but no benchmarks are available for indexing or retrieval speed. Varying different requirements of corpus linguists and the various tools, both general-purpose and bespoke, has led to some corpus management platforms such as Korap (Diewald et al., 2016) being developed. Korap contains various components including a front-end and API interface, management component and a back end data system with an indexing structure built on Lucene. Whilst this type of structure may be daunting for casual users to get to grips with Korap does address a key issue in being able to use its Koral component to translate queries in various corpus query languages such as CQL (Evert, 2005) and Annis QL (Chiarcos et al., 2008) into its own query language for execution. This provides a means of easy entry for linguists familiar with supported query languages. Many di"
2020.lrec-1.383,schafer-bildhauer-2012-building,0,0.0293273,"ent Systems) that have moved away from the traditional relational database model. The No-SQL movement, in particular, has concerned itself largely with a system’s ability to scale out to handle increasingly large datasets. Language data in the worlds of corpus and computational linguistics, digital humanities and lexicography has also seen the scale of what is considered a ‘large’ corpus grow from one million words in LOB (Johansson et al., 1986), and one hundred million words British National Corpus (Leech, 1992) to billions and tens of billions of words TenTen (Jakubíček et al., 2013), COW (Schäfer and Bildhauer, 2012), Hansard1 , EEBO-TCP2 over the last forty years. However, specialist corpus storage and retrieval tools have lagged behind the developments made by more generic DBMSs. In addition, modern DBMSs do not provide the necessary query functionality to satisfy the types of queries used by corpus linguists. Moreover, existing database models do not provide efficient storage and retrieval for consecutive words in running text, sequential queries and contextual sorting that are central to the requirements for textual corpora. These issues present a problem for linguists, humanists and others wishing to"
2020.lrec-1.596,W19-5604,1,0.641387,"For instance, the word ( Ñë@QÓ ëX) and the two terms need to be added to a synonym ( àA ontology because they represent a treatment of Leishmania disease. 4.6. Adding Informal Terms In order to find the slang terms related to infectious diseases, we consulted native speakers. This included eliciting the informal term equivalents of the infectious disease that may be used by the study participants when they write about disease in social media. A questionnaire was sent to five different types of people: academic, health professional, students, non-educational people and others as classified by Alsudias and Rayson (2019). Around 4844 English Name Arabic Name Link World Health Organization   ¢ JÓ éjË@ éJ ÖÏ AªË@ éÒ  èP@Pð éK XñªË@ éjË@ AK YJ .J ºK ð JJ Ê¿ ñK AÓ I.£ I.K ð ú æ.¢Ë@ https://www.who.int The Ministry of Health in Saudi Arabia Wikipedia Mayo clinic WebTeb Altibbi https://www.moh.gov.sa https://ar.wikipedia.org https://www.mayoclinic.org/ar https://www.webteb.com https://www.altibbi.com Table 1: Some Arabic Web Sources Related to Infectious Disease. 100 people responded to the questionnaire with three informal names of each disease on average. For example, ùÔ g), Dengue fever in English, ha"
2020.lrec-1.596,C10-1025,0,0.232006,"either fully manually or partially manually which leads to issues of high cost and a requirement for significant investment of time (Al-Zoghby et al., 2018). In this paper, we present a new Arabic Ontology from the infectious disease domain. The overall goal of this ontology is to be a key source of information related to infectious diseases in Arabic. We envisage that it can be used for many important applications in academia and the real world including Question Answering on the Semantic Web (AlAgha and Abu-Taha, 2015) and, in particular in our case, for online monitoring of health events (Collier et al., 2010). In the case of building an ontology in the Arabic language there is some previous work related to specific topics such as the Islamic domain, Computing domain, News domain, and Legal domain (Al-Zoghby et al., 2018). Raisan and Abdullah (2017) implemented an ontology in the Arabic language related to the Iraqi News domain. They used a manual ontology development strategy to build the schema. A study by Albukhitan and Helmy (2013) produced an automatic ontology based annotation of Arabic Web resources related to food, nutrition and health domains. It used linguistic patterns to discover releva"
2020.lrec-1.596,P14-5010,0,0.00366749,"s the process of chunking the text into words. This is a vital first step as we will be working with domain terms. Normalization: In normalisation, some Arabic letters are  normalised such as (@ , @ , @ ) are converted to ( @ ). Removing Stopwords: Arabic stopwords were removed since they are less useful for ontology creation. There is also need to remove the non-Arabic words, special characters, and numbers. POS Tagging: Part of speech tagging is the process of labeling corpus words with their corresponding part of speech tags. We used the Stanford Arabic Part of speech tagger in this step (Manning et al., 2014). Sentence Splitting: To allow better extraction and linking of terms, we used (.) to split the sentences in our corpus. 4.4. Term Extraction Terms are linguistic representations of domain-specific concepts. The target here is to discover a set of significant terms for concepts and relations (Cimiano, 2006). We 2 https://github.com/alsudias 4.4.2. Using C-value C-value is a domain-independent method for automatic term recognition that aims to improve the extraction of nested multi-word terms (Frantzi et al., 2000). The algorithm first tags the words with their POS tags by Stanford Arabic Part"
2020.lrec-1.855,L18-1726,1,0.819929,"Missing"
2020.lrec-1.855,L18-1327,1,0.92361,"ntities. In addition, several infrastructures supporting biomedical text mining have been developed, including U-Compare (Kano et al., 2008) and Argo (Rak et al., 2012). The General Architecture for Text Engineering (GATE) (Cunningham et al., 2011), a broader-based framework for text mining, also includes some tools for handling biomedical texts. These tools and infrastructures are typically self-contained and focused on lexical, syntactic and shallow semantic (namedentity) approaches. More recently, the LAPPS Grid (Ide et al., 2014) has been augmented to support mining biomedical literature (Ide et al., 2018), as well as sophisticated interactive annotation and machine learning tools for domain adaptation to support mining literature in the life sciences. 3. In order to fully support the complete cycle of literaturebased discovery and symbiotic improvement in language resources in the genomics domain with an existing vast body of work, we need large scale infrastructures. The following subsections discuss our Gene Ontology Semantic Tagger (GOST) (section 3.1.) and its integration with both our new Buster NLP pipeline (section 3.3.) and the LAPPS Grid (section 3.2.) which was previously developed b"
2020.lrec-1.855,I08-2122,0,0.0508389,"portance of the biomedical text mining, a variety of NLP tools have been developed and modified to support it. Among the main tools and corpora developed for such purposes include the Genia tagger and corpus (Tsuruoka et al., 2005; Thompson et al., 2017), GOST tagger (El-Haj et al., 2018), and Termine7 . A related biomedical annotation tool is the Penn BioTagger8 (Jin et al., 2006), which is capable of tagging gene entities, genomic variations entities and malignancy type entities. In addition, several infrastructures supporting biomedical text mining have been developed, including U-Compare (Kano et al., 2008) and Argo (Rak et al., 2012). The General Architecture for Text Engineering (GATE) (Cunningham et al., 2011), a broader-based framework for text mining, also includes some tools for handling biomedical texts. These tools and infrastructures are typically self-contained and focused on lexical, syntactic and shallow semantic (namedentity) approaches. More recently, the LAPPS Grid (Ide et al., 2014) has been augmented to support mining biomedical literature (Ide et al., 2018), as well as sophisticated interactive annotation and machine learning tools for domain adaptation to support mining litera"
2020.lrec-1.855,W12-2425,0,0.0356477,"atural Language Processing (NLP) field. GOST output can then be processed by tools that may generate additional annotation layers (“views&quot; in LAPPS Grid terminology), with or without using GOST’s semantic annotations. The LAPPS Grid also provides a Solr-based query engine for PubMed data that is augmented with ranking rules whose weights can be tweaked as desired by the user, results from which can be used as input to GOST. A major advantage of incorporating GOST into the LAPPS Grid is the access to PubMed data provided by the newlyestablished incorporation of the facilities of PubAnnotation (Kim and Wang, 2012) into the Grid. PubAnnotation not only provides access to all PubMed texts, but also, crucially, serves as a repository of annotations that are linked together by common reference (via standoff annotation) to the canonical texts. A common annotation repository enables combining GOST’s semantic annotations with annotations generated by other software and/or by human annotators, which in turn can yield insight into linguistic and semantic properties of biomedical terminology and improve our ability to extract meaningful information from biomedical publications. PubAnnotation also provides an ann"
2020.lrec-1.855,W04-1804,0,0.103141,"or biomedical text mining, 3) a large annotated corpus consisting of open access PubMed Central papers, 4) open platforms to support research reproducibility, and 5) supporting literature based discovery with a novel combination of NLP and CL methods. 2. Related Work Analysing biomedical data using Natural Language Processing (NLP) and text mining requires a significant amount of domain knowledge (Tan and Lambrix, 2009). Such knowledge is usually found in domain specific ontologies such as the Gene ontology resource1 which contains important information related to gene products and functions (Kumar et al., 2004). Over many years, NLP techniques have been widely applied to biomedical text mining to facilitate large-scale information extraction and knowledge discovery from the rapidly increasing body of biomedical literature. Since the begin1 6921 http://www.geneontology.org ning of biomedical language processing in the late 1990s, the field continued to receive great attention with specialised events and workshops focusing on biomedical NLP, such as the BioNLP Workshop series. Current biomedical libraries such as MEDLINE2 by the US National Library of Medicine (NLM)3 provide searchable databases that"
2020.lrec-1.855,W09-1307,0,0.0349332,"uction observed in the 1980s and 1990s. Our specific contributions in this paper are as follows: 1) entity identification and semantic linking in the genomics domain, 2) a novel open infrastructure for biomedical text mining, 3) a large annotated corpus consisting of open access PubMed Central papers, 4) open platforms to support research reproducibility, and 5) supporting literature based discovery with a novel combination of NLP and CL methods. 2. Related Work Analysing biomedical data using Natural Language Processing (NLP) and text mining requires a significant amount of domain knowledge (Tan and Lambrix, 2009). Such knowledge is usually found in domain specific ontologies such as the Gene ontology resource1 which contains important information related to gene products and functions (Kumar et al., 2004). Over many years, NLP techniques have been widely applied to biomedical text mining to facilitate large-scale information extraction and knowledge discovery from the rapidly increasing body of biomedical literature. Since the begin1 6921 http://www.geneontology.org ning of biomedical language processing in the late 1990s, the field continued to receive great attention with specialised events and work"
2020.nlpcovid19-acl.16,W19-5604,1,0.839322,"Missing"
2020.nlpcovid19-acl.16,2020.lrec-1.596,1,0.809594,"identifying rumours. Although the above studies have produced datasets related to COVID-19, they do not analyse them deeply using NLP methods. Previous studies representing earlier epidemics present good techniques and results, however none of them are related to Arabic tweets. Therefore, to assist PHOs in Arabic speaking countries there is an urgent need to analyse tweets related to COVID-19 using multiple Arabic NLP techniques. 3 Update Arabic Infectious Disease Ontology With the recent appearance of COVID-19 as a new disease, there is need to update our Arabic Infectious Disease Ontology (Alsudias and Rayson, 2020), which integrates the scientific and medical vocabularies of infectious diseases with their informal equivalents used in general discourse. We collated COVID-19 information from the World Health Organization6 and Ministry of health in Saudi Arabia. This included symptom, cause, prevention, infection, organ, treatment, diagnosis, place of the disease spread, and slang terms for COVID-19 and extended our ontology7 . These terms were then used in our collection process. 4 Data Collection We began collecting Arabic tweets about a number of infectious diseases from September 2019. Here in this pap"
2020.nlpcovid19-acl.16,Q17-1010,0,0.0409411,"Missing"
2020.nlpcovid19-acl.16,2021.wanlp-1.9,0,0.0392588,"Missing"
2020.parlaclarin-1.5,W19-4332,1,0.878657,"Missing"
2020.parlaclarin-1.5,C94-1103,0,0.736983,"Missing"
2020.parlaclarin-1.5,L16-1416,1,0.896673,"Missing"
2020.parlaclarin-1.5,wattam-etal-2014-experiences,1,0.82337,"Parliamentary discourse is of concern not only to political and linguistic scholars but also social charities and community groups. The transcriptions of speeches and discussions in the UK Houses of Lords and the Commons are better known as Hansard. Recent reports from these proceedings are freely available online. Historical transcriptions are available in the form of the Historical Hansard corpus which includes the transcriptions from 1803-2005. Previously the SAMUELS1 (Semantic Annotation and Mark-Up For Enhancing Lexical Searches) project has researched tokenising and tagging this corpus (Wattam et al., 2014). As political engagement grows daily alongside the Hansard corpus transcripts, bridging this gap from 2005 to the present and maintaining an up to date, fully tokenised and tagged version of this dataset becomes increasingly relevant and important to improve search functionality and timeliness. This paper presents the process, tools and output of our efforts to build a complete corpus of Hansard that contains linguistic and semantic annotations and runs right up to the present day. We also describe how this corpus is made available through a bespoke search interface built on top of the corpus"
2021.clpsych-1.1,C18-1126,0,0.130096,"uish several BD subtypes based on the lifetime frequency and intensity of (hypo-)manic and depressed episodes. The only requirement for a diagnosis of bipolar I disorder (BDI) is at least one lifetime manic episode, whereas Online forums as research data source Online forums have become an increasingly attractive source for research data, enabling non-reactive data collection, where researchers do not influence data creation, at large scale (Fielding et al., 2016). Natural language processing (NLP) research in this area has focused on predicting people at risk of BD (Coppersmith et al., 2014; Cohan et al., 2018; Sekuli´c et al., 2018). Health researchers have explored the lived experience of BD with qualitative analyses of online posts (Mandla et al., 2017; Sahota and Sankar, 2019). Unlike in clinical studies, usually little or no demographic information is available for online forum users, so it is unclear to what populations these results generalise (Ruths and Pfeffer, 2014). For example, language differences between Twitter users with self-reported Major depressive disorder (MDD) or Post-traumatic stress disorder (PTSD) correlated highly with their personality and demographic characteristics (Pre"
2021.clpsych-1.1,W14-3207,0,0.23889,"124). DSM and ICD distinguish several BD subtypes based on the lifetime frequency and intensity of (hypo-)manic and depressed episodes. The only requirement for a diagnosis of bipolar I disorder (BDI) is at least one lifetime manic episode, whereas Online forums as research data source Online forums have become an increasingly attractive source for research data, enabling non-reactive data collection, where researchers do not influence data creation, at large scale (Fielding et al., 2016). Natural language processing (NLP) research in this area has focused on predicting people at risk of BD (Coppersmith et al., 2014; Cohan et al., 2018; Sekuli´c et al., 2018). Health researchers have explored the lived experience of BD with qualitative analyses of online posts (Mandla et al., 2017; Sahota and Sankar, 2019). Unlike in clinical studies, usually little or no demographic information is available for online forum users, so it is unclear to what populations these results generalise (Ruths and Pfeffer, 2014). For example, language differences between Twitter users with self-reported Major depressive disorder (MDD) or Post-traumatic stress disorder (PTSD) correlated highly with their personality and demographic"
2021.clpsych-1.1,W15-1201,0,0.0236694,"free access to all historic posts. Reddit profiles do not provide any user characteristics besides the username and sign-up date in a structured format or comparable to a Twitter bio. While some surveys provide general information on Reddit users, none of the BD-specific studies looked at particular user characteristics of their sample, which is important (Amaya et al., 2019). 1.3 2 2.1 2 User identification In this work, the identification of Reddit users with lived experience of BD adapts previous approaches based on self-reported diagnosis statements, e.g., ‘I was diagnosed with BD today’ (Coppersmith et al., 2015; Cohan et al., 2018; Sekuli´c et al., 2018). Importantly, this captures self-reported diagnoses by a professional and not self-diagnoses, which were excluded. Contrary to existing datasets of Reddit posts by people with a self-reported BD diagnosis, all posts of identified people were retained and not only those unrelated to MH concerns. This enables subsequent research on the lived experience of people with BD. All available Reddit posts (January 05 - March 19) that mentioned ‘diagnosis’ and a BD term (see below) were downloaded from Google BigQuery. User account meta-data (id, username, UTC"
2021.clpsych-1.1,Q18-1041,0,0.0179927,"The Hybrid method assigns a binary gender identity in a sequential approach, disregarding possible disagreements between methods: If the Username method found the username to perform f or m gender, it takes this prediction, otherwise assumes the selfreported gender if available, and else resorts to the predictions of the Language use method. 3.3 3 Rationales for user characteristics As stated in the introduction, user characteristics are important to determine about which populations research on this dataset may generalise. The NLP community increasingly expects data statements for datasets (Bender and Friedman, 2018), which include speaker age and gender specifications. As Section 4.3 shows, characteristics of Reddit users with a self-reported BD diagnosis deviate from both general Reddit user statistics and epidemiological studies, which therefore do not constitute useful proxies. Relying entirely on self-reported information introduces selection biases because not all user groups may be equally inclined to explicitly share certain characteristics. This motivates using statistical methods to infer Reddit users’ age, country, and gender here. The user characteristics comorbid MH issues, age, country, and"
2021.clpsych-1.1,W17-1612,0,0.0173158,"username strongly performs f or m gender, otherwise it assigns no label. Till, 2001; Beninger et al., 2014; Paul and Dredze, 2017), mainly because it is not straightforward to determine if posts pertain to a public or private context. Legally, the Reddit privacy policy5 explicitly allows copying of user contents by third parties via the Reddit API, but it is unclear to what extends users are aware of this (Ahmed et al., 2017). In practice it is often infeasible to seek retrospective consent from hundreds or thousands of social media users. Current ethical guidelines for social media research (Benton et al., 2017; Williams et al., 2017) and practice in comparable research projects (O’Dea et al., 2015; Ahmed et al., 2017), regard it as acceptable to waive explicit consent if users’ anonymity is protected. Therefore, Reddit users in this work were not asked for consent. • Self-reported: See Section 2.2.2. 3.2 • Language use: The neural network model by Tigunova et al. (2019) predicts gender for Reddit users with at least ten posts from the post texts. It was trained on data automatically labelled with self-reported gender provided by Tigunova et al. (2020) (see Appendix A.1). In line with guidelines for"
2021.clpsych-1.1,W16-0307,0,0.0197384,"ubgroups of Reddit users. The evaluation with manual annotations evaluates published NLP methods in an applied setting. Besides MH-specific platforms (Kramer et al., 2004; Vayreda and Antaki, 2009; Bauer et al., 2013; Latalova et al., 2014; Poole et al., 2015; McDonald and Woodward-Kron, 2016; Campbell and Campbell, 2019), blogs (Mandla et al., 2017), and Twitter (Coppersmith et al., 2014; Ji et al., 2015; Saravia et al., 2016; Budenz et al., 2019; Huang et al., 2019), much recent research of user-generated online content in BD has focused on the international online discussion forum Reddit1 (Gkotsis et al., 2016, 2017; Cohan et al., 2018; Sekuli´c et al., 2018; Sahota and Sankar, 2019; Yoo et al., 2019). The platform Reddit is among the most visited internet sites worldwide (Alexa Internet, 2020), hosting a number of subforums (‘subreddits’) for general topics as well as interest groups. There is a vast and growing amount of BD-related content on Reddit, with more than 50K new posts per month in the four largest BD-related subreddits2 . Anyone can view posts without registration and the Reddit API offers free access to all historic posts. Reddit profiles do not provide any user characteristics beside"
2021.clpsych-1.1,W17-1601,0,0.158075,"rocess mixture model4 . It uses the distribution of words, posts per subreddit, and posts per hour of the day (timezone proxy) of a user’s up to 250 most recent comments. 4 3 https://github.com/kharrigian/smgeo 2.2.4 Binary gender Three methods to recognise binary gender (feminine (f)/masculine (m)) leveraging different types of information were compared. All three methods pertain to a performative gender view, which posits that people understand their and others’ gender identity by certain behaviours (including language) and appearances that society stipulates for bodies of a particular sex (Larson, 2017). Non-binary gender identities were not included due to a lack of NLP methods to detect them. • Username: The character-based neural network model of Wang and Jurgens (2018) predicts whether a username strongly performs f or m gender, otherwise it assigns no label. Till, 2001; Beninger et al., 2014; Paul and Dredze, 2017), mainly because it is not straightforward to determine if posts pertain to a public or private context. Legally, the Reddit privacy policy5 explicitly allows copying of user contents by third parties via the Reddit API, but it is unclear to what extends users are aware of thi"
2021.clpsych-1.1,W18-6103,0,0.129226,"th comorbidities Frequencies for other self-reported MH diagnoses were obtained by matching all dataset posts against inclusion patterns for other diagnoses, in the same way as for identifying self-reported BD diagnoses. Condition terms for nine major DSM-5 and ICD-11 diagnoses were extended from Cohan et al. (2018): Anxiety disorder (Generalised/Social anxiety disorder, Panic disorder), Attention deficit hyperactivity disorder (ADHD), Borderline personality disorder (BPD), MDD, PTSD, Psychotic disorder 2.2.3 Country of residence The only published method for Reddit user localisation to date (Harrigian, 2018) infers a user’s country of residence via a dirichlet process mixture model4 . It uses the distribution of words, posts per subreddit, and posts per hour of the day (timezone proxy) of a user’s up to 250 most recent comments. 4 3 https://github.com/kharrigian/smgeo 2.2.4 Binary gender Three methods to recognise binary gender (feminine (f)/masculine (m)) leveraging different types of information were compared. All three methods pertain to a performative gender view, which posits that people understand their and others’ gender identity by certain behaviours (including language) and appearances t"
2021.clpsych-1.1,W18-6211,0,0.0298173,"Missing"
2021.clpsych-1.1,2020.lrec-1.751,0,0.18256,"re compared. An approximate date of birth was calculated from the post timestamp to then calculate the user’s age when posting for the first time and their mean age over all posts. • Self-reported: Reddit users sometimes selfreport their age and gender in a bracketed format, e.g. ‘I [17f] just broke up with bf [18m]‘. Regular expressions extracted age and gender from such self-reports in submission titles. • Language use: Tigunova’s (2019) neural network model predicts the age group of users with at least ten posts from their contents and language style. Training data for this model came from Tigunova et al. (2020) who automatically labelled Reddit users with their self-reported age (see Appendix A.1). User characteristics extraction/inference Several NLP methods were applied and compared to extract or infer clinical (MH comorbidities = diagnoses additional to BD), demographic (age, country of residence), and identity (gender) characteristics of Reddit users with a self-reported BD diagnosis. See Appendix A for more details on the age, country, and gender methods and their previously published performance. The first and third author manually annotated self-reported BD diagnoses, age, country, and gender"
2021.clpsych-1.1,D18-1004,0,0.020229,"mments. 4 3 https://github.com/kharrigian/smgeo 2.2.4 Binary gender Three methods to recognise binary gender (feminine (f)/masculine (m)) leveraging different types of information were compared. All three methods pertain to a performative gender view, which posits that people understand their and others’ gender identity by certain behaviours (including language) and appearances that society stipulates for bodies of a particular sex (Larson, 2017). Non-binary gender identities were not included due to a lack of NLP methods to detect them. • Username: The character-based neural network model of Wang and Jurgens (2018) predicts whether a username strongly performs f or m gender, otherwise it assigns no label. Till, 2001; Beninger et al., 2014; Paul and Dredze, 2017), mainly because it is not straightforward to determine if posts pertain to a public or private context. Legally, the Reddit privacy policy5 explicitly allows copying of user contents by third parties via the Reddit API, but it is unclear to what extends users are aware of this (Ahmed et al., 2017). In practice it is often infeasible to seek retrospective consent from hundreds or thousands of social media users. Current ethical guidelines for soc"
2021.clpsych-1.1,W15-1203,0,0.0479005,"Missing"
2021.fnp-1.19,2021.fnp-1.22,1,0.833966,"Missing"
2021.fnp-1.19,P18-1063,0,0.134289,"cal at longer sequence lengths, as memory constraints limit batching across examples. In order to compute current outputs, the model needs to process previous outputs and inputs, therefore outputs cannot be calculated using parallel computation. This method is not appropriate if text is too long since it takes long time to process the outputs and calculate the loss after several time steps. Therefore, attention mechanisms have become critical for sequence modeling in various tasks, allowing modeling of dependencies without caring too much about their distance in the input or output sequences (Chen and Bansal, 2018). Long sequence NLP presents many challenges for current models. In fact, long range dependencies often require complex reasoning and forces models to both locate relevant information and combine it. Models need to ignore a lot of irrelevant text. Many popular algorithms are designed to work in short sequence setting, and have limitations in long setting. RNN/LSTM: process input sequentially and stores relevant information from previous states therefore it is slow for long sequences. Transformers are based on self-attention and cannot process long input with current hardware. (e.g. BERT pre-tr"
2021.fnp-1.19,N16-1012,0,0.0263295,"itic methods; policy gradients. 1 Introduction The task of text summarization is to condense long documents into short summaries while preserving the content and meaning. It can be performed using two main techniques: extraction and abstraction. The extractive summarization method directly chooses and outputs the salient phrases in the original document (Jing and McKeown (1999); Knight and Marcu (2002)). The abstractive summarization approach involves rewriting the summary (Rush et al. (2015); Liu et al. (2015)); and has seen substantial recent gains due to neural sequence-to sequence models (Chopra et al. (2016); Nallapati et al. (2016a); See et al. (2017a); El-Haj et al. (2018); Paulus et al. (2017) ). In the general case, extractive summarization approaches usually show a better performance compared to the abstractive approaches especially when evaluated using ROU GE metrics (Kiyoumarsi, Mahmoud El-Haj1 Paul Rayson1 2 Samsung R&D Bangalore, India abhishek.s.eee15@iitbhu.ac.in 2015). One of the advantages of the extractive approaches is that they can summarize source articles by extracting salient snippets and sentences directly from these documents, while abstractive approaches rely on word-level a"
2021.fnp-1.19,N18-2097,0,0.0133622,"veral studies ( (Widyassari et al., 2020) ; (Tretyak and Stepanov, 2020) ) proposed to combine extractive and abstractive techniques in order to improve performance. Abstractive models can be more concise by generating summaries from scratch in a context where the gold summaries were deleted from the original annual reports. However, this method suffers from slow and inaccurate encoding of very long documents which is the case with financial annual reports (above 50,000 tokens per report). Abstractive models also suffer from redundancy, especially when generating summaries of long documents. (Cohan et al., 2018) . Therefore, the proposed summarizer follows a hybrid extractive-abstractive architecture, with policy-based reinforcement learning (RL) to bridge together the two networks. The model first uses an extractor agent to select salient phrases, and then employs an abstractor network to rewrite (compress and paraphrase) each of these extracted sentences. We then use actor critic policy gradient with sentence-level metric rewards to jointly train these two summarization models in order to perform Reinforcement Learning and learn sentence saliency. 2 Background Recurrent models typically take in a s"
2021.fnp-1.19,W19-8902,1,0.802607,"Missing"
2021.fnp-1.19,2020.fnp-1.1,1,0.846885,"Missing"
2021.fnp-1.19,2021.naacl-main.112,0,0.047795,"Missing"
2021.fnp-1.19,W04-1013,0,0.0157409,"input, then returns the best action, or a policy that refers to a probability distribution over the actions. In our case we use Pointer Network to perform the actor job. On the other hand, the critic evaluates the actions returned by the actor neural network and returns a score representing the value of taking that action given the state. The figure 1 gives a concise description of the end to end summarizer system. the model and calculates respective precision, recall, and F1-score for each measure.3 For the entire dataset, we evaluate standard ROU GE1, ROU GE-2, and ROU GE-L and ROU GE-SU4 (Lin, 2004) on full length F1 (with stemming) following previous works ( See et al. (2017a); Nallapati et al. (2016a) ). The ROU GE 2.0 package Ganesan (2015) is used for calculations. 6.2 Scores In this section, we present results from our experiments and compare with different baselines MUSE (Litvak et al., 2010), Text-rank (Mihalcea and Tarau, 2004), Lex-Rank (Erkan and Radev, 2004), and Polynomial Summarisation (Litvak and Vanetik, 2013). Overall, our model achieves better results than all the proposed baselines with ROU GE1 : 0.52, ROU GE-2 : 0.30, ROU GE-L : 0.46 and ROU GE-SU4 : 0.32 Metric TextRa"
2021.fnp-1.19,I13-1075,0,0.0206461,"the model and calculates respective precision, recall, and F1-score for each measure.3 For the entire dataset, we evaluate standard ROU GE1, ROU GE-2, and ROU GE-L and ROU GE-SU4 (Lin, 2004) on full length F1 (with stemming) following previous works ( See et al. (2017a); Nallapati et al. (2016a) ). The ROU GE 2.0 package Ganesan (2015) is used for calculations. 6.2 Scores In this section, we present results from our experiments and compare with different baselines MUSE (Litvak et al., 2010), Text-rank (Mihalcea and Tarau, 2004), Lex-Rank (Erkan and Radev, 2004), and Polynomial Summarisation (Litvak and Vanetik, 2013). Overall, our model achieves better results than all the proposed baselines with ROU GE1 : 0.52, ROU GE-2 : 0.30, ROU GE-L : 0.46 and ROU GE-SU4 : 0.32 Metric TextRank LexRank Polynomial MUSE rnn-ext + abs + RL R-1/F 0.17 0.26 0.37 0.5 0.52 R-2/F 0.07 0.12 0.12 0.28 0.3 R-L/F 0.21 0.22 0.26 0.45 0.46 Table 1: FNS shared task results 7 Figure 1: The end to end summarizer 5 Experimental setup In order to train our extractor, abstractor and RL models, we use a Tesla P100-PCIE GPU with accelerated high RAM of gigabytes with batch size of 16 and check point frequency of 16 batches. Please refer to"
2021.fnp-1.19,N15-1114,0,0.0146154,"ords: Summarization, Neural networks, Reinforcement learning, sequence to sequence learning; actor-critic methods; policy gradients. 1 Introduction The task of text summarization is to condense long documents into short summaries while preserving the content and meaning. It can be performed using two main techniques: extraction and abstraction. The extractive summarization method directly chooses and outputs the salient phrases in the original document (Jing and McKeown (1999); Knight and Marcu (2002)). The abstractive summarization approach involves rewriting the summary (Rush et al. (2015); Liu et al. (2015)); and has seen substantial recent gains due to neural sequence-to sequence models (Chopra et al. (2016); Nallapati et al. (2016a); See et al. (2017a); El-Haj et al. (2018); Paulus et al. (2017) ). In the general case, extractive summarization approaches usually show a better performance compared to the abstractive approaches especially when evaluated using ROU GE metrics (Kiyoumarsi, Mahmoud El-Haj1 Paul Rayson1 2 Samsung R&D Bangalore, India abhishek.s.eee15@iitbhu.ac.in 2015). One of the advantages of the extractive approaches is that they can summarize source articles by extracting salient"
2021.fnp-1.19,D15-1166,0,0.11673,"Missing"
2021.fnp-1.19,W04-3252,0,0.0128422,"en the state. The figure 1 gives a concise description of the end to end summarizer system. the model and calculates respective precision, recall, and F1-score for each measure.3 For the entire dataset, we evaluate standard ROU GE1, ROU GE-2, and ROU GE-L and ROU GE-SU4 (Lin, 2004) on full length F1 (with stemming) following previous works ( See et al. (2017a); Nallapati et al. (2016a) ). The ROU GE 2.0 package Ganesan (2015) is used for calculations. 6.2 Scores In this section, we present results from our experiments and compare with different baselines MUSE (Litvak et al., 2010), Text-rank (Mihalcea and Tarau, 2004), Lex-Rank (Erkan and Radev, 2004), and Polynomial Summarisation (Litvak and Vanetik, 2013). Overall, our model achieves better results than all the proposed baselines with ROU GE1 : 0.52, ROU GE-2 : 0.30, ROU GE-L : 0.46 and ROU GE-SU4 : 0.32 Metric TextRank LexRank Polynomial MUSE rnn-ext + abs + RL R-1/F 0.17 0.26 0.37 0.5 0.52 R-2/F 0.07 0.12 0.12 0.28 0.3 R-L/F 0.21 0.22 0.26 0.45 0.46 Table 1: FNS shared task results 7 Figure 1: The end to end summarizer 5 Experimental setup In order to train our extractor, abstractor and RL models, we use a Tesla P100-PCIE GPU with accelerated high RAM"
2021.fnp-1.19,K16-1028,0,0.0237848,"radients. 1 Introduction The task of text summarization is to condense long documents into short summaries while preserving the content and meaning. It can be performed using two main techniques: extraction and abstraction. The extractive summarization method directly chooses and outputs the salient phrases in the original document (Jing and McKeown (1999); Knight and Marcu (2002)). The abstractive summarization approach involves rewriting the summary (Rush et al. (2015); Liu et al. (2015)); and has seen substantial recent gains due to neural sequence-to sequence models (Chopra et al. (2016); Nallapati et al. (2016a); See et al. (2017a); El-Haj et al. (2018); Paulus et al. (2017) ). In the general case, extractive summarization approaches usually show a better performance compared to the abstractive approaches especially when evaluated using ROU GE metrics (Kiyoumarsi, Mahmoud El-Haj1 Paul Rayson1 2 Samsung R&D Bangalore, India abhishek.s.eee15@iitbhu.ac.in 2015). One of the advantages of the extractive approaches is that they can summarize source articles by extracting salient snippets and sentences directly from these documents, while abstractive approaches rely on word-level attention mechanism to de"
2021.fnp-1.19,2021.sdp-1.12,0,0.0625286,"Missing"
2021.fnp-1.19,D14-1162,0,0.0861837,". In fact, long range dependencies often require complex reasoning and forces models to both locate relevant information and combine it. Models need to ignore a lot of irrelevant text. Many popular algorithms are designed to work in short sequence setting, and have limitations in long setting. RNN/LSTM: process input sequentially and stores relevant information from previous states therefore it is slow for long sequences. Transformers are based on self-attention and cannot process long input with current hardware. (e.g. BERT pre-trained Language model is limited to 512 tokens). such as Glove (Pennington et al., 2014). However the usage of these words will be different and therefore the representation in the vector space should be different as well. The jargon used in financial disclosures is different from ‘general’ language. For example, corporate earnings releases use nuanced language not fully reflected in GloVE vectors pretrained on Wikipedia articles. For all these reasons, working on training custom word embedding for financial domain is helpful in our case. To implement a financial word embedding model using word2vec model, we used the Gensim1 library. We perform pre-processing using the NLTK2 libr"
2021.fnp-1.19,2020.acl-main.458,0,0.0331253,"Missing"
2021.fnp-1.19,D15-1044,0,0.0182912,"sk competition. Keywords: Summarization, Neural networks, Reinforcement learning, sequence to sequence learning; actor-critic methods; policy gradients. 1 Introduction The task of text summarization is to condense long documents into short summaries while preserving the content and meaning. It can be performed using two main techniques: extraction and abstraction. The extractive summarization method directly chooses and outputs the salient phrases in the original document (Jing and McKeown (1999); Knight and Marcu (2002)). The abstractive summarization approach involves rewriting the summary (Rush et al. (2015); Liu et al. (2015)); and has seen substantial recent gains due to neural sequence-to sequence models (Chopra et al. (2016); Nallapati et al. (2016a); See et al. (2017a); El-Haj et al. (2018); Paulus et al. (2017) ). In the general case, extractive summarization approaches usually show a better performance compared to the abstractive approaches especially when evaluated using ROU GE metrics (Kiyoumarsi, Mahmoud El-Haj1 Paul Rayson1 2 Samsung R&D Bangalore, India abhishek.s.eee15@iitbhu.ac.in 2015). One of the advantages of the extractive approaches is that they can summarize source articles by"
2021.fnp-1.19,P17-1099,0,0.0995746,"he task of text summarization is to condense long documents into short summaries while preserving the content and meaning. It can be performed using two main techniques: extraction and abstraction. The extractive summarization method directly chooses and outputs the salient phrases in the original document (Jing and McKeown (1999); Knight and Marcu (2002)). The abstractive summarization approach involves rewriting the summary (Rush et al. (2015); Liu et al. (2015)); and has seen substantial recent gains due to neural sequence-to sequence models (Chopra et al. (2016); Nallapati et al. (2016a); See et al. (2017a); El-Haj et al. (2018); Paulus et al. (2017) ). In the general case, extractive summarization approaches usually show a better performance compared to the abstractive approaches especially when evaluated using ROU GE metrics (Kiyoumarsi, Mahmoud El-Haj1 Paul Rayson1 2 Samsung R&D Bangalore, India abhishek.s.eee15@iitbhu.ac.in 2015). One of the advantages of the extractive approaches is that they can summarize source articles by extracting salient snippets and sentences directly from these documents, while abstractive approaches rely on word-level attention mechanism to determine the most rel"
2021.fnp-1.22,W19-8900,0,0.324639,"summarisation system for financial news was proposed in (Filippova et al., 2009) generating query-based companytailored summaries. This was done through using unsupervised sentence ranking with simple frequency-based features. Recently, statistical features with heuristic approaches have been used to summarise financial textual disclosures (Cardinaels et al., 2019), generating summaries with reduced positive bias, leading to more conservative valuation judgements by investors that receive them. Further, the Financial Narrative Summarisation task (El-Haj, 2019) of the Multiling 2019 workshop (Giannakopoulos, 2019) involved the generation of structured summaries from financial narrative disclosures. Considering this body of work, the Financial Narrative Summarisation task (FNS 2020 (ElHaj et al., 2020a)) task resulted in the first large scale experimental results and state-of-the-art summarisation methods applied to financial data. The task focused on annual reports produced by UK firms listed on the London Stock Exchange (LSE). The shared task was held as part of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020) (El-Haj et al., 2020b). The par"
2021.fnp-1.22,2020.fnp-1.1,1,0.708924,"Missing"
2021.fnp-1.22,2020.fnp-1.20,0,0.405851,"London Stock Exchange (LSE). The shared task was held as part of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020) (El-Haj et al., 2020b). The participating systems used a variety of techniques and methods ranging from rule based extraction methods (Litvak et al., 2020; Vhatkar et al., 2020; Arora and Radhakrishnan, 2020; Azzi and Kang, 2020) to traditional machine learning methods (Suarez et al., 2020; Vhatkar et al., 2020; Arora and Radhakrishnan, 2020) and high performing deep learning models (Agarwal et al., 2020; Singh, 2020; La Quatra and Cagliero, 2020; Vhatkar et al., 2020; Arora and Radhakrishnan, 2020; Azzi and Kang, 2020; Zheng et al., 2020). One of the main challenges and limitations reported by the participants was the average length of annual reports (around 60,000 words), which made the training process difficult as it requires powerful resources (e.g. GPUs) to avoid long training time. In addition, participants argued that extracting both text and structure from PDF files with numerous tables, charts, and numerical data resulted in noisy data being extracted. Such feedback highlights interesting aspects and challenging components o"
2021.fnp-1.22,W13-3111,1,0.925593,"Missing"
2021.fnp-1.22,el-haj-etal-2014-detecting,1,0.876315,"uch a shared task in the FNP 2021 workshop1 . 3 Data Description In the Financial Narrative Summarisation task we focus on annual reports produced by UK firms listed on The London Stock Exchange (LSE). In the UK and elsewhere, annual report structure is much less rigid than those produced in the US. Companies produce glossy brochures with a much looser structure, which makes automatic summarisation of narratives in UK annual reports a challenging task. For the FNS 2021 Shared task2 we use approximately 4,000 UK annual reports for firms listed on LSE, covering the period between 2002 and 2017 (El-Haj et al., 2014, 2019a). We divided the full text within annual reports into training, testing and validation sets providing both the full text of each annual report along with gold-standard summaries. In total there are 3,863 annual reports divided into training, testing and validation sets. Table 1 shows the dataset details. Data Type Report full text Gold summaries Main workshop: cfie/fnp2021/ http://wp.lancs.ac.uk/ Validate 363 1,250 Test 500 1,673 Table 1: FNS 2021 Shared Task Dataset 4 Data Availability For the shared task we first provide the training and validation sets, which include the full text o"
2021.fnp-1.22,P10-1095,1,0.894618,"paratively high requirements of abstractive methods for computational resources and available data. Extractive summarisation utilises scoring approaches to identify and reorder parts of the input (e.g. sentences, phrases and/or passages), using a variety of feature extraction and evaluation methods (Luhn, 1958; Baxendale, 1958; Edmundson, 1969; Mori, 2002; McCargar, 2004; Giannakopoulos et al., 2008). Where adequate data is available, machine learning methods have been employed, such as Hidden Markov Models (Fung and Ngai, 2006), topic-based modelling (Aries et al., 2015), genetic algorithms (Litvak et al., 2010) and clustering methods (Radev et al., 2000; Liu and Lindroos, 2006; Kruengkrai and Jaruskulchai, 2003). The employment of summarisation and natural language processing techniques in general has promising applications in the financial domain (El-Haj et al., 2019b). The SummariserPort system (de Oliveira et al., 2002) has been used to produce summaries for financial news, where it utilized lexical cohesion (Flowerdew and Mahlberg, 2009), using sentence linkage heuristics to generate the output summary. A summarisation system for financial news was proposed in (Filippova et al., 2009) generating"
2021.fnp-1.22,E09-1029,0,0.489442,"c algorithms (Litvak et al., 2010) and clustering methods (Radev et al., 2000; Liu and Lindroos, 2006; Kruengkrai and Jaruskulchai, 2003). The employment of summarisation and natural language processing techniques in general has promising applications in the financial domain (El-Haj et al., 2019b). The SummariserPort system (de Oliveira et al., 2002) has been used to produce summaries for financial news, where it utilized lexical cohesion (Flowerdew and Mahlberg, 2009), using sentence linkage heuristics to generate the output summary. A summarisation system for financial news was proposed in (Filippova et al., 2009) generating query-based companytailored summaries. This was done through using unsupervised sentence ranking with simple frequency-based features. Recently, statistical features with heuristic approaches have been used to summarise financial textual disclosures (Cardinaels et al., 2019), generating summaries with reduced positive bias, leading to more conservative valuation judgements by investors that receive them. Further, the Financial Narrative Summarisation task (El-Haj, 2019) of the Multiling 2019 workshop (Giannakopoulos, 2019) involved the generation of structured summaries from financ"
2021.fnp-1.22,I13-1075,1,0.675922,"Missing"
2021.fnp-1.22,2020.fnp-1.21,1,0.876056,"Missing"
2021.fnp-1.22,2020.fnp-1.22,0,0.589435,"ancial Narrative Summarisation task (FNS 2020 (ElHaj et al., 2020a)) task resulted in the first large scale experimental results and state-of-the-art summarisation methods applied to financial data. The task focused on annual reports produced by UK firms listed on the London Stock Exchange (LSE). The shared task was held as part of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020) (El-Haj et al., 2020b). The participating systems used a variety of techniques and methods ranging from rule based extraction methods (Litvak et al., 2020; Vhatkar et al., 2020; Arora and Radhakrishnan, 2020; Azzi and Kang, 2020) to traditional machine learning methods (Suarez et al., 2020; Vhatkar et al., 2020; Arora and Radhakrishnan, 2020) and high performing deep learning models (Agarwal et al., 2020; Singh, 2020; La Quatra and Cagliero, 2020; Vhatkar et al., 2020; Arora and Radhakrishnan, 2020; Azzi and Kang, 2020; Zheng et al., 2020). One of the main challenges and limitations reported by the participants was the average length of annual reports (around 60,000 words), which made the training process difficult as it requires powerful resources (e.g. GPUs) to av"
2021.fnp-1.22,W04-3252,0,0.306686,"Missing"
2021.fnp-1.22,C02-1018,0,0.784313,"e approaches ((Gupta and Lehal, 2010)), or by generating the summary from scratch (i.e. abstractive methods (Moratanch and Chitrakala, 2016)). Extractive methods have been a popular venue for summarising text due to their relative simplicity and the comparatively high requirements of abstractive methods for computational resources and available data. Extractive summarisation utilises scoring approaches to identify and reorder parts of the input (e.g. sentences, phrases and/or passages), using a variety of feature extraction and evaluation methods (Luhn, 1958; Baxendale, 1958; Edmundson, 1969; Mori, 2002; McCargar, 2004; Giannakopoulos et al., 2008). Where adequate data is available, machine learning methods have been employed, such as Hidden Markov Models (Fung and Ngai, 2006), topic-based modelling (Aries et al., 2015), genetic algorithms (Litvak et al., 2010) and clustering methods (Radev et al., 2000; Liu and Lindroos, 2006; Kruengkrai and Jaruskulchai, 2003). The employment of summarisation and natural language processing techniques in general has promising applications in the financial domain (El-Haj et al., 2019b). The SummariserPort system (de Oliveira et al., 2002) has been used to p"
2021.fnp-1.22,2020.fnp-1.18,0,0.429755,"s listed on the London Stock Exchange (LSE). The shared task was held as part of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020) (El-Haj et al., 2020b). The participating systems used a variety of techniques and methods ranging from rule based extraction methods (Litvak et al., 2020; Vhatkar et al., 2020; Arora and Radhakrishnan, 2020; Azzi and Kang, 2020) to traditional machine learning methods (Suarez et al., 2020; Vhatkar et al., 2020; Arora and Radhakrishnan, 2020) and high performing deep learning models (Agarwal et al., 2020; Singh, 2020; La Quatra and Cagliero, 2020; Vhatkar et al., 2020; Arora and Radhakrishnan, 2020; Azzi and Kang, 2020; Zheng et al., 2020). One of the main challenges and limitations reported by the participants was the average length of annual reports (around 60,000 words), which made the training process difficult as it requires powerful resources (e.g. GPUs) to avoid long training time. In addition, participants argued that extracting both text and structure from PDF files with numerous tables, charts, and numerical data resulted in noisy data being extracted. Such feedback highlights interesting aspect"
C18-1097,N18-1172,0,0.0168678,"CRF (Zhang et al., 2015). Both approaches found that combining the two tasks did not improve results compared to treating the two tasks separately, apart from when considering POS and NEG when the joint task performs better. Finally, Marrese-Taylor et al. (2017) created an attention RNN for this task which was evaluated on two very different datasets containing written and spoken (video-based) reviews where the domain adaptation between the two shows some promise. Overall, within the field of sentiment analysis there are other granularities such as sentence level (Socher et al., 2013), topic (Augenstein et al., 2018), and aspect (Wang et al., 2016; Tay et al., 2017). Aspect-level sentiment analysis relates to identifying the sentiment of (potentially multiple) topics in the 2 http://direct.dei.unipd.it/ http://ecir2016.dei.unipd.it/call_for_papers.html 4 http://coling2018.org/ 3 1134 same text although this can be seen as a similar task to TDSA. However the clear distinction between aspect and TDSA is that TDSA requires the target to be mentioned in the text itself while aspect-level employs a conceptual category with potentially multiple related instantiations in the text. Tang et al. (2016a) created a T"
C18-1097,W17-5202,0,0.0187177,"14) and their own Chinese news comments dataset. They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014), Rec-NN (Dong et al., 2014), TDLSTM (Tang et al., 2016a), Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method. However, the Chinese dataset was not released, and the methods were not compared across all datasets. By contrast, we compare all methods across all datasets, using techniques that are not just from the Recurrent Neural Network (RNN) family. A second paper, by Barnes et al. (2017) compares seven approaches to (document and sentence level) sentiment analysis on six benchmark datasets, but does not systematically explore reproduction issues as we do in our paper. 3 Datasets used in our experiments We are evaluating our models over six different English datasets deliberately chosen to represent a range of domains, types and mediums. As highlighted above, previous papers tend to only carry out evaluations on one or two datasets which limits the generalisability of their results. In this paper, we do not consider the quality or inter-annotator agreement levels of these data"
C18-1097,D17-1047,0,0.101025,"a code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2015; Zhang et al., 2016; Liu and Zhang, 2017; Marrese-Taylor et al., 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017). In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a). Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of Tang et al. (2016a) they also produce different results to the original authors. Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future. In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced. Of course, we would not expect researchers to produce industr"
C18-1097,P14-2009,0,0.0857973,"In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) (Vo and Zhang, 2015), NP with dependency parsing (Wang et al., 2017), and RNN (Tang et al., 2016a), as well as having been applied largely to different datasets. At the end of the paper, we reflect on bringing together elements of repeatability and generalisability which we find are crucial to NLP and data science based disciplines more widely to enable others to make use of the science created. Datasets Methods 1 2 3 4 5 6 Mitchell et al. (2013) 3 Kiritchenko et al. (2014) 3 Dong et al. (2014) 3 Vo and Zhang (2015) 3 3 3 Zhang et al. (2015) 3 Zhang et al. (2016) 3 3 3 Tang et al. (2016a) 3 3 Tang et al. (2016b) 3 Wang et al. (2016) 3 Chen et al. (2017) 3 3 Liu and Zhang (2017) 3 3 3 Wang et al. (2017) 3 3 Marrese-Taylor et al. (2017) 3 3 1=Dong et al. (2014), 2=Wilson (2008), 3=Mitchell et al. (2013), 4=Pontiki et al. (2014), 5=Wang et al. (2017), 6=Marrese-Taylor et al. (2017) Table 1: Methods and Datasets 2 Related work Reproducibility and replicability have long been key elements of the scientific method, but have been gaining renewed prominence recently across a number of disci"
C18-1097,P13-1166,0,0.355059,"Missing"
C18-1097,P11-2008,0,0.0158548,"Missing"
C18-1097,S14-2076,0,0.0779419,"systems has not been a high priority, e.g. in SemEval 2016 task 5 (Pontiki et al., 2016) only 1 out of 21 papers released their source code. In IR, specific reproducible research tracks have been created3 and we are pleased to see the same happening at COLING 20184 . Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (Nasukawa and Yi, 2003) arose as an extension to the coarse grained analysis of document level sentiment analysis (Pang et al., 2002; Turney, 2002). Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014), Recursive Neural Networks (RecNN) (Dong et al., 2014), Recurrent Neural Networks (RNN) (Tang et al., 2016a), attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017), Neural Pooling (NP) (Vo and Zhang, 2015; Wang et al., 2017), RNN combined with NP (Zhang et al., 2016), and attention based neural networks (Tang et al., 2016b). Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitchell et al. (2013) carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neura"
C18-1097,E17-2091,0,0.0519894,"a, review), or medium (written or spoken), and to date there has been no comparative evaluation of methods from these multiple classes. Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2015; Zhang et al., 2016; Liu and Zhang, 2017; Marrese-Taylor et al., 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017). In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a). Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of"
C18-1097,E17-4003,0,0.135492,"leasing source code (Fokkens et al., 2013; Potthast et al., 2016; Sygkounas et al., 2016). The act of reproducing or replicating results is not just for validating research but to also show how it can be improved. Ferro and Silvello (2016) followed up their initial research and were able to analyse which pre-processing techniques were important on a French monolingual dataset and how the different techniques affected each other given an IR system. Fokkens et al. (2013) showed how changes in the five key aspects affected results. The closest related work to our reproducibility study is that of Marrese-Taylor and Matsuo (2017) which they replicate three different syntactic based aspect extraction methods. They found that parameter tuning was very important however using different pre-processing pipelines such as Stanford’s CoreNLP did not have a consistent effect on the results. They found that the methods stated in the original papers are not detailed enough to replicate the study as evidenced by their large results differential. Dashtipour et al. (2016) undertook a replication study in sentiment prediction, however this was at the document level and on different datasets and languages to the originals. In other a"
C18-1097,W17-5213,0,0.145331,"(written or spoken), and to date there has been no comparative evaluation of methods from these multiple classes. Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2015; Zhang et al., 2016; Liu and Zhang, 2017; Marrese-Taylor et al., 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017). In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a). Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of Tang et al. (2016a) they als"
C18-1097,D13-1171,0,0.0489643,"Missing"
C18-1097,W10-0204,0,0.036385,"es issue as originally raised by Wang et al. (2017). As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use. We therefore took the approach of Wang et al. (2017) and found all of the features for each appearance and performed median pooling over features. This change could explain the subtle differences between the results we report and those of the original paper. 4.1.1 Sentiment Lexicons Vo and Zhang (2015) used three different sentiment lexicons: MPQA5 (Wilson et al., 2005), NRC6 (Mohammad and Turney, 2010), and HL7 (Hu and Liu, 2004). We found a small difference in word counts between their reported statistics for the MPQA lexicons and those we performed ourselves, as can be seen in the bold numbers in table 3. Originally, we assumed that a word can only occur in one sentiment class within the same lexicon, and this resulted in differing counts for all lexicons. This distinction is not clearly documented in the paper or code. However, our assumption turned out to be incorrect, giving a further illustration of why detailed descriptions and documentation of all decisions is important. We ran the"
C18-1097,W02-1011,0,0.0240055,"ets and languages to the originals. In other areas of (aspectbased) sentiment analysis, releasing code for published systems has not been a high priority, e.g. in SemEval 2016 task 5 (Pontiki et al., 2016) only 1 out of 21 papers released their source code. In IR, specific reproducible research tracks have been created3 and we are pleased to see the same happening at COLING 20184 . Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (Nasukawa and Yi, 2003) arose as an extension to the coarse grained analysis of document level sentiment analysis (Pang et al., 2002; Turney, 2002). Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014), Recursive Neural Networks (RecNN) (Dong et al., 2014), Recurrent Neural Networks (RNN) (Tang et al., 2016a), attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017), Neural Pooling (NP) (Vo and Zhang, 2015; Wang et al., 2017), RNN combined with NP (Zhang et al., 2016), and attention based neural networks (Tang et al., 2016b). Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitch"
C18-1097,W14-1306,0,0.0258555,"six benchmark datasets, but does not systematically explore reproduction issues as we do in our paper. 3 Datasets used in our experiments We are evaluating our models over six different English datasets deliberately chosen to represent a range of domains, types and mediums. As highlighted above, previous papers tend to only carry out evaluations on one or two datasets which limits the generalisability of their results. In this paper, we do not consider the quality or inter-annotator agreement levels of these datasets but it has been noted that some datasets may have issues here. For example, Pavlopoulos and Androutsopoulos (2014) point out that the Hu and Liu (2004) dataset does not state their inter-annotator agreement scores nor do they have aspect terms that express neutral opinion. We only use a subset of the English datasets available. For two reasons. First, the time it takes to write parsers and run the models. Second, we only used datasets that contain three distinct sentiments (Wilson (2008) only has two). From the datasets we have used, we have only had issue with parsing Wang et al. (2017) where the annotations for the first set of the data contains the target span but the second set does not. Thus making i"
C18-1097,D14-1162,0,0.0823396,"alised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not “set the clipping threshold of softmax layer as 200” (Tang et al., 2016a) as we were unsure what this meant. With regards to the number of epochs trained, we used early stopping with a patience of 10 and allowed 300 epochs. Within their experiments they used SSWE (Tang et al., 2014) and Glove Twitter vectors11 (Pennington et al., 2014). As the paper being reproduced does not define the number of epochs they trained for, we use early stopping. Thus for early stopping we require to split the training data into train and validation sets to know when to stop. As it has been shown by Reimers and Gurevych (2017) that the random seed statistically significantly changes the results of experiments we ran each model over each word embedding thirty times, using a different seed value but keeping the same stratified train and validation split, and 11 https://nlp.stanford.edu/projects/glove/ 1139 Macro F1 Methods O R (Max) R (Mean) LSTM"
C18-1097,S14-2004,0,0.121809,"Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 We follow the definitions in Antske Fokkens’ guest blog post “replication (obtaining the same results using the same experiment) as well as reproduction (reach the same conclusion through different means)” from http://coling2018. org/slowly-growing-offspring-zigglebottom-anno-2017-guest-post/ 1132 Proceedings of the 27th International Conference on Computational Linguistics, pages 1132–1144 Santa Fe, New Mexico, USA, August 20-26, 2018. are evaluated on the SemEval dataset (Pontiki et al., 2014) but not all. Datasets can vary by domain (e.g. product), type (social media, review), or medium (written or spoken), and to date there has been no comparative evaluation of methods from these multiple classes. Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside th"
C18-1097,D17-1035,0,0.0209948,"nd we did not “set the clipping threshold of softmax layer as 200” (Tang et al., 2016a) as we were unsure what this meant. With regards to the number of epochs trained, we used early stopping with a patience of 10 and allowed 300 epochs. Within their experiments they used SSWE (Tang et al., 2014) and Glove Twitter vectors11 (Pennington et al., 2014). As the paper being reproduced does not define the number of epochs they trained for, we use early stopping. Thus for early stopping we require to split the training data into train and validation sets to know when to stop. As it has been shown by Reimers and Gurevych (2017) that the random seed statistically significantly changes the results of experiments we ran each model over each word embedding thirty times, using a different seed value but keeping the same stratified train and validation split, and 11 https://nlp.stanford.edu/projects/glove/ 1139 Macro F1 Methods O R (Max) R (Mean) LSTM 64.70 64.34 60.69 TDLSTM 69.00 67.04 65.63 TCLSTM 69.50 67.66 65.23 O=Original, R=Reproduction Table 5: LSTM Final Results Figure 4: Distribution of the LSTM results reported the results on the same test data as the original paper. As can be seen in Figure 4, the initial see"
C18-1097,D13-1170,0,0.0201533,"then extended using a neural CRF (Zhang et al., 2015). Both approaches found that combining the two tasks did not improve results compared to treating the two tasks separately, apart from when considering POS and NEG when the joint task performs better. Finally, Marrese-Taylor et al. (2017) created an attention RNN for this task which was evaluated on two very different datasets containing written and spoken (video-based) reviews where the domain adaptation between the two shows some promise. Overall, within the field of sentiment analysis there are other granularities such as sentence level (Socher et al., 2013), topic (Augenstein et al., 2018), and aspect (Wang et al., 2016; Tay et al., 2017). Aspect-level sentiment analysis relates to identifying the sentiment of (potentially multiple) topics in the 2 http://direct.dei.unipd.it/ http://ecir2016.dei.unipd.it/call_for_papers.html 4 http://coling2018.org/ 3 1134 same text although this can be seen as a similar task to TDSA. However the clear distinction between aspect and TDSA is that TDSA requires the target to be mentioned in the text itself while aspect-level employs a conceptual category with potentially multiple related instantiations in the text"
C18-1097,P14-1146,0,0.331639,"Gated Recurrent Unit (GRU) which they called Recurrent Attention on Memory (RAM), and they found this method to allow models to better understand more complex sentiment for each comparison. Vo and Zhang (2015) used neural pooling features e.g. max, min, etc of the word embeddings of the left and right context of the target word, the target itself, and the whole Tweet. They inputted the features into a linear SVM, and showed the importance of using the left and right context for the first time. They found in their study that using a combination of Word2Vec embeddings and sentiment embeddings (Tang et al., 2014) performed best alongside using sentiment lexicons to filter the embedding space. Other studies have adopted more linguistic approaches. Wang et al. (2017) extended the work of Vo and Zhang (2015) by using the dependency linked words from the target. Dong et al. (2014) used the dependency tree to create a Recursive Neural Network (RecNN) inspired by Socher et al. (2013) but compared to Socher et al. (2013) they also utilised the dependency tags to create an Adaptive RecNN (ARecNN). Critically, the methods reported above have not been applied to the same datasets, therefore a true comparative e"
C18-1097,C16-1311,0,0.0786875,"across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2015; Zhang et al., 2016; Liu and Zhang, 2017; Marrese-Taylor et al., 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017). In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a). Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of Tang et al. (2016a) they also produce different results to the original authors. Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future. In some"
C18-1097,D16-1021,0,0.0623451,"across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2015; Zhang et al., 2016; Liu and Zhang, 2017; Marrese-Taylor et al., 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017). In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a). Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of Tang et al. (2016a) they also produce different results to the original authors. Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future. In some"
C18-1097,P02-1053,0,0.0165026,"o the originals. In other areas of (aspectbased) sentiment analysis, releasing code for published systems has not been a high priority, e.g. in SemEval 2016 task 5 (Pontiki et al., 2016) only 1 out of 21 papers released their source code. In IR, specific reproducible research tracks have been created3 and we are pleased to see the same happening at COLING 20184 . Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (Nasukawa and Yi, 2003) arose as an extension to the coarse grained analysis of document level sentiment analysis (Pang et al., 2002; Turney, 2002). Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014), Recursive Neural Networks (RecNN) (Dong et al., 2014), Recurrent Neural Networks (RNN) (Tang et al., 2016a), attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017), Neural Pooling (NP) (Vo and Zhang, 2015; Wang et al., 2017), RNN combined with NP (Zhang et al., 2016), and attention based neural networks (Tang et al., 2016b). Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitchell et al. (201"
C18-1097,D16-1058,0,0.0812728,"m these multiple classes. Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2015; Zhang et al., 2016; Liu and Zhang, 2017; Marrese-Taylor et al., 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017). In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a). Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of Tang et al. (2016a) they also produce different results to the original authors. Our observations within th"
C18-1097,E17-1046,0,0.0674324,"date there has been no comparative evaluation of methods from these multiple classes. Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2015; Zhang et al., 2016; Liu and Zhang, 2017; Marrese-Taylor et al., 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017). In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a). Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of Tang et al. (2016a) they also produce different"
C18-1097,H05-1044,0,0.00971972,"me target multiple appearances issue as originally raised by Wang et al. (2017). As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use. We therefore took the approach of Wang et al. (2017) and found all of the features for each appearance and performed median pooling over features. This change could explain the subtle differences between the results we report and those of the original paper. 4.1.1 Sentiment Lexicons Vo and Zhang (2015) used three different sentiment lexicons: MPQA5 (Wilson et al., 2005), NRC6 (Mohammad and Turney, 2010), and HL7 (Hu and Liu, 2004). We found a small difference in word counts between their reported statistics for the MPQA lexicons and those we performed ourselves, as can be seen in the bold numbers in table 3. Originally, we assumed that a word can only occur in one sentiment class within the same lexicon, and this resulted in differing counts for all lexicons. This distinction is not clearly documented in the paper or code. However, our assumption turned out to be incorrect, giving a further illustration of why detailed descriptions and documentation of all d"
C18-1097,D15-1073,0,0.0816182,"domain (e.g. product), type (social media, review), or medium (written or spoken), and to date there has been no comparative evaluation of methods from these multiple classes. Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2015; Zhang et al., 2016; Liu and Zhang, 2017; Marrese-Taylor et al., 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017). In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a). Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017"
C18-1252,C16-1091,0,0.0246546,"Missing"
C18-1252,P06-4018,0,0.0411018,"Missing"
C18-1252,C16-1264,0,0.0623305,"Missing"
C18-1252,P14-2009,0,0.0844025,"Missing"
C18-1252,J00-4006,0,0.177228,"Missing"
C18-1252,P11-1015,0,0.0606219,"Missing"
C18-1252,C18-1097,1,0.882458,"Missing"
C18-1252,P14-1146,0,0.0569795,"Missing"
C18-1252,C16-1311,0,0.0788489,"Missing"
E06-2014,bennison-bowker-2000-designing,0,0.0158976,"or a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus. 1 Introduction It is widely acknowledged that human translators can benefit from a wide range of applications in computational linguistics, including Machine Translation (Carl and Way, 2003), Translation Memory (Planas and Furuse, 2000), etc. There have been recent research on tools detecting translation equivalents for technical vocabulary in a restricted domain, e.g. (Dagan and Church, 1997; Bennison and Bowker, 2000). The methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (Déjean et al., 2002). In this project we concentrate on words from the general lexicon instead of terminology. The rationale for this focus is related to the fact that translation of terms is (should be) stable, while general words can vary significantly in their translation. It is important to populate the terminological database with terms that are missed in dictionaries or specific to a problem domain. However"
E06-2014,C02-1166,0,0.0130442,"duction It is widely acknowledged that human translators can benefit from a wide range of applications in computational linguistics, including Machine Translation (Carl and Way, 2003), Translation Memory (Planas and Furuse, 2000), etc. There have been recent research on tools detecting translation equivalents for technical vocabulary in a restricted domain, e.g. (Dagan and Church, 1997; Bennison and Bowker, 2000). The methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (Déjean et al., 2002). In this project we concentrate on words from the general lexicon instead of terminology. The rationale for this focus is related to the fact that translation of terms is (should be) stable, while general words can vary significantly in their translation. It is important to populate the terminological database with terms that are missed in dictionaries or specific to a problem domain. However, once the translation of a term in a domain has been identified, stored in a dictionary and learned by the translator, the process of translation can go on without consulting a dictionary or a corpus. In"
E06-2014,C00-2090,0,0.0118943,"contextual examples of translation equivalents for words from the general lexicon using comparable corpora and semantic annotation that is uniform for the source and target languages. For a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus. 1 Introduction It is widely acknowledged that human translators can benefit from a wide range of applications in computational linguistics, including Machine Translation (Carl and Way, 2003), Translation Memory (Planas and Furuse, 2000), etc. There have been recent research on tools detecting translation equivalents for technical vocabulary in a restricted domain, e.g. (Dagan and Church, 1997; Bennison and Bowker, 2000). The methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (Déjean et al., 2002). In this project we concentrate on words from the general lexicon instead of terminology. The rationale for this focus is related to the fact that translation of terms is (should be) stable, while general wor"
E06-2014,rapp-2004-freely,0,0.0194653,"are common for concepts in both languages. The search space is further restricted by applying knowledge-based and statistical filters (such as part-of-speech and semantic class filters, IDF filter, etc), by testing the co-occurrence of members of different similarity classes or by manually selecting the presented variants. These procedures are elementary building blocks that are used in designing different search strategies efficient for different types of translation equivalents 4 Simclasses consist of words sharing collocates and are computed using Singular Value Decomposition, as used by (Rapp, 2004), e.g. Paris and Strasbourg are produced for Brussels, or bus, tram and driver for passenger. and contexts. The core functionality of the system is intended to be self-explanatory and to have a shallow learning curve: in many cases default search parameters work well, so it is sufficient to input a word or an expression in the source language in order to get back a useful list of translation equivalents, which can be manually checked by a translator to identify the most suitable solution for a given context. For example, the word combination frustrated passenger is not found in the major Engli"
L16-1038,2009.mtsummit-posters.15,0,0.0106473,"rsions (0.035) which indicates the importance of the presence of diacritics, which plays a vital role in determining the text ease of reading. On the other hand the OSMAN scores (0.329) for the diacriticised Arabic showed a higher positive correlation with the English Flesch scores. This was noticed when measuring the Flesch score difference between the two Arabic texts. We found the diacriticised text to score lower (harder to read) when there are more syllables. This is not surprising as the UN resolution data are of high quality of translation and strict adherence to editorial conventions (Rafalovitch and Dale, 2009). We measured mean and standard deviation of each metric on each version of the corpus. We would expect that good measures should show the same variability, and for the UN corpus which is fairly homogeneous and that this variability should be quite low. Here again, our OSMAN measure performs consistently on the diacriticised Arabic (mean: 25.89 and stdev: 9.08) with the Flesch on English (mean: 27.26 and stdev: 8.63). The non-diacriticised Arabic mean (88.49) shows how the absence of diacritics (thus syllables and complex words) resulting in inaccurate scores indicating the text to be easy to"
L16-1038,P05-1065,0,0.359439,"Missing"
L16-1287,P07-1124,0,0.0380372,"narratives. Section 3. describes the dataset that we have collated for our experiments. We explain our process of manual coding in section 4. and the ML models in section 5. Results and discussion appear in section 6. and we conclude in section 7. 2. Related Work There has been growing interest in recent years in the application of Natural Language Processing (NLP) and text analysis techniques in the financial domain. One of the largest areas of recent research has been the application of sentiment analysis to stock–related tweets with the intention of predicting the stock market performance (Devitt and Ahmad, 2007; Schumaker, 2010; Im et al., 2013; Ferreira et al., 2014; Neuenschwander et al., 2014). Some studies have taken this a step further by trying to explain the impact on investors’ behaviour from negative reports in the financial media of corporate actions (Moniz and de Jong, 2014). The work by Shuyu (2016) describes the use of computer techniques to measure causal reasoning in financial earningsrelated outcomes of a large sample of 10-K (annual reports) filings of US firms. Their work showed positive and sig1820 nificant association between firms’ causal reasoning intensity and other analyst ea"
L16-1287,N09-1031,0,0.121834,"rge sample of 10-K (annual reports) filings of US firms. Their work showed positive and sig1820 nificant association between firms’ causal reasoning intensity and other analyst earning and forecasts. In their work they focused on using non-language dependent approaches by applying simple text analysis techniques and frequency count using PERL. Whitelaw and Patrick (2004) and Goel and Gangolly (2012) have investigated systemic and other predictive features for the task of identifying financial scam documents. NLP researchers have also attempted to develop empirical techniques for ranking risk (Kogan et al., 2009; Tsai and Wang, 2012) particularly in the American context. In the Accounting and Finance literature, there is an increasing body of work using basic word-list and ML approaches to examine the information content of forward-looking statements (e.g., Li (Li, 2010)) and other work has looked at the relationship of optimistic versus mild statements in the context of positive and negative financial performance (Chen et al., 2013). However, there is no large-scale empirical work on detecting and measuring tone and attribution in financial narratives. In terms of novelty in the ML experiments over"
L16-1287,W02-1011,0,0.0159002,"+ + Key + POST + Sem POST SMO 76.3 76.2 76.3 76.3 LR 79.1 79.1 79.7 76.3 RF 76.9 76.1 78.3 75.5 NB 69.9 72.8 79.2 75.9 Table 6: Most Frequent Class Results Table 3: Effect of attribution’s tone on attribution could help in detecting attribution’s tone. In addition, the results show that part of speech tagging helped in slightly enhancing the detection process, this shows that singular and plural pronouns in addition to past and present verbs and the shift between them could help in telling when there exist a positive or negative internal or external attribution. This aligns with Pang et al. (Pang et al., 2002) where using POS tends to improve accuracy for a NB classifier. The use of singular and plural pronouns could be considered as an indicator of attribution’s positivity where a company usually tends to speak positively when discussing attribution by its own management. ML Att + Key + POST SMO 75.7 LR 76.7 RF 74.0 NB 69.1 Att Key + Att + POST 75.7 77.0 73.8 70.4 75.8 77.7 76.6 74.6 Att Sem Accuracy 71.0 56.8 59.9 + We also ran another experiment using Weka’s StringToWordVector unsupervised filter to convert all PEA sentences into vectors of words contained in the sentences. We kept the top 100 w"
L16-1287,W10-0502,0,0.0317338,"escribes the dataset that we have collated for our experiments. We explain our process of manual coding in section 4. and the ML models in section 5. Results and discussion appear in section 6. and we conclude in section 7. 2. Related Work There has been growing interest in recent years in the application of Natural Language Processing (NLP) and text analysis techniques in the financial domain. One of the largest areas of recent research has been the application of sentiment analysis to stock–related tweets with the intention of predicting the stock market performance (Devitt and Ahmad, 2007; Schumaker, 2010; Im et al., 2013; Ferreira et al., 2014; Neuenschwander et al., 2014). Some studies have taken this a step further by trying to explain the impact on investors’ behaviour from negative reports in the financial media of corporate actions (Moniz and de Jong, 2014). The work by Shuyu (2016) describes the use of computer techniques to measure causal reasoning in financial earningsrelated outcomes of a large sample of 10-K (annual reports) filings of US firms. Their work showed positive and sig1820 nificant association between firms’ causal reasoning intensity and other analyst earning and forecas"
L16-1287,C12-3056,0,0.014444,"annual reports) filings of US firms. Their work showed positive and sig1820 nificant association between firms’ causal reasoning intensity and other analyst earning and forecasts. In their work they focused on using non-language dependent approaches by applying simple text analysis techniques and frequency count using PERL. Whitelaw and Patrick (2004) and Goel and Gangolly (2012) have investigated systemic and other predictive features for the task of identifying financial scam documents. NLP researchers have also attempted to develop empirical techniques for ranking risk (Kogan et al., 2009; Tsai and Wang, 2012) particularly in the American context. In the Accounting and Finance literature, there is an increasing body of work using basic word-list and ML approaches to examine the information content of forward-looking statements (e.g., Li (Li, 2010)) and other work has looked at the relationship of optimistic versus mild statements in the context of positive and negative financial performance (Chen et al., 2013). However, there is no large-scale empirical work on detecting and measuring tone and attribution in financial narratives. In terms of novelty in the ML experiments over and above the state of"
L16-1287,U04-1013,0,0.0196724,"investors’ behaviour from negative reports in the financial media of corporate actions (Moniz and de Jong, 2014). The work by Shuyu (2016) describes the use of computer techniques to measure causal reasoning in financial earningsrelated outcomes of a large sample of 10-K (annual reports) filings of US firms. Their work showed positive and sig1820 nificant association between firms’ causal reasoning intensity and other analyst earning and forecasts. In their work they focused on using non-language dependent approaches by applying simple text analysis techniques and frequency count using PERL. Whitelaw and Patrick (2004) and Goel and Gangolly (2012) have investigated systemic and other predictive features for the task of identifying financial scam documents. NLP researchers have also attempted to develop empirical techniques for ranking risk (Kogan et al., 2009; Tsai and Wang, 2012) particularly in the American context. In the Accounting and Finance literature, there is an increasing body of work using basic word-list and ML approaches to examine the information content of forward-looking statements (e.g., Li (Li, 2010)) and other work has looked at the relationship of optimistic versus mild statements in the"
L16-1289,J13-4005,0,0.0399108,"Missing"
L16-1289,C10-2115,0,0.0893866,"Missing"
L16-1416,J03-1002,0,0.00536035,"lexicon entries to their translation equivalents in these languages using bilingual dictionaries and other freely available bilingual lexicons (see Piao et al., 2015 for a full description of this process and resources used). In the case of Czech, a large parallel corpus InterCorp (Čermák and Rosen, 2012) (version 8) was used to automatically extract a Czech--English bilingual dictionary. In particular, we used its manually sentence-aligned fiction core data, as well as journalistic packages from PressEurop/VoxEurop and Project Syndicate. Next, an automatic word-to-word alignment with GIZA++ (Och and Ney, 2003) was carried out, followed by a summarisation of the resulting word pairs as possible translation equivalents, where the summarisation process kept the POS differences of the words, i.e. if a word has multiple POS tags the word-tag pairs form separate entries in the translation lexicon. Finally, the Czech semantic lexicon was generated automatically by transferring the English semantic tags to Czech via the aligned word translation equivalents. The multilingual WordNet was also tested to extract a bilingual lexicon for the Malay language by porting the semantic lexicons via synset IDs, where t"
L16-1416,N15-1137,1,0.623521,"onstructing larger-scale and higher-quality multilingual semantic lexical resources and corpus annotation tools. In this paper, we focus on the lexical coverage of multilingual semantic lexicon resources developed by the UCREL 3 team based at Lancaster University, in collaboration with partner research teams. This multilingual resource is an extension of the core USAS English semantic lexicon, which provides a knowledge base for the USAS semantic annotation system (Rayson et al., 2004), and has been continuously expanded during a number of projects (Löfberg et al., 2005; Mudraya et al., 2006; Piao et al., 2015). Different from many existing lexical resources, which are built as independent lexical knowledge bases, our semantic lexicons form components of the USAS system, in which the lexicons and software framework are integrated seamlessly to provide a software system for automatically annotating text. In addition, they all apply a common unified lexicographically inspired framework via the same semantic taxonomy across all languages. This enables us to port the USAS software rapidly by generating the semantic lexicons for new languages with a pre-defined information and presentation format. The le"
L16-1416,piao-etal-2004-evaluating,1,0.551546,"Missing"
L18-1158,L18-1623,1,0.246805,"ification scheme and tagset for Welsh language. During the course of development, we have first constructed large Welsh semantic lexicons containing approximately 136,468 Welsh word entries, which provide a lexical knowledge base for the semantic tagging system3 . Based on the lexicons, we have developed an initial version of the tagger software system that is designed to accommodate different Welsh POS taggers and tagsets that exist or will be developed. In the CorCenCC project, CySemTagger will be mainly based on a new Welsh POS tagger, named CyTag, which has been developed in this Project (Neale et al., 2018). In this paper, we describe the CySemTagger system and report on its initial evaluation. 1 For further details of USAS, see website http://ucrel.lancs.ac.uk/usas/ 2 For information about this project, see website http://www.corcencc.org/ 3 These lexicons are made available at https://github.com/UCREL/Multilingual-USAS The remaining part of this paper is organised as follows. Section 2 will discuss related work, Section 3 will describe the architecture of the CySemTagger, Section 4 will discuss the construction of Welsh semantic lexicon, Section 5 will explain about detailed working mechanism"
L18-1158,padro-stanilovsky-2012-freeling,0,0.0255034,"Section 6 will discuss an evaluation of the current version of CySemTagger, and Section 7 will conclude our work and discuss future work. 2. Related Work Over recent years, various semantic annotation tools have been developed in the NLP community. These tools are used to automatically recognise and annotate various semantic categories and concepts at different syntactic levels, such as word level, phrase level, sentence level etc. Among the major existing semantic taggers developed in NLP communities is USAS (Rayson et al., 2004; Piao et al., 2017), GATE4 (Cunningham et al., 2011), Freeling (Padro and Stanilovsky, 2012), NLTK5 (Bird et al., 2009) etc., which provide functionalities of semantic annotation of various types, such as WordNet’s Word sense IDs or Named Entity types etc. For example, GATE and KIM (Popov et al., 2003), combined together, provide multilingual semantic tagging function based on ontologies. Freeling is capable of detecting and tagging multilingual texts with named entity types and WordNet senses. Zhang and Rettinger (2014) developed a toolkit that carries out Wikipedia-based annotation. NLTK (Bird et al., 2009) provides a function for analysing the meaning of sentences. What is directl"
L18-1158,N15-1137,1,0.859116,"Missing"
L18-1158,L16-1416,1,0.851768,"ons along with useful information such as lemma forms and part-of-speech (POS) labels. It also contains many Welsh multi-word expressions (MWEs), which is a valuable resource for creating semantic MWE lexicons for the semantic tagger in later stages. Because it is time-consuming work to manually compile new semantic lexicons from scratch, we applied automatic methods by mapping and porting semantic categories and tags for Welsh words via their English translations through the existing English semantic lexicons. This method has been proven effective in our previous research on other languages (Piao et al., 2016). The high quality of the Eurfa bilingual lexicon helped us to achieve a good initial result for the automatically generated Welsh semantic lexicon. Obviously, the automatically generated lexicon will need be pruned manually or by other methods to guarantee the accuracy of the semantic annotation. Through the automatic process, we extracted a lexicon containing 136,468 Welsh words (including many inflected forms) mapped to semantic category/ies. It provides a solid basis for developing a system of Welsh semantic tagger. In addition to the automatic lexicon generation, we also collected 264 Wel"
L18-1158,E14-2004,0,0.0308656,"Among the major existing semantic taggers developed in NLP communities is USAS (Rayson et al., 2004; Piao et al., 2017), GATE4 (Cunningham et al., 2011), Freeling (Padro and Stanilovsky, 2012), NLTK5 (Bird et al., 2009) etc., which provide functionalities of semantic annotation of various types, such as WordNet’s Word sense IDs or Named Entity types etc. For example, GATE and KIM (Popov et al., 2003), combined together, provide multilingual semantic tagging function based on ontologies. Freeling is capable of detecting and tagging multilingual texts with named entity types and WordNet senses. Zhang and Rettinger (2014) developed a toolkit that carries out Wikipedia-based annotation. NLTK (Bird et al., 2009) provides a function for analysing the meaning of sentences. What is directly related to our work is the past development of multilingual functionality of the USAS framework. As mentioned earlier, initially developed for English, it has been extended and modified to cover an increasing number of languages. Currently USAS is capable of carrying out semantic annotation on 12 languages, including Italian, Finish, Russian, Chinese, Spanish, Portuguese, Swedish, 4 5 980 See website https://gate.ac.uk/ http://w"
L18-1573,W09-0807,0,0.0450668,"itful in fields such as natural language processing, corpus linguistics and machine translation (Zaidan and Callison-Burch, 2014). The process of hiring human participants to identify dialects is very costly and it is a time consuming job. Therefore, machine automation could work as a quick and cheap alternative provided we can create an effective mix of new methods with the appropriate dataset. In this study, we tackle the problem of automatically identifying Arabic dialects using a variety of approaches in order to address bivalency and dialectal written code-switching (Habash et al., 2008; Biadsy et al., 2009) which pose significant challenges for existing approaches. We apply our novel Subtractive Bivalency Profiling (SBP) approach to address the issue of bivalent words across the Arabic dialects examined here. The results show that our new methods can achieve good levels of accuracy on unseen data. Bivalency is defined by Woolard and Genovese (2007) as the “simultaneous membership of a given linguistic segment in more than one linguistic system in a contact setting”. It is typically a feature of linguistic codes that are closely related to each other, like Standard Arabic and the various Arabic c"
L18-1573,L16-1038,1,0.70631,"fication (Holmes, 1994). TTR is the ratio obtained by dividing the total number of different words (i.e. types) occurring in a text by the total number of words (tokens). Higher TTR indicates a high degree of lexical variation. We calculated TTR by simply dividing the number of types by the number of tokens in each instance (Holmes, 1994): TTR = types tokens We normalised the output by dividing by the number of sentences in each instance, this was achieved by using the Stanford Arabic sentence splitter5 . Furthermore, we measured the readability of the text using the OSMAN readability metric (El-Haj and Rayson, 2016). In addition to providing a readability score between 0 (hard to read) and 100 (easy to read), OSMAN also provides information about the number of syllables, hard words (words with more than 5 letters), complex words (&gt;4 syllables) and Faseeh (aspects of script usually dropped in informal Arabic writing). 4.2.3. Subtractive Bivalency Profiling As mentioned earlier the dataset contains a high level of language bivalency, which is typical when speakers switch between closely related language varieties. We used an approach influenced by earlier work in corpus linguistics in order to select featu"
L18-1573,W14-3911,0,0.0506109,"Missing"
L18-1573,W14-1300,0,0.140833,"bic writing system as “ ÕÎ¯”. Hence, written bivalency (bivalency hereafter) is not simply the result of overlap in vocabulary but also the loss of important linguistic information when different pronunciations are encoded using the same standard representation in Arabic script. 2. Related Work For differentiating texts at the language level, comparing the relative ranks of character n-grams has proved to be a very successful approach (Cavnar and Trenkle, 1994). Recent research has tackled language identification in noisy settings such as online forums and social media using ensemble methods (Lui and Baldwin, 2014) or more complex statistical approaches (Abainia et al., 2016). Much other research focuses on language identification or recognition from speech signals but that is out of scope for this paper. In the area of corpus linguistics, language identification is not studied directly, but the field has a long history of comparing language varieties and has developed a number of approaches to explore this issue e.g. keywords used in an American versus British English study (Hofland and Johansson, 1982) and multidimensional approaches (Biber, 1988). 3622 Previous work on the more fine-grained task of d"
L18-1573,J14-1006,0,0.238109,"nar and Trenkle, 1994; Dunning, 1994; Souter et al., 1994). More recently, this has been seen as a classification problem where machine learning is used to distinguish between languages (Gupta et al., 2015). The vast majority of language identification research has focused on differentiating between languages. In this paper, we instead focus on differentiating regional varieties of the same language (i.e. dialects), taking Arabic as our case study. Automatically identifying dialects could prove fruitful in fields such as natural language processing, corpus linguistics and machine translation (Zaidan and Callison-Burch, 2014). The process of hiring human participants to identify dialects is very costly and it is a time consuming job. Therefore, machine automation could work as a quick and cheap alternative provided we can create an effective mix of new methods with the appropriate dataset. In this study, we tackle the problem of automatically identifying Arabic dialects using a variety of approaches in order to address bivalency and dialectal written code-switching (Habash et al., 2008; Biadsy et al., 2009) which pose significant challenges for existing approaches. We apply our novel Subtractive Bivalency Profilin"
L18-1726,L16-1073,0,0.0190377,"nding useful information from the plethora of biomedical scientific literature which are manually unmanageable. Kann (2007) also suggested that Text Mining approaches are essential for discovering information about disease and protein interactions buried within millions of biomedical records. Since the recognition of the importance of the Biomedical text mining, a variety of NLP tools have been developed and modified to support it. Among the main tools and corpora developed for such purposes include Genia tagger/corpus (Tsuruoka et al., 2005; Thompson et al., 2017), Termine1 , and LAPPS GRID (Ide et al., 2016). These tools have typically focused only on lexical, syntactic and shallow semantic (named-entity) approaches. Another related biomedical annotation tool is the Penn BioTagger2 (Jin et al., 2006), which is capable of tagging gene entities, genomic variations entities and malignancy type entities. Despite the progress over the past years, there are still various issues which remain unsolved, including the lack of NLP tools tailored for specific subfields of biomedical research, and the need to link entities at the conceptual level. In this work, we report on our experiment in which we modify a"
L18-1726,W00-0901,1,0.565566,"to link entities at the conceptual level. In this work, we report on our experiment in which we modify a semantic tagger and create a corpus semantically tagged with both related sub-sets of the Gene Ontology categories and generic semantic field categories for an aetiology study. Comparing corpora is a key method in corpus linguistics, and is a vital step towards measuring the differences between collections of textual documents. Previous approaches have been focused on word level comparisons only, finding terms or keywords that can differentiate one corpus from the other (Kilgarriff, 2001; Rayson and Garside, 2000). When the method is applied at the semantic level (for example with the general purpose USAS taxonomy3 ), this enables confirmation of the word-level findings but also the ability to uncover key semantic categories, which are more dispersed across a wider group of words and would not otherwise be highlighted as key (Rayson, 2008). In a medical context, we hypothesise that it is impor4593 1 http://www.nactem.ac.uk/software/termine/ http://seas.upenn.edu/∼strctlrn/BioTagger/BioTagger.html 3 http://ucrel.lancs.ac.uk/usas/ 2 tant to use a more fine-grained semantic taxonomy which embodies greater"
N15-1137,R09-1010,0,0.0332526,"ide multilingual semantic annotation functionalities based on ontologies. Freeling (Padró et al., 2012) provides multilingual annotations such as named entity recognition and WordNet sense tagging. Recent developments in this area include Zhang and Rettinger’s work (2014) in which they tested a toolkit for Wikipedia-based annotation (wikification) of multilingual texts. However, in the work described here we employ a lexicographically-informed semantic classification scheme and we perform all1269 words annotation. In terms of porting tools from one language to another by translating lexicons, Brooke et al. (2009) obtained poor results from a small dictionary in cross-linguistic sentiment analysis. 3 Generating Multilingual Semantic Lexicons by Automatic Mapping The USAS tagger relies heavily on the semantic dictionary as its knowledge source, so the main task in the development of our prototype semantic annotation tools for new languages was to generate semantic lexicons, both for single word and multiword expressions (MWE), in which words and MWEs can be associated with appropriate semantic tags. For this purpose, our approach involves mapping existing English semantic lexicons into target languages"
N15-1137,mcenery-xiao-2004-lancaster,0,0.0204004,"ions: lexical coverage and annotation precision. The lexical coverage is a particularly interesting metric for our evaluation, as we expect this is where an automatic approach can make significant contribution to the development of annotation systems. On the other hand, high annotation precision normally entails manual improvement of the lexical resources or a period of training on manually tagged corpora. 1271 For the lexical coverage evaluation, three reference corpora were chosen: PAISÀ Italian corpus (Borghetti et al., 2011), LCMC Corpus (Lancaster Corpus of Mandarin Chinese) (McEnery and Xiao, 2004) and Lacio-Ref Portuguese corpus (Aluisio et al., 2003). Because PAISÀ and Lacio-Ref corpora are too large for our purpose, we extracted subsections of about 1.5 million Italian words and 1.7 million Portuguese words from them. For the evaluation, we annotated the corpus data using the annotation tools of the corresponding target languages, and examined what percentage of the words were assigned with semantic tags. Punctuation marks were excluded in this evaluation process. Table 3 shows the statistics of the evaluation for each language. Language Italian Chinese Portuguese Average Number of w"
N15-1137,padro-stanilovsky-2012-freeling,0,0.138063,"Missing"
N15-1137,N03-1033,0,0.00816363,"3,100 64,413 13,942 MWE entries 5,622 19,039 1,799 Table 2: Sizes of current semantic lexicons. 4 Architecture of Annotation System Based on the multilingual semantic lexicons described in the previous section, prototype semantic taggers were built for the three languages by deploying the lexicons into the existing software architecture, which employs disambiguation methods reported by Rayson et al. (2004). A set of POS tagging tools were incorporated to pre-process texts from the target languages. The TreeTagger (Schmid, 1994) was used for Italian and Portuguese, and the Stanford POS tagger (Toutanova et al., 2003) was used for Chinese. These tools and semantic lexicon look-up components form pipelines to annotate words in running texts. Figure 1 shows the architecture of the software framework. raw text context rules mwe lexicon pos tagger lemmatizer sem tagger word lexicon annotated Figure 1: Architecture of the semantic tagger. 5 Evaluation of Prototype System Following the initial manual evaluation of the prototype semantic taggers described in section 3, we then carried out larger scale automatic evaluations using a set of sample corpora. We conducted two complementary types of evaluations: lexical"
N15-1137,E14-2004,0,0.136444,"Missing"
P19-1281,W18-2501,0,0.0185342,"een eight Target Dependent Sentiment Analysis (TDSA) models based on their macro F1 score. We consider two variants of four reimplementations of well-known TDSA models: ATAE (Wang et al., 2016), IAN (Ma et al., 2017), TDLSTM (Tang et al., 2016) (without target words in the left and right LSTM), and a non-targetaware LSTM method used as the baseline in Tang et al. (2016). These methods represent state-of-the-art within TDSA, with only small differences in performance between TDLSTM, IAN, and ATAE (see figure 3). All the models are re-implemented in PyTorch (Paszke et al., 2017) using AllenNLP (Gardner et al., 2018). To ensure the only difference between the models is their network architecture the models use the same optimiser settings and the same regularisation. All words are lower cased and we use the same Glove common crawl 840B token 300 dimension word embedding (Pennington et al., 2014). We use variational (Gal and Ghahramani, 2016) and regular (Hinton et al., 2012) dropout for regularisation and an ADAM (Kingma and Ba, 2014) optimiser with standard settings, a batch size of 32 and use at most 100 epochs (with early stopping on a validation set). Many of these settings are not the same as original"
P19-1281,P18-1128,0,0.0255713,"on sampling (TTTS) (Russo, 2016). We will see that this algorithm can efficiently allocate computational resources to quickly find optimal models. Furthermore, this approach provides full uncertainty estimation over the final choice of model, providing the confidence guarantees required for FC model selection. Our implementation makes the assumption that the evaluations of each model roughly follow a Gaussian distribution, with different means and variances. Although such assumptions are common in the model evaluation literature (Reimers and Gurevych, 2018) and for statistical testing in NLP (Dror et al., 2018), they could be problematic for the bounded metrics common in NLP. Therefore we also experimented with modelling the logit transformation of our evaluations, mapping our evaluation metric to the whole real line. However, for our examples of Section 5 we found that this mapping provided a negligible improvement in reliability and so was not worth including in our experimental results. This may not be the case for other tasks or less well-behaved evaluation metrics and so we include this functionality in the FIESTA package. 3 We enforce a minimum of three evaluations to ensure that the t distrib"
P19-1281,E17-1039,0,0.0309732,"ovided motivation for efficient algorithms in a wide range of domains, including clinical trials (Villar et al., 2015) and recommendation systems (Li et al., 2010). Although we believe that we are the first to use bandits to reduce the cost and improve the reliability of model selection, we are not the first to use them in NLP. Recent work in machine translation makes use of another major part of the MAB literature, seeking to optimise the long-term performance of translation algorithms (Nguyen et al., 2017; Sokolov et al., 2016; Lawrence et al., 2017). Within NLP, our work is most similar to Haffari et al. (2017), who use bandits to minimise the number of data queries required to calculate the F-scores of models. However, this work does not consider the stochasticity of the resulting estimates or easily extend to other evaluation metrics. The main contribution of this paper is the application of three intuitive algorithms to model selection in NLP, alongside a user-friendly Python implementation: FIESTA (Fast IdEntification of State-of-The-Art)1 . We can automatically identify an optimal model from large collections of candidate models to a user-chosen confidence level in a small number of model evalu"
P19-1281,N16-1030,0,0.0441092,"to fit multiple models in parallel. We demonstrate the effectiveness of our procedures over current model selection approaches when identifying an optimal target-dependent sentiment analysis model from a set of eight competing candidate models (Section 5). 2921 1 https://github.com/apmoore1/fiesta 2 Motivating example We now provide evidence for the need to vary both data splits and random seeds for reliable model selection. We extend the motivating example used in the work of Reimers and Gurevych (2017), comparing two LSTM-based Named Entity Recognition (NER) models by Ma and Hovy (2016) and Lample et al. (2016), differing only in character representation (via a CNN and a LSTM respectively). We base model training on Ma and Hovy (2016), however, following the settings of Yang et al. (2018) we use a batch size of 64, a weight decay of 10e−9 and removed momentum. We ran each of the NER models five times with a different random seed on 150 different train, validation, and test splits2 . Reimers and Gurevych (2017) showed the effect of model instability between these two models, where changing the model’s random seeds can lead to drawing different conclusions about which model performed best. We extend t"
P19-1281,D17-1272,0,0.0194037,"t a first glance, this problem is deceptively complex and has provided motivation for efficient algorithms in a wide range of domains, including clinical trials (Villar et al., 2015) and recommendation systems (Li et al., 2010). Although we believe that we are the first to use bandits to reduce the cost and improve the reliability of model selection, we are not the first to use them in NLP. Recent work in machine translation makes use of another major part of the MAB literature, seeking to optimise the long-term performance of translation algorithms (Nguyen et al., 2017; Sokolov et al., 2016; Lawrence et al., 2017). Within NLP, our work is most similar to Haffari et al. (2017), who use bandits to minimise the number of data queries required to calculate the F-scores of models. However, this work does not consider the stochasticity of the resulting estimates or easily extend to other evaluation metrics. The main contribution of this paper is the application of three intuitive algorithms to model selection in NLP, alongside a user-friendly Python implementation: FIESTA (Fast IdEntification of State-of-The-Art)1 . We can automatically identify an optimal model from large collections of candidate models to"
P19-1281,P16-1101,0,0.0177095,"computational capacity to fit multiple models in parallel. We demonstrate the effectiveness of our procedures over current model selection approaches when identifying an optimal target-dependent sentiment analysis model from a set of eight competing candidate models (Section 5). 2921 1 https://github.com/apmoore1/fiesta 2 Motivating example We now provide evidence for the need to vary both data splits and random seeds for reliable model selection. We extend the motivating example used in the work of Reimers and Gurevych (2017), comparing two LSTM-based Named Entity Recognition (NER) models by Ma and Hovy (2016) and Lample et al. (2016), differing only in character representation (via a CNN and a LSTM respectively). We base model training on Ma and Hovy (2016), however, following the settings of Yang et al. (2018) we use a batch size of 64, a weight decay of 10e−9 and removed momentum. We ran each of the NER models five times with a different random seed on 150 different train, validation, and test splits2 . Reimers and Gurevych (2017) showed the effect of model instability between these two models, where changing the model’s random seeds can lead to drawing different conclusions about which model pe"
P19-1281,C18-1252,1,0.872474,"Missing"
P19-1281,D17-1153,0,0.0418223,"Missing"
P19-1281,D14-1162,0,0.0835644,"nd right LSTM), and a non-targetaware LSTM method used as the baseline in Tang et al. (2016). These methods represent state-of-the-art within TDSA, with only small differences in performance between TDLSTM, IAN, and ATAE (see figure 3). All the models are re-implemented in PyTorch (Paszke et al., 2017) using AllenNLP (Gardner et al., 2018). To ensure the only difference between the models is their network architecture the models use the same optimiser settings and the same regularisation. All words are lower cased and we use the same Glove common crawl 840B token 300 dimension word embedding (Pennington et al., 2014). We use variational (Gal and Ghahramani, 2016) and regular (Hinton et al., 2012) dropout for regularisation and an ADAM (Kingma and Ba, 2014) optimiser with standard settings, a batch size of 32 and use at most 100 epochs (with early stopping on a validation set). Many of these settings are not the same as originally implemented, however, having the same training setup is required for fair comparison (this explains the differences between our results and the original implementations). To increase the difficulty of our model selection problem, we additionally create four extra models by reduci"
P19-1281,S14-2004,0,0.0318375,"is required for fair comparison (this explains the differences between our results and the original implementations). To increase the difficulty of our model selection problem, we additionally create four extra models by reducing the dimensions of the Glove vectors to 50 and removing dropout. Although these models are clearly not state-of-the-art, they increase the size of our candidate model set and so provide a more complicated model selection problem (an intuition discussed in Appendix A). All of the TDSA experiments are conducted on the well-studied SemEval 2014 task 4 Restaurant dataset (Pontiki et al., 2014) and we force trainval-test splits to follow the same ratios as this dataset’s official train-test split. Each individual model evaluation is then made on a randomly generated train-test split and random seed to access both sources of evaluation variability. Figure 3: F1 scores for our candidate TDSA models. After 500 evaluations of each model on different data splits and model seeds we see that the TDLSTM is the state-of-the-art model. 5.1 Fixed Budget Model Selection We use the TDSA model selection problem to test fixed budget model selection. To thoroughly test our algorithm, we consider an"
P19-1281,D17-1035,0,0.0793439,"training. Common sources of model instability in modern NLP include weight initialisation, data sub-sampling for stochastic gradient calculation, negative sampling used to train word embeddings (Mikolov et al., 2013) and feature sub-sampling for ensemble methods. In particular, the often state-of-the-art LSTMs (and its many variants) have been 2920 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2920–2930 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics shown to exhibit high sensitivity to random seeds (Reimers and Gurevych, 2017). For reliable model selection, it is crucial to take into account both sources of variability when estimating model performance. Observing a higher score for one model could be a consequence of a particularly non-representative train-test split and/or random seed used to evaluate the model rather than a genuine model improvement. This subtlety is ignored by large scale NLP competitions such as SemEval with evaluations based on a pre-determined train-test split. Although more precise model evaluations can be obtained with higher computation, calculating overly precise model evaluations is a hu"
P19-1281,P16-1152,0,0.013374,"ugh appearing simple at a first glance, this problem is deceptively complex and has provided motivation for efficient algorithms in a wide range of domains, including clinical trials (Villar et al., 2015) and recommendation systems (Li et al., 2010). Although we believe that we are the first to use bandits to reduce the cost and improve the reliability of model selection, we are not the first to use them in NLP. Recent work in machine translation makes use of another major part of the MAB literature, seeking to optimise the long-term performance of translation algorithms (Nguyen et al., 2017; Sokolov et al., 2016; Lawrence et al., 2017). Within NLP, our work is most similar to Haffari et al. (2017), who use bandits to minimise the number of data queries required to calculate the F-scores of models. However, this work does not consider the stochasticity of the resulting estimates or easily extend to other evaluation metrics. The main contribution of this paper is the application of three intuitive algorithms to model selection in NLP, alongside a user-friendly Python implementation: FIESTA (Fast IdEntification of State-of-The-Art)1 . We can automatically identify an optimal model from large collections"
P19-1281,C16-1311,0,0.015312,"dels to be evaluated before choosing the next batch, each worker can draw a new model to evaluate according to the updated π. This flexibility provides an additional efficiency gain for problems where the different models have a wide range of run times. 2925 5 Experiments We now test our three algorithms on a challenging model selection task typical of NLP, selecting between eight Target Dependent Sentiment Analysis (TDSA) models based on their macro F1 score. We consider two variants of four reimplementations of well-known TDSA models: ATAE (Wang et al., 2016), IAN (Ma et al., 2017), TDLSTM (Tang et al., 2016) (without target words in the left and right LSTM), and a non-targetaware LSTM method used as the baseline in Tang et al. (2016). These methods represent state-of-the-art within TDSA, with only small differences in performance between TDLSTM, IAN, and ATAE (see figure 3). All the models are re-implemented in PyTorch (Paszke et al., 2017) using AllenNLP (Gardner et al., 2018). To ensure the only difference between the models is their network architecture the models use the same optimiser settings and the same regularisation. All words are lower cased and we use the same Glove common crawl 840B"
P19-1281,D16-1058,0,0.0131783,"ere rather than waiting for the whole batch of B models to be evaluated before choosing the next batch, each worker can draw a new model to evaluate according to the updated π. This flexibility provides an additional efficiency gain for problems where the different models have a wide range of run times. 2925 5 Experiments We now test our three algorithms on a challenging model selection task typical of NLP, selecting between eight Target Dependent Sentiment Analysis (TDSA) models based on their macro F1 score. We consider two variants of four reimplementations of well-known TDSA models: ATAE (Wang et al., 2016), IAN (Ma et al., 2017), TDLSTM (Tang et al., 2016) (without target words in the left and right LSTM), and a non-targetaware LSTM method used as the baseline in Tang et al. (2016). These methods represent state-of-the-art within TDSA, with only small differences in performance between TDLSTM, IAN, and ATAE (see figure 3). All the models are re-implemented in PyTorch (Paszke et al., 2017) using AllenNLP (Gardner et al., 2018). To ensure the only difference between the models is their network architecture the models use the same optimiser settings and the same regularisation. All words are lower"
P19-1281,C18-1327,0,0.0130854,"nalysis model from a set of eight competing candidate models (Section 5). 2921 1 https://github.com/apmoore1/fiesta 2 Motivating example We now provide evidence for the need to vary both data splits and random seeds for reliable model selection. We extend the motivating example used in the work of Reimers and Gurevych (2017), comparing two LSTM-based Named Entity Recognition (NER) models by Ma and Hovy (2016) and Lample et al. (2016), differing only in character representation (via a CNN and a LSTM respectively). We base model training on Ma and Hovy (2016), however, following the settings of Yang et al. (2018) we use a batch size of 64, a weight decay of 10e−9 and removed momentum. We ran each of the NER models five times with a different random seed on 150 different train, validation, and test splits2 . Reimers and Gurevych (2017) showed the effect of model instability between these two models, where changing the model’s random seeds can lead to drawing different conclusions about which model performed best. We extend this argument by showing that different conclusions can also be drawn if we instead vary the train-test split used for the model evaluation (Figure 1). We see that while data splits"
piao-etal-2004-evaluating,W03-1807,1,\N,Missing
S17-2095,S16-1044,0,0.0459273,"Missing"
S17-2095,S17-2089,0,0.104046,"Missing"
S17-2095,N16-1041,0,0.0667773,"Missing"
S17-2095,S14-2004,0,0.314677,"16). Peng and Jiang (2016) used a multi layer neural network to predict the stock market and found that incorporating textual features from financial news can improve the accuracy of prediction. Kazemian et al. (2016) showed the importance of tuning sentiment analysis to the task of stock market prediction. However, much of the previous work was based on numerical financial stock market data rather than on aspect level financial textual data. In aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016 task 5 (Pontiki et al., 2014). The winning system (Brun et al., 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al., 2016) used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Deep learning methods have also been applied to aspect polarity prediction. Ruder et al. (2016) created a hierarchical BLSTM with a sentence level BLSTM inputting into a review level BLSTM thus allowing them to take into account inter- and intra-sentence context. They used only word embeddings making their system less dependent on extensive feature engineerin"
S17-2095,D16-1103,0,0.0124818,"us work was based on numerical financial stock market data rather than on aspect level financial textual data. In aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016 task 5 (Pontiki et al., 2014). The winning system (Brun et al., 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al., 2016) used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Deep learning methods have also been applied to aspect polarity prediction. Ruder et al. (2016) created a hierarchical BLSTM with a sentence level BLSTM inputting into a review level BLSTM thus allowing them to take into account inter- and intra-sentence context. They used only word embeddings making their system less dependent on extensive feature engineering or manual feature creation. This system outperformed all others on certain languages Introduction The objective of Task 5 Track 2 of SemEval (2017) was to predict the sentiment of news headlines with respect to companies mentioned within the headlines. This task can be seen as a financespecific aspect-based sentiment task (Nasukaw"
S17-2095,P16-1197,0,0.0617075,"Missing"
S17-2095,P02-1053,0,0.0111442,"dding model3 on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva4 . The articles stem from a range of sources including the Financial Times and relate to companies from the United States only. We trained the model on domain specific data as it has been shown many times that the financial domain can contain very different language. 4 Tokenisation 3. Negative - The same as the positive group however the word used was ‘poor’ instead of ‘excellent’. In the positive and negative groups, we chose the words ‘excellent’ and ‘poor’ following Turney (2002) to group the terms together under nondomain specific sentiment words. SVR 4.1.5 The system was created using ScitKit learn (Pedregosa et al., 2011) linear Support Vector Regression model (Drucker et al., 1997). We experTarget aspect In order to incorporated the company as an aspect, we employed a boolean vector to represent the sentiment of the sentence. This was done in order to see if the system could better differentiate the sentiment when the sentence was the same but the company was different. 3 For reproducibility, the model can be downloaded, however the articles cannot be due to copyr"
S17-2095,D16-1058,0,0.0259201,"d BLSTMs. We found that BLSTMs outperform an SVR without having any knowledge of the company that the sentiment is with respect to. For replicability purposes, with this paper 1 https://github.com/apmoore1/semeval https://github.com/apmoore1/semeval/ tree/master/models/word2vec_models 2 581 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 581–585, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics on the SemEval-2016 task 5 dataset (Pontiki et al., 2014) and on other languages performed close to the best systems. Wang et al. (2016) also created an LSTM based model using word embeddings but instead of a hierarchical model it was a one layered LSTM with attention which puts more emphasis on learning the sentiment of words specific to a given aspect. imented with the following different features and parameter settings: 3 4.1.2 4.1.1 For comparison purposes, we tested whether or not a simple whitespace tokeniser can perform just as well as a full tokeniser, and in this case we used Unitok5 . Data 4.1.3 SVR parameters We tested different penalty parameters C and different epsilon parameters of the SVR. 4.1.4 Word Replacement"
S17-2095,S16-1174,0,0.0281221,"ncial news can improve the accuracy of prediction. Kazemian et al. (2016) showed the importance of tuning sentiment analysis to the task of stock market prediction. However, much of the previous work was based on numerical financial stock market data rather than on aspect level financial textual data. In aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016 task 5 (Pontiki et al., 2014). The winning system (Brun et al., 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al., 2016) used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Deep learning methods have also been applied to aspect polarity prediction. Ruder et al. (2016) created a hierarchical BLSTM with a sentence level BLSTM inputting into a review level BLSTM thus allowing them to take into account inter- and intra-sentence context. They used only word embeddings making their system less dependent on extensive feature engineering or manual feature creation. This system outperformed all others on certain languages Introduction The objective of Task 5 Track 2 of SemEval"
W00-0901,J93-1003,0,0.0323045,"Missing"
W00-0901,W97-0122,0,0.0258394,"Missing"
W00-0901,W98-1506,0,0.0256948,"y list is then sorted by the resulting LL values. This gives the effect of placing the largest LL value at the top of the list representing the word which has the most significant relative frequency difference between the two corpora. In this way, we can see the words most indicative (or characteristic) of one corpus, as compared to the other corpus, at the top of the list. The words which appear with roughly similar relative frequencies in the two corpora appear lower down the list. Note that we do not use the hypothesis-test by comparing the LL values to a chi-squared distribution table. As Kilgarriff & Rose (1998) note, even Pearson~ X 2 is suitable without the hypothesis-testing link: Given the non-random nature of words in a text, we are always likely to find frequencies of words which differ across any two texts, and the higher the frequencies, the more information the statistical test has to work with. Hence, it is at this point that the researcher must intervene and qualitatively examine examples of the significant words highlighted by this technique. We are not proposing a completely automated approach. 3 Applications This method has already been applied to study social differentiation in the use"
W03-1807,clough-etal-2002-building,0,0.0491979,"approaches have been suggested and tested to address this problem. However, efficient extraction of MWEs still remains an unsolved issue, to the extent that Sag et al. (2001b) call it “a pain in the neck of NLP”. In this paper, we present our work in which we approach the issue of MWE extraction by using a semantic field annotator. Specifically, we use the UCREL Semantic Analysis System (henceforth USAS), developed at Lancaster University to identify multiword units that depict single semantic concepts, i.e. multiword expressions. We have drawn from the Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) a collection of British newspaper reports on court stories to evaluate our approach. Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies. In the following sections, we describe this approach to MWE extraction and its evaluation. 2 Related Works Generally speaking, approaches to MWE extraction proposed so far can be divided into three categories: a) statistical approaches based on frequency and co-occurrence affinity, b) knowledge–based or symbolic approaches using parsers, lexicons and language filters, and c) hybrid approaches combining differ"
W03-1807,A94-1006,0,0.135214,"ewspaper reports on court stories to evaluate our approach. Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies. In the following sections, we describe this approach to MWE extraction and its evaluation. 2 Related Works Generally speaking, approaches to MWE extraction proposed so far can be divided into three categories: a) statistical approaches based on frequency and co-occurrence affinity, b) knowledge–based or symbolic approaches using parsers, lexicons and language filters, and c) hybrid approaches combining different methods (Smadja 1993; Dagan and Church 1994; Daille 1995; McEnery et al. 1997; Wu 1997; Wermter et al. 1997; Michiels and Dufour 1998; Merkel and Andersson 2000; Piao and McEnery 2001; Sag et al. 2001a, 2001b; Biber et al. 2003). In practice, most statistical approaches use linguistic filters to collect candidate MWEs. Such approaches include Dagan and Church’s (1994) Termight Tool. In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units. In his Xtract system, Smadja (1993) first extracted significant pairs of words that cons"
W03-1807,H92-1045,0,0.0476719,"experience and intuition. 3. Overlapping MWU resolution. Normally, semantic multi-word units take priority over single word tagging, but in some cases a set of templates will produce overlapping candidate taggings for the same set of words. A set of heuristics is applied to enable the most likely template to be treated as the preferred one for tag assignment. 4. Domain of discourse. Knowledge of the current domain or topic of discourse is used to alter rank ordering of semantic tags in the lexicon and template list for a particular domain. 5. Text-based disambiguation. It has been claimed (by Gale et al, 1992) on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text. 6. Contextual rules. The template mechanism is also used in identifying regular contexts in which a word is constrained to occur in a particular sense. 7. Local probabilistic disambiguation. It is generally supposed that the correct semantic tag for a given word is substantially determined by the local surrounding context. 1. Prefer longer templates over shorter templates 2. For templates of the same length, prefer shorter span matches over longer span matches (a longer span indicates"
W03-1807,J93-1007,0,0.874895,"of British newspaper reports on court stories to evaluate our approach. Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies. In the following sections, we describe this approach to MWE extraction and its evaluation. 2 Related Works Generally speaking, approaches to MWE extraction proposed so far can be divided into three categories: a) statistical approaches based on frequency and co-occurrence affinity, b) knowledge–based or symbolic approaches using parsers, lexicons and language filters, and c) hybrid approaches combining different methods (Smadja 1993; Dagan and Church 1994; Daille 1995; McEnery et al. 1997; Wu 1997; Wermter et al. 1997; Michiels and Dufour 1998; Merkel and Andersson 2000; Piao and McEnery 2001; Sag et al. 2001a, 2001b; Biber et al. 2003). In practice, most statistical approaches use linguistic filters to collect candidate MWEs. Such approaches include Dagan and Church’s (1994) Termight Tool. In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units. In his Xtract system, Smadja (1993) first extracted significant p"
W03-1807,P98-2226,0,0.049811,"he words in most corpora have low frequencies, occurring only once or twice. This means that a major part of true multiword expressions are left out by statistical approaches. Lexical resources and parsers are used to obtain better coverage of the lexicon in MWE extraction. For example, Wu (1997) used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions. In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language. Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system. Sag et al. (2001b) introduced Head-driven Phrase Structure Grammar for analyzing MWEs. Like pure statistical approaches, purely knowledgebased symbolic approaches also face problems. They are language dependent and not flexible enough to cope with complex structures of MWEs. As Sag et al. (2001b) suggest, it is important to find the right balance between symbolic and statistical approaches. In this paper, we propose a new approach to MWEs extraction using semantic field informatio"
W03-1807,J97-3002,0,0.0495279,"h. Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies. In the following sections, we describe this approach to MWE extraction and its evaluation. 2 Related Works Generally speaking, approaches to MWE extraction proposed so far can be divided into three categories: a) statistical approaches based on frequency and co-occurrence affinity, b) knowledge–based or symbolic approaches using parsers, lexicons and language filters, and c) hybrid approaches combining different methods (Smadja 1993; Dagan and Church 1994; Daille 1995; McEnery et al. 1997; Wu 1997; Wermter et al. 1997; Michiels and Dufour 1998; Merkel and Andersson 2000; Piao and McEnery 2001; Sag et al. 2001a, 2001b; Biber et al. 2003). In practice, most statistical approaches use linguistic filters to collect candidate MWEs. Such approaches include Dagan and Church’s (1994) Termight Tool. In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units. In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic"
W03-1807,C98-2221,0,\N,Missing
W06-1202,piao-etal-2004-evaluating,1,0.895673,"Missing"
W06-1202,W03-1809,0,0.0133817,"e our semantic lexical-based approach not as a substitute for the statistical approaches. Related Work In recent years, various approaches have been proposed to the analysis of MWE compositionality. Many of the suggested approaches employ statistical algorithms. One of the earliest studies in this area was reported by Lin (1999) who assumes that “noncompositional phrases have a significantly different mutual information value than the phrases that are similar to their literal meanings” and proposed to identify non-compositional MWEs in a corpus based on distributional characteristics of MWEs. Bannard et al. (2003) tested techniques using statistical models to infer the meaning of verb-particle constructions (VPCs), focus2 In this lexicon, many MWEs are encoded as templates, such as driv*_* {Np/P*/J*/R*} mad_JJ, which represent variational forms of a single MWE, For further details, see Rayson et al., 2004. 3 Rather we propose it as a potential complement to them. In the following sections, we describe our experiment and explore this approach to the issue of automatic estimation of MWE compositionality. 3 Given a MWE M and its constituent words wi (i = 1, .., n), the compositionality D can be measured b"
W06-1202,E06-2014,1,0.820056,"s, Canary Islands. reveal, the semantic information provided by the lexicon alone may not be rich enough for a very fine-grained distinction of MWE compositionality. In order to obtain better results, this algorithm needs to be combined with statistical techniques. A limitation of our approach is languagedependency. In order to port our algorithm to languages other than English, one needs to build similar semantic lexicon in those languages. However, similar semantic lexical resources are already under construction for some other languages, including Finnish and Russian (Löfberg et al., 2005; Sharoff et al., 2006), which will allow us to port our algorithm to those languages. 7 Ann Copestake, Fabre Lambeau, Aline Villavicencio, Francis Bond, Timothy Baldwin, Ivan A. Sag, and Dan Flickinger. 2002. Multiword expressions: Linguistic precision and reusability. In Proc. of the Third International Conference on Language Resources and Evaluation (LREC 2002), pages 1941– 1947, Las Palmas, Canary Islands. Matthias Gamer. 2005. The irr Package: Various Coefficients of Interrater Reliability and Agreement. Version 0.61 of 11 October 2005. Available from: cran.r-project.org/src/contrib/Descriptions/irr.html Dekang"
W06-1202,copestake-etal-2002-multiword,0,\N,Missing
W06-1202,H05-1113,0,\N,Missing
W06-1202,W03-1810,0,\N,Missing
W06-1202,calzolari-etal-2002-towards,0,\N,Missing
W06-1202,W03-1812,0,\N,Missing
W06-1202,P99-1041,0,\N,Missing
W06-1705,W03-0806,0,0.0175518,"Rundell). They have also served as the starting point for high-accuracy Word Sense Disambiguation. More recently, the Sketch Engine was used to develop the new edition of the Oxford Thesaurus of English (2004, edited by Maurice Waite). Parallelising or distributing processing has been suggested before. Clark and Curran’s (2004) work is in parallelising an implementation of log-linear parsing on the Wall Street Journal Corpus, whereas we focus on part-of-speech tagging of a far larger and more varied web corpus, a technique more widely considered a prerequisite for corpus linguistics research. Curran (2003) 9 http://pie.usna.edu/ 29 suggested distributed processing in terms of web services but only to “allow components developed by different researchers in different locations to be composed to build larger systems” and not for parallel processing. Most significantly, previous investigations have not examined three essential questions: how to apply distributed techniques to vast quantities of corpus data derived from the web, how to ensure that web-derived corpora are representative, and how to provide verifiability and replicability. These core foci of our work represent crucial innovations lack"
W06-1705,baroni-bernardini-2004-bootcat,0,0.0396504,"vided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist’s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable (Veronis, 2005). Tools based on static corpora do not suffer from th"
W06-1705,W02-1030,0,0.0230867,"rds) to annotate • Crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by Fletcher (2004b) and shingling techniques described by Chakrabarti (2002). The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API). We have designed this environment so that specific application functionality Research hypothesis and aims Our research hypothesis is that distributed computational techniques can alleviate the annotation bottleneck for processing corpus data from the web. This leads us to a number of research questions: • How can corpus data from the web be divided into u"
W06-1705,J03-3001,1,0.845748,"ural language processing where models of sparse data are built. The motivation for increasingly large data sets remains the same. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005; Granger and Rayson, 1998). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing po"
W06-1705,W04-0858,0,\N,Missing
W06-1705,P04-1014,0,\N,Missing
W06-2403,W03-1812,0,0.015423,"Can practise a ball game this afternoon? I hope can not. You cannot do like that, and let us make it Dutch. Perhaps no way out(ly) let you sit with table, are you situated between not mind to separate to sit? 2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing (NLP) community, including Smadja, 1993; Dagan and Church, 1994; Daille, 1995; 1995; McEnery et al., 1997; Wu, 1997; Michiels and Dufour, 1998; Maynard and Ananiadou, 2000; Merkel and Andersson, 2000; Piao and McEnery, 2001; Sag et al., 2001; Tanaka and Baldwin, 2003; Dias, 2003; Baldwin et al., 2003; Nivre and Nilsson, 2004 Pereira et al,. 2004; Piao et al., 2005. Study in this area covers a wide range of sub-issues, including MWE identification and extraction from monolingual and multilingual corpora, classification of MWEs according to a variety of viewpoints such as types, compositionality and alignment of MWEs across different languages. However studies in this area on Chinese language are limited. A number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents. Despite this research, however, MWE processing st"
W06-2403,A94-1006,0,0.166236,"你们坐 同桌 ， not have the 你们介不介意 means to let you sit shares a 分开坐呢？ table, did you mind sits separately? the 来点冰镇的 奶 Selects milk coffee 咖啡。 which ices. 好的，我要啤 Good, I want 酒， 再来点 咖 the beer, again comes to select 啡。 the coffee. English (HYT) Can practise a ball game this afternoon? I hope can not. You cannot do like that, and let us make it Dutch. Perhaps no way out(ly) let you sit with table, are you situated between not mind to separate to sit? 2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing (NLP) community, including Smadja, 1993; Dagan and Church, 1994; Daille, 1995; 1995; McEnery et al., 1997; Wu, 1997; Michiels and Dufour, 1998; Maynard and Ananiadou, 2000; Merkel and Andersson, 2000; Piao and McEnery, 2001; Sag et al., 2001; Tanaka and Baldwin, 2003; Dias, 2003; Baldwin et al., 2003; Nivre and Nilsson, 2004 Pereira et al,. 2004; Piao et al., 2005. Study in this area covers a wide range of sub-issues, including MWE identification and extraction from monolingual and multilingual corpora, classification of MWEs according to a variety of viewpoints such as types, compositionality and alignment of MWEs across different languages. However stud"
W06-2403,W03-1806,0,0.0538485,"nglish (HYT) Can practise a ball game this afternoon? I hope can not. You cannot do like that, and let us make it Dutch. Perhaps no way out(ly) let you sit with table, are you situated between not mind to separate to sit? 2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing (NLP) community, including Smadja, 1993; Dagan and Church, 1994; Daille, 1995; 1995; McEnery et al., 1997; Wu, 1997; Michiels and Dufour, 1998; Maynard and Ananiadou, 2000; Merkel and Andersson, 2000; Piao and McEnery, 2001; Sag et al., 2001; Tanaka and Baldwin, 2003; Dias, 2003; Baldwin et al., 2003; Nivre and Nilsson, 2004 Pereira et al,. 2004; Piao et al., 2005. Study in this area covers a wide range of sub-issues, including MWE identification and extraction from monolingual and multilingual corpora, classification of MWEs according to a variety of viewpoints such as types, compositionality and alignment of MWEs across different languages. However studies in this area on Chinese language are limited. A number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents. Despite this research, howe"
W06-2403,J93-1003,0,0.0793759,"Missing"
W06-2403,J93-1007,0,0.117152,"让 Perhaps does 你们坐 同桌 ， not have the 你们介不介意 means to let you sit shares a 分开坐呢？ table, did you mind sits separately? the 来点冰镇的 奶 Selects milk coffee 咖啡。 which ices. 好的，我要啤 Good, I want 酒， 再来点 咖 the beer, again comes to select 啡。 the coffee. English (HYT) Can practise a ball game this afternoon? I hope can not. You cannot do like that, and let us make it Dutch. Perhaps no way out(ly) let you sit with table, are you situated between not mind to separate to sit? 2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing (NLP) community, including Smadja, 1993; Dagan and Church, 1994; Daille, 1995; 1995; McEnery et al., 1997; Wu, 1997; Michiels and Dufour, 1998; Maynard and Ananiadou, 2000; Merkel and Andersson, 2000; Piao and McEnery, 2001; Sag et al., 2001; Tanaka and Baldwin, 2003; Dias, 2003; Baldwin et al., 2003; Nivre and Nilsson, 2004 Pereira et al,. 2004; Piao et al., 2005. Study in this area covers a wide range of sub-issues, including MWE identification and extraction from monolingual and multilingual corpora, classification of MWEs according to a variety of viewpoints such as types, compositionality and alignment of MWEs across different"
W06-2403,W03-1803,0,0.0283657,"to select 啡。 the coffee. English (HYT) Can practise a ball game this afternoon? I hope can not. You cannot do like that, and let us make it Dutch. Perhaps no way out(ly) let you sit with table, are you situated between not mind to separate to sit? 2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing (NLP) community, including Smadja, 1993; Dagan and Church, 1994; Daille, 1995; 1995; McEnery et al., 1997; Wu, 1997; Michiels and Dufour, 1998; Maynard and Ananiadou, 2000; Merkel and Andersson, 2000; Piao and McEnery, 2001; Sag et al., 2001; Tanaka and Baldwin, 2003; Dias, 2003; Baldwin et al., 2003; Nivre and Nilsson, 2004 Pereira et al,. 2004; Piao et al., 2005. Study in this area covers a wide range of sub-issues, including MWE identification and extraction from monolingual and multilingual corpora, classification of MWEs according to a variety of viewpoints such as types, compositionality and alignment of MWEs across different languages. However studies in this area on Chinese language are limited. A number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents. Despite this re"
W06-2403,J97-3002,0,0.0399511,"table, did you mind sits separately? the 来点冰镇的 奶 Selects milk coffee 咖啡。 which ices. 好的，我要啤 Good, I want 酒， 再来点 咖 the beer, again comes to select 啡。 the coffee. English (HYT) Can practise a ball game this afternoon? I hope can not. You cannot do like that, and let us make it Dutch. Perhaps no way out(ly) let you sit with table, are you situated between not mind to separate to sit? 2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing (NLP) community, including Smadja, 1993; Dagan and Church, 1994; Daille, 1995; 1995; McEnery et al., 1997; Wu, 1997; Michiels and Dufour, 1998; Maynard and Ananiadou, 2000; Merkel and Andersson, 2000; Piao and McEnery, 2001; Sag et al., 2001; Tanaka and Baldwin, 2003; Dias, 2003; Baldwin et al., 2003; Nivre and Nilsson, 2004 Pereira et al,. 2004; Piao et al., 2005. Study in this area covers a wide range of sub-issues, including MWE identification and extraction from monolingual and multilingual corpora, classification of MWEs according to a variety of viewpoints such as types, compositionality and alignment of MWEs across different languages. However studies in this area on Chinese language are limited. A"
W06-2403,W03-1807,1,0.917229,"Missing"
W06-2403,C94-2178,0,\N,Missing
W13-3109,H01-1065,0,0.0161979,"eighted features of these candidates to construct summaries. Hennig (2009) introduced a query-based latent Semantic Analysis (LSA) automatic text summariser. It finds statistical semantic relationships between the extracted sentences rather than word by word matching relations (Hofmann, 1999). The summariser selects sentences with the highest likelihood score. In our work we used log-likelihood to select sentences with the maximum sum of log likelihood scores, unlike the traditional method of measuring cosine similarity overlap between articles or sentences to indicate importance (Luhn, 1958; Barzilay et al., 2001; Radev et al., 2004). The main advantage of our approach is that the automatic summariser does not need to compare sentences in a document with an initial one (e.g. first sentence or a query). Our approach works by calculating the keyness (or log-likelihood) score for each token (word) in a sentence, then picks, to a limit of 250 words, the sentences with the highest sum of the tokens’ log-likelihood scores. To the best of our knowledge the use of corpusbased frequency list to calculate the log-likelihood 3 Dataset and Evaluation Metrics 3.1 Test Collection The test collection for the MultiLi"
W13-3109,R09-1028,0,0.0319957,"mmarisation system. EM is an iterative method for finding Maximum Likelihood (ML) or Maximum A Posteriori (MAP) estimates of parameters in statistical models. In their summariser, EM was used in the sentences compression process to shorten many sentences into one by compressing a syntactic parse tree of a sentence in order to produce a shorter but maximally grammatical version. Similarly, Madnani et al. (2007) performed multi-document summarisation by generating compressed versions of source sentences as summary candidates and used weighted features of these candidates to construct summaries. Hennig (2009) introduced a query-based latent Semantic Analysis (LSA) automatic text summariser. It finds statistical semantic relationships between the extracted sentences rather than word by word matching relations (Hofmann, 1999). The summariser selects sentences with the highest likelihood score. In our work we used log-likelihood to select sentences with the maximum sum of log likelihood scores, unlike the traditional method of measuring cosine similarity overlap between articles or sentences to indicate importance (Luhn, 1958; Barzilay et al., 2001; Radev et al., 2004). The main advantage of our appr"
W13-3109,W04-1013,0,0.0359121,"sed. There are various models for system evaluation that may help in solving this problem. This include automatic evaluations (e.g. ROUGE and AutoSummENG), and humanperformed evaluations. For the MultiLing 2013 task, the summaries generated by the participants 2 http://multiling.iit.demokritos.gr/file/all http://www.wikinews.org/ 4 http://www.wikipedia.org/ 3 66 were evaluated automatically based on humangenerated model summaries provided by fluent speakers of each corresponding language (native speakers in the general case). The models used were, ROUGE variations (ROUGE1, ROUGE2, ROUGE-SU4) (Lin, 2004), the MeMoG variation (Giannakopoulos and Karkaletsis, 2011) of AutoSummENG (Giannakopoulos et al., 2008) and NPowER (Giannakopoulos and Karkaletsis, 2013). ROUGE was not used to evaluate the single-document summaries. The summaries were also evaluated manually by human participants. For the manual evaluation the human evaluators were provided with the following guidelines: Each summary is to be assigned an integer grade from 1 to 5, related to the overall responsiveness of the summary. We consider a text to be worth a 5, if it appears to cover all the important aspects of the corresponding do"
W13-3109,P11-2039,0,0.0258021,"aphs the most important sentence was the first one. Multi-document summarisation produces a single summary of a set of documents. The documents are assumed to be about the same genre and topic. The analysis in this area is performed typically at either the sentence or document level. 2.3 Statistical Summarisation The use of statistical approaches (e.g. loglikelihood) in text summarisation is a common 1 65 http://duc.nist.gov/duc2003/tasks.html score for text summarisation has not been reported for the Arabic language. technique, especially when building a language independent text summariser. Morita et al. (2011) introduced what they called “query-snowball”, a method for query-oriented extractive multi-document summarisation. They worked on closing the gap between the query and the relevant sentences. They formulated the summarisation problem based on word pairs as a maximum cover problem with Knapsack Constraints (MCKP), which is an optimisation problem that maximises the total score of words covered by a summary within a certain length limit. Knight and Marcu (2000) used the Expectation Maximisation (EM) algorithm to compress sentences for an abstractive text summarisation system. EM is an iterative"
W13-3109,W00-0901,1,0.820806,"op on Multilingual Multi-document Summarization, pages 64–71, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics 2.2 single-document and multi-document summarisers at the MultiLing 2013 summarisation tasks. We used a language independent corpus-based word frequency technique and the log-likelihood statistic to extract sentences with the maximum sum of log likelihood. The output summary is expected to be no more than 250 words. 2 2.1 Corpus-based and Word Frequency in Summarisation Corpus-based techniques are mainly used to compare corpora for linguistic analysis (Rayson and Garside, 2000; Rayson et al., 2004). There are two main types of corpora comparisons, 1) comparing a sample corpus with a larger standard corpus (Scott, 2000). 2) comparing two corpora of equal size (Granger, 1998). In our work we adopted the first approach, where we used a much larger reference corpus. The first word list is the frequency list of all the words in the document (or group of documents) to be summarised which is compared to the word frequency list of a much larger standard corpus. We do that for both Arabic and English texts. Word frequency has been proven as an important feature when determi"
W17-1908,W10-1806,0,0.0158184,"sted for a variety of tasks, such as image annotation (Sorokin and Forsyth, 2008), Wikipedia article quality assessment (Kittur et al., 2008), machine translation (Callison-Burch, 2009), extracting key phrases from documents (Yang et al., 2009), and summarization (El-Haj et al., 2010). Practical issues such as payment and task design play an important part in ensuring the quality of the resulting work. Many designers pay between $0.01 to $0.10 for a task taking a few minutes. Quality control and evaluation are usually achieved through confidence scores and gold-standards (Donmez et al., 2009; Bhardwaj et al., 2010). Past research has Table 1: USAS top level semantic fields In terms of main contributions, our research goes beyond the previous work on crowdsourcing word meanings which requires workers to pick 62 shown (Aker et al., 2012) that the use of radio button design seems to lead to better results compared to the free text design. Particularly important in our case is the language demographics of MTurk (Pavlick et al., 2014), since we need to find enough native speakers in a number of languages. There is a growing body of crowdsourcing work related to semantic annotation. Snow et al. (2008) applied"
W17-1908,D09-1030,0,0.30127,"n particular Mechanical Turk (MTurk), has been successfully applied for a number of different Natural Language Processing (NLP) tasks. Alonso and Mizzaro (2009) adopted MTurk for five types of NLP tasks, resulting in high agreement between expert gold standard labels and non-expert annotations, where a small number of workers can emulate an expert. With the possibility of achieving good results quickly and cheaply, MTurk has been tested for a variety of tasks, such as image annotation (Sorokin and Forsyth, 2008), Wikipedia article quality assessment (Kittur et al., 2008), machine translation (Callison-Burch, 2009), extracting key phrases from documents (Yang et al., 2009), and summarization (El-Haj et al., 2010). Practical issues such as payment and task design play an important part in ensuring the quality of the resulting work. Many designers pay between $0.01 to $0.10 for a task taking a few minutes. Quality control and evaluation are usually achieved through confidence scores and gold-standards (Donmez et al., 2009; Bhardwaj et al., 2010). Past research has Table 1: USAS top level semantic fields In terms of main contributions, our research goes beyond the previous work on crowdsourcing word meanin"
W17-1908,passonneau-etal-2006-inter,0,0.0955398,"Missing"
W17-1908,C12-2053,0,0.0410422,"Missing"
W17-1908,N15-1137,1,0.866502,"Missing"
W17-1908,rumshisky-etal-2012-word,0,0.0383423,"Missing"
W17-1908,D08-1027,0,0.207302,"Missing"
W17-1908,W10-0731,0,\N,Missing
W17-1908,aker-etal-2012-assessing,1,\N,Missing
W19-4332,E17-1088,0,0.0306156,"existing taxonomy of categories, e.g. in USAS (Rayson et al., 2004). Previously, semantic tagging in multiple languages has been shown to greatly benefit from POS tagging in the NLP pipeline, since it can help to filter out inapplicable semantic fields from the set of possible candidates (Piao et al., 2015). Over the past few years, researchers started to port NLP tools and methods into low resource languages using a various approaches, such as porting lexicons from one language to another using bilingual dictionaries and parallel corpora (Piao et al., 2016) and cross-lingual word embeddings (Adams et al., 2017; Sharoff, 2018). Multi-task learning has also been proved useful in transferring the 2.1 CyTag The rule-based POS tagger under consideration in our work, CyTag (Neale et al., 2018), was built based on Constraint Grammar (CG) (Karlsson, 1990), in particular built around the latest version of the software, VISL CG-33 . The CyTag tagset4 contains 145 fine-grained POS tags that can collapse into 13 EAGLES5 -conformant broader categories. CyTag utilises three steps to assign POS tags to tokens: • A list of candidate POS tags is produced for each token. • The list of candidate tags for each token i"
W19-4332,W15-3222,0,0.0185711,"languages in a multilingual setting where one of the languages has only sparse resources available (Junczys-Dowmunt et al., 2018; Lin et al., 2018; Choi et al., 2018), although less successful in named entity recognition settings (Enghoff et al., 2018). In our experiments, we focus on a low-resource mono-lingual setting with a small manually corrected corpus, and combine the Welsh POS and SEM annotation for the first time. Background POS tagging is a well studied NLP task. Much recent work on this task has moved away from English and European languages to other major languages such as Arabic (Aldarmaki and Diab, 2015), Chinese (Sun and Wan, 2016), dialects thereof (Darwish et al., 2018), and text types containing more noise such as historical (Yang and Eisenstein, 2016; Janssen et al., 2017), learner language (Nagata et al., 2018), code switching (Vyas et al., 2014) and social media varieties (Horsmann and Zesch, 2016; van der Goot et al., 2017). More recently, joint and multi-task learning approaches have been applied to link POS tagging and other tasks such as segmentation or tokenisation (AlGahtani and McNaught, 2015; Shao et al., 2017), dependency parsing (Nguyen and Verspoor, 2018) and lemmatisation ("
W19-4332,C90-3030,0,0.0879624,"fields from the set of possible candidates (Piao et al., 2015). Over the past few years, researchers started to port NLP tools and methods into low resource languages using a various approaches, such as porting lexicons from one language to another using bilingual dictionaries and parallel corpora (Piao et al., 2016) and cross-lingual word embeddings (Adams et al., 2017; Sharoff, 2018). Multi-task learning has also been proved useful in transferring the 2.1 CyTag The rule-based POS tagger under consideration in our work, CyTag (Neale et al., 2018), was built based on Constraint Grammar (CG) (Karlsson, 1990), in particular built around the latest version of the software, VISL CG-33 . The CyTag tagset4 contains 145 fine-grained POS tags that can collapse into 13 EAGLES5 -conformant broader categories. CyTag utilises three steps to assign POS tags to tokens: • A list of candidate POS tags is produced for each token. • The list of candidate tags for each token is pruned to as few as possible (ideally one) using CG-formatted rules. • The optimal tag for each token is selected, helped by some small additional processing steps for any cases that were still ambiguous after post-CG. In the second step li"
W19-4332,L18-1015,0,0.0315547,"arse resources available (Junczys-Dowmunt et al., 2018; Lin et al., 2018; Choi et al., 2018), although less successful in named entity recognition settings (Enghoff et al., 2018). In our experiments, we focus on a low-resource mono-lingual setting with a small manually corrected corpus, and combine the Welsh POS and SEM annotation for the first time. Background POS tagging is a well studied NLP task. Much recent work on this task has moved away from English and European languages to other major languages such as Arabic (Aldarmaki and Diab, 2015), Chinese (Sun and Wan, 2016), dialects thereof (Darwish et al., 2018), and text types containing more noise such as historical (Yang and Eisenstein, 2016; Janssen et al., 2017), learner language (Nagata et al., 2018), code switching (Vyas et al., 2014) and social media varieties (Horsmann and Zesch, 2016; van der Goot et al., 2017). More recently, joint and multi-task learning approaches have been applied to link POS tagging and other tasks such as segmentation or tokenisation (AlGahtani and McNaught, 2015; Shao et al., 2017), dependency parsing (Nguyen and Verspoor, 2018) and lemmatisation (Arakelyan et al., 2018). Besides being applied to other NLP applicatio"
W19-4332,C18-1288,0,0.0657895,"Missing"
W19-4332,P18-1074,0,0.0310039,"rtunate to have a fund that supports an on-going inter-disciplinary and multi-institutional project, the National Corpus of Contemporary Welsh (Corpws Cenedlaethol Cymraeg Cyfoes CorCenCC)1 , which has been building a large1 http://www.corcencc.org/ 270 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 270–280 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics source language.2 2 learning across languages in a multilingual setting where one of the languages has only sparse resources available (Junczys-Dowmunt et al., 2018; Lin et al., 2018; Choi et al., 2018), although less successful in named entity recognition settings (Enghoff et al., 2018). In our experiments, we focus on a low-resource mono-lingual setting with a small manually corrected corpus, and combine the Welsh POS and SEM annotation for the first time. Background POS tagging is a well studied NLP task. Much recent work on this task has moved away from English and European languages to other major languages such as Arabic (Aldarmaki and Diab, 2015), Chinese (Sun and Wan, 2016), dialects thereof (Darwish et al., 2018), and text types containing more noise such as hist"
W19-4332,W18-6125,0,0.0586223,"Missing"
W19-4332,W17-4404,0,0.0607211,"Missing"
W19-4332,N18-1054,0,0.0666737,"Missing"
W19-4332,L18-1550,0,0.0278652,"9.04 0.032 200 99.09 0.027 300 99.05 0.030 -Mini-batch size 8 99.08 0.032 16 99.10 0.030 32 99.04 0.034 64 99.13 0.030 -Dropout rates 10 99.11 0.033 20 98.60 0.051 30 97.68 0.083 40 95.92 0.137 50 93.08 0.232 Table 1: Basic statistics from the training data and embedding model used in this experiment. that allows one to create embeddings from the training data, it is more beneficial to leverage existing models trained with much larger Welsh text data than to only rely on what is currently available. To that effect, we used the Welsh pre-trained embedding models built by the FastText Project8 (Grave et al., 2018). 3.3 70.26 94.51 94.76 95.23 95.38 3.517 5.313 4.775 4.650 4.994 95.29 95.55 95.03 94.97 4.450 4.552 4.758 4.905 95.38 94.80 94.27 92.85 90.32 3.807 3.873 3.434 3.362 3.280 Table 2: Parameter optimisation: Training and Evaluation of scores on Accuracy and Loss. Parameter values in bold were chosen. Design of experiment The key input data to our pipeline consists of the 611 sentences that are jointly annotated with the POS and semantic tags. The combination of the annotation tags on the gold standard data makes it possible to extract the data in the different formats, as shown in Table 3. Howe"
W19-4332,W18-6106,0,0.025783,"ettings (Enghoff et al., 2018). In our experiments, we focus on a low-resource mono-lingual setting with a small manually corrected corpus, and combine the Welsh POS and SEM annotation for the first time. Background POS tagging is a well studied NLP task. Much recent work on this task has moved away from English and European languages to other major languages such as Arabic (Aldarmaki and Diab, 2015), Chinese (Sun and Wan, 2016), dialects thereof (Darwish et al., 2018), and text types containing more noise such as historical (Yang and Eisenstein, 2016; Janssen et al., 2017), learner language (Nagata et al., 2018), code switching (Vyas et al., 2014) and social media varieties (Horsmann and Zesch, 2016; van der Goot et al., 2017). More recently, joint and multi-task learning approaches have been applied to link POS tagging and other tasks such as segmentation or tokenisation (AlGahtani and McNaught, 2015; Shao et al., 2017), dependency parsing (Nguyen and Verspoor, 2018) and lemmatisation (Arakelyan et al., 2018). Besides being applied to other NLP applications and levels, multi-task learning has been applied with promising results to the semantic level in various scenarios, including cross-lingual sent"
W19-4332,I17-1018,0,0.0301082,"European languages to other major languages such as Arabic (Aldarmaki and Diab, 2015), Chinese (Sun and Wan, 2016), dialects thereof (Darwish et al., 2018), and text types containing more noise such as historical (Yang and Eisenstein, 2016; Janssen et al., 2017), learner language (Nagata et al., 2018), code switching (Vyas et al., 2014) and social media varieties (Horsmann and Zesch, 2016; van der Goot et al., 2017). More recently, joint and multi-task learning approaches have been applied to link POS tagging and other tasks such as segmentation or tokenisation (AlGahtani and McNaught, 2015; Shao et al., 2017), dependency parsing (Nguyen and Verspoor, 2018) and lemmatisation (Arakelyan et al., 2018). Besides being applied to other NLP applications and levels, multi-task learning has been applied with promising results to the semantic level in various scenarios, including cross-lingual sentiment analysis (Wang et al., 2018), opinion and semantic role labelling (Marasovi´c and Frank, 2018), semantic parsing (Bordes et al., 2012), emotion prediction (Buechel and Hahn, 2018), irony detection (Wu et al., 2018) and rumour verification (Kochkina et al., 2018). However, there is very little research that a"
W19-4332,L18-1135,0,0.025397,"categories, e.g. in USAS (Rayson et al., 2004). Previously, semantic tagging in multiple languages has been shown to greatly benefit from POS tagging in the NLP pipeline, since it can help to filter out inapplicable semantic fields from the set of possible candidates (Piao et al., 2015). Over the past few years, researchers started to port NLP tools and methods into low resource languages using a various approaches, such as porting lexicons from one language to another using bilingual dictionaries and parallel corpora (Piao et al., 2016) and cross-lingual word embeddings (Adams et al., 2017; Sharoff, 2018). Multi-task learning has also been proved useful in transferring the 2.1 CyTag The rule-based POS tagger under consideration in our work, CyTag (Neale et al., 2018), was built based on Constraint Grammar (CG) (Karlsson, 1990), in particular built around the latest version of the software, VISL CG-33 . The CyTag tagset4 contains 145 fine-grained POS tags that can collapse into 13 EAGLES5 -conformant broader categories. CyTag utilises three steps to assign POS tags to tokens: • A list of candidate POS tags is produced for each token. • The list of candidate tags for each token is pruned to as f"
W19-4332,L18-1623,1,0.852944,"P pipeline, since it can help to filter out inapplicable semantic fields from the set of possible candidates (Piao et al., 2015). Over the past few years, researchers started to port NLP tools and methods into low resource languages using a various approaches, such as porting lexicons from one language to another using bilingual dictionaries and parallel corpora (Piao et al., 2016) and cross-lingual word embeddings (Adams et al., 2017; Sharoff, 2018). Multi-task learning has also been proved useful in transferring the 2.1 CyTag The rule-based POS tagger under consideration in our work, CyTag (Neale et al., 2018), was built based on Constraint Grammar (CG) (Karlsson, 1990), in particular built around the latest version of the software, VISL CG-33 . The CyTag tagset4 contains 145 fine-grained POS tags that can collapse into 13 EAGLES5 -conformant broader categories. CyTag utilises three steps to assign POS tags to tokens: • A list of candidate POS tags is produced for each token. • The list of candidate tags for each token is pruned to as few as possible (ideally one) using CG-formatted rules. • The optimal tag for each token is selected, helped by some small additional processing steps for any cases t"
W19-4332,K18-2008,0,0.0316791,"ages such as Arabic (Aldarmaki and Diab, 2015), Chinese (Sun and Wan, 2016), dialects thereof (Darwish et al., 2018), and text types containing more noise such as historical (Yang and Eisenstein, 2016; Janssen et al., 2017), learner language (Nagata et al., 2018), code switching (Vyas et al., 2014) and social media varieties (Horsmann and Zesch, 2016; van der Goot et al., 2017). More recently, joint and multi-task learning approaches have been applied to link POS tagging and other tasks such as segmentation or tokenisation (AlGahtani and McNaught, 2015; Shao et al., 2017), dependency parsing (Nguyen and Verspoor, 2018) and lemmatisation (Arakelyan et al., 2018). Besides being applied to other NLP applications and levels, multi-task learning has been applied with promising results to the semantic level in various scenarios, including cross-lingual sentiment analysis (Wang et al., 2018), opinion and semantic role labelling (Marasovi´c and Frank, 2018), semantic parsing (Bordes et al., 2012), emotion prediction (Buechel and Hahn, 2018), irony detection (Wu et al., 2018) and rumour verification (Kochkina et al., 2018). However, there is very little research that applies multi-task learning to link Word Sense Di"
W19-4332,J16-3002,0,0.0305908,"where one of the languages has only sparse resources available (Junczys-Dowmunt et al., 2018; Lin et al., 2018; Choi et al., 2018), although less successful in named entity recognition settings (Enghoff et al., 2018). In our experiments, we focus on a low-resource mono-lingual setting with a small manually corrected corpus, and combine the Welsh POS and SEM annotation for the first time. Background POS tagging is a well studied NLP task. Much recent work on this task has moved away from English and European languages to other major languages such as Arabic (Aldarmaki and Diab, 2015), Chinese (Sun and Wan, 2016), dialects thereof (Darwish et al., 2018), and text types containing more noise such as historical (Yang and Eisenstein, 2016; Janssen et al., 2017), learner language (Nagata et al., 2018), code switching (Vyas et al., 2014) and social media varieties (Horsmann and Zesch, 2016; van der Goot et al., 2017). More recently, joint and multi-task learning approaches have been applied to link POS tagging and other tasks such as segmentation or tokenisation (AlGahtani and McNaught, 2015; Shao et al., 2017), dependency parsing (Nguyen and Verspoor, 2018) and lemmatisation (Arakelyan et al., 2018). Besi"
W19-4332,N15-1137,1,0.900865,"Missing"
W19-4332,D14-1105,0,0.0234838,"experiments, we focus on a low-resource mono-lingual setting with a small manually corrected corpus, and combine the Welsh POS and SEM annotation for the first time. Background POS tagging is a well studied NLP task. Much recent work on this task has moved away from English and European languages to other major languages such as Arabic (Aldarmaki and Diab, 2015), Chinese (Sun and Wan, 2016), dialects thereof (Darwish et al., 2018), and text types containing more noise such as historical (Yang and Eisenstein, 2016; Janssen et al., 2017), learner language (Nagata et al., 2018), code switching (Vyas et al., 2014) and social media varieties (Horsmann and Zesch, 2016; van der Goot et al., 2017). More recently, joint and multi-task learning approaches have been applied to link POS tagging and other tasks such as segmentation or tokenisation (AlGahtani and McNaught, 2015; Shao et al., 2017), dependency parsing (Nguyen and Verspoor, 2018) and lemmatisation (Arakelyan et al., 2018). Besides being applied to other NLP applications and levels, multi-task learning has been applied with promising results to the semantic level in various scenarios, including cross-lingual sentiment analysis (Wang et al., 2018),"
W19-4332,D18-1031,0,0.0308549,"(Vyas et al., 2014) and social media varieties (Horsmann and Zesch, 2016; van der Goot et al., 2017). More recently, joint and multi-task learning approaches have been applied to link POS tagging and other tasks such as segmentation or tokenisation (AlGahtani and McNaught, 2015; Shao et al., 2017), dependency parsing (Nguyen and Verspoor, 2018) and lemmatisation (Arakelyan et al., 2018). Besides being applied to other NLP applications and levels, multi-task learning has been applied with promising results to the semantic level in various scenarios, including cross-lingual sentiment analysis (Wang et al., 2018), opinion and semantic role labelling (Marasovi´c and Frank, 2018), semantic parsing (Bordes et al., 2012), emotion prediction (Buechel and Hahn, 2018), irony detection (Wu et al., 2018) and rumour verification (Kochkina et al., 2018). However, there is very little research that applies multi-task learning to link Word Sense Disambiguation (WSD) or semantic tagging with another task. Here, we refer to the semantic tagging as coarse-grained word sense disambiguation based on an existing taxonomy of categories, e.g. in USAS (Rayson et al., 2004). Previously, semantic tagging in multiple language"
W19-4332,S18-1006,0,0.0645683,"Missing"
W19-4332,L18-1158,1,0.854225,"h Natural Language Toolkit (WNLT)6 . WNLT-Tagger is one of the four main modules in WNLT, which is itself built on the GATE (General Architecture for Text Engineering) framework (Cunningham, 2002). results. The semantic tagger relies heavily on a part-of-speech tagger to function. The key aim of this paper is to implement a tagging system that: • learns from unstructured data, • leverages available embedding models, • performs both tasks, POS and semantic tagging, simultaneously using a multi-task learning set up. 2.2 CySemTagger: The Welsh Semantic tagger CyTag is a precursor to CySemTagger (Piao et al., 2018) which is an automatic semantic annotation tool that depends on the POS tagged output to assign semantic tags to tokens in Welsh texts. CySemTagger employs the semantic tagset of Lancaster University’s UCREL Semantic Analysis System, USAS7 . The semantic tagset, which was originally derived from Tom McArthur’s Longman Lexicon of Contemporary English (McArthur and McArthur, 1981), has 21 major discourse fields and 232 tags. The CySemTagger is a knowledge-based and rule-based system with the following key components: 3.1 As mentioned earlier in section 2.1, the instances for training the POS and"
W19-4332,N16-1157,0,0.030554,"t al., 2018), although less successful in named entity recognition settings (Enghoff et al., 2018). In our experiments, we focus on a low-resource mono-lingual setting with a small manually corrected corpus, and combine the Welsh POS and SEM annotation for the first time. Background POS tagging is a well studied NLP task. Much recent work on this task has moved away from English and European languages to other major languages such as Arabic (Aldarmaki and Diab, 2015), Chinese (Sun and Wan, 2016), dialects thereof (Darwish et al., 2018), and text types containing more noise such as historical (Yang and Eisenstein, 2016; Janssen et al., 2017), learner language (Nagata et al., 2018), code switching (Vyas et al., 2014) and social media varieties (Horsmann and Zesch, 2016; van der Goot et al., 2017). More recently, joint and multi-task learning approaches have been applied to link POS tagging and other tasks such as segmentation or tokenisation (AlGahtani and McNaught, 2015; Shao et al., 2017), dependency parsing (Nguyen and Verspoor, 2018) and lemmatisation (Arakelyan et al., 2018). Besides being applied to other NLP applications and levels, multi-task learning has been applied with promising results to the se"
W19-5604,D11-1145,0,0.127527,"Missing"
W19-5604,J08-4004,0,0.0135866,"coder Reliability Annotation Process The process starts with labeling the tweets by two Arabic native speakers, including the first author of the paper, who were provided with the guidelines detailed above. The classification depends first on the text in the tweet itself. If the tweet has an ambiguous classification, the annotator may need to look at the bio, a description written by the twitter user, of the user tweet. 4.2 Figure 1: Heat map of confusion matrix between the two annotators. Inter-coder Reliability We used the Kappa Statistic to test the robustness of the classification scheme (Artstein and Poesio, 2008). The result shows that Cohen’s Kappa score is 0.84 which indicates strong agreement between the two manual coders. Figure 1 shows the confusion matrix between the two annotators. The most 5 Machine Learning Models In our study, we used Python Scikit-learn 0.20.2 (Pedregosa et al., 2011) software and applied four ML models: Logistic Regression, Random Forest, Multinomial Na¨ıve Bayes and LSVC: Linear SupTweet in Arabic Translated tweet to English x¤ry y`W @ Ty}wt . §rAsml TbO Recommend vaccination of the measles virus to travelers. z r £d Antt : ¢`yr A rWR¯ ¤ YRrm ¢yAR"
W19-5604,P14-5010,0,0.00924874,"Missing"
W19-5604,W15-3821,0,0.114532,"Missing"
wattam-etal-2012-document,kilgarriff-etal-2010-corpus,0,\N,Missing
wattam-etal-2012-document,baroni-bernardini-2004-bootcat,0,\N,Missing
wattam-etal-2012-document,J03-3001,0,\N,Missing
wattam-etal-2014-experiences,varadi-etal-2008-clarin,0,\N,Missing
